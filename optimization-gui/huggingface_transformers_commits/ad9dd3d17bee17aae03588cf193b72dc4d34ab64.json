{
    "author": "zucchini-nlp",
    "message": "ðŸ”´ [VLM] modeling updates (#38317)\n\n* updates\n\n* fixup\n\n* fix tests\n\n* fix test\n\n* fix\n\n* let it be here for now, till monday\n\n* two more fixes\n\n* persimmon\n\n* fixup\n\n* fix\n\n* fixup\n\n* make sure fuyu runs now that LM has new attn API\n\n* fixup + tests\n\n* qwen vl uses new mask interface as well\n\n* qwen image features format\n\n* update\n\n* remove image_sizes\n\n* address comments\n\n* i am dumb...",
    "sha": "ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
    "files": [
        {
            "sha": "ebddfcfe71152027209554394b27411826bf3257",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -673,7 +673,7 @@ class AriaPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = False  # MoE models don't work with torch.compile (dynamic slicing)\n-    _supports_attention_backend = False\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "348da86bb14259b279a5c79dfc147a5b1deac26a",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -1299,7 +1299,7 @@ class AriaPreTrainedModel(LlamaPreTrainedModel):\n     config_class = AriaConfig\n     base_model_prefix = \"\"\n     _supports_static_cache = False  # MoE models don't work with torch.compile (dynamic slicing)\n-    _supports_attention_backend = False\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "519800f928572c45c6712cb448d82333f1503979",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -282,7 +282,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        image_sizes: torch.Tensor = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, AyaVisionModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -310,7 +309,6 @@ def forward(\n                 pixel_values=pixel_values,\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n-                image_sizes=image_sizes,\n             )\n \n             if input_ids is None:"
        },
        {
            "sha": "533a7f444472321514ae098b66d730d7adfe70a7",
            "filename": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "status": "modified",
            "additions": 148,
            "deletions": 2,
            "changes": 150,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -24,12 +24,14 @@\n     LlavaCausalLMOutputWithPast,\n     LlavaForConditionalGeneration,\n     LlavaModel,\n+    LlavaModelOutputWithPast,\n     LlavaPreTrainedModel,\n )\n \n from ...activations import ACT2FN\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...processing_utils import Unpack\n-from ...utils import logging\n+from ...utils import auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n from .configuration_aya_vision import AyaVisionConfig\n \n \n@@ -110,10 +112,154 @@ class AyaVisionCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n     pass\n \n \n-class AyaVisionModel(LlavaModel):\n+class AyaVisionModelOutputWithPast(LlavaModelOutputWithPast):\n     pass\n \n \n+class AyaVisionModel(LlavaModel):\n+    # Unlike LLaVA, the model doesn't have to deal with Pixtral-style image states\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n+               The tensors corresponding to the input images.\n+            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n+            vision_feature_select_strategy (`str`, *optional*):\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n+                Can be one of `\"default\"` or `\"full\"`\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n+        if vision_feature_select_strategy not in [\"default\", \"full\"]:\n+            raise ValueError(f\"Unexpected select feature strategy: {self.config.vision_feature_select_strategy}\")\n+\n+        kwargs = {k: v for k, v in kwargs.items() if v is not None}\n+        # this is not memory efficient at all (output_hidden_states=True) will save all the hidden states.\n+        image_outputs = self.vision_tower(pixel_values, output_hidden_states=True, **kwargs)\n+\n+        # If we have one vision feature layer, return the corresponding hidden states,\n+        # otherwise, select the hidden states of each feature layer and concatenate them\n+        if isinstance(vision_feature_layer, int):\n+            selected_image_feature = image_outputs.hidden_states[vision_feature_layer]\n+            if vision_feature_select_strategy == \"default\":\n+                selected_image_feature = selected_image_feature[:, 1:]\n+        else:\n+            hs_pool = [image_outputs.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n+            # For default; crop CLS from each hidden state in the hidden state pool\n+            if vision_feature_select_strategy == \"default\":\n+                hs_pool = [hs[:, 1:] for hs in hs_pool]\n+            selected_image_feature = torch.cat(hs_pool, dim=-1)\n+\n+        image_features = self.multi_modal_projector(selected_image_feature)\n+        return image_features\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, AyaVisionModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+            )\n+\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+            else:\n+                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                n_image_features = image_features.shape[0] * image_features.shape[1]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        return AyaVisionModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+\n class AyaVisionForConditionalGeneration(LlavaForConditionalGeneration):\n     def forward(\n         self,"
        },
        {
            "sha": "70d9a7cd7a04862f15e9999a010d64145d2d10f6",
            "filename": "src/transformers/models/chameleon/configuration_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -247,6 +247,7 @@ def __init__(\n         self.vq_config = ChameleonVQVAEConfig(**vq_config)\n \n         self.vocabulary_map = vocabulary_map\n+        self.image_token_id = vocabulary_map.get(\"<image>\") if vocabulary_map is not None else None\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "c816fe610530b4d3e32a2d1e1cc0027d5414e455",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 14,
            "deletions": 7,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -904,12 +904,6 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     def get_image_tokens(self, pixel_values: torch.FloatTensor):\n-        logger.warning(\n-            \"`model.get_image_tokens()` is deprecated and will be removed in v4.58. To obtain discrete token use `model.get_image_features()`\"\n-        )\n-        return self.get_image_featues(pixel_values)\n-\n-    def get_image_features(self, pixel_values: torch.FloatTensor):\n         \"\"\"\n         Tokenizes images into discrete tokens with VQGAN module. Converts\n         obtained image tokens into BPE tokens and wraps with \"boi\" and \"eoi\"\n@@ -925,6 +919,19 @@ def get_image_features(self, pixel_values: torch.FloatTensor):\n         bpe_toks = bpe_toks.view(batch_size, -1)\n         return bpe_toks\n \n+    def get_image_features(self, pixel_values: torch.FloatTensor):\n+        \"\"\"\n+        Tokenizes images into discrete tokens with VQGAN module and embeds\n+        them with text embeddings layer\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n+                The tensors corresponding to the input images.\n+        \"\"\"\n+        image_tokens = self.get_image_tokens(pixel_values)\n+        vision_embeddings = self.get_input_embeddings()(image_tokens)\n+        return vision_embeddings\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -963,7 +970,7 @@ def forward(\n             )\n \n         if pixel_values is not None:\n-            image_tokens = self.get_image_features(pixel_values)\n+            image_tokens = self.get_image_tokens(pixel_values)\n             special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n             if not is_torchdynamo_compiling() and input_ids[special_image_mask].numel() != image_tokens.numel():\n                 n_image_tokens_in_text = (input_ids == self.vocabulary_mapping.image_token_id).sum()"
        },
        {
            "sha": "19315003dfb1c92089a4125c34cc96d6ee1a9765",
            "filename": "src/transformers/models/emu3/configuration_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -320,6 +320,7 @@ def __init__(\n         self.vq_config = vq_config\n         self.text_config = text_config\n         self.vocabulary_map = vocabulary_map\n+        self.image_token_id = vocabulary_map.get(\"<image>\") if vocabulary_map is not None else None\n \n         super().__init__(**kwargs)\n "
        },
        {
            "sha": "6540e9fc714996ef683246822bcede2ecefa67fd",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 19,
            "deletions": 7,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -1451,12 +1451,6 @@ def set_input_embeddings(self, value):\n         self.text_model.set_input_embeddings(value)\n \n     def get_image_tokens(self, pixel_values: torch.FloatTensor, image_sizes: torch.LongTensor):\n-        logger.warning(\n-            \"`model.get_image_tokens()` is deprecated and will be removed in v4.58. To obtain discrete token use `model.get_image_features()`\"\n-        )\n-        return self.get_image_featues(pixel_values)\n-\n-    def get_image_features(self, pixel_values: torch.FloatTensor, image_sizes: torch.LongTensor):\n         \"\"\"\n         Tokenizes images into discrete tokens with VQGAN module. Converts\n         obtained image tokens into BPE tokens and wraps with \"boi\" and \"eoi\"\n@@ -1473,6 +1467,24 @@ def get_image_features(self, pixel_values: torch.FloatTensor, image_sizes: torch\n         bpe_tokens = torch.cat(bpe_tokens_list)\n         return bpe_tokens\n \n+    def get_image_features(self, pixel_values: torch.FloatTensor, image_sizes: torch.LongTensor):\n+        \"\"\"\n+        Tokenizes images into discrete tokens with VQGAN module and embeds\n+        them with text embeddings layer\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n+                The tensors corresponding to the input images.\n+        \"\"\"\n+        image_tokens = self.get_image_tokens(pixel_values, image_sizes)\n+        split_sizes = [\n+            (height // self.vqmodel.vision_spatial_factor) * (width // self.vqmodel.vision_spatial_factor + 1)\n+            for height, width in image_sizes\n+        ]\n+        image_features = self.get_input_embeddings()(image_tokens)\n+        image_features = torch.split(image_features, split_sizes)\n+        return image_features\n+\n     @torch.no_grad\n     def decode_image_tokens(self, image_tokens: torch.LongTensor, height: int, width: int):\n         \"\"\"\n@@ -1533,7 +1545,7 @@ def forward(\n             )\n \n         if pixel_values is not None:\n-            image_tokens = self.get_image_features(pixel_values, image_sizes)\n+            image_tokens = self.get_image_tokens(pixel_values, image_sizes)\n             special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n             image_tokens = image_tokens.to(input_ids.device, input_ids.dtype)\n             input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)"
        },
        {
            "sha": "d6e34eb14af398b1b1f340987afc31c56299f8df",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 19,
            "deletions": 7,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -938,12 +938,6 @@ def set_input_embeddings(self, value):\n         self.text_model.set_input_embeddings(value)\n \n     def get_image_tokens(self, pixel_values: torch.FloatTensor, image_sizes: torch.LongTensor):\n-        logger.warning(\n-            \"`model.get_image_tokens()` is deprecated and will be removed in v4.58. To obtain discrete token use `model.get_image_features()`\"\n-        )\n-        return self.get_image_featues(pixel_values)\n-\n-    def get_image_features(self, pixel_values: torch.FloatTensor, image_sizes: torch.LongTensor):\n         \"\"\"\n         Tokenizes images into discrete tokens with VQGAN module. Converts\n         obtained image tokens into BPE tokens and wraps with \"boi\" and \"eoi\"\n@@ -960,6 +954,24 @@ def get_image_features(self, pixel_values: torch.FloatTensor, image_sizes: torch\n         bpe_tokens = torch.cat(bpe_tokens_list)\n         return bpe_tokens\n \n+    def get_image_features(self, pixel_values: torch.FloatTensor, image_sizes: torch.LongTensor):\n+        \"\"\"\n+        Tokenizes images into discrete tokens with VQGAN module and embeds\n+        them with text embeddings layer\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n+                The tensors corresponding to the input images.\n+        \"\"\"\n+        image_tokens = self.get_image_tokens(pixel_values, image_sizes)\n+        split_sizes = [\n+            (height // self.vqmodel.vision_spatial_factor) * (width // self.vqmodel.vision_spatial_factor + 1)\n+            for height, width in image_sizes\n+        ]\n+        image_features = self.get_input_embeddings()(image_tokens)\n+        image_features = torch.split(image_features, split_sizes)\n+        return image_features\n+\n     @torch.no_grad\n     def decode_image_tokens(self, image_tokens: torch.LongTensor, height: int, width: int):\n         \"\"\"\n@@ -1020,7 +1032,7 @@ def forward(\n             )\n \n         if pixel_values is not None:\n-            image_tokens = self.get_image_features(pixel_values, image_sizes)\n+            image_tokens = self.get_image_tokens(pixel_values, image_sizes)\n             special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n             image_tokens = image_tokens.to(input_ids.device, input_ids.dtype)\n             input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)"
        },
        {
            "sha": "4584184b0aeb9a5439af40787a1759dc1c786846",
            "filename": "src/transformers/models/fuyu/configuration_fuyu.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -16,7 +16,7 @@\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -89,6 +89,8 @@ class FuyuConfig(PretrainedConfig):\n             The id of the *beginning-of-sequence* token.\n         eos_token_id (`Union[int, List[int]]`, *optional*, defaults to 2):\n             The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n+        image_token_id (`int`, *optional*, defaults to 71011):\n+            The id of the image placeholder token.\n         text_config (`dict`, *optional*):\n             Dictionary of configuration options used to initialize the `language``[`Aut`].\n \n@@ -100,6 +102,7 @@ class FuyuConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"fuyu\"\n+    sub_configs = {\"text_config\": AutoConfig}\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n \n     def __init__(\n@@ -127,6 +130,7 @@ def __init__(\n         pad_token_id=None,\n         bos_token_id=1,\n         eos_token_id=2,\n+        image_token_id=71011,\n         text_config=None,\n         **kwargs,\n     ):\n@@ -176,6 +180,7 @@ def __init__(\n         self.hidden_dropout = hidden_dropout\n         self.attention_dropout = attention_dropout\n         self.partial_rotary_factor = partial_rotary_factor\n+        self.image_token_id = image_token_id\n         self._rope_scaling_validation()\n \n         super().__init__("
        },
        {
            "sha": "78b7ae7d4db9be2f7b0bd9b63c3154ae12592731",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -202,11 +202,12 @@ def forward(\n             inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n             if image_patches is not None and past_key_values is None:\n                 patch_embeddings = self.get_image_features(image_patches)\n-                inputs_embeds = self.gather_continuous_embeddings(\n-                    word_embeddings=inputs_embeds,\n-                    continuous_embeddings=patch_embeddings,\n-                    image_patch_input_indices=image_patches_indices,\n-                )\n+                patch_embeddings = torch.cat(patch_embeddings, dim=0)\n+\n+                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+                patch_embeddings = patch_embeddings.to(inputs_embeds.device, inputs_embeds.dtype)\n+                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, patch_embeddings)\n \n         outputs = self.language_model(\n             inputs_embeds=inputs_embeds,"
        },
        {
            "sha": "3c71c34c6f9e79debb4e2eea49a0c8dc790297da",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -620,12 +620,16 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n                 width_scale_factor = padded_width / image_size[1]\n                 optimal_scale_factor = min(height_scale_factor, width_scale_factor)\n \n-                # We can use torch here because Fuyu processor has hard dependency on torch\n+                image_unpadded_h = min(int(image_size[0] * optimal_scale_factor), image_size[0])\n+                image_unpadded_w = min(int(image_size[0] * optimal_scale_factor), image_size[0])\n+\n+                # We can use torch here because Fuyu processor has hard dependency on torch. NOTE: Fuyu can't do multi-image\n+                # thus the below (1, 1, 1) is hardcoded. Same as when calling the processor\n                 model_image_input = self.image_processor.preprocess_with_tokenizer_info(\n                     image_input=torch.zeros(1, 1, 3, padded_height, padded_width),\n                     image_present=torch.ones(1, 1, 1),\n-                    image_unpadded_h=torch.tensor([[int(image_size[0] * optimal_scale_factor)]]),\n-                    image_unpadded_w=torch.tensor([[int(image_size[1] * optimal_scale_factor)]]),\n+                    image_unpadded_h=torch.tensor([[image_unpadded_h]]),\n+                    image_unpadded_w=torch.tensor([[image_unpadded_w]]),\n                     image_placeholder_id=0,  # dummy ids, we can be sure `id=0` is never out-of-range\n                     image_newline_id=0,\n                     variable_sized=True,"
        },
        {
            "sha": "95b50039eb9b6ff9c3def7716c5dd3e908621d1d",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -377,7 +377,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,  # NOOP kwarg for now\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -446,6 +446,7 @@ def forward(\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n+                **kwargs,\n             )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "648caea73d796a1b096a4aa317e43abe1eb04963",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -23,7 +23,9 @@\n from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PretrainedConfig\n from ...masking_utils import create_causal_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n+from ...processing_utils import Unpack\n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n from ..llama.modeling_llama import (\n@@ -378,7 +380,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,  # NOOP kwarg for now\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -447,6 +449,7 @@ def forward(\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n+                **kwargs,\n             )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "41043b3e7455d84164d511cb3ed58f5f7754c120",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -693,8 +693,10 @@ def inputs_merger(\n         - To fit the format of that sequence, `input_ids`, `input_embeds`, `attention_mask` are all 3 adapted to insert the image hidden states.\n         \"\"\"\n         special_image_token_mask = input_ids == self.image_token_id\n-        #  Fixes RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.\n+        # Fixes RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.\n         new_inputs_embeds = inputs_embeds.clone()\n+        # Flatten `image_hidden_states` if not flat yet\n+        image_hidden_states = image_hidden_states.view(-1, image_hidden_states.shape[-1])\n         # cast to the dtype of the input_embeds to support quantized models\n         image_hidden_states = image_hidden_states.to(inputs_embeds.device, inputs_embeds.dtype)\n         new_inputs_embeds[special_image_token_mask] = image_hidden_states\n@@ -742,7 +744,6 @@ def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_ma\n \n         # Modality projection & resampling\n         image_hidden_states = self.connector(image_hidden_states.last_hidden_state)\n-        image_hidden_states = image_hidden_states.view(-1, image_hidden_states.shape[-1])\n         return image_hidden_states\n \n     @can_return_tuple\n@@ -807,9 +808,6 @@ def forward(\n                 past_key_values = DynamicCache()\n             past_seen_tokens = past_key_values.get_seq_length()\n \n-        if inputs_embeds is not None and input_ids is None and past_seen_tokens == 0:\n-            raise ValueError(\"When first calling the model, if input_embeds are passed, input_ids should not be None.\")\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.text_model.get_input_embeddings()(input_ids).to(self.device)\n \n@@ -821,7 +819,7 @@ def forward(\n         elif image_hidden_states is not None:\n             image_hidden_states = image_hidden_states.to(dtype=self.dtype, device=input_ids.device)\n \n-        if past_seen_tokens == 0 and inputs_embeds is not None and image_hidden_states is not None:\n+        if past_seen_tokens == 0 and input_ids is not None and image_hidden_states is not None:\n             # When we generate, we don't want to replace the potential image_token_id that we generated by images\n             # that simply don't exist\n             inputs_embeds = self.inputs_merger("
        },
        {
            "sha": "b2e539e7bcea144debff8741ee532da47f3964ee",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 11,
            "deletions": 5,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -627,8 +627,8 @@ def set_input_embeddings(self, value):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        vision_feature_layer: Union[int, List[int]],\n-        vision_feature_select_strategy: str,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -642,6 +642,15 @@ def get_image_features(\n         Returns:\n             vision_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`.\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n         downsample_ratio = self.config.downsample_ratio\n         if vision_feature_layer == -1:\n             vision_features = self.vision_tower(pixel_values=pixel_values).last_hidden_state\n@@ -666,7 +675,6 @@ def get_image_features(\n \n         # Project features through multi-modal projector\n         vision_features = self.multi_modal_projector(vision_features)\n-\n         return vision_features\n \n     @can_return_tuple\n@@ -686,7 +694,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        image_sizes: torch.Tensor = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, InternVLModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -714,7 +721,6 @@ def forward(\n                 pixel_values=pixel_values,\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n-                image_sizes=image_sizes,\n             )\n \n             if input_ids is None:"
        },
        {
            "sha": "fcf4958f62383120488663917c8c771915964c75",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 103,
            "deletions": 4,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -27,14 +27,15 @@\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils import auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging, torch_int\n from ..clip.modeling_clip import CLIPMLP\n from ..janus.modeling_janus import JanusVisionAttention\n from ..llama.modeling_llama import LlamaRMSNorm\n from ..llava.modeling_llava import (\n     LlavaCausalLMOutputWithPast,\n     LlavaForConditionalGeneration,\n     LlavaModel,\n+    LlavaModelOutputWithPast,\n     LlavaPreTrainedModel,\n )\n from .configuration_internvl import InternVLConfig, InternVLVisionConfig\n@@ -510,6 +511,10 @@ def forward(self, image_features):\n         return hidden_states\n \n \n+class InternVLModelOutputWithPast(LlavaModelOutputWithPast):\n+    pass\n+\n+\n class InternVLModel(LlavaModel):\n     def pixel_shuffle(self, vision_features: torch.Tensor, scale_factor: float = 0.5):\n         \"\"\"Perform pixel shuffle downsampling on vision features.\n@@ -549,8 +554,8 @@ def pixel_shuffle(self, vision_features: torch.Tensor, scale_factor: float = 0.5\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        vision_feature_layer: Union[int, List[int]],\n-        vision_feature_select_strategy: str,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -564,6 +569,15 @@ def get_image_features(\n         Returns:\n             vision_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`.\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n         downsample_ratio = self.config.downsample_ratio\n         if vision_feature_layer == -1:\n             vision_features = self.vision_tower(pixel_values=pixel_values).last_hidden_state\n@@ -588,9 +602,94 @@ def get_image_features(\n \n         # Project features through multi-modal projector\n         vision_features = self.multi_modal_projector(vision_features)\n-\n         return vision_features\n \n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, InternVLModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+            )\n+\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+            else:\n+                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                n_image_features = image_features.shape[0] * image_features.shape[1]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        return InternVLModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n \n class InternVLCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n     pass"
        },
        {
            "sha": "0ab2333d1ba9d4d94f9872213ee7b4b8494404d5",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -233,6 +233,15 @@ def get_image_features(\n             selected_image_feature = torch.cat(hs_pool, dim=-1)\n \n         image_features = self.multi_modal_projector(selected_image_feature)\n+\n+        if \"image_sizes\" in kwargs:\n+            split_sizes = [\n+                (height // self.vision_tower.patch_size) * (width // self.vision_tower.patch_size)\n+                for height, width in kwargs[\"image_sizes\"]\n+            ]\n+            image_features = torch.split(image_features.squeeze(0), split_sizes)\n+        else:\n+            image_features = list(image_features)\n         return image_features\n \n     @can_return_tuple\n@@ -282,6 +291,7 @@ def forward(\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n                 image_sizes=image_sizes,\n             )\n+            image_features = torch.cat(image_features, dim=0)\n \n             if input_ids is None:\n                 special_image_mask = inputs_embeds == self.get_input_embeddings()("
        },
        {
            "sha": "bd1e1aae2536624eecba3e109bdaf21a9ebe84d5",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -357,9 +357,8 @@ def pack_image_features(self, image_features, image_sizes, vision_feature_select\n                     image_feature = torch.cat((image_feature, image_newline[None].to(image_feature)), dim=0)\n             new_image_features.append(image_feature)\n             feature_lens.append(image_feature.size(0))\n-        image_features = torch.cat(new_image_features, dim=0)\n-        feature_lens = torch.tensor(feature_lens, dtype=torch.long, device=image_features.device)\n-        return image_features, feature_lens\n+        feature_lens = torch.tensor(feature_lens, dtype=torch.long, device=image_features[0].device)\n+        return new_image_features, feature_lens\n \n     def get_image_features(\n         self,\n@@ -429,6 +428,14 @@ def get_image_features(\n \n         image_features = self.multi_modal_projector(selected_image_feature)\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n+\n+        # NOTE we only support multimodal_patch_merge_type == \"spatial_unpad\"\n+        image_features, feature_lens = self.pack_image_features(\n+            image_features,\n+            image_sizes,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+            image_newline=self.image_newline,\n+        )\n         return image_features\n \n     @can_return_tuple\n@@ -489,14 +496,7 @@ def forward(\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n             )\n-\n-            # NOTE we only support multimodal_patch_merge_type == \"spatial_unpad\"\n-            image_features, feature_lens = self.pack_image_features(\n-                image_features,\n-                image_sizes,\n-                vision_feature_select_strategy=vision_feature_select_strategy,\n-                image_newline=self.image_newline,\n-            )\n+            image_features = torch.cat(image_features, dim=0)\n \n             special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)"
        },
        {
            "sha": "20b1647c0bd672b9efa90f447fb0c1f877f97dc8",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -411,9 +411,8 @@ def pack_image_features(self, image_features, image_sizes, vision_feature_select\n                     image_feature = torch.cat((image_feature, image_newline[None].to(image_feature)), dim=0)\n             new_image_features.append(image_feature)\n             feature_lens.append(image_feature.size(0))\n-        image_features = torch.cat(new_image_features, dim=0)\n-        feature_lens = torch.tensor(feature_lens, dtype=torch.long, device=image_features.device)\n-        return image_features, feature_lens\n+        feature_lens = torch.tensor(feature_lens, dtype=torch.long, device=image_features[0].device)\n+        return new_image_features, feature_lens\n \n     def get_image_features(\n         self,\n@@ -482,6 +481,13 @@ def get_image_features(\n             selected_image_feature = selected_image_feature\n         image_features = self.multi_modal_projector(selected_image_feature)\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n+\n+        image_features, feature_lens = self.pack_image_features(\n+            image_features,\n+            image_sizes,\n+            vision_feature_select_strategy,\n+            image_newline=self.image_newline,\n+        )\n         return image_features\n \n     @can_return_tuple\n@@ -544,12 +550,7 @@ def forward(\n                 vision_feature_layer=self.vision_feature_layer,\n                 vision_feature_select_strategy=self.vision_feature_select_strategy,\n             )\n-            image_features, feature_lens = self.pack_image_features(\n-                image_features,\n-                image_sizes,\n-                self.vision_feature_select_strategy,\n-                image_newline=self.image_newline,\n-            )\n+            image_features = torch.cat(image_features, dim=0)\n \n             special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)"
        },
        {
            "sha": "e5d2fd92cb47088557ffc26e9153fda976b32e3f",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -315,6 +315,13 @@ def get_image_features(\n             selected_image_feature = selected_image_feature\n         image_features = self.multi_modal_projector(selected_image_feature)\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n+\n+        image_features, feature_lens = self.pack_image_features(\n+            image_features,\n+            image_sizes,\n+            vision_feature_select_strategy,\n+            image_newline=self.image_newline,\n+        )\n         return image_features\n \n     def get_video_features(\n@@ -430,12 +437,7 @@ def forward(\n                 vision_feature_layer=self.vision_feature_layer,\n                 vision_feature_select_strategy=self.vision_feature_select_strategy,\n             )\n-            image_features, feature_lens = self.pack_image_features(\n-                image_features,\n-                image_sizes,\n-                self.vision_feature_select_strategy,\n-                image_newline=self.image_newline,\n-            )\n+            image_features = torch.cat(image_features, dim=0)\n \n             special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)"
        },
        {
            "sha": "a43674d294215e14966f41a3369b3559fac0b774",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 26,
            "deletions": 11,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -409,18 +409,19 @@ def pack_image_features(self, image_features, image_sizes, image_newline=None, v\n                 image_feature = image_feature[0]\n                 if image_newline is not None:\n                     image_feature = torch.cat((image_feature, image_newline[None].to(image_feature)), dim=0)\n+                image_feature = image_feature.flatten(0, 1)\n             new_image_features.append(image_feature)\n             feature_lens.append(image_feature.size(0))\n-        image_features = torch.cat(new_image_features, dim=0)\n-        feature_lens = torch.tensor(feature_lens, dtype=torch.long, device=image_features.device)\n-        return image_features, feature_lens\n+        feature_lens = torch.tensor(feature_lens, dtype=torch.long, device=image_features[0].device)\n+        return new_image_features, feature_lens\n \n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n         image_sizes: torch.Tensor,\n-        vision_feature_layer: Union[int, List[int]],\n-        vision_feature_select_strategy: str,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        vision_aspect_ratio: Optional[str] = None,\n         batch_num_images: Optional[torch.LongTensor] = None,\n     ):\n         \"\"\"\n@@ -444,6 +445,18 @@ def get_image_features(\n             image_features (List[`torch.Tensor`]): List of image feature tensor, each contains all the visual feature of all patches\n             and are of shape `(num_patches, image_length, embed_dim)`).\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+        vision_aspect_ratio = (\n+            vision_aspect_ratio if vision_aspect_ratio is not None else self.config.vision_aspect_ratio\n+        )\n+\n         # ! infer image_num_patches from image_sizes\n         if batch_num_images is None:\n             # treat this as a single-image case for backward compatibility\n@@ -483,6 +496,13 @@ def get_image_features(\n             selected_image_feature = selected_image_feature\n         image_features = self.multi_modal_projector(selected_image_feature)\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n+\n+        image_features, feature_lens = self.pack_image_features(\n+            image_features,\n+            image_sizes,\n+            image_newline=self.image_newline,\n+            vision_aspect_ratio=vision_aspect_ratio,\n+        )\n         return image_features\n \n     @can_return_tuple\n@@ -564,12 +584,7 @@ def forward(\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n                 batch_num_images=batch_num_images,\n             )\n-            image_features, feature_lens = self.pack_image_features(\n-                image_features,\n-                image_sizes,\n-                image_newline=self.image_newline,\n-                vision_aspect_ratio=vision_aspect_ratio,\n-            )\n+            image_features = torch.cat(image_features, dim=0)\n \n             special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)"
        },
        {
            "sha": "e61eca445e908283ecae63ac27590ba119285c2e",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 26,
            "deletions": 11,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -316,11 +316,11 @@ def pack_image_features(self, image_features, image_sizes, image_newline=None, v\n                 image_feature = image_feature[0]\n                 if image_newline is not None:\n                     image_feature = torch.cat((image_feature, image_newline[None].to(image_feature)), dim=0)\n+                image_feature = image_feature.flatten(0, 1)\n             new_image_features.append(image_feature)\n             feature_lens.append(image_feature.size(0))\n-        image_features = torch.cat(new_image_features, dim=0)\n-        feature_lens = torch.tensor(feature_lens, dtype=torch.long, device=image_features.device)\n-        return image_features, feature_lens\n+        feature_lens = torch.tensor(feature_lens, dtype=torch.long, device=image_features[0].device)\n+        return new_image_features, feature_lens\n \n     def apply_pooling(self, image_features):\n         height = width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n@@ -340,8 +340,9 @@ def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n         image_sizes: torch.Tensor,\n-        vision_feature_layer: Union[int, List[int]],\n-        vision_feature_select_strategy: str,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        vision_aspect_ratio: Optional[str] = None,\n         batch_num_images: Optional[torch.LongTensor] = None,\n     ):\n         \"\"\"\n@@ -365,6 +366,18 @@ def get_image_features(\n             image_features (List[`torch.Tensor`]): List of image feature tensor, each contains all the visual feature of all patches\n             and are of shape `(num_patches, image_length, embed_dim)`).\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+        vision_aspect_ratio = (\n+            vision_aspect_ratio if vision_aspect_ratio is not None else self.config.vision_aspect_ratio\n+        )\n+\n         # ! infer image_num_patches from image_sizes\n         if batch_num_images is None:\n             # treat this as a single-image case for backward compatibility\n@@ -404,6 +417,13 @@ def get_image_features(\n             selected_image_feature = selected_image_feature\n         image_features = self.multi_modal_projector(selected_image_feature)\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n+\n+        image_features, feature_lens = self.pack_image_features(\n+            image_features,\n+            image_sizes,\n+            image_newline=self.image_newline,\n+            vision_aspect_ratio=vision_aspect_ratio,\n+        )\n         return image_features\n \n     def get_video_features(\n@@ -529,12 +549,7 @@ def forward(\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n                 batch_num_images=batch_num_images,\n             )\n-            image_features, feature_lens = self.pack_image_features(\n-                image_features,\n-                image_sizes,\n-                image_newline=self.image_newline,\n-                vision_aspect_ratio=vision_aspect_ratio,\n-            )\n+            image_features = torch.cat(image_features, dim=0)\n \n             special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)"
        },
        {
            "sha": "c5a5205947f3aeb84be666cdc4ee02c0503db83c",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -285,6 +285,9 @@ def get_image_features(\n             selected_image_feature = torch.cat(hs_pool, dim=-1)\n \n         image_features = self.multi_modal_projector(selected_image_feature.squeeze(0), image_sizes)\n+        downsample_ratio = self.vision_tower.patch_size * self.config.spatial_merge_size\n+        split_sizes = [(height // downsample_ratio) * (width // downsample_ratio) for height, width in image_sizes]\n+        image_features = torch.split(image_features.squeeze(0), split_sizes)\n         return image_features\n \n     @can_return_tuple\n@@ -332,6 +335,7 @@ def forward(\n                 vision_feature_layer=vision_feature_layer,\n                 image_sizes=image_sizes,\n             )\n+            image_features = torch.cat(image_features, dim=0)\n \n             special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)"
        },
        {
            "sha": "666b4cde4d5aa55aadd4d92b106f0562f412b07a",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -170,6 +170,9 @@ def get_image_features(\n             selected_image_feature = torch.cat(hs_pool, dim=-1)\n \n         image_features = self.multi_modal_projector(selected_image_feature.squeeze(0), image_sizes)\n+        downsample_ratio = self.vision_tower.patch_size * self.config.spatial_merge_size\n+        split_sizes = [(height // downsample_ratio) * (width // downsample_ratio) for height, width in image_sizes]\n+        image_features = torch.split(image_features.squeeze(0), split_sizes)\n         return image_features\n \n     def forward(\n@@ -215,6 +218,7 @@ def forward(\n                 vision_feature_layer=vision_feature_layer,\n                 image_sizes=image_sizes,\n             )\n+            image_features = torch.cat(image_features, dim=0)\n \n             special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)"
        },
        {
            "sha": "a5376d651cee180410e2ff149ed35e890085a1bc",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 51,
            "deletions": 22,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -19,8 +19,7 @@\n # limitations under the License.\n \"\"\"PyTorch Persimmon model.\"\"\"\n \n-import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -30,14 +29,16 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n from .configuration_persimmon import PersimmonConfig\n \n@@ -137,6 +138,29 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class PersimmonAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -166,6 +190,7 @@ def __init__(self, config: PersimmonConfig, layer_idx: Optional[int] = None):\n         self.query_key_value = nn.Linear(self.hidden_size, 3 * self.hidden_size, bias=True)\n         self.dense = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=True)\n         self.qk_layernorm = config.qk_layernorm\n+        self.scaling = self.head_dim**-0.5\n \n         if self.qk_layernorm:\n             self.q_layernorm = nn.LayerNorm(\n@@ -203,6 +228,7 @@ def forward(\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -249,27 +275,22 @@ def forward(\n             }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dtype=torch.float32, dim=-1).to(query_states.dtype)\n-        attn_weights = self.attention_dropout(attn_weights)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.config.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n \n+        attn_output = attn_output.reshape(bsz, q_len, -1)\n         attn_output = self.dense(attn_output)\n \n         if not output_attentions:\n@@ -298,6 +319,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -337,6 +359,7 @@ def forward(\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -369,6 +392,9 @@ class PersimmonPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -430,6 +456,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -512,6 +539,7 @@ def forward(\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n                     position_embeddings=position_embeddings,\n+                    **kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n@@ -758,6 +786,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs.last_hidden_state"
        },
        {
            "sha": "9225d4df6b28cb72d3fe96d06ee00b938460cdb1",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -257,7 +257,7 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n             num_image_tokens = []\n             for height, width in image_sizes:\n                 resized_height, resized_width = get_resize_output_image_size(\n-                    image=np.zeros((height, width, 3)),\n+                    np.zeros((height, width, 3)),\n                     size=(size[\"longest_edge\"], size[\"longest_edge\"]),\n                     patch_size=(patch_size, patch_size),\n                 )"
        },
        {
            "sha": "c790cceefc95d1d1a07a3cf1c8bfc5584eb5b69a",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -20,7 +20,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n@@ -257,6 +257,8 @@ class Qwen2_5OmniTextConfig(PretrainedConfig):\n             Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n         max_window_layers (`int`, *optional*, defaults to 28):\n             The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         rope_scaling (`Dict`, *optional*):\n@@ -358,6 +360,7 @@ def __init__(\n         use_sliding_window=False,\n         sliding_window=32768,\n         max_window_layers=28,\n+        layer_types=None,\n         attention_dropout=0.0,\n         **kwargs,\n     ):\n@@ -396,6 +399,16 @@ def __init__(\n         if self.rope_scaling is None:\n             self.rope_scaling = {\"mrope_section\": [16, 24, 24], \"rope_type\": \"default\", \"type\": \"default\"}\n \n+        self.layer_types = layer_types\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\"\n+                if self.sliding_window is not None and i >= self.max_window_layers\n+                else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n \n class Qwen2_5OmniThinkerConfig(PretrainedConfig):\n     r\"\"\""
        },
        {
            "sha": "be1c55051bea15feb82d59617f381337c0f3e6c6",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 101,
            "deletions": 581,
            "changes": 682,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -22,7 +22,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Any, Dict, List, Optional, Tuple, Union\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -31,19 +31,20 @@\n from torch.nn import Parameter\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n     auto_docstring,\n     check_torch_load_is_safe,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torch_flex_attn_available,\n     logging,\n )\n from ...utils.hub import cached_file\n@@ -68,15 +69,6 @@\n     apply_rotary_emb = None\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -110,7 +102,8 @@ class Qwen2_5OmniPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n-    _supports_static_cache = True\n+    _supports_static_cache = False\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         # important: this ported version of Qwen2.5OmniThinker isn't meant for training from scratch - only\n@@ -1444,6 +1437,32 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class Qwen2_5OmniAttention(nn.Module):\n     \"\"\"\n     Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n@@ -1469,11 +1488,13 @@ def __init__(self, config: Qwen2_5OmniConfig, layer_idx: Optional[int] = None):\n         self.is_causal = True\n         self.attention_dropout = config.attention_dropout\n         self.rope_scaling = config.rope_scaling\n+        self.scaling = self.head_dim**-0.5\n \n         self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n+        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n         self.rotary_emb = Qwen2_5OmniRotaryEmbedding(config=config)\n \n@@ -1487,6 +1508,7 @@ def forward(\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -1507,40 +1529,24 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # Fix precision issues in Qwen2-VL float16 inference\n-        # Replace inf values with zeros in attention weights to prevent NaN propagation\n-        if query_states.dtype == torch.float16:\n-            attn_weights = torch.where(torch.isinf(attn_weights), torch.zeros_like(attn_weights), attn_weights)\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window,\n+            **kwargs,\n+        )\n \n+        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights, past_key_value\n \n \n@@ -1558,216 +1564,7 @@ def forward(self, hidden_state):\n         return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))\n \n \n-class Qwen2_5OmniFlashAttention2(Qwen2_5OmniAttention):\n-    \"\"\"\n-    Qwen2_5Omni flash attention module, following Qwen2_5Omni attention module. This module inherits from `Qwen2_5OmniAttention`\n-    as the weights of the module stays untouched. The only required change would be on the forward pass\n-    where it needs to correctly call the public API of flash attention and deal with padding tokens\n-    in case the input contains any of them. Additionally, for sliding window attention, we apply SWA only to the bottom\n-    config.max_window_layers layers.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-    ):\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        # Because the input can be padded, the absolute sequence length depends on the max position id.\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_multimodal_rotary_pos_emb(\n-            query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n-        )\n-\n-        if past_key_value is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        dropout_rate = 0.0 if not self.training else self.attention_dropout\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in float16 just to be sure everything works as expected.\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        # Reashape to the expected shape for Flash Attention\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        if (\n-            self.config.use_sliding_window\n-            and getattr(self.config, \"sliding_window\", None) is not None\n-            and self.layer_idx >= self.config.max_window_layers\n-        ):\n-            sliding_window = self.config.sliding_window\n-        else:\n-            sliding_window = None\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n-            sliding_window=sliding_window,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class Qwen2_5OmniSdpaAttention(Qwen2_5OmniAttention):\n-    \"\"\"\n-    Qwen2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `Qwen2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from Qwen2Attention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Qwen2_5OmniModel is using Qwen2_5OmniSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_multimodal_rotary_pos_emb(\n-            query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n-        )\n-\n-        if past_key_value is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and attention_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-QWEN2_5_OMNI_ATTENTION_CLASSES = {\n-    \"eager\": Qwen2_5OmniAttention,\n-    \"flash_attention_2\": Qwen2_5OmniFlashAttention2,\n-    \"sdpa\": Qwen2_5OmniSdpaAttention,\n-}\n-\n-\n-class Qwen2_5OmniDecoderLayer(nn.Module):\n+class Qwen2_5OmniDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Qwen2_5OmniTextConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -1777,11 +1574,12 @@ def __init__(self, config: Qwen2_5OmniTextConfig, layer_idx: int):\n                 f\"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; \"\n                 \"unexpected results may be encountered.\"\n             )\n-        self.self_attn = QWEN2_5_OMNI_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n+        self.self_attn = Qwen2_5OmniAttention(config, layer_idx)\n \n         self.mlp = Qwen2MLP(config)\n         self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.attention_type = config.layer_types[layer_idx]\n \n     def forward(\n         self,\n@@ -1793,7 +1591,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -1831,6 +1629,7 @@ def forward(\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -1868,6 +1667,7 @@ def __init__(self, config: Qwen2_5OmniTextConfig):\n         self._attn_implementation = config._attn_implementation\n         self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = Qwen2_5OmniRotaryEmbedding(config=config)\n+        self.has_sliding_layers = \"sliding_attention\" in self.config.layer_types\n \n         self.gradient_checkpointing = False\n         # Initialize weights and apply final processing\n@@ -1892,6 +1692,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1930,9 +1731,23 @@ def forward(\n         elif position_ids.dim() == 2:\n             position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+            }\n+            # The sliding window alternating layers are not always activated depending on the config\n+            if self.has_sliding_layers:\n+                causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n \n         hidden_states = inputs_embeds\n \n@@ -1952,7 +1767,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    causal_mask,\n+                    causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids,\n                     past_key_values,\n                     output_attentions,\n@@ -1963,13 +1778,14 @@ def forward(\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=causal_mask,\n+                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids=position_ids,\n                     past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n                     position_embeddings=position_embeddings,\n+                    **kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n@@ -1997,161 +1813,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and past_key_values is not None:\n-                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n-                if is_padding_right:\n-                    raise ValueError(\n-                        \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Qwen25OmniThinker. Make sure to \"\n-                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n-                    )\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                sliding_window=self.config.sliding_window,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        # DynamicCache or no cache\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-            config=self.config,\n-            past_key_values=past_key_values,\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        config: Qwen2_5OmniConfig,\n-        past_key_values: Cache,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-            config (`Qwen25OmniThinkerConfig`):\n-                The model's configuration class\n-            past_key_values (`Cache`):\n-                The cache class that is being used currently to generate\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n-                -1, 1\n-            )\n-            text_config = config.get_text_config()\n-            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n-                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n-                # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - text_config.sliding_window\n-                    )\n-                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n-            causal_mask *= diagonal_attend_mask\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.shape[-1] > target_length:\n-                    attention_mask = attention_mask[:, :target_length]\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-        return causal_mask\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -2398,9 +2059,6 @@ def forward(\n                 video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n-            if attention_mask is not None:\n-                attention_mask = attention_mask.to(inputs_embeds.device)\n-\n         if feature_attention_mask is not None:\n             audio_feature_lengths = torch.sum(feature_attention_mask, dim=1)\n         else:\n@@ -2574,6 +2232,7 @@ def __init__(self, config: Qwen2_5OmniTalkerConfig):\n         self._attn_implementation = config._attn_implementation\n         self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = Qwen2_5OmniRotaryEmbedding(config=config)\n+        self.has_sliding_layers = \"sliding_attention\" in self.config.layer_types\n \n         self.gradient_checkpointing = False\n         # Initialize weights and apply final processing\n@@ -2598,6 +2257,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -2636,9 +2296,23 @@ def forward(\n         elif position_ids.dim() == 2:\n             position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+            }\n+            # The sliding window alternating layers are not always activated depending on the config\n+            if self.has_sliding_layers:\n+                causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n \n         hidden_states = inputs_embeds\n \n@@ -2658,7 +2332,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    causal_mask,\n+                    causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids,\n                     past_key_values,\n                     output_attentions,\n@@ -2669,13 +2343,14 @@ def forward(\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=causal_mask,\n+                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids=position_ids,\n                     past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n                     position_embeddings=position_embeddings,\n+                    **kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n@@ -2703,161 +2378,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and past_key_values is not None:\n-                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n-                if is_padding_right:\n-                    raise ValueError(\n-                        \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Qwen2_5Omni. Make sure to \"\n-                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n-                    )\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                sliding_window=self.config.sliding_window,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        # DynamicCache or no cache\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-            config=self.config,\n-            past_key_values=past_key_values,\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        config: Qwen2_5OmniConfig,\n-        past_key_values: Cache,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-            config (`Qwen2_5OmniConfig`):\n-                The model's configuration class\n-            past_key_values (`Cache`):\n-                The cache class that is being used currently to generate\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n-                -1, 1\n-            )\n-            text_config = config.get_text_config()\n-            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n-                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n-                # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - text_config.sliding_window\n-                    )\n-                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n-            causal_mask *= diagonal_attend_mask\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.shape[-1] > target_length:\n-                    attention_mask = attention_mask[:, :target_length]\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-        return causal_mask\n-\n \n class Qwen2_5OmniTalkerForConditionalGeneration(Qwen2_5OmniPreTrainedModelForConditionalGeneration, GenerationMixin):\n     config_class = Qwen2_5OmniTalkerConfig"
        },
        {
            "sha": "3ced88af768a4433bad996934ed9a53f3bcc5215",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 17,
            "deletions": 5,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -41,7 +41,7 @@\n from transformers.models.qwen2_audio.modeling_qwen2_audio import Qwen2AudioEncoderLayer\n from transformers.models.qwen2_vl.modeling_qwen2_vl import Qwen2VLRotaryEmbedding\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_rope_utils import rope_config_validation\n@@ -298,6 +298,8 @@ class Qwen2_5OmniTextConfig(PretrainedConfig):\n             Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n         max_window_layers (`int`, *optional*, defaults to 28):\n             The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         rope_scaling (`Dict`, *optional*):\n@@ -399,6 +401,7 @@ def __init__(\n         use_sliding_window=False,\n         sliding_window=32768,\n         max_window_layers=28,\n+        layer_types=None,\n         attention_dropout=0.0,\n         **kwargs,\n     ):\n@@ -437,6 +440,16 @@ def __init__(\n         if self.rope_scaling is None:\n             self.rope_scaling = {\"mrope_section\": [16, 24, 24], \"rope_type\": \"default\", \"type\": \"default\"}\n \n+        self.layer_types = layer_types\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\"\n+                if self.sliding_window is not None and i >= self.max_window_layers\n+                else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n \n class Qwen2_5OmniThinkerConfig(PretrainedConfig):\n     r\"\"\"\n@@ -1104,7 +1117,7 @@ def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n \n class Qwen2_5OmniPreTrainedModel(Qwen2_5_VLPreTrainedModel):\n     config_class = Qwen2_5OmniConfig\n-    _supports_static_cache = True\n+    _supports_static_cache = False\n \n     def _init_weights(self, module):\n         # important: this ported version of Qwen2.5OmniThinker isn't meant for training from scratch - only\n@@ -2184,11 +2197,13 @@ def __init__(self, config: Qwen2_5OmniConfig, layer_idx: Optional[int] = None):\n         self.is_causal = True\n         self.attention_dropout = config.attention_dropout\n         self.rope_scaling = config.rope_scaling\n+        self.scaling = self.head_dim**-0.5\n \n         self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n+        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n         self.rotary_emb = Qwen2_5OmniRotaryEmbedding(config=config)\n \n@@ -2450,9 +2465,6 @@ def forward(\n                 video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n-            if attention_mask is not None:\n-                attention_mask = attention_mask.to(inputs_embeds.device)\n-\n         if feature_attention_mask is not None:\n             audio_feature_lengths = torch.sum(feature_attention_mask, dim=1)\n         else:"
        },
        {
            "sha": "c053de54b39dd364d59b3d78090d5649ccaa5c34",
            "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
            "status": "modified",
            "additions": 15,
            "deletions": 2,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -23,7 +23,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...modeling_rope_utils import rope_config_validation\n \n \n@@ -117,6 +117,8 @@ class Qwen2_5_VLTextConfig(PretrainedConfig):\n             Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n         max_window_layers (`int`, *optional*, defaults to 80):\n             The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         rope_scaling (`Dict`, *optional*):\n@@ -211,6 +213,7 @@ def __init__(\n         use_sliding_window=False,\n         sliding_window=4096,\n         max_window_layers=80,\n+        layer_types=None,\n         attention_dropout=0.0,\n         rope_scaling=None,\n         image_token_id=None,\n@@ -224,7 +227,7 @@ def __init__(\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n         self.use_sliding_window = use_sliding_window\n-        self.sliding_window = sliding_window\n+        self.sliding_window = sliding_window if self.use_sliding_window else None\n         self.max_window_layers = max_window_layers\n \n         # for backward compatibility\n@@ -240,6 +243,16 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.rope_scaling = rope_scaling\n \n+        self.layer_types = layer_types\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\"\n+                if self.sliding_window is not None and i >= self.max_window_layers\n+                else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n         # Validate the correctness of rotary position embeddings parameters\n         # BC: if there is a 'type' field, move it to 'rope_type'.\n         # and change type from 'mrope' to 'default' because `mrope` does default RoPE calculations"
        },
        {
            "sha": "b9d8985ebbc998b65eed24f6b99c3a4f37340e4d",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 93,
            "deletions": 474,
            "changes": 567,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -26,37 +26,30 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Any, Dict, List, Optional, Tuple, Union\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n from .configuration_qwen2_5_vl import Qwen2_5_VLConfig, Qwen2_5_VLTextConfig, Qwen2_5_VLVisionConfig\n \n \n if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import apply_rotary_emb, flash_attn_varlen_func\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -359,6 +352,7 @@ class Qwen2_5_VLPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.get_text_config().initializer_range\n@@ -681,6 +675,32 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class Qwen2_5_VLAttention(nn.Module):\n     \"\"\"\n     Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n@@ -706,6 +726,7 @@ def __init__(self, config: Qwen2_5_VLTextConfig, layer_idx: Optional[int] = None\n         self.is_causal = True\n         self.attention_dropout = config.attention_dropout\n         self.rope_scaling = config.rope_scaling\n+        self.scaling = self.head_dim**-0.5\n \n         if (self.head_dim * self.num_heads) != self.hidden_size:\n             raise ValueError(\n@@ -716,6 +737,7 @@ def __init__(self, config: Qwen2_5_VLTextConfig, layer_idx: Optional[int] = None\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n+        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n         self.rotary_emb = Qwen2_5_VLRotaryEmbedding(config=config)\n \n@@ -729,6 +751,7 @@ def forward(\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -749,253 +772,28 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # Fix precision issues in Qwen2-VL float16 inference\n-        # Replace inf values with zeros in attention weights to prevent NaN propagation\n-        if query_states.dtype == torch.float16:\n-            attn_weights = torch.where(torch.isinf(attn_weights), torch.zeros_like(attn_weights), attn_weights)\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class Qwen2_5_VLFlashAttention2(Qwen2_5_VLAttention):\n-    \"\"\"\n-    Qwen2_5_VL flash attention module, following Qwen2_5_VL attention module. This module inherits from `Qwen2_5_VLAttention`\n-    as the weights of the module stays untouched. The only required change would be on the forward pass\n-    where it needs to correctly call the public API of flash attention and deal with padding tokens\n-    in case the input contains any of them. Additionally, for sliding window attention, we apply SWA only to the bottom\n-    config.max_window_layers layers.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-    ):\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        # Because the input can be padded, the absolute sequence length depends on the max position id.\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_multimodal_rotary_pos_emb(\n-            query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n-        )\n-\n-        if past_key_value is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        dropout_rate = 0.0 if not self.training else self.attention_dropout\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in float16 just to be sure everything works as expected.\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        # Reashape to the expected shape for Flash Attention\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        if (\n-            self.config.use_sliding_window\n-            and getattr(self.config, \"sliding_window\", None) is not None\n-            and self.layer_idx >= self.config.max_window_layers\n-        ):\n-            sliding_window = self.config.sliding_window\n-        else:\n-            sliding_window = None\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n-            sliding_window=sliding_window,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window,\n+            **kwargs,\n         )\n \n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights, past_key_value\n \n \n-class Qwen2_5_VLSdpaAttention(Qwen2_5_VLAttention):\n-    \"\"\"\n-    Qwen2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `Qwen2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from Qwen2Attention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Qwen2_5_VLModel is using Qwen2_5_VLSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_multimodal_rotary_pos_emb(\n-            query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n-        )\n-\n-        if past_key_value is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and attention_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-QWEN2_5_VL_ATTENTION_CLASSES = {\n-    \"eager\": Qwen2_5_VLAttention,\n-    \"flash_attention_2\": Qwen2_5_VLFlashAttention2,\n-    \"sdpa\": Qwen2_5_VLSdpaAttention,\n-}\n-\n-\n-class Qwen2_5_VLDecoderLayer(nn.Module):\n+class Qwen2_5_VLDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Qwen2_5_VLTextConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -1005,11 +803,12 @@ def __init__(self, config: Qwen2_5_VLTextConfig, layer_idx: int):\n                 f\"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; \"\n                 \"unexpected results may be encountered.\"\n             )\n-        self.self_attn = QWEN2_5_VL_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n+        self.self_attn = Qwen2_5_VLAttention(config, layer_idx)\n \n         self.mlp = Qwen2MLP(config)\n         self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.attention_type = config.layer_types[layer_idx]\n \n     def forward(\n         self,\n@@ -1021,7 +820,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -1059,6 +858,7 @@ def forward(\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -1095,6 +895,7 @@ def __init__(self, config: Qwen2_5_VLTextConfig):\n         self._attn_implementation = config._attn_implementation\n         self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = Qwen2_5_VLRotaryEmbedding(config=config)\n+        self.has_sliding_layers = \"sliding_attention\" in self.config.layer_types\n \n         self.gradient_checkpointing = False\n         # Initialize weights and apply final processing\n@@ -1119,6 +920,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1157,9 +959,23 @@ def forward(\n         elif position_ids.dim() == 2:\n             position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+            }\n+            # The sliding window alternating layers are not always activated depending on the config\n+            if self.has_sliding_layers:\n+                causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n \n         hidden_states = inputs_embeds\n \n@@ -1179,7 +995,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    causal_mask,\n+                    causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids,\n                     past_key_values,\n                     output_attentions,\n@@ -1190,13 +1006,14 @@ def forward(\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=causal_mask,\n+                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids=position_ids,\n                     past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n                     position_embeddings=position_embeddings,\n+                    **kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n@@ -1224,160 +1041,8 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and past_key_values is not None:\n-                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n-                if is_padding_right:\n-                    raise ValueError(\n-                        \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Qwen2_5_VL. Make sure to \"\n-                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n-                    )\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                sliding_window=self.config.sliding_window,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        # DynamicCache or no cache\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n \n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-            config=self.config,\n-            past_key_values=past_key_values,\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        config: Qwen2_5_VLConfig,\n-        past_key_values: Cache,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-            config (`Qwen2_5_VLConfig`):\n-                The model's configuration class\n-            past_key_values (`Cache`):\n-                The cache class that is being used currently to generate\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n-                -1, 1\n-            )\n-            text_config = config.get_text_config()\n-            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n-                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n-                # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - text_config.sliding_window\n-                    )\n-                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n-            causal_mask *= diagonal_attend_mask\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.shape[-1] > target_length:\n-                    attention_mask = attention_mask[:, :target_length]\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-        return causal_mask\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n @auto_docstring\n@@ -1598,6 +1263,8 @@ def get_video_features(\n         \"\"\"\n         pixel_values_videos = pixel_values_videos.type(self.visual.dtype)\n         video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n+        split_sizes = (video_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()\n+        video_embeds = torch.split(video_embeds, split_sizes)\n         return video_embeds\n \n     def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n@@ -1612,6 +1279,8 @@ def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Op\n         \"\"\"\n         pixel_values = pixel_values.type(self.visual.dtype)\n         image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n+        split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()\n+        image_embeds = torch.split(image_embeds, split_sizes)\n         return image_embeds\n \n     @auto_docstring\n@@ -1633,6 +1302,7 @@ def forward(\n         rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         second_per_grid_ts: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, Qwen2_5_VLModelOutputWithPast]:\n         r\"\"\"\n         pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):\n@@ -1659,6 +1329,7 @@ def forward(\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n             if pixel_values is not None:\n                 image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n+                image_embeds = torch.cat(image_embeds, dim=0)\n                 n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_embeds.shape[0]\n                 if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n@@ -1676,6 +1347,7 @@ def forward(\n \n             if pixel_values_videos is not None:\n                 video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n+                video_embeds = torch.cat(video_embeds, dim=0)\n                 n_video_tokens = (input_ids == self.config.video_token_id).sum()\n                 n_video_features = video_embeds.shape[0]\n                 if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n@@ -1691,15 +1363,14 @@ def forward(\n                 video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n-            if attention_mask is not None:\n-                attention_mask = attention_mask.to(inputs_embeds.device)\n-\n         if position_ids is None:\n-            attention_mask_2d = attention_mask\n-            if attention_mask is not None and attention_mask.ndim == 4:\n-                attention_mask_2d = torch.diagonal(attention_mask_2d[:, 0], dim1=1, dim2=2)\n-                attention_mask_2d = attention_mask_2d / torch.finfo(attention_mask_2d.dtype).min\n-                attention_mask_2d = (1.0 - attention_mask_2d).int()\n+            attention_mask_tensor = (\n+                attention_mask if not isinstance(attention_mask, dict) else attention_mask[\"full_attention\"]\n+            )\n+            if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n+                attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n+                attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n+                attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n \n             # Calculate RoPE index once per generation in the pre-fill stage only.\n             # When compiling, we can't check tensor values thus we check only input length\n@@ -1719,7 +1390,7 @@ def forward(\n                     image_grid_thw,\n                     video_grid_thw,\n                     second_per_grid_ts=second_per_grid_ts,\n-                    attention_mask=attention_mask_2d,\n+                    attention_mask=attention_mask_tensor,\n                 )\n                 self.rope_deltas = rope_deltas\n             # then use the prev pre-calculated rope-deltas to get the correct position ids\n@@ -1748,6 +1419,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         output = Qwen2_5_VLModelOutputWithPast(\n@@ -1759,61 +1431,6 @@ def forward(\n         )\n         return output if return_dict else output.to_tuple()\n \n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n @dataclass\n class Qwen2_5_VLCausalLMOutputWithPast(ModelOutput):\n@@ -1916,6 +1533,7 @@ def forward(\n         rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         second_per_grid_ts: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, Qwen2_5_VLCausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1988,6 +1606,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]"
        },
        {
            "sha": "e38f2952144cb7b5dec23527a79045c9f8505061",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 15,
            "deletions": 9,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -30,6 +30,7 @@\n \n from transformers.models.qwen2_vl.configuration_qwen2_vl import Qwen2VLConfig, Qwen2VLTextConfig\n from transformers.models.qwen2_vl.modeling_qwen2_vl import (\n+    KwargsForCausalLM,\n     PatchEmbed,\n     PatchMerger,\n     Qwen2RMSNorm,\n@@ -622,6 +623,7 @@ def forward(\n         rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         second_per_grid_ts: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, Qwen2_5_VLModelOutputWithPast]:\n         r\"\"\"\n         pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):\n@@ -648,6 +650,7 @@ def forward(\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n             if pixel_values is not None:\n                 image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n+                image_embeds = torch.cat(image_embeds, dim=0)\n                 n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_embeds.shape[0]\n                 if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n@@ -665,6 +668,7 @@ def forward(\n \n             if pixel_values_videos is not None:\n                 video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n+                video_embeds = torch.cat(video_embeds, dim=0)\n                 n_video_tokens = (input_ids == self.config.video_token_id).sum()\n                 n_video_features = video_embeds.shape[0]\n                 if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n@@ -680,15 +684,14 @@ def forward(\n                 video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n-            if attention_mask is not None:\n-                attention_mask = attention_mask.to(inputs_embeds.device)\n-\n         if position_ids is None:\n-            attention_mask_2d = attention_mask\n-            if attention_mask is not None and attention_mask.ndim == 4:\n-                attention_mask_2d = torch.diagonal(attention_mask_2d[:, 0], dim1=1, dim2=2)\n-                attention_mask_2d = attention_mask_2d / torch.finfo(attention_mask_2d.dtype).min\n-                attention_mask_2d = (1.0 - attention_mask_2d).int()\n+            attention_mask_tensor = (\n+                attention_mask if not isinstance(attention_mask, dict) else attention_mask[\"full_attention\"]\n+            )\n+            if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n+                attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n+                attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n+                attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n \n             # Calculate RoPE index once per generation in the pre-fill stage only.\n             # When compiling, we can't check tensor values thus we check only input length\n@@ -708,7 +711,7 @@ def forward(\n                     image_grid_thw,\n                     video_grid_thw,\n                     second_per_grid_ts=second_per_grid_ts,\n-                    attention_mask=attention_mask_2d,\n+                    attention_mask=attention_mask_tensor,\n                 )\n                 self.rope_deltas = rope_deltas\n             # then use the prev pre-calculated rope-deltas to get the correct position ids\n@@ -737,6 +740,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         output = Qwen2_5_VLModelOutputWithPast(\n@@ -774,6 +778,7 @@ def forward(\n         rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         second_per_grid_ts: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, Qwen2_5_VLCausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -846,6 +851,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]"
        },
        {
            "sha": "03bfa66c41f5c3164bb3870f40df1e712125f1f0",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 15,
            "deletions": 2,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Qwen2VL model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n@@ -106,6 +106,8 @@ class Qwen2VLTextConfig(PretrainedConfig):\n             Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n         max_window_layers (`int`, *optional*, defaults to 80):\n             The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         rope_scaling (`Dict`, *optional*):\n@@ -200,6 +202,7 @@ def __init__(\n         use_sliding_window=False,\n         sliding_window=4096,\n         max_window_layers=80,\n+        layer_types=None,\n         attention_dropout=0.0,\n         rope_scaling=None,\n         image_token_id=None,\n@@ -213,7 +216,7 @@ def __init__(\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n         self.use_sliding_window = use_sliding_window\n-        self.sliding_window = sliding_window\n+        self.sliding_window = sliding_window if self.use_sliding_window else None\n         self.max_window_layers = max_window_layers\n \n         # for backward compatibility\n@@ -229,6 +232,16 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.rope_scaling = rope_scaling\n \n+        self.layer_types = layer_types\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\"\n+                if self.sliding_window is not None and i >= self.max_window_layers\n+                else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n         # Validate the correctness of rotary position embeddings parameters\n         # BC: if there is a 'type' field, move it to 'rope_type'.\n         # and change type from 'mrope' to 'default' because `mrope` does default RoPE calculations"
        },
        {
            "sha": "4581ed08279a563cea742f11b62179a8bbfc20b8",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 100,
            "deletions": 474,
            "changes": 574,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -21,7 +21,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Any, Dict, List, Optional, Tuple, Union\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -30,24 +30,27 @@\n from torch.nn import LayerNorm\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    LossKwargs,\n+    auto_docstring,\n+    can_return_tuple,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n from .configuration_qwen2_vl import Qwen2VLConfig, Qwen2VLTextConfig, Qwen2VLVisionConfig\n \n \n if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_varlen_func\n-\n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n+    from ...modeling_flash_attention_utils import flash_attn_varlen_func\n \n \n logger = logging.get_logger(__name__)\n@@ -516,6 +519,32 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class Qwen2VLAttention(nn.Module):\n     \"\"\"\n     Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n@@ -541,6 +570,7 @@ def __init__(self, config: Qwen2VLTextConfig, layer_idx: Optional[int] = None):\n         self.is_causal = True\n         self.attention_dropout = config.attention_dropout\n         self.rope_scaling = config.rope_scaling\n+        self.scaling = self.head_dim**-0.5\n \n         if (self.head_dim * self.num_heads) != self.hidden_size:\n             raise ValueError(\n@@ -551,6 +581,7 @@ def __init__(self, config: Qwen2VLTextConfig, layer_idx: Optional[int] = None):\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n+        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n         self.rotary_emb = Qwen2VLRotaryEmbedding(config=config)\n \n@@ -564,6 +595,7 @@ def forward(\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -584,253 +616,28 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # Fix precision issues in Qwen2-VL float16 inference\n-        # Replace inf values with zeros in attention weights to prevent NaN propagation\n-        if query_states.dtype == torch.float16:\n-            attn_weights = torch.where(torch.isinf(attn_weights), torch.zeros_like(attn_weights), attn_weights)\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class Qwen2VLFlashAttention2(Qwen2VLAttention):\n-    \"\"\"\n-    Qwen2VL flash attention module, following Qwen2VL attention module. This module inherits from `Qwen2VLAttention`\n-    as the weights of the module stays untouched. The only required change would be on the forward pass\n-    where it needs to correctly call the public API of flash attention and deal with padding tokens\n-    in case the input contains any of them. Additionally, for sliding window attention, we apply SWA only to the bottom\n-    config.max_window_layers layers.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-    ):\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        # Because the input can be padded, the absolute sequence length depends on the max position id.\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_multimodal_rotary_pos_emb(\n-            query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n-        )\n-\n-        if past_key_value is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        dropout_rate = 0.0 if not self.training else self.attention_dropout\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in float16 just to be sure everything works as expected.\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        # Reashape to the expected shape for Flash Attention\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        if (\n-            self.config.use_sliding_window\n-            and getattr(self.config, \"sliding_window\", None) is not None\n-            and self.layer_idx >= self.config.max_window_layers\n-        ):\n-            sliding_window = self.config.sliding_window\n-        else:\n-            sliding_window = None\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n-            sliding_window=sliding_window,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window,\n+            **kwargs,\n         )\n \n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights, past_key_value\n \n \n-class Qwen2VLSdpaAttention(Qwen2VLAttention):\n-    \"\"\"\n-    Qwen2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `Qwen2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from Qwen2Attention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Qwen2VLModel is using Qwen2VLSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_multimodal_rotary_pos_emb(\n-            query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n-        )\n-\n-        if past_key_value is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and attention_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-QWEN2_VL_ATTENTION_CLASSES = {\n-    \"eager\": Qwen2VLAttention,\n-    \"flash_attention_2\": Qwen2VLFlashAttention2,\n-    \"sdpa\": Qwen2VLSdpaAttention,\n-}\n-\n-\n-class Qwen2VLDecoderLayer(nn.Module):\n+class Qwen2VLDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Qwen2VLTextConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -840,11 +647,12 @@ def __init__(self, config: Qwen2VLTextConfig, layer_idx: int):\n                 f\"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; \"\n                 \"unexpected results may be encountered.\"\n             )\n-        self.self_attn = QWEN2_VL_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n+        self.self_attn = Qwen2VLAttention(config, layer_idx)\n \n         self.mlp = Qwen2MLP(config)\n         self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.attention_type = config.layer_types[layer_idx]\n \n     def forward(\n         self,\n@@ -856,7 +664,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -894,6 +702,7 @@ def forward(\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -925,6 +734,7 @@ class Qwen2VLPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.get_text_config().initializer_range\n@@ -1053,6 +863,7 @@ def __init__(self, config: Qwen2VLTextConfig):\n         self._attn_implementation = config._attn_implementation\n         self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = Qwen2VLRotaryEmbedding(config=config)\n+        self.has_sliding_layers = \"sliding_attention\" in self.config.layer_types\n \n         self.gradient_checkpointing = False\n         # Initialize weights and apply final processing\n@@ -1077,6 +888,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1115,9 +927,23 @@ def forward(\n         elif position_ids.dim() == 2:\n             position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+            }\n+            # The sliding window alternating layers are not always activated depending on the config\n+            if self.has_sliding_layers:\n+                causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n \n         hidden_states = inputs_embeds\n \n@@ -1137,7 +963,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    causal_mask,\n+                    causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids,\n                     past_key_values,\n                     output_attentions,\n@@ -1148,13 +974,14 @@ def forward(\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=causal_mask,\n+                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids=position_ids,\n                     past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n                     position_embeddings=position_embeddings,\n+                    **kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n@@ -1182,162 +1009,8 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._update_causal_mask with Phimoe->Qwen2VL\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and past_key_values is not None:\n-                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n-                if is_padding_right:\n-                    raise ValueError(\n-                        \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Qwen2VL. Make sure to \"\n-                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n-                    )\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                sliding_window=self.config.sliding_window,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        # DynamicCache or no cache\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-            config=self.config,\n-            past_key_values=past_key_values,\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._prepare_4d_causal_attention_mask_with_cache_position with Phimoe->Qwen2VL\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        config: Qwen2VLConfig,\n-        past_key_values: Cache,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n \n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-            config (`Qwen2VLConfig`):\n-                The model's configuration class\n-            past_key_values (`Cache`):\n-                The cache class that is being used currently to generate\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n-                -1, 1\n-            )\n-            text_config = config.get_text_config()\n-            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n-                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n-                # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - text_config.sliding_window\n-                    )\n-                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n-            causal_mask *= diagonal_attend_mask\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.shape[-1] > target_length:\n-                    attention_mask = attention_mask[:, :target_length]\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-        return causal_mask\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n @auto_docstring\n@@ -1523,6 +1196,8 @@ def get_video_features(\n         \"\"\"\n         pixel_values_videos = pixel_values_videos.type(self.visual.dtype)\n         video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n+        split_sizes = (video_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()\n+        video_embeds = torch.split(video_embeds, split_sizes)\n         return video_embeds\n \n     def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n@@ -1537,6 +1212,8 @@ def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Op\n         \"\"\"\n         pixel_values = pixel_values.type(self.visual.dtype)\n         image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n+        split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()\n+        image_embeds = torch.split(image_embeds, split_sizes)\n         return image_embeds\n \n     @auto_docstring\n@@ -1557,6 +1234,7 @@ def forward(\n         video_grid_thw: Optional[torch.LongTensor] = None,\n         rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, Qwen2VLModelOutputWithPast]:\n         r\"\"\"\n         pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):\n@@ -1581,6 +1259,7 @@ def forward(\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n             if pixel_values is not None:\n                 image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n+                image_embeds = torch.cat(image_embeds, dim=0)\n                 n_image_tokens = (input_ids == self.config.image_token_id).sum().item()\n                 n_image_features = image_embeds.shape[0]\n                 if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n@@ -1598,6 +1277,7 @@ def forward(\n \n             if pixel_values_videos is not None:\n                 video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n+                video_embeds = torch.cat(video_embeds, dim=0)\n                 n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n                 n_video_features = video_embeds.shape[0]\n                 if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n@@ -1613,15 +1293,14 @@ def forward(\n                 video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n-            if attention_mask is not None:\n-                attention_mask = attention_mask.to(inputs_embeds.device)\n-\n         if position_ids is None:\n-            attention_mask_2d = attention_mask\n-            if attention_mask is not None and attention_mask.ndim == 4:\n-                attention_mask_2d = torch.diagonal(attention_mask_2d[:, 0], dim1=1, dim2=2)\n-                attention_mask_2d = attention_mask_2d / torch.finfo(attention_mask_2d.dtype).min\n-                attention_mask_2d = (1.0 - attention_mask_2d).int()\n+            attention_mask_tensor = (\n+                attention_mask if not isinstance(attention_mask, dict) else attention_mask[\"full_attention\"]\n+            )\n+            if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n+                attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n+                attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n+                attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n \n             # Calculate RoPE index once per generation in the pre-fill stage only.\n             # When compiling, we can't check tensor values thus we check only input length\n@@ -1637,7 +1316,7 @@ def forward(\n             )\n             if (prefill_compiled_stage or prefill_noncompiled_stage) or self.rope_deltas is None:\n                 position_ids, rope_deltas = self.get_rope_index(\n-                    input_ids, image_grid_thw, video_grid_thw, attention_mask_2d\n+                    input_ids, image_grid_thw, video_grid_thw, attention_mask_tensor\n                 )\n                 self.rope_deltas = rope_deltas\n             # then use the prev pre-calculated rope-deltas to get the correct position ids\n@@ -1663,6 +1342,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         output = Qwen2VLModelOutputWithPast(\n@@ -1674,62 +1354,6 @@ def forward(\n         )\n         return output if return_dict else output.to_tuple()\n \n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class Qwen2VLForConditionalGeneration(Qwen2VLPreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {\n@@ -1792,6 +1416,7 @@ def forward(\n         video_grid_thw: Optional[torch.LongTensor] = None,\n         rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, Qwen2VLCausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1861,6 +1486,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]"
        },
        {
            "sha": "70abf26ee20a8baedbfed3c4763acb041533d226",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -181,7 +181,9 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def get_image_features(self, pixel_values: torch.FloatTensor, vision_feature_layers: Union[int, List[int]]):\n+    def get_image_features(\n+        self, pixel_values: torch.FloatTensor, vision_feature_layers: Optional[Union[int, List[int]]] = None\n+    ):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n \n@@ -194,6 +196,9 @@ def get_image_features(self, pixel_values: torch.FloatTensor, vision_feature_lay\n         Returns:\n             image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n         \"\"\"\n+        vision_feature_layers = (\n+            vision_feature_layers if vision_feature_layers is not None else self.config.vision_feature_layers\n+        )\n         image_outputs = self.vision_tower(pixel_values, output_hidden_states=True)\n \n         # If multiple feature layers are provided (which is usually the case)"
        },
        {
            "sha": "5ae33e771c6a9b5e879963339988f7e8d5846434",
            "filename": "src/transformers/models/vipllava/modular_vipllava.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -71,7 +71,9 @@ class VipLlavaPreTrainedModel(LlavaPreTrainedModel):\n \n \n class VipLlavaModel(LlavaModel):\n-    def get_image_features(self, pixel_values: torch.FloatTensor, vision_feature_layers: Union[int, List[int]]):\n+    def get_image_features(\n+        self, pixel_values: torch.FloatTensor, vision_feature_layers: Optional[Union[int, List[int]]] = None\n+    ):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n \n@@ -84,6 +86,9 @@ def get_image_features(self, pixel_values: torch.FloatTensor, vision_feature_lay\n         Returns:\n             image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n         \"\"\"\n+        vision_feature_layers = (\n+            vision_feature_layers if vision_feature_layers is not None else self.config.vision_feature_layers\n+        )\n         image_outputs = self.vision_tower(pixel_values, output_hidden_states=True)\n \n         # If multiple feature layers are provided (which is usually the case)"
        },
        {
            "sha": "ddbb8d02d9a4e4d7059e2ed0651367e93a183353",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -326,9 +326,7 @@ class Emu3Vision2TextModelTest(ModelTesterMixin, GenerationTesterMixin, Pipeline\n \n     def setUp(self):\n         self.model_tester = Emu3Vision2TextModelTester(self)\n-        self.config_tester = ConfigTester(\n-            self, config_class=Emu3Config, has_text_modality=False, common_properties=[\"vocabulary_map\"]\n-        )\n+        self.config_tester = ConfigTester(self, config_class=Emu3Config, has_text_modality=False, hidden_size=37)\n \n     def test_config(self):\n         self.config_tester.run_common_tests()"
        },
        {
            "sha": "9fc992049a40178f6b12c3056c3abe991642d9b4",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad9dd3d17bee17aae03588cf193b72dc4d34ab64/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=ad9dd3d17bee17aae03588cf193b72dc4d34ab64",
            "patch": "@@ -46,7 +46,10 @@\n     ],\n     \"Qwen2Config\": [\"use_sliding_window\", \"max_window_layers\"],\n     \"Qwen2MoeConfig\": [\"use_sliding_window\"],\n-    \"Qwen2VLConfig\": [\"use_sliding_window\"],\n+    \"Qwen2VLTextConfig\": [\"use_sliding_window\", \"max_window_layers\"],\n+    \"Qwen2_5_VLTextConfig\": [\"use_sliding_window\", \"max_window_layers\"],\n+    \"Qwen2_5OmniTextConfig\": [\"use_sliding_window\", \"max_window_layers\"],\n+    \"Qwen2_5OmniTalkerConfig\": [\"use_sliding_window\", \"max_window_layers\"],\n     \"Qwen3Config\": [\"max_window_layers\", \"use_sliding_window\"],  # now use `layer_types` instead\n     \"Qwen3MoeConfig\": [\"max_window_layers\", \"use_sliding_window\"],\n     # `cache_implementation` should be in the default generation config, but we don't yet support per-model"
        }
    ],
    "stats": {
        "total": 2563,
        "additions": 885,
        "deletions": 1678
    }
}