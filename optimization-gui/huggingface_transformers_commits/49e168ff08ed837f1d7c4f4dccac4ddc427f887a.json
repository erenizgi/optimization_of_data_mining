{
    "author": "manueldeprada",
    "message": "ðŸš¨ Remove Contrastive Search decoding strategy (#40428)\n\n* delete go brrr\n\n* fix tests\n\n* review",
    "sha": "49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
    "files": [
        {
            "sha": "5c7d27192292ce4b84d843d263b9422b1750c6e8",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -225,29 +225,6 @@ outputs = model.generate(**inputs, assistant_model=assistant_model, tokenizer=to\n tokenizer.batch_decode(outputs, skip_special_tokens=True)\n ['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']\n ```\n-\n-### Contrastive search\n-\n-[Contrastive search](https://huggingface.co/papers/2202.06417) is a decoding strategy that aims to reduce repetition even while generating longer sequences. This strategy compares how similar a generated token is against previous tokens, and if they're more similar, a penalty is applied.\n-\n-Enable contrastive search with the `penalty_alpha` and `top_k` parameters. The `penalty_alpha` manages the penalty applied and `top_k` is the number of most likely tokens to return.\n-\n-```py\n-import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n-\n-device = infer_device()\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n-inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n-\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", dtype=torch.float16).to(device)\n-# explicitly set to 100 because Llama2 generation length is 4096\n-outputs = model.generate(**inputs, max_new_tokens=100, penalty_alpha=0.6, top_k=4)\n-tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-'Hugging Face is an open-source company that provides a platform for building and deploying AI models.\\nHugging Face is an open-source company that provides a platform for building and deploying AI models. The platform allows developers to build and deploy AI models, as well as collaborate with other developers.\\nHugging Face was founded in 2019 by Thibault Wittemberg and ClÃ©ment Delangue. The company is based in Paris, France.\\nHugging Face has'\n-```\n-\n ### Diverse beam search\n \n [Diverse beam search](https://hf.co/papers/1610.02424) is a variant of beam search that produces more diverse output candidates to choose from. This strategy measures the dissimilarity of sequences and a penalty is applied if sequences are too similar. To avoid high computation costs, the number of beams is divided into groups."
        },
        {
            "sha": "856c4856c52f2ad420c05edd931cbeac17c75fa1",
            "filename": "docs/source/ja/generation_strategies.md",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/docs%2Fsource%2Fja%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/docs%2Fsource%2Fja%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fgeneration_strategies.md?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -168,29 +168,6 @@ An increasing sequence: one, two, three, four, five, six, seven, eight, nine, te\n ['I look forward to seeing you all again!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n']\n ```\n \n-### Contrastive search\n-\n-ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒ†ã‚£ãƒ–æ¤œç´¢ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆ¦ç•¥ã¯ã€2022å¹´ã®è«–æ–‡[A Contrastive Framework for Neural Text Generation](https://huggingface.co/papers/2202.06417)ã§ææ¡ˆã•ã‚Œã¾ã—ãŸã€‚\n-ã“ã‚Œã¯ã€éžåå¾©çš„ã§ã‚ã‚ŠãªãŒã‚‰ä¸€è²«æ€§ã®ã‚ã‚‹é•·ã„å‡ºåŠ›ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«å„ªã‚ŒãŸçµæžœã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒ†ã‚£ãƒ–æ¤œç´¢ã®å‹•ä½œåŽŸç†ã‚’å­¦ã¶ã«ã¯ã€[ã“ã®ãƒ–ãƒ­ã‚°ãƒã‚¹ãƒˆ](https://huggingface.co/blog/introducing-csearch)ã‚’ã”è¦§ãã ã•ã„ã€‚\n-ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒ†ã‚£ãƒ–æ¤œç´¢ã®å‹•ä½œã‚’æœ‰åŠ¹ã«ã—ã€åˆ¶å¾¡ã™ã‚‹2ã¤ã®ä¸»è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€Œpenalty_alphaã€ã¨ã€Œtop_kã€ã§ã™ï¼š\n-\n-```python\n->>> from transformers import AutoTokenizer, AutoModelForCausalLM\n-\n->>> checkpoint = \"openai-community/gpt2-large\"\n->>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n->>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n-\n->>> prompt = \"Hugging Face Company is\"\n->>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n-\n->>> outputs = model.generate(**inputs, penalty_alpha=0.6, top_k=4, max_new_tokens=100)\n->>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-['Hugging Face Company is a family owned and operated business. We pride ourselves on being the best\n-in the business and our customer service is second to none.\\n\\nIf you have any questions about our\n-products or services, feel free to contact us at any time. We look forward to hearing from you!']\n-```\n-\n ### Multinomial sampling\n \n å¸¸ã«æœ€é«˜ç¢ºçŽ‡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦é¸æŠžã™ã‚‹è²ªæ¬²æ¤œç´¢ã¨ã¯ç•°ãªã‚Šã€å¤šé …åˆ†å¸ƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆã¾ãŸã¯ç¥–å…ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ã‚‚å‘¼ã°ã‚Œã¾ã™ï¼‰ã¯ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦æä¾›ã•ã‚Œã‚‹èªžå½™å…¨ä½“ã®ç¢ºçŽ‡åˆ†å¸ƒã«åŸºã¥ã„ã¦æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠžã—ã¾ã™ã€‚ã‚¼ãƒ­ä»¥å¤–ã®ç¢ºçŽ‡ã‚’æŒã¤ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ã¯é¸æŠžã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€ã“ã‚Œã«ã‚ˆã‚Šç¹°ã‚Šè¿”ã—ã®ãƒªã‚¹ã‚¯ãŒæ¸›å°‘ã—ã¾ã™ã€‚"
        },
        {
            "sha": "da38e4f418f2cd9a3d152013d973c1c821d1c775",
            "filename": "docs/source/ko/generation_strategies.md",
            "status": "modified",
            "additions": 1,
            "deletions": 22,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/docs%2Fsource%2Fko%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/docs%2Fsource%2Fko%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fgeneration_strategies.md?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -68,7 +68,7 @@ GenerationConfig {\n - `max_new_tokens`: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ìž…ë‹ˆë‹¤. ì¦‰, í”„ë¡¬í”„íŠ¸ì— ìžˆëŠ” í† í°ì„ ì œì™¸í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ì˜ í¬ê¸°ìž…ë‹ˆë‹¤. ì¶œë ¥ì˜ ê¸¸ì´ë¥¼ ì¤‘ë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ , ì „ì²´ ìƒì„±ë¬¼ì´ ì¼ì • ì‹œê°„ì„ ì´ˆê³¼í•  ë•Œ ìƒì„±ì„ ì¤‘ë‹¨í•˜ê¸°ë¡œ ì„ íƒí•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤. ë” ì•Œì•„ë³´ë ¤ë©´ [`StoppingCriteria`]ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n - `num_beams`: 1ë³´ë‹¤ í° ìˆ˜ì˜ ë¹”ì„ ì§€ì •í•¨ìœ¼ë¡œì¨, íƒìš• íƒìƒ‰(greedy search)ì—ì„œ ë¹” íƒìƒ‰(beam search)ìœ¼ë¡œ ì „í™˜í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ ì „ëžµì€ ê° ì‹œê°„ ë‹¨ê³„ì—ì„œ ì—¬ëŸ¬ ê°€ì„¤ì„ í‰ê°€í•˜ê³  ê²°êµ­ ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ê°€ìž¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ê°€ì„¤ì„ ì„ íƒí•©ë‹ˆë‹¤. ì´ëŠ” ì´ˆê¸° í† í°ì˜ í™•ë¥ ì´ ë‚®ì•„ íƒìš• íƒìƒ‰ì— ì˜í•´ ë¬´ì‹œë˜ì—ˆì„ ë†’ì€ í™•ë¥ ì˜ ì‹œí€€ìŠ¤ë¥¼ ì‹ë³„í•  ìˆ˜ ìžˆëŠ” ìž¥ì ì„ ê°€ì§‘ë‹ˆë‹¤.\n - `do_sample`: ì´ ë§¤ê°œë³€ìˆ˜ë¥¼ `True`ë¡œ ì„¤ì •í•˜ë©´, ë‹¤í•­ ìƒ˜í”Œë§, ë¹” íƒìƒ‰ ë‹¤í•­ ìƒ˜í”Œë§, Top-K ìƒ˜í”Œë§ ë° Top-p ìƒ˜í”Œë§ê³¼ ê°™ì€ ë””ì½”ë”© ì „ëžµì„ í™œì„±í™”í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì „ëžµë“¤ì€ ì „ì²´ ì–´íœ˜ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ì—ì„œ ë‹¤ìŒ í† í°ì„ ì„ íƒí•˜ë©°, ì „ëžµë³„ë¡œ íŠ¹ì • ì¡°ì •ì´ ì ìš©ë©ë‹ˆë‹¤.\n-- `num_return_sequences`: ê° ìž…ë ¥ì— ëŒ€í•´ ë°˜í™˜í•  ì‹œí€€ìŠ¤ í›„ë³´ì˜ ìˆ˜ìž…ë‹ˆë‹¤. ì´ ì˜µì…˜ì€ ë¹” íƒìƒ‰(beam search)ì˜ ë³€í˜•ê³¼ ìƒ˜í”Œë§ê³¼ ê°™ì´ ì—¬ëŸ¬ ì‹œí€€ìŠ¤ í›„ë³´ë¥¼ ì§€ì›í•˜ëŠ” ë””ì½”ë”© ì „ëžµì—ë§Œ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. íƒìš• íƒìƒ‰(greedy search)ê³¼ ëŒ€ì¡° íƒìƒ‰(contrastive search) ê°™ì€ ë””ì½”ë”© ì „ëžµì€ ë‹¨ì¼ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n+- `num_return_sequences`: ê° ìž…ë ¥ì— ëŒ€í•´ ë°˜í™˜í•  ì‹œí€€ìŠ¤ í›„ë³´ì˜ ìˆ˜ìž…ë‹ˆë‹¤. ì´ ì˜µì…˜ì€ ë¹” íƒìƒ‰(beam search)ì˜ ë³€í˜•ê³¼ ìƒ˜í”Œë§ê³¼ ê°™ì´ ì—¬ëŸ¬ ì‹œí€€ìŠ¤ í›„ë³´ë¥¼ ì§€ì›í•˜ëŠ” ë””ì½”ë”© ì „ëžµì—ë§Œ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. íƒìš• íƒìƒ‰(greedy search) ê°™ì€ ë””ì½”ë”© ì „ëžµì€ ë‹¨ì¼ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n \n ## ëª¨ë¸ì— ì‚¬ìš©ìž ì •ì˜ ë””ì½”ë”© ì „ëžµ ì €ìž¥[[save-a-custom-decoding-strategy-with-your-model]]\n \n@@ -165,27 +165,6 @@ An increasing sequence: one, two, three, four, five, six, seven, eight, nine, te\n ['I look forward to seeing you all again!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n']\n ```\n \n-### ëŒ€ì¡° íƒìƒ‰(Contrastive search)[[contrastive-search]]\n-\n-2022ë…„ ë…¼ë¬¸ [A Contrastive Framework for Neural Text Generation](https://huggingface.co/papers/2202.06417)ì—ì„œ ì œì•ˆëœ ëŒ€ì¡° íƒìƒ‰ ë””ì½”ë”© ì „ëžµì€ ë°˜ë³µë˜ì§€ ì•Šìœ¼ë©´ì„œë„ ì¼ê´€ëœ ê¸´ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ë° ìžˆì–´ ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤. ëŒ€ì¡° íƒìƒ‰ì´ ìž‘ë™í•˜ëŠ” ë°©ì‹ì„ ì•Œì•„ë³´ë ¤ë©´ [ì´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸](https://huggingface.co/blog/introducing-csearch)ë¥¼ í™•ì¸í•˜ì„¸ìš”. ëŒ€ì¡° íƒìƒ‰ì˜ ë™ìž‘ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê³  ì œì–´í•˜ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ë§¤ê°œë³€ìˆ˜ëŠ” `penalty_alpha`ì™€ `top_k`ìž…ë‹ˆë‹¤:\n-\n-```python\n->>> from transformers import AutoTokenizer, AutoModelForCausalLM\n-\n->>> checkpoint = \"openai-community/gpt2-large\"\n->>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n->>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n-\n->>> prompt = \"Hugging Face Company is\"\n->>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n-\n->>> outputs = model.generate(**inputs, penalty_alpha=0.6, top_k=4, max_new_tokens=100)\n->>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-['Hugging Face Company is a family owned and operated business. We pride ourselves on being the best\n-in the business and our customer service is second to none.\\n\\nIf you have any questions about our\n-products or services, feel free to contact us at any time. We look forward to hearing from you!']\n-```\n-\n ### ë‹¤í•­ ìƒ˜í”Œë§(Multinomial sampling)[[multinomial-sampling]]\n \n íƒìš• íƒìƒ‰(greedy search)ì´ í•­ìƒ ê°€ìž¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í† í°ì„ ë‹¤ìŒ í† í°ìœ¼ë¡œ ì„ íƒí•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬, ë‹¤í•­ ìƒ˜í”Œë§(multinomial sampling, ì¡°ìƒ ìƒ˜í”Œë§(ancestral sampling)ì´ë¼ê³ ë„ í•¨)ì€ ëª¨ë¸ì´ ì œê³µí•˜ëŠ” ì „ì²´ ì–´íœ˜ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í† í°ì„ ë¬´ìž‘ìœ„ë¡œ ì„ íƒí•©ë‹ˆë‹¤. 0ì´ ì•„ë‹Œ í™•ë¥ ì„ ê°€ì§„ ëª¨ë“  í† í°ì€ ì„ íƒë  ê¸°íšŒê°€ ìžˆìœ¼ë¯€ë¡œ, ë°˜ë³µì˜ ìœ„í—˜ì„ ì¤„ì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤."
        },
        {
            "sha": "879229c062e314aac8536b00ebfc93d882b91a68",
            "filename": "examples/pytorch/text-generation/run_generation_contrastive_search.py",
            "status": "removed",
            "additions": 0,
            "deletions": 146,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6/examples%2Fpytorch%2Ftext-generation%2Frun_generation_contrastive_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6/examples%2Fpytorch%2Ftext-generation%2Frun_generation_contrastive_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-generation%2Frun_generation_contrastive_search.py?ref=b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6",
            "patch": "@@ -1,146 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2022 University of Cambridge, Tencent AI Lab, DeepMind and The University of Hong Kong Authors and The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-# /// script\n-# dependencies = [\n-#     \"transformers @ git+https://github.com/huggingface/transformers.git\",\n-#     \"accelerate >= 0.21.0\",\n-#     \"sentencepiece != 0.1.92\",\n-#     \"protobuf\",\n-#     \"torch >= 1.3\",\n-# ]\n-# ///\n-\n-\"\"\"The examples of running contrastive search on the auto-APIs;\n-\n-Running this example:\n-python run_generation_contrastive_search.py --model_name_or_path=openai-community/gpt2-large --penalty_alpha=0.6 --k=4 --length=256\n-\"\"\"\n-\n-import argparse\n-import logging\n-\n-from accelerate import PartialState\n-from accelerate.utils import set_seed\n-\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n-\n-logging.basicConfig(\n-    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-    datefmt=\"%m/%d/%Y %H:%M:%S\",\n-    level=logging.INFO,\n-)\n-logger = logging.getLogger(__name__)\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\n-        \"--model_name_or_path\",\n-        default=None,\n-        type=str,\n-        required=True,\n-    )\n-    parser.add_argument(\"--prompt\", type=str, default=\"\")\n-    parser.add_argument(\"--length\", type=int, default=20)\n-    parser.add_argument(\"--stop_token\", type=str, default=None, help=\"Token at which text generation is stopped\")\n-    parser.add_argument(\n-        \"--temperature\",\n-        type=float,\n-        default=1.0,\n-        help=\"temperature of 1.0 has no effect, lower tend toward greedy sampling\",\n-    )\n-    parser.add_argument(\n-        \"--repetition_penalty\", type=float, default=1.0, help=\"primarily useful for CTRL model; in that case, use 1.2\"\n-    )\n-    parser.add_argument(\"--k\", type=int, default=0)\n-    parser.add_argument(\"--penalty_alpha\", type=float, default=0.0)\n-    parser.add_argument(\"--p\", type=float, default=0.9)\n-\n-    parser.add_argument(\"--prefix\", type=str, default=\"\", help=\"Text added prior to input.\")\n-    parser.add_argument(\"--padding_text\", type=str, default=\"\", help=\"Deprecated, the use of `--prefix` is preferred.\")\n-    parser.add_argument(\"--xlm_language\", type=str, default=\"\", help=\"Optional language when used with the XLM model.\")\n-\n-    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n-    parser.add_argument(\n-        \"--use_cpu\",\n-        action=\"store_true\",\n-        help=\"Whether or not to use cpu. If set to False, we will use gpu/npu or mps device if available\",\n-    )\n-    parser.add_argument(\n-        \"--fp16\",\n-        action=\"store_true\",\n-        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n-    )\n-    args = parser.parse_args()\n-\n-    # Initialize the distributed state.\n-    distributed_state = PartialState(cpu=args.use_cpu)\n-\n-    logger.warning(f\"device: {distributed_state.device}, 16-bits inference: {args.fp16}\")\n-\n-    if args.seed is not None:\n-        set_seed(args.seed)\n-\n-    # Initialize the model and tokenizer\n-    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n-    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n-\n-    # tokenizer = GPT2Tokenizer.from_pretrained(args.model_name_or_path)\n-    # model = OPTForCausalLM.from_pretrained(args.model_name_or_path)\n-    # Set the model to the right device\n-    model.to(distributed_state.device)\n-\n-    if args.fp16:\n-        model.half()\n-\n-    logger.info(args)\n-    prompt_text = args.prompt if args.prompt else input(\"Model prompt >>> \")\n-\n-    inputs = tokenizer(prompt_text, return_tensors=\"pt\", add_special_tokens=False)\n-    inputs = {key: value.to(distributed_state.device) for key, value in inputs.items()}\n-\n-    output_sequences = model.generate(\n-        **inputs,\n-        max_length=args.length + len(inputs[\"input_ids\"][0]),\n-        penalty_alpha=args.penalty_alpha,\n-        top_k=args.k,\n-    )\n-\n-    generated_sequences = []\n-    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n-        print(f\"=== GENERATED SEQUENCE {generated_sequence_idx + 1} ===\")\n-        generated_sequence = generated_sequence.tolist()\n-\n-        # Decode text\n-        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True, add_special_tokens=False)\n-\n-        # Remove all text after the stop token\n-        text = text[: text.find(args.stop_token) if args.stop_token else None]\n-\n-        # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n-        total_sequence = (\n-            prompt_text + text[len(tokenizer.decode(inputs[\"input_ids\"][0], clean_up_tokenization_spaces=True)) :]\n-        )\n-\n-        generated_sequences.append(total_sequence)\n-        print(total_sequence)\n-\n-    return generated_sequences\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "e1e38fc76da6e05634e4c6643513ca6f0049a79b",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -1358,7 +1358,7 @@ def check_dynamic_cache(self, method: str):\n     def crop(self, maximum_length: int):\n         \"\"\"\n         Crop the past key values up to a new `maximum_length` in terms of tokens. `maximum_length` can also be\n-        negative to remove `maximum_length` tokens. This is used in assisted decoding and contrastive search.\n+        negative to remove `maximum_length` tokens. This is used in assisted decoding and contrastive search (on the Hub).\n         \"\"\"\n         self.check_dynamic_cache(self.crop.__name__)\n         self.self_attention_cache.crop(maximum_length)\n@@ -1378,13 +1378,13 @@ def batch_split(self, full_batch_size: int, split_size: int) -> \"list[EncoderDec\n         return out\n \n     def batch_repeat_interleave(self, repeats: int):\n-        \"\"\"Repeat the cache `repeats` times in the batch dimension. Used in contrastive search.\"\"\"\n+        \"\"\"Repeat the cache `repeats` times in the batch dimension. Used in contrastive search (on the Hub).\"\"\"\n         self.check_dynamic_cache(self.batch_repeat_interleave.__name__)\n         self.self_attention_cache.batch_repeat_interleave(repeats)\n         self.cross_attention_cache.batch_repeat_interleave(repeats)\n \n     def batch_select_indices(self, indices: torch.Tensor):\n-        \"\"\"Only keep the `indices` in the batch dimension of the cache. Used in contrastive search.\"\"\"\n+        \"\"\"Only keep the `indices` in the batch dimension of the cache. Used in contrastive search (on the Hub).\"\"\"\n         self.check_dynamic_cache(self.batch_select_indices.__name__)\n         self.self_attention_cache.batch_select_indices(indices)\n         self.cross_attention_cache.batch_select_indices(indices)"
        },
        {
            "sha": "1edaf19948e82d9b9219fa4e9d896cb669cd0246",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 13,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -44,7 +44,7 @@\n logger = logging.get_logger(__name__)\n METADATA_FIELDS = (\"_from_model_config\", \"_commit_hash\", \"_original_object_hash\", \"transformers_version\")\n STATIC_CACHE_IMPLEMENTATIONS = (\"static\", \"offloaded_static\")\n-DYNAMIC_CACHE_IMPLEMENTATIONS = (\"dynamic\", \"offloaded\", \"quantized\")\n+DYNAMIC_CACHE_IMPLEMENTATIONS = (\"dynamic\", \"dynamic_full\", \"offloaded\", \"quantized\")\n # All the following are redundant and deprecated, but kept for BC\n DEPRECATED_STATIC_CACHE_IMPLEMENTATIONS = (\n     \"sliding_window\",\n@@ -86,7 +86,6 @@ class GenerationConfig(PushToHubMixin):\n     for text-decoder, text-to-text, speech-to-text, and vision-to-text models:\n \n         - *greedy decoding* if `num_beams=1` and `do_sample=False`\n-        - *contrastive search* if `penalty_alpha>0.` and `top_k>1`\n         - *multinomial sampling* if `num_beams=1` and `do_sample=True`\n         - *beam-search decoding* if `num_beams>1` and `do_sample=False`\n         - *beam-search multinomial sampling* if `num_beams>1` and `do_sample=True`\n@@ -138,8 +137,6 @@ class GenerationConfig(PushToHubMixin):\n         num_beam_groups (`int`, *optional*, defaults to 1):\n             Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.\n             [this paper](https://huggingface.co/papers/1610.02424) for more details.\n-        penalty_alpha (`float`, *optional*):\n-            The values balance the model confidence and the degeneration penalty in contrastive search decoding.\n \n         > Parameters that control the cache\n \n@@ -255,9 +252,6 @@ class GenerationConfig(PushToHubMixin):\n             The guidance scale for classifier free guidance (CFG). CFG is enabled by setting `guidance_scale > 1`.\n             Higher guidance scale encourages the model to generate samples that are more closely linked to the input\n             prompt, usually at the expense of poorer quality.\n-        low_memory (`bool`, *optional*):\n-            Switch to sequential beam search and sequential topk for contrastive search to reduce peak memory.\n-            Used with beam search and contrastive search.\n         watermarking_config (`BaseWatermarkingConfig` or `dict`, *optional*):\n             Arguments used to watermark the model outputs by adding a small bias to randomly selected set of \"green\"\n             tokens. See the docs of [`SynthIDTextWatermarkingConfig`] and [`WatermarkingConfig`] for more\n@@ -366,8 +360,6 @@ def __init__(self, **kwargs):\n         self.do_sample = kwargs.pop(\"do_sample\", False)\n         self.num_beams = kwargs.pop(\"num_beams\", 1)\n         self.num_beam_groups = kwargs.pop(\"num_beam_groups\", 1)\n-        self.penalty_alpha = kwargs.pop(\"penalty_alpha\", None)\n-        self.dola_layers = kwargs.pop(\"dola_layers\", None)\n \n         # Parameters that control the cache\n         self.use_cache = kwargs.pop(\"use_cache\", True)\n@@ -403,7 +395,7 @@ def __init__(self, **kwargs):\n         self.sequence_bias = kwargs.pop(\"sequence_bias\", None)\n         self.token_healing = kwargs.pop(\"token_healing\", False)\n         self.guidance_scale = kwargs.pop(\"guidance_scale\", None)\n-        self.low_memory = kwargs.pop(\"low_memory\", None)\n+\n         watermarking_config = kwargs.pop(\"watermarking_config\", None)\n         if watermarking_config is None:\n             self.watermarking_config = None\n@@ -445,6 +437,11 @@ def __init__(self, **kwargs):\n         self.compile_config = kwargs.pop(\"compile_config\", None)\n         self.disable_compile = kwargs.pop(\"disable_compile\", False)\n \n+        # Deprecated (moved to the Hub). TODO joao, manuel: remove in v4.62.0\n+        self.low_memory = kwargs.pop(\"low_memory\", None)\n+        self.penalty_alpha = kwargs.pop(\"penalty_alpha\", None)\n+        self.dola_layers = kwargs.pop(\"dola_layers\", None)\n+\n         # The remaining attributes do not parametrize `.generate()`, but are informative and/or used by the hub\n         # interface.\n         self._from_model_config = kwargs.pop(\"_from_model_config\", False)\n@@ -610,9 +607,7 @@ def validate(self, strict=False):\n                 minor_issues[\"typical_p\"] = greedy_wrong_parameter_msg.format(\n                     flag_name=\"typical_p\", flag_value=self.typical_p\n                 )\n-            if (\n-                self.top_k is not None and self.top_k != 50 and self.penalty_alpha is None\n-            ):  # contrastive search uses top_k\n+            if self.top_k is not None and self.top_k != 50:\n                 minor_issues[\"top_k\"] = greedy_wrong_parameter_msg.format(flag_name=\"top_k\", flag_value=self.top_k)\n             if self.epsilon_cutoff is not None and self.epsilon_cutoff != 0.0:\n                 minor_issues[\"epsilon_cutoff\"] = greedy_wrong_parameter_msg.format("
        },
        {
            "sha": "e03ad600deb334393f696807488bfad7866bedaf",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 26,
            "deletions": 475,
            "changes": 501,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -43,7 +43,6 @@\n from ..integrations.deepspeed import is_deepspeed_zero3_enabled\n from ..integrations.fsdp import is_fsdp_managed_module\n from ..masking_utils import create_masks_for_generate\n-from ..modeling_outputs import CausalLMOutputWithPast, Seq2SeqLMOutput\n from ..pytorch_utils import isin_mps_friendly\n from ..tokenization_utils import ExtensionsTrie\n from ..utils import (\n@@ -369,7 +368,6 @@ class GenerationMixin(ContinuousMixin):\n \n     The class exposes [`~generation.GenerationMixin.generate`], which can be used for:\n         - *greedy decoding* if `num_beams=1` and `do_sample=False`\n-        - *contrastive search* if `penalty_alpha>0` and `top_k>1`\n         - *multinomial sampling* if `num_beams=1` and `do_sample=True`\n         - *beam-search decoding* if `num_beams>1` and `do_sample=False`\n         - *beam-search multinomial sampling* if `num_beams>1` and `do_sample=True`\n@@ -1946,15 +1944,17 @@ def _prepare_cache_for_generation(\n             )\n             generation_config.cache_implementation = None\n \n-        # assisted decoding and contrastive search need to roll-back the Cache, which is not supported if\n-        # it has sliding layers - so if we use any of those 2, do not pass the config to DynamicCache, which\n-        # will result in creating a Cache with only full layers even if model uses sliding window\n+        # Assisted decoding and contrastive search require cache rollback, which is incompatible with sliding layers.\n+        # To handle this, we skip passing the model config to DynamicCache (forcing a full-layer cache).\n+        # The \"dynamic_full\" option is a shortcut for generate() users to avoid sliding layers on their own.\n         generation_mode = generation_config.get_generation_mode(assistant_model)\n-        dynamic_cache_kwargs = (\n-            {\"config\": self.config}\n-            if generation_mode not in (GenerationMode.ASSISTED_GENERATION, GenerationMode.CONTRASTIVE_SEARCH)\n-            else {}\n-        )\n+        if (\n+            generation_mode in (GenerationMode.ASSISTED_GENERATION, GenerationMode.CONTRASTIVE_SEARCH)\n+            or generation_config.cache_implementation == \"dynamic_full\"\n+        ):\n+            dynamic_cache_kwargs = {}\n+        else:\n+            dynamic_cache_kwargs = {\"config\": self.config}\n         if generation_config.cache_implementation is not None:\n             if generation_config.cache_implementation in ALL_STATIC_CACHE_IMPLEMENTATIONS:\n                 if generation_config.cache_implementation in DEPRECATED_STATIC_CACHE_IMPLEMENTATIONS:\n@@ -1995,7 +1995,7 @@ def _prepare_cache_for_generation(\n                 model_kwargs[cache_name] = QuantizedCache(backend=backend, **cache_config)\n             elif generation_config.cache_implementation == \"offloaded\":\n                 model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs, offloading=True)\n-            elif generation_config.cache_implementation == \"dynamic\":\n+            elif \"dynamic\" in generation_config.cache_implementation:\n                 model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs)\n \n         # Use DynamicCache() instance by default. This will avoid back and forth from legacy format that\n@@ -2512,29 +2512,26 @@ def generate(\n                 trust_remote_code=trust_remote_code,\n                 **kwargs,\n             )\n-\n+        # TODO joao, manuel: remove this in v4.62.0\n         elif generation_mode == GenerationMode.CONTRASTIVE_SEARCH:\n+            logger.warning_once(\n+                \"Contrastive search was moved to a `custom_generate` repo: https://hf.co/transformers-community/contrastive-search. \"\n+                \"To prevent loss of backward compatibility, add `custom_generate='transformers-community/contrastive-search'` \"\n+                \"to your `generate` call before v4.62.0.\"\n+            )\n             if not trust_remote_code:\n                 logger.warning_once(\n-                    \"Contrastive Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. \"\n-                    \"To prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\"\n+                    \"Contrastive search requires `trust_remote_code=True` in your `generate` call, since \"\n+                    \"it loads https://hf.co/transformers-community/contrastive-search.\"\n                 )\n-            if not model_kwargs[\"use_cache\"]:\n-                raise ValueError(\"Contrastive search requires `use_cache=True`\")\n-            if self._is_stateful:\n-                # Just like assisted generation, we need to be able to rollback to a previous state (see comment above)\n-                raise ValueError(\n-                    f\"contrastive search is not supported with stateful models, such as {self.__class__.__name__}\"\n-                )\n-\n-            result = self._contrastive_search(\n-                input_ids,\n-                logits_processor=prepared_logits_processor,\n-                stopping_criteria=prepared_stopping_criteria,\n+            # Avoid calling the model-defined `generate` method, since some models (e.g. Janus, Whisper) override it.\n+            return GenerationMixin.generate(\n+                self,\n+                inputs,\n+                custom_generate=\"transformers-community/contrastive-search\",\n                 generation_config=generation_config,\n-                synced_gpus=synced_gpus,\n-                streamer=streamer,\n-                **model_kwargs,\n+                trust_remote_code=trust_remote_code,\n+                **kwargs,\n             )\n \n         elif generation_mode in (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n@@ -2765,421 +2762,6 @@ def heal_tokens(\n \n         return input_ids\n \n-    @torch.no_grad()\n-    def _contrastive_search(\n-        self,\n-        input_ids: torch.LongTensor,\n-        logits_processor: LogitsProcessorList,\n-        stopping_criteria: StoppingCriteriaList,\n-        generation_config: GenerationConfig,\n-        synced_gpus: bool,\n-        streamer: Optional[\"BaseStreamer\"],\n-        **model_kwargs,\n-    ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:\n-        r\"\"\"\n-        Generates sequences of token ids for models with a language modeling head using **contrastive search** and can\n-        be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n-\n-        Parameters:\n-            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                The sequence used as a prompt for the generation.\n-            logits_processor (`LogitsProcessorList`):\n-                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n-                used to modify the prediction scores of the language modeling head applied at each generation step.\n-            stopping_criteria (`StoppingCriteriaList`):\n-                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n-                used to tell if the generation loop should stop.\n-            generation_config ([`~generation.GenerationConfig`]):\n-                The generation configuration to be used as parametrization of the decoding method.\n-            synced_gpus (`bool`):\n-                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n-                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n-            streamer (`BaseStreamer`, *optional*):\n-                Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n-                through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n-            model_kwargs:\n-                Additional model specific keyword arguments will be forwarded to the `forward` function of the model.\n-                If model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n-\n-        Return:\n-            [`~generation.GenerateDecoderOnlyOutput`], [`~generation.GenerateEncoderDecoderOutput`]\n-            or `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n-            [`~generation.GenerateDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n-            `return_dict_in_generate=True` or a [`~generation.GenerateEncoderDecoderOutput`] if\n-            `model.config.is_encoder_decoder=True`.\n-        \"\"\"\n-        # init values\n-        has_eos_stopping_criteria = any(hasattr(criteria, \"eos_token_id\") for criteria in stopping_criteria)\n-        top_k = generation_config.top_k\n-        penalty_alpha = generation_config.penalty_alpha\n-        pad_token_id = generation_config._pad_token_tensor\n-        output_attentions = generation_config.output_attentions\n-        output_hidden_states = generation_config.output_hidden_states\n-        output_scores = generation_config.output_scores\n-        output_logits = generation_config.output_logits\n-        return_dict_in_generate = generation_config.return_dict_in_generate\n-        sequential = generation_config.low_memory\n-\n-        # init attention / hidden states / scores tuples\n-        raw_logits = () if (return_dict_in_generate and output_logits) else None\n-        scores = () if (return_dict_in_generate and output_scores) else None\n-        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n-        cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n-        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n-\n-        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n-        if return_dict_in_generate and self.config.is_encoder_decoder:\n-            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n-            encoder_hidden_states = (\n-                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n-            )\n-\n-        # keep track of which sequences are already finished\n-        batch_size, cur_len = input_ids.shape[:2]\n-        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n-        model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)\n-\n-        # Create cosine_matrix_mask based on the attention_mask\n-        cosine_matrix_mask = torch.ones_like(input_ids, dtype=torch.long)\n-        if self.config.is_encoder_decoder:\n-            if \"decoder_attention_mask\" in model_kwargs and model_kwargs[\"decoder_attention_mask\"] is not None:\n-                cosine_matrix_mask = model_kwargs[\"decoder_attention_mask\"]\n-        else:\n-            cosine_matrix_mask = model_kwargs[\"attention_mask\"]\n-        cosine_matrix_mask = cosine_matrix_mask.repeat_interleave(top_k, dim=0)\n-\n-        this_peer_finished = False\n-\n-        while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n-            # if the first step in the loop, encode all the prefix and obtain: (1) past_key_values;\n-            # (2) last_hidden_states; (3) logit_for_next_step; (4) update model kwargs for the next step\n-            if model_kwargs.get(\"past_key_values\") is None or (\n-                isinstance(model_kwargs[\"past_key_values\"], (Cache, EncoderDecoderCache))\n-                and model_kwargs[\"past_key_values\"].get_seq_length() == 0\n-            ):\n-                # prepare inputs\n-                model_kwargs[\"use_cache\"] = True\n-                model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n-\n-                # encode the given prefix and prepare model inputs; encoder-decoder model process the prefix and save\n-                # the `encoder_outputs`\n-                outputs = self(\n-                    **model_inputs, return_dict=True, output_hidden_states=True, output_attentions=output_attentions\n-                )\n-\n-                # last decoder hidden states will be used to compute the degeneration penalty (cosine similarity with\n-                # previous tokens)\n-                if self.config.is_encoder_decoder:\n-                    last_hidden_states = outputs.decoder_hidden_states[-1]\n-                else:\n-                    last_hidden_states = outputs.hidden_states[-1]\n-\n-                # next logit for contrastive search to select top-k candidate tokens\n-                # Copy is needed to avoid keeping a hanging ref to outputs.logits which may be very large for this first iteration\n-                # (the clone itself is always small)\n-                # torch.float32 is needed to retain precision for later logits manipulations\n-                logit_for_next_step = outputs.logits[:, -1, :].to(\n-                    copy=True, dtype=torch.float32, device=input_ids.device\n-                )\n-\n-                model_kwargs = self._update_model_kwargs_for_generation(\n-                    outputs,\n-                    model_kwargs,\n-                    is_encoder_decoder=self.config.is_encoder_decoder,\n-                )\n-\n-                if not sequential:\n-                    # Expands model inputs top_k times, for batched forward passes (akin to beam search).\n-                    # input_ids is required for expanding visual inputs in qwen2vl\n-                    _, model_kwargs = self._expand_inputs_for_generation(\n-                        input_ids=input_ids,\n-                        expand_size=top_k,\n-                        is_encoder_decoder=self.config.is_encoder_decoder,\n-                        **model_kwargs,\n-                    )\n-\n-                past_key_values = model_kwargs.get(\"past_key_values\")\n-                if past_key_values is None:\n-                    raise ValueError(\n-                        f\"{self.__class__.__name__} does not support caching and therefore **can't** be used \"\n-                        \"for contrastive search.\"\n-                    )\n-                elif (\n-                    not isinstance(past_key_values[0], (tuple, torch.Tensor))\n-                    or past_key_values[0][0].shape[0] != batch_size\n-                ):\n-                    raise ValueError(\n-                        f\"{self.__class__.__name__} does not have a standard cache format and therefore **can't** be \"\n-                        \"used for contrastive search without further modifications.\"\n-                    )\n-\n-            # contrastive_search main logic start:\n-            # contrastive search decoding consists of two steps: (1) candidate tokens recall; (2) candidate re-rank by\n-            # degeneration penalty\n-            processed_logit_for_next_step = logits_processor(input_ids, logit_for_next_step)\n-            next_probs = nn.functional.softmax(processed_logit_for_next_step, dim=-1)\n-\n-            top_k_probs, top_k_ids = torch.topk(next_probs, dim=-1, k=top_k)\n-\n-            # Store scores, attentions and hidden_states when required\n-            if return_dict_in_generate:\n-                if output_logits:\n-                    raw_logits += (logit_for_next_step,)\n-                if output_scores:\n-                    scores += (processed_logit_for_next_step,)\n-                if output_attentions:\n-                    decoder_attentions += (\n-                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n-                    )\n-                    if self.config.is_encoder_decoder:\n-                        cross_attentions += (outputs.cross_attentions,)\n-\n-                if output_hidden_states:\n-                    decoder_hidden_states += (\n-                        (outputs.decoder_hidden_states,)\n-                        if self.config.is_encoder_decoder\n-                        else (outputs.hidden_states,)\n-                    )\n-\n-            # This is needed to properly delete outputs.logits which may be very large for this first iteration\n-            # Otherwise a reference to outputs.logits is kept all along until after the next call to self.forward()\n-            del outputs\n-\n-            if not sequential:\n-                # Replicates the new past_key_values to match the `top_k` candidates\n-                past = model_kwargs[\"past_key_values\"]\n-                # If it is a static cache, modify it in-place layer after layer to save memory\n-                if isinstance(past, DynamicCache) or (\n-                    isinstance(past, EncoderDecoderCache) and isinstance(past.self_attention_cache, DynamicCache)\n-                ):\n-                    past.batch_repeat_interleave(top_k)\n-                else:\n-                    new_key_values = []\n-                    for layer in past:\n-                        items = []\n-                        # item is either the key or the value matrix\n-                        for item in layer:\n-                            items.append(item.repeat_interleave(top_k, dim=0))\n-                        new_key_values.append(tuple(items))\n-\n-                    past = tuple(new_key_values)\n-\n-                model_kwargs[\"past_key_values\"] = past\n-\n-            if sequential:\n-                all_outputs = []\n-                for i in range(top_k):\n-                    # compute the candidate tokens by the language model and collect their hidden_states\n-                    next_model_inputs = self.prepare_inputs_for_generation(top_k_ids[:, i].view(-1, 1), **model_kwargs)\n-\n-                    outputs = self(\n-                        **next_model_inputs,\n-                        return_dict=True,\n-                        output_hidden_states=True,\n-                        output_attentions=output_attentions,\n-                    )\n-                    if isinstance(outputs[\"past_key_values\"], DynamicCache) or (\n-                        isinstance(outputs[\"past_key_values\"], EncoderDecoderCache)\n-                        and isinstance(outputs[\"past_key_values\"].self_attention_cache, DynamicCache)\n-                    ):\n-                        # Remove past K-V from output since we don't need to stack later\n-                        outputs[\"past_key_values\"] = None\n-                        # Remove last token from past K-V since we don't want to append it at this point\n-                        model_kwargs[\"past_key_values\"].crop(-1)\n-\n-                    all_outputs.append(outputs)\n-                outputs = stack_model_outputs(all_outputs, self.config.get_text_config())\n-\n-            else:\n-                # compute the candidate tokens by the language model and collect their hidden_states\n-                # assembles top_k_ids into batch of size k\n-                next_model_inputs = self.prepare_inputs_for_generation(top_k_ids.view(-1, 1), **model_kwargs)\n-\n-                outputs = self(\n-                    **next_model_inputs,\n-                    return_dict=True,\n-                    output_hidden_states=True,\n-                    output_attentions=output_attentions,\n-                )\n-\n-            # This is essential to avoid having a last reference to the big past K-V and double the necessary memory\n-            # in the next loop\n-            del next_model_inputs\n-\n-            # name is different for encoder-decoder and decoder-only models\n-            if self.config.is_encoder_decoder:\n-                next_hidden = outputs.decoder_hidden_states[-1]\n-                full_hidden_states = outputs.decoder_hidden_states\n-            else:\n-                next_hidden = outputs.hidden_states[-1]\n-                full_hidden_states = outputs.hidden_states\n-\n-            # .float() is needed to retain precision for later logits manipulations\n-            logits = outputs.logits[:, -1, :].float()\n-            context_hidden = last_hidden_states.repeat_interleave(top_k, dim=0)\n-\n-            # compute the degeneration penalty and re-rank the candidates based on the degeneration penalty and the\n-            # model confidence. Keeping `selected_idx` on CPU enables multi-device contrastive search and doesn't\n-            # introduce (noticeable) slowdowns on single-device runs.\n-            selected_idx = _ranking_fast(\n-                context_hidden, next_hidden, top_k_probs, cosine_matrix_mask, penalty_alpha, top_k\n-            )\n-            cosine_matrix_mask = torch.cat(\n-                [cosine_matrix_mask, cosine_matrix_mask.new_ones((cosine_matrix_mask.shape[0], 1))], dim=-1\n-            )\n-            selected_idx = selected_idx.to(\"cpu\")\n-\n-            # This will be used instead of the previous inneficient torch.stack(torch.split())\n-            augmented_idx = torch.tensor([x + i * top_k for i, x in enumerate(selected_idx)])\n-\n-            # prepare for the next step: (1) next token_id; (2) past_key_values; (3) last_hidden_states for computing\n-            # the degeneration penalty; (4) logits for selecting next top-k candidates; (5) selected tokens scores\n-            # (model confidence minus degeneration penalty); (6) decoder hidden_states\n-            next_tokens = top_k_ids[range(len(top_k_ids)), selected_idx]\n-            next_hidden = torch.stack(torch.split(next_hidden.squeeze(dim=1), top_k))\n-            next_hidden = next_hidden[range(batch_size), selected_idx, :]\n-            last_hidden_states = torch.cat([last_hidden_states, next_hidden.unsqueeze(1)], dim=1)\n-\n-            next_decoder_hidden_states = ()\n-            for layer in full_hidden_states:\n-                layer = torch.stack(torch.split(layer, top_k))[range(batch_size), selected_idx, :]\n-                next_decoder_hidden_states += (layer,)\n-\n-            # generate past_key_values cache of only the selected token\n-            if sequential:\n-                next_model_input = self.prepare_inputs_for_generation(\n-                    top_k_ids[:, selected_idx].view(-1, 1), **model_kwargs\n-                )\n-\n-                selected_outputs = self(\n-                    **next_model_input,\n-                    return_dict=True,\n-                    output_hidden_states=False,\n-                    output_attentions=False,\n-                )\n-                next_past_key_values = selected_outputs[\"past_key_values\"]\n-\n-            else:\n-                next_past_key_values = None\n-                for possible_cache_name in ALL_CACHE_NAMES:\n-                    next_past_key_values = next_past_key_values or getattr(outputs, possible_cache_name, None)\n-                # Do it in-place layer per layer to save memory\n-                if isinstance(next_past_key_values, DynamicCache) or (\n-                    isinstance(next_past_key_values, EncoderDecoderCache)\n-                    and isinstance(next_past_key_values.self_attention_cache, DynamicCache)\n-                ):\n-                    next_past_key_values.batch_select_indices(augmented_idx)\n-                else:\n-                    new_key_values = []\n-                    for layer in next_past_key_values:\n-                        items = []\n-                        # item is either the key or the value matrix\n-                        for item in layer:\n-                            items.append(item[augmented_idx, ...])\n-                        new_key_values.append(tuple(items))\n-\n-                    next_past_key_values = tuple(new_key_values)\n-\n-            logit_for_next_step = torch.stack(torch.split(logits, top_k))[range(batch_size), selected_idx, :]\n-            logit_for_next_step = logit_for_next_step.to(input_ids.device)\n-\n-            # Rebuilds the relevant parts of the model output for the selected token, for use in the next iteration\n-            if self.config.is_encoder_decoder:\n-                next_step_cross_attentions = ()\n-                next_step_decoder_attentions = ()\n-                if output_attentions:\n-                    for layer in outputs.cross_attentions:\n-                        layer = torch.stack(torch.split(layer, top_k, dim=0))[range(batch_size), selected_idx, ...]\n-                        next_step_cross_attentions += (layer,)\n-                    for layer in outputs.decoder_attentions:\n-                        layer = torch.stack(torch.split(layer, top_k, dim=0))[range(batch_size), selected_idx, ...]\n-                        next_step_decoder_attentions += (layer,)\n-                outputs = Seq2SeqLMOutput(\n-                    past_key_values=next_past_key_values,\n-                    decoder_hidden_states=next_decoder_hidden_states,\n-                    decoder_attentions=next_step_decoder_attentions or None,\n-                    cross_attentions=next_step_cross_attentions or None,\n-                )\n-            else:\n-                next_step_attentions = ()\n-                if output_attentions:\n-                    for layer in outputs.attentions:\n-                        layer = torch.stack(torch.split(layer, top_k, dim=0))[range(batch_size), selected_idx, ...]\n-                        next_step_attentions += (layer,)\n-                outputs = CausalLMOutputWithPast(\n-                    past_key_values=next_past_key_values,\n-                    hidden_states=next_decoder_hidden_states,\n-                    attentions=next_step_attentions or None,\n-                )\n-            # contrastive_search main logic end\n-\n-            # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n-            model_kwargs = self._update_model_kwargs_for_generation(\n-                outputs,\n-                model_kwargs,\n-                is_encoder_decoder=self.config.is_encoder_decoder,\n-            )\n-            if synced_gpus and this_peer_finished:\n-                continue\n-\n-            # finished sentences should have their next token be a padding token\n-            if has_eos_stopping_criteria:\n-                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n-\n-            # update generated ids, model inputs, and length for next step\n-            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n-            if streamer is not None:\n-                streamer.put(next_tokens.cpu())\n-\n-            # stop when each sentence is finished\n-            unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n-            this_peer_finished = unfinished_sequences.max() == 0\n-\n-        if streamer is not None:\n-            streamer.end()\n-\n-        if return_dict_in_generate:\n-            # Contrastive search works by forward looking at the next token, so we need to exclude it from\n-            # `past_key_values` to be consistent with the other decoding methods\n-            if model_kwargs.get(\"past_key_values\") is not None:\n-                if isinstance(model_kwargs[\"past_key_values\"], DynamicCache) or (\n-                    isinstance(model_kwargs[\"past_key_values\"], EncoderDecoderCache)\n-                    and isinstance(model_kwargs[\"past_key_values\"].self_attention_cache, DynamicCache)\n-                ):\n-                    model_kwargs[\"past_key_values\"].crop(-1)\n-                else:\n-                    past_key_values = []\n-                    for layer in model_kwargs[\"past_key_values\"]:\n-                        layer_past_key_values = []\n-                        for item in layer:\n-                            layer_past_key_values.append(item[..., :-1, :])\n-                        past_key_values.append(tuple(layer_past_key_values))\n-                    model_kwargs[\"past_key_values\"] = tuple(past_key_values)\n-\n-            if self.config.is_encoder_decoder:\n-                return GenerateEncoderDecoderOutput(\n-                    sequences=input_ids,\n-                    scores=scores,\n-                    logits=raw_logits,\n-                    encoder_attentions=encoder_attentions,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    decoder_attentions=decoder_attentions,\n-                    cross_attentions=cross_attentions,\n-                    decoder_hidden_states=decoder_hidden_states,\n-                    past_key_values=model_kwargs.get(\"past_key_values\"),\n-                )\n-            else:\n-                return GenerateDecoderOnlyOutput(\n-                    sequences=input_ids,\n-                    scores=scores,\n-                    logits=raw_logits,\n-                    attentions=decoder_attentions,\n-                    hidden_states=decoder_hidden_states,\n-                    past_key_values=model_kwargs.get(\"past_key_values\"),\n-                )\n-        else:\n-            return input_ids\n-\n     def _sample(\n         self,\n         input_ids: torch.LongTensor,\n@@ -4873,37 +4455,6 @@ def _split_model_outputs(outputs, new_outputs, cur_len, added_len, is_decoder_at\n     return outputs\n \n \n-def _ranking_fast(\n-    context_hidden: torch.FloatTensor,\n-    next_hidden: torch.FloatTensor,\n-    next_top_k_probs: torch.FloatTensor,\n-    cosine_matrix_mask: torch.LongTensor,\n-    alpha: float,\n-    beam_width: int,\n-) -> torch.FloatTensor:\n-    \"\"\"\n-    Reranks the top_k candidates based on a degeneration penalty (cosine similarity with previous tokens), as described\n-    in the paper \"A Contrastive Framework for Neural Text Generation\". Returns the index of the best candidate for each\n-    row in the batch.\n-    \"\"\"\n-    norm_context_hidden = context_hidden / context_hidden.norm(dim=2, keepdim=True)\n-    norm_next_hidden = next_hidden / next_hidden.norm(dim=2, keepdim=True)\n-    cosine_matrix = torch.matmul(norm_context_hidden, norm_next_hidden.transpose(1, 2)).squeeze(-1)  # [B*K, S]\n-\n-    # Penalize cosine_matrix based on the cosine_matrix_mask (ignore padding positions)\n-    # Using a large negative value for masked positions\n-    cosine_matrix_mask = cosine_matrix_mask.to(dtype=cosine_matrix.dtype)\n-    cosine_matrix_mask = (1 - cosine_matrix_mask) * torch.finfo(cosine_matrix.dtype).min\n-    cosine_matrix = cosine_matrix + cosine_matrix_mask\n-\n-    degeneration_penalty, _ = torch.max(cosine_matrix, dim=-1)  # [B*K]\n-    next_top_k_probs = next_top_k_probs.view(-1)  # [B*K]\n-    contrastive_score = (1.0 - alpha) * next_top_k_probs - alpha * degeneration_penalty\n-    contrastive_score = torch.stack(torch.split(contrastive_score, beam_width))  # [B, K]\n-    _, selected_idx = contrastive_score.max(dim=-1)  # [B]\n-    return selected_idx\n-\n-\n def stack_model_outputs(model_outputs: list[ModelOutput], config: PretrainedConfig) -> ModelOutput:\n     \"\"\"\n     Stack a list of ModelOutput objects (or its subclasses) along the batch_size dimension. The function infers the"
        },
        {
            "sha": "429d61cbd26aaf8188b7c9908c427e8198ad1047",
            "filename": "tests/generation/test_configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_configuration_utils.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -289,6 +289,7 @@ def test_generation_mode(self):\n         config = GenerationConfig(num_beams=2)\n         self.assertEqual(config.get_generation_mode(), GenerationMode.BEAM_SEARCH)\n \n+        # TODO joao, manuel: remove this in v4.62.0\n         config = GenerationConfig(top_k=10, do_sample=False, penalty_alpha=0.6)\n         self.assertEqual(config.get_generation_mode(), GenerationMode.CONTRASTIVE_SEARCH)\n "
        },
        {
            "sha": "449d8122c12b0f366d211911b9f1f3069896a58f",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 303,
            "changes": 315,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -62,7 +62,6 @@\n \n if is_torch_available():\n     import torch\n-    import torch.nn.functional as F\n \n     from transformers import (\n         AutoModelForCausalLM,\n@@ -76,7 +75,6 @@\n         GPT2Tokenizer,\n         ImageGPTForCausalImageModeling,\n         SpeechEncoderDecoderModel,\n-        T5ForConditionalGeneration,\n     )\n     from transformers.cache_utils import (\n         Cache,\n@@ -415,41 +413,6 @@ def _constrained_beam_search_generate(\n \n         return output_generate\n \n-    def _contrastive_generate(\n-        self,\n-        model,\n-        inputs_dict,\n-        output_scores=False,\n-        output_logits=False,\n-        output_attentions=False,\n-        output_hidden_states=False,\n-        return_dict_in_generate=False,\n-        use_cache=True,\n-    ):\n-        contrastive_search_kwargs = {\n-            \"penalty_alpha\": 0.6,\n-            \"top_k\": 5,\n-        }\n-\n-        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n-        output_generate = model.generate(\n-            do_sample=False,\n-            num_beams=1,\n-            max_new_tokens=self.max_new_tokens,\n-            min_new_tokens=self.max_new_tokens,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            output_scores=output_scores,\n-            output_logits=output_logits,\n-            return_dict_in_generate=return_dict_in_generate,\n-            use_cache=use_cache,\n-            **logits_processor_kwargs,\n-            **contrastive_search_kwargs,\n-            **inputs_dict,\n-        )\n-\n-        return output_generate\n-\n     @pytest.mark.generate\n     def test_greedy_generate(self):\n         for model_class in self.all_generative_model_classes:\n@@ -964,108 +927,6 @@ def test_constrained_beam_search_generate_dict_output(self):\n                 num_beams=beam_kwargs[\"num_beams\"],\n             )\n \n-    @pytest.mark.generate\n-    def test_contrastive_generate(self):\n-        for model_class in self.all_generative_model_classes:\n-            if model_class._is_stateful:\n-                self.skipTest(reason=\"Stateful models don't support contrastive search generation\")\n-\n-            # won't fix: FSMT and Reformer have a different cache variable type (and format).\n-            if any(model_name in model_class.__name__.lower() for model_name in [\"reformer\"]):\n-                self.skipTest(reason=\"Won't fix: old model with different cache format\")\n-\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-\n-            # NOTE: contrastive search only works with cache on at the moment.\n-            if not hasattr(config.get_text_config(), \"use_cache\"):\n-                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n-            config.is_decoder = True\n-\n-            # test old generation output for backwards compatibility\n-            model = model_class(config).to(torch_device).eval()\n-            output_generate = self._contrastive_generate(\n-                model=model,\n-                inputs_dict=inputs_dict,\n-                use_cache=True,  # Enable cache\n-            )\n-            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n-                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n-            else:\n-                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n-\n-    @pytest.mark.generate\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        for model_class in self.all_generative_model_classes:\n-            if model_class._is_stateful:\n-                self.skipTest(reason=\"Stateful models don't support contrastive search generation\")\n-\n-            # won't fix: FSMT and Reformer have a different cache variable type (and format).\n-            if any(model_name in model_class.__name__.lower() for model_name in [\"reformer\"]):\n-                self.skipTest(reason=\"Won't fix: old model with different cache format\")\n-\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-\n-            # NOTE: contrastive search only works with cache on at the moment.\n-            if not hasattr(config.get_text_config(), \"use_cache\"):\n-                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n-            config.is_decoder = True\n-            if self.has_attentions:\n-                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n-\n-            model = model_class(config).to(torch_device).eval()\n-            output_generate = self._contrastive_generate(\n-                model=model,\n-                inputs_dict=inputs_dict,\n-                output_scores=True,\n-                output_logits=True,\n-                output_hidden_states=True,\n-                output_attentions=self.has_attentions,\n-                return_dict_in_generate=True,\n-                use_cache=True,  # Enable cache\n-            )\n-\n-            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n-                self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n-            else:\n-                self.assertTrue(\n-                    output_generate.sequences.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1]\n-                )\n-\n-            self._check_generate_outputs(output_generate, model.config, use_cache=True)\n-\n-    @pytest.mark.generate\n-    def test_contrastive_generate_low_memory(self):\n-        # Check that choosing 'low_memory' does not change the model output\n-        for model_class in self.all_generative_model_classes:\n-            if model_class._is_stateful:\n-                self.skipTest(reason=\"Stateful models don't support contrastive search generation\")\n-\n-            if any(model_name in model_class.__name__.lower() for model_name in [\"reformer\"]):\n-                self.skipTest(reason=\"Won't fix: old model with different cache format\")\n-\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n-\n-            # NOTE: contrastive search only works with cache on at the moment.\n-            if not hasattr(config.get_text_config(), \"use_cache\"):\n-                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n-\n-            config.is_decoder = True\n-\n-            # test output equality of low versus high memory\n-            model = model_class(config).to(torch_device).eval()\n-            generate_kwargs = {\n-                \"top_k\": 4,\n-                \"penalty_alpha\": 0.6,\n-                \"max_new_tokens\": self.max_new_tokens,\n-                \"use_cache\": True,\n-                \"return_dict_in_generate\": True,\n-                \"output_scores\": True,\n-            }\n-\n-            low_output = model.generate(**inputs_dict, **generate_kwargs, low_memory=True)\n-            high_output = model.generate(**inputs_dict, **generate_kwargs, low_memory=False)\n-            self.assertTrue(has_similar_generate_outputs(low_output, high_output))\n-\n     @parameterized.expand([(\"random\",), (\"same\",)])\n     @pytest.mark.generate\n     def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n@@ -3443,31 +3304,6 @@ def test_decoder_start_id_from_config(self):\n         with self.assertRaises(ValueError):\n             outputs = bart_model.generate(input_ids, generation_config=GenerationConfig(do_sample=False))\n \n-    def test_contrastive_search_batched(self):\n-        # Tests that contrastive search works with batched inputs (i.e. has the same output as for non-batched inputs)\n-        articles = [\"Foo\", \"Bar Baz\"]\n-        tokenizer = BartTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-bart\")\n-        model = BartForConditionalGeneration.from_pretrained(\"hf-internal-testing/tiny-random-bart\").to(torch_device)\n-\n-        model.config.eos_token_id = None\n-        input_ids_batched = tokenizer(articles, padding=True, return_tensors=\"pt\").input_ids.to(torch_device)\n-        input_ids = tokenizer(articles[1], return_tensors=\"pt\").input_ids.to(torch_device)\n-\n-        output_sequences_batched = model.generate(\n-            input_ids=input_ids_batched, penalty_alpha=0.6, top_k=4, return_dict_in_generate=True, output_scores=True\n-        )\n-        output_sequences = model.generate(\n-            input_ids=input_ids, penalty_alpha=0.6, top_k=4, return_dict_in_generate=True, output_scores=True\n-        )\n-\n-        batched_out = tokenizer.decode(output_sequences_batched.sequences[1], skip_special_tokens=True)\n-        out = tokenizer.decode(output_sequences.sequences[0], skip_special_tokens=True)\n-        self.assertEqual(batched_out, out)\n-\n-        # output_sequences_batched.scores[0][1] -> 1st set of logits, 2nd sequence\n-        max_score_diff = (output_sequences_batched.scores[0][1] - output_sequences.scores[0][0]).abs().max()\n-        self.assertTrue(max_score_diff < 1e-5)\n-\n     def test_logits_processor_not_inplace(self):\n         article = \"Today a dragon flew over Paris.\"\n         model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n@@ -4052,139 +3888,6 @@ def test_init_static_cache_multi_accelerator(self):\n         values_1 = results.past_key_values.layers[1].values\n         self.assertTrue(keys_1.device == values_1.device == torch.device(1))\n \n-    @slow\n-    def test_padding_input_contrastive_search_gpt2(self):\n-        # Load the pre-trained GPT-2 model and tokenizer\n-        model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n-        model.to(torch_device)\n-        tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", clean_up_tokenization_spaces=True)\n-\n-        # Set the tokenizer to left-pad the sequences\n-        tokenizer.padding_side = \"left\"\n-\n-        # Define the PAD token as the EOS token\n-        tokenizer.pad_token = tokenizer.eos_token\n-        model.generation_config.pad_token_id = model.generation_config.eos_token_id\n-\n-        # Define the input prompt\n-        prompt_text = \"The whispered legends of the haunted mansion spoke\"\n-\n-        # Tokenize the input prompt\n-        encoded_prompt = tokenizer(prompt_text, return_tensors=\"pt\", padding=True)\n-        input_ids = encoded_prompt.input_ids.to(torch_device)\n-        attention_mask = encoded_prompt.attention_mask.to(torch_device)\n-\n-        # Define the contrastive search params\n-        penalty_alpha = 0.6\n-        top_k = 4\n-\n-        # Define the padding length to add to the input IDs and attention mask\n-        padding_length = 10\n-\n-        # Generate text without padding\n-        outputs = model.generate(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            do_sample=False,\n-            penalty_alpha=penalty_alpha,\n-            top_k=top_k,\n-            max_new_tokens=64,\n-        )\n-        generated_text_no_padding = tokenizer.decode(outputs[0], skip_special_tokens=True)\n-\n-        # Pad the input IDs and attention mask on the left\n-        padded_input_ids = F.pad(\n-            input_ids, (padding_length, 0), \"constant\", value=model.generation_config.pad_token_id\n-        )\n-        padded_attention_mask = F.pad(attention_mask, (padding_length, 0), \"constant\", value=0)\n-\n-        # Generate text with padded inputs\n-        outputs_with_padding = model.generate(\n-            input_ids=padded_input_ids,\n-            attention_mask=padded_attention_mask,\n-            do_sample=False,\n-            penalty_alpha=penalty_alpha,\n-            top_k=top_k,\n-            max_new_tokens=64,\n-        )\n-        generated_text_with_padding = tokenizer.decode(outputs_with_padding[0], skip_special_tokens=True)\n-\n-        # Assert that the generated texts are identical for padded and non-padded inputs\n-        self.assertEqual(generated_text_no_padding, generated_text_with_padding)\n-        self.assertEqual(\n-            generated_text_with_padding,\n-            'The whispered legends of the haunted mansion spoke of the \"souls of the dead\" who were \"falling '\n-            'out of the sky\" and \"falling into the sea.\"\\n\\nThe ghostly apparitions were said to have been '\n-            'created by the spirits of the dead, who were \"falling out of the sky\" and \"falling into the sea',\n-        )\n-\n-    @slow\n-    def test_padding_input_contrastive_search_t5(self):\n-        # Load the pre-trained T5 model and tokenizer\n-        model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n-        model.to(torch_device)\n-        tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\", clean_up_tokenization_spaces=True)\n-\n-        # Define the input prompt\n-        prompt_text = \"translate English to German: I need to finish this task before the end of the day.\"\n-\n-        # Tokenize the input prompt\n-        encoded_prompt = tokenizer(prompt_text, return_tensors=\"pt\")\n-        input_ids = encoded_prompt.input_ids.to(torch_device)\n-        attention_mask = encoded_prompt.attention_mask.to(torch_device)\n-\n-        # Define the decoder prompt\n-        decoder_prompt_text = \"Ich muss diese Aufgabe\"\n-        encoded_decoder_prompt = tokenizer(decoder_prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n-        decoder_input_ids = encoded_decoder_prompt.input_ids.to(torch_device)\n-        decoder_attention_mask = encoded_decoder_prompt.attention_mask.to(torch_device)\n-\n-        # Define the contrastive search params\n-        penalty_alpha = 0.6\n-        top_k = 4\n-\n-        # Generate text without padding\n-        outputs = model.generate(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            decoder_input_ids=decoder_input_ids,\n-            decoder_attention_mask=decoder_attention_mask,\n-            do_sample=False,\n-            penalty_alpha=penalty_alpha,\n-            top_k=top_k,\n-            max_new_tokens=64,\n-        )\n-        generated_text_no_padding = tokenizer.decode(outputs[0], skip_special_tokens=True)\n-\n-        # Define the padding length to add to the input IDs and attention mask\n-        padding_length = 10\n-\n-        # Pad the decoder input IDs and attention mask on the left\n-        padded_decoder_input_ids = F.pad(\n-            decoder_input_ids, (padding_length, 0), \"constant\", value=model.generation_config.pad_token_id\n-        )\n-        padded_decoder_attention_mask = F.pad(decoder_attention_mask, (padding_length, 0), \"constant\", value=0)\n-        # Since the decoder_start_token_id is the same as the pad_token_id,\n-        # the last padded token represents the decoder start token.\n-        # Set the attention mask for the decoder_start_token_id to True (1).\n-        padded_decoder_attention_mask[:, padding_length - 1] = 1\n-        # Generate text with padded inputs\n-        outputs_with_padding = model.generate(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            decoder_input_ids=padded_decoder_input_ids,\n-            decoder_attention_mask=padded_decoder_attention_mask,\n-            do_sample=False,\n-            penalty_alpha=penalty_alpha,\n-            top_k=top_k,\n-            max_new_tokens=64,\n-        )\n-        generated_text_with_padding = tokenizer.decode(outputs_with_padding[0], skip_special_tokens=True)\n-\n-        # Assert that the generated texts are identical for padded and non-padded inputs\n-        self.assertEqual(generated_text_no_padding, generated_text_with_padding)\n-        self.assertEqual(generated_text_no_padding, \"Ich muss diese Aufgabe vor Ende des Tages beenden.\")\n-\n     def test_prepare_inputs_for_generation_decoder_llm(self):\n         \"\"\"Tests GenerationMixin.prepare_inputs_for_generation against expected usage with decoder-only llms.\"\"\"\n \n@@ -5113,7 +4816,13 @@ def test_generate_custom_cache_position(self):\n                 )\n \n     @pytest.mark.generate\n-    def test_dola_hub_runs(self):\n+    @parameterized.expand(\n+        [\n+            (\"transformers-community/dola\", {\"dola_layers\": \"low\"}),\n+            (\"transformers-community/contrastive-search\", {\"penalty_alpha\": 0.6, \"top_k\": 4}),\n+        ]\n+    )\n+    def test_hub_gen_strategies(self, custom_generate, extra_kwargs):\n         model = AutoModelForCausalLM.from_pretrained(\n             \"hf-internal-testing/tiny-random-MistralForCausalLM\",\n             device_map=torch_device,\n@@ -5123,7 +4832,7 @@ def test_dola_hub_runs(self):\n             \"input_ids\": torch.tensor([[1, 22557, 28725, 1526, 28808]], device=torch_device),\n             \"attention_mask\": torch.tensor([[1, 1, 1, 1, 1]], device=torch_device),\n         }\n-        # Sets dola generation arguments such that:\n+        # Sets generation arguments such that:\n         # a) no EOS is generated, to ensure generation doesn't break early\n         # b) there are at least two forward passes in the main model, to ensure the input preparation of\n         #    the main model is correct\n@@ -5138,13 +4847,13 @@ def test_dola_hub_runs(self):\n             \"output_attentions\": True,\n             \"return_dict_in_generate\": True,\n             \"use_cache\": True,\n-            \"dola_layers\": \"low\",\n             \"trust_remote_code\": True,\n-            \"custom_generate\": \"transformers-community/dola\",\n+            \"custom_generate\": custom_generate,\n         }\n+        generation_kwargs.update(extra_kwargs)\n         torch.manual_seed(0)\n-        output_dola = model.generate(**generation_kwargs, **model_inputs)\n-        self.assertEqual(output_dola.sequences.shape, (1, 9))\n+        output = model.generate(**generation_kwargs, **model_inputs)\n+        self.assertEqual(output.sequences.shape, (1, 9))\n \n \n @require_torch"
        },
        {
            "sha": "9d887895b94148997477e66b8525f56c8b1a3fdc",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -1205,6 +1205,7 @@ def test_cnn_summarization_same_as_fairseq(self):\n         generated_summaries = tok.batch_decode(hypotheses_batch.tolist())\n         assert generated_summaries == EXPECTED\n \n+    # TODO joao, manuel: remove this in v4.62.0\n     @slow\n     def test_contrastive_search_bart(self):\n         article = (\n@@ -1238,7 +1239,15 @@ def test_contrastive_search_bart(self):\n             article, add_special_tokens=False, truncation=True, max_length=512, return_tensors=\"pt\"\n         ).input_ids.to(torch_device)\n \n-        outputs = bart_model.generate(input_ids, penalty_alpha=0.5, top_k=5, max_length=64, num_beams=1)\n+        outputs = bart_model.generate(\n+            input_ids,\n+            penalty_alpha=0.5,\n+            top_k=5,\n+            max_length=64,\n+            num_beams=1,\n+            trust_remote_code=True,\n+            custom_generate=\"transformers-community/contrastive-search\",\n+        )\n         generated_text = bart_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n \n         self.assertListEqual("
        },
        {
            "sha": "f81685abd0919d0d035c16a1f9ca9ca6b54ce7bc",
            "filename": "tests/models/csm/test_modeling_csm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -292,21 +292,6 @@ def test_constrained_beam_search_generate(self):\n     def test_constrained_beam_search_generate_dict_output(self):\n         pass\n \n-    @pytest.mark.generate\n-    @unittest.skip(reason=\"CSM does not support contrastive search.\")\n-    def test_contrastive_generate(self):\n-        pass\n-\n-    @pytest.mark.generate\n-    @unittest.skip(reason=\"CSM does not support contrastive search.\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @pytest.mark.generate\n-    @unittest.skip(reason=\"CSM does not support contrastive search.\")\n-    def test_contrastive_generate_low_memory(self):\n-        pass\n-\n     @pytest.mark.generate\n     @unittest.skip(reason=\"CSM does not support prompt lookup decoding.\")\n     def test_prompt_lookup_decoding_matches_greedy_search(self):"
        },
        {
            "sha": "e15d6d012ebbeceae0decc0fe786d6e8bb4da921",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -519,6 +519,7 @@ def test_model_2b_bf16_dola(self):\n             dola_layers=\"low\",\n             repetition_penalty=1.2,\n             trust_remote_code=True,\n+            custom_generate=\"transformers-community/dola\",\n         )\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n         self.assertEqual(output_text, EXPECTED_TEXTS)"
        },
        {
            "sha": "072bbd081643018712f06550e54de3e744a4041c",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -837,6 +837,7 @@ def test_gpt2_sample(self):\n             all(output_seq_strs[idx] != output_seq_tt_strs[idx] for idx in range(len(output_seq_tt_strs)))\n         )  # token_type_ids should change output\n \n+    # TODO joao, manuel: remove this in v4.62.0\n     @slow\n     def test_contrastive_search_gpt2(self):\n         article = (\n@@ -848,7 +849,14 @@ def test_contrastive_search_gpt2(self):\n         gpt2_model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2-large\").to(torch_device)\n         input_ids = gpt2_tokenizer(article, return_tensors=\"pt\").input_ids.to(torch_device)\n \n-        outputs = gpt2_model.generate(input_ids, penalty_alpha=0.6, top_k=4, max_length=256)\n+        outputs = gpt2_model.generate(\n+            input_ids,\n+            penalty_alpha=0.6,\n+            top_k=4,\n+            max_length=256,\n+            trust_remote_code=True,\n+            custom_generate=\"transformers-community/contrastive-search\",\n+        )\n \n         generated_text = gpt2_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n "
        },
        {
            "sha": "b24f47c32bcadc0147810c48a6b60bdb332443f0",
            "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -424,14 +424,6 @@ def test_config(self):\n     def test_retain_grad_hidden_states_attentions(self):\n         pass\n \n-    @unittest.skip(reason=\"Contrastive search not supported due to non-standard caching mechanism\")\n-    def test_contrastive_generate(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Contrastive search not supported due to non-standard caching mechanism\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n     @unittest.skip(reason=\"CPU offload seems to be broken for some reason - tiny models keep hitting corner cases\")\n     def test_cpu_offload(self):\n         pass"
        },
        {
            "sha": "073660b49cf091db361fd214979f074018050cfb",
            "filename": "tests/models/gptj/test_modeling_gptj.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -541,6 +541,7 @@ def test_gptj_sample(self):\n             all(output_seq_strs[idx] != output_seq_tt_strs[idx] for idx in range(len(output_seq_tt_strs)))\n         )  # token_type_ids should change output\n \n+    # TODO joao, manuel: remove this in v4.62.0\n     @tooslow\n     def test_contrastive_search_gptj(self):\n         article = (\n@@ -554,7 +555,14 @@ def test_contrastive_search_gptj(self):\n         )\n         input_ids = tokenizer(article, return_tensors=\"pt\").input_ids.to(torch_device)\n \n-        outputs = model.generate(input_ids, penalty_alpha=0.6, top_k=4, max_length=256)\n+        outputs = model.generate(\n+            input_ids,\n+            penalty_alpha=0.6,\n+            top_k=4,\n+            max_length=256,\n+            trust_remote_code=True,\n+            custom_generate=\"transformers-community/contrastive-search\",\n+        )\n         generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n \n         self.assertListEqual("
        },
        {
            "sha": "454b38975cdd6b0d2cd95b611718163032ce795d",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -844,18 +844,6 @@ def _check_attentions_for_generate(\n         \"\"\"\n         pass\n \n-    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n-    def test_contrastive_generate(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n-    def test_contrastive_generate_low_memory(self):\n-        pass\n-\n     @unittest.skip(reason=\"We only test the model that takes in multiple images\")\n     def test_custom_4d_attention_mask(self):\n         pass"
        },
        {
            "sha": "199664a73d85e83a5d96fdb1e0a7499d613c6a4b",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -390,18 +390,6 @@ def test_flash_attn_2_generate_padding_right(self):\n     def test_flash_attn_2_inference_padding_right(self):\n         pass\n \n-    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n-    def test_contrastive_generate(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n-    def test_contrastive_generate_low_memory(self):\n-        pass\n-\n     @unittest.skip(\n         reason=\"Prompt lookup decoding needs a way to indicate `bad_word_ids` that should not be suggested as candidates\"\n     )"
        },
        {
            "sha": "97cff53643bc5fc1a612d76507dc080b2c3adc2a",
            "filename": "tests/models/idefics3/test_modeling_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -351,18 +351,6 @@ def test_inputs_embeds():\n     def test_flash_attn_2_inference_padding_right(self):\n         pass\n \n-    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n-    def test_contrastive_generate(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n-    def test_contrastive_generate_low_memory(self):\n-        pass\n-\n     @unittest.skip(\n         reason=\"Prompt lookup decoding needs a way to indicate `bad_word_ids` that should not be suggested as candidates\"\n     )"
        },
        {
            "sha": "62ee1be2dbe650cebfe79df1aabdd147127d6a77",
            "filename": "tests/models/kosmos2_5/test_modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -585,14 +585,6 @@ def test_flash_attn_2_generate_reuse_cache(self):\n     def test_generate_from_inputs_embeds(self):\n         pass\n \n-    # TODO: ydshieh\n-    @pytest.mark.generate\n-    @unittest.skip(\n-        \"Kosmos2_5ForConditionalGeneration returns `vision_model_output` which is currently not working with `stack_model_outputs`\",\n-    )\n-    def test_beam_search_low_memory(self):\n-        pass\n-\n     @pytest.mark.generate\n     def test_left_padding_compatibility(self):\n         # Overwrite because Kosmos-2.5 need to padd pixel values and pad image-attn-mask"
        },
        {
            "sha": "52d4b4d6fce19ed17fd55b7e5bc7b248a3e8a9e8",
            "filename": "tests/models/lfm2/test_modeling_lfm2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -75,18 +75,6 @@ def test_attention_outputs(self):\n     def test_past_key_values_format(self):\n         pass\n \n-    @unittest.skip(\"Lfm2 has a special cache format which is not compatible with contrastive search\")\n-    def test_contrastive_generate(self):\n-        pass\n-\n-    @unittest.skip(\"Lfm2 has a special cache format which is not compatible with contrastive search\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Lfm2 has a special cache format which is not compatible with contrastive search\")\n-    def test_contrastive_generate_low_memory(self):\n-        pass\n-\n     @unittest.skip(\n         \"Lfm2 has a special cache format which is not compatible with compile as it has static address for conv cache\"\n     )"
        },
        {
            "sha": "9217510fb0b0f078bbc44dfea4a99553b498a2cd",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -249,7 +249,14 @@ def test_model_7b_dola_generation(self):\n \n         # greedy generation outputs\n         generated_ids = model.generate(\n-            **model_inputs, max_new_tokens=64, top_p=None, temperature=1, do_sample=False, dola_layers=\"low\"\n+            **model_inputs,\n+            max_new_tokens=64,\n+            top_p=None,\n+            temperature=1,\n+            do_sample=False,\n+            dola_layers=\"low\",\n+            trust_remote_code=True,\n+            custom_generate=\"transformers-community/dola\",\n         )\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)"
        },
        {
            "sha": "8380b49c3735ff030afeb2037aaa2ec7397fb8db",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -192,6 +192,7 @@ def test_model_7b_dola_generation(self):\n             dola_layers=\"low\",\n             repetition_penalty=1.2,\n             trust_remote_code=True,\n+            custom_generate=\"transformers-community/dola\",\n         )\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)"
        },
        {
            "sha": "331d1aba498be655da6d7fa072586618ab7a99d2",
            "filename": "tests/models/opt/test_modeling_opt.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -543,6 +543,7 @@ def test_batched_nan_fp16(self):\n                 torch.isnan(outputs.logits[0]).any().item()\n             )  # the first logits could contain NaNs if it fails\n \n+    # TODO joao, manuel: remove this in v4.62.0\n     @slow\n     def test_contrastive_search_opt(self):\n         article = (\n@@ -555,7 +556,14 @@ def test_contrastive_search_opt(self):\n         opt_model = OPTForCausalLM.from_pretrained(\"facebook/opt-1.3b\").to(torch_device)\n         input_ids = opt_tokenizer(article, return_tensors=\"pt\").input_ids.to(torch_device)\n \n-        outputs = opt_model.generate(input_ids, penalty_alpha=0.6, top_k=5, max_length=256)\n+        outputs = opt_model.generate(\n+            input_ids,\n+            penalty_alpha=0.6,\n+            top_k=5,\n+            max_length=256,\n+            trust_remote_code=True,\n+            custom_generate=\"transformers-community/contrastive-search\",\n+        )\n         generated_text = opt_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n \n         self.assertListEqual("
        },
        {
            "sha": "ad345e70e03e4e23752b68cd61bf0969889d45bd",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -273,10 +273,6 @@ def test_feed_forward_chunking(self):\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n-    @unittest.skip(\"Low memory will be removed soon so no need to fix it\")\n-    def test_beam_search_low_memory(self):\n-        pass\n-\n     @parameterized.expand([(\"random\",), (\"same\",)])\n     @pytest.mark.generate\n     @unittest.skip(\"Paligemma2 does not seem to be compatible with assisted decoding\")"
        },
        {
            "sha": "45aec1da4ba9810c2eb28911725484d2a4da01b7",
            "filename": "tests/models/smolvlm/test_modeling_smolvlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -345,18 +345,6 @@ def setUp(self):\n     def test_flash_attn_2_inference_padding_right(self):\n         pass\n \n-    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n-    def test_contrastive_generate(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n-    def test_contrastive_generate_low_memory(self):\n-        pass\n-\n     @unittest.skip(\n         reason=\"Prompt lookup decoding needs a way to indicate `bad_word_ids` that should not be suggested as candidates\"\n     )"
        },
        {
            "sha": "9de1467fa0618a0854df63eef3a3fa99f2cc3f6e",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -1569,6 +1569,7 @@ def test_translation_en_to_ro(self):\n         translation = tok.decode(output[0])\n         self.assertEqual(translation, expected_translation)\n \n+    # TODO joao, manuel: remove this in v4.62.0\n     @slow\n     def test_contrastive_search_t5(self):\n         article = (\n@@ -1603,7 +1604,14 @@ def test_contrastive_search_t5(self):\n             article, add_special_tokens=False, truncation=True, max_length=512, return_tensors=\"pt\"\n         ).input_ids.to(torch_device)\n \n-        outputs = t5_model.generate(input_ids, penalty_alpha=0.5, top_k=5, max_length=64)\n+        outputs = t5_model.generate(\n+            input_ids,\n+            penalty_alpha=0.5,\n+            top_k=5,\n+            max_length=64,\n+            trust_remote_code=True,\n+            custom_generate=\"transformers-community/contrastive-search\",\n+        )\n         generated_text = t5_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n \n         # TODO: @arthur?"
        }
    ],
    "stats": {
        "total": 1216,
        "additions": 107,
        "deletions": 1109
    }
}