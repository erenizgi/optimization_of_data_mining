{
    "author": "MekkCyber",
    "message": "[Quantization] per tensor quantization kernel (#42560)\n\n* fix\n\n* style\n\n* initial\n\n* fix\n\n* comment\n\n* style\n\n* fix",
    "sha": "51c5a7a658b0d56f418d491c4a77073e113dce12",
    "files": [
        {
            "sha": "35f725f9b69635dbe1261fbf3a07b97a38173d01",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 151,
            "deletions": 47,
            "changes": 198,
            "blob_url": "https://github.com/huggingface/transformers/blob/51c5a7a658b0d56f418d491c4a77073e113dce12/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51c5a7a658b0d56f418d491c4a77073e113dce12/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=51c5a7a658b0d56f418d491c4a77073e113dce12",
            "patch": "@@ -156,6 +156,79 @@ def _w8a8_block_fp8_matmul(\n     tl.store(c_ptrs, c, mask=c_mask)\n \n \n+@triton.jit\n+def _w8a8_block_fp8_matmul_per_tensor(\n+    # Pointers to inputs and output\n+    A,\n+    B,\n+    C,\n+    As,\n+    Bs,\n+    # Shape for matmul\n+    M,\n+    N,\n+    K,\n+    # Block size for block-wise quantization\n+    group_n,\n+    group_k,\n+    # Stride for inputs and output\n+    stride_am,\n+    stride_ak,\n+    stride_bk,\n+    stride_bn,\n+    stride_cm,\n+    stride_cn,\n+    # Meta-parameters\n+    BLOCK_SIZE_M: tl.constexpr,\n+    BLOCK_SIZE_N: tl.constexpr,\n+    BLOCK_SIZE_K: tl.constexpr,\n+    GROUP_SIZE_M: tl.constexpr,\n+):\n+    \"\"\"Triton-accelerated function used to perform linear operations (dot\n+    product) on input tensors `A` and `B` with per-tensor quantization, and\n+    store the result in output tensor `C`.\n+    \"\"\"\n+\n+    pid = tl.program_id(axis=0)\n+    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n+    group_id = pid // num_pid_in_group\n+    first_pid_m = group_id * GROUP_SIZE_M\n+    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n+    pid_m = first_pid_m + (pid % group_size_m)\n+    pid_n = (pid % num_pid_in_group) // group_size_m\n+\n+    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n+    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n+    offs_k = tl.arange(0, BLOCK_SIZE_K)\n+    a_ptrs = A + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+    b_ptrs = B + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+    scale_a = tl.load(As)\n+    scale_b = tl.load(Bs)\n+    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n+        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n+        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n+\n+        accumulator += tl.dot(a, b) * scale_a * scale_b\n+        a_ptrs += BLOCK_SIZE_K * stride_ak\n+        b_ptrs += BLOCK_SIZE_K * stride_bk\n+\n+    if C.dtype.element_ty == tl.bfloat16:\n+        c = accumulator.to(tl.bfloat16)\n+    elif C.dtype.element_ty == tl.float16:\n+        c = accumulator.to(tl.float16)\n+    else:\n+        c = accumulator.to(tl.float32)\n+\n+    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+    c_ptrs = C + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n+    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n+    tl.store(c_ptrs, c, mask=c_mask)\n+\n+\n def w8a8_block_fp8_matmul_triton(\n     A: torch.Tensor,\n     B: torch.Tensor,\n@@ -179,19 +252,31 @@ def w8a8_block_fp8_matmul_triton(\n     Returns:\n         torch.Tensor: The result of matmul.\n     \"\"\"\n-    assert len(block_size) == 2\n-    block_n, block_k = block_size[0], block_size[1]\n+    if block_size is None:\n+        block_n, block_k = 128, 128\n+    else:\n+        assert len(block_size) == 2\n+        block_n, block_k = block_size[0], block_size[1]\n+\n+    # if we have per-tensor quantization, we use 128x128 block size for tiled matmul multiplication\n+    if block_n == B.shape[-2] and block_k == B.shape[-1]:\n+        block_n = 128\n+        block_k = 128\n \n     assert A.shape[-1] == B.shape[-1]\n \n-    assert A.shape[:-1] == As.shape[:-1] and A.is_contiguous()\n-    assert triton.cdiv(A.shape[-1], block_k) == As.shape[-1]\n+    if As.numel() != 1:\n+        assert A.shape[:-1] == As.shape[:-1] and A.is_contiguous()\n+        assert triton.cdiv(A.shape[-1], block_k) == As.shape[-1]\n+\n     M = A.numel() // A.shape[-1]\n \n-    assert B.ndim == 2 and B.is_contiguous() and Bs.ndim == 2\n     N, K = B.shape\n-    assert triton.cdiv(N, block_n) == Bs.shape[0], f\"{N}, {block_n}, {Bs.shape}\"\n-    assert triton.cdiv(K, block_k) == Bs.shape[1], f\"{K}, {block_k}, {Bs.shape}\"\n+    assert B.ndim == 2 and B.is_contiguous()\n+    if Bs.numel() != 1:\n+        assert Bs.ndim == 2\n+        assert triton.cdiv(N, block_n) == Bs.shape[0], f\"{N}, {block_n}, {Bs.shape}\"\n+        assert triton.cdiv(K, block_k) == Bs.shape[1], f\"{K}, {block_k}, {Bs.shape}\"\n \n     C_shape = A.shape[:-1] + (N,)\n     C = A.new_empty(C_shape, dtype=output_dtype)\n@@ -207,32 +292,56 @@ def w8a8_block_fp8_matmul_triton(\n     def grid(META):\n         return (triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"]),)\n \n-    _w8a8_block_fp8_matmul[grid](\n-        A,\n-        B,\n-        C,\n-        As,\n-        Bs,\n-        M,\n-        N,\n-        K,\n-        block_n,\n-        block_k,\n-        A.stride(-2),\n-        A.stride(-1),\n-        B.stride(1),\n-        B.stride(0),\n-        C.stride(-2),\n-        C.stride(-1),\n-        As.stride(-2),\n-        As.stride(-1),\n-        Bs.stride(1),\n-        Bs.stride(0),\n-        BLOCK_SIZE_M=BLOCK_SIZE_M,\n-        BLOCK_SIZE_N=BLOCK_SIZE_N,\n-        BLOCK_SIZE_K=BLOCK_SIZE_K,\n-        GROUP_SIZE_M=8,\n-    )\n+    if As.numel() == 1 and Bs.numel() == 1:\n+        _w8a8_block_fp8_matmul_per_tensor[grid](\n+            A,\n+            B,\n+            C,\n+            As,\n+            Bs,\n+            M,\n+            N,\n+            K,\n+            block_n,\n+            block_k,\n+            A.stride(-2),\n+            A.stride(-1),\n+            B.stride(1),\n+            B.stride(0),\n+            C.stride(-2),\n+            C.stride(-1),\n+            BLOCK_SIZE_M=BLOCK_SIZE_M,\n+            BLOCK_SIZE_N=BLOCK_SIZE_N,\n+            BLOCK_SIZE_K=BLOCK_SIZE_K,\n+            GROUP_SIZE_M=8,\n+        )\n+    else:\n+        _w8a8_block_fp8_matmul[grid](\n+            A,\n+            B,\n+            C,\n+            As,\n+            Bs,\n+            M,\n+            N,\n+            K,\n+            block_n,\n+            block_k,\n+            A.stride(-2),\n+            A.stride(-1),\n+            B.stride(1),\n+            B.stride(0),\n+            C.stride(-2),\n+            C.stride(-1),\n+            As.stride(-2),\n+            As.stride(-1),\n+            Bs.stride(1),\n+            Bs.stride(0),\n+            BLOCK_SIZE_M=BLOCK_SIZE_M,\n+            BLOCK_SIZE_N=BLOCK_SIZE_N,\n+            BLOCK_SIZE_K=BLOCK_SIZE_K,\n+            GROUP_SIZE_M=8,\n+        )\n \n     return C\n \n@@ -356,23 +465,18 @@ def forward(self, input: torch.Tensor) -> torch.Tensor:\n                 if self.activation_scheme == \"dynamic\":\n                     qinput, scale = act_quant(input, self.block_size[1])\n                 elif self.activation_scheme == \"static\":\n-                    scale = self.activation_scale\n+                    scale = self.activation_scale.to(torch.float32)\n                     qinput = (input / scale).to(torch.float8_e4m3fn)\n                 else:\n                     raise NotImplementedError(\"Not supported\")\n-                # TODO: fix this later to use the triton kernel\n-                if self.activation_scheme == \"static\":\n-                    output = F.linear(qinput.to(torch.bfloat16), weight.to(torch.bfloat16), None) * scale_inv * scale\n-                    output = output.to(input.dtype)\n-                else:\n-                    output = w8a8_block_fp8_matmul_triton(\n-                        qinput,\n-                        weight,\n-                        scale,\n-                        scale_inv,\n-                        self.block_size,\n-                        output_dtype=input.dtype,\n-                    )\n+                output = w8a8_block_fp8_matmul_triton(\n+                    qinput,\n+                    weight,\n+                    scale,\n+                    scale_inv,\n+                    self.block_size,\n+                    output_dtype=input.dtype,\n+                )\n \n             # Blocks the CPU until all accelerator operations on the specified device are complete. It is used to ensure that the results of the\n             # preceding operations are ready before proceeding"
        }
    ],
    "stats": {
        "total": 198,
        "additions": 151,
        "deletions": 47
    }
}