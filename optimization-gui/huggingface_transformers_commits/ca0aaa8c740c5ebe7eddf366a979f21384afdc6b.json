{
    "author": "emapco",
    "message": "[`fix`] Pass adamw optimizer parameters to StableAdamW (#40184)\n\n* fix: pass adamw optimizer parameters to StableAdamW\n\n* add test for stable_adamw initialization with trainer arguments\n\n* address copilot suggestion\n\n* fix: update weight_decay handling in stable_adamw kwargs\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "ca0aaa8c740c5ebe7eddf366a979f21384afdc6b",
    "files": [
        {
            "sha": "23777e8866704b864c203362388522dfd2c9d284",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca0aaa8c740c5ebe7eddf366a979f21384afdc6b/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca0aaa8c740c5ebe7eddf366a979f21384afdc6b/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=ca0aaa8c740c5ebe7eddf366a979f21384afdc6b",
            "patch": "@@ -1819,6 +1819,7 @@ def optimizer_hook(param):\n             if kahan_sum is not None:\n                 kahan_sum = bool(kahan_sum)\n \n+            adam_kwargs[\"weight_decay\"] = args.weight_decay\n             stable_adamw_kwargs = {\n                 \"decouple_lr\": bool(optim_args.pop(\"decouple_lr\", False)),\n                 \"max_lr\": max_lr,"
        },
        {
            "sha": "1884b5e93e066edc9f4d6b2bd69c29da0457cbc2",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca0aaa8c740c5ebe7eddf366a979f21384afdc6b/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca0aaa8c740c5ebe7eddf366a979f21384afdc6b/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=ca0aaa8c740c5ebe7eddf366a979f21384afdc6b",
            "patch": "@@ -2569,6 +2569,38 @@ def test_stable_adamw_lr_display_without_scheduler(self):\n         # reflects displayed lr in trainer\n         self.assertEqual(trainer.get_learning_rates(), [learning_rate, learning_rate])\n \n+    @require_torch_optimi\n+    @require_torch_accelerator\n+    def test_stable_adamw_trainer_adamw_args(self):\n+        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+        tiny_llama = LlamaForCausalLM(config)\n+        x = torch.randint(0, 100, (128,))\n+        train_dataset = RepeatDataset(x)\n+\n+        learning_rate = 1e-9\n+        num_steps = 10\n+\n+        # Trainer without inf/nan filter\n+        args = TrainingArguments(\n+            self.get_auto_remove_tmp_dir(),\n+            learning_rate=learning_rate,\n+            logging_steps=5,\n+            weight_decay=0.001,\n+            adam_beta1=0.89,\n+            adam_beta2=0.98,\n+            adam_epsilon=1e-8,\n+            optim=\"stable_adamw\",\n+            optim_target_modules=[r\".*attn.*\", r\".*mlp.*\"],\n+        )\n+        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+        trainer.create_optimizer_and_scheduler(num_training_steps=num_steps)\n+\n+        # check StableAdamW optimizer is created with the correct parameters\n+        self.assertEqual(trainer.optimizer.defaults[\"beta1\"], args.adam_beta1)\n+        self.assertEqual(trainer.optimizer.defaults[\"beta2\"], args.adam_beta2)\n+        self.assertEqual(trainer.optimizer.defaults[\"eps\"], args.adam_epsilon)\n+        self.assertEqual(trainer.optimizer.defaults[\"weight_decay\"], args.weight_decay)\n+\n     @require_torch_optimi\n     @require_torch_accelerator\n     def test_stable_adamw_lr_display_with_scheduler(self):"
        }
    ],
    "stats": {
        "total": 33,
        "additions": 33,
        "deletions": 0
    }
}