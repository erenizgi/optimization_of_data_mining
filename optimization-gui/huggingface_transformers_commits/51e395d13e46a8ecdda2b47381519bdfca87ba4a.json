{
    "author": "Cyrilvallez",
    "message": "Fix FA2 attention for models supporting sliding window (#34093)\n\nFix FA2",
    "sha": "51e395d13e46a8ecdda2b47381519bdfca87ba4a",
    "files": [
        {
            "sha": "737be17cfc1694b5d57469761c7212a9aa6cbc40",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=51e395d13e46a8ecdda2b47381519bdfca87ba4a",
            "patch": "@@ -417,34 +417,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = cache_position[-1]\n-\n         if past_key_value is not None:\n-            # Activate slicing cache only if the config has a value `sliding_windows` attribute\n-            cache_has_contents = cache_position[0] > 0\n-            if (\n-                getattr(self.config, \"sliding_window\", None) is not None\n-                and kv_seq_len > self.config.sliding_window\n-                and cache_has_contents\n-            ):\n-                slicing_tokens = 1 - self.config.sliding_window\n-\n-                past_key = past_key_value[self.layer_idx][0]\n-                past_value = past_key_value[self.layer_idx][1]\n-\n-                past_key = past_key[:, :, slicing_tokens:, :].contiguous()\n-                past_value = past_value[:, :, slicing_tokens:, :].contiguous()\n-\n-                if past_key.shape[-2] != self.config.sliding_window - 1:\n-                    raise ValueError(\n-                        f\"past key must have a shape of (`batch_size, num_heads, self.config.sliding_window-1, head_dim`), got\"\n-                        f\" {past_key.shape}\"\n-                    )\n-\n-                if attention_mask is not None:\n-                    attention_mask = attention_mask[:, slicing_tokens:]\n-                    attention_mask = torch.cat([attention_mask, torch.ones_like(attention_mask[:, -1:])], dim=-1)\n-\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx)\n \n         # repeat k/v heads if n_kv_heads < n_heads"
        },
        {
            "sha": "ef225e1598823715dda9e954791c7d084563875d",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=51e395d13e46a8ecdda2b47381519bdfca87ba4a",
            "patch": "@@ -320,31 +320,6 @@ def forward(\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n-            # Activate slicing cache only if the config has a value `sliding_windows` attribute\n-            cache_has_contents = past_key_value.get_seq_length(self.layer_idx) > 0\n-            if (\n-                getattr(self.config, \"sliding_window\", None) is not None\n-                and kv_seq_len > self.config.sliding_window\n-                and cache_has_contents\n-            ):\n-                slicing_tokens = 1 - self.config.sliding_window\n-\n-                past_key = past_key_value[self.layer_idx][0]\n-                past_value = past_key_value[self.layer_idx][1]\n-\n-                past_key = past_key[:, :, slicing_tokens:, :].contiguous()\n-                past_value = past_value[:, :, slicing_tokens:, :].contiguous()\n-\n-                if past_key.shape[-2] != self.config.sliding_window - 1:\n-                    raise ValueError(\n-                        f\"past key must have a shape of (`batch_size, num_heads, self.config.sliding_window-1, head_dim`), got\"\n-                        f\" {past_key.shape}\"\n-                    )\n-\n-                if attention_mask is not None:\n-                    attention_mask = attention_mask[:, slicing_tokens:]\n-                    attention_mask = torch.cat([attention_mask, torch.ones_like(attention_mask[:, -1:])], dim=-1)\n-\n             cache_kwargs = {\"sin\": sin, \"cos\": cos}  # Specific to RoPE models\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n "
        },
        {
            "sha": "3ff851b45ea1611c6150045af49d99d4eae08043",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=51e395d13e46a8ecdda2b47381519bdfca87ba4a",
            "patch": "@@ -431,31 +431,6 @@ def forward(\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n \n         if past_key_value is not None:\n-            # Activate slicing cache only if the config has a value `sliding_windows` attribute\n-            cache_has_contents = past_key_value.get_seq_length(self.layer_idx) > 0\n-            if (\n-                getattr(self.config, \"sliding_window\", None) is not None\n-                and kv_seq_len > self.config.sliding_window\n-                and cache_has_contents\n-            ):\n-                slicing_tokens = 1 - self.config.sliding_window\n-\n-                past_key = past_key_value[self.layer_idx][0]\n-                past_value = past_key_value[self.layer_idx][1]\n-\n-                past_key = past_key[:, :, slicing_tokens:, :].contiguous()\n-                past_value = past_value[:, :, slicing_tokens:, :].contiguous()\n-\n-                if past_key.shape[-2] != self.config.sliding_window - 1:\n-                    raise ValueError(\n-                        f\"past key must have a shape of (`batch_size, num_heads, self.config.sliding_window-1, head_dim`), got\"\n-                        f\" {past_key.shape}\"\n-                    )\n-\n-                if attention_mask is not None:\n-                    attention_mask = attention_mask[:, slicing_tokens:]\n-                    attention_mask = torch.cat([attention_mask, torch.ones_like(attention_mask[:, -1:])], dim=-1)\n-\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n "
        },
        {
            "sha": "16601e1f9957d52e483f5b4408db1306c0fcfff6",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=51e395d13e46a8ecdda2b47381519bdfca87ba4a",
            "patch": "@@ -492,31 +492,6 @@ def forward(\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n \n         if past_key_value is not None:\n-            # Activate slicing cache only if the config has a value `sliding_windows` attribute\n-            cache_has_contents = past_key_value.get_seq_length(self.layer_idx) > 0\n-            if (\n-                getattr(self.config, \"sliding_window\", None) is not None\n-                and kv_seq_len > self.config.sliding_window\n-                and cache_has_contents\n-            ):\n-                slicing_tokens = 1 - self.config.sliding_window\n-\n-                past_key = past_key_value[self.layer_idx][0]\n-                past_value = past_key_value[self.layer_idx][1]\n-\n-                past_key = past_key[:, :, slicing_tokens:, :].contiguous()\n-                past_value = past_value[:, :, slicing_tokens:, :].contiguous()\n-\n-                if past_key.shape[-2] != self.config.sliding_window - 1:\n-                    raise ValueError(\n-                        f\"past key must have a shape of (`batch_size, num_heads, self.config.sliding_window-1, head_dim`), got\"\n-                        f\" {past_key.shape}\"\n-                    )\n-\n-                if attention_mask is not None:\n-                    attention_mask = attention_mask[:, slicing_tokens:]\n-                    attention_mask = torch.cat([attention_mask, torch.ones_like(attention_mask[:, -1:])], dim=-1)\n-\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n "
        },
        {
            "sha": "559daeca694dbefec7163ab56373999005372a97",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=51e395d13e46a8ecdda2b47381519bdfca87ba4a",
            "patch": "@@ -369,31 +369,6 @@ def forward(\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n \n         if past_key_value is not None:\n-            # Activate slicing cache only if the config has a value `sliding_windows` attribute\n-            cache_has_contents = past_key_value.get_seq_length(self.layer_idx) > 0\n-            if (\n-                getattr(self.config, \"sliding_window\", None) is not None\n-                and kv_seq_len > self.config.sliding_window\n-                and cache_has_contents\n-            ):\n-                slicing_tokens = 1 - self.config.sliding_window\n-\n-                past_key = past_key_value[self.layer_idx][0]\n-                past_value = past_key_value[self.layer_idx][1]\n-\n-                past_key = past_key[:, :, slicing_tokens:, :].contiguous()\n-                past_value = past_value[:, :, slicing_tokens:, :].contiguous()\n-\n-                if past_key.shape[-2] != self.config.sliding_window - 1:\n-                    raise ValueError(\n-                        f\"past key must have a shape of (`batch_size, num_heads, self.config.sliding_window-1, head_dim`), got\"\n-                        f\" {past_key.shape}\"\n-                    )\n-\n-                if attention_mask is not None:\n-                    attention_mask = attention_mask[:, slicing_tokens:]\n-                    attention_mask = torch.cat([attention_mask, torch.ones_like(attention_mask[:, -1:])], dim=-1)\n-\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n "
        },
        {
            "sha": "8bd552e66ecbe1c1e4a6a950230c50eaf70adc9f",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=51e395d13e46a8ecdda2b47381519bdfca87ba4a",
            "patch": "@@ -394,32 +394,6 @@ def forward(\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n-            # Activate slicing cache only if the config has a value `sliding_windows` attribute\n-            cache_has_contents = past_key_value.get_seq_length(self.layer_idx) > 0\n-            kv_seq_len = key_states.shape[-2] + cache_position[0]\n-            if (\n-                getattr(self.config, \"sliding_window\", None) is not None\n-                and kv_seq_len > self.config.sliding_window\n-                and cache_has_contents\n-            ):\n-                slicing_tokens = 1 - self.config.sliding_window\n-\n-                past_key = past_key_value[self.layer_idx][0]\n-                past_value = past_key_value[self.layer_idx][1]\n-\n-                past_key = past_key[:, :, slicing_tokens:, :].contiguous()\n-                past_value = past_value[:, :, slicing_tokens:, :].contiguous()\n-\n-                if past_key.shape[-2] != self.config.sliding_window - 1:\n-                    raise ValueError(\n-                        f\"past key must have a shape of (`batch_size, num_heads, self.config.sliding_window-1, head_dim`), got\"\n-                        f\" {past_key.shape}\"\n-                    )\n-\n-                if attention_mask is not None:\n-                    attention_mask = attention_mask[:, slicing_tokens:]\n-                    attention_mask = torch.cat([attention_mask, torch.ones_like(attention_mask[:, -1:])], dim=-1)\n-\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n "
        },
        {
            "sha": "60cd5e4722857e20f3c1e3169c17cfb1cdb3c2df",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=51e395d13e46a8ecdda2b47381519bdfca87ba4a",
            "patch": "@@ -481,32 +481,6 @@ def forward(\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n-            # Activate slicing cache only if the config has a value `sliding_windows` attribute\n-            cache_has_contents = past_key_value.get_seq_length(self.layer_idx) > 0\n-            kv_seq_len = key_states.shape[-2] + cache_position[0]\n-            if (\n-                getattr(self.config, \"sliding_window\", None) is not None\n-                and kv_seq_len > self.config.sliding_window\n-                and cache_has_contents\n-            ):\n-                slicing_tokens = 1 - self.config.sliding_window\n-\n-                past_key = past_key_value[self.layer_idx][0]\n-                past_value = past_key_value[self.layer_idx][1]\n-\n-                past_key = past_key[:, :, slicing_tokens:, :].contiguous()\n-                past_value = past_value[:, :, slicing_tokens:, :].contiguous()\n-\n-                if past_key.shape[-2] != self.config.sliding_window - 1:\n-                    raise ValueError(\n-                        f\"past key must have a shape of (`batch_size, num_heads, self.config.sliding_window-1, head_dim`), got\"\n-                        f\" {past_key.shape}\"\n-                    )\n-\n-                if attention_mask is not None:\n-                    attention_mask = attention_mask[:, slicing_tokens:]\n-                    attention_mask = torch.cat([attention_mask, torch.ones_like(attention_mask[:, -1:])], dim=-1)\n-\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n "
        },
        {
            "sha": "4e9401c77e4d7d123ec91f8e5ade8eac68116abd",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=51e395d13e46a8ecdda2b47381519bdfca87ba4a",
            "patch": "@@ -673,31 +673,6 @@ def forward(\n         )\n \n         if past_key_value is not None:\n-            # Activate slicing cache only if the config has a value `sliding_windows` attribute\n-            cache_has_contents = past_key_value.get_seq_length(self.layer_idx) > 0\n-            if (\n-                getattr(self.config, \"sliding_window\", None) is not None\n-                and kv_seq_len > self.config.sliding_window\n-                and cache_has_contents\n-            ):\n-                slicing_tokens = 1 - self.config.sliding_window\n-\n-                past_key = past_key_value[self.layer_idx][0]\n-                past_value = past_key_value[self.layer_idx][1]\n-\n-                past_key = past_key[:, :, slicing_tokens:, :].contiguous()\n-                past_value = past_value[:, :, slicing_tokens:, :].contiguous()\n-\n-                if past_key.shape[-2] != self.config.sliding_window - 1:\n-                    raise ValueError(\n-                        f\"past key must have a shape of (`batch_size, num_heads, self.config.sliding_window-1, head_dim`), got\"\n-                        f\" {past_key.shape}\"\n-                    )\n-\n-                if attention_mask is not None:\n-                    attention_mask = attention_mask[:, slicing_tokens:]\n-                    attention_mask = torch.cat([attention_mask, torch.ones_like(attention_mask[:, -1:])], dim=-1)\n-\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n "
        },
        {
            "sha": "b81dac38c7ea55b69872f6aa11970ca0d0be1732",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51e395d13e46a8ecdda2b47381519bdfca87ba4a/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=51e395d13e46a8ecdda2b47381519bdfca87ba4a",
            "patch": "@@ -373,32 +373,6 @@ def forward(\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n-            # Activate slicing cache only if the config has a value `sliding_windows` attribute\n-            cache_has_contents = past_key_value.get_seq_length(self.layer_idx) > 0\n-            kv_seq_len = key_states.shape[-2] + cache_position[0]\n-            if (\n-                getattr(self.config, \"sliding_window\", None) is not None\n-                and kv_seq_len > self.config.sliding_window\n-                and cache_has_contents\n-            ):\n-                slicing_tokens = 1 - self.config.sliding_window\n-\n-                past_key = past_key_value[self.layer_idx][0]\n-                past_value = past_key_value[self.layer_idx][1]\n-\n-                past_key = past_key[:, :, slicing_tokens:, :].contiguous()\n-                past_value = past_value[:, :, slicing_tokens:, :].contiguous()\n-\n-                if past_key.shape[-2] != self.config.sliding_window - 1:\n-                    raise ValueError(\n-                        f\"past key must have a shape of (`batch_size, num_heads, self.config.sliding_window-1, head_dim`), got\"\n-                        f\" {past_key.shape}\"\n-                    )\n-\n-                if attention_mask is not None:\n-                    attention_mask = attention_mask[:, slicing_tokens:]\n-                    attention_mask = torch.cat([attention_mask, torch.ones_like(attention_mask[:, -1:])], dim=-1)\n-\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n "
        }
    ],
    "stats": {
        "total": 230,
        "additions": 0,
        "deletions": 230
    }
}