{
    "author": "zhanluxianshen",
    "message": "Fix typos . (#36551)\n\nSigned-off-by: zhanluxianshen <zhanluxianshen@163.com>",
    "sha": "6966fa190172b48b2fb46fe4552a13b943e692cf",
    "files": [
        {
            "sha": "78d0ea570b07693116eeb88cc337a9cc79b019fe",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6966fa190172b48b2fb46fe4552a13b943e692cf/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6966fa190172b48b2fb46fe4552a13b943e692cf/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=6966fa190172b48b2fb46fe4552a13b943e692cf",
            "patch": "@@ -1218,7 +1218,7 @@ def setup(self, args, state, model):\n             Whether to use MLflow nested runs. If set to `True` or *1*, will create a nested run inside the current\n             run.\n         - **MLFLOW_RUN_ID** (`str`, *optional*):\n-            Allow to reattach to an existing run which can be usefull when resuming training from a checkpoint. When\n+            Allow to reattach to an existing run which can be useful when resuming training from a checkpoint. When\n             `MLFLOW_RUN_ID` environment variable is set, `start_run` attempts to resume a run with the specified run ID\n             and other parameters are ignored.\n         - **MLFLOW_FLATTEN_PARAMS** (`str`, *optional*, defaults to `False`):"
        },
        {
            "sha": "f161930d90d0435ce4e8603eb12c9903d4e7832f",
            "filename": "src/transformers/models/nllb_moe/configuration_nllb_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6966fa190172b48b2fb46fe4552a13b943e692cf/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconfiguration_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6966fa190172b48b2fb46fe4552a13b943e692cf/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconfiguration_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconfiguration_nllb_moe.py?ref=6966fa190172b48b2fb46fe4552a13b943e692cf",
            "patch": "@@ -100,7 +100,7 @@ class NllbMoeConfig(PretrainedConfig):\n             experts.\n         router_bias (`bool`, *optional*, defaults to `False`):\n             Whether or not the classifier of the router should have a bias.\n-        moe_token_dropout (`float`, *optional*, defualt ot 0.2):\n+        moe_token_dropout (`float`, *optional*, default to 0.2):\n             Masking rate for MoE expert output masking (EOM), which is implemented via a Dropout2d on the expert\n             outputs.\n         output_router_logits (`bool`, *optional*, defaults to `False`):"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}