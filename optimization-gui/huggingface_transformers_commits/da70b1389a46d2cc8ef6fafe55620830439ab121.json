{
    "author": "sbucaille",
    "message": "docs: Update EfficientLoFTR documentation (#39620)\n\n* docs: Update EfficientLoFTR documentation\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "da70b1389a46d2cc8ef6fafe55620830439ab121",
    "files": [
        {
            "sha": "9c6964295ff94dab31b3eaa99b5325cd960c173a",
            "filename": "docs/source/en/model_doc/efficientloftr.md",
            "status": "modified",
            "additions": 90,
            "deletions": 55,
            "changes": 145,
            "blob_url": "https://github.com/huggingface/transformers/blob/da70b1389a46d2cc8ef6fafe55620830439ab121/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/da70b1389a46d2cc8ef6fafe55620830439ab121/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md?ref=da70b1389a46d2cc8ef6fafe55620830439ab121",
            "patch": "@@ -10,84 +10,114 @@ specific language governing permissions and limitations under the License.\n ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n rendered properly in your Markdown viewer.\n \n-\n -->\n \n-# EfficientLoFTR\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\" >\n+    </div>\n </div>\n \n-## Overview\n-\n-The EfficientLoFTR model was proposed in [Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed](https://arxiv.org/abs/2403.04765) by Yifan Wang, Xingyi He, Sida Peng, Dongli Tan and Xiaowei Zhou.\n-\n-This model consists of matching two images together by finding pixel correspondences. It can be used to estimate the pose between them. \n-This model is useful for tasks such as image matching, homography estimation, etc.\n+# EfficientLoFTR\n \n-The abstract from the paper is the following:\n+[EfficientLoFTR](https://huggingface.co/papers/2403.04765) is an efficient detector-free local feature matching method that produces semi-dense matches across images with sparse-like speed. It builds upon the original [LoFTR](https://huggingface.co/papers/2104.00680) architecture but introduces significant improvements for both efficiency and accuracy. The key innovation is an aggregated attention mechanism with adaptive token selection that makes the model ~2.5× faster than LoFTR while achieving higher accuracy. EfficientLoFTR can even surpass state-of-the-art efficient sparse matching pipelines like [SuperPoint](./superpoint) + [LightGlue](./lightglue) in terms of speed, making it suitable for large-scale or latency-sensitive applications such as image retrieval and 3D reconstruction.\n \n-*We present a novel method for efficiently producing semidense matches across images. Previous detector-free matcher \n-LoFTR has shown remarkable matching capability in handling large-viewpoint change and texture-poor scenarios but suffers\n-from low efficiency. We revisit its design choices and derive multiple improvements for both efficiency and accuracy. \n-One key observation is that performing the transformer over the entire feature map is redundant due to shared local \n-information, therefore we propose an aggregated attention mechanism with adaptive token selection for efficiency. \n-Furthermore, we find spatial variance exists in LoFTR’s fine correlation module, which is adverse to matching accuracy. \n-A novel two-stage correlation layer is proposed to achieve accurate subpixel correspondences for accuracy improvement. \n-Our efficiency optimized model is ∼ 2.5× faster than LoFTR which can even surpass state-of-the-art efficient sparse \n-matching pipeline SuperPoint + LightGlue. Moreover, extensive experiments show that our method can achieve higher \n-accuracy compared with competitive semi-dense matchers, with considerable efficiency benefits. This opens up exciting \n-prospects for large-scale or latency-sensitive applications such as image retrieval and 3D reconstruction. \n-Project page: [https://zju3dv.github.io/efficientloftr/](https://zju3dv.github.io/efficientloftr/).*\n+> [!TIP]\n+> This model was contributed by [stevenbucaille](https://huggingface.co/stevenbucaille).\n+>\n+> Click on the EfficientLoFTR models in the right sidebar for more examples of how to apply EfficientLoFTR to different computer vision tasks.\n \n-## How to use\n+The example below demonstrates how to match keypoints between two images with the [`AutoModel`] class.\n \n-Here is a quick example of using the model. \n-```python\n-import torch\n+<hfoptions id=\"usage\">\n+<hfoption id=\"AutoModel\">\n \n+```py\n from transformers import AutoImageProcessor, AutoModelForKeypointMatching\n-from transformers.image_utils import load_image\n-\n+import torch\n+from PIL import Image\n+import requests\n \n-image1 = load_image(\"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg\")\n-image2 = load_image(\"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg\")\n+url_image1 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg\"\n+image1 = Image.open(requests.get(url_image1, stream=True).raw)\n+url_image2 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg\"\n+image2 = Image.open(requests.get(url_image2, stream=True).raw)\n \n images = [image1, image2]\n \n-processor = AutoImageProcessor.from_pretrained(\"stevenbucaille/efficientloftr\")\n-model = AutoModelForKeypointMatching.from_pretrained(\"stevenbucaille/efficientloftr\")\n+processor = AutoImageProcessor.from_pretrained(\"zju-community/efficientloftr\")\n+model = AutoModelForKeypointMatching.from_pretrained(\"zju-community/efficientloftr\")\n \n inputs = processor(images, return_tensors=\"pt\")\n with torch.no_grad():\n     outputs = model(**inputs)\n-```\n \n-You can use the `post_process_keypoint_matching` method from the `ImageProcessor` to get the keypoints and matches in a more readable format:\n-\n-```python\n+# Post-process to get keypoints and matches\n image_sizes = [[(image.height, image.width) for image in images]]\n-outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=0.2)\n-for i, output in enumerate(outputs):\n-    print(\"For the image pair\", i)\n-    for keypoint0, keypoint1, matching_score in zip(\n-            output[\"keypoints0\"], output[\"keypoints1\"], output[\"matching_scores\"]\n-    ):\n-        print(\n-            f\"Keypoint at coordinate {keypoint0.numpy()} in the first image matches with keypoint at coordinate {keypoint1.numpy()} in the second image with a score of {matching_score}.\"\n-        )\n+processed_outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=0.2)\n ```\n \n-From the post processed outputs, you can visualize the matches between the two images using the following code:\n-```python\n-images_with_matching = processor.visualize_keypoint_matching(images, outputs)\n-```\n+</hfoption>\n+</hfoptions>\n+\n+## Notes\n+\n+- EfficientLoFTR is designed for efficiency while maintaining high accuracy. It uses an aggregated attention mechanism with adaptive token selection to reduce computational overhead compared to the original LoFTR.\n+\n+    ```py\n+    from transformers import AutoImageProcessor, AutoModelForKeypointMatching\n+    import torch\n+    from PIL import Image\n+    import requests\n+    \n+    processor = AutoImageProcessor.from_pretrained(\"zju-community/efficientloftr\")\n+    model = AutoModelForKeypointMatching.from_pretrained(\"zju-community/efficientloftr\")\n+    \n+    # EfficientLoFTR requires pairs of images\n+    images = [image1, image2]\n+    inputs = processor(images, return_tensors=\"pt\")\n+    outputs = model(**inputs)\n+    \n+    # Extract matching information\n+    keypoints = outputs.keypoints        # Keypoints in both images\n+    matches = outputs.matches            # Matching indices \n+    matching_scores = outputs.matching_scores  # Confidence scores\n+    ```\n+\n+- The model produces semi-dense matches, offering a good balance between the density of matches and computational efficiency. It excels in handling large viewpoint changes and texture-poor scenarios.\n+\n+- For better visualization and analysis, use the [`~EfficientLoFTRImageProcessor.post_process_keypoint_matching`] method to get matches in a more readable format.\n+\n+    ```py\n+    # Process outputs for visualization\n+    image_sizes = [[(image.height, image.width) for image in images]]\n+    processed_outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=0.2)\n+    \n+    for i, output in enumerate(processed_outputs):\n+        print(f\"For the image pair {i}\")\n+        for keypoint0, keypoint1, matching_score in zip(\n+                output[\"keypoints0\"], output[\"keypoints1\"], output[\"matching_scores\"]\n+        ):\n+            print(f\"Keypoint at {keypoint0.numpy()} matches with keypoint at {keypoint1.numpy()} with score {matching_score}\")\n+    ```\n+\n+- Visualize the matches between the images using the built-in plotting functionality.\n+\n+    ```py\n+    # Easy visualization using the built-in plotting method\n+    visualized_images = processor.visualize_keypoint_matching(images, processed_outputs)\n+    ```\n+\n+- EfficientLoFTR uses a novel two-stage correlation layer that achieves accurate subpixel correspondences, improving upon the original LoFTR's fine correlation module.\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://cdn-uploads.huggingface.co/production/uploads/632885ba1558dac67c440aa8/2nJZQlFToCYp_iLurvcZ4.png\">\n+</div>\n \n-![image/png](https://cdn-uploads.huggingface.co/production/uploads/632885ba1558dac67c440aa8/2nJZQlFToCYp_iLurvcZ4.png)\n+## Resources\n \n-This model was contributed by [stevenbucaille](https://huggingface.co/stevenbucaille).\n-The original code can be found [here](https://github.com/zju3dv/EfficientLoFTR).\n+- Refer to the [original EfficientLoFTR repository](https://github.com/zju3dv/EfficientLoFTR) for more examples and implementation details.\n+- [EfficientLoFTR project page](https://zju3dv.github.io/efficientloftr/) with interactive demos and additional information.\n \n ## EfficientLoFTRConfig\n \n@@ -101,6 +131,8 @@ The original code can be found [here](https://github.com/zju3dv/EfficientLoFTR).\n - post_process_keypoint_matching\n - visualize_keypoint_matching\n \n+<frameworkcontent>\n+<pt>\n ## EfficientLoFTRModel\n \n [[autodoc]] EfficientLoFTRModel\n@@ -111,4 +143,7 @@ The original code can be found [here](https://github.com/zju3dv/EfficientLoFTR).\n \n [[autodoc]] EfficientLoFTRForKeypointMatching\n \n-- forward\n\\ No newline at end of file\n+- forward\n+\n+</pt>\n+</frameworkcontent>\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 145,
        "additions": 90,
        "deletions": 55
    }
}