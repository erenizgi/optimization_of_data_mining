{
    "author": "SunMarc",
    "message": "deprecate `jit_mode_eval` (#41376)",
    "sha": "081391b20e6af33a3489f45a505b122f4208417c",
    "files": [
        {
            "sha": "a6900cb6de5ecda907519d28c1116c7fc69039ec",
            "filename": "docs/source/en/perf_infer_cpu.md",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/081391b20e6af33a3489f45a505b122f4208417c/docs%2Fsource%2Fen%2Fperf_infer_cpu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/081391b20e6af33a3489f45a505b122f4208417c/docs%2Fsource%2Fen%2Fperf_infer_cpu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_cpu.md?ref=081391b20e6af33a3489f45a505b122f4208417c",
            "patch": "@@ -38,24 +38,3 @@ pred = onnx_qa(question, context)\n \n > [!TIP]\n > Optimum includes an [Intel](https://hf.co/docs/optimum/intel/index) extension that provides additional optimizations such as quantization, pruning, and knowledge distillation for Intel CPUs. This extension also includes tools to convert models to [OpenVINO](https://hf.co/docs/optimum/intel/inference), a toolkit for optimizing and deploying models, for even faster inference.\n-\n-## TorchScript\n-\n-[TorchScript](https://pytorch.org/docs/stable/jit.html) is an intermediate PyTorch model format that can be run in non-Python environments, like C++, where performance is critical. Train a PyTorch model and convert it to a TorchScript function or module with [torch.jit.trace](https://pytorch.org/docs/stable/generated/torch.jit.trace.html). This function optimizes the model with just-in-time (JIT) compilation, and compared to the default eager mode, JIT-compiled models offer better inference performance.\n-\n-> [!TIP]\n-> Refer to the [Introduction to PyTorch TorchScript](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html) tutorial for a gentle introduction to TorchScript.\n-\n-On a CPU, enable `torch.jit.trace` with the `--jit_mode_eval` flag in [`Trainer`].\n-\n-```bash\n-python examples/pytorch/question-answering/run_qa.py \\\n---model_name_or_path csarron/bert-base-uncased-squad-v1 \\\n---dataset_name squad \\\n---do_eval \\\n---max_seq_length 384 \\\n---doc_stride 128 \\\n---output_dir /tmp/ \\\n---no_cuda \\\n---jit_mode_eval\n-```"
        },
        {
            "sha": "2bb54d30f943eabce0661da336e4fb24b82f234c",
            "filename": "docs/source/it/perf_infer_cpu.md",
            "status": "modified",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/081391b20e6af33a3489f45a505b122f4208417c/docs%2Fsource%2Fit%2Fperf_infer_cpu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/081391b20e6af33a3489f45a505b122f4208417c/docs%2Fsource%2Fit%2Fperf_infer_cpu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fperf_infer_cpu.md?ref=081391b20e6af33a3489f45a505b122f4208417c",
            "patch": "@@ -33,43 +33,3 @@ Vedi maggiori informazioni per [IPEX Graph Optimization](https://intel.github.io\n #### Installazione di IPEX\n \n I rilasci di IPEX seguono PyTorch, verifica i vari approcci per [IPEX installation](https://intel.github.io/intel-extension-for-pytorch/).\n-\n-### Utilizzo del JIT-mode\n-\n-Per abilitare JIT-mode in Trainer per evaluation e prediction, devi aggiungere `jit_mode_eval` negli argomenti di Trainer.\n-\n-<Tip warning={true}>\n-\n-per PyTorch >= 1.14.0. JIT-mode potrebe giovare a qualsiasi modello di prediction e evaluaion visto che il dict input è supportato in jit.trace\n-\n-per PyTorch < 1.14.0. JIT-mode potrebbe giovare ai modelli il cui ordine dei parametri corrisponde all'ordine delle tuple in ingresso in jit.trace, come i modelli per question-answering.\n-Nel caso in cui l'ordine dei parametri seguenti non corrisponda all'ordine delle tuple in ingresso in jit.trace, come nei modelli di text-classification, jit.trace fallirà e lo cattureremo con una eccezione al fine di renderlo un fallback. Il logging è usato per notificare gli utenti.\n-\n-</Tip>\n-\n-Trovi un esempo con caso d'uso in [Transformers question-answering](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)\n-\n-- Inference using jit mode on CPU:\n-\n-<pre>python run_qa.py \\\n---model_name_or_path csarron/bert-base-uncased-squad-v1 \\\n---dataset_name squad \\\n---do_eval \\\n---max_seq_length 384 \\\n---doc_stride 128 \\\n---output_dir /tmp/ \\\n---no_cuda \\\n-<b>--jit_mode_eval </b></pre> \n-\n-- Inference with IPEX using jit mode on CPU:\n-\n-<pre>python run_qa.py \\\n---model_name_or_path csarron/bert-base-uncased-squad-v1 \\\n---dataset_name squad \\\n---do_eval \\\n---max_seq_length 384 \\\n---doc_stride 128 \\\n---output_dir /tmp/ \\\n---no_cuda \\\n-<b>--use_ipex \\</b>\n-<b>--jit_mode_eval</b></pre> "
        },
        {
            "sha": "4121e83e22c8ef40d84297f6a6051f078650a260",
            "filename": "docs/source/ja/perf_infer_cpu.md",
            "status": "modified",
            "additions": 0,
            "deletions": 36,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/081391b20e6af33a3489f45a505b122f4208417c/docs%2Fsource%2Fja%2Fperf_infer_cpu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/081391b20e6af33a3489f45a505b122f4208417c/docs%2Fsource%2Fja%2Fperf_infer_cpu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fperf_infer_cpu.md?ref=081391b20e6af33a3489f45a505b122f4208417c",
            "patch": "@@ -32,39 +32,3 @@ Intel® Extension for PyTorchは、Transformersシリーズモデルのjitモー\n #### IPEX installation:\n \n IPEXのリリースはPyTorchに従っています。[IPEXのインストール方法](https://intel.github.io/intel-extension-for-pytorch/)を確認してください。\n-\n-### Usage of JIT-mode\n-Trainerで評価または予測のためにJITモードを有効にするには、ユーザーはTrainerコマンド引数に`jit_mode_eval`を追加する必要があります。\n-\n-<Tip warning={true}>\n-\n-PyTorch >= 1.14.0の場合、jitモードはjit.traceでdict入力がサポートされているため、予測と評価に任意のモデルに利益をもたらす可能性があります。\n-\n-PyTorch < 1.14.0の場合、jitモードはforwardパラメーターの順序がjit.traceのタプル入力の順序と一致するモデルに利益をもたらす可能性があります（質問応答モデルなど）。jit.traceがタプル入力の順序と一致しない場合、テキスト分類モデルなど、jit.traceは失敗し、これをフォールバックさせるために例外でキャッチしています。ログはユーザーに通知するために使用されます。\n-\n-</Tip>\n-\n-[Transformers質問応答の使用例](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)を参考にしてください。\n-\n-- Inference using jit mode on CPU:\n-<pre>python run_qa.py \\\n---model_name_or_path csarron/bert-base-uncased-squad-v1 \\\n---dataset_name squad \\\n---do_eval \\\n---max_seq_length 384 \\\n---doc_stride 128 \\\n---output_dir /tmp/ \\\n---no_cuda \\\n-<b>--jit_mode_eval </b></pre> \n-\n-- Inference with IPEX using jit mode on CPU:\n-<pre>python run_qa.py \\\n---model_name_or_path csarron/bert-base-uncased-squad-v1 \\\n---dataset_name squad \\\n---do_eval \\\n---max_seq_length 384 \\\n---doc_stride 128 \\\n---output_dir /tmp/ \\\n---no_cuda \\\n-<b>--use_ipex \\</b>\n-<b>--jit_mode_eval</b></pre> "
        },
        {
            "sha": "5b7a9d37d796385a1df1c8d7f40097ff402e41b2",
            "filename": "docs/source/ko/perf_infer_cpu.md",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/081391b20e6af33a3489f45a505b122f4208417c/docs%2Fsource%2Fko%2Fperf_infer_cpu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/081391b20e6af33a3489f45a505b122f4208417c/docs%2Fsource%2Fko%2Fperf_infer_cpu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fperf_infer_cpu.md?ref=081391b20e6af33a3489f45a505b122f4208417c",
            "patch": "@@ -30,40 +30,3 @@ Intel® Extension for PyTorch(IPEX)는 Transformers 계열 모델의 jit 모드\n #### IPEX 설치: [[ipex-installation]]\n \n IPEX 배포 주기는 PyTorch를 따라서 이루어집니다. 자세한 정보는 [IPEX 설치 방법](https://intel.github.io/intel-extension-for-pytorch/)을 확인하세요.\n-\n-### JIT 모드 사용법 [[usage-of-jitmode]]\n-평가 또는 예측을 위해 Trainer에서 JIT 모드를 사용하려면 Trainer의 명령 인수에 `jit_mode_eval`을 추가해야 합니다.\n-\n-<Tip warning={true}>\n-\n-PyTorch의 버전이 1.14.0 이상이라면, jit 모드는 jit.trace에서 dict 입력이 지원되므로, 모든 모델의 예측과 평가가 개선될 수 있습니다.\n-\n-PyTorch의 버전이 1.14.0 미만이라면, 질의 응답 모델과 같이 forward 매개변수의 순서가 jit.trace의 튜플 입력 순서와 일치하는 모델에 득이 될 수 있습니다. 텍스트 분류 모델과 같이 forward 매개변수 순서가 jit.trace의 튜플 입력 순서와 다른 경우, jit.trace가 실패하며 예외가 발생합니다. 이때 예외상황을 사용자에게 알리기 위해 Logging이 사용됩니다.\n-\n-</Tip>\n-\n-[Transformers 질의 응답](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)의 사용 사례 예시를 참조하세요.\n-\n-\n-- CPU에서 jit 모드를 사용한 추론:\n-<pre>python run_qa.py \\\n---model_name_or_path csarron/bert-base-uncased-squad-v1 \\\n---dataset_name squad \\\n---do_eval \\\n---max_seq_length 384 \\\n---doc_stride 128 \\\n---output_dir /tmp/ \\\n---no_cuda \\\n-<b>--jit_mode_eval </b></pre> \n-\n-- CPU에서 IPEX와 함께 jit 모드를 사용한 추론:\n-<pre>python run_qa.py \\\n---model_name_or_path csarron/bert-base-uncased-squad-v1 \\\n---dataset_name squad \\\n---do_eval \\\n---max_seq_length 384 \\\n---doc_stride 128 \\\n---output_dir /tmp/ \\\n---no_cuda \\\n-<b>--use_ipex \\</b>\n-<b>--jit_mode_eval</b></pre> "
        },
        {
            "sha": "de6a1d76734cbbf14f2a297e73de9ec5f48d70c4",
            "filename": "examples/pytorch/question-answering/trainer_qa.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/081391b20e6af33a3489f45a505b122f4208417c/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/081391b20e6af33a3489f45a505b122f4208417c/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_qa.py?ref=081391b20e6af33a3489f45a505b122f4208417c",
            "patch": "@@ -55,8 +55,6 @@ def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metr\n         finally:\n             self.compute_metrics = compute_metrics\n         total_batch_size = self.args.eval_batch_size * self.args.world_size\n-        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n-            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n         output.metrics.update(\n             speed_metrics(\n                 metric_key_prefix,\n@@ -109,8 +107,6 @@ def predict(self, predict_dataset, predict_examples, ignore_keys=None, metric_ke\n         finally:\n             self.compute_metrics = compute_metrics\n         total_batch_size = self.args.eval_batch_size * self.args.world_size\n-        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n-            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n         output.metrics.update(\n             speed_metrics(\n                 metric_key_prefix,"
        },
        {
            "sha": "4da0707f85f414b665d884eedc75784b78486473",
            "filename": "examples/pytorch/question-answering/trainer_seq2seq_qa.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/081391b20e6af33a3489f45a505b122f4208417c/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_seq2seq_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/081391b20e6af33a3489f45a505b122f4208417c/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_seq2seq_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_seq2seq_qa.py?ref=081391b20e6af33a3489f45a505b122f4208417c",
            "patch": "@@ -76,8 +76,6 @@ def evaluate(\n         finally:\n             self.compute_metrics = compute_metrics\n         total_batch_size = self.args.eval_batch_size * self.args.world_size\n-        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n-            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n         output.metrics.update(\n             speed_metrics(\n                 metric_key_prefix,\n@@ -137,8 +135,6 @@ def predict(\n             self.compute_metrics = compute_metrics\n \n         total_batch_size = self.args.eval_batch_size * self.args.world_size\n-        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n-            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n         output.metrics.update(\n             speed_metrics(\n                 metric_key_prefix,"
        },
        {
            "sha": "2622fdda8079264f8a4bf7e52d7a9c44a10bd3bb",
            "filename": "src/transformers/modelcard.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/081391b20e6af33a3489f45a505b122f4208417c/src%2Ftransformers%2Fmodelcard.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/081391b20e6af33a3489f45a505b122f4208417c/src%2Ftransformers%2Fmodelcard.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodelcard.py?ref=081391b20e6af33a3489f45a505b122f4208417c",
            "patch": "@@ -649,7 +649,6 @@ def parse_log_history(log_history):\n             _ = metrics.pop(\"eval_runtime\", None)\n             _ = metrics.pop(\"eval_samples_per_second\", None)\n             _ = metrics.pop(\"eval_steps_per_second\", None)\n-            _ = metrics.pop(\"eval_jit_compilation_time\", None)\n             values = {\"Training Loss\": training_loss, \"Epoch\": epoch, \"Step\": step}\n             for k, v in metrics.items():\n                 if k == \"eval_loss\":"
        },
        {
            "sha": "83ec5da884f3e9ed5f3f67024e8afbc8cd6fc972",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 47,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/081391b20e6af33a3489f45a505b122f4208417c/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/081391b20e6af33a3489f45a505b122f4208417c/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=081391b20e6af33a3489f45a505b122f4208417c",
            "patch": "@@ -16,7 +16,6 @@\n \"\"\"\n \n import contextlib\n-import copy\n import functools\n import glob\n import importlib.metadata\n@@ -212,7 +211,6 @@\n     from accelerate import __version__ as accelerate_version\n     from accelerate.state import AcceleratorState\n     from accelerate.utils import (\n-        AutocastKwargs,\n         DataLoaderConfiguration,\n         DistributedDataParallelKwargs,\n         DistributedType,\n@@ -1881,40 +1879,6 @@ def call_model_init(self, trial=None):\n \n         return model\n \n-    def torch_jit_model_eval(self, model, dataloader, training=False):\n-        if not training:\n-            if dataloader is None:\n-                logger.warning(\"failed to use PyTorch jit mode due to current dataloader is none.\")\n-                return model\n-            example_batch = next(iter(dataloader))\n-            example_batch = self._prepare_inputs(example_batch)\n-            try:\n-                jit_model = copy.copy(model)\n-                jit_model.eval()\n-                original_forward = jit_model.__dict__.pop(\"_original_forward\", None)\n-                # remove mixed precision hooks from the model\n-                if original_forward:\n-                    jit_model.forward = original_forward\n-                autocast_handler = AutocastKwargs(cache_enabled=False)\n-                with self.accelerator.autocast(autocast_handler=autocast_handler), torch.no_grad():\n-                    if isinstance(example_batch, dict):\n-                        jit_model = torch.jit.trace(jit_model, example_kwarg_inputs=example_batch, strict=False)\n-                    else:\n-                        jit_model = torch.jit.trace(\n-                            jit_model,\n-                            example_kwarg_inputs={key: example_batch[key] for key in example_batch},\n-                            strict=False,\n-                        )\n-                jit_model = torch.jit.freeze(jit_model)\n-                with torch.no_grad():\n-                    jit_model(**example_batch)\n-                    jit_model(**example_batch)\n-                model = jit_model\n-            except (RuntimeError, TypeError, ValueError, NameError, IndexError) as e:\n-                logger.warning(f\"failed to use PyTorch jit mode due to: {e}.\")\n-\n-        return model\n-\n     def compare_trainer_and_checkpoint_args(self, training_args, trainer_state):\n         attributes_map = {\n             \"logging_steps\": \"logging_steps\",\n@@ -1958,11 +1922,6 @@ def _wrap_model(self, model, training=True, dataloader=None):\n         if self.args.n_gpu > 1 and not getattr(model, \"is_loaded_in_8bit\", False):\n             model = nn.DataParallel(model)\n \n-        if self.args.jit_mode_eval:\n-            start_time = time.time()\n-            model = self.torch_jit_model_eval(model, dataloader, training)\n-            self.jit_compilation_time = round(time.time() - start_time, 4)\n-\n         # Note: in torch.distributed mode, there's no point in wrapping the model\n         # inside a DistributedDataParallel as we'll be under `no_grad` anyways.\n         if not training:\n@@ -4250,8 +4209,6 @@ def evaluate(\n         )\n \n         total_batch_size = self.args.eval_batch_size * self.args.world_size\n-        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n-            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n         if f\"{metric_key_prefix}_model_preparation_time\" in output.metrics:\n             start_time += output.metrics[f\"{metric_key_prefix}_model_preparation_time\"]\n         output.metrics.update(\n@@ -4320,8 +4277,6 @@ def predict(\n             test_dataloader, description=\"Prediction\", ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix\n         )\n         total_batch_size = self.args.eval_batch_size * self.args.world_size\n-        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n-            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n         if f\"{metric_key_prefix}_model_preparation_time\" in output.metrics:\n             start_time += output.metrics[f\"{metric_key_prefix}_model_preparation_time\"]\n         output.metrics.update(\n@@ -4536,8 +4491,6 @@ def evaluation_loop(\n             metrics[f\"{metric_key_prefix}_loss\"] = np.concatenate(all_losses).mean().item()\n         elif isinstance(all_losses, np.ndarray):\n             metrics[f\"{metric_key_prefix}_loss\"] = all_losses.mean().item()\n-        if hasattr(self, \"jit_compilation_time\"):\n-            metrics[f\"{metric_key_prefix}_jit_compilation_time\"] = self.jit_compilation_time\n         if hasattr(self, \"model_preparation_time\"):\n             metrics[f\"{metric_key_prefix}_model_preparation_time\"] = self.model_preparation_time\n "
        },
        {
            "sha": "f3ff925abb9c6871a33cb73581f1cc86fc9b8231",
            "filename": "src/transformers/trainer_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/081391b20e6af33a3489f45a505b122f4208417c/src%2Ftransformers%2Ftrainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/081391b20e6af33a3489f45a505b122f4208417c/src%2Ftransformers%2Ftrainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_utils.py?ref=081391b20e6af33a3489f45a505b122f4208417c",
            "patch": "@@ -269,9 +269,7 @@ def default_compute_objective(metrics: dict[str, float]) -> float:\n     loss = metrics.pop(\"eval_loss\", None)\n     _ = metrics.pop(\"epoch\", None)\n     # Remove speed metrics\n-    speed_metrics = [\n-        m for m in metrics if m.endswith(\"_runtime\") or m.endswith(\"_per_second\") or m.endswith(\"_compilation_time\")\n-    ]\n+    speed_metrics = [m for m in metrics if m.endswith(\"_runtime\") or m.endswith(\"_per_second\")]\n     for sm in speed_metrics:\n         _ = metrics.pop(sm, None)\n     return loss if len(metrics) == 0 else sum(metrics.values())"
        },
        {
            "sha": "12b657e43bab602389415d2f8bccfa16b7331d1c",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/081391b20e6af33a3489f45a505b122f4208417c/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/081391b20e6af33a3489f45a505b122f4208417c/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=081391b20e6af33a3489f45a505b122f4208417c",
            "patch": "@@ -367,8 +367,6 @@ class TrainingArguments:\n             Random seed to be used with data samplers. If not set, random generators for data sampling will use the\n             same seed as `seed`. This can be used to ensure reproducibility of data sampling, independent of the model\n             seed.\n-        jit_mode_eval (`bool`, *optional*, defaults to `False`):\n-            Whether or not to use PyTorch jit trace for inference.\n         bf16 (`bool`, *optional*, defaults to `False`):\n             Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher\n             NVIDIA architecture or Intel XPU or using CPU (use_cpu) or Ascend NPU.\n@@ -964,9 +962,6 @@ class TrainingArguments:\n     )\n     seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n     data_seed: Optional[int] = field(default=None, metadata={\"help\": \"Random seed to be used with data samplers.\"})\n-    jit_mode_eval: bool = field(\n-        default=False, metadata={\"help\": \"Whether or not to use PyTorch jit trace for inference\"}\n-    )\n     bf16: bool = field(\n         default=False,\n         metadata={\n@@ -2335,7 +2330,6 @@ def set_evaluate(\n         accumulation_steps: Optional[int] = None,\n         delay: Optional[float] = None,\n         loss_only: bool = False,\n-        jit_mode: bool = False,\n     ):\n         \"\"\"\n         A method that regroups all arguments linked to evaluation.\n@@ -2362,8 +2356,6 @@ def set_evaluate(\n                 eval_strategy.\n             loss_only (`bool`, *optional*, defaults to `False`):\n                 Ignores all outputs except the loss.\n-            jit_mode (`bool`, *optional*):\n-                Whether or not to use PyTorch jit trace for inference.\n \n         Example:\n \n@@ -2385,14 +2377,12 @@ def set_evaluate(\n         self.eval_accumulation_steps = accumulation_steps\n         self.eval_delay = delay\n         self.prediction_loss_only = loss_only\n-        self.jit_mode_eval = jit_mode\n         return self\n \n     def set_testing(\n         self,\n         batch_size: int = 8,\n         loss_only: bool = False,\n-        jit_mode: bool = False,\n     ):\n         \"\"\"\n         A method that regroups all basic arguments linked to testing on a held-out dataset.\n@@ -2408,8 +2398,6 @@ def set_testing(\n                 The batch size per device (GPU/TPU core/CPU...) used for testing.\n             loss_only (`bool`, *optional*, defaults to `False`):\n                 Ignores all outputs except the loss.\n-            jit_mode (`bool`, *optional*):\n-                Whether or not to use PyTorch jit trace for inference.\n \n         Example:\n \n@@ -2425,7 +2413,6 @@ def set_testing(\n         self.do_predict = True\n         self.per_device_eval_batch_size = batch_size\n         self.prediction_loss_only = loss_only\n-        self.jit_mode_eval = jit_mode\n         return self\n \n     def set_save("
        },
        {
            "sha": "3d783d1749a056397e348819286c1dd90e570b90",
            "filename": "src/transformers/utils/notebook.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/081391b20e6af33a3489f45a505b122f4208417c/src%2Ftransformers%2Futils%2Fnotebook.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/081391b20e6af33a3489f45a505b122f4208417c/src%2Ftransformers%2Futils%2Fnotebook.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fnotebook.py?ref=081391b20e6af33a3489f45a505b122f4208417c",
            "patch": "@@ -360,7 +360,6 @@ def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n             _ = metrics.pop(f\"{metric_key_prefix}_runtime\", None)\n             _ = metrics.pop(f\"{metric_key_prefix}_samples_per_second\", None)\n             _ = metrics.pop(f\"{metric_key_prefix}_steps_per_second\", None)\n-            _ = metrics.pop(f\"{metric_key_prefix}_jit_compilation_time\", None)\n             for k, v in metrics.items():\n                 splits = k.split(\"_\")\n                 name = \" \".join([part.capitalize() for part in splits[1:]])"
        },
        {
            "sha": "49c74f8558c9cbde77af61849d3e9371b6a04b95",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 96,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/081391b20e6af33a3489f45a505b122f4208417c/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/081391b20e6af33a3489f45a505b122f4208417c/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=081391b20e6af33a3489f45a505b122f4208417c",
            "patch": "@@ -2859,56 +2859,6 @@ def test_evaluate_with_batch_eval_metrics(self):\n             expected_acc = AlmostAccuracy()((pred + 1, y))[\"accuracy\"]\n             self.assertAlmostEqual(results[\"eval_accuracy\"], expected_acc)\n \n-    def test_evaluate_with_jit(self):\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            trainer = get_regression_trainer(\n-                a=1.5, b=2.5, compute_metrics=AlmostAccuracy(), jit_mode_eval=True, output_dir=tmp_dir\n-            )\n-            # Make sure the trainer doesn't pass num_items_in_batch to the model's forward method,\n-            # since it's not in the model forward's signature when using JIT\n-            trainer.model_accepts_loss_kwargs = False\n-            results = trainer.evaluate()\n-\n-            x, y = trainer.eval_dataset.x, trainer.eval_dataset.ys[0]\n-            pred = 1.5 * x + 2.5\n-            expected_loss = ((pred - y) ** 2).mean()\n-            self.assertAlmostEqual(results[\"eval_loss\"], expected_loss)\n-            expected_acc = AlmostAccuracy()((pred, y))[\"accuracy\"]\n-            self.assertAlmostEqual(results[\"eval_accuracy\"], expected_acc)\n-\n-            # With a number of elements not a round multiple of the batch size\n-            trainer = get_regression_trainer(\n-                a=1.5, b=2.5, eval_len=66, compute_metrics=AlmostAccuracy(), jit_mode_eval=True, output_dir=tmp_dir\n-            )\n-            trainer.model_accepts_loss_kwargs = False\n-            results = trainer.evaluate()\n-\n-            x, y = trainer.eval_dataset.x, trainer.eval_dataset.ys[0]\n-            pred = 1.5 * x + 2.5\n-            expected_loss = ((pred - y) ** 2).mean()\n-            self.assertAlmostEqual(results[\"eval_loss\"], expected_loss)\n-            expected_acc = AlmostAccuracy()((pred, y))[\"accuracy\"]\n-            self.assertAlmostEqual(results[\"eval_accuracy\"], expected_acc)\n-\n-            # With logits preprocess\n-            trainer = get_regression_trainer(\n-                a=1.5,\n-                b=2.5,\n-                compute_metrics=AlmostAccuracy(),\n-                preprocess_logits_for_metrics=lambda logits, labels: logits + 1,\n-                jit_mode_eval=True,\n-                output_dir=tmp_dir,\n-            )\n-            trainer.model_accepts_loss_kwargs = False\n-            results = trainer.evaluate()\n-\n-            x, y = trainer.eval_dataset.x, trainer.eval_dataset.ys[0]\n-            pred = 1.5 * x + 2.5\n-            expected_loss = ((pred - y) ** 2).mean()\n-            self.assertAlmostEqual(results[\"eval_loss\"], expected_loss)\n-            expected_acc = AlmostAccuracy()((pred + 1, y))[\"accuracy\"]\n-            self.assertAlmostEqual(results[\"eval_accuracy\"], expected_acc)\n-\n     def test_predict(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             trainer = get_regression_trainer(a=1.5, b=2.5, output_dir=tmp_dir)\n@@ -3042,52 +2992,6 @@ def test_predict_with_batch_eval_metrics(self):\n             self.assertTrue(np.array_equal(labels[0], trainer.eval_dataset.ys[0]))\n             self.assertTrue(np.array_equal(labels[1], trainer.eval_dataset.ys[1]))\n \n-    def test_predict_with_jit(self):\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            trainer = get_regression_trainer(a=1.5, b=2.5, jit_mode_eval=True, output_dir=tmp_dir)\n-            # Make sure the trainer doesn't pass num_items_in_batch to the model's forward method,\n-            # since it's not in the model forward's signature when using JIT\n-            trainer.model_accepts_loss_kwargs = False\n-            preds = trainer.predict(trainer.eval_dataset).predictions\n-            x = trainer.eval_dataset.x\n-            self.assertTrue(np.allclose(preds, 1.5 * x + 2.5))\n-\n-            # With a number of elements not a round multiple of the batch size\n-            trainer = get_regression_trainer(a=1.5, b=2.5, eval_len=66, jit_mode_eval=True, output_dir=tmp_dir)\n-            trainer.model_accepts_loss_kwargs = False\n-            preds = trainer.predict(trainer.eval_dataset).predictions\n-            x = trainer.eval_dataset.x\n-            self.assertTrue(np.allclose(preds, 1.5 * x + 2.5))\n-\n-            # With more than one output of the model\n-            trainer = get_regression_trainer(a=1.5, b=2.5, double_output=True, jit_mode_eval=True, output_dir=tmp_dir)\n-            trainer.model_accepts_loss_kwargs = False\n-            preds = trainer.predict(trainer.eval_dataset).predictions\n-            x = trainer.eval_dataset.x\n-            self.assertEqual(len(preds), 2)\n-            self.assertTrue(np.allclose(preds[0], 1.5 * x + 2.5))\n-            self.assertTrue(np.allclose(preds[1], 1.5 * x + 2.5))\n-\n-            # With more than one output/label of the model\n-            trainer = get_regression_trainer(\n-                a=1.5,\n-                b=2.5,\n-                double_output=True,\n-                label_names=[\"labels\", \"labels_2\"],\n-                jit_mode_eval=True,\n-                output_dir=tmp_dir,\n-            )\n-            trainer.model_accepts_loss_kwargs = False\n-            outputs = trainer.predict(trainer.eval_dataset)\n-            preds = outputs.predictions\n-            labels = outputs.label_ids\n-            x = trainer.eval_dataset.x\n-            self.assertEqual(len(preds), 2)\n-            self.assertTrue(np.allclose(preds[0], 1.5 * x + 2.5))\n-            self.assertTrue(np.allclose(preds[1], 1.5 * x + 2.5))\n-            self.assertTrue(np.array_equal(labels[0], trainer.eval_dataset.ys[0]))\n-            self.assertTrue(np.array_equal(labels[1], trainer.eval_dataset.ys[1]))\n-\n     def test_dynamic_shapes(self):\n         eval_dataset = DynamicShapesDataset(batch_size=self.batch_size)\n         model = RegressionModel(a=2, b=1)"
        }
    ],
    "stats": {
        "total": 304,
        "additions": 1,
        "deletions": 303
    }
}