{
    "author": "vasqu",
    "message": "[`Qwen Omni/VL`] Fix fa tests (#40528)\n\n* fix\n\n* style\n\n* flaky flaky\n\n* flaky flaky\n\n* oopsie, we need the out of place for sure\n\n* flaky flaky\n\n* flaky flaky",
    "sha": "2e0f1d6a37f52823946ed877b5f15f328666c31a",
    "files": [
        {
            "sha": "21cb0fcf178e14bea715beb928bf085f5ed36823",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e0f1d6a37f52823946ed877b5f15f328666c31a/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e0f1d6a37f52823946ed877b5f15f328666c31a/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=2e0f1d6a37f52823946ed877b5f15f328666c31a",
            "patch": "@@ -4739,6 +4739,7 @@ def _prepare_config_headdim(config, requested_dim):\n         (There are many more examples especially now that the `kernels` library is\n         supported)\n         \"\"\"\n+        config = copy.deepcopy(config)\n \n         def update_config_headdim(config, requested_dim):\n             # Flex Attention cannot use dropout\n@@ -4784,6 +4785,14 @@ def update_config_headdim(config, requested_dim):\n                 )\n                 config.cross_hidden_size *= max(requested_dim // cross_head_dim, 1)\n \n+            # 3d rope also depends on the head dim\n+            # (we assume easy shapes here where we get to the requested head dim at least)\n+            if hasattr(config, \"rope_scaling\") and len(config.rope_scaling.get(\"mrope_section\", None)) > 0:\n+                scaling_factor = max(requested_dim // (sum(config.rope_scaling[\"mrope_section\"]) * 2), 1)\n+                config.rope_scaling[\"mrope_section\"] = [\n+                    section * scaling_factor for section in config.rope_scaling[\"mrope_section\"]\n+                ]\n+\n         # Update config values\n         update_config_headdim(config, requested_dim)\n         for key in config.sub_configs:"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 9,
        "deletions": 0
    }
}