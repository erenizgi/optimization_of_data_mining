{
    "author": "daniel-bogdoll",
    "message": "Option to set 'non_blocking' for to(device) in BatchEncoding and BatchFeature (#34883)\n\n* Option to set 'non_blocking' for to(device) operation for performance improvements. Defaults to 'false', thus no behavioral changes.\n\n* Enabling non_blocking in to() operation of BatchFeature.\n\n* Improved docstring on utilization of non_blocking\n\n* Force non_blocking as keyword argument\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n---------\n\nCo-authored-by: Daniel Bogdoll <dbogdoll@umich.edu>\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "de8a0b7547451f8f7ef2c0ac1f338ba77c614cec",
    "files": [
        {
            "sha": "6e8007edbc0b782970887e83a08ba518de0f264a",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de8a0b7547451f8f7ef2c0ac1f338ba77c614cec/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de8a0b7547451f8f7ef2c0ac1f338ba77c614cec/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=de8a0b7547451f8f7ef2c0ac1f338ba77c614cec",
            "patch": "@@ -213,6 +213,7 @@ def to(self, *args, **kwargs) -> \"BatchFeature\":\n                 Will be passed to the `to(...)` function of the tensors.\n             kwargs (`Dict`, *optional*):\n                 Will be passed to the `to(...)` function of the tensors.\n+                To enable asynchronous data transfer, set the `non_blocking` flag in `kwargs` (defaults to `False`).\n \n         Returns:\n             [`BatchFeature`]: The same instance after modification.\n@@ -222,6 +223,7 @@ def to(self, *args, **kwargs) -> \"BatchFeature\":\n \n         new_data = {}\n         device = kwargs.get(\"device\")\n+        non_blocking = kwargs.get(\"non_blocking\", False)\n         # Check if the args are a device or a dtype\n         if device is None and len(args) > 0:\n             # device should be always the first argument\n@@ -241,7 +243,7 @@ def to(self, *args, **kwargs) -> \"BatchFeature\":\n                 # cast and send to device\n                 new_data[k] = v.to(*args, **kwargs)\n             elif isinstance(v, torch.Tensor) and device is not None:\n-                new_data[k] = v.to(device=device)\n+                new_data[k] = v.to(device=device, non_blocking=non_blocking)\n             else:\n                 new_data[k] = v\n         self.data = new_data"
        },
        {
            "sha": "f4e5b9b3aaf3142eb0770467f8bf29aeb3cdb480",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/de8a0b7547451f8f7ef2c0ac1f338ba77c614cec/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de8a0b7547451f8f7ef2c0ac1f338ba77c614cec/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=de8a0b7547451f8f7ef2c0ac1f338ba77c614cec",
            "patch": "@@ -799,12 +799,13 @@ def as_tensor(value, dtype=None):\n \n         return self\n \n-    def to(self, device: Union[str, \"torch.device\"]) -> \"BatchEncoding\":\n+    def to(self, device: Union[str, \"torch.device\"], *, non_blocking: bool = False) -> \"BatchEncoding\":\n         \"\"\"\n-        Send all values to device by calling `v.to(device)` (PyTorch only).\n+        Send all values to device by calling `v.to(device, non_blocking=non_blocking)` (PyTorch only).\n \n         Args:\n             device (`str` or `torch.device`): The device to put the tensors on.\n+            non_blocking (`bool`): Whether to perform the copy asynchronously.\n \n         Returns:\n             [`BatchEncoding`]: The same instance after modification.\n@@ -816,7 +817,10 @@ def to(self, device: Union[str, \"torch.device\"]) -> \"BatchEncoding\":\n         # Otherwise it passes the casts down and casts the LongTensor containing the token idxs\n         # into a HalfTensor\n         if isinstance(device, str) or is_torch_device(device) or isinstance(device, int):\n-            self.data = {k: v.to(device=device) if isinstance(v, torch.Tensor) else v for k, v in self.data.items()}\n+            self.data = {\n+                k: v.to(device=device, non_blocking=non_blocking) if isinstance(v, torch.Tensor) else v\n+                for k, v in self.data.items()\n+            }\n         else:\n             logger.warning(f\"Attempting to cast a BatchEncoding to type {str(device)}. This is not supported.\")\n         return self"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 10,
        "deletions": 4
    }
}