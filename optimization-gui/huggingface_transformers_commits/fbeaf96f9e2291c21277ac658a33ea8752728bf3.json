{
    "author": "nlhmnlhmnlhm",
    "message": "Update OLMoE model card (#39344)\n\n* Update OLMoE model card\n\n* Checks Test\n\n* Add license and code\n\n* Update docs/source/en/model_doc/olmoe.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update olmoe.md\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "fbeaf96f9e2291c21277ac658a33ea8752728bf3",
    "files": [
        {
            "sha": "1db6853cecaac03aa55206ecebf6f90b674653bf",
            "filename": "docs/source/en/model_doc/olmoe.md",
            "status": "modified",
            "additions": 71,
            "deletions": 9,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbeaf96f9e2291c21277ac658a33ea8752728bf3/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbeaf96f9e2291c21277ac658a33ea8752728bf3/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md?ref=fbeaf96f9e2291c21277ac658a33ea8752728bf3",
            "patch": "@@ -14,27 +14,89 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# OLMoE\n-\n+<div style=\"float: right;\">\n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n+</div>\n+\n+# OLMoE\n+\n+[OLMoE](https://huggingface.co/papers/2409.02060) is a sparse Mixture-of-Experts (MoE) language model with 7B parameters but only 1B parameters are used per input token. It has similar inference costs as dense models but trains ~3x faster. OLMoE uses fine-grained routing with 64 small experts in each layer and uses a dropless token-based routing algorithm.\n+\n+You can find all the original OLMoE checkpoints under the [OLMoE](https://huggingface.co/collections/allenai/olmoe-november-2024-66cf678c047657a30c8cd3da) collection.\n+\n+> [!TIP]\n+> This model was contributed by [Muennighoff](https://hf.co/Muennighoff).\n+>\n+> Click on the OLMoE models in the right sidebar for more examples of how to apply OLMoE to different language tasks.\n+\n+The example below demonstrates how to generate text with [`Pipeline`] or the [`AutoModel`] class.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipe = pipeline(\n+    task=\"text-generation\",\n+    model=\"allenai/OLMoE-1B-7B-0125\",\n+    torch_dtype=torch.float16,\n+    device=0,\n+)\n+\n+result = pipe(\"Dionysus is the god of\")\n+print(result)\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMoE-1B-7B-0924\", attn_implementation=\"sdpa\", torch_dtype=\"auto\", device_map=\"auto\").to(device)\n+tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMoE-1B-7B-0924\")\n+\n+inputs = tokenizer(\"Bitcoin is\", return_tensors=\"pt\")\n+inputs = {k: v.to(device) for k, v in inputs.items()}\n+output = model.generate(**inputs, max_length=64)\n+print(tokenizer.decode(output[0]))\n+```\n \n-## Overview\n+## Quantization\n \n-The OLMoE model was proposed in [OLMoE: Open Mixture-of-Experts Language Models](https://huggingface.co/papers/2409.02060) by Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, Hannaneh Hajishirzi.\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quantize the weights to 4-bits.\n \n-OLMoE is a series of **O**pen **L**anguage **Mo**dels using sparse **M**ixture-**o**f-**E**xperts designed to enable the science of language models. We release all code, checkpoints, logs, and details involved in training these models.\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n \n-The abstract from the paper is the following:\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n \n-*We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.*\n+quantization_config = BitsAndBytesConfig(\n+   load_in_4bit=True,\n+   bnb_4bit_compute_dtype=torch.float16,\n+   bnb_4bit_use_double_quant=True,\n+   bnb_4bit_quant_type=\"nf4\"\n+)\n \n-This model was contributed by [Muennighoff](https://hf.co/Muennighoff).\n-The original code can be found [here](https://github.com/allenai/OLMoE).\n+model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMoE-1B-7B-0924\", attn_implementation=\"sdpa\", torch_dtype=\"auto\", device_map=\"auto\", quantization_config=quantization_config).to(device)\n+tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMoE-1B-7B-0924\")\n \n+inputs = tokenizer(\"Bitcoin is\", return_tensors=\"pt\")\n+inputs = {k: v.to(device) for k, v in inputs.items()}\n+output = model.generate(**inputs, max_length=64)\n+print(tokenizer.decode(output[0]))\n+```\n \n ## OlmoeConfig\n "
        }
    ],
    "stats": {
        "total": 80,
        "additions": 71,
        "deletions": 9
    }
}