{
    "author": "cyyever",
    "message": "Fix typos in tests and util (#40780)\n\nFix typos\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
    "files": [
        {
            "sha": "9fbd79464b17ca6582e18bf8cef8310391319c3e",
            "filename": "tests/commands/test_serving.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fcommands%2Ftest_serving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fcommands%2Ftest_serving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcommands%2Ftest_serving.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -72,7 +72,7 @@ def test_parsed_args(self):\n \n     def test_build_chat_completion_chunk(self):\n         \"\"\"\n-        Tests that the chunks are correctly built for the Chat Completion API. The `choices` checks implictly\n+        Tests that the chunks are correctly built for the Chat Completion API. The `choices` checks implicitly\n         confirm that empty fields are not emitted.\n         \"\"\"\n         dummy = ServeCommand.__new__(ServeCommand)"
        },
        {
            "sha": "b9527327b71d281426201d862c55c6139f8354bf",
            "filename": "tests/generation/test_stopping_criteria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fgeneration%2Ftest_stopping_criteria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fgeneration%2Ftest_stopping_criteria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_stopping_criteria.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -239,7 +239,7 @@ def test_single_letter_stop_string(self):\n         for input_ids in false_input_ids[\"input_ids\"]:\n             self.assertFalse(criteria(input_ids.unsqueeze(0), scores))\n \n-    def test_criterias_per_row(self):\n+    def test_criteria_per_row(self):\n         text = \"They completed the challenging puzzle, revealing the hidden image at the end\"\n         stop_strings = [\"end\"]\n \n@@ -261,7 +261,7 @@ def test_criterias_per_row(self):\n         # return False when neither is satisfied\n         self.assertFalse(criteria(inputs[\"input_ids\"][:, :-1], scores))\n \n-    def test_criterias_per_row_batched(self):\n+    def test_criteria_per_row_batched(self):\n         text = [\n             \"They completed the challenging puzzle, revealing the hidden image at the end\",\n             \"Today a dragon flew over France\","
        },
        {
            "sha": "bd0c9a2c76efae7159efacb245c291348a05f00b",
            "filename": "tests/models/auto/test_image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fauto%2Ftest_image_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fauto%2Ftest_image_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_image_processing_auto.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -91,7 +91,7 @@ def test_image_processor_from_local_directory_from_config(self):\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             model_config = CLIPConfig()\n \n-            # Create a dummy config file with image_proceesor_type\n+            # Create a dummy config file with image_processor_type\n             processor_tmpfile = Path(tmpdirname) / \"preprocessor_config.json\"\n             config_tmpfile = Path(tmpdirname) / \"config.json\"\n             json.dump("
        },
        {
            "sha": "06325f148c00810de394071a64d354155f0d98ff",
            "filename": "tests/models/auto/test_video_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fauto%2Ftest_video_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fauto%2Ftest_video_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_video_processing_auto.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -83,7 +83,7 @@ def test_video_processor_from_local_directory_from_config(self):\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             model_config = LlavaOnevisionConfig()\n \n-            # Create a dummy config file with image_proceesor_type\n+            # Create a dummy config file with image_processor_type\n             processor_tmpfile = Path(tmpdirname) / \"video_preprocessor_config.json\"\n             config_tmpfile = Path(tmpdirname) / \"config.json\"\n             json.dump("
        },
        {
            "sha": "9651efbde21d1bdfbba366ee1759117dfdbfbcd1",
            "filename": "tests/models/bart/test_tokenization_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fbart%2Ftest_tokenization_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fbart%2Ftest_tokenization_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_tokenization_bart.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -157,7 +157,7 @@ def test_special_tokens(self):\n     def test_pretokenized_inputs(self):\n         pass\n \n-    def test_embeded_special_tokens(self):\n+    def test_embedded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)"
        },
        {
            "sha": "b9d46dea3a556f3ca542d4f24db4e6073dd1a4f6",
            "filename": "tests/models/bert/test_tokenization_bert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fbert%2Ftest_tokenization_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fbert%2Ftest_tokenization_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_tokenization_bert.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -307,8 +307,8 @@ def test_offsets_with_special_characters(self):\n                 self.assertEqual([e[0] for e in expected_results], tokens[\"offset_mapping\"])\n \n     def test_change_tokenize_chinese_chars(self):\n-        list_of_commun_chinese_char = [\"的\", \"人\", \"有\"]\n-        text_with_chinese_char = \"\".join(list_of_commun_chinese_char)\n+        list_of_common_chinese_char = [\"的\", \"人\", \"有\"]\n+        text_with_chinese_char = \"\".join(list_of_common_chinese_char)\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 kwargs[\"tokenize_chinese_chars\"] = True\n@@ -322,8 +322,8 @@ def test_change_tokenize_chinese_chars(self):\n                 tokens_without_spe_char_p = tokenizer_p.convert_ids_to_tokens(ids_without_spe_char_p)\n \n                 # it is expected that each Chinese character is not preceded by \"##\"\n-                self.assertListEqual(tokens_without_spe_char_p, list_of_commun_chinese_char)\n-                self.assertListEqual(tokens_without_spe_char_r, list_of_commun_chinese_char)\n+                self.assertListEqual(tokens_without_spe_char_p, list_of_common_chinese_char)\n+                self.assertListEqual(tokens_without_spe_char_r, list_of_common_chinese_char)\n \n                 kwargs[\"tokenize_chinese_chars\"] = False\n                 tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n@@ -337,7 +337,7 @@ def test_change_tokenize_chinese_chars(self):\n \n                 # it is expected that only the first Chinese character is not preceded by \"##\".\n                 expected_tokens = [\n-                    f\"##{token}\" if idx != 0 else token for idx, token in enumerate(list_of_commun_chinese_char)\n+                    f\"##{token}\" if idx != 0 else token for idx, token in enumerate(list_of_common_chinese_char)\n                 ]\n                 self.assertListEqual(tokens_without_spe_char_p, expected_tokens)\n                 self.assertListEqual(tokens_without_spe_char_r, expected_tokens)"
        },
        {
            "sha": "0b3ab74d519cfb6f9ff157bf1812682bce8df914",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -513,7 +513,7 @@ def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n         This tests only by looking at layer names, as usually SDPA layers are called \"SDPAAttention\".\n-        In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n+        In contrast to the above test, this one checks if the \"config._attn_implementation\" is a dict after the model\n         is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n         See https://github.com/huggingface/transformers/pull/32238 for more info\n \n@@ -949,7 +949,7 @@ def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n         This tests only by looking at layer names, as usually SDPA layers are called \"SDPAAttention\".\n-        In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n+        In contrast to the above test, this one checks if the \"config._attn_implementation\" is a dict after the model\n         is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n         See https://github.com/huggingface/transformers/pull/32238 for more info\n "
        },
        {
            "sha": "930f2504dee84f9f984c2a4f52a65b6fcee17075",
            "filename": "tests/models/deepseek_v2/test_modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -98,7 +98,9 @@ def test_model_rope_scaling_frequencies(self):\n         long_input_length = int(config.max_position_embeddings * 1.5)\n \n         # Inputs\n-        x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n+        x = torch.randn(\n+            1, dtype=torch.float32, device=torch_device\n+        )  # used exclusively to get the dtype and the device\n         position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n         position_ids_short = position_ids_short.unsqueeze(0)\n         position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)\n@@ -161,7 +163,7 @@ def test_past_key_values_format(self):\n         super().test_past_key_values_format(custom_all_cache_shapes=all_cache_shapes)\n \n     def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n-        \"\"\"Needs to be overriden as deepseek has special MLA cache format (though we don't really use the MLA)\"\"\"\n+        \"\"\"Needs to be overridden as deepseek has special MLA cache format (though we don't really use the MLA)\"\"\"\n         self.assertIsInstance(decoder_past_key_values, Cache)\n \n         # (batch, head, seq_length, head_features)"
        },
        {
            "sha": "989608d686eae5bb404c3357d487ab94a5a45db9",
            "filename": "tests/models/dia/test_modeling_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -250,7 +250,7 @@ def skip_non_greedy_generate(self):\n                 self.skipTest(reason=\"Dia only supports greedy search / sampling with one sequence.\")\n \n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n-        \"\"\"Overriden to account for the 2D flattened structure\"\"\"\n+        \"\"\"Overridden to account for the 2D flattened structure\"\"\"\n         inputs_dict = copy.deepcopy(inputs_dict)\n \n         if return_labels:"
        },
        {
            "sha": "f2ac66e21ae984556ea4d22d36c3c133fcf0a6bc",
            "filename": "tests/models/electra/test_tokenization_electra.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Felectra%2Ftest_tokenization_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Felectra%2Ftest_tokenization_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Felectra%2Ftest_tokenization_electra.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -300,8 +300,8 @@ def test_offsets_with_special_characters(self):\n                 self.assertEqual([e[0] for e in expected_results], tokens[\"offset_mapping\"])\n \n     def test_change_tokenize_chinese_chars(self):\n-        list_of_commun_chinese_char = [\"的\", \"人\", \"有\"]\n-        text_with_chinese_char = \"\".join(list_of_commun_chinese_char)\n+        list_of_common_chinese_char = [\"的\", \"人\", \"有\"]\n+        text_with_chinese_char = \"\".join(list_of_common_chinese_char)\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 kwargs[\"tokenize_chinese_chars\"] = True\n@@ -315,8 +315,8 @@ def test_change_tokenize_chinese_chars(self):\n                 tokens_without_spe_char_p = tokenizer_p.convert_ids_to_tokens(ids_without_spe_char_p)\n \n                 # it is expected that each Chinese character is not preceded by \"##\"\n-                self.assertListEqual(tokens_without_spe_char_p, list_of_commun_chinese_char)\n-                self.assertListEqual(tokens_without_spe_char_r, list_of_commun_chinese_char)\n+                self.assertListEqual(tokens_without_spe_char_p, list_of_common_chinese_char)\n+                self.assertListEqual(tokens_without_spe_char_r, list_of_common_chinese_char)\n \n                 kwargs[\"tokenize_chinese_chars\"] = False\n                 tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n@@ -330,7 +330,7 @@ def test_change_tokenize_chinese_chars(self):\n \n                 # it is expected that only the first Chinese character is not preceded by \"##\".\n                 expected_tokens = [\n-                    f\"##{token}\" if idx != 0 else token for idx, token in enumerate(list_of_commun_chinese_char)\n+                    f\"##{token}\" if idx != 0 else token for idx, token in enumerate(list_of_common_chinese_char)\n                 ]\n                 self.assertListEqual(tokens_without_spe_char_p, expected_tokens)\n                 self.assertListEqual(tokens_without_spe_char_r, expected_tokens)"
        },
        {
            "sha": "28ef2eeb8b57c14b926227eccf32fb81fa40157a",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -387,7 +387,7 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n             self.skipTest(\"FlashAttention2 is required for this test.\")\n \n         if torch_device == \"xpu\" and attn_implementation == \"flash_attention_2\":\n-            self.skipTest(reason=\"Intel XPU doesn't support falsh_attention_2 as of now.\")\n+            self.skipTest(reason=\"Intel XPU doesn't support flash_attention_2 as of now.\")\n \n         model_id = \"google/gemma-2-2b\"\n         EXPECTED_COMPLETIONS = [\n@@ -433,7 +433,7 @@ def test_generation_beyond_sliding_window_dynamic(self, attn_implementation: str\n             self.skipTest(\"FlashAttention2 is required for this test.\")\n \n         if torch_device == \"xpu\" and attn_implementation == \"flash_attention_2\":\n-            self.skipTest(reason=\"Intel XPU doesn't support falsh_attention_2 as of now.\")\n+            self.skipTest(reason=\"Intel XPU doesn't support flash_attention_2 as of now.\")\n \n         model_id = \"google/gemma-2-2b\"\n         EXPECTED_COMPLETIONS = ["
        },
        {
            "sha": "4802604efb55bda85095081f5572b9b3617182e3",
            "filename": "tests/models/grounding_dino/test_modeling_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -638,7 +638,7 @@ def test_tied_weights_keys(self):\n \n             # GroundingDino when sharing weights also uses the shared ones in GroundingDinoDecoder\n             # Therefore, differently from DeformableDetr, we expect the group lens to be 2\n-            # one for self.bbox_embed in GroundingDinoForObejectDetection and another one\n+            # one for self.bbox_embed in GroundingDinoForObjectDetection and another one\n             # in the decoder\n             tied_params = [group for group in tied_params if len(group) > 2]\n             self.assertListEqual("
        },
        {
            "sha": "a91d31082da9595301b16dbbfca3cac395ec760a",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -669,8 +669,8 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n-        This tests only by looking at layer names, as usually SDPA layers are calles \"SDPAAttention\".\n-        In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n+        This tests only by looking at layer names, as usually SDPA layers call \"SDPAAttention\".\n+        In contrast to the above test, this one checks if the \"config._attn_implementation\" is a dict after the model\n         is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n         See https://github.com/huggingface/transformers/pull/32238 for more info\n "
        },
        {
            "sha": "ac16e62c55f3639febd7ada411965c7f5bce2f1e",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -481,7 +481,7 @@ def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n \n     @pytest.mark.generate\n     def test_left_padding_compatibility(self):\n-        # Overwrite because Kosmos-2 need to padd pixel values and pad image-attn-mask\n+        # Overwrite because Kosmos-2 need to pad pixel values and pad image-attn-mask\n \n         def _prepare_model_kwargs(input_ids, attention_mask, pad_size, signature):\n             model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}"
        },
        {
            "sha": "c2a18cb5b690cf79747cb357b0e08e3f1556c7c6",
            "filename": "tests/models/kosmos2_5/test_modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -570,7 +570,7 @@ def test_generate_from_inputs_embeds(self):\n \n     @pytest.mark.generate\n     def test_left_padding_compatibility(self):\n-        # Overwrite because Kosmos-2.5 need to padd pixel values and pad image-attn-mask\n+        # Overwrite because Kosmos-2.5 need to pad pixel values and pad image-attn-mask\n \n         def _prepare_model_kwargs(input_ids, attention_mask, pad_size, signature):\n             model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}"
        },
        {
            "sha": "c87e345a542de9ee0db44e8b68af2e28fd6dcaff",
            "filename": "tests/models/layoutlmv2/test_tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -1337,7 +1337,7 @@ def test_tokenization_python_rust_equals(self):\n                 ):\n                     self.assertSequenceEqual(input_p[key], input_r[key][0])\n \n-    def test_embeded_special_tokens(self):\n+    def test_embedded_special_tokens(self):\n         if not self.test_slow_tokenizer:\n             # as we don't have a slow version, we can't compare the outputs between slow and fast versions\n             self.skipTest(reason=\"test_slow_tokenizer is set to False\")\n@@ -1733,7 +1733,7 @@ def test_batch_encode_dynamic_overflowing(self):\n                         self.assertEqual(tokens[key].shape[-1], 4)\n \n     @unittest.skip(reason=\"TO DO: overwrite this very extensive test.\")\n-    def test_alignement_methods(self):\n+    def test_alignment_methods(self):\n         pass\n \n     def get_clean_sequence(self, tokenizer, with_prefix_space=False, max_length=20, min_length=5):"
        },
        {
            "sha": "ae484e7459dc6787843cbfe457b7f7b494c84a92",
            "filename": "tests/models/layoutlmv3/test_tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -1222,7 +1222,7 @@ def test_tokenization_python_rust_equals(self):\n                 ):\n                     self.assertSequenceEqual(input_p[key], input_r[key][0])\n \n-    def test_embeded_special_tokens(self):\n+    def test_embedded_special_tokens(self):\n         if not self.test_slow_tokenizer:\n             # as we don't have a slow version, we can't compare the outputs between slow and fast versions\n             self.skipTest(reason=\"test_slow_tokenizer is set to False\")\n@@ -1623,7 +1623,7 @@ def test_batch_encode_dynamic_overflowing(self):\n                         self.assertEqual(tokens[key].shape[-1], 4)\n \n     @unittest.skip(reason=\"TO DO: overwrite this very extensive test.\")\n-    def test_alignement_methods(self):\n+    def test_alignment_methods(self):\n         pass\n \n     def get_clean_sequence(self, tokenizer, with_prefix_space=False, max_length=20, min_length=5):"
        },
        {
            "sha": "185525702c633cc4f7f252ba6379f40e1c8b58e5",
            "filename": "tests/models/layoutxlm/test_tokenization_layoutxlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -1266,7 +1266,7 @@ def test_tokenization_python_rust_equals(self):\n                 ):\n                     self.assertSequenceEqual(input_p[key], input_r[key][0])\n \n-    def test_embeded_special_tokens(self):\n+    def test_embedded_special_tokens(self):\n         if not self.test_slow_tokenizer:\n             # as we don't have a slow version, we can't compare the outputs between slow and fast versions\n             self.skipTest(reason=\"test_slow_tokenizer is set to False\")\n@@ -1734,7 +1734,7 @@ def test_save_pretrained(self):\n                 shutil.rmtree(tmpdirname2)\n \n     @unittest.skip(reason=\"TO DO: overwrite this very extensive test.\")\n-    def test_alignement_methods(self):\n+    def test_alignment_methods(self):\n         pass\n \n     @unittest.skip(reason=\"layoutxlm tokenizer requires boxes besides sequences.\")"
        },
        {
            "sha": "a8e47955dfa17c91574436b9b6b307861ec41aa2",
            "filename": "tests/models/led/test_modeling_led.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fled%2Ftest_modeling_led.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -515,7 +515,7 @@ def _long_tensor(tok_lst):\n class LEDModelIntegrationTests(unittest.TestCase):\n     \"\"\"All the below results were obtained with the original checkpoints and code\n     base from https://github.com/allenai/longformer.\n-    IMPORTANT: Note that the original checkpoints include a `postion_embeddings` \"hack\"\n+    IMPORTANT: Note that the original checkpoints include a `position_embeddings` \"hack\"\n     and have to be cut to have the correct shape.\n     See: https://github.com/huggingface/transformers/pull/9278#issue-544709661.\n     \"\"\""
        },
        {
            "sha": "6d647c4785e969449d81e9e63021d099c863c8ff",
            "filename": "tests/models/led/test_tokenization_led.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fled%2Ftest_tokenization_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fled%2Ftest_tokenization_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fled%2Ftest_tokenization_led.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -164,7 +164,7 @@ def test_global_attention_mask(self):\n     def test_pretokenized_inputs(self):\n         pass\n \n-    def test_embeded_special_tokens(self):\n+    def test_embedded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)"
        },
        {
            "sha": "d1b4447930fc3f9751735ff5aeba9f92172dfd08",
            "filename": "tests/models/longformer/test_tokenization_longformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Flongformer%2Ftest_tokenization_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Flongformer%2Ftest_tokenization_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongformer%2Ftest_tokenization_longformer.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -174,7 +174,7 @@ def test_space_encoding(self):\n     def test_pretokenized_inputs(self):\n         pass\n \n-    def test_embeded_special_tokens(self):\n+    def test_embedded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)"
        },
        {
            "sha": "d85075d2ee291fb969cf8c4e765d1684bd2d8148",
            "filename": "tests/models/luke/test_tokenization_luke.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fluke%2Ftest_tokenization_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fluke%2Ftest_tokenization_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fluke%2Ftest_tokenization_luke.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -134,7 +134,7 @@ def test_space_encoding(self):\n     def test_pretokenized_inputs(self):\n         pass\n \n-    def test_embeded_special_tokens(self):\n+    def test_embedded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)"
        },
        {
            "sha": "253f525faa63291d205c68b99ded3d0b2a579a90",
            "filename": "tests/models/markuplm/test_tokenization_markuplm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -1107,7 +1107,7 @@ def test_tokenization_python_rust_equals(self):\n                 ):\n                     self.assertSequenceEqual(input_p[key], input_r[key][0])\n \n-    def test_embeded_special_tokens(self):\n+    def test_embedded_special_tokens(self):\n         if not self.test_slow_tokenizer:\n             # as we don't have a slow version, we can't compare the outputs between slow and fast versions\n             self.skipTest(reason=\"test_slow_tokenizer is set to False\")\n@@ -1508,7 +1508,7 @@ def test_batch_encode_dynamic_overflowing(self):\n                         self.assertEqual(tokens[key].shape[-2], 6)\n \n     @unittest.skip(reason=\"TO DO: overwrite this very extensive test.\")\n-    def test_alignement_methods(self):\n+    def test_alignment_methods(self):\n         pass\n \n     def get_clean_sequence(self, tokenizer, with_prefix_space=False, max_length=20, min_length=5):"
        },
        {
            "sha": "439a111db8f2f87d70fe6b2bc59e4efae795d33a",
            "filename": "tests/models/mask2former/test_image_processing_mask2former.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -477,16 +477,16 @@ def test_binary_mask_to_rle(self):\n \n     def test_post_process_semantic_segmentation(self):\n         for image_processing_class in self.image_processor_list:\n-            fature_extractor = image_processing_class(num_labels=self.image_processor_tester.num_classes)\n+            feature_extractor = image_processing_class(num_labels=self.image_processor_tester.num_classes)\n             outputs = self.image_processor_tester.get_fake_mask2former_outputs()\n \n-            segmentation = fature_extractor.post_process_semantic_segmentation(outputs)\n+            segmentation = feature_extractor.post_process_semantic_segmentation(outputs)\n \n             self.assertEqual(len(segmentation), self.image_processor_tester.batch_size)\n             self.assertEqual(segmentation[0].shape, (384, 384))\n \n             target_sizes = [(1, 4) for i in range(self.image_processor_tester.batch_size)]\n-            segmentation = fature_extractor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n+            segmentation = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n \n             self.assertEqual(segmentation[0].shape, target_sizes[0])\n "
        },
        {
            "sha": "39b6f94e6aea7bd091bd1daa697c7ba5d6d22262",
            "filename": "tests/models/mluke/test_tokenization_mluke.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fmluke%2Ftest_tokenization_mluke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fmluke%2Ftest_tokenization_mluke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmluke%2Ftest_tokenization_mluke.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -97,7 +97,7 @@ def get_clean_sequence(self, tokenizer, max_length=20) -> tuple[str, list]:\n     def test_pretokenized_inputs(self):\n         pass\n \n-    def test_embeded_special_tokens(self):\n+    def test_embedded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)"
        },
        {
            "sha": "a84c4bb17078aaf747337b927ee4b47dbbbabf80",
            "filename": "tests/models/mm_grounding_dino/test_modeling_mm_grounding_dino.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fmm_grounding_dino%2Ftest_modeling_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fmm_grounding_dino%2Ftest_modeling_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmm_grounding_dino%2Ftest_modeling_mm_grounding_dino.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -645,7 +645,7 @@ def test_tied_weights_keys(self):\n \n             # MMGroundingDino when sharing weights also uses the shared ones in MMGroundingDinoDecoder\n             # Therefore, differently from DeformableDetr, we expect the group lens to be 2\n-            # one for self.bbox_embed in MMGroundingDinoForObejectDetection and another one\n+            # one for self.bbox_embed in MMGroundingDinoForObjectDetection and another one\n             # in the decoder\n             tied_params = [group for group in tied_params if len(group) > 2]\n             self.assertListEqual(\n@@ -746,7 +746,7 @@ def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n         )\n         # HACK: the issue happens during top-k (k=900) after the encoder\n         # there are some flips between cpu and gpu query ordering (idxs 195<->196 and 267<->268 on my machine)\n-        # which causes different query position embedding assingments\n+        # which causes different query position embedding assignments\n         # which in turn significantly changes the decoder pass due to self attention\n         model.config.num_queries = 100\n         model.model.query_position_embeddings.weight.data = model.model.query_position_embeddings.weight.data[:100]\n@@ -788,7 +788,7 @@ def test_cross_attention_mask(self):\n         ).to(torch_device)\n         # HACK: the issue happens during top-k (k=900) after the encoder\n         # there are some flips between cpu and gpu query ordering\n-        # which causes different query position embedding assingments\n+        # which causes different query position embedding assignments\n         # which in turn significantly changes the decoder pass due to self attention\n         model.config.num_queries = 100\n         model.model.query_position_embeddings.weight.data = model.model.query_position_embeddings.weight.data[:100]"
        },
        {
            "sha": "c0a179a8ff1c7a8cb4112a85ae7eec01b37a799a",
            "filename": "tests/models/mobilebert/test_tokenization_mobilebert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fmobilebert%2Ftest_tokenization_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fmobilebert%2Ftest_tokenization_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilebert%2Ftest_tokenization_mobilebert.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -326,8 +326,8 @@ def test_offsets_with_special_characters(self):\n \n     # Copied from tests.models.bert.test_tokenization_bert.BertTokenizationTest.test_change_tokenize_chinese_chars\n     def test_change_tokenize_chinese_chars(self):\n-        list_of_commun_chinese_char = [\"的\", \"人\", \"有\"]\n-        text_with_chinese_char = \"\".join(list_of_commun_chinese_char)\n+        list_of_common_chinese_char = [\"的\", \"人\", \"有\"]\n+        text_with_chinese_char = \"\".join(list_of_common_chinese_char)\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 kwargs[\"tokenize_chinese_chars\"] = True\n@@ -341,8 +341,8 @@ def test_change_tokenize_chinese_chars(self):\n                 tokens_without_spe_char_p = tokenizer_p.convert_ids_to_tokens(ids_without_spe_char_p)\n \n                 # it is expected that each Chinese character is not preceded by \"##\"\n-                self.assertListEqual(tokens_without_spe_char_p, list_of_commun_chinese_char)\n-                self.assertListEqual(tokens_without_spe_char_r, list_of_commun_chinese_char)\n+                self.assertListEqual(tokens_without_spe_char_p, list_of_common_chinese_char)\n+                self.assertListEqual(tokens_without_spe_char_r, list_of_common_chinese_char)\n \n                 kwargs[\"tokenize_chinese_chars\"] = False\n                 tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n@@ -356,7 +356,7 @@ def test_change_tokenize_chinese_chars(self):\n \n                 # it is expected that only the first Chinese character is not preceded by \"##\".\n                 expected_tokens = [\n-                    f\"##{token}\" if idx != 0 else token for idx, token in enumerate(list_of_commun_chinese_char)\n+                    f\"##{token}\" if idx != 0 else token for idx, token in enumerate(list_of_common_chinese_char)\n                 ]\n                 self.assertListEqual(tokens_without_spe_char_p, expected_tokens)\n                 self.assertListEqual(tokens_without_spe_char_r, expected_tokens)"
        },
        {
            "sha": "2b6030f6d7924584d4cce8bb043e6249c40ae136",
            "filename": "tests/models/moshi/test_tokenization_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fmoshi%2Ftest_tokenization_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fmoshi%2Ftest_tokenization_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_tokenization_moshi.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -312,7 +312,7 @@ def test_training_new_tokenizer_with_special_tokens_change(self):\n \n         self.assertEqual(expected_result, decoded_input)\n \n-    def test_alignement_methods(self):\n+    def test_alignment_methods(self):\n         # TODO: @ArthurZucker - alignment is broken\n         pass\n "
        },
        {
            "sha": "4ba264388e2df829e865db311dbf8a54f7a19043",
            "filename": "tests/models/mvp/test_tokenization_mvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fmvp%2Ftest_tokenization_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fmvp%2Ftest_tokenization_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmvp%2Ftest_tokenization_mvp.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -156,7 +156,7 @@ def test_special_tokens(self):\n     def test_pretokenized_inputs(self):\n         pass\n \n-    def test_embeded_special_tokens(self):\n+    def test_embedded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)"
        },
        {
            "sha": "2a4f4ad6139be34224d13fa0f7cf4b1919c25bb1",
            "filename": "tests/models/nougat/test_image_processing_nougat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -315,7 +315,7 @@ def test_slow_fast_equivalence(self):\n \n         encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n         encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n-        # Adding a larget than usual tolerance because the slow processor uses reducing_gap=2.0 during resizing.\n+        # Adding a larger than usual tolerance because the slow processor uses reducing_gap=2.0 during resizing.\n         torch.testing.assert_close(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=2e-1, rtol=0)\n         self.assertLessEqual(\n             torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 2e-2"
        },
        {
            "sha": "4fe89959bf0beb17c08783f8d713172fec5204d6",
            "filename": "tests/models/oneformer/test_image_processing_oneformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Foneformer%2Ftest_image_processing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Foneformer%2Ftest_image_processing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Foneformer%2Ftest_image_processing_oneformer.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -273,7 +273,7 @@ def test_binary_mask_to_rle(self):\n \n     def test_post_process_semantic_segmentation(self):\n         for image_processing_class in self.image_processor_list:\n-            fature_extractor = image_processing_class(\n+            feature_extractor = image_processing_class(\n                 num_labels=self.image_processor_tester.num_classes,\n                 max_seq_length=77,\n                 task_seq_length=77,\n@@ -283,7 +283,7 @@ def test_post_process_semantic_segmentation(self):\n             )\n             outputs = self.image_processor_tester.get_fake_oneformer_outputs()\n \n-            segmentation = fature_extractor.post_process_semantic_segmentation(outputs)\n+            segmentation = feature_extractor.post_process_semantic_segmentation(outputs)\n \n             self.assertEqual(len(segmentation), self.image_processor_tester.batch_size)\n             self.assertEqual(\n@@ -295,7 +295,7 @@ def test_post_process_semantic_segmentation(self):\n             )\n \n             target_sizes = [(1, 4) for i in range(self.image_processor_tester.batch_size)]\n-            segmentation = fature_extractor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n+            segmentation = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n \n             self.assertEqual(segmentation[0].shape, target_sizes[0])\n "
        },
        {
            "sha": "6cbdba8e26c044602c4f91d61f922993950cf852",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -235,7 +235,7 @@ def test_mismatching_num_image_tokens(self):\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n             curr_input_dict = copy.deepcopy(input_dict)\n-            _ = model(**curr_input_dict)  # successfull forward with no modifications\n+            _ = model(**curr_input_dict)  # successful forward with no modifications\n \n             # remove one image but leave the image token in text\n             patch_size = config.vision_config.patch_size"
        },
        {
            "sha": "a9e8007347126e9804a7dd4cadcce6a164cfaee9",
            "filename": "tests/models/qwen2_vl/test_video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -156,7 +156,7 @@ def test_video_processor_properties(self):\n         self.assertTrue(hasattr(video_processing, \"image_std\"))\n         self.assertTrue(hasattr(video_processing, \"do_convert_rgb\"))\n \n-    # OVERRIDEN BECAUSE QWEN2_VL HAS SPECIAL OUTPUT SHAPES\n+    # OVERRIDDEN BECAUSE QWEN2_VL HAS SPECIAL OUTPUT SHAPES\n     def test_video_processor_from_dict_with_kwargs(self):\n         for video_processing_class in self.video_processor_list:\n             video_processor = video_processing_class(**self.video_processor_dict)"
        },
        {
            "sha": "8f2b1cdc995748b6a5489ae1514f1a12b231ddf6",
            "filename": "tests/models/reformer/test_modeling_reformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -620,7 +620,7 @@ def test_model_from_pretrained(self):\n     def _check_attentions_for_generate(\n         self, batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n     ):\n-        # NOTE (joao): this function is substancially different from the original, the attention has different\n+        # NOTE (joao): this function is substantially different from the original, the attention has different\n         # *number* of shapes in certain conditions\n         self.assertIsInstance(attentions, tuple)\n         self.assertListEqual(\n@@ -663,7 +663,7 @@ def _check_attentions_for_generate(\n     def _check_hidden_states_for_generate(\n         self, batch_size, hidden_states, prompt_length, output_length, config, use_cache=False\n     ):\n-        # NOTE (joao): this function is substancially different from the original, the hidden states have different\n+        # NOTE (joao): this function is substantially different from the original, the hidden states have different\n         # length in certain conditions\n         self.assertIsInstance(hidden_states, tuple)\n         self.assertListEqual(\n@@ -798,7 +798,7 @@ def setUp(self):\n     def _check_attentions_for_generate(\n         self, batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n     ):\n-        # NOTE (joao): this function is substancially different from the original, the attention has different\n+        # NOTE (joao): this function is substantially different from the original, the attention has different\n         # *number* of shapes in certain conditions\n         self.assertIsInstance(attentions, tuple)\n         self.assertListEqual(\n@@ -841,7 +841,7 @@ def _check_attentions_for_generate(\n     def _check_hidden_states_for_generate(\n         self, batch_size, hidden_states, prompt_length, output_length, config, use_cache=False\n     ):\n-        # NOTE (joao): this function is substancially different from the original, the hidden states have different\n+        # NOTE (joao): this function is substantially different from the original, the hidden states have different\n         # length in certain conditions\n         self.assertIsInstance(hidden_states, tuple)\n         self.assertListEqual("
        },
        {
            "sha": "bf55614d4c87bfd50bb9813cd1487eb3b4a165c6",
            "filename": "tests/models/roberta/test_tokenization_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Froberta%2Ftest_tokenization_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Froberta%2Ftest_tokenization_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_tokenization_roberta.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -172,7 +172,7 @@ def test_space_encoding(self):\n     def test_pretokenized_inputs(self):\n         pass\n \n-    def test_embeded_special_tokens(self):\n+    def test_embedded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)"
        },
        {
            "sha": "3b7c4d0eb2280f7ded1f67a226f55be642c9758e",
            "filename": "tests/models/roc_bert/test_tokenization_roc_bert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Froc_bert%2Ftest_tokenization_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Froc_bert%2Ftest_tokenization_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froc_bert%2Ftest_tokenization_roc_bert.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -255,8 +255,8 @@ def test_offsets_with_special_characters(self):\n \n     # Copied from tests.models.bert.test_tokenization_bert.BertTokenizationTest.test_change_tokenize_chinese_chars\n     def test_change_tokenize_chinese_chars(self):\n-        list_of_commun_chinese_char = [\"的\", \"人\", \"有\"]\n-        text_with_chinese_char = \"\".join(list_of_commun_chinese_char)\n+        list_of_common_chinese_char = [\"的\", \"人\", \"有\"]\n+        text_with_chinese_char = \"\".join(list_of_common_chinese_char)\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 kwargs[\"tokenize_chinese_chars\"] = True\n@@ -270,8 +270,8 @@ def test_change_tokenize_chinese_chars(self):\n                 tokens_without_spe_char_p = tokenizer_p.convert_ids_to_tokens(ids_without_spe_char_p)\n \n                 # it is expected that each Chinese character is not preceded by \"##\"\n-                self.assertListEqual(tokens_without_spe_char_p, list_of_commun_chinese_char)\n-                self.assertListEqual(tokens_without_spe_char_r, list_of_commun_chinese_char)\n+                self.assertListEqual(tokens_without_spe_char_p, list_of_common_chinese_char)\n+                self.assertListEqual(tokens_without_spe_char_r, list_of_common_chinese_char)\n \n                 kwargs[\"tokenize_chinese_chars\"] = False\n                 tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n@@ -285,7 +285,7 @@ def test_change_tokenize_chinese_chars(self):\n \n                 # it is expected that only the first Chinese character is not preceded by \"##\".\n                 expected_tokens = [\n-                    f\"##{token}\" if idx != 0 else token for idx, token in enumerate(list_of_commun_chinese_char)\n+                    f\"##{token}\" if idx != 0 else token for idx, token in enumerate(list_of_common_chinese_char)\n                 ]\n                 self.assertListEqual(tokens_without_spe_char_p, expected_tokens)\n                 self.assertListEqual(tokens_without_spe_char_r, expected_tokens)"
        },
        {
            "sha": "5923ce5bc8a5bd8af24fb55837ab4fb8615a5c3f",
            "filename": "tests/models/sam/test_modeling_sam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -666,7 +666,7 @@ def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n         This tests only by looking at layer names, as usually SDPA layers are called \"SDPAAttention\".\n-        In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n+        In contrast to the above test, this one checks if the \"config._attn_implementation\" is a dict after the model\n         is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n         See https://github.com/huggingface/transformers/pull/32238 for more info\n "
        },
        {
            "sha": "b1456a3eb2731c935ddd466b6b87ef9682b83a0d",
            "filename": "tests/models/sam2/test_modeling_sam2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -563,7 +563,7 @@ def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n         This tests only by looking at layer names, as usually SDPA layers are called \"SDPAAttention\".\n-        In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n+        In contrast to the above test, this one checks if the \"config._attn_implementation\" is a dict after the model\n         is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n         See https://github.com/huggingface/transformers/pull/32238 for more info\n \n@@ -696,7 +696,7 @@ def flash_attn_inference_equivalence(self, attn_implementation: str, padding_sid\n                 else:\n                     assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n \n-    # Override as diffence slightly higher than the threshold\n+    # Override as difference slightly higher than the threshold\n     def test_batching_equivalence(self, atol=5e-4, rtol=5e-4):\n         super().test_batching_equivalence(atol=atol, rtol=rtol)\n "
        },
        {
            "sha": "d008b788f6addfb746d85a2fe15deebe4da7facd",
            "filename": "tests/models/sam_hq/test_modeling_sam_hq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -714,7 +714,7 @@ def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n         This tests only by looking at layer names, as usually SDPA layers are calles \"SDPAAttention\".\n-        In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n+        In contrast to the above test, this one checks if the \"config._attn_implementation\" is a dict after the model\n         is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n         See https://github.com/huggingface/transformers/pull/32238 for more info\n "
        },
        {
            "sha": "5ca837bd9f8ea5e2a732bfb0893f826e10df07ac",
            "filename": "tests/models/udop/test_tokenization_udop.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -1210,7 +1210,7 @@ def test_tokenization_python_rust_equals(self):\n                 ):\n                     self.assertSequenceEqual(input_p[key], input_r[key][0])\n \n-    def test_embeded_special_tokens(self):\n+    def test_embedded_special_tokens(self):\n         if not self.test_slow_tokenizer:\n             # as we don't have a slow version, we can't compare the outputs between slow and fast versions\n             self.skipTest(reason=\"test_slow_tokenizer is set to False\")\n@@ -1606,7 +1606,7 @@ def test_batch_encode_dynamic_overflowing(self):\n                         self.assertEqual(tokens[key].shape[-1], 4)\n \n     @unittest.skip(reason=\"TO DO: overwrite this very extensive test.\")\n-    def test_alignement_methods(self):\n+    def test_alignment_methods(self):\n         pass\n \n     @unittest.skip(reason=\"UDOP tokenizer requires boxes besides sequences.\")"
        },
        {
            "sha": "426dfe96021ec6c78398eae318bb00ff10c6a16c",
            "filename": "tests/models/umt5/test_modeling_umt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -144,7 +144,7 @@ def prepare_config_and_inputs(self):\n         # all pad tokens have pos id = 2 and rest are between 2..seq_length\n         # and the seq_length here is seq_length - num_pad_tokens\n         # but when using past, there is no way of knowing if the past input ids had\n-        # pad tokens in them, which results in incorrect seq_lenth and which in turn results in\n+        # pad tokens in them, which results in incorrect seq_length and which in turn results in\n         # position_ids being off by num_pad_tokens in past input\n         input_ids = input_ids.clamp(self.pad_token_id + 2)\n         input_ids[:, -1] = self.eos_token_id  # Eos Token"
        },
        {
            "sha": "3325d29f16aaa532b14fa45cb7072604cd9adc14",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -1404,8 +1404,8 @@ def test_small_en_logits_librispeech(self):\n \n         input_speech = self._load_datasamples(1)\n \n-        feaure_extractor = WhisperFeatureExtractor()\n-        input_features = feaure_extractor(input_speech, return_tensors=\"pt\").input_features.to(torch_device)\n+        feature_extractor = WhisperFeatureExtractor()\n+        input_features = feature_extractor(input_speech, return_tensors=\"pt\").input_features.to(torch_device)\n \n         logits = model(\n             input_features,"
        },
        {
            "sha": "3fd8f7b5c44a14ba853acdcd97a04d84941d08fb",
            "filename": "tests/pipelines/test_pipelines_mask_generation.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fpipelines%2Ftest_pipelines_mask_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fpipelines%2Ftest_pipelines_mask_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_mask_generation.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -110,9 +110,9 @@ def test_small_model_pt(self):\n         outputs = image_segmenter(\"http://images.cocodataset.org/val2017/000000039769.jpg\", points_per_batch=256)\n \n         # Shortening by hashing\n-        new_outupt = []\n+        new_output = []\n         for i, o in enumerate(outputs[\"masks\"]):\n-            new_outupt += [{\"mask\": mask_to_test_readable(o), \"scores\": outputs[\"scores\"][i]}]\n+            new_output += [{\"mask\": mask_to_test_readable(o), \"scores\": outputs[\"scores\"][i]}]\n \n         # fmt: off\n         last_output = Expectations({\n@@ -121,7 +121,7 @@ def test_small_model_pt(self):\n         }).get_expectation()\n \n         self.assertEqual(\n-            nested_simplify(new_outupt, decimals=4),\n+            nested_simplify(new_output, decimals=4),\n             [\n                 {'mask': {'hash': '115ad19f5f', 'shape': (480, 640)}, 'scores': 1.0444},\n                 {'mask': {'hash': '6affa964c6', 'shape': (480, 640)}, 'scores': 1.021},\n@@ -168,12 +168,12 @@ def test_threshold(self):\n         )\n \n         # Shortening by hashing\n-        new_outupt = []\n+        new_output = []\n         for i, o in enumerate(outputs[\"masks\"]):\n-            new_outupt += [{\"mask\": mask_to_test_readable(o), \"scores\": outputs[\"scores\"][i]}]\n+            new_output += [{\"mask\": mask_to_test_readable(o), \"scores\": outputs[\"scores\"][i]}]\n \n         self.assertEqual(\n-            nested_simplify(new_outupt, decimals=4),\n+            nested_simplify(new_output, decimals=4),\n             [\n                 {\"mask\": {\"hash\": \"115ad19f5f\", \"shape\": (480, 640)}, \"scores\": 1.0444},\n                 {\"mask\": {\"hash\": \"6affa964c6\", \"shape\": (480, 640)}, \"scores\": 1.0210},"
        },
        {
            "sha": "cca65df7306400fbd79416bd544a6588eb09ec54",
            "filename": "tests/quantization/autoawq/test_awq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoawq%2Ftest_awq.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -336,7 +336,7 @@ def test_raise_save_pretrained(self):\n \n     def test_fused_modules_to_not_convert(self):\n         \"\"\"\n-        Test if fused + modules to_not_covnert work as expected\n+        Test if fused + modules to_not_convert work as expected\n         \"\"\"\n         model_id = \"hf-internal-testing/Mixtral-tiny-AWQ\"\n "
        },
        {
            "sha": "45de422adc437d60edba6b07e589a9cfa95567db",
            "filename": "tests/quantization/quark_integration/test_quark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -36,7 +36,7 @@\n \n @require_quark\n class QuarkConfigTest(unittest.TestCase):\n-    def test_commmon_args(self):\n+    def test_common_args(self):\n         config = AutoConfig.from_pretrained(\"amd/Llama-3.1-8B-Instruct-w-int8-a-int8-sym-test\")\n         QuarkConfig(**config.quantization_config)\n "
        },
        {
            "sha": "727cb2affa08166fd72ea805e03aeb962c8fe38b",
            "filename": "tests/repo_utils/test_tests_fetcher.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Frepo_utils%2Ftest_tests_fetcher.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Frepo_utils%2Ftest_tests_fetcher.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Frepo_utils%2Ftest_tests_fetcher.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -177,14 +177,14 @@ def patch_transformer_repo_path(new_folder):\n     old_repo_path = tests_fetcher.PATH_TO_REPO\n     tests_fetcher.PATH_TO_REPO = Path(new_folder).resolve()\n     tests_fetcher.PATH_TO_EXAMPLES = tests_fetcher.PATH_TO_REPO / \"examples\"\n-    tests_fetcher.PATH_TO_TRANFORMERS = tests_fetcher.PATH_TO_REPO / \"src/transformers\"\n+    tests_fetcher.PATH_TO_TRANSFORMERS = tests_fetcher.PATH_TO_REPO / \"src/transformers\"\n     tests_fetcher.PATH_TO_TESTS = tests_fetcher.PATH_TO_REPO / \"tests\"\n     try:\n         yield\n     finally:\n         tests_fetcher.PATH_TO_REPO = old_repo_path\n         tests_fetcher.PATH_TO_EXAMPLES = tests_fetcher.PATH_TO_REPO / \"examples\"\n-        tests_fetcher.PATH_TO_TRANFORMERS = tests_fetcher.PATH_TO_REPO / \"src/transformers\"\n+        tests_fetcher.PATH_TO_TRANSFORMERS = tests_fetcher.PATH_TO_REPO / \"src/transformers\"\n         tests_fetcher.PATH_TO_TESTS = tests_fetcher.PATH_TO_REPO / \"tests\"\n \n "
        },
        {
            "sha": "e25873e54aea5a6cb3e8a56725fccad26b33a4c1",
            "filename": "tests/sagemaker/README.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fsagemaker%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fsagemaker%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fsagemaker%2FREADME.md?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -12,7 +12,7 @@ This document explains the testing strategy for releasing the new Hugging Face D\n Before we can run the tests we need to adjust the `requirements.txt` for PyTorch under `/tests/sagemaker/scripts/pytorch` and for TensorFlow under `/tests/sagemaker/scripts/pytorch`. We adjust the branch to the new RC-tag.\n \n ```\n-git+https://github.com/huggingface/transformers.git@v4.5.0.rc0 # install main or adjust ist with vX.X.X for installing version specific-transforms\n+git+https://github.com/huggingface/transformers.git@v4.5.0.rc0 # install main or adjust it with vX.X.X for installing version specific-transforms\n ```\n \n After we adjusted the `requirements.txt` we can run Amazon SageMaker tests with:  \n@@ -140,9 +140,9 @@ images:\n \n | ID                                  | Description                                                       | Platform                   | #GPUs | Collected & evaluated metrics            |\n |-------------------------------------|-------------------------------------------------------------------|-----------------------------|-------|------------------------------------------|\n-| pytorch-transfromers-test-single    | test bert finetuning using BERT fromtransformerlib+PT             | SageMaker createTrainingJob | 1     | train_runtime, eval_accuracy & eval_loss |\n-| pytorch-transfromers-test-2-ddp     | test bert finetuning using BERT from transformer lib+ PT DPP      | SageMaker createTrainingJob | 16    | train_runtime, eval_accuracy & eval_loss |\n-| pytorch-transfromers-test-2-smd     | test bert finetuning using BERT from transformer lib+ PT SM DDP   | SageMaker createTrainingJob | 16    | train_runtime, eval_accuracy & eval_loss |\n-| pytorch-transfromers-test-1-smp     | test roberta finetuning using BERT from transformer lib+ PT SM MP | SageMaker createTrainingJob | 8     | train_runtime, eval_accuracy & eval_loss |\n-| tensorflow-transfromers-test-single | Test bert finetuning using BERT from transformer lib+TF           | SageMaker createTrainingJob | 1     | train_runtime, eval_accuracy & eval_loss |\n-| tensorflow-transfromers-test-2-smd  | test bert finetuning using BERT from transformer lib+ TF SM DDP   | SageMaker createTrainingJob | 16    | train_runtime, eval_accuracy & eval_loss |\n+| pytorch-transformers-test-single    | test bert finetuning using BERT fromtransformerlib+PT             | SageMaker createTrainingJob | 1     | train_runtime, eval_accuracy & eval_loss |\n+| pytorch-transformers-test-2-ddp     | test bert finetuning using BERT from transformer lib+ PT DPP      | SageMaker createTrainingJob | 16    | train_runtime, eval_accuracy & eval_loss |\n+| pytorch-transformers-test-2-smd     | test bert finetuning using BERT from transformer lib+ PT SM DDP   | SageMaker createTrainingJob | 16    | train_runtime, eval_accuracy & eval_loss |\n+| pytorch-transformers-test-1-smp     | test roberta finetuning using BERT from transformer lib+ PT SM MP | SageMaker createTrainingJob | 8     | train_runtime, eval_accuracy & eval_loss |\n+| tensorflow-transformers-test-single | Test bert finetuning using BERT from transformer lib+TF           | SageMaker createTrainingJob | 1     | train_runtime, eval_accuracy & eval_loss |\n+| tensorflow-transformers-test-2-smd  | test bert finetuning using BERT from transformer lib+ TF SM DDP   | SageMaker createTrainingJob | 16    | train_runtime, eval_accuracy & eval_loss |"
        },
        {
            "sha": "89b89966d542db701ab50aa51ee5391c01689481",
            "filename": "tests/sagemaker/conftest.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fsagemaker%2Fconftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Fsagemaker%2Fconftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fsagemaker%2Fconftest.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -45,7 +45,7 @@ def metric_definitions(self) -> str:\n \n     @property\n     def base_job_name(self) -> str:\n-        return f\"{self.framework}-transfromers-test\"\n+        return f\"{self.framework}-transformers-test\"\n \n     @property\n     def test_path(self) -> str:"
        },
        {
            "sha": "8330c19a5e6b99acce1e736a037ef31bb0c0b7ca",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -3238,7 +3238,7 @@ def test_problem_types(self):\n \n     def test_load_with_mismatched_shapes(self):\n         if not self.test_mismatched_shapes:\n-            self.skipTest(reason=\"test_missmatched_shapes is set to False\")\n+            self.skipTest(reason=\"test_mismatched_shapes is set to False\")\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n         for model_class in self.all_model_classes:\n@@ -3282,7 +3282,7 @@ def test_load_with_mismatched_shapes(self):\n \n     def test_mismatched_shapes_have_properly_initialized_weights(self):\n         if not self.test_mismatched_shapes:\n-            self.skipTest(reason=\"test_missmatched_shapes is set to False\")\n+            self.skipTest(reason=\"test_mismatched_shapes is set to False\")\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n         configs_no_init = _config_zero_init(config)\n@@ -3715,7 +3715,7 @@ def test_attn_implementation_composite_models(self):\n             model = model_class(config)\n             self.assertTrue(model.config.get_text_config(decoder=True)._attn_implementation == \"eager\")\n \n-            # Test that using `dict` atttention implementation works with `from_pretrained`\n+            # Test that using `dict` attention implementation works with `from_pretrained`\n             #  Set all backbones to \"eager\" because \"eager\" attention is always available\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n@@ -3770,7 +3770,7 @@ def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n         This tests only by looking at layer names, as usually SDPA layers are called \"SDPAAttention\".\n-        In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n+        In contrast to the above test, this one checks if the \"config._attn_implementation\" is a dict after the model\n         is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n         See https://github.com/huggingface/transformers/pull/32238 for more info\n "
        },
        {
            "sha": "b5c74c8f25d047c880ded7a9b82107a9d8de7405",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -289,7 +289,7 @@ def test_processor_text_has_no_visual(self):\n         if \"videos\" in inputs_dict:\n             processing_kwargs[\"do_sample_frames\"] = False\n \n-        # Firts call processor with all inputs and use nested input type, which is the format supported by all multimodal processors\n+        # First call processor with all inputs and use nested input type, which is the format supported by all multimodal processors\n         image_inputs_nested = [[image] if not isinstance(image, list) else image for image in image_inputs]\n         video_inputs_nested = [[video] for video in video_inputs]\n         inputs_dict_nested = {\"text\": text, \"images\": image_inputs_nested, \"videos\": video_inputs_nested}\n@@ -1172,7 +1172,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n         # 3 frames are inferred from input video's length and FPS, so can be hardcoded\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), 3)\n \n-        # Whan `do_sample_frames=False` no sampling is done and whole video is loaded, even if number of frames is passed\n+        # When `do_sample_frames=False` no sampling is done and whole video is loaded, even if number of frames is passed\n         fps = 10\n         out_dict_with_video = processor.apply_chat_template(\n             messages,"
        },
        {
            "sha": "c1b3bd796b407463ab7ea6c5e94156ddf3dfe265",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -3212,7 +3212,7 @@ def test_fast_only_inputs(self):\n                 self.assertRaises(TypeError, tokenizer_r.encode_plus, None)\n                 self.assertRaises(TypeError, tokenizer_r.batch_encode_plus, None)\n \n-    def test_alignement_methods(self):\n+    def test_alignment_methods(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n@@ -4101,7 +4101,7 @@ def test_save_pretrained(self):\n \n                 shutil.rmtree(tmpdirname2)\n \n-    def test_embeded_special_tokens(self):\n+    def test_embedded_special_tokens(self):\n         if not self.test_slow_tokenizer:\n             # as we don't have a slow version, we can't compare the outputs between slow and fast versions\n             self.skipTest(reason=\"test_slow_tokenizer is set to False\")"
        },
        {
            "sha": "7c8f633d54b8651ea87d772e935f5056d24923c0",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -1652,7 +1652,7 @@ def test_train_and_eval_dataloaders(self):\n         if torch_device in [\"cuda\"]:\n             n_gpu = max(1, backend_device_count(torch_device))\n         else:\n-            # DP is decprecated by PyTorch, accelerators like XPU doesn't support DP\n+            # DP is deprecated by PyTorch, accelerators like XPU doesn't support DP\n             n_gpu = 1\n \n         tmp_dir = self.get_auto_remove_tmp_dir()\n@@ -4980,7 +4980,7 @@ def test_best_model_checkpoint_behavior(self):\n \n             assert len(os.listdir(tmpdir)) == trainer.state.global_step // 2\n \n-    def test_special_token_aligment(self):\n+    def test_special_token_alignment(self):\n         \"\"\"\n         Tests that special token changes in the tokenizer result in model configs updates when using the trainer, to\n         ensure special tokens are aligned across configs"
        },
        {
            "sha": "d7c2734f34fd046d0cf9aecafbb9b5421066c70b",
            "filename": "tests/utils/test_masking_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Futils%2Ftest_masking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Futils%2Ftest_masking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_masking_utils.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -138,7 +138,7 @@ def test_find_packed_sequence_indices(self):\n         self.assertTrue((find_packed_sequence_indices(position_ids) == EXPECTED_SEQUENCE_INDICES).all())\n \n     def test_chunked_mask_with_left_padding_and_large_prefill(self):\n-        # Make sur we have an attention_chunk_size in the config\n+        # Make sure we have an attention_chunk_size in the config\n         config = LlamaConfig(attention_chunk_size=3, attn_implementation=\"sdpa\")\n \n         batch_size = 2\n@@ -193,7 +193,7 @@ def test_chunked_mask_with_left_padding_and_large_prefill(self):\n         self.assertTrue((chunked_attention_mask == EXPECTED_CHUNKED_MASK).all())\n \n     def test_chunked_mask_with_left_padding_decoding(self):\n-        # Make sur we have an attention_chunk_size in the config\n+        # Make sure we have an attention_chunk_size in the config\n         config = LlamaConfig(attention_chunk_size=4, attn_implementation=\"sdpa\", num_hidden_layers=1)\n \n         cache = DynamicCache(config=config)"
        },
        {
            "sha": "7f24c9882540061259548264c965f146489b02d5",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -714,9 +714,9 @@ def test_model_from_pretrained_attn_implementation(self):\n \n     def test_model_from_config_attn_implementation(self):\n         # test that the model can be instantiated with attn_implementation of either\n-        # 1. config created with explicit attn_implementatation and from_config\n+        # 1. config created with explicit attn_implementation and from_config\n         # 2. explicit from_config's attn_implementation argument with a config argument\n-        # 3. config created with explicit attn_implementatation and from_config overriding with explicit attn_implementation argument\n+        # 3. config created with explicit attn_implementation and from_config overriding with explicit attn_implementation argument\n         attn_implementation_available = [\"eager\", \"sdpa\"]\n \n         if is_flash_attn_available():\n@@ -1588,7 +1588,7 @@ def test_modifying_model_config_gets_moved_to_generation_config(self):\n             with tempfile.TemporaryDirectory() as tmp_dir:\n                 model.save_pretrained(tmp_dir)\n                 # 1 - That parameter will be removed from `model.config`. We don't want to use `model.config` to store\n-                # generative parameters, and the old default (1.0) would no longer relect the user's wishes.\n+                # generative parameters, and the old default (1.0) would no longer reflect the user's wishes.\n                 self.assertTrue(model.config.repetition_penalty is None)\n                 # 2 - That parameter will be set in `model.generation_config` instead.\n                 self.assertTrue(model.generation_config.repetition_penalty == 3.0)"
        },
        {
            "sha": "8a73468a1e494f4f4d48afb2a4383b4603fc4443",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -151,13 +151,13 @@\n         \"ChameleonVQVAE\",  # VQVAE here is used only for encoding (discretizing) and is tested as part of bigger model\n         \"Qwen2VLModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2VLForConditionalGeneration.\n         \"Qwen2_5_VLModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5_VLForConditionalGeneration.\n-        \"Qwen2_5OmniForConditionalGeneration\",  # Not a regular model. Testted in Qwen2_5OmniModelIntergrationTest\n-        \"Qwen2_5OmniTalkerForConditionalGeneration\",  #  Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntergrationTest.\n-        \"Qwen2_5OmniTalkerModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntergrationTest.\n-        \"Qwen2_5OmniThinkerTextModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntergrationTest.\n-        \"Qwen2_5OmniToken2WavModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntergrationTest.\n-        \"Qwen2_5OmniToken2WavDiTModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntergrationTest.\n-        \"Qwen2_5OmniToken2WavBigVGANModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntergrationTest.\n+        \"Qwen2_5OmniForConditionalGeneration\",  # Not a regular model. Testted in Qwen2_5OmniModelIntegrationTest\n+        \"Qwen2_5OmniTalkerForConditionalGeneration\",  #  Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntegrationTest.\n+        \"Qwen2_5OmniTalkerModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntegrationTest.\n+        \"Qwen2_5OmniThinkerTextModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntegrationTest.\n+        \"Qwen2_5OmniToken2WavModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntegrationTest.\n+        \"Qwen2_5OmniToken2WavDiTModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntegrationTest.\n+        \"Qwen2_5OmniToken2WavBigVGANModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntegrationTest.\n         \"MllamaTextModel\",  # Building part of bigger (tested) model. # TODO: add tests\n         \"MllamaVisionModel\",  # Building part of bigger (tested) model. # TODO: add tests\n         \"Llama4TextModel\",  # Building part of bigger (tested) model. # TODO: add tests\n@@ -891,7 +891,7 @@ def check_all_auto_mappings_importable():\n \n def check_objects_being_equally_in_main_init():\n     \"\"\"\n-    Check if a (TensorFlow or Flax) object is in the main __init__ iif its counterpart in PyTorch is.\n+    Check if a (TensorFlow or Flax) object is in the main __init__ if its counterpart in PyTorch is.\n     \"\"\"\n     attrs = dir(transformers)\n "
        },
        {
            "sha": "0e0df9b66ef34f7d847ba8a50d2cdd3d27eec333",
            "filename": "utils/create_dependency_mapping.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fcreate_dependency_mapping.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fcreate_dependency_mapping.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcreate_dependency_mapping.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -37,7 +37,7 @@ def topological_sort(dependencies: dict) -> list[list[str]]:\n         leaf_nodes = {node for node in graph if len(graph[node]) == 0}\n         # Add them to the list as next level\n         sorting_list.append([name_mapping[node] for node in leaf_nodes])\n-        # Remove the leafs from the graph (and from the deps of other nodes)\n+        # Remove the leaves from the graph (and from the deps of other nodes)\n         graph = {node: deps - leaf_nodes for node, deps in graph.items() if node not in leaf_nodes}\n \n     return sorting_list"
        },
        {
            "sha": "53ee7597d89c107c03a92023b1c4e633fb466815",
            "filename": "utils/create_dummy_models.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fcreate_dummy_models.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fcreate_dummy_models.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcreate_dummy_models.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -1458,7 +1458,7 @@ def create_tiny_models(\n             all_build_args.append((c, models_to_create, os.path.join(output_path, c.model_type)))\n         with multiprocessing.Pool() as pool:\n             results = pool.starmap(build, all_build_args)\n-            results = {buid_args[0].__name__: result for buid_args, result in zip(all_build_args, results)}\n+            results = {build_args[0].__name__: result for build_args, result in zip(all_build_args, results)}\n \n     if upload:\n         if organization is None:"
        },
        {
            "sha": "7bea9310f31b42aea0e5c8e0175a7c24202d1703",
            "filename": "utils/custom_init_isort.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fcustom_init_isort.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fcustom_init_isort.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcustom_init_isort.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -252,7 +252,7 @@ def sort_imports(file: str, check_only: bool = True):\n         code, start_prompt=\"_import_structure = {\", end_prompt=\"if TYPE_CHECKING:\"\n     )\n \n-    # We ignore block 0 (everything untils start_prompt) and the last block (everything after end_prompt).\n+    # We ignore block 0 (everything until start_prompt) and the last block (everything after end_prompt).\n     for block_idx in range(1, len(main_blocks) - 1):\n         # Check if the block contains some `_import_structure`s thingy to sort.\n         block = main_blocks[block_idx]"
        },
        {
            "sha": "ca907aa6a2e6e31fe0c20ddd1b2d3f09acdbf4fe",
            "filename": "utils/get_ci_error_statistics.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fget_ci_error_statistics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fget_ci_error_statistics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fget_ci_error_statistics.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -62,14 +62,16 @@ def get_job_links(workflow_run_id, token=None):\n     return {}\n \n \n-def get_artifacts_links(worflow_run_id, token=None):\n+def get_artifacts_links(workflow_run_id, token=None):\n     \"\"\"Get all artifact links from a workflow run\"\"\"\n \n     headers = None\n     if token is not None:\n         headers = {\"Accept\": \"application/vnd.github+json\", \"Authorization\": f\"Bearer {token}\"}\n \n-    url = f\"https://api.github.com/repos/huggingface/transformers/actions/runs/{worflow_run_id}/artifacts?per_page=100\"\n+    url = (\n+        f\"https://api.github.com/repos/huggingface/transformers/actions/runs/{workflow_run_id}/artifacts?per_page=100\"\n+    )\n     result = requests.get(url, headers=headers).json()\n     artifacts = {}\n "
        },
        {
            "sha": "23c2359491f2b62403d16bf36803a10ea29e33ab",
            "filename": "utils/get_previous_daily_ci.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fget_previous_daily_ci.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fget_previous_daily_ci.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fget_previous_daily_ci.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -103,7 +103,7 @@ def get_last_daily_ci_artifacts(\n         token, workflow_run_id=workflow_run_id, workflow_id=workflow_id, commit_sha=commit_sha\n     )\n     if workflow_run_id is not None:\n-        artifacts_links = get_artifacts_links(worflow_run_id=workflow_run_id, token=token)\n+        artifacts_links = get_artifacts_links(workflow_run_id=workflow_run_id, token=token)\n \n         if artifact_names is None:\n             artifact_names = artifacts_links.keys()"
        },
        {
            "sha": "b039a86870f3ef3fa79b36846048ccd3422411d7",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -105,7 +105,7 @@ class ReplaceNameTransformer(m.MatcherDecoratableTransformer):\n         - llama -> my_new_model     and     my_new_model    -> llama\n         - Llama -> MyNewModel       and     MyNewModel      -> Llama\n         - LLAMA -> MY_NEW_MODEL     and     MY_NEW_MODEL    -> LLAMA\n-        - LLaMa -> MyNewModel       abd     MyNewModel      -> Llama\n+        - LLaMa -> MyNewModel       and     MyNewModel      -> Llama\n     \"\"\"\n \n     def __init__(self, old_name: str, new_name: str, original_new_model_name: str = \"\", only_doc: bool = False):\n@@ -378,7 +378,7 @@ def find_all_dependencies(\n             If provided, entities already present in `initial_checked_dependencies` will not be part of the returned dependencies.\n         return_parent (bool, *optional*):\n             If `True`, will return a list consisting of tuples (dependency, parent) instead of a simple set of dependencies. Note\n-            that the order of the items in the list reflects the traversal order. Thus, no parent can ever appear before childs.\n+            that the order of the items in the list reflects the traversal order. Thus, no parent can ever appear before children.\n     Returns:\n         A set of all the dependencies, or a list of tuples `(dependency, parent)` if `return_parent=True`.\n \n@@ -864,7 +864,7 @@ def replace_class_node(\n     \"\"\"\n     Replace a class node which inherits from another modeling class. This function works in the following way:\n     - start from the methods and class attributes of the original modeling code node, and replace their definition\n-    if overriden in the modular\n+    if overridden in the modular\n     - append all new methods and class attributes defined in the child class\n     - all potential method/class docstrings and decorators use the ones found in modular if any, else in original modeling\n     - replace all calls to super() with the unravelled code"
        },
        {
            "sha": "0b62a6060274880c0f28932167cf5d3916914d1d",
            "filename": "utils/release.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Frelease.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Frelease.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Frelease.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -110,7 +110,7 @@ def update_version_in_examples(version: str, patch: bool = False):\n         for fname in fnames:\n             if fname.endswith(\".py\"):\n                 if UV_SCRIPT_MARKER in Path(folder, fname).read_text():\n-                    # Update the depdendencies in UV scripts\n+                    # Update the dependencies in UV scripts\n                     uv_script_file_type = \"uv_script_dev\" if \".dev\" in version else \"uv_script_release\"\n                     update_version_in_file(os.path.join(folder, fname), version, file_type=uv_script_file_type)\n                 if not patch:"
        },
        {
            "sha": "c796be0092cc542f01d98e8168f6f038094a51af",
            "filename": "utils/scan_skipped_tests.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fscan_skipped_tests.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fscan_skipped_tests.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fscan_skipped_tests.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -161,7 +161,7 @@ def summarize_all_tests(\n \n def main() -> None:\n     parser = argparse.ArgumentParser(\n-        description=\"Scan model tests for overridden or skipped common or generat tests.\",\n+        description=\"Scan model tests for overridden or skipped common or generate tests.\",\n     )\n     parser.add_argument(\n         \"--output_dir\","
        },
        {
            "sha": "d200fc83b742de1d0197a467e1df4b564e00ba72",
            "filename": "utils/tests_fetcher.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Ftests_fetcher.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Ftests_fetcher.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ftests_fetcher.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -67,7 +67,7 @@\n \n PATH_TO_REPO = Path(__file__).parent.parent.resolve()\n PATH_TO_EXAMPLES = PATH_TO_REPO / \"examples\"\n-PATH_TO_TRANFORMERS = PATH_TO_REPO / \"src/transformers\"\n+PATH_TO_TRANSFORMERS = PATH_TO_REPO / \"src/transformers\"\n PATH_TO_TESTS = PATH_TO_REPO / \"tests\"\n \n # The value is just a heuristic to determine if we `guess` all models are impacted.\n@@ -734,7 +734,7 @@ def create_reverse_dependency_tree() -> list[tuple[str, str]]:\n     Create a list of all edges (a, b) which mean that modifying a impacts b with a going over all module and test files.\n     \"\"\"\n     cache = {}\n-    all_modules = list(PATH_TO_TRANFORMERS.glob(\"**/*.py\"))\n+    all_modules = list(PATH_TO_TRANSFORMERS.glob(\"**/*.py\"))\n     all_modules = [x for x in all_modules if not (\"models\" in x.parts and x.parts[-1].startswith(\"convert_\"))]\n     all_modules += list(PATH_TO_TESTS.glob(\"**/*.py\"))\n     all_modules = [str(mod.relative_to(PATH_TO_REPO)) for mod in all_modules]\n@@ -820,7 +820,7 @@ def init_test_examples_dependencies() -> tuple[dict[str, list[str]], list[str]]:\n     for framework in [\"flax\", \"pytorch\", \"tensorflow\"]:\n         test_files = list((PATH_TO_EXAMPLES / framework).glob(\"test_*.py\"))\n         all_examples.extend(test_files)\n-        # Remove the files at the root of examples/framework since they are not proper examples (they are eith utils\n+        # Remove the files at the root of examples/framework since they are not proper examples (they are either utils\n         # or example test files).\n         examples = [\n             f for f in (PATH_TO_EXAMPLES / framework).glob(\"**/*.py\") if f.parent != PATH_TO_EXAMPLES / framework\n@@ -854,7 +854,7 @@ def create_reverse_dependency_map() -> dict[str, list[str]]:\n     # Start from the example deps init.\n     example_deps, examples = init_test_examples_dependencies()\n     # Add all modules and all tests to all examples\n-    all_modules = list(PATH_TO_TRANFORMERS.glob(\"**/*.py\"))\n+    all_modules = list(PATH_TO_TRANSFORMERS.glob(\"**/*.py\"))\n     all_modules = [x for x in all_modules if not (\"models\" in x.parts and x.parts[-1].startswith(\"convert_\"))]\n     all_modules += list(PATH_TO_TESTS.glob(\"**/*.py\")) + examples\n     all_modules = [str(mod.relative_to(PATH_TO_REPO)) for mod in all_modules]"
        },
        {
            "sha": "9f04300382e4575c56d857903e7a6a541712dac0",
            "filename": "utils/update_metadata.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fupdate_metadata.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c9f412105bb94a22d5adfb347ac6bce1cd1c066/utils%2Fupdate_metadata.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fupdate_metadata.py?ref=6c9f412105bb94a22d5adfb347ac6bce1cd1c066",
            "patch": "@@ -152,9 +152,9 @@ def get_frameworks_table() -> pd.DataFrame:\n     modules.\n     \"\"\"\n     # Dictionary model names to config.\n-    config_maping_names = transformers_module.models.auto.configuration_auto.CONFIG_MAPPING_NAMES\n+    config_mapping_names = transformers_module.models.auto.configuration_auto.CONFIG_MAPPING_NAMES\n     model_prefix_to_model_type = {\n-        config.replace(\"Config\", \"\"): model_type for model_type, config in config_maping_names.items()\n+        config.replace(\"Config\", \"\"): model_type for model_type, config in config_mapping_names.items()\n     }\n \n     # Dictionaries flagging if each model prefix has a backend in PT/TF/Flax."
        }
    ],
    "stats": {
        "total": 278,
        "additions": 141,
        "deletions": 137
    }
}