{
    "author": "yao-matrix",
    "message": "enable xpu in test_trainer (#37774)\n\n* enable xpu in test_trainer\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* enhance _device_agnostic_dispatch to cover value\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n* add default values for torch not available case\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>",
    "sha": "5534b80b7f2d8dc8f72b89ae41c92b6c3ca9cd98",
    "files": [
        {
            "sha": "93f9be678221871bf87da3a01ab043293de82192",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 38,
            "deletions": 4,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/5534b80b7f2d8dc8f72b89ae41c92b6c3ca9cd98/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5534b80b7f2d8dc8f72b89ae41c92b6c3ca9cd98/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=5534b80b7f2d8dc8f72b89ae41c92b6c3ca9cd98",
            "patch": "@@ -2946,10 +2946,10 @@ def _device_agnostic_dispatch(device: str, dispatch_table: dict[str, Callable],\n \n     fn = dispatch_table[device]\n \n-    # Some device agnostic functions return values. Need to guard against `None`\n-    # instead at user level.\n-    if fn is None:\n-        return None\n+    # Some device agnostic functions return values or None, will return then directly.\n+    if not callable(fn):\n+        return fn\n+\n     return fn(*args, **kwargs)\n \n \n@@ -2971,10 +2971,29 @@ def _device_agnostic_dispatch(device: str, dispatch_table: dict[str, Callable],\n         \"cpu\": lambda: 0,\n         \"default\": lambda: 1,\n     }\n+    BACKEND_RESET_MAX_MEMORY_ALLOCATED = {\n+        \"cuda\": torch.cuda.reset_max_memory_allocated,\n+        \"cpu\": None,\n+        \"default\": None,\n+    }\n+    BACKEND_MAX_MEMORY_ALLOCATED = {\n+        \"cuda\": torch.cuda.max_memory_allocated,\n+        \"cpu\": 0,\n+        \"default\": 0,\n+    }\n+    BACKEND_MEMORY_ALLOCATED = {\n+        \"cuda\": torch.cuda.memory_allocated,\n+        \"cpu\": 0,\n+        \"default\": 0,\n+    }\n else:\n     BACKEND_MANUAL_SEED = {\"default\": None}\n     BACKEND_EMPTY_CACHE = {\"default\": None}\n     BACKEND_DEVICE_COUNT = {\"default\": lambda: 0}\n+    BACKEND_RESET_MAX_MEMORY_ALLOCATED = {\"default\": None}\n+    BACKEND_MAX_MEMORY_ALLOCATED = {\"default\": 0}\n+    BACKEND_MEMORY_ALLOCATED = {\"default\": 0}\n+\n \n if is_torch_hpu_available():\n     BACKEND_MANUAL_SEED[\"hpu\"] = torch.hpu.manual_seed\n@@ -2994,6 +3013,9 @@ def _device_agnostic_dispatch(device: str, dispatch_table: dict[str, Callable],\n     BACKEND_EMPTY_CACHE[\"xpu\"] = torch.xpu.empty_cache\n     BACKEND_MANUAL_SEED[\"xpu\"] = torch.xpu.manual_seed\n     BACKEND_DEVICE_COUNT[\"xpu\"] = torch.xpu.device_count\n+    BACKEND_RESET_MAX_MEMORY_ALLOCATED[\"xpu\"] = torch.xpu.reset_peak_memory_stats\n+    BACKEND_MAX_MEMORY_ALLOCATED[\"xpu\"] = torch.xpu.max_memory_allocated\n+    BACKEND_MEMORY_ALLOCATED[\"xpu\"] = torch.xpu.memory_allocated\n \n if is_torch_xla_available():\n     BACKEND_EMPTY_CACHE[\"xla\"] = torch.cuda.empty_cache\n@@ -3013,6 +3035,18 @@ def backend_device_count(device: str):\n     return _device_agnostic_dispatch(device, BACKEND_DEVICE_COUNT)\n \n \n+def backend_reset_max_memory_allocated(device: str):\n+    return _device_agnostic_dispatch(device, BACKEND_RESET_MAX_MEMORY_ALLOCATED)\n+\n+\n+def backend_max_memory_allocated(device: str):\n+    return _device_agnostic_dispatch(device, BACKEND_MAX_MEMORY_ALLOCATED)\n+\n+\n+def backend_memory_allocated(device: str):\n+    return _device_agnostic_dispatch(device, BACKEND_MEMORY_ALLOCATED)\n+\n+\n if is_torch_available():\n     # If `TRANSFORMERS_TEST_DEVICE_SPEC` is enabled we need to import extra entries\n     # into device to function mappings."
        },
        {
            "sha": "bf2ad14e98113684f2fc0137c1e3240f0b98052e",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 29,
            "deletions": 27,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/5534b80b7f2d8dc8f72b89ae41c92b6c3ca9cd98/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5534b80b7f2d8dc8f72b89ae41c92b6c3ca9cd98/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=5534b80b7f2d8dc8f72b89ae41c92b6c3ca9cd98",
            "patch": "@@ -62,6 +62,10 @@\n     TemporaryHubRepo,\n     TestCasePlus,\n     backend_device_count,\n+    backend_empty_cache,\n+    backend_max_memory_allocated,\n+    backend_memory_allocated,\n+    backend_reset_max_memory_allocated,\n     evaluate_side_effect_factory,\n     execute_subprocess_async,\n     get_gpu_count,\n@@ -78,7 +82,6 @@\n     require_liger_kernel,\n     require_lomo,\n     require_non_hpu,\n-    require_non_xpu,\n     require_optuna,\n     require_peft,\n     require_ray,\n@@ -245,18 +248,18 @@ def bytes2megabytes(x):\n class TorchTracemalloc:\n     def __enter__(self):\n         gc.collect()\n-        if torch.cuda.is_available():\n-            torch.cuda.empty_cache()\n-            torch.cuda.reset_max_memory_allocated()  # reset the peak gauge to zero\n-            self.begin = torch.cuda.memory_allocated()\n+        if torch_device in [\"cuda\", \"xpu\"]:\n+            backend_empty_cache(torch_device)\n+            backend_reset_max_memory_allocated(torch_device)  # reset the peak gauge to zero\n+            self.begin = backend_memory_allocated(torch_device)\n         return self\n \n     def __exit__(self, *exc):\n         gc.collect()\n-        if torch.cuda.is_available():\n-            torch.cuda.empty_cache()\n-            self.end = torch.cuda.memory_allocated()\n-            self.peak = torch.cuda.max_memory_allocated()\n+        if torch_device in [\"cuda\", \"xpu\"]:\n+            backend_empty_cache(torch_device)\n+            self.end = backend_memory_allocated(torch_device)\n+            self.peak = backend_max_memory_allocated(torch_device)\n         self.used = bytes2megabytes(self.end - self.begin)\n         self.peaked = bytes2megabytes(self.peak - self.begin)\n \n@@ -1246,7 +1249,6 @@ def test_mixed_bf16(self):\n \n         # will add more specific tests once there are some bugs to fix\n \n-    @require_non_xpu\n     @require_torch_gpu\n     @require_torch_tf32\n     def test_tf32(self):\n@@ -1838,7 +1840,7 @@ def test_use_liger_kernel_trainer(self):\n         _ = trainer.train()\n \n     @require_lomo\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_lomo(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -1861,7 +1863,7 @@ def test_lomo(self):\n             self.assertFalse(torch.allclose(param, previous_params[name].to(param.device), rtol=1e-12, atol=1e-12))\n \n     @require_lomo\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_adalomo(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -2027,7 +2029,7 @@ def test_galore_matched_modules(self):\n                 self.assertFalse(is_regex)\n \n     @require_galore_torch\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_galore(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -2048,7 +2050,7 @@ def test_galore(self):\n         _ = trainer.train()\n \n     @require_galore_torch\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_galore_extra_args(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -2070,7 +2072,7 @@ def test_galore_extra_args(self):\n         _ = trainer.train()\n \n     @require_galore_torch\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_galore_layerwise(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -2091,7 +2093,7 @@ def test_galore_layerwise(self):\n         _ = trainer.train()\n \n     @require_galore_torch\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_galore_layerwise_with_scheduler(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -2113,7 +2115,7 @@ def test_galore_layerwise_with_scheduler(self):\n         _ = trainer.train()\n \n     @require_galore_torch\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_galore_adamw_8bit(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -2134,7 +2136,7 @@ def test_galore_adamw_8bit(self):\n         _ = trainer.train()\n \n     @require_galore_torch\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_galore_adafactor(self):\n         # These are the intervals of the peak memory usage of training such a tiny model\n         # if the peak memory goes outside that range, then we know there might be a bug somewhere\n@@ -2166,7 +2168,7 @@ def test_galore_adafactor(self):\n         self.assertTrue(lower_bound_pm < galore_peak_memory)\n \n     @require_galore_torch\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_galore_adafactor_attention_only(self):\n         # These are the intervals of the peak memory usage of training such a tiny model\n         # if the peak memory goes outside that range, then we know there might be a bug somewhere\n@@ -2197,7 +2199,7 @@ def test_galore_adafactor_attention_only(self):\n         self.assertTrue(lower_bound_pm < galore_peak_memory)\n \n     @require_galore_torch\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_galore_adafactor_all_linear(self):\n         # These are the intervals of the peak memory usage of training such a tiny model\n         # if the peak memory goes outside that range, then we know there might be a bug somewhere\n@@ -2305,7 +2307,7 @@ def test_galore_lr_display_with_scheduler(self):\n         self.assertTrue(len(decreasing_lrs) > len(increasing_lrs))\n \n     @require_apollo_torch\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_apollo(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -2326,7 +2328,7 @@ def test_apollo(self):\n         _ = trainer.train()\n \n     @require_apollo_torch\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_apollo_extra_args(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -2348,7 +2350,7 @@ def test_apollo_extra_args(self):\n         _ = trainer.train()\n \n     @require_apollo_torch\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_apollo_layerwise(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -2369,7 +2371,7 @@ def test_apollo_layerwise(self):\n         _ = trainer.train()\n \n     @require_apollo_torch\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_apollo_layerwise_with_scheduler(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -2391,7 +2393,7 @@ def test_apollo_layerwise_with_scheduler(self):\n         _ = trainer.train()\n \n     @require_apollo_torch\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_apollo_lr_display_without_scheduler(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -2416,7 +2418,7 @@ def test_apollo_lr_display_without_scheduler(self):\n         self.assertEqual(trainer.get_learning_rates(), [learning_rate, learning_rate])\n \n     @require_apollo_torch\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_apollo_lr_display_with_scheduler(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -3995,7 +3997,7 @@ def test_fp16_full_eval(self):\n             # perfect world: fp32_init/2 == fp16_eval\n             self.assertAlmostEqual(fp16_eval, fp32_init / 2, delta=5_000)\n \n-    @require_non_xpu\n+    @require_torch_gpu\n     @require_torch_non_multi_gpu\n     @require_torch_tensorrt_fx\n     def test_torchdynamo_full_eval(self):"
        }
    ],
    "stats": {
        "total": 98,
        "additions": 67,
        "deletions": 31
    }
}