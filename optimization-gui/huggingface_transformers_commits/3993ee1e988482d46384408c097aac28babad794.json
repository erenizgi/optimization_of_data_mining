{
    "author": "simonreise",
    "message": "Add `segmentation_maps` support to MobileNetV2ImageProcessor (#37312)\n\n* Add `segmentation_maps` support to mobilenet_v2 image processor and `reduce_labels` to mobilevit\n\n* Changed mobilenetv2 tests to support fastimageprocessor\n\n* added `segmentation_maps` support to fast image processor\n\n* reverted to upstream/main\n\n* Add optional\n\n* Use autodocstring\n\n* Changed docs\n\n* Docs fix\n\n* Changed fp to match beit fp\n\n* Change typing imports\n\n* Fixed repo inconsistency\n\n* Added fast-slow equivalence tests\n\n* Removed unnecessary call\n\n* Add `reduce_labels` to Mobilevit fast processor\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "3993ee1e988482d46384408c097aac28babad794",
    "files": [
        {
            "sha": "5ddc4f0ea3ff12475487e62fa5552f09baa47d55",
            "filename": "docs/source/en/model_doc/mobilenet_v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3993ee1e988482d46384408c097aac28babad794/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3993ee1e988482d46384408c097aac28babad794/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md?ref=3993ee1e988482d46384408c097aac28babad794",
            "patch": "@@ -114,6 +114,7 @@ print(f\"The predicted class label is: {predicted_class_label}\")\n \n [[autodoc]] MobileNetV2ImageProcessor\n     - preprocess\n+    - post_process_semantic_segmentation\n \n ## MobileNetV2ImageProcessorFast\n "
        },
        {
            "sha": "41d765ea314aa9ad8f413a1babe7422de9259c5e",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py",
            "status": "modified",
            "additions": 197,
            "deletions": 30,
            "changes": 227,
            "blob_url": "https://github.com/huggingface/transformers/blob/3993ee1e988482d46384408c097aac28babad794/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3993ee1e988482d46384408c097aac28babad794/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py?ref=3993ee1e988482d46384408c097aac28babad794",
            "patch": "@@ -88,6 +88,11 @@ class MobileNetV2ImageProcessor(BaseImageProcessor):\n         image_std (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n             Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n             number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_reduce_labels (`bool`, *optional*, defaults to `False`):\n+            Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0 is\n+            used for background, and background itself is not included in all classes of a dataset (e.g. ADE20k). The\n+            background label will be replaced by 255. Can be overridden by the `do_reduce_labels` parameter in the\n+            `preprocess` method.\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n@@ -104,6 +109,7 @@ def __init__(\n         do_normalize: bool = True,\n         image_mean: Optional[Union[float, list[float]]] = None,\n         image_std: Optional[Union[float, list[float]]] = None,\n+        do_reduce_labels: bool = False,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -121,6 +127,7 @@ def __init__(\n         self.do_normalize = do_normalize\n         self.image_mean = image_mean if image_mean is not None else IMAGENET_STANDARD_MEAN\n         self.image_std = image_std if image_std is not None else IMAGENET_STANDARD_STD\n+        self.do_reduce_labels = do_reduce_labels\n \n     # Copied from transformers.models.mobilenet_v1.image_processing_mobilenet_v1.MobileNetV1ImageProcessor.resize\n     def resize(\n@@ -172,10 +179,151 @@ def resize(\n             **kwargs,\n         )\n \n+    # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.reduce_label\n+    def reduce_label(self, label: ImageInput) -> np.ndarray:\n+        label = to_numpy_array(label)\n+        # Avoid using underflow conversion\n+        label[label == 0] = 255\n+        label = label - 1\n+        label[label == 254] = 255\n+        return label\n+\n+    def __call__(self, images, segmentation_maps=None, **kwargs):\n+        \"\"\"\n+        Preprocesses a batch of images and optionally segmentation maps.\n+\n+        Overrides the `__call__` method of the `Preprocessor` class so that both images and segmentation maps can be\n+        passed in as positional arguments.\n+        \"\"\"\n+        return super().__call__(images, segmentation_maps=segmentation_maps, **kwargs)\n+\n+    def _preprocess(\n+        self,\n+        image: ImageInput,\n+        do_reduce_labels: bool,\n+        do_resize: bool,\n+        do_rescale: bool,\n+        do_center_crop: bool,\n+        do_normalize: bool,\n+        size: Optional[dict[str, int]] = None,\n+        resample: PILImageResampling = None,\n+        rescale_factor: Optional[float] = None,\n+        crop_size: Optional[dict[str, int]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        if do_reduce_labels:\n+            image = self.reduce_label(image)\n+\n+        if do_resize:\n+            image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n+\n+        if do_center_crop:\n+            image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n+\n+        if do_rescale:\n+            image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+        if do_normalize:\n+            image = self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n+\n+        return image\n+\n+    def _preprocess_image(\n+        self,\n+        image: ImageInput,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[dict[str, int]] = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_center_crop: Optional[bool] = None,\n+        crop_size: Optional[dict[str, int]] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.ndarray:\n+        \"\"\"Preprocesses a single image.\"\"\"\n+        # All transformations expect numpy arrays.\n+        image = to_numpy_array(image)\n+        if do_rescale and is_scaled_image(image):\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+        if input_data_format is None:\n+            input_data_format = infer_channel_dimension_format(image)\n+\n+        image = self._preprocess(\n+            image=image,\n+            do_reduce_labels=False,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_center_crop=do_center_crop,\n+            crop_size=crop_size,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            input_data_format=input_data_format,\n+        )\n+\n+        image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+\n+        return image\n+\n+    def _preprocess_mask(\n+        self,\n+        segmentation_map: ImageInput,\n+        do_reduce_labels: Optional[bool] = None,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[dict[str, int]] = None,\n+        do_center_crop: Optional[bool] = None,\n+        crop_size: Optional[dict[str, int]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.ndarray:\n+        \"\"\"Preprocesses a single mask.\"\"\"\n+        segmentation_map = to_numpy_array(segmentation_map)\n+        # Add channel dimension if missing - needed for certain transformations\n+        if segmentation_map.ndim == 2:\n+            added_channel_dim = True\n+            segmentation_map = segmentation_map[None, ...]\n+            input_data_format = ChannelDimension.FIRST\n+        else:\n+            added_channel_dim = False\n+            if input_data_format is None:\n+                input_data_format = infer_channel_dimension_format(segmentation_map, num_channels=1)\n+\n+        segmentation_map = self._preprocess(\n+            image=segmentation_map,\n+            do_reduce_labels=do_reduce_labels,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=PILImageResampling.NEAREST,\n+            do_rescale=False,\n+            do_center_crop=do_center_crop,\n+            crop_size=crop_size,\n+            do_normalize=False,\n+            image_mean=None,\n+            image_std=None,\n+            input_data_format=input_data_format,\n+        )\n+        # Remove extra channel dimension if added for processing\n+        if added_channel_dim:\n+            segmentation_map = segmentation_map.squeeze(0)\n+        segmentation_map = segmentation_map.astype(np.int64)\n+        return segmentation_map\n+\n     @filter_out_non_signature_kwargs()\n     def preprocess(\n         self,\n         images: ImageInput,\n+        segmentation_maps: Optional[ImageInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = None,\n@@ -186,6 +334,7 @@ def preprocess(\n         do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, list[float]]] = None,\n         image_std: Optional[Union[float, list[float]]] = None,\n+        do_reduce_labels: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -197,6 +346,8 @@ def preprocess(\n             images (`ImageInput`):\n                 Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            segmentation_maps (`ImageInput`, *optional*):\n+                Segmentation map to preprocess.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`dict[str, int]`, *optional*, defaults to `self.size`):\n@@ -219,6 +370,10 @@ def preprocess(\n                 Image mean to use if `do_normalize` is set to `True`.\n             image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n                 Image standard deviation to use if `do_normalize` is set to `True`.\n+            do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n+                Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n+                is used for background, and background itself is not included in all classes of a dataset (e.g.\n+                ADE20k). The background label will be replaced by 255.\n             return_tensors (`str` or `TensorType`, *optional*):\n                 The type of tensors to return. Can be one of:\n                 - Unset: Return a list of `np.ndarray`.\n@@ -241,6 +396,7 @@ def preprocess(\n         do_resize = do_resize if do_resize is not None else self.do_resize\n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)\n+        do_reduce_labels = do_reduce_labels if do_reduce_labels is not None else self.do_reduce_labels\n         resample = resample if resample is not None else self.resample\n         do_center_crop = do_center_crop if do_center_crop is not None else self.do_center_crop\n         crop_size = crop_size if crop_size is not None else self.crop_size\n@@ -253,11 +409,21 @@ def preprocess(\n \n         images = make_list_of_images(images)\n \n+        if segmentation_maps is not None:\n+            segmentation_maps = make_list_of_images(segmentation_maps, expected_ndims=2)\n+\n         if not valid_images(images):\n             raise ValueError(\n                 \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n                 \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n             )\n+\n+        if segmentation_maps is not None and not valid_images(segmentation_maps):\n+            raise ValueError(\n+                \"Invalid segmentation map type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n         validate_preprocess_arguments(\n             do_rescale=do_rescale,\n             rescale_factor=rescale_factor,\n@@ -270,42 +436,43 @@ def preprocess(\n             size=size,\n             resample=resample,\n         )\n-        # All transformations expect numpy arrays.\n-        images = [to_numpy_array(image) for image in images]\n \n-        if do_rescale and is_scaled_image(images[0]):\n-            logger.warning_once(\n-                \"It looks like you are trying to rescale already rescaled images. If the input\"\n-                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+        images = [\n+            self._preprocess_image(\n+                image=img,\n+                do_resize=do_resize,\n+                size=size,\n+                resample=resample,\n+                do_rescale=do_rescale,\n+                rescale_factor=rescale_factor,\n+                do_center_crop=do_center_crop,\n+                crop_size=crop_size,\n+                do_normalize=do_normalize,\n+                image_mean=image_mean,\n+                image_std=image_std,\n+                data_format=data_format,\n+                input_data_format=input_data_format,\n             )\n+            for img in images\n+        ]\n \n-        if input_data_format is None:\n-            # We assume that all images have the same channel dimension format.\n-            input_data_format = infer_channel_dimension_format(images[0])\n-\n-        all_images = []\n-        for image in images:\n-            if do_resize:\n-                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-\n-            if do_center_crop:\n-                image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n-\n-            if do_rescale:\n-                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+        data = {\"pixel_values\": images}\n \n-            if do_normalize:\n-                image = self.normalize(\n-                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+        if segmentation_maps is not None:\n+            segmentation_maps = [\n+                self._preprocess_mask(\n+                    segmentation_map=segmentation_map,\n+                    do_reduce_labels=do_reduce_labels,\n+                    do_resize=do_resize,\n+                    size=size,\n+                    do_center_crop=do_center_crop,\n+                    crop_size=crop_size,\n+                    input_data_format=input_data_format,\n                 )\n+                for segmentation_map in segmentation_maps\n+            ]\n+            data[\"labels\"] = segmentation_maps\n \n-            all_images.append(image)\n-        images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n-            for image in all_images\n-        ]\n-\n-        data = {\"pixel_values\": images}\n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.post_process_semantic_segmentation with Beit->MobileNetV2"
        },
        {
            "sha": "be01f33c7917f538f3751d04e0f5e2cf9af10db9",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2_fast.py",
            "status": "modified",
            "additions": 215,
            "deletions": 5,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/3993ee1e988482d46384408c097aac28babad794/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3993ee1e988482d46384408c097aac28babad794/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py?ref=3993ee1e988482d46384408c097aac28babad794",
            "patch": "@@ -14,16 +14,57 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for MobileNetV2.\"\"\"\n \n-from typing import Optional\n+from typing import Optional, Union\n \n-from ...image_processing_utils_fast import BaseImageProcessorFast\n-from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, PILImageResampling\n-from ...utils import auto_docstring, is_torch_available, is_torch_tensor\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+    is_torch_tensor,\n+    make_list_of_images,\n+    pil_torch_interpolation_mapping,\n+    validate_kwargs,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n \n \n if is_torch_available():\n     import torch\n \n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+class MobileNetV2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n+        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n+        is used for background, and background itself is not included in all classes of a dataset (e.g.\n+        ADE20k). The background label will be replaced by 255.\n+    \"\"\"\n+\n+    do_reduce_labels: Optional[bool]\n+\n \n @auto_docstring\n class MobileNetV2ImageProcessorFast(BaseImageProcessorFast):\n@@ -37,8 +78,177 @@ class MobileNetV2ImageProcessorFast(BaseImageProcessorFast):\n     do_center_crop = True\n     do_rescale = True\n     do_normalize = True\n-    do_convert_rgb = None\n+    do_reduce_labels = False\n+    valid_kwargs = MobileNetV2FastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[MobileNetV2FastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    # Copied from transformers.models.beit.image_processing_beit_fast.BeitImageProcessorFast.reduce_label\n+    def reduce_label(self, labels: list[\"torch.Tensor\"]):\n+        for idx in range(len(labels)):\n+            label = labels[idx]\n+            label = torch.where(label == 0, torch.tensor(255, dtype=label.dtype), label)\n+            label = label - 1\n+            label = torch.where(label == 254, torch.tensor(255, dtype=label.dtype), label)\n+            labels[idx] = label\n+\n+        return label\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_reduce_labels: bool,\n+        do_resize: bool,\n+        do_rescale: bool,\n+        do_center_crop: bool,\n+        do_normalize: bool,\n+        size: Optional[SizeDict],\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        rescale_factor: Optional[float],\n+        crop_size: Optional[SizeDict],\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: bool,\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        processed_images = []\n+\n+        if do_reduce_labels:\n+            images = self.reduce_label(images)\n+\n+        # Group images by shape for more efficient batch processing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+\n+        # Process each group of images with the same shape\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+\n+        # Reorder images to original sequence\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group again after resizing (in case resize produced different sizes)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            if do_center_crop:\n+                stacked_images = self.center_crop(stacked_images, crop_size)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        # Stack all processed images if return_tensors is specified\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return processed_images\n+\n+    def _preprocess_images(\n+        self,\n+        images,\n+        **kwargs,\n+    ):\n+        \"\"\"Preprocesses images.\"\"\"\n+        kwargs[\"do_reduce_labels\"] = False\n+        processed_images = self._preprocess(images=images, **kwargs)\n+        return processed_images\n+\n+    def _preprocess_segmentation_maps(\n+        self,\n+        segmentation_maps,\n+        **kwargs,\n+    ):\n+        \"\"\"Preprocesses segmentation maps.\"\"\"\n+        processed_segmentation_maps = []\n+        for segmentation_map in segmentation_maps:\n+            segmentation_map = self._process_image(\n+                segmentation_map, do_convert_rgb=False, input_data_format=ChannelDimension.FIRST\n+            )\n+\n+            if segmentation_map.ndim == 2:\n+                segmentation_map = segmentation_map[None, ...]\n+\n+            processed_segmentation_maps.append(segmentation_map)\n+\n+        kwargs[\"do_normalize\"] = False\n+        kwargs[\"do_rescale\"] = False\n+        kwargs[\"interpolation\"] = pil_torch_interpolation_mapping[PILImageResampling.NEAREST]\n+        processed_segmentation_maps = self._preprocess(images=processed_segmentation_maps, **kwargs)\n+\n+        processed_segmentation_maps = processed_segmentation_maps.squeeze(1)\n+\n+        processed_segmentation_maps = processed_segmentation_maps.to(torch.int64)\n+        return processed_segmentation_maps\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        **kwargs: Unpack[MobileNetV2FastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to preprocess.\n+        \"\"\"\n+        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n+        # Set default kwargs from self. This ensures that if a kwarg is not provided\n+        # by the user, it gets its default value from the instance, or is set to None.\n+        for kwarg_name in self.valid_kwargs.__annotations__:\n+            kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n+\n+        # Extract parameters that are only used for preparing the input images\n+        do_convert_rgb = kwargs.pop(\"do_convert_rgb\")\n+        input_data_format = kwargs.pop(\"input_data_format\")\n+        device = kwargs.pop(\"device\")\n+        # Prepare input images\n+        images = self._prepare_input_images(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+\n+        # Prepare segmentation maps\n+        if segmentation_maps is not None:\n+            segmentation_maps = make_list_of_images(images=segmentation_maps, expected_ndims=2)\n+\n+        # Update kwargs that need further processing before being validated\n+        kwargs = self._further_process_kwargs(**kwargs)\n+\n+        # Validate kwargs\n+        self._validate_preprocess_kwargs(**kwargs)\n+\n+        # torch resize uses interpolation instead of resample\n+        resample = kwargs.pop(\"resample\")\n+        kwargs[\"interpolation\"] = (\n+            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n+        )\n+\n+        # Pop kwargs that are not needed in _preprocess\n+        kwargs.pop(\"default_to_square\")\n+        kwargs.pop(\"data_format\")\n+\n+        images = self._preprocess_images(\n+            images=images,\n+            **kwargs,\n+        )\n+\n+        if segmentation_maps is not None:\n+            segmentation_maps = self._preprocess_segmentation_maps(\n+                segmentation_maps=segmentation_maps,\n+                **kwargs,\n+            )\n+            return BatchFeature(data={\"pixel_values\": images, \"labels\": segmentation_maps})\n+\n+        return BatchFeature(data={\"pixel_values\": images})\n \n+    # Copied from transformers.models.beit.image_processing_beit_fast.BeitImageProcessorFast.post_process_semantic_segmentation with Beit->MobileNetV2\n     def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n         \"\"\"\n         Converts the output of [`MobileNetV2ForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch."
        },
        {
            "sha": "86707613764f4202f0899cb3043dfe5f5f880506",
            "filename": "src/transformers/models/mobilevit/image_processing_mobilevit.py",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/3993ee1e988482d46384408c097aac28babad794/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3993ee1e988482d46384408c097aac28babad794/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py?ref=3993ee1e988482d46384408c097aac28babad794",
            "patch": "@@ -83,6 +83,11 @@ class MobileViTImageProcessor(BaseImageProcessor):\n         do_flip_channel_order (`bool`, *optional*, defaults to `True`):\n             Whether to flip the color channels from RGB to BGR. Can be overridden by the `do_flip_channel_order`\n             parameter in the `preprocess` method.\n+        do_reduce_labels (`bool`, *optional*, defaults to `False`):\n+            Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0 is\n+            used for background, and background itself is not included in all classes of a dataset (e.g. ADE20k). The\n+            background label will be replaced by 255. Can be overridden by the `do_reduce_labels` parameter in the\n+            `preprocess` method.\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n@@ -97,6 +102,7 @@ def __init__(\n         do_center_crop: bool = True,\n         crop_size: Optional[dict[str, int]] = None,\n         do_flip_channel_order: bool = True,\n+        do_reduce_labels: bool = False,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -113,6 +119,7 @@ def __init__(\n         self.do_center_crop = do_center_crop\n         self.crop_size = crop_size\n         self.do_flip_channel_order = do_flip_channel_order\n+        self.do_reduce_labels = do_reduce_labels\n \n     # Copied from transformers.models.mobilenet_v1.image_processing_mobilenet_v1.MobileNetV1ImageProcessor.resize with PILImageResampling.BICUBIC->PILImageResampling.BILINEAR\n     def resize(\n@@ -183,6 +190,15 @@ def flip_channel_order(\n         \"\"\"\n         return flip_channel_order(image, data_format=data_format, input_data_format=input_data_format)\n \n+    # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.reduce_label\n+    def reduce_label(self, label: ImageInput) -> np.ndarray:\n+        label = to_numpy_array(label)\n+        # Avoid using underflow conversion\n+        label[label == 0] = 255\n+        label = label - 1\n+        label[label == 254] = 255\n+        return label\n+\n     def __call__(self, images, segmentation_maps=None, **kwargs):\n         \"\"\"\n         Preprocesses a batch of images and optionally segmentation maps.\n@@ -195,6 +211,7 @@ def __call__(self, images, segmentation_maps=None, **kwargs):\n     def _preprocess(\n         self,\n         image: ImageInput,\n+        do_reduce_labels: bool,\n         do_resize: bool,\n         do_rescale: bool,\n         do_center_crop: bool,\n@@ -205,6 +222,9 @@ def _preprocess(\n         crop_size: Optional[dict[str, int]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ):\n+        if do_reduce_labels:\n+            image = self.reduce_label(image)\n+\n         if do_resize:\n             image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n \n@@ -246,6 +266,7 @@ def _preprocess_image(\n \n         image = self._preprocess(\n             image=image,\n+            do_reduce_labels=False,\n             do_resize=do_resize,\n             size=size,\n             resample=resample,\n@@ -264,6 +285,7 @@ def _preprocess_image(\n     def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n+        do_reduce_labels: Optional[bool] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         do_center_crop: Optional[bool] = None,\n@@ -284,6 +306,7 @@ def _preprocess_mask(\n \n         segmentation_map = self._preprocess(\n             image=segmentation_map,\n+            do_reduce_labels=do_reduce_labels,\n             do_resize=do_resize,\n             size=size,\n             resample=PILImageResampling.NEAREST,\n@@ -312,6 +335,7 @@ def preprocess(\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[dict[str, int]] = None,\n         do_flip_channel_order: Optional[bool] = None,\n+        do_reduce_labels: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -342,6 +366,10 @@ def preprocess(\n                 Size of the center crop if `do_center_crop` is set to `True`.\n             do_flip_channel_order (`bool`, *optional*, defaults to `self.do_flip_channel_order`):\n                 Whether to flip the channel order of the image.\n+            do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n+                Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n+                is used for background, and background itself is not included in all classes of a dataset (e.g.\n+                ADE20k). The background label will be replaced by 255.\n             return_tensors (`str` or `TensorType`, *optional*):\n                 The type of tensors to return. Can be one of:\n                     - Unset: Return a list of `np.ndarray`.\n@@ -374,6 +402,8 @@ def preprocess(\n         crop_size = crop_size if crop_size is not None else self.crop_size\n         crop_size = get_size_dict(crop_size, param_name=\"crop_size\")\n \n+        do_reduce_labels = do_reduce_labels if do_reduce_labels is not None else self.do_reduce_labels\n+\n         images = make_list_of_images(images)\n \n         if segmentation_maps is not None:\n@@ -426,6 +456,7 @@ def preprocess(\n             segmentation_maps = [\n                 self._preprocess_mask(\n                     segmentation_map=segmentation_map,\n+                    do_reduce_labels=do_reduce_labels,\n                     do_resize=do_resize,\n                     size=size,\n                     do_center_crop=do_center_crop,"
        },
        {
            "sha": "d727e9a30e323b495f8f76a04df945a610c7caf6",
            "filename": "src/transformers/models/mobilevit/image_processing_mobilevit_fast.py",
            "status": "modified",
            "additions": 75,
            "deletions": 13,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/3993ee1e988482d46384408c097aac28babad794/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3993ee1e988482d46384408c097aac28babad794/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py?ref=3993ee1e988482d46384408c097aac28babad794",
            "patch": "@@ -14,9 +14,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for MobileViT.\"\"\"\n \n-from typing import Optional\n-\n-import torch\n+from typing import Optional, Union\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -27,23 +25,46 @@\n )\n from ...image_utils import (\n     ChannelDimension,\n+    ImageInput,\n     PILImageResampling,\n+    SizeDict,\n     is_torch_tensor,\n     make_list_of_images,\n     pil_torch_interpolation_mapping,\n     validate_kwargs,\n )\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n \n \n class MobileVitFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n     do_flip_channel_order (`bool`, *optional*, defaults to `self.do_flip_channel_order`):\n         Whether to flip the color channels from RGB to BGR or vice versa.\n+    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n+        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n+        is used for background, and background itself is not included in all classes of a dataset (e.g.\n+        ADE20k). The background label will be replaced by 255.\n     \"\"\"\n \n     do_flip_channel_order: Optional[bool]\n+    do_reduce_labels: Optional[bool]\n \n \n @auto_docstring\n@@ -58,28 +79,44 @@ class MobileViTImageProcessorFast(BaseImageProcessorFast):\n     do_normalize = None\n     do_convert_rgb = None\n     do_flip_channel_order = True\n+    do_reduce_labels = False\n     valid_kwargs = MobileVitFastImageProcessorKwargs\n \n     def __init__(self, **kwargs: Unpack[MobileVitFastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n+    # Copied from transformers.models.beit.image_processing_beit_fast.BeitImageProcessorFast.reduce_label\n+    def reduce_label(self, labels: list[\"torch.Tensor\"]):\n+        for idx in range(len(labels)):\n+            label = labels[idx]\n+            label = torch.where(label == 0, torch.tensor(255, dtype=label.dtype), label)\n+            label = label - 1\n+            label = torch.where(label == 254, torch.tensor(255, dtype=label.dtype), label)\n+            labels[idx] = label\n+\n+        return label\n+\n     def _preprocess(\n         self,\n-        images,\n+        images: list[\"torch.Tensor\"],\n+        do_reduce_labels: bool,\n         do_resize: bool,\n-        size: Optional[dict],\n-        interpolation: Optional[str],\n+        size: Optional[SizeDict],\n+        interpolation: Optional[\"F.InterpolationMode\"],\n         do_rescale: bool,\n         rescale_factor: Optional[float],\n         do_center_crop: bool,\n-        crop_size: Optional[dict],\n+        crop_size: Optional[SizeDict],\n         do_flip_channel_order: bool,\n         disable_grouping: bool,\n-        return_tensors: Optional[str],\n+        return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n-    ):\n+    ) -> BatchFeature:\n         processed_images = []\n \n+        if do_reduce_labels:\n+            images = self.reduce_label(images)\n+\n         # Group images by shape for more efficient batch processing\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n@@ -119,6 +156,16 @@ def _preprocess(\n \n         return processed_images\n \n+    def _preprocess_images(\n+        self,\n+        images,\n+        **kwargs,\n+    ):\n+        \"\"\"Preprocesses images.\"\"\"\n+        kwargs[\"do_reduce_labels\"] = False\n+        processed_images = self._preprocess(images=images, **kwargs)\n+        return processed_images\n+\n     def _preprocess_segmentation_maps(\n         self,\n         segmentation_maps,\n@@ -149,8 +196,8 @@ def _preprocess_segmentation_maps(\n     @auto_docstring\n     def preprocess(\n         self,\n-        images,\n-        segmentation_maps=None,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput] = None,\n         **kwargs: Unpack[MobileVitFastImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -192,7 +239,7 @@ def preprocess(\n         kwargs.pop(\"default_to_square\")\n         kwargs.pop(\"data_format\")\n \n-        images = self._preprocess(\n+        images = self._preprocess_images(\n             images=images,\n             **kwargs,\n         )\n@@ -207,6 +254,21 @@ def preprocess(\n         return BatchFeature(data={\"pixel_values\": images})\n \n     def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n+        \"\"\"\n+        Converts the output of [`MobileNetV2ForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`MobileNetV2ForSemanticSegmentation`]):\n+                Raw outputs of the model.\n+            target_sizes (`list[Tuple]` of length `batch_size`, *optional*):\n+                List of tuples corresponding to the requested final size (height, width) of each prediction. If unset,\n+                predictions will not be resized.\n+\n+        Returns:\n+            semantic_segmentation: `list[torch.Tensor]` of length `batch_size`, where each item is a semantic\n+            segmentation map of shape (height, width) corresponding to the target_sizes entry (if `target_sizes` is\n+            specified). Each entry of each `torch.Tensor` correspond to a semantic class id.\n+        \"\"\"\n         logits = outputs.logits\n \n         # Resize logits and compute semantic segmentation maps"
        },
        {
            "sha": "7027a0b77a3c2518d790b8671e9e53ee80c2b6a8",
            "filename": "tests/models/mobilenet_v2/test_image_processing_mobilenet_v2.py",
            "status": "modified",
            "additions": 178,
            "deletions": 2,
            "changes": 180,
            "blob_url": "https://github.com/huggingface/transformers/blob/3993ee1e988482d46384408c097aac28babad794/tests%2Fmodels%2Fmobilenet_v2%2Ftest_image_processing_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3993ee1e988482d46384408c097aac28babad794/tests%2Fmodels%2Fmobilenet_v2%2Ftest_image_processing_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilenet_v2%2Ftest_image_processing_mobilenet_v2.py?ref=3993ee1e988482d46384408c097aac28babad794",
            "patch": "@@ -15,13 +15,21 @@\n \n import unittest\n \n+import requests\n+from datasets import load_dataset\n+\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torchvision_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n+if is_torch_available():\n+    import torch\n+\n if is_vision_available():\n+    from PIL import Image\n+\n     from transformers import MobileNetV2ImageProcessor\n \n     if is_torchvision_available():\n@@ -41,6 +49,7 @@ def __init__(\n         size=None,\n         do_center_crop=True,\n         crop_size=None,\n+        do_reduce_labels=False,\n     ):\n         size = size if size is not None else {\"shortest_edge\": 20}\n         crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n@@ -54,13 +63,15 @@ def __init__(\n         self.size = size\n         self.do_center_crop = do_center_crop\n         self.crop_size = crop_size\n+        self.do_reduce_labels = do_reduce_labels\n \n     def prepare_image_processor_dict(self):\n         return {\n             \"do_resize\": self.do_resize,\n             \"size\": self.size,\n             \"do_center_crop\": self.do_center_crop,\n             \"crop_size\": self.crop_size,\n+            \"do_reduce_labels\": self.do_reduce_labels,\n         }\n \n     def expected_output_image_shape(self, images):\n@@ -78,6 +89,17 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n         )\n \n \n+def prepare_semantic_single_inputs():\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    example = ds[0]\n+    return example[\"image\"], example[\"map\"]\n+\n+\n+def prepare_semantic_batch_inputs():\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    return list(ds[\"image\"][:2]), list(ds[\"map\"][:2])\n+\n+\n @require_torch\n @require_vision\n class MobileNetV2ImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n@@ -99,13 +121,167 @@ def test_image_processor_properties(self):\n             self.assertTrue(hasattr(image_processor, \"size\"))\n             self.assertTrue(hasattr(image_processor, \"do_center_crop\"))\n             self.assertTrue(hasattr(image_processor, \"crop_size\"))\n+            self.assertTrue(hasattr(image_processor, \"do_reduce_labels\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n         for image_processing_class in self.image_processor_list:\n             image_processor = image_processing_class.from_dict(self.image_processor_dict)\n             self.assertEqual(image_processor.size, {\"shortest_edge\": 20})\n             self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n+            self.assertEqual(image_processor.do_reduce_labels, False)\n \n-            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, size=42, crop_size=84, do_reduce_labels=True\n+            )\n             self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n             self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n+            self.assertEqual(image_processor.do_reduce_labels, True)\n+\n+    def test_call_segmentation_maps(self):\n+        # Initialize image_processing\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+            maps = []\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+                maps.append(torch.zeros(image.shape[-2:]).long())\n+\n+            # Test not batched input\n+            encoding = image_processing(image_inputs[0], maps[0], return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test batched\n+            encoding = image_processing(image_inputs, maps, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test not batched input (PIL images)\n+            image, segmentation_map = prepare_semantic_single_inputs()\n+\n+            encoding = image_processing(image, segmentation_map, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test batched input (PIL images)\n+            images, segmentation_maps = prepare_semantic_batch_inputs()\n+\n+            encoding = image_processing(images, segmentation_maps, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    2,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    2,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+    def test_reduce_labels(self):\n+        # Initialize image_processing\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+\n+            # ADE20k has 150 classes, and the background is included, so labels should be between 0 and 150\n+            image, map = prepare_semantic_single_inputs()\n+            encoding = image_processing(image, map, return_tensors=\"pt\")\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 150)\n+\n+            image_processing.do_reduce_labels = True\n+            encoding = image_processing(image, map, return_tensors=\"pt\")\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        # Test with single image\n+        dummy_image = Image.open(\n+            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n+        )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+\n+        # Test with single image and segmentation map\n+        image, segmentation_map = prepare_semantic_single_inputs()\n+\n+        encoding_slow = image_processor_slow(image, segmentation_map, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(image, segmentation_map, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        torch.testing.assert_close(encoding_slow.labels, encoding_fast.labels, atol=1e-1, rtol=1e-3)"
        },
        {
            "sha": "a09c2824ca09a1466d44777695d59bb4cf09f06a",
            "filename": "tests/models/mobilevit/test_image_processing_mobilevit.py",
            "status": "modified",
            "additions": 25,
            "deletions": 1,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/3993ee1e988482d46384408c097aac28babad794/tests%2Fmodels%2Fmobilevit%2Ftest_image_processing_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3993ee1e988482d46384408c097aac28babad794/tests%2Fmodels%2Fmobilevit%2Ftest_image_processing_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilevit%2Ftest_image_processing_mobilevit.py?ref=3993ee1e988482d46384408c097aac28babad794",
            "patch": "@@ -50,6 +50,7 @@ def __init__(\n         do_center_crop=True,\n         crop_size=None,\n         do_flip_channel_order=True,\n+        do_reduce_labels=False,\n     ):\n         size = size if size is not None else {\"shortest_edge\": 20}\n         crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n@@ -64,6 +65,7 @@ def __init__(\n         self.do_center_crop = do_center_crop\n         self.crop_size = crop_size\n         self.do_flip_channel_order = do_flip_channel_order\n+        self.do_reduce_labels = do_reduce_labels\n \n     def prepare_image_processor_dict(self):\n         return {\n@@ -72,6 +74,7 @@ def prepare_image_processor_dict(self):\n             \"do_center_crop\": self.do_center_crop,\n             \"crop_size\": self.crop_size,\n             \"do_flip_channel_order\": self.do_flip_channel_order,\n+            \"do_reduce_labels\": self.do_reduce_labels,\n         }\n \n     def expected_output_image_shape(self, images):\n@@ -122,16 +125,21 @@ def test_image_processor_properties(self):\n             self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n             self.assertTrue(hasattr(image_processing, \"center_crop\"))\n             self.assertTrue(hasattr(image_processing, \"do_flip_channel_order\"))\n+            self.assertTrue(hasattr(image_processing, \"do_reduce_labels\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n         for image_processing_class in self.image_processor_list:\n             image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n             self.assertEqual(image_processor.size, {\"shortest_edge\": 20})\n             self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n+            self.assertEqual(image_processor.do_reduce_labels, False)\n \n-            image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n+            image_processor = self.image_processing_class.from_dict(\n+                self.image_processor_dict, size=42, crop_size=84, do_reduce_labels=True\n+            )\n             self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n             self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n+            self.assertEqual(image_processor.do_reduce_labels, True)\n \n     def test_call_segmentation_maps(self):\n         for image_processing_class in self.image_processor_list:\n@@ -240,6 +248,22 @@ def test_call_segmentation_maps(self):\n             self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n             self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n \n+    def test_reduce_labels(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = self.image_processing_class(**self.image_processor_dict)\n+\n+            # ADE20k has 150 classes, and the background is included, so labels should be between 0 and 150\n+            image, map = prepare_semantic_single_inputs()\n+            encoding = image_processing(image, map, return_tensors=\"pt\")\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 150)\n+\n+            image_processing.do_reduce_labels = True\n+            encoding = image_processing(image, map, return_tensors=\"pt\")\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n     @require_vision\n     @require_torch\n     def test_slow_fast_equivalence(self):"
        }
    ],
    "stats": {
        "total": 773,
        "additions": 722,
        "deletions": 51
    }
}