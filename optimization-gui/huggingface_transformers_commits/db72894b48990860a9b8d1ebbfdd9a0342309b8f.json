{
    "author": "zucchini-nlp",
    "message": "Chat template: save and load correctly for processors (#33462)\n\n* fix\r\n\r\n* add tests\r\n\r\n* fix tests\r\n\r\n* Update tests/models/llava/test_processor_llava.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* fix\r\n\r\n* fix tests\r\n\r\n* update tests\r\n\r\n---------\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>",
    "sha": "db72894b48990860a9b8d1ebbfdd9a0342309b8f",
    "files": [
        {
            "sha": "8b33c456924dedeec303838c86621f7eb939928d",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/db72894b48990860a9b8d1ebbfdd9a0342309b8f/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/db72894b48990860a9b8d1ebbfdd9a0342309b8f/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=db72894b48990860a9b8d1ebbfdd9a0342309b8f",
            "patch": "@@ -502,9 +502,12 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n         output_chat_template_file = os.path.join(save_directory, CHAT_TEMPLATE_NAME)\n \n         processor_dict = self.to_dict()\n-        chat_template = processor_dict.pop(\"chat_template\", None)\n-        if chat_template is not None:\n-            chat_template_json_string = json.dumps({\"chat_template\": chat_template}, indent=2, sort_keys=True) + \"\\n\"\n+        # Save `chat_template` in its own file. We can't get it from `processor_dict` as we popped it in `to_dict`\n+        # to avoid serializing chat template in json config file. So let's get it from `self` directly\n+        if self.chat_template is not None:\n+            chat_template_json_string = (\n+                json.dumps({\"chat_template\": self.chat_template}, indent=2, sort_keys=True) + \"\\n\"\n+            )\n             with open(output_chat_template_file, \"w\", encoding=\"utf-8\") as writer:\n                 writer.write(chat_template_json_string)\n             logger.info(f\"chat template saved in {output_chat_template_file}\")"
        },
        {
            "sha": "e62769e345091793205ad00a9bb5f2f35684a96f",
            "filename": "tests/models/llava/test_processor_llava.py",
            "status": "modified",
            "additions": 26,
            "deletions": 3,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/db72894b48990860a9b8d1ebbfdd9a0342309b8f/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/db72894b48990860a9b8d1ebbfdd9a0342309b8f/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py?ref=db72894b48990860a9b8d1ebbfdd9a0342309b8f",
            "patch": "@@ -11,6 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import json\n import shutil\n import tempfile\n import unittest\n@@ -32,11 +33,11 @@ class LlavaProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n \n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n+\n         image_processor = CLIPImageProcessor(do_center_crop=False)\n         tokenizer = LlamaTokenizerFast.from_pretrained(\"huggyllama/llama-7b\")\n-\n-        processor = LlavaProcessor(image_processor=image_processor, tokenizer=tokenizer)\n-\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = LlavaProcessor(image_processor, tokenizer, **processor_kwargs)\n         processor.save_pretrained(self.tmpdirname)\n \n     def get_tokenizer(self, **kwargs):\n@@ -48,6 +49,28 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n+    def prepare_processor_dict(self):\n+        return {\"chat_template\": \"dummy_template\"}\n+\n+    @unittest.skip(\n+        \"Skip because the model has no processor kwargs except for chat template and\"\n+        \"chat template is saved as a separate file. Stop skipping this test when the processor\"\n+        \"has new kwargs saved in config file.\"\n+    )\n+    def test_processor_to_json_string(self):\n+        pass\n+\n+    def test_chat_template_is_saved(self):\n+        processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)\n+        processor_dict_loaded = json.loads(processor_loaded.to_json_string())\n+        # chat templates aren't serialized to json in processors\n+        self.assertFalse(\"chat_template\" in processor_dict_loaded.keys())\n+\n+        # they have to be saved as separate file and loaded back from that file\n+        # so we check if the same template is loaded\n+        processor_dict = self.prepare_processor_dict()\n+        self.assertTrue(processor_loaded.chat_template == processor_dict.get(\"chat_template\", None))\n+\n     def test_can_load_various_tokenizers(self):\n         for checkpoint in [\"Intel/llava-gemma-2b\", \"llava-hf/llava-1.5-7b-hf\"]:\n             processor = LlavaProcessor.from_pretrained(checkpoint)"
        },
        {
            "sha": "450034f4151dd07a522e3b3817d3f19a23e1ed30",
            "filename": "tests/models/llava_next/test_processor_llava_next.py",
            "status": "modified",
            "additions": 47,
            "deletions": 2,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/db72894b48990860a9b8d1ebbfdd9a0342309b8f/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/db72894b48990860a9b8d1ebbfdd9a0342309b8f/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py?ref=db72894b48990860a9b8d1ebbfdd9a0342309b8f",
            "patch": "@@ -11,20 +11,65 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import json\n+import tempfile\n import unittest\n \n import torch\n \n+from transformers import AutoProcessor, LlamaTokenizerFast, LlavaNextProcessor\n from transformers.testing_utils import require_vision\n from transformers.utils import is_vision_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n+\n \n if is_vision_available():\n-    from transformers import AutoProcessor\n+    from transformers import CLIPImageProcessor\n \n \n @require_vision\n-class LlavaProcessorTest(unittest.TestCase):\n+class LlavaNextProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = LlavaNextProcessor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+\n+        image_processor = CLIPImageProcessor()\n+        tokenizer = LlamaTokenizerFast.from_pretrained(\"huggyllama/llama-7b\")\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = LlavaNextProcessor(image_processor, tokenizer, **processor_kwargs)\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return LlavaNextProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return LlavaNextProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def prepare_processor_dict(self):\n+        return {\"chat_template\": \"dummy_template\"}\n+\n+    @unittest.skip(\n+        \"Skip because the model has no processor kwargs except for chat template and\"\n+        \"chat template is saved as a separate file. Stop skipping this test when the processor\"\n+        \"has new kwargs saved in config file.\"\n+    )\n+    def test_processor_to_json_string(self):\n+        pass\n+\n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_chat_template_is_saved\n+    def test_chat_template_is_saved(self):\n+        processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)\n+        processor_dict_loaded = json.loads(processor_loaded.to_json_string())\n+        # chat templates aren't serialized to json in processors\n+        self.assertFalse(\"chat_template\" in processor_dict_loaded.keys())\n+\n+        # they have to be saved as separate file and loaded back from that file\n+        # so we check if the same template is loaded\n+        processor_dict = self.prepare_processor_dict()\n+        self.assertTrue(processor_loaded.chat_template == processor_dict.get(\"chat_template\", None))\n+\n     def test_chat_template(self):\n         processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-vicuna-7b-hf\")\n         expected_prompt = \"USER: <image>\\nWhat is shown in this image? ASSISTANT:\""
        },
        {
            "sha": "f747c18250b620b3d5e776b2f1092453c764b3d0",
            "filename": "tests/models/llava_onevision/test_processing_llava_onevision.py",
            "status": "modified",
            "additions": 27,
            "deletions": 2,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/db72894b48990860a9b8d1ebbfdd9a0342309b8f/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/db72894b48990860a9b8d1ebbfdd9a0342309b8f/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py?ref=db72894b48990860a9b8d1ebbfdd9a0342309b8f",
            "patch": "@@ -11,6 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import json\n import shutil\n import tempfile\n import unittest\n@@ -40,9 +41,10 @@ def setUp(self):\n         image_processor = LlavaOnevisionImageProcessor()\n         video_processor = LlavaOnevisionVideoProcessor()\n         tokenizer = Qwen2TokenizerFast.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n+        processor_kwargs = self.prepare_processor_dict()\n \n         processor = LlavaOnevisionProcessor(\n-            video_processor=video_processor, image_processor=image_processor, tokenizer=tokenizer\n+            video_processor=video_processor, image_processor=image_processor, tokenizer=tokenizer, **processor_kwargs\n         )\n         processor.save_pretrained(self.tmpdirname)\n \n@@ -52,9 +54,32 @@ def get_tokenizer(self, **kwargs):\n     def get_image_processor(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n \n-    def get_Video_processor(self, **kwargs):\n+    def get_video_processor(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).video_processor\n \n+    def prepare_processor_dict(self):\n+        return {\"chat_template\": \"dummy_template\"}\n+\n+    @unittest.skip(\n+        \"Skip because the model has no processor kwargs except for chat template and\"\n+        \"chat template is saved as a separate file. Stop skipping this test when the processor\"\n+        \"has new kwargs saved in config file.\"\n+    )\n+    def test_processor_to_json_string(self):\n+        pass\n+\n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_chat_template_is_saved\n+    def test_chat_template_is_saved(self):\n+        processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)\n+        processor_dict_loaded = json.loads(processor_loaded.to_json_string())\n+        # chat templates aren't serialized to json in processors\n+        self.assertFalse(\"chat_template\" in processor_dict_loaded.keys())\n+\n+        # they have to be saved as separate file and loaded back from that file\n+        # so we check if the same template is loaded\n+        processor_dict = self.prepare_processor_dict()\n+        self.assertTrue(processor_loaded.chat_template == processor_dict.get(\"chat_template\", None))\n+\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n "
        }
    ],
    "stats": {
        "total": 116,
        "additions": 106,
        "deletions": 10
    }
}