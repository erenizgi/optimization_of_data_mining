{
    "author": "Cyrilvallez",
    "message": "ðŸš¨ Bump to Python 3.10 and rework how we check 3rd-party libraries existence (#41268)\n\n* cleanup\n\n* add check\n\n* fix\n\n* remove all global variables\n\n* fix\n\n* add lru caches everywhere\n\n* fix\n\n* fix\n\n* style\n\n* improve\n\n* reorder all functions\n\n* fix order\n\n* improve\n\n* fix\n\n* fix\n\n* fix",
    "sha": "55b172b8eb839300e697e91911eac66db7441316",
    "files": [
        {
            "sha": "8084c15330264659d12d30b38bbd3f19631fcb83",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b172b8eb839300e697e91911eac66db7441316/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b172b8eb839300e697e91911eac66db7441316/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=55b172b8eb839300e697e91911eac66db7441316",
            "patch": "@@ -143,7 +143,7 @@\n     \"pytest-timeout\",\n     \"pytest-xdist\",\n     \"pytest-order\",\n-    \"python>=3.9.0\",\n+    \"python>=3.10.0\",\n     \"ray[tune]>=2.7.0\",\n     \"regex!=2019.12.17\",\n     \"requests\","
        },
        {
            "sha": "40e1339bd11665283eb03912275ed0a19007bd98",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=55b172b8eb839300e697e91911eac66db7441316",
            "patch": "@@ -225,7 +225,6 @@\n         \"is_py3nvml_available\",\n         \"is_pyctcdecode_available\",\n         \"is_sacremoses_available\",\n-        \"is_safetensors_available\",\n         \"is_scipy_available\",\n         \"is_sentencepiece_available\",\n         \"is_sklearn_available\",\n@@ -746,7 +745,6 @@\n     from .utils import is_py3nvml_available as is_py3nvml_available\n     from .utils import is_pyctcdecode_available as is_pyctcdecode_available\n     from .utils import is_sacremoses_available as is_sacremoses_available\n-    from .utils import is_safetensors_available as is_safetensors_available\n     from .utils import is_sklearn_available as is_sklearn_available\n     from .utils import is_torch_hpu_available as is_torch_hpu_available\n     from .utils import is_torch_mlu_available as is_torch_mlu_available"
        },
        {
            "sha": "df4777fe9b34190950372320cdbab6f66ff9aea4",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=55b172b8eb839300e697e91911eac66db7441316",
            "patch": "@@ -52,7 +52,7 @@\n     \"pytest-timeout\": \"pytest-timeout\",\n     \"pytest-xdist\": \"pytest-xdist\",\n     \"pytest-order\": \"pytest-order\",\n-    \"python\": \"python>=3.9.0\",\n+    \"python\": \"python>=3.10.0\",\n     \"ray[tune]\": \"ray[tune]>=2.7.0\",\n     \"regex\": \"regex!=2019.12.17\",\n     \"requests\": \"requests\","
        },
        {
            "sha": "9b8cc1e2846821293290235573cf9895b21a67bb",
            "filename": "src/transformers/file_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Ffile_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Ffile_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffile_utils.py?ref=55b172b8eb839300e697e91911eac66db7441316",
            "patch": "@@ -38,7 +38,6 @@\n     S3_BUCKET_PREFIX,\n     SENTENCEPIECE_UNDERLINE,\n     SPIECE_UNDERLINE,\n-    TORCH_FX_REQUIRED_VERSION,\n     TRANSFORMERS_CACHE,\n     TRANSFORMERS_DYNAMIC_MODULE_NAME,\n     WEIGHTS_INDEX_NAME,\n@@ -99,9 +98,7 @@\n     is_timm_available,\n     is_tokenizers_available,\n     is_torch_available,\n-    is_torch_bf16_available,\n     is_torch_cuda_available,\n-    is_torch_fx_available,\n     is_torch_fx_proxy,\n     is_torch_mps_available,\n     is_torch_tf32_available,"
        },
        {
            "sha": "1f18539514dcad1fb79253bf70ded258d1661cb5",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=55b172b8eb839300e697e91911eac66db7441316",
            "patch": "@@ -32,7 +32,7 @@\n from packaging import version\n \n from ..utils import is_torch_flex_attn_available, logging\n-from ..utils.import_utils import _torch_version, is_torch_less_or_equal, is_torchdynamo_compiling\n+from ..utils.import_utils import get_torch_version, is_torch_less_or_equal, is_torchdynamo_compiling\n \n \n if is_torch_flex_attn_available():\n@@ -70,7 +70,7 @@ def __init__(self, training):\n             # In PyTorch 2.6.0, there's a known issue with flex attention compilation which may\n             # cause errors. The suggested fix is to compile with \"max-autotune-no-cudagraphs\"\n             # see https://github.com/pytorch/pytorch/issues/146260 for training\n-            elif version.parse(_torch_version).base_version == \"2.6.0\" and training:\n+            elif version.parse(get_torch_version()).base_version == \"2.6.0\" and training:\n                 self._compiled_flex_attention = torch.compile(\n                     flex_attention, dynamic=False, mode=\"max-autotune-no-cudagraphs\"\n                 )"
        },
        {
            "sha": "b9948c5c354e2c090db0236ed228f27af45b62b2",
            "filename": "src/transformers/models/deprecated/jukebox/tokenization_jukebox.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Ftokenization_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Ftokenization_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Ftokenization_jukebox.py?ref=55b172b8eb839300e697e91911eac66db7441316",
            "patch": "@@ -27,7 +27,7 @@\n from ....tokenization_utils import AddedToken, PreTrainedTokenizer\n from ....tokenization_utils_base import BatchEncoding\n from ....utils import TensorType, is_torch_available, logging\n-from ....utils.generic import _is_numpy\n+from ....utils.generic import is_numpy_array\n \n \n logger = logging.get_logger(__name__)\n@@ -288,7 +288,7 @@ def convert_to_tensors(\n             is_tensor = torch.is_tensor\n         else:\n             as_tensor = np.asarray\n-            is_tensor = _is_numpy\n+            is_tensor = is_numpy_array\n \n         # Do the tensor conversion in batch\n "
        },
        {
            "sha": "bb65394e682c39d21510f61e1c0a8c2ba548c7bb",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=55b172b8eb839300e697e91911eac66db7441316",
            "patch": "@@ -130,7 +130,6 @@\n     is_qutlass_available,\n     is_rjieba_available,\n     is_sacremoses_available,\n-    is_safetensors_available,\n     is_schedulefree_available,\n     is_scipy_available,\n     is_sentencepiece_available,\n@@ -160,7 +159,6 @@\n     is_torchao_available,\n     is_torchaudio_available,\n     is_torchcodec_available,\n-    is_torchdynamo_available,\n     is_torchvision_available,\n     is_triton_available,\n     is_vision_available,\n@@ -499,13 +497,6 @@ def require_g2p_en(test_case):\n     return unittest.skipUnless(is_g2p_en_available(), \"test requires g2p_en\")(test_case)\n \n \n-def require_safetensors(test_case):\n-    \"\"\"\n-    Decorator marking a test that requires safetensors. These tests are skipped when safetensors isn't installed.\n-    \"\"\"\n-    return unittest.skipUnless(is_safetensors_available(), \"test requires safetensors\")(test_case)\n-\n-\n def require_rjieba(test_case):\n     \"\"\"\n     Decorator marking a test that requires rjieba. These tests are skipped when rjieba isn't installed.\n@@ -1012,11 +1003,6 @@ def require_torch_multi_hpu(test_case):\n     torch_device = None\n \n \n-def require_torchdynamo(test_case):\n-    \"\"\"Decorator marking a test that requires TorchDynamo\"\"\"\n-    return unittest.skipUnless(is_torchdynamo_available(), \"test requires TorchDynamo\")(test_case)\n-\n-\n def require_torchao(test_case):\n     \"\"\"Decorator marking a test that requires torchao\"\"\"\n     return unittest.skipUnless(is_torchao_available(), \"test requires torchao\")(test_case)"
        },
        {
            "sha": "f0aaf3fed9e739fe024a9559e7c949ef18d37cc8",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=55b172b8eb839300e697e91911eac66db7441316",
            "patch": "@@ -108,13 +108,11 @@\n     ENV_VARS_TRUE_AND_AUTO_VALUES,\n     ENV_VARS_TRUE_VALUES,\n     GGUF_MIN_VERSION,\n-    TORCH_FX_REQUIRED_VERSION,\n     TRITON_MIN_VERSION,\n     XLA_FSDPV2_MIN_VERSION,\n     DummyObject,\n     OptionalDependencyNotAvailable,\n     _LazyModule,\n-    ccl_version,\n     check_torch_load_is_safe,\n     direct_transformers_import,\n     get_torch_version,\n@@ -199,7 +197,6 @@\n     is_rjieba_available,\n     is_rocm_platform,\n     is_sacremoses_available,\n-    is_safetensors_available,\n     is_sagemaker_dp_enabled,\n     is_sagemaker_mp_enabled,\n     is_schedulefree_available,\n@@ -218,16 +215,12 @@\n     is_tokenizers_available,\n     is_torch_accelerator_available,\n     is_torch_available,\n-    is_torch_bf16_available,\n     is_torch_bf16_available_on_device,\n-    is_torch_bf16_cpu_available,\n     is_torch_bf16_gpu_available,\n-    is_torch_compile_available,\n     is_torch_cuda_available,\n     is_torch_deterministic,\n     is_torch_flex_attn_available,\n     is_torch_fp16_available_on_device,\n-    is_torch_fx_available,\n     is_torch_fx_proxy,\n     is_torch_greater_or_equal,\n     is_torch_hpu_available,\n@@ -237,7 +230,6 @@\n     is_torch_neuroncore_available,\n     is_torch_npu_available,\n     is_torch_optimi_available,\n-    is_torch_sdpa_available,\n     is_torch_tensorrt_fx_available,\n     is_torch_tf32_available,\n     is_torch_xla_available,\n@@ -246,7 +238,6 @@\n     is_torchaudio_available,\n     is_torchcodec_available,\n     is_torchdistx_available,\n-    is_torchdynamo_available,\n     is_torchdynamo_compiling,\n     is_torchdynamo_exporting,\n     is_torchvision_available,"
        },
        {
            "sha": "7e1f4b48507f754c3279c50f4825f2c8789f71a9",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 17,
            "deletions": 45,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=55b172b8eb839300e697e91911eac66db7441316",
            "patch": "@@ -31,25 +31,23 @@\n import numpy as np\n \n from ..utils import logging\n-from .import_utils import (\n-    is_mlx_available,\n-    is_torch_available,\n-    is_torch_fx_proxy,\n-    requires,\n-)\n+from .import_utils import is_mlx_available, is_torch_available, is_torch_fx_proxy, requires\n \n \n _CAN_RECORD_REGISTRY = {}\n \n \n logger = logging.get_logger(__name__)\n \n+_is_torch_available = False\n if is_torch_available():\n     # required for @can_return_tuple decorator to work with torchdynamo\n     import torch\n \n     from ..model_debugging_utils import model_addition_debugger_context\n \n+    _is_torch_available = True\n+\n \n # vendored from distutils.util\n def strtobool(val):\n@@ -116,46 +114,33 @@ def is_tensor(x):\n     return False\n \n \n-def _is_numpy(x):\n-    return isinstance(x, np.ndarray)\n-\n-\n def is_numpy_array(x):\n     \"\"\"\n     Tests if `x` is a numpy array or not.\n     \"\"\"\n-    return _is_numpy(x)\n-\n-\n-def _is_torch(x):\n-    import torch\n-\n-    return isinstance(x, torch.Tensor)\n+    return isinstance(x, np.ndarray)\n \n \n def is_torch_tensor(x):\n     \"\"\"\n     Tests if `x` is a torch tensor or not. Safe to call even if torch is not installed.\n     \"\"\"\n-    return False if not is_torch_available() else _is_torch(x)\n-\n-\n-def _is_torch_device(x):\n-    import torch\n-\n-    return isinstance(x, torch.device)\n+    return _is_torch_available and isinstance(x, torch.Tensor)\n \n \n def is_torch_device(x):\n     \"\"\"\n     Tests if `x` is a torch device or not. Safe to call even if torch is not installed.\n     \"\"\"\n-    return False if not is_torch_available() else _is_torch_device(x)\n-\n+    return _is_torch_available and isinstance(x, torch.device)\n \n-def _is_torch_dtype(x):\n-    import torch\n \n+def is_torch_dtype(x):\n+    \"\"\"\n+    Tests if `x` is a torch dtype or not. Safe to call even if torch is not installed.\n+    \"\"\"\n+    if not _is_torch_available:\n+        return False\n     if isinstance(x, str):\n         if hasattr(torch, x):\n             x = getattr(torch, x)\n@@ -164,13 +149,6 @@ def _is_torch_dtype(x):\n     return isinstance(x, torch.dtype)\n \n \n-def is_torch_dtype(x):\n-    \"\"\"\n-    Tests if `x` is a torch dtype or not. Safe to call even if torch is not installed.\n-    \"\"\"\n-    return False if not is_torch_available() else _is_torch_dtype(x)\n-\n-\n def _is_mlx(x):\n     import mlx.core as mx\n \n@@ -263,7 +241,7 @@ def __init_subclass__(cls) -> None:\n         This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with\n         `static_graph=True` with modules that output `ModelOutput` subclasses.\n         \"\"\"\n-        if is_torch_available():\n+        if _is_torch_available:\n             from torch.utils._pytree import register_pytree_node\n \n             register_pytree_node(\n@@ -387,7 +365,7 @@ def to_tuple(self) -> tuple:\n         return tuple(self[k] for k in self.keys())\n \n \n-if is_torch_available():\n+if _is_torch_available:\n     import torch.utils._pytree as _torch_pytree\n \n     def _model_output_flatten(output: ModelOutput) -> tuple[list[Any], \"_torch_pytree.Context\"]:\n@@ -579,23 +557,19 @@ def torch_int(x):\n     \"\"\"\n     Casts an input to a torch int64 tensor if we are in a tracing context, otherwise to a Python int.\n     \"\"\"\n-    if not is_torch_available():\n+    if not _is_torch_available:\n         return int(x)\n \n-    import torch\n-\n     return x.to(torch.int64) if torch.jit.is_tracing() and isinstance(x, torch.Tensor) else int(x)\n \n \n def torch_float(x):\n     \"\"\"\n     Casts an input to a torch float32 tensor if we are in a tracing context, otherwise to a Python float.\n     \"\"\"\n-    if not is_torch_available():\n+    if not _is_torch_available:\n         return int(x)\n \n-    import torch\n-\n     return x.to(torch.float32) if torch.jit.is_tracing() and isinstance(x, torch.Tensor) else int(x)\n \n \n@@ -788,8 +762,6 @@ def wrapper(self, *args, **kwargs):\n     return wrapper\n \n \n-# if is_torch_available():\n-# @torch._dynamo.disable\n @dataclass\n @requires(backends=(\"torch\",))\n class OutputRecorder:"
        },
        {
            "sha": "b55cfff3bd46c289a0dc46c293e08e5303d79772",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=55b172b8eb839300e697e91911eac66db7441316",
            "patch": "@@ -62,7 +62,7 @@\n from .generic import working_or_temp_dir\n from .import_utils import (\n     ENV_VARS_TRUE_VALUES,\n-    _torch_version,\n+    get_torch_version,\n     is_torch_available,\n     is_training_run_on_sagemaker,\n )\n@@ -227,7 +227,7 @@ def http_user_agent(user_agent: Union[dict, str, None] = None) -> str:\n     \"\"\"\n     ua = f\"transformers/{__version__}; python/{sys.version.split()[0]}; session_id/{SESSION_ID}\"\n     if is_torch_available():\n-        ua += f\"; torch/{_torch_version}\"\n+        ua += f\"; torch/{get_torch_version()}\"\n     if constants.HF_HUB_DISABLE_TELEMETRY:\n         return ua + \"; telemetry/off\"\n     if is_training_run_on_sagemaker():"
        },
        {
            "sha": "f20853e67165984dcdadcec85356548f21d04675",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 755,
            "deletions": 990,
            "changes": 1745,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b172b8eb839300e697e91911eac66db7441316/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=55b172b8eb839300e697e91911eac66db7441316",
            "patch": "@@ -25,7 +25,6 @@\n import shutil\n import subprocess\n import sys\n-import warnings\n from collections import OrderedDict\n from enum import Enum\n from functools import lru_cache\n@@ -41,54 +40,24 @@\n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n \n \n-# TODO: This doesn't work for all packages (`bs4`, `faiss`, etc.) Talk to Sylvain to see how to do with it better.\n+PACKAGE_DISTRIBUTION_MAPPING = importlib.metadata.packages_distributions()\n+\n+\n def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[tuple[bool, str], bool]:\n-    # Check if the package spec exists and grab its version to avoid importing a local directory\n+    \"\"\"Check if `pkg_name` exist, and optionally try to get its version\"\"\"\n     package_exists = importlib.util.find_spec(pkg_name) is not None\n     package_version = \"N/A\"\n-    if package_exists:\n+    if package_exists and return_version:\n         try:\n-            # TODO: Once python 3.9 support is dropped, `importlib.metadata.packages_distributions()`\n-            # should be used here to map from package name to distribution names\n-            # e.g. PIL -> Pillow, Pillow-SIMD; quark -> amd-quark; onnxruntime -> onnxruntime-gpu.\n-            # `importlib.metadata.packages_distributions()` is not available in Python 3.9.\n-\n-            # Primary method to get the package version\n-            package_version = importlib.metadata.version(pkg_name)\n+            # importlib.metadata works with the distribution package, which may be different from the import\n+            # name (e.g. `PIL` is the import name, but `pillow` is the distribution name)\n+            distribution_name = PACKAGE_DISTRIBUTION_MAPPING[pkg_name][0]\n+            package_version = importlib.metadata.version(distribution_name)\n         except importlib.metadata.PackageNotFoundError:\n-            # Fallback method: Only for \"torch\" and versions containing \"dev\"\n-            if pkg_name == \"torch\":\n-                try:\n-                    package = importlib.import_module(pkg_name)\n-                    temp_version = getattr(package, \"__version__\", \"N/A\")\n-                    # Check if the version contains \"dev\"\n-                    if \"dev\" in temp_version:\n-                        package_version = temp_version\n-                        package_exists = True\n-                    else:\n-                        package_exists = False\n-                except ImportError:\n-                    # If the package can't be imported, it's not available\n-                    package_exists = False\n-            elif pkg_name == \"quark\":\n-                # TODO: remove once `importlib.metadata.packages_distributions()` is supported.\n-                try:\n-                    package_version = importlib.metadata.version(\"amd-quark\")\n-                except Exception:\n-                    package_exists = False\n-            elif pkg_name == \"triton\":\n-                try:\n-                    # import triton works for both linux and windows\n-                    package = importlib.import_module(pkg_name)\n-                    package_version = getattr(package, \"__version__\", \"N/A\")\n-                except Exception:\n-                    try:\n-                        package_version = importlib.metadata.version(\"pytorch-triton\")  # pytorch-triton\n-                    except Exception:\n-                        package_exists = False\n-            else:\n-                # For packages other than \"torch\", don't attempt the fallback and set as not available\n-                package_exists = False\n+            # If we cannot find the metadata (because of editable install for example), try to import directly.\n+            # Note that this branch will almost never be run, so we do not import packages for nothing here\n+            package = importlib.import_module(pkg_name)\n+            package_version = getattr(package, \"__version__\", \"N/A\")\n         logger.debug(f\"Detected {pkg_name} version: {package_version}\")\n     if return_version:\n         return package_exists, package_version\n@@ -102,10 +71,6 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n # Try to run a native pytorch job in an environment with TorchXLA installed by setting this value to 0.\n USE_TORCH_XLA = os.environ.get(\"USE_TORCH_XLA\", \"1\").upper()\n \n-# `transformers` requires `torch>=1.11` but this variable is exposed publicly, and we can't simply remove it.\n-# This is the version of torch required to run torch.fx features and torch.onnx with dictionary inputs.\n-TORCH_FX_REQUIRED_VERSION = version.parse(\"1.10\")\n-\n ACCELERATE_MIN_VERSION = \"0.26.0\"\n SCHEDULEFREE_MIN_VERSION = \"1.2.6\"\n FSDP_MIN_VERSION = \"1.12.0\"\n@@ -117,217 +82,54 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n AUTOROUND_MIN_VERSION = \"0.5.0\"\n TRITON_MIN_VERSION = \"1.0.0\"\n \n-_accelerate_available, _accelerate_version = _is_package_available(\"accelerate\", return_version=True)\n-_apex_available = _is_package_available(\"apex\")\n-_apollo_torch_available = _is_package_available(\"apollo_torch\")\n-_aqlm_available = _is_package_available(\"aqlm\")\n-_vptq_available, _vptq_version = _is_package_available(\"vptq\", return_version=True)\n-_av_available = importlib.util.find_spec(\"av\") is not None\n-_decord_available = importlib.util.find_spec(\"decord\") is not None\n-_torchcodec_available = importlib.util.find_spec(\"torchcodec\") is not None\n-_libcst_available = _is_package_available(\"libcst\")\n-_bitsandbytes_available = _is_package_available(\"bitsandbytes\")\n-_eetq_available = _is_package_available(\"eetq\")\n-_fbgemm_gpu_available = _is_package_available(\"fbgemm_gpu\")\n-_galore_torch_available = _is_package_available(\"galore_torch\")\n-_lomo_available = _is_package_available(\"lomo_optim\")\n-_grokadamw_available = _is_package_available(\"grokadamw\")\n-_schedulefree_available, _schedulefree_version = _is_package_available(\"schedulefree\", return_version=True)\n-_torch_optimi_available = importlib.util.find_spec(\"optimi\") is not None\n-# `importlib.metadata.version` doesn't work with `bs4` but `beautifulsoup4`. For `importlib.util.find_spec`, reversed.\n-_bs4_available = importlib.util.find_spec(\"bs4\") is not None\n-_coloredlogs_available = _is_package_available(\"coloredlogs\")\n-# `importlib.metadata.util` doesn't work with `opencv-python-headless`.\n-_cv2_available = importlib.util.find_spec(\"cv2\") is not None\n-_yt_dlp_available = importlib.util.find_spec(\"yt_dlp\") is not None\n-_datasets_available = _is_package_available(\"datasets\")\n-_detectron2_available = _is_package_available(\"detectron2\")\n-# We need to check `faiss`, `faiss-cpu` and `faiss-gpu`.\n-_faiss_available = importlib.util.find_spec(\"faiss\") is not None\n-try:\n-    _faiss_version = importlib.metadata.version(\"faiss\")\n-    logger.debug(f\"Successfully imported faiss version {_faiss_version}\")\n-except importlib.metadata.PackageNotFoundError:\n-    try:\n-        _faiss_version = importlib.metadata.version(\"faiss-cpu\")\n-        logger.debug(f\"Successfully imported faiss version {_faiss_version}\")\n-    except importlib.metadata.PackageNotFoundError:\n-        try:\n-            _faiss_version = importlib.metadata.version(\"faiss-gpu\")\n-            logger.debug(f\"Successfully imported faiss version {_faiss_version}\")\n-        except importlib.metadata.PackageNotFoundError:\n-            _faiss_available = False\n-_ftfy_available = _is_package_available(\"ftfy\")\n-_g2p_en_available = _is_package_available(\"g2p_en\")\n-_hadamard_available = _is_package_available(\"fast_hadamard_transform\")\n-_ipex_available, _ipex_version = _is_package_available(\"intel_extension_for_pytorch\", return_version=True)\n-_jinja_available = _is_package_available(\"jinja2\")\n-_kenlm_available = _is_package_available(\"kenlm\")\n-_keras_nlp_available = _is_package_available(\"keras_nlp\")\n-_levenshtein_available = _is_package_available(\"Levenshtein\")\n-_librosa_available = _is_package_available(\"librosa\")\n-_natten_available = _is_package_available(\"natten\")\n-_nltk_available = _is_package_available(\"nltk\")\n-_onnx_available = _is_package_available(\"onnx\")\n-_openai_available = _is_package_available(\"openai\")\n-_optimum_available = _is_package_available(\"optimum\")\n-_auto_gptq_available = _is_package_available(\"auto_gptq\")\n-_gptqmodel_available = _is_package_available(\"gptqmodel\")\n-_auto_round_available, _auto_round_version = _is_package_available(\"auto_round\", return_version=True)\n-# `importlib.metadata.version` doesn't work with `awq`\n-_auto_awq_available = importlib.util.find_spec(\"awq\") is not None\n-_quark_available = _is_package_available(\"quark\")\n-_fp_quant_available, _fp_quant_version = _is_package_available(\"fp_quant\", return_version=True)\n-_qutlass_available, _qutlass_version = _is_package_available(\"qutlass\", return_version=True)\n-_is_optimum_quanto_available = False\n-try:\n-    importlib.metadata.version(\"optimum_quanto\")\n-    _is_optimum_quanto_available = True\n-except importlib.metadata.PackageNotFoundError:\n-    _is_optimum_quanto_available = False\n-# For compressed_tensors, only check spec to allow compressed_tensors-nightly package\n-_compressed_tensors_available = importlib.util.find_spec(\"compressed_tensors\") is not None\n-_pandas_available = _is_package_available(\"pandas\")\n-_peft_available = _is_package_available(\"peft\")\n-_phonemizer_available = _is_package_available(\"phonemizer\")\n-_uroman_available = _is_package_available(\"uroman\")\n-_psutil_available = _is_package_available(\"psutil\")\n-_py3nvml_available = _is_package_available(\"py3nvml\")\n-_pyctcdecode_available = _is_package_available(\"pyctcdecode\")\n-_pygments_available = _is_package_available(\"pygments\")\n-_pytesseract_available = _is_package_available(\"pytesseract\")\n-_pytest_available = _is_package_available(\"pytest\")\n-_pytorch_quantization_available = _is_package_available(\"pytorch_quantization\")\n-_rjieba_available = _is_package_available(\"rjieba\")\n-_sacremoses_available = _is_package_available(\"sacremoses\")\n-_safetensors_available = _is_package_available(\"safetensors\")\n-_scipy_available = _is_package_available(\"scipy\")\n-_sentencepiece_available = _is_package_available(\"sentencepiece\")\n-_is_seqio_available = _is_package_available(\"seqio\")\n-_is_gguf_available, _gguf_version = _is_package_available(\"gguf\", return_version=True)\n-_sklearn_available = importlib.util.find_spec(\"sklearn\") is not None\n-if _sklearn_available:\n-    try:\n-        importlib.metadata.version(\"scikit-learn\")\n-    except importlib.metadata.PackageNotFoundError:\n-        _sklearn_available = False\n-_smdistributed_available = importlib.util.find_spec(\"smdistributed\") is not None\n-_soundfile_available = _is_package_available(\"soundfile\")\n-_spacy_available = _is_package_available(\"spacy\")\n-_sudachipy_available, _sudachipy_version = _is_package_available(\"sudachipy\", return_version=True)\n-_timm_available = _is_package_available(\"timm\")\n-_tokenizers_available = _is_package_available(\"tokenizers\")\n-_torchaudio_available = _is_package_available(\"torchaudio\")\n-_torchao_available, _torchao_version = _is_package_available(\"torchao\", return_version=True)\n-_torchdistx_available = _is_package_available(\"torchdistx\")\n-_torchvision_available, _torchvision_version = _is_package_available(\"torchvision\", return_version=True)\n-_mlx_available = _is_package_available(\"mlx\")\n-_num2words_available = _is_package_available(\"num2words\")\n-_hqq_available, _hqq_version = _is_package_available(\"hqq\", return_version=True)\n-_tiktoken_available = _is_package_available(\"tiktoken\")\n-_blobfile_available = _is_package_available(\"blobfile\")\n-_liger_kernel_available = _is_package_available(\"liger_kernel\")\n-_spqr_available = _is_package_available(\"spqr_quant\")\n-_rich_available = _is_package_available(\"rich\")\n-_kernels_available = _is_package_available(\"kernels\")\n-_matplotlib_available = _is_package_available(\"matplotlib\")\n-_mistral_common_available = _is_package_available(\"mistral_common\")\n-_triton_available, _triton_version = _is_package_available(\"triton\", return_version=True)\n-\n-_torch_available, _torch_version = _is_package_available(\"torch\", return_version=True)\n-if _torch_available:\n-    _torch_available = version.parse(_torch_version) >= version.parse(\"2.2.0\")\n-    if not _torch_available:\n-        logger.warning(f\"Disabling PyTorch because PyTorch >= 2.2 is required but found {_torch_version}\")\n-\n-\n-_essentia_available = importlib.util.find_spec(\"essentia\") is not None\n-try:\n-    _essentia_version = importlib.metadata.version(\"essentia\")\n-    logger.debug(f\"Successfully imported essentia version {_essentia_version}\")\n-except importlib.metadata.PackageNotFoundError:\n-    _essentia_version = False\n-\n-\n-_pydantic_available = importlib.util.find_spec(\"pydantic\") is not None\n-try:\n-    _pydantic_version = importlib.metadata.version(\"pydantic\")\n-    logger.debug(f\"Successfully imported pydantic version {_pydantic_version}\")\n-except importlib.metadata.PackageNotFoundError:\n-    _pydantic_available = False\n-\n-\n-_fastapi_available = importlib.util.find_spec(\"fastapi\") is not None\n-try:\n-    _fastapi_version = importlib.metadata.version(\"fastapi\")\n-    logger.debug(f\"Successfully imported pydantic version {_fastapi_version}\")\n-except importlib.metadata.PackageNotFoundError:\n-    _fastapi_available = False\n-\n-\n-_uvicorn_available = importlib.util.find_spec(\"uvicorn\") is not None\n-try:\n-    _uvicorn_version = importlib.metadata.version(\"uvicorn\")\n-    logger.debug(f\"Successfully imported pydantic version {_uvicorn_version}\")\n-except importlib.metadata.PackageNotFoundError:\n-    _uvicorn_available = False\n-\n-\n-_pretty_midi_available = importlib.util.find_spec(\"pretty_midi\") is not None\n-try:\n-    _pretty_midi_version = importlib.metadata.version(\"pretty_midi\")\n-    logger.debug(f\"Successfully imported pretty_midi version {_pretty_midi_version}\")\n-except importlib.metadata.PackageNotFoundError:\n-    _pretty_midi_available = False\n-\n-\n-ccl_version = \"N/A\"\n-_is_ccl_available = (\n-    importlib.util.find_spec(\"torch_ccl\") is not None\n-    or importlib.util.find_spec(\"oneccl_bindings_for_pytorch\") is not None\n-)\n-try:\n-    ccl_version = importlib.metadata.version(\"oneccl_bind_pt\")\n-    logger.debug(f\"Detected oneccl_bind_pt version {ccl_version}\")\n-except importlib.metadata.PackageNotFoundError:\n-    _is_ccl_available = False\n-\n-\n-_torch_xla_available = False\n-if USE_TORCH_XLA in ENV_VARS_TRUE_VALUES:\n-    _torch_xla_available, _torch_xla_version = _is_package_available(\"torch_xla\", return_version=True)\n-    if _torch_xla_available:\n-        logger.info(f\"Torch XLA version {_torch_xla_version} available.\")\n \n-\n-def is_kenlm_available() -> Union[tuple[bool, str], bool]:\n-    return _kenlm_available\n-\n-\n-def is_kernels_available() -> Union[tuple[bool, str], bool]:\n-    return _kernels_available\n-\n-\n-def is_cv2_available() -> Union[tuple[bool, str], bool]:\n-    return _cv2_available\n+@lru_cache\n+def is_torch_available() -> bool:\n+    is_available, torch_version = _is_package_available(\"torch\", return_version=True)\n+    if is_available and version.parse(torch_version) < version.parse(\"2.2.0\"):\n+        logger.warning_once(f\"Disabling PyTorch because PyTorch >= 2.2 is required but found {torch_version}\")\n+    return is_available and version.parse(torch_version) >= version.parse(\"2.2.0\")\n \n \n-def is_yt_dlp_available() -> Union[tuple[bool, str], bool]:\n-    return _yt_dlp_available\n+@lru_cache\n+def get_torch_version() -> str:\n+    _, torch_version = _is_package_available(\"torch\", return_version=True)\n+    return torch_version\n \n \n-def is_torch_available() -> Union[tuple[bool, str], bool]:\n-    return _torch_available\n+@lru_cache\n+def is_torch_greater_or_equal(library_version: str, accept_dev: bool = False) -> bool:\n+    \"\"\"\n+    Accepts a library version and returns True if the current version of the library is greater than or equal to the\n+    given version. If `accept_dev` is True, it will also accept development versions (e.g. 2.7.0.dev20250320 matches\n+    2.7.0).\n+    \"\"\"\n+    if not is_torch_available():\n+        return False\n \n+    if accept_dev:\n+        return version.parse(version.parse(get_torch_version()).base_version) >= version.parse(library_version)\n+    else:\n+        return version.parse(get_torch_version()) >= version.parse(library_version)\n \n-def is_libcst_available() -> Union[tuple[bool, str], bool]:\n-    return _libcst_available\n \n+@lru_cache\n+def is_torch_less_or_equal(library_version: str, accept_dev: bool = False) -> bool:\n+    \"\"\"\n+    Accepts a library version and returns True if the current version of the library is less than or equal to the\n+    given version. If `accept_dev` is True, it will also accept development versions (e.g. 2.7.0.dev20250320 matches\n+    2.7.0).\n+    \"\"\"\n+    if not is_torch_available():\n+        return False\n \n-def is_accelerate_available(min_version: str = ACCELERATE_MIN_VERSION) -> bool:\n-    return _accelerate_available and version.parse(_accelerate_version) >= version.parse(min_version)\n+    if accept_dev:\n+        return version.parse(version.parse(get_torch_version()).base_version) <= version.parse(library_version)\n+    else:\n+        return version.parse(get_torch_version()) <= version.parse(library_version)\n \n \n+@lru_cache\n def is_torch_accelerator_available() -> bool:\n     if is_torch_available():\n         import torch\n@@ -337,233 +139,276 @@ def is_torch_accelerator_available() -> bool:\n     return False\n \n \n-def is_torch_deterministic() -> bool:\n-    \"\"\"\n-    Check whether pytorch uses deterministic algorithms by looking if torch.set_deterministic_debug_mode() is set to 1 or 2\"\n-    \"\"\"\n+@lru_cache\n+def is_torch_cuda_available() -> bool:\n     if is_torch_available():\n         import torch\n \n-        if torch.get_deterministic_debug_mode() == 0:\n-            return False\n-        else:\n-            return True\n-\n+        return torch.cuda.is_available()\n     return False\n \n \n-def is_triton_available(min_version: str = TRITON_MIN_VERSION) -> bool:\n-    return _triton_available and version.parse(_triton_version) >= version.parse(min_version)\n-\n-\n-def is_hadamard_available() -> Union[tuple[bool, str], bool]:\n-    return _hadamard_available\n-\n-\n-def is_hqq_available(min_version: str = HQQ_MIN_VERSION) -> bool:\n-    return _hqq_available and version.parse(_hqq_version) >= version.parse(min_version)\n-\n-\n-def is_pygments_available() -> Union[tuple[bool, str], bool]:\n-    return _pygments_available\n-\n-\n-def get_torch_version() -> str:\n-    return _torch_version\n+@lru_cache\n+def is_cuda_platform() -> bool:\n+    if is_torch_available():\n+        import torch\n \n+        return torch.version.cuda is not None\n+    return False\n \n-def get_torch_major_and_minor_version() -> str:\n-    if _torch_version == \"N/A\":\n-        return \"N/A\"\n-    parsed_version = version.parse(_torch_version)\n-    return str(parsed_version.major) + \".\" + str(parsed_version.minor)\n \n+@lru_cache\n+def is_rocm_platform() -> bool:\n+    if is_torch_available():\n+        import torch\n \n-def is_torch_sdpa_available():\n-    # Mostly retained for backward compatibility in remote code, since sdpa works correctly on all torch versions >= 2.2\n-    if not is_torch_available() or _torch_version == \"N/A\":\n-        return False\n-    return True\n+        return torch.version.hip is not None\n+    return False\n \n \n-def is_torch_flex_attn_available() -> bool:\n-    if not is_torch_available() or _torch_version == \"N/A\":\n+@lru_cache\n+def is_habana_gaudi1() -> bool:\n+    if not is_torch_hpu_available():\n         return False\n \n-    # TODO check if some bugs cause push backs on the exact version\n-    # NOTE: We require torch>=2.5.0 as it is the first release\n-    return version.parse(_torch_version) >= version.parse(\"2.5.0\")\n-\n-\n-def is_torchvision_available() -> bool:\n-    return _torchvision_available\n-\n-\n-def is_torchvision_v2_available() -> bool:\n-    return is_torchvision_available()\n+    import habana_frameworks.torch.utils.experimental as htexp\n \n+    # Check if the device is Gaudi1 (vs Gaudi2, Gaudi3)\n+    return htexp._get_device_type() == htexp.synDeviceType.synDeviceGaudi\n \n-def is_galore_torch_available() -> Union[tuple[bool, str], bool]:\n-    return _galore_torch_available\n \n+@lru_cache\n+def is_torch_mps_available(min_version: Optional[str] = None) -> bool:\n+    if is_torch_available():\n+        import torch\n \n-def is_apollo_torch_available() -> Union[tuple[bool, str], bool]:\n-    return _apollo_torch_available\n+        if hasattr(torch.backends, \"mps\"):\n+            backend_available = torch.backends.mps.is_available() and torch.backends.mps.is_built()\n+            if min_version is not None:\n+                flag = version.parse(get_torch_version()) >= version.parse(min_version)\n+                backend_available = backend_available and flag\n+            return backend_available\n+    return False\n \n \n-def is_torch_optimi_available() -> Union[tuple[bool, str], bool]:\n-    return _torch_optimi_available\n+@lru_cache\n+def is_torch_npu_available(check_device=False) -> bool:\n+    \"Checks if `torch_npu` is installed and potentially if a NPU is in the environment\"\n+    if not is_torch_available() or not _is_package_available(\"torch_npu\"):\n+        return False\n \n+    import torch\n+    import torch_npu  # noqa: F401\n \n-def is_lomo_available() -> Union[tuple[bool, str], bool]:\n-    return _lomo_available\n+    if check_device:\n+        try:\n+            # Will raise a RuntimeError if no NPU is found\n+            _ = torch.npu.device_count()\n+            return torch.npu.is_available()\n+        except RuntimeError:\n+            return False\n+    return hasattr(torch, \"npu\") and torch.npu.is_available()\n \n \n-def is_grokadamw_available() -> Union[tuple[bool, str], bool]:\n-    return _grokadamw_available\n+@lru_cache\n+def is_torch_xpu_available(check_device: bool = False) -> bool:\n+    \"\"\"\n+    Checks if XPU acceleration is available either via native PyTorch (>=2.6),\n+    `intel_extension_for_pytorch` or via stock PyTorch (>=2.4) and potentially\n+    if a XPU is in the environment.\n+    \"\"\"\n+    if not is_torch_available():\n+        return False\n \n+    torch_version = version.parse(get_torch_version())\n+    if torch_version.major == 2 and torch_version.minor < 6:\n+        if is_ipex_available():\n+            import intel_extension_for_pytorch  # noqa: F401\n+        elif torch_version.major == 2 and torch_version.minor < 4:\n+            return False\n \n-def is_schedulefree_available(min_version: str = SCHEDULEFREE_MIN_VERSION) -> bool:\n-    return _schedulefree_available and version.parse(_schedulefree_version) >= version.parse(min_version)\n+    import torch\n \n+    if check_device:\n+        try:\n+            # Will raise a RuntimeError if no XPU  is found\n+            _ = torch.xpu.device_count()\n+            return torch.xpu.is_available()\n+        except RuntimeError:\n+            return False\n+    return hasattr(torch, \"xpu\") and torch.xpu.is_available()\n \n-def is_pyctcdecode_available() -> Union[tuple[bool, str], bool]:\n-    return _pyctcdecode_available\n \n+@lru_cache\n+def is_torch_mlu_available() -> bool:\n+    \"\"\"\n+    Checks if `mlu` is available via an `cndev-based` check which won't trigger the drivers and leave mlu\n+    uninitialized.\n+    \"\"\"\n+    if not is_torch_available() or not _is_package_available(\"torch_mlu\"):\n+        return False\n \n-def is_librosa_available() -> Union[tuple[bool, str], bool]:\n-    return _librosa_available\n+    import torch\n+    import torch_mlu  # noqa: F401\n \n+    pytorch_cndev_based_mlu_check_previous_value = os.environ.get(\"PYTORCH_CNDEV_BASED_MLU_CHECK\")\n+    try:\n+        os.environ[\"PYTORCH_CNDEV_BASED_MLU_CHECK\"] = str(1)\n+        available = torch.mlu.is_available()\n+    finally:\n+        if pytorch_cndev_based_mlu_check_previous_value:\n+            os.environ[\"PYTORCH_CNDEV_BASED_MLU_CHECK\"] = pytorch_cndev_based_mlu_check_previous_value\n+        else:\n+            os.environ.pop(\"PYTORCH_CNDEV_BASED_MLU_CHECK\", None)\n \n-def is_essentia_available() -> Union[tuple[bool, str], bool]:\n-    return _essentia_available\n+    return available\n \n \n-def is_pydantic_available() -> Union[tuple[bool, str], bool]:\n-    return _pydantic_available\n+@lru_cache\n+def is_torch_musa_available(check_device=False) -> bool:\n+    \"Checks if `torch_musa` is installed and potentially if a MUSA is in the environment\"\n+    if not is_torch_available() or not _is_package_available(\"torch_musa\"):\n+        return False\n \n+    import torch\n+    import torch_musa  # noqa: F401\n \n-def is_fastapi_available() -> Union[tuple[bool, str], bool]:\n-    return _fastapi_available\n+    torch_musa_min_version = \"0.33.0\"\n+    accelerate_available, accelerate_version = _is_package_available(\"accelerate\", return_version=True)\n+    if accelerate_available and version.parse(accelerate_version) < version.parse(torch_musa_min_version):\n+        return False\n \n+    if check_device:\n+        try:\n+            # Will raise a RuntimeError if no MUSA is found\n+            _ = torch.musa.device_count()\n+            return torch.musa.is_available()\n+        except RuntimeError:\n+            return False\n+    return hasattr(torch, \"musa\") and torch.musa.is_available()\n \n-def is_uvicorn_available() -> Union[tuple[bool, str], bool]:\n-    return _uvicorn_available\n \n+@lru_cache\n+def is_torch_xla_available(check_is_tpu=False, check_is_gpu=False) -> bool:\n+    \"\"\"\n+    Check if `torch_xla` is available. To train a native pytorch job in an environment with torch xla installed, set\n+    the USE_TORCH_XLA to false.\n+    \"\"\"\n+    assert not (check_is_tpu and check_is_gpu), \"The check_is_tpu and check_is_gpu cannot both be true.\"\n \n-def is_openai_available() -> Union[tuple[bool, str], bool]:\n-    return _openai_available\n+    torch_xla_available = USE_TORCH_XLA in ENV_VARS_TRUE_VALUES and _is_package_available(\"torch_xla\")\n+    if not torch_xla_available:\n+        return False\n \n+    import torch_xla\n \n-def is_pretty_midi_available() -> Union[tuple[bool, str], bool]:\n-    return _pretty_midi_available\n+    if check_is_gpu:\n+        return torch_xla.runtime.device_type() in [\"GPU\", \"CUDA\"]\n+    elif check_is_tpu:\n+        return torch_xla.runtime.device_type() == \"TPU\"\n \n+    return True\n \n-def is_torch_cuda_available() -> bool:\n-    if is_torch_available():\n-        import torch\n \n-        return torch.cuda.is_available()\n-    else:\n+@lru_cache\n+def is_torch_hpu_available() -> bool:\n+    \"Checks if `torch.hpu` is available and potentially if a HPU is in the environment\"\n+    if (\n+        not is_torch_available()\n+        or not _is_package_available(\"habana_frameworks\")\n+        or not _is_package_available(\"habana_frameworks.torch\")\n+    ):\n         return False\n \n-\n-def is_cuda_platform() -> bool:\n-    if is_torch_available():\n-        import torch\n-\n-        return torch.version.cuda is not None\n-    else:\n+    torch_hpu_min_accelerate_version = \"1.5.0\"\n+    accelerate_available, accelerate_version = _is_package_available(\"accelerate\", return_version=True)\n+    if accelerate_available and version.parse(accelerate_version) < version.parse(torch_hpu_min_accelerate_version):\n         return False\n \n+    import torch\n \n-def is_rocm_platform() -> bool:\n-    if is_torch_available():\n-        import torch\n+    if os.environ.get(\"PT_HPU_LAZY_MODE\", \"1\") == \"1\":\n+        # import habana_frameworks.torch in case of lazy mode to patch torch with torch.hpu\n+        import habana_frameworks.torch  # noqa: F401\n \n-        return torch.version.hip is not None\n-    else:\n+    if not hasattr(torch, \"hpu\") or not torch.hpu.is_available():\n         return False\n \n+    # We patch torch.gather for int64 tensors to avoid a bug on Gaudi\n+    # Graph compile failed with synStatus 26 [Generic failure]\n+    # This can be removed once bug is fixed but for now we need it.\n+    original_gather = torch.gather\n \n-def is_mamba_ssm_available() -> Union[tuple[bool, str], bool]:\n-    if is_torch_available():\n-        import torch\n-\n-        if not torch.cuda.is_available():\n-            return False\n+    def patched_gather(input: torch.Tensor, dim: int, index: torch.LongTensor) -> torch.Tensor:\n+        if input.dtype == torch.int64 and input.device.type == \"hpu\":\n+            return original_gather(input.to(torch.int32), dim, index).to(torch.int64)\n         else:\n-            return _is_package_available(\"mamba_ssm\")\n-    return False\n+            return original_gather(input, dim, index)\n \n+    torch.gather = patched_gather\n+    torch.Tensor.gather = patched_gather\n \n-def is_mamba_2_ssm_available() -> bool:\n-    if is_torch_available():\n-        import torch\n+    original_take_along_dim = torch.take_along_dim\n \n-        if not torch.cuda.is_available():\n-            return False\n+    def patched_take_along_dim(\n+        input: torch.Tensor, indices: torch.LongTensor, dim: Optional[int] = None\n+    ) -> torch.Tensor:\n+        if input.dtype == torch.int64 and input.device.type == \"hpu\":\n+            return original_take_along_dim(input.to(torch.int32), indices, dim).to(torch.int64)\n         else:\n-            if _is_package_available(\"mamba_ssm\"):\n-                import mamba_ssm\n-\n-                if version.parse(mamba_ssm.__version__) >= version.parse(\"2.0.4\"):\n-                    return True\n-    return False\n-\n+            return original_take_along_dim(input, indices, dim)\n \n-def is_flash_linear_attention_available():\n-    if is_torch_available():\n-        import torch\n+    torch.take_along_dim = patched_take_along_dim\n \n-        if not torch.cuda.is_available():\n-            return False\n+    original_cholesky = torch.linalg.cholesky\n \n-        try:\n-            import fla\n+    def safe_cholesky(A, *args, **kwargs):\n+        output = original_cholesky(A, *args, **kwargs)\n \n-            if version.parse(fla.__version__) >= version.parse(\"0.2.2\"):\n-                return True\n-        except Exception:\n-            pass\n-    return False\n+        if torch.isnan(output).any():\n+            jitter_value = 1e-9\n+            diag_jitter = torch.eye(A.size(-1), dtype=A.dtype, device=A.device) * jitter_value\n+            output = original_cholesky(A + diag_jitter, *args, **kwargs)\n \n+        return output\n \n-def is_causal_conv1d_available() -> Union[tuple[bool, str], bool]:\n-    if is_torch_available():\n-        import torch\n+    torch.linalg.cholesky = safe_cholesky\n \n-        if not torch.cuda.is_available():\n-            return False\n-        return _is_package_available(\"causal_conv1d\")\n-    return False\n+    original_scatter = torch.scatter\n \n+    def patched_scatter(\n+        input: torch.Tensor, dim: int, index: torch.Tensor, src: torch.Tensor, *args, **kwargs\n+    ) -> torch.Tensor:\n+        if input.device.type == \"hpu\" and input is src:\n+            return original_scatter(input, dim, index, src.clone(), *args, **kwargs)\n+        else:\n+            return original_scatter(input, dim, index, src, *args, **kwargs)\n \n-def is_xlstm_available() -> Union[tuple[bool, str], bool]:\n-    if is_torch_available():\n-        return _is_package_available(\"xlstm\")\n-    return False\n+    torch.scatter = patched_scatter\n+    torch.Tensor.scatter = patched_scatter\n \n+    # IlyasMoutawwakil: we patch torch.compile to use the HPU backend by default\n+    # https://github.com/huggingface/transformers/pull/38790#discussion_r2157043944\n+    # This is necessary for cases where torch.compile is used as a decorator (defaulting to inductor)\n+    # https://github.com/huggingface/transformers/blob/af6120b3eb2470b994c21421bb6eaa76576128b0/src/transformers/models/modernbert/modeling_modernbert.py#L204\n+    original_compile = torch.compile\n \n-def is_mambapy_available() -> Union[tuple[bool, str], bool]:\n-    if is_torch_available():\n-        return _is_package_available(\"mambapy\")\n-    return False\n+    def hpu_backend_compile(*args, **kwargs):\n+        if kwargs.get(\"backend\") not in [\"hpu_backend\", \"eager\"]:\n+            logger.warning(\n+                f\"Calling torch.compile with backend={kwargs.get('backend')} on a Gaudi device is not supported. \"\n+                \"We will override the backend with 'hpu_backend' to avoid errors.\"\n+            )\n+            kwargs[\"backend\"] = \"hpu_backend\"\n \n+        return original_compile(*args, **kwargs)\n \n-def is_torch_mps_available(min_version: Optional[str] = None) -> bool:\n-    if is_torch_available():\n-        import torch\n+    torch.compile = hpu_backend_compile\n \n-        if hasattr(torch.backends, \"mps\"):\n-            backend_available = torch.backends.mps.is_available() and torch.backends.mps.is_built()\n-            if min_version is not None:\n-                flag = version.parse(_torch_version) >= version.parse(min_version)\n-                backend_available = backend_available and flag\n-            return backend_available\n-    return False\n+    return True\n \n \n+@lru_cache\n def is_torch_bf16_gpu_available() -> bool:\n     if not is_torch_available():\n         return False\n@@ -586,21 +431,6 @@ def is_torch_bf16_gpu_available() -> bool:\n     return False\n \n \n-def is_torch_bf16_cpu_available() -> Union[tuple[bool, str], bool]:\n-    return is_torch_available()\n-\n-\n-def is_torch_bf16_available() -> bool:\n-    # the original bf16 check was for gpu only, but later a cpu/bf16 combo has emerged so this util\n-    # has become ambiguous and therefore deprecated\n-    warnings.warn(\n-        \"The util is_torch_bf16_available is deprecated, please use is_torch_bf16_gpu_available \"\n-        \"or is_torch_bf16_cpu_available instead according to whether it's used with cpu or gpu\",\n-        FutureWarning,\n-    )\n-    return is_torch_bf16_gpu_available()\n-\n-\n @lru_cache\n def is_torch_fp16_available_on_device(device: str) -> bool:\n     if not is_torch_available():\n@@ -617,21 +447,16 @@ def is_torch_fp16_available_on_device(device: str) -> bool:\n     try:\n         x = torch.zeros(2, 2, dtype=torch.float16, device=device)\n         _ = x @ x\n-\n         # At this moment, let's be strict of the check: check if `LayerNorm` is also supported on device, because many\n         # models use this layer.\n         batch, sentence_length, embedding_dim = 3, 4, 5\n         embedding = torch.randn(batch, sentence_length, embedding_dim, dtype=torch.float16, device=device)\n         layer_norm = torch.nn.LayerNorm(embedding_dim, dtype=torch.float16, device=device)\n         _ = layer_norm(embedding)\n-\n-    except:  # noqa: E722\n-        # TODO: more precise exception matching, if possible.\n-        # most backends should return `RuntimeError` however this is not guaranteed.\n+        return True\n+    except Exception:\n         return False\n \n-    return True\n-\n \n @lru_cache\n def is_torch_bf16_available_on_device(device: str) -> bool:\n@@ -649,14 +474,12 @@ def is_torch_bf16_available_on_device(device: str) -> bool:\n     try:\n         x = torch.zeros(2, 2, dtype=torch.bfloat16, device=device)\n         _ = x @ x\n-    except:  # noqa: E722\n-        # TODO: more precise exception matching, if possible.\n-        # most backends should return `RuntimeError` however this is not guaranteed.\n+        return True\n+    except Exception:\n         return False\n \n-    return True\n-\n \n+@lru_cache\n def is_torch_tf32_available() -> bool:\n     if not is_torch_available():\n         return False\n@@ -675,342 +498,285 @@ def is_torch_tf32_available() -> bool:\n     return True\n \n \n-def is_torch_fx_available() -> Union[tuple[bool, str], bool]:\n-    return is_torch_available()\n+@lru_cache\n+def is_torch_flex_attn_available() -> bool:\n+    return is_torch_available() and version.parse(get_torch_version()) >= version.parse(\"2.5.0\")\n \n \n-def is_peft_available() -> Union[tuple[bool, str], bool]:\n-    return _peft_available\n+@lru_cache\n+def is_kenlm_available() -> bool:\n+    return _is_package_available(\"kenlm\")\n \n \n-def is_bs4_available() -> Union[tuple[bool, str], bool]:\n-    return _bs4_available\n+@lru_cache\n+def is_kernels_available() -> bool:\n+    return _is_package_available(\"kernels\")\n \n \n-def is_coloredlogs_available() -> Union[tuple[bool, str], bool]:\n-    return _coloredlogs_available\n+@lru_cache\n+def is_cv2_available() -> bool:\n+    return _is_package_available(\"cv2\")\n \n \n-def is_onnx_available() -> Union[tuple[bool, str], bool]:\n-    return _onnx_available\n+@lru_cache\n+def is_yt_dlp_available() -> bool:\n+    return _is_package_available(\"yt_dlp\")\n \n \n-def is_flute_available() -> bool:\n-    try:\n-        return importlib.util.find_spec(\"flute\") is not None and importlib.metadata.version(\"flute-kernel\") >= \"0.4.1\"\n-    except importlib.metadata.PackageNotFoundError:\n-        return False\n+@lru_cache\n+def is_libcst_available() -> bool:\n+    return _is_package_available(\"libcst\")\n \n \n-def is_ftfy_available() -> Union[tuple[bool, str], bool]:\n-    return _ftfy_available\n+@lru_cache\n+def is_accelerate_available(min_version: str = ACCELERATE_MIN_VERSION) -> bool:\n+    is_available, accelerate_version = _is_package_available(\"accelerate\", return_version=True)\n+    return is_available and version.parse(accelerate_version) >= version.parse(min_version)\n \n \n-def is_g2p_en_available() -> Union[tuple[bool, str], bool]:\n-    return _g2p_en_available\n+@lru_cache\n+def is_triton_available(min_version: str = TRITON_MIN_VERSION) -> bool:\n+    is_available, triton_version = _is_package_available(\"triton\", return_version=True)\n+    return is_available and version.parse(triton_version) >= version.parse(min_version)\n \n \n @lru_cache\n-def is_torch_xla_available(check_is_tpu=False, check_is_gpu=False) -> bool:\n-    \"\"\"\n-    Check if `torch_xla` is available. To train a native pytorch job in an environment with torch xla installed, set\n-    the USE_TORCH_XLA to false.\n-    \"\"\"\n-    assert not (check_is_tpu and check_is_gpu), \"The check_is_tpu and check_is_gpu cannot both be true.\"\n+def is_hadamard_available() -> bool:\n+    return _is_package_available(\"fast_hadamard_transform\")\n \n-    if not _torch_xla_available:\n-        return False\n \n-    import torch_xla\n+@lru_cache\n+def is_hqq_available(min_version: str = HQQ_MIN_VERSION) -> bool:\n+    is_available, hqq_version = _is_package_available(\"hqq\", return_version=True)\n+    return is_available and version.parse(hqq_version) >= version.parse(min_version)\n \n-    if check_is_gpu:\n-        return torch_xla.runtime.device_type() in [\"GPU\", \"CUDA\"]\n-    elif check_is_tpu:\n-        return torch_xla.runtime.device_type() == \"TPU\"\n \n-    return True\n+@lru_cache\n+def is_pygments_available() -> bool:\n+    return _is_package_available(\"pygments\")\n \n \n @lru_cache\n-def is_torch_neuroncore_available(check_device=True) -> bool:\n-    if importlib.util.find_spec(\"torch_neuronx\") is not None:\n-        return is_torch_xla_available()\n-    return False\n+def is_torchvision_available() -> bool:\n+    return _is_package_available(\"torchvision\")\n \n \n @lru_cache\n-def is_torch_npu_available(check_device=False) -> bool:\n-    \"Checks if `torch_npu` is installed and potentially if a NPU is in the environment\"\n-    if not _torch_available or importlib.util.find_spec(\"torch_npu\") is None:\n-        return False\n+def is_torchvision_v2_available() -> bool:\n+    return is_torchvision_available()\n \n-    import torch\n-    import torch_npu  # noqa: F401\n \n-    if check_device:\n-        try:\n-            # Will raise a RuntimeError if no NPU is found\n-            _ = torch.npu.device_count()\n-            return torch.npu.is_available()\n-        except RuntimeError:\n-            return False\n-    return hasattr(torch, \"npu\") and torch.npu.is_available()\n+@lru_cache\n+def is_galore_torch_available() -> bool:\n+    return _is_package_available(\"galore_torch\")\n \n \n @lru_cache\n-def is_torch_mlu_available() -> bool:\n-    \"\"\"\n-    Checks if `mlu` is available via an `cndev-based` check which won't trigger the drivers and leave mlu\n-    uninitialized.\n-    \"\"\"\n-    if not _torch_available or importlib.util.find_spec(\"torch_mlu\") is None:\n-        return False\n-\n-    import torch\n-    import torch_mlu  # noqa: F401\n+def is_apollo_torch_available() -> bool:\n+    return _is_package_available(\"apollo_torch\")\n \n-    pytorch_cndev_based_mlu_check_previous_value = os.environ.get(\"PYTORCH_CNDEV_BASED_MLU_CHECK\")\n-    try:\n-        os.environ[\"PYTORCH_CNDEV_BASED_MLU_CHECK\"] = str(1)\n-        available = torch.mlu.is_available()\n-    finally:\n-        if pytorch_cndev_based_mlu_check_previous_value:\n-            os.environ[\"PYTORCH_CNDEV_BASED_MLU_CHECK\"] = pytorch_cndev_based_mlu_check_previous_value\n-        else:\n-            os.environ.pop(\"PYTORCH_CNDEV_BASED_MLU_CHECK\", None)\n \n-    return available\n+@lru_cache\n+def is_torch_optimi_available() -> bool:\n+    return _is_package_available(\"optimi\")\n \n \n @lru_cache\n-def is_torch_musa_available(check_device=False) -> bool:\n-    \"Checks if `torch_musa` is installed and potentially if a MUSA is in the environment\"\n-    if not _torch_available or importlib.util.find_spec(\"torch_musa\") is None:\n-        return False\n+def is_lomo_available() -> bool:\n+    return _is_package_available(\"lomo_optim\")\n \n-    import torch\n-    import torch_musa  # noqa: F401\n \n-    torch_musa_min_version = \"0.33.0\"\n-    if _accelerate_available and version.parse(_accelerate_version) < version.parse(torch_musa_min_version):\n-        return False\n+@lru_cache\n+def is_grokadamw_available() -> bool:\n+    return _is_package_available(\"grokadamw\")\n \n-    if check_device:\n-        try:\n-            # Will raise a RuntimeError if no MUSA is found\n-            _ = torch.musa.device_count()\n-            return torch.musa.is_available()\n-        except RuntimeError:\n-            return False\n-    return hasattr(torch, \"musa\") and torch.musa.is_available()\n+\n+@lru_cache\n+def is_schedulefree_available(min_version: str = SCHEDULEFREE_MIN_VERSION) -> bool:\n+    is_available, schedulefree_version = _is_package_available(\"schedulefree\", return_version=True)\n+    return is_available and version.parse(schedulefree_version) >= version.parse(min_version)\n \n \n @lru_cache\n-def is_torch_hpu_available() -> bool:\n-    \"Checks if `torch.hpu` is available and potentially if a HPU is in the environment\"\n-    if (\n-        not _torch_available\n-        or importlib.util.find_spec(\"habana_frameworks\") is None\n-        or importlib.util.find_spec(\"habana_frameworks.torch\") is None\n-    ):\n-        return False\n+def is_pyctcdecode_available() -> bool:\n+    return _is_package_available(\"pyctcdecode\")\n \n-    torch_hpu_min_accelerate_version = \"1.5.0\"\n-    if _accelerate_available and version.parse(_accelerate_version) < version.parse(torch_hpu_min_accelerate_version):\n-        return False\n \n-    import torch\n+@lru_cache\n+def is_librosa_available() -> bool:\n+    return _is_package_available(\"librosa\")\n \n-    if os.environ.get(\"PT_HPU_LAZY_MODE\", \"1\") == \"1\":\n-        # import habana_frameworks.torch in case of lazy mode to patch torch with torch.hpu\n-        import habana_frameworks.torch  # noqa: F401\n \n-    if not hasattr(torch, \"hpu\") or not torch.hpu.is_available():\n-        return False\n+@lru_cache\n+def is_essentia_available() -> bool:\n+    return _is_package_available(\"essentia\")\n \n-    # We patch torch.gather for int64 tensors to avoid a bug on Gaudi\n-    # Graph compile failed with synStatus 26 [Generic failure]\n-    # This can be removed once bug is fixed but for now we need it.\n-    original_gather = torch.gather\n \n-    def patched_gather(input: torch.Tensor, dim: int, index: torch.LongTensor) -> torch.Tensor:\n-        if input.dtype == torch.int64 and input.device.type == \"hpu\":\n-            return original_gather(input.to(torch.int32), dim, index).to(torch.int64)\n-        else:\n-            return original_gather(input, dim, index)\n+@lru_cache\n+def is_pydantic_available() -> bool:\n+    return _is_package_available(\"pydantic\")\n \n-    torch.gather = patched_gather\n-    torch.Tensor.gather = patched_gather\n \n-    original_take_along_dim = torch.take_along_dim\n+@lru_cache\n+def is_fastapi_available() -> bool:\n+    return _is_package_available(\"fastapi\")\n \n-    def patched_take_along_dim(\n-        input: torch.Tensor, indices: torch.LongTensor, dim: Optional[int] = None\n-    ) -> torch.Tensor:\n-        if input.dtype == torch.int64 and input.device.type == \"hpu\":\n-            return original_take_along_dim(input.to(torch.int32), indices, dim).to(torch.int64)\n-        else:\n-            return original_take_along_dim(input, indices, dim)\n \n-    torch.take_along_dim = patched_take_along_dim\n+@lru_cache\n+def is_uvicorn_available() -> bool:\n+    return _is_package_available(\"uvicorn\")\n \n-    original_cholesky = torch.linalg.cholesky\n \n-    def safe_cholesky(A, *args, **kwargs):\n-        output = original_cholesky(A, *args, **kwargs)\n+@lru_cache\n+def is_openai_available() -> bool:\n+    return _is_package_available(\"openai\")\n \n-        if torch.isnan(output).any():\n-            jitter_value = 1e-9\n-            diag_jitter = torch.eye(A.size(-1), dtype=A.dtype, device=A.device) * jitter_value\n-            output = original_cholesky(A + diag_jitter, *args, **kwargs)\n \n-        return output\n+@lru_cache\n+def is_pretty_midi_available() -> bool:\n+    return _is_package_available(\"pretty_midi\")\n \n-    torch.linalg.cholesky = safe_cholesky\n \n-    original_scatter = torch.scatter\n+@lru_cache\n+def is_mamba_ssm_available() -> bool:\n+    return is_torch_cuda_available() and _is_package_available(\"mamba_ssm\")\n \n-    def patched_scatter(\n-        input: torch.Tensor, dim: int, index: torch.Tensor, src: torch.Tensor, *args, **kwargs\n-    ) -> torch.Tensor:\n-        if input.device.type == \"hpu\" and input is src:\n-            return original_scatter(input, dim, index, src.clone(), *args, **kwargs)\n-        else:\n-            return original_scatter(input, dim, index, src, *args, **kwargs)\n \n-    torch.scatter = patched_scatter\n-    torch.Tensor.scatter = patched_scatter\n+@lru_cache\n+def is_mamba_2_ssm_available() -> bool:\n+    is_available, mamba_ssm_version = _is_package_available(\"mamba_ssm\", return_version=True)\n+    return is_torch_cuda_available() and is_available and version.parse(mamba_ssm_version) >= version.parse(\"2.0.4\")\n \n-    # IlyasMoutawwakil: we patch torch.compile to use the HPU backend by default\n-    # https://github.com/huggingface/transformers/pull/38790#discussion_r2157043944\n-    # This is necessary for cases where torch.compile is used as a decorator (defaulting to inductor)\n-    # https://github.com/huggingface/transformers/blob/af6120b3eb2470b994c21421bb6eaa76576128b0/src/transformers/models/modernbert/modeling_modernbert.py#L204\n-    original_compile = torch.compile\n \n-    def hpu_backend_compile(*args, **kwargs):\n-        if kwargs.get(\"backend\") not in [\"hpu_backend\", \"eager\"]:\n-            logger.warning(\n-                f\"Calling torch.compile with backend={kwargs.get('backend')} on a Gaudi device is not supported. \"\n-                \"We will override the backend with 'hpu_backend' to avoid errors.\"\n-            )\n-            kwargs[\"backend\"] = \"hpu_backend\"\n+@lru_cache\n+def is_flash_linear_attention_available():\n+    is_available, fla_version = _is_package_available(\"fla\", return_version=True)\n+    return is_torch_cuda_available() and is_available and version.parse(fla_version) >= version.parse(\"0.2.2\")\n \n-        return original_compile(*args, **kwargs)\n \n-    torch.compile = hpu_backend_compile\n+@lru_cache\n+def is_causal_conv1d_available() -> bool:\n+    return is_torch_cuda_available() and _is_package_available(\"causal_conv1d\")\n \n-    return True\n+\n+@lru_cache\n+def is_xlstm_available() -> bool:\n+    return is_torch_available() and _is_package_available(\"xlstm\")\n \n \n @lru_cache\n-def is_habana_gaudi1() -> bool:\n-    if not is_torch_hpu_available():\n-        return False\n+def is_mambapy_available() -> bool:\n+    return is_torch_available() and _is_package_available(\"mambapy\")\n \n-    import habana_frameworks.torch.utils.experimental as htexp\n \n-    # Check if the device is Gaudi1 (vs Gaudi2, Gaudi3)\n-    return htexp._get_device_type() == htexp.synDeviceType.synDeviceGaudi\n+@lru_cache\n+def is_peft_available() -> bool:\n+    return _is_package_available(\"peft\")\n \n \n-def is_torchdynamo_available() -> Union[tuple[bool, str], bool]:\n-    return is_torch_available()\n+@lru_cache\n+def is_bs4_available() -> bool:\n+    return _is_package_available(\"bs4\")\n \n \n-def is_torch_compile_available() -> Union[tuple[bool, str], bool]:\n-    return is_torch_available()\n+@lru_cache\n+def is_coloredlogs_available() -> bool:\n+    return _is_package_available(\"coloredlogs\")\n \n \n-def is_torchdynamo_compiling() -> Union[tuple[bool, str], bool]:\n-    if not is_torch_available():\n-        return False\n+@lru_cache\n+def is_onnx_available() -> bool:\n+    return _is_package_available(\"onnx\")\n \n-    # Importing torch._dynamo causes issues with PyTorch profiler (https://github.com/pytorch/pytorch/issues/130622)\n-    # hence rather relying on `torch.compiler.is_compiling()` when possible (torch>=2.3)\n-    try:\n-        import torch\n \n-        return torch.compiler.is_compiling()\n-    except Exception:\n-        try:\n-            import torch._dynamo as dynamo\n+@lru_cache\n+def is_flute_available() -> bool:\n+    is_available, flute_version = _is_package_available(\"flute\", return_version=True)\n+    return is_available and version.parse(flute_version) >= version.parse(\"0.4.1\")\n \n-            return dynamo.is_compiling()\n-        except Exception:\n-            return False\n \n+@lru_cache\n+def is_ftfy_available() -> bool:\n+    return _is_package_available(\"ftfy\")\n \n-def is_torchdynamo_exporting() -> bool:\n-    if not is_torch_available():\n-        return False\n \n-    try:\n-        import torch\n+@lru_cache\n+def is_g2p_en_available() -> bool:\n+    return _is_package_available(\"g2p_en\")\n \n-        return torch.compiler.is_exporting()\n-    except Exception:\n-        try:\n-            import torch._dynamo as dynamo\n \n-            return dynamo.is_exporting()\n-        except Exception:\n-            return False\n+@lru_cache\n+def is_torch_neuroncore_available(check_device=True) -> bool:\n+    return is_torch_xla_available() and _is_package_available(\"torch_neuronx\")\n \n \n+@lru_cache\n def is_torch_tensorrt_fx_available() -> bool:\n-    if importlib.util.find_spec(\"torch_tensorrt\") is None:\n-        return False\n-    return importlib.util.find_spec(\"torch_tensorrt.fx\") is not None\n+    return _is_package_available(\"torch_tensorrt\") and _is_package_available(\"torch_tensorrt.fx\")\n \n \n-def is_datasets_available() -> Union[tuple[bool, str], bool]:\n-    return _datasets_available\n+@lru_cache\n+def is_datasets_available() -> bool:\n+    return _is_package_available(\"datasets\")\n \n \n-def is_detectron2_available() -> Union[tuple[bool, str], bool]:\n-    return _detectron2_available\n+@lru_cache\n+def is_detectron2_available() -> bool:\n+    return _is_package_available(\"detectron2\")\n \n \n-def is_rjieba_available() -> Union[tuple[bool, str], bool]:\n-    return _rjieba_available\n+@lru_cache\n+def is_rjieba_available() -> bool:\n+    return _is_package_available(\"rjieba\")\n \n \n-def is_psutil_available() -> Union[tuple[bool, str], bool]:\n-    return _psutil_available\n+@lru_cache\n+def is_psutil_available() -> bool:\n+    return _is_package_available(\"psutil\")\n \n \n-def is_py3nvml_available() -> Union[tuple[bool, str], bool]:\n-    return _py3nvml_available\n+@lru_cache\n+def is_py3nvml_available() -> bool:\n+    return _is_package_available(\"py3nvml\")\n \n \n-def is_sacremoses_available() -> Union[tuple[bool, str], bool]:\n-    return _sacremoses_available\n+@lru_cache\n+def is_sacremoses_available() -> bool:\n+    return _is_package_available(\"sacremoses\")\n \n \n-def is_apex_available() -> Union[tuple[bool, str], bool]:\n-    return _apex_available\n+@lru_cache\n+def is_apex_available() -> bool:\n+    return _is_package_available(\"apex\")\n \n \n-def is_aqlm_available() -> Union[tuple[bool, str], bool]:\n-    return _aqlm_available\n+@lru_cache\n+def is_aqlm_available() -> bool:\n+    return _is_package_available(\"aqlm\")\n \n \n+@lru_cache\n def is_vptq_available(min_version: str = VPTQ_MIN_VERSION) -> bool:\n-    return _vptq_available and version.parse(_vptq_version) >= version.parse(min_version)\n+    is_available, vptq_version = _is_package_available(\"vptq\", return_version=True)\n+    return is_available and version.parse(vptq_version) >= version.parse(min_version)\n \n \n+@lru_cache\n def is_av_available() -> bool:\n-    return _av_available\n+    return _is_package_available(\"av\")\n \n \n+@lru_cache\n def is_decord_available() -> bool:\n-    return _decord_available\n+    return _is_package_available(\"decord\")\n \n \n+@lru_cache\n def is_torchcodec_available() -> bool:\n-    return _torchcodec_available\n+    return _is_package_available(\"torchcodec\")\n \n \n+@lru_cache\n def is_ninja_available() -> bool:\n     r\"\"\"\n     Code comes from *torch.utils.cpp_extension.is_ninja_available()*. Returns `True` if the\n@@ -1024,77 +790,44 @@ def is_ninja_available() -> bool:\n         return True\n \n \n+@lru_cache\n def is_ipex_available(min_version: str = \"\") -> bool:\n     def get_major_and_minor_from_version(full_version):\n         return str(version.parse(full_version).major) + \".\" + str(version.parse(full_version).minor)\n \n-    if not is_torch_available() or not _ipex_available:\n+    ipex_available, ipex_version = _is_package_available(\"intel_extension_for_pytorch\", return_version=True)\n+\n+    if not is_torch_available() or not ipex_available:\n         return False\n \n-    torch_major_and_minor = get_major_and_minor_from_version(_torch_version)\n-    ipex_major_and_minor = get_major_and_minor_from_version(_ipex_version)\n+    torch_major_and_minor = get_major_and_minor_from_version(get_torch_version())\n+    ipex_major_and_minor = get_major_and_minor_from_version(ipex_version)\n     if torch_major_and_minor != ipex_major_and_minor:\n-        logger.warning(\n+        logger.warning_once(\n             f\"Intel Extension for PyTorch {ipex_major_and_minor} needs to work with PyTorch {ipex_major_and_minor}.*,\"\n-            f\" but PyTorch {_torch_version} is found. Please switch to the matching version and run again.\"\n+            f\" but PyTorch {get_torch_version()} is found. Please switch to the matching version and run again.\"\n         )\n         return False\n     if min_version:\n-        return version.parse(_ipex_version) >= version.parse(min_version)\n+        return version.parse(ipex_version) >= version.parse(min_version)\n     return True\n \n \n-@lru_cache\n-def is_torch_xpu_available(check_device: bool = False) -> bool:\n-    \"\"\"\n-    Checks if XPU acceleration is available either via native PyTorch (>=2.6),\n-    `intel_extension_for_pytorch` or via stock PyTorch (>=2.4) and potentially\n-    if a XPU is in the environment.\n-    \"\"\"\n-    if not is_torch_available():\n-        return False\n-\n-    torch_version = version.parse(_torch_version)\n-    if torch_version.major == 2 and torch_version.minor < 6:\n-        if is_ipex_available():\n-            import intel_extension_for_pytorch  # noqa: F401\n-        elif torch_version.major == 2 and torch_version.minor < 4:\n-            return False\n-\n-    import torch\n-\n-    if check_device:\n-        try:\n-            # Will raise a RuntimeError if no XPU  is found\n-            _ = torch.xpu.device_count()\n-            return torch.xpu.is_available()\n-        except RuntimeError:\n-            return False\n-    return hasattr(torch, \"xpu\") and torch.xpu.is_available()\n-\n-\n @lru_cache\n def is_bitsandbytes_available(check_library_only: bool = False) -> bool:\n-    if not _bitsandbytes_available:\n-        return False\n-\n-    if check_library_only:\n-        return True\n-\n-    if not is_torch_available():\n-        return False\n-\n-    import torch\n+    is_available, bitsandbytes_version = _is_package_available(\"bitsandbytes\", return_version=True)\n+    if check_library_only or not is_available:\n+        return is_available\n \n     # `bitsandbytes` versions older than 0.43.1 eagerly require CUDA at import time,\n     # so those versions of the library are practically only available when CUDA is too.\n-    if version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.43.1\"):\n-        return torch.cuda.is_available()\n-\n+    if version.parse(bitsandbytes_version) < version.parse(\"0.43.1\"):\n+        return is_torch_cuda_available() and is_available\n     # Newer versions of `bitsandbytes` can be imported on systems without CUDA.\n-    return True\n+    return is_torch_available() and is_available\n \n \n+@lru_cache\n def is_bitsandbytes_multi_backend_available() -> bool:\n     if not is_bitsandbytes_available():\n         return False\n@@ -1104,262 +837,428 @@ def is_bitsandbytes_multi_backend_available() -> bool:\n     return \"multi_backend\" in getattr(bnb, \"features\", set())\n \n \n+@lru_cache\n def is_flash_attn_2_available() -> bool:\n-    if not is_torch_available():\n-        return False\n-\n-    if not _is_package_available(\"flash_attn\"):\n+    is_available, flash_attn_version = _is_package_available(\"flash_attn\", return_version=True)\n+    if not is_available or not (is_torch_cuda_available() or is_torch_mlu_available()):\n         return False\n \n-    # Let's add an extra check to see if cuda is available\n     import torch\n \n-    if not (torch.cuda.is_available() or is_torch_mlu_available()):\n-        return False\n-\n     if torch.version.cuda:\n-        return version.parse(importlib.metadata.version(\"flash_attn\")) >= version.parse(\"2.1.0\")\n+        return version.parse(flash_attn_version) >= version.parse(\"2.1.0\")\n     elif torch.version.hip:\n         # TODO: Bump the requirement to 2.1.0 once released in https://github.com/ROCmSoftwarePlatform/flash-attention\n-        return version.parse(importlib.metadata.version(\"flash_attn\")) >= version.parse(\"2.0.4\")\n+        return version.parse(flash_attn_version) >= version.parse(\"2.0.4\")\n     elif is_torch_mlu_available():\n-        return version.parse(importlib.metadata.version(\"flash_attn\")) >= version.parse(\"2.3.3\")\n+        return version.parse(flash_attn_version) >= version.parse(\"2.3.3\")\n+    else:\n+        return False\n+\n+\n+@lru_cache\n+def is_flash_attn_3_available() -> bool:\n+    return is_torch_cuda_available() and _is_package_available(\"flash_attn_3\")\n+\n+\n+@lru_cache\n+def is_flash_attn_greater_or_equal_2_10() -> bool:\n+    _, flash_attn_version = _is_package_available(\"flash_attn\", return_version=True)\n+    return is_flash_attn_2_available() and version.parse(flash_attn_version) >= version.parse(\"2.1.0\")\n+\n+\n+@lru_cache\n+def is_flash_attn_greater_or_equal(library_version: str) -> bool:\n+    is_available, flash_attn_version = _is_package_available(\"flash_attn\", return_version=True)\n+    return is_available and version.parse(flash_attn_version) >= version.parse(library_version)\n+\n+\n+@lru_cache\n+def is_huggingface_hub_greater_or_equal(library_version: str, accept_dev: bool = False) -> bool:\n+    is_available, hub_version = _is_package_available(\"huggingface_hub\", return_version=True)\n+    if not is_available:\n+        return False\n+\n+    if accept_dev:\n+        return version.parse(version.parse(hub_version).base_version) >= version.parse(library_version)\n     else:\n+        return version.parse(hub_version) >= version.parse(library_version)\n+\n+\n+@lru_cache\n+def is_quanto_greater(library_version: str, accept_dev: bool = False) -> bool:\n+    \"\"\"\n+    Accepts a library version and returns True if the current version of the library is greater than or equal to the\n+    given version. If `accept_dev` is True, it will also accept development versions (e.g. 2.7.0.dev20250320 matches\n+    2.7.0).\n+    \"\"\"\n+    if not is_optimum_quanto_available():\n         return False\n \n+    _, quanto_version = _is_package_available(\"optimum.quanto\", return_version=True)\n+    if accept_dev:\n+        return version.parse(version.parse(quanto_version).base_version) > version.parse(library_version)\n+    else:\n+        return version.parse(quanto_version) > version.parse(library_version)\n+\n+\n+@lru_cache\n+def is_torchdistx_available():\n+    return _is_package_available(\"torchdistx\")\n+\n+\n+@lru_cache\n+def is_faiss_available() -> bool:\n+    return _is_package_available(\"faiss\")\n+\n+\n+@lru_cache\n+def is_scipy_available() -> bool:\n+    return _is_package_available(\"scipy\")\n+\n+\n+@lru_cache\n+def is_sklearn_available() -> bool:\n+    return _is_package_available(\"sklearn\")\n+\n+\n+@lru_cache\n+def is_sentencepiece_available() -> bool:\n+    return _is_package_available(\"sentencepiece\")\n+\n+\n+@lru_cache\n+def is_seqio_available() -> bool:\n+    return _is_package_available(\"seqio\")\n+\n+\n+@lru_cache\n+def is_gguf_available(min_version: str = GGUF_MIN_VERSION) -> bool:\n+    is_available, gguf_version = _is_package_available(\"gguf\", return_version=True)\n+    return is_available and version.parse(gguf_version) >= version.parse(min_version)\n+\n+\n+@lru_cache\n+def is_protobuf_available() -> bool:\n+    return _is_package_available(\"google\") and _is_package_available(\"google.protobuf\")\n+\n+\n+@lru_cache\n+def is_fsdp_available(min_version: str = FSDP_MIN_VERSION) -> bool:\n+    return is_torch_available() and version.parse(get_torch_version()) >= version.parse(min_version)\n+\n+\n+@lru_cache\n+def is_optimum_available() -> bool:\n+    return _is_package_available(\"optimum\")\n+\n+\n+@lru_cache\n+def is_auto_awq_available() -> bool:\n+    return _is_package_available(\"awq\")\n+\n+\n+@lru_cache\n+def is_auto_round_available(min_version: str = AUTOROUND_MIN_VERSION) -> bool:\n+    is_available, auto_round_version = _is_package_available(\"auto_round\", return_version=True)\n+    return is_available and version.parse(auto_round_version) >= version.parse(min_version)\n+\n+\n+@lru_cache\n+def is_optimum_quanto_available():\n+    return is_optimum_available() and _is_package_available(\"optimum.quanto\")\n+\n+\n+@lru_cache\n+def is_quark_available() -> bool:\n+    return _is_package_available(\"quark\")\n+\n+\n+@lru_cache\n+def is_fp_quant_available():\n+    is_available, fp_quant_version = _is_package_available(\"fp_quant\", return_version=True)\n+    return is_available and version.parse(fp_quant_version) >= version.parse(\"0.2.0\")\n+\n+\n+@lru_cache\n+def is_qutlass_available():\n+    is_available, qutlass_version = _is_package_available(\"qutlass\", return_version=True)\n+    return is_available and version.parse(qutlass_version) >= version.parse(\"0.1.0\")\n+\n+\n+@lru_cache\n+def is_compressed_tensors_available() -> bool:\n+    return _is_package_available(\"compressed_tensors\")\n+\n \n @lru_cache\n-def is_flash_attn_3_available() -> bool:\n-    if not is_torch_available():\n-        return False\n+def is_auto_gptq_available() -> bool:\n+    return _is_package_available(\"auto_gptq\")\n \n-    if not _is_package_available(\"flash_attn_3\"):\n-        return False\n \n-    import torch\n+@lru_cache\n+def is_gptqmodel_available() -> bool:\n+    return _is_package_available(\"gptqmodel\")\n \n-    if not torch.cuda.is_available():\n-        return False\n \n-    # TODO: Check for a minimum version when FA3 is stable\n-    # return version.parse(importlib.metadata.version(\"flash_attn_3\")) >= version.parse(\"3.0.0\")\n+@lru_cache\n+def is_eetq_available() -> bool:\n+    return _is_package_available(\"eetq\")\n \n-    return True\n+\n+@lru_cache\n+def is_fbgemm_gpu_available() -> bool:\n+    return _is_package_available(\"fbgemm_gpu\")\n \n \n @lru_cache\n-def is_flash_attn_greater_or_equal_2_10() -> bool:\n-    if not _is_package_available(\"flash_attn\"):\n-        return False\n+def is_levenshtein_available() -> bool:\n+    return _is_package_available(\"Levenshtein\")\n \n-    return version.parse(importlib.metadata.version(\"flash_attn\")) >= version.parse(\"2.1.0\")\n+\n+@lru_cache\n+def is_optimum_neuron_available() -> bool:\n+    return is_optimum_available() and _is_package_available(\"optimum.neuron\")\n \n \n @lru_cache\n-def is_flash_attn_greater_or_equal(library_version: str) -> bool:\n-    if not _is_package_available(\"flash_attn\"):\n-        return False\n+def is_tokenizers_available() -> bool:\n+    return _is_package_available(\"tokenizers\")\n \n-    return version.parse(importlib.metadata.version(\"flash_attn\")) >= version.parse(library_version)\n+\n+@lru_cache\n+def is_vision_available() -> bool:\n+    return _is_package_available(\"PIL\")\n \n \n @lru_cache\n-def is_torch_greater_or_equal(library_version: str, accept_dev: bool = False) -> bool:\n-    \"\"\"\n-    Accepts a library version and returns True if the current version of the library is greater than or equal to the\n-    given version. If `accept_dev` is True, it will also accept development versions (e.g. 2.7.0.dev20250320 matches\n-    2.7.0).\n-    \"\"\"\n-    if not _is_package_available(\"torch\"):\n-        return False\n+def is_pytesseract_available() -> bool:\n+    return _is_package_available(\"pytesseract\")\n \n-    if accept_dev:\n-        return version.parse(version.parse(importlib.metadata.version(\"torch\")).base_version) >= version.parse(\n-            library_version\n-        )\n-    else:\n-        return version.parse(importlib.metadata.version(\"torch\")) >= version.parse(library_version)\n+\n+@lru_cache\n+def is_pytest_available() -> bool:\n+    return _is_package_available(\"pytest\")\n \n \n @lru_cache\n-def is_torch_less_or_equal(library_version: str, accept_dev: bool = False) -> bool:\n-    \"\"\"\n-    Accepts a library version and returns True if the current version of the library is less than or equal to the\n-    given version. If `accept_dev` is True, it will also accept development versions (e.g. 2.7.0.dev20250320 matches\n-    2.7.0).\n-    \"\"\"\n-    if not _is_package_available(\"torch\"):\n-        return False\n+def is_spacy_available() -> bool:\n+    return _is_package_available(\"spacy\")\n \n-    if accept_dev:\n-        return version.parse(version.parse(importlib.metadata.version(\"torch\")).base_version) <= version.parse(\n-            library_version\n-        )\n-    else:\n-        return version.parse(importlib.metadata.version(\"torch\")) <= version.parse(library_version)\n+\n+@lru_cache\n+def is_pytorch_quantization_available() -> bool:\n+    return _is_package_available(\"pytorch_quantization\")\n \n \n @lru_cache\n-def is_huggingface_hub_greater_or_equal(library_version: str, accept_dev: bool = False) -> bool:\n-    if not _is_package_available(\"huggingface_hub\"):\n-        return False\n+def is_pandas_available() -> bool:\n+    return _is_package_available(\"pandas\")\n \n-    if accept_dev:\n-        return version.parse(\n-            version.parse(importlib.metadata.version(\"huggingface_hub\")).base_version\n-        ) >= version.parse(library_version)\n-    else:\n-        return version.parse(importlib.metadata.version(\"huggingface_hub\")) >= version.parse(library_version)\n+\n+@lru_cache\n+def is_soundfile_available() -> bool:\n+    return _is_package_available(\"soundfile\")\n \n \n @lru_cache\n-def is_quanto_greater(library_version: str, accept_dev: bool = False) -> bool:\n-    \"\"\"\n-    Accepts a library version and returns True if the current version of the library is greater than or equal to the\n-    given version. If `accept_dev` is True, it will also accept development versions (e.g. 2.7.0.dev20250320 matches\n-    2.7.0).\n-    \"\"\"\n-    if not _is_package_available(\"optimum.quanto\"):\n-        return False\n+def is_timm_available() -> bool:\n+    return _is_package_available(\"timm\")\n \n-    if accept_dev:\n-        return version.parse(version.parse(importlib.metadata.version(\"optimum-quanto\")).base_version) > version.parse(\n-            library_version\n-        )\n-    else:\n-        return version.parse(importlib.metadata.version(\"optimum-quanto\")) > version.parse(library_version)\n \n+@lru_cache\n+def is_natten_available() -> bool:\n+    return _is_package_available(\"natten\")\n \n-def is_torchdistx_available():\n-    return _torchdistx_available\n \n+@lru_cache\n+def is_nltk_available() -> bool:\n+    return _is_package_available(\"nltk\")\n \n-def is_faiss_available() -> bool:\n-    return _faiss_available\n \n+@lru_cache\n+def is_torchaudio_available() -> bool:\n+    return _is_package_available(\"torchaudio\")\n \n-def is_scipy_available() -> Union[tuple[bool, str], bool]:\n-    return _scipy_available\n \n+@lru_cache\n+def is_torchao_available(min_version: str = TORCHAO_MIN_VERSION) -> bool:\n+    is_available, torchao_version = _is_package_available(\"torchao\", return_version=True)\n+    return is_available and version.parse(torchao_version) >= version.parse(min_version)\n \n-def is_sklearn_available() -> Union[tuple[bool, str], bool]:\n-    return _sklearn_available\n \n+@lru_cache\n+def is_speech_available() -> bool:\n+    # For now this depends on torchaudio but the exact dependency might evolve in the future.\n+    return is_torchaudio_available()\n \n-def is_sentencepiece_available() -> Union[tuple[bool, str], bool]:\n-    return _sentencepiece_available\n \n+@lru_cache\n+def is_spqr_available() -> bool:\n+    return _is_package_available(\"spqr_quant\")\n \n-def is_seqio_available() -> Union[tuple[bool, str], bool]:\n-    return _is_seqio_available\n \n+@lru_cache\n+def is_phonemizer_available() -> bool:\n+    return _is_package_available(\"phonemizer\")\n \n-def is_gguf_available(min_version: str = GGUF_MIN_VERSION) -> bool:\n-    return _is_gguf_available and version.parse(_gguf_version) >= version.parse(min_version)\n \n+@lru_cache\n+def is_uroman_available() -> bool:\n+    return _is_package_available(\"uroman\")\n \n-def is_protobuf_available() -> bool:\n-    if importlib.util.find_spec(\"google\") is None:\n-        return False\n-    return importlib.util.find_spec(\"google.protobuf\") is not None\n \n+@lru_cache\n+def is_ccl_available() -> bool:\n+    return _is_package_available(\"torch_ccl\") or _is_package_available(\"oneccl_bindings_for_pytorch\")\n \n-def is_fsdp_available(min_version: str = FSDP_MIN_VERSION) -> bool:\n-    return is_torch_available() and version.parse(_torch_version) >= version.parse(min_version)\n \n+@lru_cache\n+def is_sudachi_available() -> bool:\n+    return _is_package_available(\"sudachipy\")\n \n-def is_optimum_available() -> Union[tuple[bool, str], bool]:\n-    return _optimum_available\n \n+@lru_cache\n+def is_sudachi_projection_available() -> bool:\n+    is_available, sudachipy_version = _is_package_available(\"sudachipy\", return_version=True)\n+    return is_available and version.parse(sudachipy_version) >= version.parse(\"0.6.8\")\n \n-def is_auto_awq_available() -> bool:\n-    return _auto_awq_available\n \n+@lru_cache\n+def is_jumanpp_available() -> bool:\n+    return _is_package_available(\"rhoknp\") and shutil.which(\"jumanpp\") is not None\n \n-def is_auto_round_available(min_version: str = AUTOROUND_MIN_VERSION) -> bool:\n-    return _auto_round_available and version.parse(_auto_round_version) >= version.parse(min_version)\n \n+@lru_cache\n+def is_cython_available() -> bool:\n+    return _is_package_available(\"pyximport\")\n \n-def is_optimum_quanto_available():\n-    # `importlib.metadata.version` doesn't work with `optimum.quanto`, need to put `optimum_quanto`\n-    return _is_optimum_quanto_available\n \n+@lru_cache\n+def is_jinja_available() -> bool:\n+    return _is_package_available(\"jinja2\")\n \n-def is_quark_available() -> Union[tuple[bool, str], bool]:\n-    return _quark_available\n \n+@lru_cache\n+def is_mlx_available() -> bool:\n+    return _is_package_available(\"mlx\")\n \n-def is_fp_quant_available():\n-    return _fp_quant_available and version.parse(_fp_quant_version) >= version.parse(\"0.2.0\")\n \n+@lru_cache\n+def is_num2words_available() -> bool:\n+    return _is_package_available(\"num2words\")\n \n-def is_qutlass_available():\n-    return _qutlass_available and version.parse(_qutlass_version) >= version.parse(\"0.1.0\")\n \n+@lru_cache\n+def is_tiktoken_available() -> bool:\n+    return _is_package_available(\"tiktoken\") and _is_package_available(\"blobfile\")\n \n-def is_compressed_tensors_available() -> bool:\n-    return _compressed_tensors_available\n \n+@lru_cache\n+def is_liger_kernel_available() -> bool:\n+    is_available, liger_kernel_version = _is_package_available(\"liger_kernel\", return_version=True)\n+    return is_available and version.parse(liger_kernel_version) >= version.parse(\"0.3.0\")\n \n-def is_auto_gptq_available() -> Union[tuple[bool, str], bool]:\n-    return _auto_gptq_available\n \n+@lru_cache\n+def is_rich_available() -> bool:\n+    return _is_package_available(\"rich\")\n \n-def is_gptqmodel_available() -> Union[tuple[bool, str], bool]:\n-    return _gptqmodel_available\n \n+@lru_cache\n+def is_matplotlib_available() -> bool:\n+    return _is_package_available(\"matplotlib\")\n \n-def is_eetq_available() -> Union[tuple[bool, str], bool]:\n-    return _eetq_available\n \n+@lru_cache\n+def is_mistral_common_available() -> bool:\n+    return _is_package_available(\"mistral_common\")\n \n-def is_fbgemm_gpu_available() -> Union[tuple[bool, str], bool]:\n-    return _fbgemm_gpu_available\n \n+def check_torch_load_is_safe() -> None:\n+    if not is_torch_greater_or_equal(\"2.6\"):\n+        raise ValueError(\n+            \"Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \"\n+            \"to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \"\n+            \"when loading files with safetensors.\"\n+            \"\\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\"\n+        )\n \n-def is_levenshtein_available() -> Union[tuple[bool, str], bool]:\n-    return _levenshtein_available\n \n+def torch_only_method(fn: Callable) -> Callable:\n+    def wrapper(*args, **kwargs):\n+        if not is_torch_available():\n+            raise ImportError(\"You need to install pytorch to use this method or class\")\n+        else:\n+            return fn(*args, **kwargs)\n \n-def is_optimum_neuron_available() -> Union[tuple[bool, str], bool]:\n-    return _optimum_available and _is_package_available(\"optimum.neuron\")\n+    return wrapper\n \n \n-def is_safetensors_available() -> Union[tuple[bool, str], bool]:\n-    return _safetensors_available\n+def is_torch_deterministic() -> bool:\n+    \"\"\"\n+    Check whether pytorch uses deterministic algorithms by looking if torch.set_deterministic_debug_mode() is set to 1 or 2\"\n+    \"\"\"\n+    if is_torch_available():\n+        import torch\n \n+        if torch.get_deterministic_debug_mode() == 0:\n+            return False\n+        else:\n+            return True\n \n-def is_tokenizers_available() -> Union[tuple[bool, str], bool]:\n-    return _tokenizers_available\n+    return False\n \n \n @lru_cache\n-def is_vision_available() -> bool:\n-    _pil_available = importlib.util.find_spec(\"PIL\") is not None\n-    if _pil_available:\n+def get_torch_major_and_minor_version() -> str:\n+    torch_version = get_torch_version()\n+    if torch_version == \"N/A\":\n+        return \"N/A\"\n+    parsed_version = version.parse(torch_version)\n+    return str(parsed_version.major) + \".\" + str(parsed_version.minor)\n+\n+\n+def is_torchdynamo_compiling() -> bool:\n+    # Importing torch._dynamo causes issues with PyTorch profiler (https://github.com/pytorch/pytorch/issues/130622)\n+    # hence rather relying on `torch.compiler.is_compiling()` when possible (torch>=2.3)\n+    try:\n+        import torch\n+\n+        return torch.compiler.is_compiling()\n+    except Exception:\n         try:\n-            package_version = importlib.metadata.version(\"Pillow\")\n-        except importlib.metadata.PackageNotFoundError:\n-            try:\n-                package_version = importlib.metadata.version(\"Pillow-SIMD\")\n-            except importlib.metadata.PackageNotFoundError:\n-                return False\n-        logger.debug(f\"Detected PIL version {package_version}\")\n-    return _pil_available\n+            import torch._dynamo as dynamo\n+\n+            return dynamo.is_compiling()\n+        except Exception:\n+            return False\n+\n \n+def is_torchdynamo_exporting() -> bool:\n+    try:\n+        import torch\n \n-def is_pytesseract_available() -> Union[tuple[bool, str], bool]:\n-    return _pytesseract_available\n+        return torch.compiler.is_exporting()\n+    except Exception:\n+        try:\n+            import torch._dynamo as dynamo\n \n+            return dynamo.is_exporting()\n+        except Exception:\n+            return False\n \n-def is_pytest_available() -> Union[tuple[bool, str], bool]:\n-    return _pytest_available\n \n+def is_torch_fx_proxy(x):\n+    try:\n+        import torch.fx\n \n-def is_spacy_available() -> Union[tuple[bool, str], bool]:\n-    return _spacy_available\n+        return isinstance(x, torch.fx.Proxy)\n+    except Exception:\n+        return False\n \n \n+@lru_cache\n def is_in_notebook() -> bool:\n     try:\n         # Check if we are running inside Marimo\n@@ -1380,14 +1279,6 @@ def is_in_notebook() -> bool:\n         return False\n \n \n-def is_pytorch_quantization_available() -> Union[tuple[bool, str], bool]:\n-    return _pytorch_quantization_available\n-\n-\n-def is_pandas_available() -> Union[tuple[bool, str], bool]:\n-    return _pandas_available\n-\n-\n def is_sagemaker_dp_enabled() -> bool:\n     # Get the sagemaker specific env variable.\n     sagemaker_params = os.getenv(\"SM_FRAMEWORK_PARAMS\", \"{}\")\n@@ -1399,7 +1290,7 @@ def is_sagemaker_dp_enabled() -> bool:\n     except json.JSONDecodeError:\n         return False\n     # Lastly, check if the `smdistributed` module is present.\n-    return _smdistributed_available\n+    return _is_package_available(\"smdistributed\")\n \n \n def is_sagemaker_mp_enabled() -> bool:\n@@ -1423,138 +1314,13 @@ def is_sagemaker_mp_enabled() -> bool:\n     except json.JSONDecodeError:\n         return False\n     # Lastly, check if the `smdistributed` module is present.\n-    return _smdistributed_available\n+    return _is_package_available(\"smdistributed\")\n \n \n def is_training_run_on_sagemaker() -> bool:\n     return \"SAGEMAKER_JOB_NAME\" in os.environ\n \n \n-def is_soundfile_available() -> Union[tuple[bool, str], bool]:\n-    return _soundfile_available\n-\n-\n-def is_timm_available() -> Union[tuple[bool, str], bool]:\n-    return _timm_available\n-\n-\n-def is_natten_available() -> Union[tuple[bool, str], bool]:\n-    return _natten_available\n-\n-\n-def is_nltk_available() -> Union[tuple[bool, str], bool]:\n-    return _nltk_available\n-\n-\n-def is_torchaudio_available() -> Union[tuple[bool, str], bool]:\n-    return _torchaudio_available\n-\n-\n-def is_torchao_available(min_version: str = TORCHAO_MIN_VERSION) -> bool:\n-    return _torchao_available and version.parse(_torchao_version) >= version.parse(min_version)\n-\n-\n-def is_speech_available() -> Union[tuple[bool, str], bool]:\n-    # For now this depends on torchaudio but the exact dependency might evolve in the future.\n-    return _torchaudio_available\n-\n-\n-def is_spqr_available() -> Union[tuple[bool, str], bool]:\n-    return _spqr_available\n-\n-\n-def is_phonemizer_available() -> Union[tuple[bool, str], bool]:\n-    return _phonemizer_available\n-\n-\n-def is_uroman_available() -> Union[tuple[bool, str], bool]:\n-    return _uroman_available\n-\n-\n-def torch_only_method(fn: Callable) -> Callable:\n-    def wrapper(*args, **kwargs):\n-        if not _torch_available:\n-            raise ImportError(\"You need to install pytorch to use this method or class\")\n-        else:\n-            return fn(*args, **kwargs)\n-\n-    return wrapper\n-\n-\n-def is_ccl_available() -> bool:\n-    return _is_ccl_available\n-\n-\n-def is_sudachi_available() -> bool:\n-    return _sudachipy_available\n-\n-\n-def get_sudachi_version() -> bool:\n-    return _sudachipy_version\n-\n-\n-def is_sudachi_projection_available() -> bool:\n-    if not is_sudachi_available():\n-        return False\n-\n-    # NOTE: We require sudachipy>=0.6.8 to use projection option in sudachi_kwargs for the constructor of BertJapaneseTokenizer.\n-    # - `projection` option is not supported in sudachipy<0.6.8, see https://github.com/WorksApplications/sudachi.rs/issues/230\n-    return version.parse(_sudachipy_version) >= version.parse(\"0.6.8\")\n-\n-\n-def is_jumanpp_available() -> bool:\n-    return (importlib.util.find_spec(\"rhoknp\") is not None) and (shutil.which(\"jumanpp\") is not None)\n-\n-\n-def is_cython_available() -> bool:\n-    return importlib.util.find_spec(\"pyximport\") is not None\n-\n-\n-def is_jinja_available() -> Union[tuple[bool, str], bool]:\n-    return _jinja_available\n-\n-\n-def is_mlx_available() -> Union[tuple[bool, str], bool]:\n-    return _mlx_available\n-\n-\n-def is_num2words_available() -> Union[tuple[bool, str], bool]:\n-    return _num2words_available\n-\n-\n-def is_tiktoken_available() -> Union[tuple[bool, str], bool]:\n-    return _tiktoken_available and _blobfile_available\n-\n-\n-def is_liger_kernel_available() -> bool:\n-    if not _liger_kernel_available:\n-        return False\n-\n-    return version.parse(importlib.metadata.version(\"liger_kernel\")) >= version.parse(\"0.3.0\")\n-\n-\n-def is_rich_available() -> Union[tuple[bool, str], bool]:\n-    return _rich_available\n-\n-\n-def is_matplotlib_available() -> Union[tuple[bool, str], bool]:\n-    return _matplotlib_available\n-\n-\n-def is_mistral_common_available() -> Union[tuple[bool, str], bool]:\n-    return _mistral_common_available\n-\n-\n-def check_torch_load_is_safe() -> None:\n-    if not is_torch_greater_or_equal(\"2.6\"):\n-        raise ValueError(\n-            \"Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \"\n-            \"to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \"\n-            \"when loading files with safetensors.\"\n-            \"\\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\"\n-        )\n-\n-\n # docstyle-ignore\n AV_IMPORT_ERROR = \"\"\"\n {0} requires the PyAv library but it was not found in your environment. You can install it with:\n@@ -1995,14 +1761,6 @@ def __getattribute__(cls, key):\n         requires_backends(cls, cls._backends)\n \n \n-def is_torch_fx_proxy(x):\n-    if is_torch_fx_available():\n-        import torch.fx\n-\n-        return isinstance(x, torch.fx.Proxy)\n-    return False\n-\n-\n BACKENDS_T = frozenset[str]\n IMPORT_STRUCTURE_T = dict[BACKENDS_T, dict[str, set[str]]]\n \n@@ -2072,7 +1830,7 @@ def __init__(\n                     try:\n                         if not callable():\n                             missing_backends.append(backend)\n-                    except (importlib.metadata.PackageNotFoundError, ModuleNotFoundError, RuntimeError):\n+                    except (ModuleNotFoundError, RuntimeError):\n                         missing_backends.append(backend)\n \n                 self._modules = self._modules.union(module_keys)\n@@ -2253,9 +2011,16 @@ def __init__(self, backend_requirement: str):\n                 f\"Backends should be defined in the BACKENDS_MAPPING. Offending backend: {self.package_name}\"\n             )\n \n+    def get_installed_version(self) -> str:\n+        \"\"\"Return the currently installed version of the backend\"\"\"\n+        is_available, current_version = _is_package_available(self.package_name, return_version=True)\n+        if not is_available:\n+            raise RuntimeError(f\"Backend {self.package_name} is not available.\")\n+        return current_version\n+\n     def is_satisfied(self) -> bool:\n         return VersionComparison.from_string(self.version_comparison)(\n-            version.parse(importlib.metadata.version(self.package_name)), version.parse(self.version)\n+            version.parse(self.get_installed_version()), version.parse(self.version)\n         )\n \n     def __repr__(self) -> str:"
        },
        {
            "sha": "c838f8c885d5f604e55d1adaaa193f12e60cf553",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b172b8eb839300e697e91911eac66db7441316/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b172b8eb839300e697e91911eac66db7441316/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=55b172b8eb839300e697e91911eac66db7441316",
            "patch": "@@ -4916,7 +4916,7 @@ def setUp(self):\n         self.original_threshold = self.assistant_model.generation_config.assistant_confidence_threshold\n \n     def assert_no_sklearn(self):\n-        with patch(\"transformers.utils.import_utils._sklearn_available\", False):\n+        with patch(\"transformers.generation.candidate_generator.is_sklearn_available\", lambda: False):\n             self.candidate_generator.update_candidate_strategy(self.input_ids, None, self.num_matches)\n             self.assertEqual(self.candidate_generator.matches, self.original_matches)\n             self.assertEqual(self.candidate_generator.probs, self.original_probs)"
        }
    ],
    "stats": {
        "total": 1853,
        "additions": 781,
        "deletions": 1072
    }
}