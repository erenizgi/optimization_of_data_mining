{
    "author": "gante",
    "message": "Generate: throw warning when `return_dict_in_generate` is False but should be True (#33146)",
    "sha": "eb5b968c5d80271ecb29917dffecc8f4c00247a8",
    "files": [
        {
            "sha": "af62d0c797514c223565d6cd728e10fc5867898a",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 23,
            "deletions": 3,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb5b968c5d80271ecb29917dffecc8f4c00247a8/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb5b968c5d80271ecb29917dffecc8f4c00247a8/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=eb5b968c5d80271ecb29917dffecc8f4c00247a8",
            "patch": "@@ -288,7 +288,9 @@ class GenerationConfig(PushToHubMixin):\n             Whether or not to return the unprocessed prediction logit scores. See `logits` under returned tensors for\n             more details.\n         return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+            Whether or not to return a [`~utils.ModelOutput`], as opposed to returning exclusively the generated\n+            sequence. This flag must be set to `True` to return the generation cache (when `use_cache` is `True`)\n+            or optional outputs (see flags starting with `output_`)\n \n         > Special tokens that can be used at generation time\n \n@@ -334,6 +336,8 @@ class GenerationConfig(PushToHubMixin):\n             present in `generate`'s signature will be used in the model forward pass.\n     \"\"\"\n \n+    extra_output_flags = (\"output_attentions\", \"output_hidden_states\", \"output_scores\", \"output_logits\")\n+\n     def __init__(self, **kwargs):\n         # Parameters that control the length of the output\n         self.max_length = kwargs.pop(\"max_length\", 20)\n@@ -727,7 +731,17 @@ def validate(self, is_init=False):\n                 self.watermarking_config = WatermarkingConfig.from_dict(self.watermarking_config)\n             self.watermarking_config.validate()\n \n-        # 7. check common issue: passing `generate` arguments inside the generation config\n+        # 7. other incorrect combinations\n+        if self.return_dict_in_generate is not True:\n+            for extra_output_flag in self.extra_output_flags:\n+                if getattr(self, extra_output_flag) is True:\n+                    warnings.warn(\n+                        f\"`return_dict_in_generate` is NOT set to `True`, but `{extra_output_flag}` is. When \"\n+                        f\"`return_dict_in_generate` is not `True`, `{extra_output_flag}` is ignored.\",\n+                        UserWarning,\n+                    )\n+\n+        # 8. check common issue: passing `generate` arguments inside the generation config\n         generate_arguments = (\n             \"logits_processor\",\n             \"stopping_criteria\",\n@@ -786,7 +800,8 @@ def save_pretrained(\n \n         if use_auth_token is not None:\n             warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n+                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. \"\n+                \"Please use `token` instead.\",\n                 FutureWarning,\n             )\n             if kwargs.get(\"token\", None) is not None:\n@@ -1189,6 +1204,11 @@ def from_model_config(cls, model_config: PretrainedConfig) -> \"GenerationConfig\"\n                     if attr in decoder_config and getattr(config, attr) == getattr(default_generation_config, attr):\n                         setattr(config, attr, decoder_config[attr])\n \n+        # If any `output_...` flag is set to `True`, we ensure `return_dict_in_generate` is set to `True`.\n+        if config.return_dict_in_generate is False:\n+            if any(getattr(config, extra_output_flag, False) for extra_output_flag in config.extra_output_flags):\n+                config.return_dict_in_generate = True\n+\n         config._original_object_hash = hash(config)  # Hash to detect whether the instance was modified\n         return config\n "
        },
        {
            "sha": "cd5f3d50162c455f32d7c0e4abae26e0d8662c03",
            "filename": "tests/generation/test_configuration_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb5b968c5d80271ecb29917dffecc8f4c00247a8/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb5b968c5d80271ecb29917dffecc8f4c00247a8/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_configuration_utils.py?ref=eb5b968c5d80271ecb29917dffecc8f4c00247a8",
            "patch": "@@ -136,6 +136,10 @@ def test_validate(self):\n             GenerationConfig(do_sample=False, temperature=0.5)\n         self.assertEqual(len(captured_warnings), 1)\n \n+        with warnings.catch_warnings(record=True) as captured_warnings:\n+            GenerationConfig(return_dict_in_generate=False, output_scores=True)\n+        self.assertEqual(len(captured_warnings), 1)\n+\n         # Expanding on the case above, we can update a bad configuration to get rid of the warning. Ideally,\n         # that is done by unsetting the parameter (i.e. setting it to None)\n         generation_config_bad_temperature = GenerationConfig(do_sample=False, temperature=0.5)"
        }
    ],
    "stats": {
        "total": 30,
        "additions": 27,
        "deletions": 3
    }
}