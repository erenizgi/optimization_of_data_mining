{
    "author": "Cyrilvallez",
    "message": "Add buffers to `_init_weights` for ALL models (#42309)\n\n* start\n\n* all until clvp\n\n* all until gpt2\n\n* until lfm2_moe\n\n* all until seamless\n\n* finally all first batch\n\n* style\n\n* Copied from\n\n* apply modulars\n\n* small fixes\n\n* add test\n\n* name\n\n* fix\n\n* more\n\n* fix typos\n\n* more\n\n* fix\n\n* typo\n\n* fix\n\n* revert annoying dates auto change....\n\n* fixes\n\n* fix\n\n* fix\n\n* oupsi\n\n* fixes\n\n* start more fixes\n\n* fix\n\n* add norm buffers\n\n* modular\n\n* improve\n\n* copies\n\n* fixes\n\n* fix advanced rope modules\n\n* more and more\n\n* improve error\n\n* fix\n\n* fixes\n\n* fix\n\n* fixes\n\n* post rebase\n\n* last fix\n\n* really last fix\n\n* stupid layoutlm2 with its external lib\n\n* stupid layoutlmv2 finally....\n\n* create functions",
    "sha": "537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
    "files": [
        {
            "sha": "873df31e84947d0125ae29db846b0b02c3ed2f7b",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -255,7 +255,7 @@ def _compute_dynamic_ntk_parameters(\n \n def _compute_yarn_parameters(\n     config: \"PreTrainedConfig\",\n-    device: \"torch.device\",\n+    device: Optional[\"torch.device\"] = None,\n     seq_len: Optional[int] = None,\n     layer_type: Optional[str] = None,\n ) -> tuple[\"torch.Tensor\", float]:\n@@ -393,7 +393,7 @@ def linear_ramp_factor(min, max, dim):\n \n def _compute_longrope_parameters(\n     config: \"PreTrainedConfig\",\n-    device: \"torch.device\",\n+    device: Optional[\"torch.device\"] = None,\n     seq_len: Optional[int] = None,\n     layer_type: Optional[str] = None,\n ) -> tuple[\"torch.Tensor\", float]:\n@@ -483,7 +483,7 @@ def _compute_longrope_parameters(\n \n def _compute_llama3_parameters(\n     config: \"PreTrainedConfig\",\n-    device: \"torch.device\",\n+    device: Optional[\"torch.device\"] = None,\n     seq_len: Optional[int] = None,\n     layer_type: Optional[str] = None,\n ) -> tuple[\"torch.Tensor\", float]:"
        },
        {
            "sha": "d729cb15e3bfa515e96314088ccee1188f08397e",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 27,
            "deletions": 11,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -86,6 +86,7 @@\n )\n from .loss.loss_utils import LOSS_MAPPING\n from .modeling_flash_attention_utils import lazy_import_flash_attention, lazy_import_paged_flash_attention\n+from .modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from .pytorch_utils import id_tensor_storage\n from .quantizers import HfQuantizer\n from .quantizers.auto import get_hf_quantizer\n@@ -2205,16 +2206,14 @@ def _init_weights(self, module):\n             std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\n \n         if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d)):\n-            if getattr(module, \"weight\", None) is not None:\n-                init.normal_(module.weight, mean=0.0, std=std)\n-            if getattr(module, \"bias\", None) is not None:\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            if module.bias is not None:\n                 init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            if getattr(module, \"weight\", None) is not None:\n-                init.normal_(module.weight, mean=0.0, std=std)\n-                # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n-                if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n-                    init.zeros_(module.weight[module.padding_idx])\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, nn.MultiheadAttention):\n             # This uses torch's original init\n             module._reset_parameters()\n@@ -2226,10 +2225,25 @@ def _init_weights(self, module):\n             or \"RMSNorm\" in module.__class__.__name__\n         ):\n             # Norms can exist without weights (in which case they are None from torch primitives)\n-            if hasattr(module, \"weight\") and module.weight is not None:\n+            if getattr(module, \"weight\", None) is not None:\n                 init.ones_(module.weight)\n-            if hasattr(module, \"bias\") and module.bias is not None:\n+            if getattr(module, \"bias\", None) is not None:\n                 init.zeros_(module.bias)\n+            # And the potential buffers for the BatchNorms\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n+        # This matches all the usual RotaryEmbeddings modules\n+        elif \"RotaryEmbedding\" in module.__class__.__name__ and hasattr(module, \"original_inv_freq\"):\n+            rope_fn = (\n+                ROPE_INIT_FUNCTIONS[module.rope_type]\n+                if module.rope_type != \"default\"\n+                else module.compute_default_rope_parameters\n+            )\n+            buffer_value, _ = rope_fn(module.config)\n+            init.copy_(module.inv_freq, buffer_value)\n+            init.copy_(module.original_inv_freq, buffer_value)\n \n     def _initialize_weights(self, module):\n         \"\"\"\n@@ -2239,7 +2253,9 @@ def _initialize_weights(self, module):\n             return\n \n         self._init_weights(module)\n-        module._is_hf_initialized = True\n+        # If we are not currently under meta device (which would virtually skip `_init_weights`), mark as initialized\n+        if get_torch_context_manager_or_global_device() != torch.device(\"meta\"):\n+            module._is_hf_initialized = True\n \n     @torch.no_grad()\n     @init.guard_torch_init_functions()"
        },
        {
            "sha": "c9ba7d8dbf59dd1d4557a5291aa3d7f588e9823a",
            "filename": "src/transformers/models/afmoe/modeling_afmoe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 14,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -25,6 +25,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -58,7 +59,7 @@ def __init__(self, config: AfmoeConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -531,20 +532,11 @@ class AfmoePreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                nn.init.zeros_(module.bias)\n-        elif isinstance(module, nn.Embedding):\n-            nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                nn.init.zeros_(module.weight[module.padding_idx])\n-        elif isinstance(module, AfmoeRMSNorm):\n-            nn.init.ones_(module.weight)\n-        elif isinstance(module, AfmoeTokenChoiceRouter):\n-            nn.init.zeros_(module.gate.weight)\n+        super()._init_weights(module)\n+        if isinstance(module, AfmoeTokenChoiceRouter):\n+            init.zeros_(module.gate.weight)\n         elif isinstance(module, AfmoeMoE):\n-            nn.init.zeros_(module.expert_bias)\n+            init.zeros_(module.expert_bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "ba240a55c8412cde13aa8e36f0819d5c32512671",
            "filename": "src/transformers/models/afmoe/modular_afmoe.py",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodular_afmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodular_afmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodular_afmoe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -20,6 +20,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n@@ -350,20 +351,11 @@ class AfmoePreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                nn.init.zeros_(module.bias)\n-        elif isinstance(module, nn.Embedding):\n-            nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                nn.init.zeros_(module.weight[module.padding_idx])\n-        elif isinstance(module, AfmoeRMSNorm):\n-            nn.init.ones_(module.weight)\n-        elif isinstance(module, AfmoeTokenChoiceRouter):\n-            nn.init.zeros_(module.gate.weight)\n+        super()._init_weights(module)\n+        if isinstance(module, AfmoeTokenChoiceRouter):\n+            init.zeros_(module.gate.weight)\n         elif isinstance(module, AfmoeMoE):\n-            nn.init.zeros_(module.expert_bias)\n+            init.zeros_(module.expert_bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "4ce77f4125f719a2c8170e6f027d5f2d91b0f9bc",
            "filename": "src/transformers/models/aimv2/modeling_aimv2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -414,6 +414,10 @@ def _init_weights(self, module):\n                 init.constant_(module.logit_scale, math.log(1 / 0.07))\n         elif isinstance(module, Aimv2AttentionPoolingHead):\n             init.normal_(module.cls_token, mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, Aimv2VisionEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+        elif isinstance(module, Aimv2TextEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n @auto_docstring("
        },
        {
            "sha": "b94d4bb362b8c93851373e091334f0e9f14a9be3",
            "filename": "src/transformers/models/aimv2/modular_aimv2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -457,6 +457,10 @@ def _init_weights(self, module):\n                 init.constant_(module.logit_scale, math.log(1 / 0.07))\n         elif isinstance(module, Aimv2AttentionPoolingHead):\n             init.normal_(module.cls_token, mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, Aimv2VisionEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+        elif isinstance(module, Aimv2TextEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n @auto_docstring("
        },
        {
            "sha": "c46aa48f70ba686ff425a6f15235c7f60277e250",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -320,6 +320,9 @@ def _init_weights(self, module):\n             init.ones_(module.weight)\n         elif isinstance(module, AlbertMLMHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, AlbertEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n \n \n @dataclass"
        },
        {
            "sha": "0e3b428a4340f84b8227208bc1fe5be6db544f6f",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -844,6 +844,13 @@ def _init_weights(self, module: nn.Module):\n         if isinstance(module, (nn.LayerNorm, nn.BatchNorm2d)):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n+        elif isinstance(module, AlignTextEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n \n \n @auto_docstring("
        },
        {
            "sha": "d2afd7ddc661a85f9df1b39e2fa4447839f1fcbd",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -780,6 +780,7 @@ def _init_weights(self, module):\n             init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n             init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n             init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.copy_(module.position_ids, torch.arange(module.num_positions).expand((1, -1)))\n         elif isinstance(module, AltCLIPAttention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n@@ -815,6 +816,9 @@ def _init_weights(self, module):\n             # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n             if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n                 init.zeros_(module.weight[module.padding_idx])\n+        elif isinstance(module, AltRobertaEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n \n \n class AltCLIPVisionTransformer(nn.Module):"
        },
        {
            "sha": "c9cd278e0d04861b7559050f462f327dbd326208",
            "filename": "src/transformers/models/apertus/modeling_apertus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -94,7 +94,7 @@ def __init__(self, config: ApertusConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "649e2bdbba7fbdeba67a6b9eeeafce98205bf290",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -99,7 +99,7 @@ def __init__(self, config: ArceeConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "7a4c7faaef38b7640b0fad00d156b8b5ae25f44c",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -636,7 +636,7 @@ def __init__(self, config: AriaTextConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "9bced3a6d383e4778600bc1534eaa846a9e3168b",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -200,7 +200,7 @@ def __init__(self, config: BambaConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "db8d9e466bf8fcf00a469e9cac0ba11d0074a7b0",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n from torch.nn import functional as F\n \n+from ... import initialization as init\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...generation.logits_process import (\n@@ -349,6 +350,14 @@ def device(self) -> torch.device:\n \n         return super().device\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, BarkSelfAttention):\n+            if module.is_causal:\n+                block_size = module.config.block_size\n+                bias = torch.tril(torch.ones((block_size, block_size), dtype=bool)).view(1, 1, block_size, block_size)\n+                init.copy_(module.bias, bias)\n+\n \n # GPT2-like autoregressive model\n class BarkCausalModel(BarkPreTrainedModel, GenerationMixin):"
        },
        {
            "sha": "80375db1971cc2fc2709c6a4fe26e0a5e506a4c9",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -476,6 +477,11 @@ class BartPreTrainedModel(PreTrainedModel):\n \n     _can_compile_fullgraph = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, BartForConditionalGeneration):\n+            init.zeros_(module.final_logits_bias)\n+\n     @property\n     def dummy_inputs(self):\n         pad_token = self.config.pad_token_id"
        },
        {
            "sha": "59537c73ddaf29d6adc11c54c3db1eece8f45efc",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -569,6 +569,9 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, BertLMPredictionHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, BertEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n \n \n @dataclass"
        },
        {
            "sha": "a9a3246ba95a5ec16a763f54fefde6ca96d51a50",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -463,6 +463,8 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, BertGenerationOnlyLMHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, BertGenerationEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n @auto_docstring("
        },
        {
            "sha": "f8b8a1b5b9f7b63fdf9260d9b6d8afe5d145ba3a",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -1521,6 +1521,9 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, BigBirdLMPredictionHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, BigBirdEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n \n \n @dataclass"
        },
        {
            "sha": "4a3eec5cf9c974717aeeaaf1186e2dc8443702d4",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -1536,6 +1537,11 @@ class BigBirdPegasusPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _can_compile_fullgraph = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, BigBirdPegasusForConditionalGeneration):\n+            init.zeros_(module.final_logits_bias)\n+\n     @property\n     def dummy_inputs(self):\n         pad_token = self.config.pad_token_id"
        },
        {
            "sha": "7240d1b02d0f813ae3e94e8e5cf08771a7267812",
            "filename": "src/transformers/models/bit/modeling_bit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -643,6 +643,10 @@ def _init_weights(self, module):\n         elif isinstance(module, (nn.BatchNorm2d, nn.GroupNorm)):\n             init.constant_(module.weight, 1)\n             init.constant_(module.bias, 0)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n \n \n @auto_docstring"
        },
        {
            "sha": "cbd1be4f2bc1b9e488e8c7edbb07dbcbf8d7e1e8",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -287,7 +287,7 @@ def __init__(self, config: BitNetConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "e93e66c22514fda298b3255a8bb7364735aed33f",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -24,6 +24,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -437,6 +438,11 @@ class BlenderbotPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _can_compile_fullgraph = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, BlenderbotForConditionalGeneration):\n+            init.zeros_(module.final_logits_bias)\n+\n     @property\n     def dummy_inputs(self):\n         pad_token = self.config.pad_token_id"
        },
        {
            "sha": "284730270e1ab2a85136000849d9e30368092d10",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -430,6 +431,11 @@ class BlenderbotSmallPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _can_compile_fullgraph = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, BlenderbotSmallForConditionalGeneration):\n+            init.zeros_(module.final_logits_bias)\n+\n     @property\n     def dummy_inputs(self):\n         pad_token = self.config.pad_token_id"
        },
        {
            "sha": "7edf72a0339075a55d98c161d1a546f336acf48f",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -430,6 +430,8 @@ def _init_weights(self, module):\n                 std = self.config.vision_config.initializer_range\n             init.trunc_normal_(module.position_embedding, mean=0.0, std=std)\n             init.trunc_normal_(module.class_embedding, mean=0.0, std=std)\n+        elif isinstance(module, BlipTextEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n class BlipEncoder(nn.Module):"
        },
        {
            "sha": "c2ae560897e895926d2eaee5fb88ac1adab06528",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -21,6 +21,7 @@\n from torch import Tensor, device, nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -504,6 +505,11 @@ class BlipTextPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"bert\"\n     _no_split_modules = []\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, BlipTextEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+\n \n # Adapted from https://github.com/salesforce/BLIP/blob/3a29b7410476bf5f2ba0955827390eb6ea1f4f9d/models/med.py#L571\n class BlipTextModel(BlipTextPreTrainedModel):"
        },
        {
            "sha": "d3f88c85571f68a32ecfb9bece65cba078a1a1aa",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -428,6 +428,8 @@ def _init_weights(self, module):\n             ),\n         ):\n             init.zeros_(module.query_tokens)\n+        elif isinstance(module, Blip2TextEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n # Copied from transformers.models.blip.modeling_blip.BlipEncoder with Blip->Blip2"
        },
        {
            "sha": "0efd94e764a3fb2435d25b81b98a3eed54195742",
            "filename": "src/transformers/models/blt/modeling_blt.py",
            "status": "modified",
            "additions": 25,
            "deletions": 16,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -27,6 +27,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -102,7 +103,7 @@ def __init__(self, config: BltConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -458,9 +459,9 @@ def _init_weights(self, module):\n         # Norms: RMSNorm / LayerNorm\n         if isinstance(module, (BltRMSNorm, nn.LayerNorm)) or \"RMSNorm\" in class_name or \"LayerNorm\" in class_name:\n             if getattr(module, \"weight\", None) is not None:\n-                nn.init.ones_(module.weight)\n+                init.ones_(module.weight)\n             if getattr(module, \"bias\", None) is not None:\n-                nn.init.zeros_(module.bias)\n+                init.zeros_(module.bias)\n             return\n \n         # Embeddings (encoder / patcher / hash embeddings)\n@@ -472,15 +473,15 @@ def _init_weights(self, module):\n                 hidden_size = module.embedding_dim\n \n             std = hidden_size**-0.5\n-            nn.init.trunc_normal_(\n+            init.trunc_normal_(\n                 module.weight,\n                 mean=0.0,\n                 std=std,\n                 a=-3 * std,\n                 b=3 * std,\n             )\n             if module.padding_idx is not None:\n-                nn.init.zeros_(module.weight[module.padding_idx])\n+                init.zeros_(module.weight[module.padding_idx])\n             return\n \n         # Self-attention / cross-attention projections\n@@ -506,28 +507,28 @@ def _init_weights(self, module):\n             for proj_name in (\"q_proj\", \"k_proj\", \"v_proj\"):\n                 proj = getattr(module, proj_name, None)\n                 if proj is not None and hasattr(proj, \"weight\"):\n-                    nn.init.trunc_normal_(\n+                    init.trunc_normal_(\n                         proj.weight,\n                         mean=0.0,\n                         std=std,\n                         a=-3 * std,\n                         b=3 * std,\n                     )\n                     if getattr(proj, \"bias\", None) is not None:\n-                        nn.init.zeros_(proj.bias)\n+                        init.zeros_(proj.bias)\n \n             # Output projection: o_proj or dense\n             o_proj = getattr(module, \"o_proj\", getattr(module, \"dense\", None))\n             if o_proj is not None and hasattr(o_proj, \"weight\"):\n-                nn.init.trunc_normal_(\n+                init.trunc_normal_(\n                     o_proj.weight,\n                     mean=0.0,\n                     std=std,\n                     a=-3 * std,\n                     b=3 * std,\n                 )\n                 if getattr(o_proj, \"bias\", None) is not None:\n-                    nn.init.zeros_(o_proj.bias)\n+                    init.zeros_(o_proj.bias)\n             return\n \n         # MLP / FFN blocks\n@@ -551,47 +552,55 @@ def _init_weights(self, module):\n             for proj in (gate_proj, up_proj):\n                 if proj is not None and hasattr(proj, \"weight\"):\n                     std = in_std or (proj.weight.shape[1] ** -0.5)\n-                    nn.init.trunc_normal_(\n+                    init.trunc_normal_(\n                         proj.weight,\n                         mean=0.0,\n                         std=std,\n                         a=-3 * std,\n                         b=3 * std,\n                     )\n                     if getattr(proj, \"bias\", None) is not None:\n-                        nn.init.zeros_(proj.bias)\n+                        init.zeros_(proj.bias)\n \n             # output/ down projections\n             if down_proj is not None and hasattr(down_proj, \"weight\"):\n                 hidden_dim = down_proj.weight.shape[1]\n                 out_std = hidden_dim**-0.5\n-                nn.init.trunc_normal_(\n+                init.trunc_normal_(\n                     down_proj.weight,\n                     mean=0.0,\n                     std=out_std,\n                     a=-3 * out_std,\n                     b=3 * out_std,\n                 )\n                 if getattr(down_proj, \"bias\", None) is not None:\n-                    nn.init.zeros_(down_proj.bias)\n+                    init.zeros_(down_proj.bias)\n             return\n \n         # Generic Linear layers (projections, lm_head, etc.)\n         if isinstance(module, nn.Linear):\n             fan_in = module.in_features\n             std = fan_in**-0.5\n-            nn.init.trunc_normal_(\n+            init.trunc_normal_(\n                 module.weight,\n                 mean=0.0,\n                 std=std,\n                 a=-3 * std,\n                 b=3 * std,\n             )\n             if module.bias is not None:\n-                nn.init.zeros_(module.bias)\n+                init.zeros_(module.bias)\n             return\n \n-        return\n+        if isinstance(module, BltRotaryEmbedding):\n+            rope_fn = (\n+                ROPE_INIT_FUNCTIONS[module.rope_type]\n+                if module.rope_type != \"default\"\n+                else module.compute_default_rope_parameters\n+            )\n+            buffer_value, _ = rope_fn(module.config)\n+            init.copy_(module.inv_freq, buffer_value)\n+            init.copy_(module.original_inv_freq, buffer_value)\n \n \n class BltLocalEncoder(BltPreTrainedModel):"
        },
        {
            "sha": "9f82173035cb3190f795cd337b4d41949ea82dcf",
            "filename": "src/transformers/models/blt/modular_blt.py",
            "status": "modified",
            "additions": 25,
            "deletions": 16,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -22,10 +22,11 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n+from ... import initialization as init\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...masking_utils import create_causal_mask\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n-from ...modeling_rope_utils import dynamic_rope_update\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, logging\n@@ -382,9 +383,9 @@ def _init_weights(self, module):\n         # Norms: RMSNorm / LayerNorm\n         if isinstance(module, (BltRMSNorm, nn.LayerNorm)) or \"RMSNorm\" in class_name or \"LayerNorm\" in class_name:\n             if getattr(module, \"weight\", None) is not None:\n-                nn.init.ones_(module.weight)\n+                init.ones_(module.weight)\n             if getattr(module, \"bias\", None) is not None:\n-                nn.init.zeros_(module.bias)\n+                init.zeros_(module.bias)\n             return\n \n         # Embeddings (encoder / patcher / hash embeddings)\n@@ -396,15 +397,15 @@ def _init_weights(self, module):\n                 hidden_size = module.embedding_dim\n \n             std = hidden_size**-0.5\n-            nn.init.trunc_normal_(\n+            init.trunc_normal_(\n                 module.weight,\n                 mean=0.0,\n                 std=std,\n                 a=-3 * std,\n                 b=3 * std,\n             )\n             if module.padding_idx is not None:\n-                nn.init.zeros_(module.weight[module.padding_idx])\n+                init.zeros_(module.weight[module.padding_idx])\n             return\n \n         # Self-attention / cross-attention projections\n@@ -430,28 +431,28 @@ def _init_weights(self, module):\n             for proj_name in (\"q_proj\", \"k_proj\", \"v_proj\"):\n                 proj = getattr(module, proj_name, None)\n                 if proj is not None and hasattr(proj, \"weight\"):\n-                    nn.init.trunc_normal_(\n+                    init.trunc_normal_(\n                         proj.weight,\n                         mean=0.0,\n                         std=std,\n                         a=-3 * std,\n                         b=3 * std,\n                     )\n                     if getattr(proj, \"bias\", None) is not None:\n-                        nn.init.zeros_(proj.bias)\n+                        init.zeros_(proj.bias)\n \n             # Output projection: o_proj or dense\n             o_proj = getattr(module, \"o_proj\", getattr(module, \"dense\", None))\n             if o_proj is not None and hasattr(o_proj, \"weight\"):\n-                nn.init.trunc_normal_(\n+                init.trunc_normal_(\n                     o_proj.weight,\n                     mean=0.0,\n                     std=std,\n                     a=-3 * std,\n                     b=3 * std,\n                 )\n                 if getattr(o_proj, \"bias\", None) is not None:\n-                    nn.init.zeros_(o_proj.bias)\n+                    init.zeros_(o_proj.bias)\n             return\n \n         # MLP / FFN blocks\n@@ -475,47 +476,55 @@ def _init_weights(self, module):\n             for proj in (gate_proj, up_proj):\n                 if proj is not None and hasattr(proj, \"weight\"):\n                     std = in_std or (proj.weight.shape[1] ** -0.5)\n-                    nn.init.trunc_normal_(\n+                    init.trunc_normal_(\n                         proj.weight,\n                         mean=0.0,\n                         std=std,\n                         a=-3 * std,\n                         b=3 * std,\n                     )\n                     if getattr(proj, \"bias\", None) is not None:\n-                        nn.init.zeros_(proj.bias)\n+                        init.zeros_(proj.bias)\n \n             # output/ down projections\n             if down_proj is not None and hasattr(down_proj, \"weight\"):\n                 hidden_dim = down_proj.weight.shape[1]\n                 out_std = hidden_dim**-0.5\n-                nn.init.trunc_normal_(\n+                init.trunc_normal_(\n                     down_proj.weight,\n                     mean=0.0,\n                     std=out_std,\n                     a=-3 * out_std,\n                     b=3 * out_std,\n                 )\n                 if getattr(down_proj, \"bias\", None) is not None:\n-                    nn.init.zeros_(down_proj.bias)\n+                    init.zeros_(down_proj.bias)\n             return\n \n         # Generic Linear layers (projections, lm_head, etc.)\n         if isinstance(module, nn.Linear):\n             fan_in = module.in_features\n             std = fan_in**-0.5\n-            nn.init.trunc_normal_(\n+            init.trunc_normal_(\n                 module.weight,\n                 mean=0.0,\n                 std=std,\n                 a=-3 * std,\n                 b=3 * std,\n             )\n             if module.bias is not None:\n-                nn.init.zeros_(module.bias)\n+                init.zeros_(module.bias)\n             return\n \n-        return\n+        if isinstance(module, BltRotaryEmbedding):\n+            rope_fn = (\n+                ROPE_INIT_FUNCTIONS[module.rope_type]\n+                if module.rope_type != \"default\"\n+                else module.compute_default_rope_parameters\n+            )\n+            buffer_value, _ = rope_fn(module.config)\n+            init.copy_(module.inv_freq, buffer_value)\n+            init.copy_(module.original_inv_freq, buffer_value)\n \n     def _update_causal_mask(self, module):\n         raise AttributeError(\"No need to inherit it!\")"
        },
        {
            "sha": "afaaade66b4f7cf43adcaf26b0253f4eff4f7e3d",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -943,6 +943,11 @@ def _init_weights(self, module: nn.Module):\n             init.ones_(module.weight)\n         elif isinstance(module, BridgeTowerForContrastiveLearning):\n             init.constant_(module.logit_scale, self.config.logit_scale_init_value)\n+        elif isinstance(module, BridgeTowerVisionEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.num_positions).expand((1, -1)))\n+        elif isinstance(module, BridgeTowerTextEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n \n         if isinstance(module, (nn.Linear, BridgeTowerMLMHead)) and module.bias is not None:\n             init.zeros_(module.bias)"
        },
        {
            "sha": "c449b5fc6083afd51fae18ee25b46eb9ebe88e17",
            "filename": "src/transformers/models/bros/modeling_bros.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -522,6 +522,14 @@ def _init_weights(self, module: nn.Module):\n         std = self.config.initializer_range\n         if isinstance(module, BrosRelationExtractor):\n             init.normal_(module.dummy_node, std=std)\n+        elif isinstance(module, BrosTextEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n+        elif isinstance(module, BrosPositionalEmbedding1D):\n+            inv_freq = 1 / (\n+                10000 ** (torch.arange(0.0, module.dim_bbox_sinusoid_emb_1d, 2.0) / module.dim_bbox_sinusoid_emb_1d)\n+            )\n+            init.copy_(module.inv_freq, inv_freq)\n \n \n @auto_docstring"
        },
        {
            "sha": "a1df2cb2c5dd76a9a3d50786f86078b8008a45f0",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 109,
            "deletions": 106,
            "changes": 215,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -54,6 +54,112 @@\n logger = logging.get_logger(__name__)\n \n \n+class CamembertEmbeddings(nn.Module):\n+    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n+        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n+\n+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n+        self.register_buffer(\n+            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n+        )\n+        self.register_buffer(\n+            \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n+        )\n+\n+        self.padding_idx = config.pad_token_id\n+        self.position_embeddings = nn.Embedding(\n+            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n+        )\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        past_key_values_length: int = 0,\n+    ) -> torch.Tensor:\n+        if position_ids is None:\n+            if input_ids is not None:\n+                # Create the position ids from the input token ids. Any padded tokens remain padded.\n+                position_ids = self.create_position_ids_from_input_ids(\n+                    input_ids, self.padding_idx, past_key_values_length\n+                )\n+            else:\n+                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds, self.padding_idx)\n+\n+        if input_ids is not None:\n+            input_shape = input_ids.size()\n+        else:\n+            input_shape = inputs_embeds.size()[:-1]\n+\n+        batch_size, seq_length = input_shape\n+\n+        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n+        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n+        # issue #5664\n+        if token_type_ids is None:\n+            if hasattr(self, \"token_type_ids\"):\n+                # NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\n+                buffered_token_type_ids = self.token_type_ids.expand(position_ids.shape[0], -1)\n+                buffered_token_type_ids = torch.gather(buffered_token_type_ids, dim=1, index=position_ids)\n+                token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n+            else:\n+                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.word_embeddings(input_ids)\n+        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n+        embeddings = inputs_embeds + token_type_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n+\n+        embeddings = self.LayerNorm(embeddings)\n+        embeddings = self.dropout(embeddings)\n+        return embeddings\n+\n+    @staticmethod\n+    def create_position_ids_from_inputs_embeds(inputs_embeds, padding_idx):\n+        \"\"\"\n+        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n+\n+        Args:\n+            inputs_embeds: torch.Tensor\n+\n+        Returns: torch.Tensor\n+        \"\"\"\n+        input_shape = inputs_embeds.size()[:-1]\n+        sequence_length = input_shape[1]\n+\n+        position_ids = torch.arange(\n+            padding_idx + 1, sequence_length + padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n+        )\n+        return position_ids.unsqueeze(0).expand(input_shape)\n+\n+    @staticmethod\n+    def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n+        \"\"\"\n+        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n+        are ignored. This is modified from fairseq's `utils.make_positions`.\n+\n+        Args:\n+            x: torch.Tensor x:\n+\n+        Returns: torch.Tensor\n+        \"\"\"\n+        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n+        mask = input_ids.ne(padding_idx).int()\n+        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n+        return incremental_indices.long() + padding_idx\n+\n+\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -417,112 +523,9 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, CamembertLMHead):\n             init.zeros_(module.bias)\n-\n-\n-class CamembertEmbeddings(nn.Module):\n-    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n-        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n-\n-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.register_buffer(\n-            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n-        )\n-        self.register_buffer(\n-            \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n-        )\n-\n-        self.padding_idx = config.pad_token_id\n-        self.position_embeddings = nn.Embedding(\n-            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n-        )\n-\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values_length: int = 0,\n-    ) -> torch.Tensor:\n-        if position_ids is None:\n-            if input_ids is not None:\n-                # Create the position ids from the input token ids. Any padded tokens remain padded.\n-                position_ids = self.create_position_ids_from_input_ids(\n-                    input_ids, self.padding_idx, past_key_values_length\n-                )\n-            else:\n-                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds, self.padding_idx)\n-\n-        if input_ids is not None:\n-            input_shape = input_ids.size()\n-        else:\n-            input_shape = inputs_embeds.size()[:-1]\n-\n-        batch_size, seq_length = input_shape\n-\n-        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n-        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n-        # issue #5664\n-        if token_type_ids is None:\n-            if hasattr(self, \"token_type_ids\"):\n-                # NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\n-                buffered_token_type_ids = self.token_type_ids.expand(position_ids.shape[0], -1)\n-                buffered_token_type_ids = torch.gather(buffered_token_type_ids, dim=1, index=position_ids)\n-                token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n-            else:\n-                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.word_embeddings(input_ids)\n-        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-        embeddings = inputs_embeds + token_type_embeddings\n-\n-        position_embeddings = self.position_embeddings(position_ids)\n-        embeddings = embeddings + position_embeddings\n-\n-        embeddings = self.LayerNorm(embeddings)\n-        embeddings = self.dropout(embeddings)\n-        return embeddings\n-\n-    @staticmethod\n-    def create_position_ids_from_inputs_embeds(inputs_embeds, padding_idx):\n-        \"\"\"\n-        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n-\n-        Args:\n-            inputs_embeds: torch.Tensor\n-\n-        Returns: torch.Tensor\n-        \"\"\"\n-        input_shape = inputs_embeds.size()[:-1]\n-        sequence_length = input_shape[1]\n-\n-        position_ids = torch.arange(\n-            padding_idx + 1, sequence_length + padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n-        )\n-        return position_ids.unsqueeze(0).expand(input_shape)\n-\n-    @staticmethod\n-    def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n-        \"\"\"\n-        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n-        are ignored. This is modified from fairseq's `utils.make_positions`.\n-\n-        Args:\n-            x: torch.Tensor x:\n-\n-        Returns: torch.Tensor\n-        \"\"\"\n-        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n-        mask = input_ids.ne(padding_idx).int()\n-        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n-        return incremental_indices.long() + padding_idx\n+        elif isinstance(module, CamembertEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n \n \n class CamembertEncoder(nn.Module):"
        },
        {
            "sha": "d44c3ff6811c549bf3d8ad3f8757ac4b7d8c726f",
            "filename": "src/transformers/models/canine/modeling_canine.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -719,6 +720,11 @@ class CaninePreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"canine\"\n     supports_gradient_checkpointing = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, CanineEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+\n \n @auto_docstring\n class CanineModel(CaninePreTrainedModel):"
        },
        {
            "sha": "e5607d4133409b623436970af5279f975dee731f",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -84,7 +84,7 @@ def __init__(self, config: ChameleonConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "fe5d0d5d41d3fe21d52722e0dd113a5258653163",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -572,10 +572,13 @@ def _init_weights(self, module):\n             init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n             init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n             init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.copy_(module.position_ids, torch.arange(module.num_positions).expand((1, -1)))\n         elif isinstance(module, ChineseCLIPTextEmbeddings):\n             init.normal_(module.word_embeddings.weight, mean=0.0, std=self.config.initializer_range)\n             init.normal_(module.position_embeddings.weight, mean=0.0, std=self.config.initializer_range)\n             init.normal_(module.token_type_embeddings.weight, mean=0.0, std=self.config.initializer_range)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n             for embedding in [module.word_embeddings, module.position_embeddings, module.token_type_embeddings]:\n                 if embedding.padding_idx is not None:\n                     init.zeros_(embedding.weight[embedding.padding_idx])"
        },
        {
            "sha": "7a846f647b8debb0ed2e34bfc55958a52778e8a6",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 22,
            "deletions": 12,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -365,18 +365,7 @@ def __init__(self, config, dim, num_heads, window_size):\n             torch.zeros((2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1), num_heads)\n         )\n \n-        # get pair-wise relative position index for each token inside the window\n-        coords_h = torch.arange(self.window_size[0])\n-        coords_w = torch.arange(self.window_size[1])\n-        coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))\n-        coords_flatten = torch.flatten(coords, 1)\n-        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n-        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n-        relative_coords[:, :, 0] += self.window_size[0] - 1\n-        relative_coords[:, :, 1] += self.window_size[1] - 1\n-        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n-        relative_position_index = relative_coords.sum(-1)\n-        self.register_buffer(\"relative_position_index\", relative_position_index)\n+        self.register_buffer(\"relative_position_index\", self.create_relative_position_index())\n \n         self.query = nn.Linear(self.all_head_size, self.all_head_size, bias=config.qkv_bias)\n         self.key = nn.Linear(self.all_head_size, self.all_head_size, bias=config.qkv_bias)\n@@ -435,6 +424,20 @@ def forward(\n \n         return outputs\n \n+    def create_relative_position_index(self):\n+        # get pair-wise relative position index for each token inside the window\n+        coords_h = torch.arange(self.window_size[0])\n+        coords_w = torch.arange(self.window_size[1])\n+        coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))\n+        coords_flatten = torch.flatten(coords, 1)\n+        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n+        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n+        relative_coords[:, :, 0] += self.window_size[0] - 1\n+        relative_coords[:, :, 1] += self.window_size[1] - 1\n+        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n+        relative_position_index = relative_coords.sum(-1)\n+        return relative_position_index\n+\n \n # Copied from transformers.models.swin.modeling_swin.SwinSelfOutput with Swin->ClapAudio\n class ClapAudioSelfOutput(nn.Module):\n@@ -1317,6 +1320,8 @@ def _init_weights(self, module: nn.Module):\n         if isinstance(module, ClapTextEmbeddings):\n             init.normal_(module.position_embeddings.weight, mean=0.0, std=factor * 0.02)\n             init.normal_(module.token_type_embeddings.weight, mean=0.0, std=factor * 0.02)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n         elif isinstance(module, ClapModel):\n             init.constant_(module.logit_scale_a, math.log(self.config.logit_scale_init_value))\n             init.constant_(module.logit_scale_t, math.log(self.config.logit_scale_init_value))\n@@ -1325,13 +1330,18 @@ def _init_weights(self, module: nn.Module):\n         elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d)):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n         elif isinstance(module, (nn.Conv2d, nn.Linear)):\n             in_proj_std = (self.config.hidden_size**-0.5) * ((2 * self.config.num_hidden_layers) ** -0.5) * factor\n             init.normal_(module.weight, std=in_proj_std)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n         elif isinstance(module, ClapAudioSelfAttention):\n             init.zeros_(module.relative_position_bias_table)\n+            init.copy_(module.relative_position_index, module.create_relative_position_index())\n \n \n class ClapAudioModel(ClapPreTrainedModel):"
        },
        {
            "sha": "9b05cfdf3a53f6cae77166e8439021f8974f0e6f",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -416,11 +416,13 @@ def _init_weights(self, module):\n         if isinstance(module, CLIPTextEmbeddings):\n             init.normal_(module.token_embedding.weight, mean=0.0, std=factor * 0.02)\n             init.normal_(module.position_embedding.weight, mean=0.0, std=factor * 0.02)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         elif isinstance(module, CLIPVisionEmbeddings):\n             factor = self.config.initializer_factor\n             init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n             init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n             init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.copy_(module.position_ids, torch.arange(module.num_positions).expand((1, -1)))\n         elif isinstance(module, CLIPAttention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor"
        },
        {
            "sha": "518b8c44e30888ed65adbec53b1cbda68ed2c08c",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -435,11 +435,13 @@ def _init_weights(self, module):\n         if isinstance(module, CLIPSegTextEmbeddings):\n             init.normal_(module.token_embedding.weight, mean=0.0, std=factor * 0.02)\n             init.normal_(module.position_embedding.weight, mean=0.0, std=factor * 0.02)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         elif isinstance(module, CLIPSegVisionEmbeddings):\n             factor = self.config.initializer_factor\n             init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n             init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n             init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.copy_(module.position_ids, torch.arange(module.num_positions).expand((1, -1)))\n         elif isinstance(module, CLIPSegAttention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor"
        },
        {
            "sha": "7666d01971d07dad5732d447abf71217f7eaf827",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -814,7 +814,16 @@ def _init_weights(self, module: nn.Module):\n                     )\n         elif isinstance(module, ClvpModelForConditionalGeneration):\n             init.constant_(module.logit_scale, self.config.logit_scale_init_value)\n-\n+        elif isinstance(module, ClvpSelfAttention):\n+            if hasattr(module.config, \"max_position_embeddings\"):\n+                max_positions = module.config.max_position_embeddings\n+                bias = torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool))\n+                bias = bias.view(1, 1, max_positions, max_positions)\n+                init.copy_(module.bias, bias)\n+        elif isinstance(module, ClvpRotaryPositionalEmbedding):\n+            dim = max(self.config.projection_dim // (self.config.num_attention_heads * 2), 32)\n+            inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n+            init.copy_(module.inv_freq, inv_freq)\n         if isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)"
        },
        {
            "sha": "78dc612537d3c2ba100d3bf23dec70c346f07627",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -83,7 +83,7 @@ def __init__(self, config: CohereConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "b1720d52f06341a67d3c760536a60aa43331c3ef",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -57,7 +57,7 @@ def __init__(self, config: Cohere2Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "ec46bccd581bf38d0aad2d6975a647abd3baef09",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -984,7 +984,7 @@ def _init_weights(self, module):\n         elif isinstance(module, ConditionalDetrLearnedPositionEmbedding):\n             init.uniform_(module.row_embeddings.weight)\n             init.uniform_(module.column_embeddings.weight)\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n             init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n@@ -993,6 +993,9 @@ def _init_weights(self, module):\n             # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n             if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n                 init.zeros_(module.weight[module.padding_idx])\n+        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n \n \n # Copied from transformers.models.detr.modeling_detr.DetrEncoder with Detr->ConditionalDetr,DETR->ConditionalDETR"
        },
        {
            "sha": "f08987eb91f012944bd00f27747d4921902ef45b",
            "filename": "src/transformers/models/convbert/modeling_convbert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -118,6 +118,9 @@ def _init_weights(self, module):\n         elif isinstance(module, GroupedLinearLayer):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             init.zeros_(module.bias)\n+        elif isinstance(module, ConvBertEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n \n \n class SeparableConv1D(nn.Module):"
        },
        {
            "sha": "d651eaf0e0e0ebe290ce66857416bffc7245744c",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -136,7 +136,7 @@ def __init__(self, config: CsmConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -421,6 +421,8 @@ def _init_weights(self, module):\n             num_codebooks = module.num_codebooks\n             for i in range(num_codebooks - 1):\n                 init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, CsmBackboneModelEmbeddings):\n+            init.copy_(module.audio_tokens_offsets, torch.arange(self.config.num_codebooks) * self.config.vocab_size)\n \n \n @auto_docstring"
        },
        {
            "sha": "7195ce18509c3d50fe86bdec126ca97b5a89db66",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -149,6 +149,8 @@ def _init_weights(self, module):\n             num_codebooks = module.num_codebooks\n             for i in range(num_codebooks - 1):\n                 init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, CsmBackboneModelEmbeddings):\n+            init.copy_(module.audio_tokens_offsets, torch.arange(self.config.num_codebooks) * self.config.vocab_size)\n \n \n @auto_docstring"
        },
        {
            "sha": "35aa3f171b056e1db073be593b612970b9637b6f",
            "filename": "src/transformers/models/cvt/modeling_cvt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -497,9 +497,13 @@ def _init_weights(self, module):\n             init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n-        elif isinstance(module, nn.LayerNorm):\n+        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d)):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n         elif isinstance(module, CvtStage):\n             if self.config.cls_token[module.stage]:\n                 init.trunc_normal_(module.cls_token, mean=0.0, std=self.config.initializer_range)"
        },
        {
            "sha": "562fded466393315462ed237f9d21d8ce65430ff",
            "filename": "src/transformers/models/cwm/modeling_cwm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -58,7 +58,7 @@ def __init__(self, config: CwmConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "02a9015cfb89a5356e163dca56df2611e26d1b31",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 46,
            "deletions": 39,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -483,6 +483,9 @@ def _init_weights(self, module):\n             init.constant_(module.attention_weights.weight, 0.0)\n             init.constant_(module.attention_weights.bias, 0.0)\n \n+            num_points_scale = [1 / n for n in module.num_points_list for _ in range(n)]\n+            init.copy_(module.num_points_scale, torch.tensor(num_points_scale, dtype=torch.float32))\n+\n         if isinstance(module, DFineModel):\n             prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n             bias = float(-math.log((1 - prior_prob) / prior_prob))\n@@ -493,6 +496,10 @@ def _init_weights(self, module):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n \n         if isinstance(module, DFineGate):\n             bias = float(-math.log((1 - 0.5) / 0.5))\n@@ -838,6 +845,45 @@ def forward(\n         )\n \n \n+class DFineFrozenBatchNorm2d(nn.Module):\n+    \"\"\"\n+    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n+\n+    Copy-paste from torchvision.misc.ops with added eps before rqsrt, without which any other models than\n+    torchvision.models.resnet[18,34,50,101] produce nans.\n+    \"\"\"\n+\n+    def __init__(self, n):\n+        super().__init__()\n+        self.register_buffer(\"weight\", torch.ones(n))\n+        self.register_buffer(\"bias\", torch.zeros(n))\n+        self.register_buffer(\"running_mean\", torch.zeros(n))\n+        self.register_buffer(\"running_var\", torch.ones(n))\n+\n+    def _load_from_state_dict(\n+        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n+    ):\n+        num_batches_tracked_key = prefix + \"num_batches_tracked\"\n+        if num_batches_tracked_key in state_dict:\n+            del state_dict[num_batches_tracked_key]\n+\n+        super()._load_from_state_dict(\n+            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n+        )\n+\n+    def forward(self, x):\n+        # move reshapes to the beginning\n+        # to make it user-friendly\n+        weight = self.weight.reshape(1, -1, 1, 1)\n+        bias = self.bias.reshape(1, -1, 1, 1)\n+        running_var = self.running_var.reshape(1, -1, 1, 1)\n+        running_mean = self.running_mean.reshape(1, -1, 1, 1)\n+        epsilon = 1e-5\n+        scale = weight * (running_var + epsilon).rsqrt()\n+        bias = bias - running_mean * scale\n+        return x * scale + bias\n+\n+\n @dataclass\n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -896,45 +942,6 @@ class DFineModelOutput(ModelOutput):\n     denoising_meta_values: Optional[dict] = None\n \n \n-class DFineFrozenBatchNorm2d(nn.Module):\n-    \"\"\"\n-    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n-\n-    Copy-paste from torchvision.misc.ops with added eps before rqsrt, without which any other models than\n-    torchvision.models.resnet[18,34,50,101] produce nans.\n-    \"\"\"\n-\n-    def __init__(self, n):\n-        super().__init__()\n-        self.register_buffer(\"weight\", torch.ones(n))\n-        self.register_buffer(\"bias\", torch.zeros(n))\n-        self.register_buffer(\"running_mean\", torch.zeros(n))\n-        self.register_buffer(\"running_var\", torch.ones(n))\n-\n-    def _load_from_state_dict(\n-        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n-    ):\n-        num_batches_tracked_key = prefix + \"num_batches_tracked\"\n-        if num_batches_tracked_key in state_dict:\n-            del state_dict[num_batches_tracked_key]\n-\n-        super()._load_from_state_dict(\n-            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n-        )\n-\n-    def forward(self, x):\n-        # move reshapes to the beginning\n-        # to make it user-friendly\n-        weight = self.weight.reshape(1, -1, 1, 1)\n-        bias = self.bias.reshape(1, -1, 1, 1)\n-        running_var = self.running_var.reshape(1, -1, 1, 1)\n-        running_mean = self.running_mean.reshape(1, -1, 1, 1)\n-        epsilon = 1e-5\n-        scale = weight * (running_var + epsilon).rsqrt()\n-        bias = bias - running_mean * scale\n-        return x * scale + bias\n-\n-\n def replace_batch_norm(model):\n     r\"\"\"\n     Recursively replace all `torch.nn.BatchNorm2d` with `DFineFrozenBatchNorm2d`."
        },
        {
            "sha": "8f79eb85286a088731f961ecc323c0050a8488e4",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -33,6 +33,7 @@\n     RTDetrDecoderOutput,\n     RTDetrEncoder,\n     RTDetrForObjectDetection,\n+    RTDetrFrozenBatchNorm2d,\n     RTDetrHybridEncoder,\n     RTDetrMLPPredictionHead,\n     RTDetrModel,\n@@ -628,6 +629,9 @@ def _init_weights(self, module):\n             init.constant_(module.attention_weights.weight, 0.0)\n             init.constant_(module.attention_weights.bias, 0.0)\n \n+            num_points_scale = [1 / n for n in module.num_points_list for _ in range(n)]\n+            init.copy_(module.num_points_scale, torch.tensor(num_points_scale, dtype=torch.float32))\n+\n         if isinstance(module, DFineModel):\n             prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n             bias = float(-math.log((1 - prior_prob) / prior_prob))\n@@ -638,6 +642,10 @@ def _init_weights(self, module):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n \n         if isinstance(module, DFineGate):\n             bias = float(-math.log((1 - 0.5) / 0.5))\n@@ -851,6 +859,10 @@ def forward(\n         )\n \n \n+class DFineFrozenBatchNorm2d(RTDetrFrozenBatchNorm2d):\n+    pass\n+\n+\n class DFineModel(RTDetrModel):\n     def __init__(self, config: DFineConfig):\n         super().__init__(config)"
        },
        {
            "sha": "8d601e46eb70c63ab2ff41eaa358713133d82cf5",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -826,7 +826,7 @@ def _init_weights(self, module):\n             init.zeros_(module.q_linear.bias)\n             init.xavier_uniform_(module.k_linear.weight, gain=xavier_std)\n             init.xavier_uniform_(module.q_linear.weight, gain=xavier_std)\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n             init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n                 init.zeros_(module.bias)"
        },
        {
            "sha": "b73d51633b1b2dd4a8ceb78b008d3710737cf21e",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -26,6 +26,7 @@\n import torch.nn as nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN, gelu\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -494,6 +495,12 @@ class Data2VecTextPreTrainedModel(PreTrainedModel):\n         \"cross_attentions\": Data2VecTextCrossAttention,\n     }\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, Data2VecTextEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n+\n \n class Data2VecTextEncoder(nn.Module):\n     def __init__(self, config):"
        },
        {
            "sha": "4c1126f3db138bf7ced447f6ba7285f3e91f56a3",
            "filename": "src/transformers/models/data2vec/modular_data2vec_text.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -20,6 +20,7 @@\n import torch.nn as nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -81,6 +82,12 @@ class Data2VecTextPreTrainedModel(PreTrainedModel):\n         \"cross_attentions\": Data2VecTextCrossAttention,\n     }\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, Data2VecTextEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n+\n \n @auto_docstring\n class Data2VecTextModel(RobertaModel):"
        },
        {
            "sha": "ae34cde47395043508fd5bc7f2498918f64e2773",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -58,7 +58,7 @@ def __init__(self, config: DbrxConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "422d380cf3b3992acbf39c2108b7a2a436ca2483",
            "filename": "src/transformers/models/deberta/modeling_deberta.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -624,6 +624,8 @@ def _init_weights(self, module):\n             init.zeros_(module.v_bias)\n         elif isinstance(module, (LegacyDebertaLMPredictionHead, DebertaLMPredictionHead)):\n             init.zeros_(module.bias)\n+        elif isinstance(module, DebertaEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n @auto_docstring"
        },
        {
            "sha": "222b9fb7e5a898e162ec442cf412793f6e4e20dc",
            "filename": "src/transformers/models/deberta_v2/modeling_deberta_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -700,6 +700,8 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, (LegacyDebertaV2LMPredictionHead, DebertaV2LMPredictionHead)):\n             init.zeros_(module.bias)\n+        elif isinstance(module, DebertaV2Embeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n @auto_docstring"
        },
        {
            "sha": "ee82fd76f8d9dfff39e3771f7b8d6187c02f72e4",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -94,7 +94,6 @@ def __init__(self, config, is_cross_attention=False, layer_idx=None):\n             ),\n             persistent=False,\n         )\n-        self.register_buffer(\"masked_bias\", torch.tensor(-1e4), persistent=False)\n \n         self.embed_dim = config.hidden_size\n         self.num_heads = config.num_attention_heads\n@@ -385,6 +384,14 @@ def _init_weights(self, module):\n                 if \"c_proj\" in name and \"weight\" in name:\n                     # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n                     init.normal_(p, mean=0.0, std=self.config.initializer_range / math.sqrt(2 * self.config.n_layer))\n+        elif isinstance(module, DecisionTransformerGPT2Attention):\n+            max_positions = module.config.max_position_embeddings\n+            init.copy_(\n+                module.bias,\n+                torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(\n+                    1, 1, max_positions, max_positions\n+                ),\n+            )\n \n \n class DecisionTransformerGPT2Model(DecisionTransformerGPT2PreTrainedModel):"
        },
        {
            "sha": "f01a136553311c0cf27d410b84ec952b25940f34",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -184,7 +184,7 @@ def __init__(self, config: DeepseekV2Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "c6c0a91fd8f4454c204cba1cd67b0ba75632e4f1",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -71,7 +71,7 @@ def __init__(self, config: DeepseekV3Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -555,6 +555,7 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, DeepseekV3TopkRouter):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            init.zeros_(module.e_score_correction_bias)\n         elif isinstance(module, DeepseekV3NaiveMoe):\n             init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n             init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)"
        },
        {
            "sha": "62792968cd5f4c913709928ed647aca44677c8a2",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -312,6 +312,7 @@ def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, DeepseekV3TopkRouter):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            init.zeros_(module.e_score_correction_bias)\n         elif isinstance(module, DeepseekV3NaiveMoe):\n             init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n             init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)"
        },
        {
            "sha": "11d4830b5a07eab0b966f8b7d871ec343535af12",
            "filename": "src/transformers/models/deepseek_vl/modular_deepseek_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -134,6 +134,9 @@ def forward(self, vision_encodings: torch.Tensor) -> torch.Tensor:\n class DeepseekVLPreTrainedModel(JanusPreTrainedModel):\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n \n+    def _init_weights(self, module):\n+        raise AttributeError(\"No need to inherit!\")\n+\n \n @auto_docstring\n class DeepseekVLModel(JanusModel):"
        },
        {
            "sha": "5ecabfafdde496fd71da65244d4a332e8302c9b1",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -956,7 +956,7 @@ def _init_weights(self, module):\n             init.constant_(module.value_proj.bias, 0.0)\n             init.xavier_uniform_(module.output_proj.weight)\n             init.constant_(module.output_proj.bias, 0.0)\n-        elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n+        elif isinstance(module, (nn.Linear, nn.Conv2d)):\n             init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n                 init.zeros_(module.bias)"
        },
        {
            "sha": "1197bfa22a70e392ab133b892f2beb2d697ed5dd",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -741,7 +741,7 @@ def _init_weights(self, module):\n         elif isinstance(module, DetrLearnedPositionEmbedding):\n             init.uniform_(module.row_embeddings.weight)\n             init.uniform_(module.column_embeddings.weight)\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n             init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n@@ -750,6 +750,9 @@ def _init_weights(self, module):\n             # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n             if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n                 init.zeros_(module.weight[module.padding_idx])\n+        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n \n \n class DetrEncoder(DetrPreTrainedModel):"
        },
        {
            "sha": "ba328ddb3e076974ec83be5184789a97ac53bc8e",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -25,6 +25,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n@@ -61,6 +62,12 @@ class DiaPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_ids\"\n     _no_split_modules = [\"DiaEncoderLayer\", \"DiaDecoderLayer\"]\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, DiaMultiChannelEmbedding):\n+            offsets = torch.arange(self.config.num_channels, dtype=torch.long) * self.config.vocab_size\n+            init.copy_(module.offsets, offsets)\n+\n \n class DiaMultiChannelEmbedding(nn.Module):\n     \"\"\"In order to efficiently compute the audio embedding from the 9 different channels,\n@@ -146,7 +153,7 @@ def __init__(self, config: DiaConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "49a8fe05afaf14983cf97edf35c12bc293aef83b",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -20,6 +20,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...cache_utils import DynamicCache, EncoderDecoderCache\n from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -59,6 +60,12 @@ class DiaPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_ids\"\n     _no_split_modules = [\"DiaEncoderLayer\", \"DiaDecoderLayer\"]\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, DiaMultiChannelEmbedding):\n+            offsets = torch.arange(self.config.num_channels, dtype=torch.long) * self.config.vocab_size\n+            init.copy_(module.offsets, offsets)\n+\n \n class DiaMultiChannelEmbedding(nn.Module):\n     \"\"\"In order to efficiently compute the audio embedding from the 9 different channels,"
        },
        {
            "sha": "44a266d5951eb96346a12b9d37e94cbe2560a8ba",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -86,7 +86,7 @@ def __init__(self, config: DiffLlamaConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "06bb52fdad3ca30d9a04700b4c931ab993b47795",
            "filename": "src/transformers/models/dinov3_vit/modeling_dinov3_vit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -466,6 +466,9 @@ def _init_weights(self, module) -> None:\n             init.zeros_(module.mask_token)\n         elif isinstance(module, DINOv3ViTLayerScale):\n             init.constant_(module.lambda1, self.config.layerscale_value)\n+        elif isinstance(module, DINOv3ViTRopePositionEmbedding):\n+            inv_freq = 1 / module.base ** torch.arange(0, 1, 4 / module.head_dim, dtype=torch.float32)\n+            init.copy_(module.inv_freq, inv_freq)\n \n \n @auto_docstring"
        },
        {
            "sha": "63bb46c2f05242d07c5dc6dee20b2edc592fb2ff",
            "filename": "src/transformers/models/dinov3_vit/modular_dinov3_vit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -361,6 +361,9 @@ def _init_weights(self, module) -> None:\n             init.zeros_(module.mask_token)\n         elif isinstance(module, DINOv3ViTLayerScale):\n             init.constant_(module.lambda1, self.config.layerscale_value)\n+        elif isinstance(module, DINOv3ViTRopePositionEmbedding):\n+            inv_freq = 1 / module.base ** torch.arange(0, 1, 4 / module.head_dim, dtype=torch.float32)\n+            init.copy_(module.inv_freq, inv_freq)\n \n \n @auto_docstring"
        },
        {
            "sha": "4579f46e3a44f0cb50404277d1b245d83971a450",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -305,15 +305,17 @@ class DistilBertPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights.\"\"\"\n         super()._init_weights(module)\n-        if isinstance(module, Embeddings) and self.config.sinusoidal_pos_embds:\n-            init.copy_(\n-                module.position_embeddings.weight,\n-                create_sinusoidal_embeddings(\n-                    self.config.max_position_embeddings,\n-                    self.config.dim,\n-                    torch.empty_like(module.position_embeddings.weight),\n-                ),\n-            )\n+        if isinstance(module, Embeddings):\n+            if self.config.sinusoidal_pos_embds:\n+                init.copy_(\n+                    module.position_embeddings.weight,\n+                    create_sinusoidal_embeddings(\n+                        self.config.max_position_embeddings,\n+                        self.config.dim,\n+                        torch.empty_like(module.position_embeddings.weight),\n+                    ),\n+                )\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n @auto_docstring"
        },
        {
            "sha": "bbebcf0773576eefdd03cb42c14b9da7aeb26a72",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -88,7 +88,7 @@ def __init__(self, config: DogeConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "7e7490f89a7cd44bf1e23008a9cdde8ab9df1bc0",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 16,
            "deletions": 12,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -381,18 +381,7 @@ def __init__(self, config, dim, num_heads, window_size):\n             torch.zeros((2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1), num_heads)\n         )\n \n-        # get pair-wise relative position index for each token inside the window\n-        coords_h = torch.arange(self.window_size[0])\n-        coords_w = torch.arange(self.window_size[1])\n-        coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))\n-        coords_flatten = torch.flatten(coords, 1)\n-        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n-        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n-        relative_coords[:, :, 0] += self.window_size[0] - 1\n-        relative_coords[:, :, 1] += self.window_size[1] - 1\n-        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n-        relative_position_index = relative_coords.sum(-1)\n-        self.register_buffer(\"relative_position_index\", relative_position_index)\n+        self.register_buffer(\"relative_position_index\", self.create_relative_position_index())\n \n         self.query = nn.Linear(self.all_head_size, self.all_head_size, bias=config.qkv_bias)\n         self.key = nn.Linear(self.all_head_size, self.all_head_size, bias=config.qkv_bias)\n@@ -451,6 +440,20 @@ def forward(\n \n         return outputs\n \n+    def create_relative_position_index(self):\n+        # get pair-wise relative position index for each token inside the window\n+        coords_h = torch.arange(self.window_size[0])\n+        coords_w = torch.arange(self.window_size[1])\n+        coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))\n+        coords_flatten = torch.flatten(coords, 1)\n+        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n+        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n+        relative_coords[:, :, 0] += self.window_size[0] - 1\n+        relative_coords[:, :, 1] += self.window_size[1] - 1\n+        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n+        relative_position_index = relative_coords.sum(-1)\n+        return relative_position_index\n+\n \n # Copied from transformers.models.swin.modeling_swin.SwinSelfOutput\n class DonutSwinSelfOutput(nn.Module):\n@@ -801,6 +804,7 @@ def _init_weights(self, module):\n                 init.zeros_(module.position_embeddings)\n         elif isinstance(module, DonutSwinSelfAttention):\n             init.zeros_(module.relative_position_bias_table)\n+            init.copy_(module.relative_position_index, module.create_relative_position_index())\n \n \n @auto_docstring"
        },
        {
            "sha": "3b25846adfaf7dad91d3efed24054b19c9ac66c1",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -80,7 +80,7 @@ def __init__(self, config: Dots1Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -476,6 +476,7 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Dots1TopkRouter):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            init.zeros_(module.e_score_correction_bias)\n         elif isinstance(module, Dots1NaiveMoe):\n             init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n             init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)"
        },
        {
            "sha": "bf05f0030551ad36d0a1d8418c0f5670a423da40",
            "filename": "src/transformers/models/edgetam/modeling_edgetam.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -30,7 +30,7 @@\n import torch.nn.functional as F\n from torch import Tensor\n \n-from transformers.utils.generic import OutputRecorder, TransformersKwargs, check_model_inputs\n+from transformers.utils.generic import OutputRecorder\n \n from ... import initialization as init\n from ...activations import ACT2FN\n@@ -39,6 +39,7 @@\n from ...processing_utils import Unpack\n from ...pytorch_utils import compile_compatible_method_lru_cache\n from ...utils import ModelOutput, auto_docstring\n+from ...utils.generic import TransformersKwargs, check_model_inputs\n from ..auto import AutoModel\n from .configuration_edgetam import (\n     EdgeTamConfig,\n@@ -50,7 +51,7 @@\n \n # fix this in modular\n if True:\n-    from transformers.models.timm_wrapper.modeling_timm_wrapper import TimmWrapperModel\n+    from ..timm_wrapper.modeling_timm_wrapper import TimmWrapperModel\n \n \n class EdgeTamLayerNorm(nn.LayerNorm):\n@@ -315,6 +316,8 @@ def _init_weights(self, module):\n         if isinstance(module, EdgeTamModel):\n             if module.no_memory_embedding is not None:\n                 init.zeros_(module.no_memory_embedding)\n+        elif hasattr(module, \"positional_embedding\"):\n+            init.normal_(module.positional_embedding, std=module.scale)\n \n \n # copied and adapted from original implementation, also practically equal to DetrSinePositionEmbedding"
        },
        {
            "sha": "948ce074ae2fd04442e6583bd2c8d41f9c83487e",
            "filename": "src/transformers/models/edgetam/modular_edgetam.py",
            "status": "modified",
            "additions": 14,
            "deletions": 13,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -19,8 +19,17 @@\n import torch\n import torch.utils.checkpoint\n \n-from transformers.models.sam2.configuration_sam2 import Sam2Config, Sam2MaskDecoderConfig, Sam2PromptEncoderConfig\n-from transformers.models.sam2.modeling_sam2 import (\n+from ... import initialization as init\n+from ...configuration_utils import PreTrainedConfig\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    auto_docstring,\n+)\n+from ...utils.generic import TransformersKwargs, check_model_inputs\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+from ..sam2.configuration_sam2 import Sam2Config, Sam2MaskDecoderConfig, Sam2PromptEncoderConfig\n+from ..sam2.modeling_sam2 import (\n     Sam2Attention,\n     Sam2FeedForward,\n     Sam2LayerNorm,\n@@ -30,21 +39,11 @@\n     Sam2VisionEncoderOutput,\n     Sam2VisionModel,\n )\n-from transformers.utils.generic import TransformersKwargs, check_model_inputs\n-\n-from ... import initialization as init\n-from ...configuration_utils import PreTrainedConfig\n-from ...modeling_utils import PreTrainedModel\n-from ...processing_utils import Unpack\n-from ...utils import (\n-    auto_docstring,\n-)\n-from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n # fix this in modular\n if True:\n-    from transformers.models.timm_wrapper.modeling_timm_wrapper import TimmWrapperModel\n+    from ..timm_wrapper.modeling_timm_wrapper import TimmWrapperModel\n \n \n class EdgeTamVisionConfig(PreTrainedConfig):\n@@ -181,6 +180,8 @@ def _init_weights(self, module):\n         if isinstance(module, EdgeTamModel):\n             if module.no_memory_embedding is not None:\n                 init.zeros_(module.no_memory_embedding)\n+        elif hasattr(module, \"positional_embedding\"):\n+            init.normal_(module.positional_embedding, std=module.scale)\n \n \n @auto_docstring("
        },
        {
            "sha": "4899382436a3300ef08e472193102f13985f5df6",
            "filename": "src/transformers/models/edgetam_video/modeling_edgetam_video.py",
            "status": "modified",
            "additions": 54,
            "deletions": 37,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -152,24 +152,17 @@ class EdgeTamVideoVisionRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: EdgeTamVideoConfig, end_x: Optional[int] = None, end_y: Optional[int] = None):\n         super().__init__()\n-        dim = config.memory_attention_hidden_size // (\n+        self.dim = config.memory_attention_hidden_size // (\n             config.memory_attention_downsample_rate * config.memory_attention_num_attention_heads\n         )\n         # Ensure even dimension for proper axial splitting\n-        if dim % 4 != 0:\n+        if self.dim % 4 != 0:\n             raise ValueError(\"Dimension must be divisible by 4 for axial RoPE\")\n-        end_x, end_y = config.memory_attention_rope_feat_sizes if end_x is None else (end_x, end_y)\n-        freqs = 1.0 / (config.memory_attention_rope_theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n+        self.end_x, self.end_y = config.memory_attention_rope_feat_sizes if end_x is None else (end_x, end_y)\n+        self.memory_attention_rope_theta = config.memory_attention_rope_theta\n \n-        # Generate 2D position indices for axial rotary embedding\n-        flattened_indices = torch.arange(end_x * end_y, dtype=torch.long)\n-        x_positions = flattened_indices % end_x\n-        y_positions = torch.div(flattened_indices, end_x, rounding_mode=\"floor\")\n-        freqs_x = torch.outer(x_positions, freqs).float()\n-        freqs_y = torch.outer(y_positions, freqs).float()\n-        inv_freq = torch.cat([freqs_x, freqs_y], dim=-1)\n-        inv_freq = inv_freq.repeat_interleave(2, dim=-1)\n         # directly register the cos and sin embeddings as we have a fixed feature shape\n+        inv_freq = self.create_inv_freq()\n         self.register_buffer(\"rope_embeddings_cos\", inv_freq.cos(), persistent=False)\n         self.register_buffer(\"rope_embeddings_sin\", inv_freq.sin(), persistent=False)\n \n@@ -178,6 +171,20 @@ def forward(self) -> tuple[torch.Tensor, torch.Tensor]:\n         # As the feature map size is fixed, we can just return the pre-computed embeddings.\n         return self.rope_embeddings_cos, self.rope_embeddings_sin\n \n+    def create_inv_freq(self):\n+        freqs = 1.0 / (\n+            self.memory_attention_rope_theta ** (torch.arange(0, self.dim, 4)[: (self.dim // 4)].float() / self.dim)\n+        )\n+        # Generate 2D position indices for axial rotary embedding\n+        flattened_indices = torch.arange(self.end_x * self.end_y, dtype=torch.long)\n+        x_positions = flattened_indices % self.end_x\n+        y_positions = torch.div(flattened_indices, self.end_x, rounding_mode=\"floor\")\n+        freqs_x = torch.outer(x_positions, freqs).float()\n+        freqs_y = torch.outer(y_positions, freqs).float()\n+        inv_freq = torch.cat([freqs_x, freqs_y], dim=-1)\n+        inv_freq = inv_freq.repeat_interleave(2, dim=-1)\n+        return inv_freq\n+\n \n def eager_attention_forward(\n     module: nn.Module,\n@@ -769,6 +776,31 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n+class EdgeTamVideoPositionalEmbedding(nn.Module):\n+    def __init__(self, config: EdgeTamVideoPromptEncoderConfig):\n+        super().__init__()\n+        self.scale = config.scale\n+        positional_embedding = self.scale * torch.randn((2, config.hidden_size // 2))\n+        self.register_buffer(\"positional_embedding\", positional_embedding)\n+\n+    def forward(self, input_coords, input_shape=None):\n+        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n+        coordinates = input_coords.clone()\n+\n+        if input_shape is not None:\n+            coordinates[:, :, :, 0] = coordinates[:, :, :, 0] / input_shape[1]\n+            coordinates[:, :, :, 1] = coordinates[:, :, :, 1] / input_shape[0]\n+        coordinates.to(torch.float32)\n+\n+        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n+        coordinates = 2 * coordinates - 1\n+        coordinates = coordinates.to(self.positional_embedding.dtype)\n+        coordinates = coordinates @ self.positional_embedding\n+        coordinates = 2 * np.pi * coordinates\n+        # outputs d_1 x ... x d_n x channel shape\n+        return torch.cat([torch.sin(coordinates), torch.cos(coordinates)], dim=-1)\n+\n+\n @auto_docstring\n class EdgeTamVideoPreTrainedModel(PreTrainedModel):\n     config_class = EdgeTamVideoConfig\n@@ -794,6 +826,16 @@ def _init_weights(self, module):\n         if isinstance(module, EdgeTamVideoMemoryFuserCXBlock):\n             if module.scale is not None:\n                 init.zeros_(module.scale)\n+        elif isinstance(module, EdgeTamVideoVisionRotaryEmbedding):\n+            inv_freq = module.create_inv_freq()\n+            init.copy_(module.rope_embeddings_cos, inv_freq.cos())\n+            init.copy_(module.rope_embeddings_sin, inv_freq.sin())\n+        elif isinstance(module, EdgeTamVideoPositionalEmbedding):\n+            init.normal_(module.positional_embedding, std=module.scale)\n+        if isinstance(module, EdgeTamVideoVisionRotaryEmbedding):\n+            inv_freq = module.create_inv_freq()\n+            init.copy_(module.rope_embeddings_cos, inv_freq.cos())\n+            init.copy_(module.rope_embeddings_sin, inv_freq.sin())\n \n \n class EdgeTamVideoInferenceCache:\n@@ -1547,31 +1589,6 @@ class EdgeTamVideoSegmentationOutput(ModelOutput):\n     frame_idx: Optional[int] = None\n \n \n-class EdgeTamVideoPositionalEmbedding(nn.Module):\n-    def __init__(self, config: EdgeTamVideoPromptEncoderConfig):\n-        super().__init__()\n-        self.scale = config.scale\n-        positional_embedding = self.scale * torch.randn((2, config.hidden_size // 2))\n-        self.register_buffer(\"positional_embedding\", positional_embedding)\n-\n-    def forward(self, input_coords, input_shape=None):\n-        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n-        coordinates = input_coords.clone()\n-\n-        if input_shape is not None:\n-            coordinates[:, :, :, 0] = coordinates[:, :, :, 0] / input_shape[1]\n-            coordinates[:, :, :, 1] = coordinates[:, :, :, 1] / input_shape[0]\n-        coordinates.to(torch.float32)\n-\n-        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n-        coordinates = 2 * coordinates - 1\n-        coordinates = coordinates.to(self.positional_embedding.dtype)\n-        coordinates = coordinates @ self.positional_embedding\n-        coordinates = 2 * np.pi * coordinates\n-        # outputs d_1 x ... x d_n x channel shape\n-        return torch.cat([torch.sin(coordinates), torch.cos(coordinates)], dim=-1)\n-\n-\n class EdgeTamVideoMaskEmbedding(nn.Module):\n     def __init__(self, config: EdgeTamVideoPromptEncoderConfig):\n         super().__init__()"
        },
        {
            "sha": "a91c8bd44c4373e31be2829badffbacb12f08478",
            "filename": "src/transformers/models/edgetam_video/modular_edgetam_video.py",
            "status": "modified",
            "additions": 13,
            "deletions": 14,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodular_edgetam_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodular_edgetam_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodular_edgetam_video.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -29,6 +29,7 @@\n )\n from transformers.utils.generic import OutputRecorder\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...configuration_utils import PreTrainedConfig\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -375,24 +376,17 @@ class EdgeTamVideoVisionEncoderOutput(Sam2VideoVisionEncoderOutput):\n class EdgeTamVideoVisionRotaryEmbedding(Sam2VideoVisionRotaryEmbedding):\n     def __init__(self, config: EdgeTamVideoConfig, end_x: Optional[int] = None, end_y: Optional[int] = None):\n         nn.Module.__init__()\n-        dim = config.memory_attention_hidden_size // (\n+        self.dim = config.memory_attention_hidden_size // (\n             config.memory_attention_downsample_rate * config.memory_attention_num_attention_heads\n         )\n         # Ensure even dimension for proper axial splitting\n-        if dim % 4 != 0:\n+        if self.dim % 4 != 0:\n             raise ValueError(\"Dimension must be divisible by 4 for axial RoPE\")\n-        end_x, end_y = config.memory_attention_rope_feat_sizes if end_x is None else (end_x, end_y)\n-        freqs = 1.0 / (config.memory_attention_rope_theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n-\n-        # Generate 2D position indices for axial rotary embedding\n-        flattened_indices = torch.arange(end_x * end_y, dtype=torch.long)\n-        x_positions = flattened_indices % end_x\n-        y_positions = torch.div(flattened_indices, end_x, rounding_mode=\"floor\")\n-        freqs_x = torch.outer(x_positions, freqs).float()\n-        freqs_y = torch.outer(y_positions, freqs).float()\n-        inv_freq = torch.cat([freqs_x, freqs_y], dim=-1)\n-        inv_freq = inv_freq.repeat_interleave(2, dim=-1)\n+        self.end_x, self.end_y = config.memory_attention_rope_feat_sizes if end_x is None else (end_x, end_y)\n+        self.memory_attention_rope_theta = config.memory_attention_rope_theta\n+\n         # directly register the cos and sin embeddings as we have a fixed feature shape\n+        inv_freq = self.create_inv_freq()\n         self.register_buffer(\"rope_embeddings_cos\", inv_freq.cos(), persistent=False)\n         self.register_buffer(\"rope_embeddings_sin\", inv_freq.sin(), persistent=False)\n \n@@ -662,7 +656,12 @@ class EdgeTamVideoFeedForward(Sam2VideoFeedForward):\n \n \n class EdgeTamVideoPreTrainedModel(Sam2VideoPreTrainedModel):\n-    pass\n+    def _init_weights(self, module):\n+        super()._init_weights()\n+        if isinstance(module, EdgeTamVideoVisionRotaryEmbedding):\n+            inv_freq = module.create_inv_freq()\n+            init.copy_(module.rope_embeddings_cos, inv_freq.cos())\n+            init.copy_(module.rope_embeddings_sin, inv_freq.sin())\n \n \n class EdgeTamVideoInferenceSession(Sam2VideoInferenceSession):"
        },
        {
            "sha": "0c6b579a578752612a8bd36977488e2f522f63b9",
            "filename": "src/transformers/models/efficientloftr/modeling_efficientloftr.py",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -103,7 +103,7 @@ def __init__(self, config: EfficientLoFTRConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     # Ignore copy\n@@ -684,9 +684,22 @@ def _init_weights(self, module: nn.Module) -> None:\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n         elif isinstance(module, nn.LayerNorm):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+        elif isinstance(module, EfficientLoFTRRotaryEmbedding):\n+            rope_fn = (\n+                ROPE_INIT_FUNCTIONS[module.rope_type]\n+                if module.rope_type != \"default\"\n+                else module.compute_default_rope_parameters\n+            )\n+            buffer_value, _ = rope_fn(module.config)\n+            init.copy_(module.inv_freq, buffer_value)\n+            init.copy_(module.original_inv_freq, buffer_value)\n \n     # Copied from transformers.models.superpoint.modeling_superpoint.SuperPointPreTrainedModel.extract_one_channel_pixel_values with SuperPoint->EfficientLoFTR\n     def extract_one_channel_pixel_values(self, pixel_values: torch.FloatTensor) -> torch.FloatTensor:"
        },
        {
            "sha": "6dee1a3379ee679aefe5457658acb1559f2c029e",
            "filename": "src/transformers/models/efficientnet/modeling_efficientnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -444,6 +444,10 @@ def _init_weights(self, module: nn.Module):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n \n \n @auto_docstring"
        },
        {
            "sha": "2a74928535cad9fc77ad065e35596339e0ff004f",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN, get_activation\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -532,6 +533,12 @@ class ElectraPreTrainedModel(PreTrainedModel):\n         \"cross_attentions\": ElectraCrossAttention,\n     }\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, ElectraEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n+\n \n @dataclass\n @auto_docstring("
        },
        {
            "sha": "98b1898689ee6fb5721d15b4f2114686ae19a643",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -958,6 +958,10 @@ def _init_weights(self, module):\n         elif isinstance(module, (nn.BatchNorm2d, nn.BatchNorm3d, nn.GroupNorm)):\n             init.constant_(module.weight, 1.0)\n             init.constant_(module.bias, 0.0)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n         elif isinstance(module, nn.Embedding):\n             init.normal_(module.weight)\n             # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n@@ -1128,7 +1132,7 @@ def __init__(self, config: Emu3Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "791af0521a8314a1ffc22a86d92204ea015fd420",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -706,6 +706,10 @@ def _init_weights(self, module):\n         elif isinstance(module, (nn.BatchNorm2d, nn.BatchNorm3d, nn.GroupNorm)):\n             init.constant_(module.weight, 1.0)\n             init.constant_(module.bias, 0.0)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n         elif isinstance(module, nn.Embedding):\n             init.normal_(module.weight)\n             # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag"
        },
        {
            "sha": "dfa1dc269d1271659c2ca3f3795dcf862a888471",
            "filename": "src/transformers/models/encodec/modeling_encodec.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -474,6 +474,20 @@ def _init_weights(self, module):\n                     init.xavier_uniform_(param)\n                 elif \"bias\" in name:\n                     init.constant_(param, 0.0)\n+        elif isinstance(module, EncodecConv1d):\n+            kernel_size = module.conv.kernel_size[0]\n+            stride = torch.tensor(module.conv.stride[0], dtype=torch.int64)\n+            dilation = module.conv.dilation[0]\n+            # Effective kernel size with dilations.\n+            kernel_size = torch.tensor((kernel_size - 1) * dilation + 1, dtype=torch.int64)\n+            init.copy_(module.stride, stride)\n+            init.copy_(module.kernel_size, kernel_size)\n+            init.copy_(module.padding_total, kernel_size - stride)\n+        elif isinstance(module, EncodecEuclideanCodebook):\n+            init.copy_(module.inited, torch.Tensor([True]))\n+            init.zeros_(module.cluster_size)\n+            init.zeros_(module.embed)\n+            init.zeros_(module.embed_avg)\n \n \n @auto_docstring("
        },
        {
            "sha": "5aeb97aac61d318aa7f357e9717c234c17b792af",
            "filename": "src/transformers/models/eomt/modeling_eomt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -1020,6 +1020,13 @@ def _init_weights(self, module: nn.Module) -> None:\n         elif isinstance(module, EomtEmbeddings):\n             init.trunc_normal_(module.cls_token, mean=0.0, std=std)\n             init.zeros_(module.register_tokens)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+        elif isinstance(module, EomtLoss):\n+            empty_weight = torch.ones(module.num_labels + 1)\n+            empty_weight[-1] = module.eos_coef\n+            init.copy_(module.empty_weight, empty_weight)\n+        elif isinstance(module, EomtForUniversalSegmentation):\n+            init.ones_(module.attn_mask_probs)\n \n \n @auto_docstring("
        },
        {
            "sha": "cd0494264895d2dc8d8fb067e099957ac3f3184d",
            "filename": "src/transformers/models/eomt/modular_eomt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -425,6 +425,13 @@ def _init_weights(self, module: nn.Module) -> None:\n         elif isinstance(module, EomtEmbeddings):\n             init.trunc_normal_(module.cls_token, mean=0.0, std=std)\n             init.zeros_(module.register_tokens)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+        elif isinstance(module, EomtLoss):\n+            empty_weight = torch.ones(module.num_labels + 1)\n+            empty_weight[-1] = module.eos_coef\n+            init.copy_(module.empty_weight, empty_weight)\n+        elif isinstance(module, EomtForUniversalSegmentation):\n+            init.ones_(module.attn_mask_probs)\n \n \n @auto_docstring("
        },
        {
            "sha": "e4a50901744c04a65f0f6be7bb33a4a3089071e0",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -556,6 +556,9 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, ErnieLMPredictionHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, ErnieEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n \n \n @auto_docstring("
        },
        {
            "sha": "6457a33c6ac498fbe66ad719424a1660c5a206ad",
            "filename": "src/transformers/models/ernie/modular_ernie.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -172,6 +172,9 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, ErnieLMPredictionHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, ErnieEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n \n \n class ErnieModel(BertModel):"
        },
        {
            "sha": "62aa78be31132fc5e8884d2a4d79190f3cdba736",
            "filename": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -56,7 +56,7 @@ def __init__(self, config: Ernie4_5Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "a2beb3573ad3ecba97c6010f33064e32b443afff",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -96,7 +96,7 @@ def __init__(self, config: Ernie4_5_MoeConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "21534aea00cdf1d1023d5fe6318abca9b23ca7fc",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -90,6 +90,7 @@ class RotaryEmbedding(torch.nn.Module):\n \n     def __init__(self, dim: int):\n         super().__init__()\n+        self.dim = dim\n         # Generate and save the inverse frequency buffer (non trainable)\n         inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n         self.register_buffer(\"inv_freq\", inv_freq)\n@@ -558,6 +559,11 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, EsmLMHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, EsmEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+        elif isinstance(module, RotaryEmbedding):\n+            inv_freq = 1.0 / (10000 ** (torch.arange(0, module.dim, 2, dtype=torch.int64).float() / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n \n     def get_output_embeddings(self):\n         # NOTE: get_output_embeddings() must return None to prevent accidental weight tying."
        },
        {
            "sha": "dfab935d6974a94a7e7f6bee48e3d415105c45c2",
            "filename": "src/transformers/models/esm/modeling_esmfold.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -912,7 +912,7 @@ def _init_weights(self, module):\n                 elif module.init == \"gating\":\n                     init.zeros_(module.weight)\n                     if module.bias:\n-                        init.ones(module.bias)\n+                        init.ones_(module.bias)\n                 elif module.init == \"normal\":\n                     init.kaiming_normal_(module.weight, nonlinearity=\"linear\")\n                 elif module.init == \"final\":\n@@ -1979,6 +1979,11 @@ class EsmForProteinFolding(EsmPreTrainedModel):\n \n     _can_record_outputs = None\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, EsmForProteinFolding):\n+            init.copy_(module.af2_to_esm, module._af2_to_esm_from_vocab_list(module.config.vocab_list))\n+\n     def __init__(self, config):\n         super().__init__(config)\n "
        },
        {
            "sha": "7383b06572b73ed3eaf22537e740c5adb42bf347",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -185,6 +185,7 @@ class EvollaSaProtRotaryEmbedding(nn.Module):\n \n     def __init__(self, dim: int):\n         super().__init__()\n+        self.dim = dim\n         # Generate and save the inverse frequency buffer (non trainable)\n         inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n         self.register_buffer(\"inv_freq\", inv_freq)\n@@ -518,6 +519,12 @@ class EvollaSaProtPreTrainedModel(PreTrainedModel):\n         ],\n     }\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, EvollaSaProtRotaryEmbedding):\n+            inv_freq = 1.0 / (10000 ** (torch.arange(0, module.dim, 2, dtype=torch.int64).float() / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n+\n \n class EvollaSaProtProteinEncoder(EvollaSaProtPreTrainedModel):\n     def __init__(self, config: SaProtConfig):\n@@ -981,7 +988,7 @@ def __init__(self, config: EvollaConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "e9cc482bdb6a448321f603fefe8098937e65dd75",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -91,6 +91,7 @@ class EvollaSaProtRotaryEmbedding(nn.Module):\n \n     def __init__(self, dim: int):\n         super().__init__()\n+        self.dim = dim\n         # Generate and save the inverse frequency buffer (non trainable)\n         inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n         self.register_buffer(\"inv_freq\", inv_freq)\n@@ -203,6 +204,12 @@ class EvollaSaProtPreTrainedModel(PreTrainedModel):\n         ],\n     }\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, EvollaSaProtRotaryEmbedding):\n+            inv_freq = 1.0 / (10000 ** (torch.arange(0, module.dim, 2, dtype=torch.int64).float() / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n+\n \n class EvollaSaProtProteinEncoder(EvollaSaProtPreTrainedModel):\n     def __init__(self, config: SaProtConfig):"
        },
        {
            "sha": "dd3c93787f0525a4cf10570a6e05cee1a2d7ef38",
            "filename": "src/transformers/models/exaone4/modeling_exaone4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -86,7 +86,7 @@ def __init__(self, config: Exaone4Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "ed29dacec0a726d4334d7f19afb6297de2a824c7",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -122,7 +122,7 @@ def __init__(self, config: FalconConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "0d0498e78fca73b7c0c4e97604dcc720667b8803",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 26,
            "deletions": 22,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -241,7 +241,7 @@ def __init__(self, config: FalconH1Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -1187,26 +1187,6 @@ def forward(\n         return outputs\n \n \n-@auto_docstring\n-class FalconH1PreTrainedModel(PreTrainedModel):\n-    config: FalconH1Config\n-    base_model_prefix = \"model\"\n-    supports_gradient_checkpointing = True\n-    _no_split_modules = [\"FalconH1DecoderLayer\"]\n-    _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn = True\n-    _supports_sdpa = True\n-    _is_stateful = True\n-\n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        super()._init_weights(module)\n-        if isinstance(module, FalconH1Mixer):\n-            init.ones_(module.dt_bias)\n-            init.copy_(module.A_log, torch.log(torch.arange(1, module.num_heads + 1)))\n-            init.ones_(module.D)\n-\n-\n def compute_mup_vector(config):\n     \"\"\"\n     Computes the MuP vector based on model configuration.\n@@ -1244,6 +1224,30 @@ def compute_mup_vector(config):\n     return mup_vector\n \n \n+@auto_docstring\n+class FalconH1PreTrainedModel(PreTrainedModel):\n+    config: FalconH1Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"FalconH1DecoderLayer\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _is_stateful = True\n+\n+    @torch.no_grad()\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, FalconH1Mixer):\n+            init.ones_(module.dt_bias)\n+            init.copy_(module.A_log, torch.log(torch.arange(1, module.num_heads + 1)))\n+            init.ones_(module.D)\n+        elif isinstance(module, FalconH1Model):\n+            mup_vector = compute_mup_vector(module.config)\n+            for layer in module.layers:\n+                init.copy_(layer.mamba.mup_vector, mup_vector)\n+\n+\n @auto_docstring\n # Adapted from transformers.models.jamba.modeling_jamba.JambaModel\n class FalconH1Model(FalconH1PreTrainedModel):\n@@ -1269,7 +1273,7 @@ def __init__(self, config: FalconH1Config):\n         # Compute the MuP vector once and register it for all layers\n         mup_vector = compute_mup_vector(config)\n         for layer in self.layers:\n-            layer.mamba.register_buffer(\"mup_vector\", mup_vector, persistent=False)\n+            layer.mamba.register_buffer(\"mup_vector\", mup_vector.clone(), persistent=False)\n \n         # Initialize weights and apply final processing\n         self.post_init()"
        },
        {
            "sha": "a1c55f2a5dd17b9c313596e47ebb600cf8087b38",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -928,6 +928,10 @@ def _init_weights(self, module):\n             init.ones_(module.dt_bias)\n             init.copy_(module.A_log, torch.log(torch.arange(1, module.num_heads + 1)))\n             init.ones_(module.D)\n+        elif isinstance(module, FalconH1Model):\n+            mup_vector = compute_mup_vector(module.config)\n+            for layer in module.layers:\n+                init.copy_(layer.mamba.mup_vector, mup_vector)\n \n \n def compute_mup_vector(config):\n@@ -992,7 +996,7 @@ def __init__(self, config: FalconH1Config):\n         # Compute the MuP vector once and register it for all layers\n         mup_vector = compute_mup_vector(config)\n         for layer in self.layers:\n-            layer.mamba.register_buffer(\"mup_vector\", mup_vector, persistent=False)\n+            layer.mamba.register_buffer(\"mup_vector\", mup_vector.clone(), persistent=False)\n \n         # Initialize weights and apply final processing\n         self.post_init()"
        },
        {
            "sha": "2a45f7646fab5088ba0176086ceadebd8ef23371",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -613,6 +613,9 @@ def _init_weights(self, module):\n             init.ones_(module.weight)\n         elif isinstance(module, nn.Embedding):\n             init.normal_(module.weight, std=std)\n+        if isinstance(module, FalconMambaMixer):\n+            init.ones_(module.b_c_rms)\n+            init.ones_(module.dt_rms)\n \n \n @dataclass"
        },
        {
            "sha": "c18e19f3b7515e688d477fba252533a7abe380cb",
            "filename": "src/transformers/models/falcon_mamba/modular_falcon_mamba.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -19,6 +19,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...utils import auto_docstring, logging\n from ...utils.import_utils import is_mambapy_available, is_torchdynamo_compiling\n from ..mamba.configuration_mamba import MambaConfig\n@@ -529,7 +530,11 @@ class FalconMambaBlock(MambaBlock):\n \n @auto_docstring\n class FalconMambaPreTrainedModel(MambaPreTrainedModel):\n-    pass\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, FalconMambaMixer):\n+            init.ones_(module.b_c_rms)\n+            init.ones_(module.dt_rms)\n \n \n class FalconMambaOutput(MambaOutput):"
        },
        {
            "sha": "e09e6baff02e352c6e19aee19e7c686bf270e730",
            "filename": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -1010,6 +1010,10 @@ def _init_weights(self, module):\n         elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n         elif isinstance(module, nn.Embedding):\n             init.normal_(module.weight)\n             # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n@@ -1410,6 +1414,12 @@ def __init__(self, config: FastSpeech2ConformerHifiGanConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, FastSpeech2ConformerHifiGan):\n+            init.zeros_(module.mean)\n+            init.ones_(module.scale)\n+\n     def apply_weight_norm(self):\n         weight_norm = nn.utils.weight_norm\n         if hasattr(nn.utils.parametrizations, \"weight_norm\"):"
        },
        {
            "sha": "b39ebcae0eca26053601e9af392e979737072176",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 14,
            "deletions": 12,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -687,15 +687,17 @@ def _init_weights(self, module):\n         if isinstance(module, nn.LayerNorm):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n-        if isinstance(module, FlaubertModel) and self.config.sinusoidal_embeddings:\n-            init.copy_(\n-                module.position_embeddings.weight,\n-                create_sinusoidal_embeddings(\n-                    self.config.max_position_embeddings,\n-                    self.config.emb_dim,\n-                    out=torch.empty_like(module.position_embeddings.weight),\n-                ),\n-            )\n+        if isinstance(module, FlaubertModel):\n+            if self.config.sinusoidal_embeddings:\n+                init.copy_(\n+                    module.position_embeddings.weight,\n+                    create_sinusoidal_embeddings(\n+                        self.config.max_position_embeddings,\n+                        self.config.emb_dim,\n+                        out=torch.empty_like(module.position_embeddings.weight),\n+                    ),\n+                )\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n @auto_docstring\n@@ -757,15 +759,15 @@ def __init__(self, config):  # , dico, is_encoder, with_output):\n             self.ffns.append(TransformerFFN(self.dim, self.hidden_dim, self.dim, config=config))\n             self.layer_norm2.append(nn.LayerNorm(self.dim, eps=config.layer_norm_eps))\n \n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n         self.layerdrop = getattr(config, \"layerdrop\", 0.0)\n         self.pre_norm = getattr(config, \"pre_norm\", False)\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n \n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n     # Copied from transformers.models.xlm.modeling_xlm.XLMModel.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.embeddings"
        },
        {
            "sha": "5209de80a05c6f6202f59e4e05e9659599ed6aca",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -677,6 +677,9 @@ def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> No\n             init.zeros_(module.position_embeddings)\n             if module.mask_token is not None:\n                 init.zeros_(module.mask_token)\n+        elif isinstance(module, FlavaTextEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n         elif isinstance(module, FlavaMultimodalModel):\n             if module.use_cls_token:\n                 init.zeros_(module.cls_token)"
        },
        {
            "sha": "b45f33a0b647b9694999ee62be3f3eaa999046b0",
            "filename": "src/transformers/models/flex_olmo/modeling_flex_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -80,7 +80,7 @@ def __init__(self, config: FlexOlmoConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "91b177c28cfaf50678a0d2e988cc765fbce4e334",
            "filename": "src/transformers/models/florence2/modeling_florence2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -26,6 +26,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n@@ -629,6 +630,18 @@ class Florence2PreTrainedModel(PreTrainedModel):\n     _supports_attention_backend = False\n     config_class = Florence2Config\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, Florence2VisionPositionalEmbeddingCosine1D):\n+            pos_idx_to_embed = torch.empty((module.max_seq_len, module.embed_dim))\n+            sine, cosine = module.get_sinusoid_embeddings(\n+                max_positions=module.max_seq_len,\n+                embed_dim=module.embed_dim,\n+            )\n+            pos_idx_to_embed[:, 0::2] = sine\n+            pos_idx_to_embed[:, 1::2] = cosine\n+            init.copy_(module.pos_idx_to_embed, pos_idx_to_embed)\n+\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "9f089f156e80197127e605f01fee7a1e42c53b38",
            "filename": "src/transformers/models/florence2/modular_florence2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -22,6 +22,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...configuration_utils import PreTrainedConfig\n@@ -1500,6 +1501,18 @@ class Florence2PreTrainedModel(LlavaPreTrainedModel):\n \n     _supports_attention_backend = False\n \n+    def _init_weights(self, module):\n+        PreTrainedModel._init_weights(self, module)\n+        if isinstance(module, Florence2VisionPositionalEmbeddingCosine1D):\n+            pos_idx_to_embed = torch.empty((module.max_seq_len, module.embed_dim))\n+            sine, cosine = module.get_sinusoid_embeddings(\n+                max_positions=module.max_seq_len,\n+                embed_dim=module.embed_dim,\n+            )\n+            pos_idx_to_embed[:, 0::2] = sine\n+            pos_idx_to_embed[:, 1::2] = cosine\n+            init.copy_(module.pos_idx_to_embed, pos_idx_to_embed)\n+\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "cf3346cfe3dbd2c8cd722ac14c16fc1e126ce92a",
            "filename": "src/transformers/models/fnet/modeling_fnet.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...utils import auto_docstring, is_scipy_available\n \n \n@@ -374,6 +375,12 @@ class FNetPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"fnet\"\n     supports_gradient_checkpointing = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, FNetEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n+\n \n @dataclass\n @auto_docstring("
        },
        {
            "sha": "ab1e0f6bb3eb03dcae965155b482658bb7d5761d",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -98,7 +98,7 @@ def __init__(self, config: GemmaConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "af616a17d1babe934296bbe02dff94e3cc0f2398",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -99,7 +99,7 @@ def __init__(self, config: Gemma2Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "65acb9b8ff5bba59a9d2c27c29f748f785ac9ae0",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -244,7 +244,7 @@ def __init__(self, config: Gemma2Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)"
        },
        {
            "sha": "68df75d4aec91ce9ee552d9db588b29826773ad9",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -100,6 +100,7 @@ class Gemma3TextScaledWordEmbedding(nn.Embedding):\n \n     def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: float = 1.0):\n         super().__init__(num_embeddings, embedding_dim, padding_idx)\n+        self.scalar_embed_scale = embed_scale\n         self.register_buffer(\"embed_scale\", torch.tensor(embed_scale), persistent=False)\n \n     def forward(self, input_ids: torch.Tensor):\n@@ -165,7 +166,7 @@ def __init__(self, config: Gemma3TextConfig, device=None, layer_type=None):\n                 rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type[layer_type]]\n             curr_inv_freq, curr_attention_scaling = rope_init_fn(self.config, device, layer_type=layer_type)\n             self.register_buffer(f\"{layer_type}_inv_freq\", curr_inv_freq, persistent=False)\n-            setattr(self, f\"{layer_type}_original_inv_freq\", curr_inv_freq)\n+            self.register_buffer(f\"{layer_type}_original_inv_freq\", curr_inv_freq.clone(), persistent=False)\n             setattr(self, f\"{layer_type}_attention_scaling\", curr_attention_scaling)\n \n     @staticmethod\n@@ -468,6 +469,16 @@ def _init_weights(self, module):\n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         elif \"RMSNorm\" in module.__class__.__name__:\n             init.zeros_(module.weight)\n+        elif isinstance(module, Gemma3TextScaledWordEmbedding):\n+            init.constant_(module.embed_scale, module.scalar_embed_scale)\n+        elif isinstance(module, Gemma3RotaryEmbedding):\n+            for layer_type in module.layer_types:\n+                rope_init_fn = module.compute_default_rope_parameters\n+                if module.rope_type[layer_type] != \"default\":\n+                    rope_init_fn = ROPE_INIT_FUNCTIONS[module.rope_type[layer_type]]\n+                curr_inv_freq, _ = rope_init_fn(module.config, layer_type=layer_type)\n+                init.copy_(getattr(module, f\"{layer_type}_inv_freq\"), curr_inv_freq)\n+                init.copy_(getattr(module, f\"{layer_type}_original_inv_freq\"), curr_inv_freq)\n \n \n def _bidirectional_window_overlay(sliding_window: int) -> Callable[[int, int, int, int], bool]:"
        },
        {
            "sha": "d9f1cfcd0798bf3cc139d452867c924625d0c820",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -352,6 +352,7 @@ class Gemma3TextScaledWordEmbedding(nn.Embedding):\n \n     def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: float = 1.0):\n         super().__init__(num_embeddings, embedding_dim, padding_idx)\n+        self.scalar_embed_scale = embed_scale\n         self.register_buffer(\"embed_scale\", torch.tensor(embed_scale), persistent=False)\n \n     def forward(self, input_ids: torch.Tensor):\n@@ -389,7 +390,7 @@ def __init__(self, config: Gemma3TextConfig, device=None, layer_type=None):\n                 rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type[layer_type]]\n             curr_inv_freq, curr_attention_scaling = rope_init_fn(self.config, device, layer_type=layer_type)\n             self.register_buffer(f\"{layer_type}_inv_freq\", curr_inv_freq, persistent=False)\n-            setattr(self, f\"{layer_type}_original_inv_freq\", curr_inv_freq)\n+            self.register_buffer(f\"{layer_type}_original_inv_freq\", curr_inv_freq.clone(), persistent=False)\n             setattr(self, f\"{layer_type}_attention_scaling\", curr_attention_scaling)\n \n     @staticmethod\n@@ -576,6 +577,16 @@ def _init_weights(self, module):\n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         elif \"RMSNorm\" in module.__class__.__name__:\n             init.zeros_(module.weight)\n+        elif isinstance(module, Gemma3TextScaledWordEmbedding):\n+            init.constant_(module.embed_scale, module.scalar_embed_scale)\n+        elif isinstance(module, Gemma3RotaryEmbedding):\n+            for layer_type in module.layer_types:\n+                rope_init_fn = module.compute_default_rope_parameters\n+                if module.rope_type[layer_type] != \"default\":\n+                    rope_init_fn = ROPE_INIT_FUNCTIONS[module.rope_type[layer_type]]\n+                curr_inv_freq, _ = rope_init_fn(module.config, layer_type=layer_type)\n+                init.copy_(getattr(module, f\"{layer_type}_inv_freq\"), curr_inv_freq)\n+                init.copy_(getattr(module, f\"{layer_type}_original_inv_freq\"), curr_inv_freq)\n \n \n def _bidirectional_window_overlay(sliding_window: int) -> Callable[[int, int, int, int], bool]:"
        },
        {
            "sha": "51af67eb7ff9c0a692539e5ac3750cab6edd64bd",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 43,
            "deletions": 8,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -329,6 +329,16 @@ def __init__(self, config: Gemma3nAudioConfig):\n         r_softplus_0 = 1.0 / torch.nn.functional.softplus(torch.tensor(0.0))\n         self.register_buffer(\"q_scale\", (q_scale * r_softplus_0).clone().detach(), persistent=False)\n \n+        local_causal_valid_mask = self.create_local_causal_valid_mask()\n+        self.register_buffer(\"local_causal_valid_mask\", local_causal_valid_mask, persistent=False)\n+\n+        self.register_buffer(\n+            \"softcap\",\n+            torch.tensor(self.attention_logits_soft_cap).float(),\n+            persistent=False,\n+        )\n+\n+    def create_local_causal_valid_mask(self):\n         lower_causal_mask = torch.tril(\n             torch.ones((self.context_size, self.chunk_size), dtype=torch.bool),\n             diagonal=0,\n@@ -339,13 +349,7 @@ def __init__(self, config: Gemma3nAudioConfig):\n         )\n         local_causal_valid_mask = torch.ones((self.chunk_size, self.context_size), dtype=torch.bool)\n         local_causal_valid_mask = local_causal_valid_mask * lower_causal_mask * upper_causal_mask\n-        self.register_buffer(\"local_causal_valid_mask\", local_causal_valid_mask, persistent=False)\n-\n-        self.register_buffer(\n-            \"softcap\",\n-            torch.tensor(self.attention_logits_soft_cap).float(),\n-            persistent=False,\n-        )\n+        return local_causal_valid_mask\n \n     def _pad_dim1(self, x: torch.Tensor, pad_left: int, pad_right: int) -> torch.Tensor:\n         batch, _, *tail_shape = x.shape\n@@ -984,6 +988,7 @@ class Gemma3nTextScaledWordEmbedding(nn.Embedding):\n \n     def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: float = 1.0):\n         super().__init__(num_embeddings, embedding_dim, padding_idx)\n+        self.scalar_embed_scale = embed_scale\n         self.register_buffer(\"embed_scale\", torch.tensor(embed_scale), persistent=False)\n \n     def forward(self, input_ids: torch.Tensor):\n@@ -1450,8 +1455,38 @@ def _init_weights(self, module):\n             init.ones_(module.weight)\n         elif isinstance(module, Gemma3nAudioAttention):\n             init.zeros_(module.per_dim_scale)\n+            q_scale = module.head_dim**-0.5\n+            r_softplus_0 = 1.0 / torch.nn.functional.softplus(torch.tensor(0.0))\n+            init.copy_(module.q_scale, q_scale * r_softplus_0)\n+            init.constant_(module.softcap, module.attention_logits_soft_cap)\n+            init.copy_(module.local_causal_valid_mask, module.create_local_causal_valid_mask())\n+        elif isinstance(module, Gemma3nTextScaledWordEmbedding):\n+            init.constant_(module.embed_scale, module.scalar_embed_scale)\n         elif isinstance(module, Gemma3nTextAltUp):\n             init.zeros_(module.correct_output_scale)\n+            init.constant_(module.router_input_scale, self.config.hidden_size**-1.0)\n+        elif isinstance(module, Gemma3nAudioRelativePositionEmbedding):\n+            min_timescale, max_timescale = 1.0, 1.0e4\n+            num_timescales = module.channels // 2\n+            log_timescale_increment = math.log(float(max_timescale) / float(min_timescale)) / max(\n+                num_timescales - 1, 1\n+            )\n+            inv_timescales = min_timescale * torch.exp(torch.arange(num_timescales) * -log_timescale_increment)\n+            init.copy_(module.inv_timescales, inv_timescales.float().unsqueeze(0).unsqueeze(0))\n+        elif isinstance(module, Gemma3nTextModel):\n+            init.constant_(module.per_layer_projection_scale, self.hidden_size**-0.5)\n+            init.constant_(module.per_layer_input_scale, 1 / math.sqrt(2.0))\n+        elif isinstance(module, Gemma3nRotaryEmbedding):\n+            for layer_type in module.layer_types:\n+                rope_init_fn = module.compute_default_rope_parameters\n+                if module.rope_type[layer_type] != \"default\":\n+                    rope_init_fn = ROPE_INIT_FUNCTIONS[module.rope_type[layer_type]]\n+                curr_inv_freq, _ = rope_init_fn(module.config, layer_type=layer_type)\n+                init.copy_(getattr(module, f\"{layer_type}_inv_freq\"), curr_inv_freq)\n+                init.copy_(getattr(module, f\"{layer_type}_original_inv_freq\"), curr_inv_freq)\n+\n+        if hasattr(module, \"gradient_clipping\"):\n+            init.constant_(module.gradient_clipping, self.config.gradient_clipping)\n \n \n class Gemma3nRotaryEmbedding(nn.Module):\n@@ -1477,7 +1512,7 @@ def __init__(self, config: Gemma3nTextConfig, device=None, layer_type=None):\n                 rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type[layer_type]]\n             curr_inv_freq, curr_attention_scaling = rope_init_fn(self.config, device, layer_type=layer_type)\n             self.register_buffer(f\"{layer_type}_inv_freq\", curr_inv_freq, persistent=False)\n-            setattr(self, f\"{layer_type}_original_inv_freq\", curr_inv_freq)\n+            self.register_buffer(f\"{layer_type}_original_inv_freq\", curr_inv_freq.clone(), persistent=False)\n             setattr(self, f\"{layer_type}_attention_scaling\", curr_attention_scaling)\n \n     @staticmethod"
        },
        {
            "sha": "60965ce40378268b5c331eaf326907bd09ddf8cd",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 47,
            "deletions": 8,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -27,7 +27,7 @@\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_outputs import BaseModelOutputWithPast\n-from ...modeling_rope_utils import RopeParameters\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, RopeParameters\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n@@ -45,6 +45,7 @@\n     Gemma3DecoderLayer,\n     Gemma3ForCausalLM,\n     Gemma3RMSNorm,\n+    Gemma3RotaryEmbedding,\n     Gemma3TextModel,\n     Gemma3TextScaledWordEmbedding,\n )\n@@ -882,6 +883,16 @@ def __init__(self, config: Gemma3nAudioConfig):\n         r_softplus_0 = 1.0 / torch.nn.functional.softplus(torch.tensor(0.0))\n         self.register_buffer(\"q_scale\", (q_scale * r_softplus_0).clone().detach(), persistent=False)\n \n+        local_causal_valid_mask = self.create_local_causal_valid_mask()\n+        self.register_buffer(\"local_causal_valid_mask\", local_causal_valid_mask, persistent=False)\n+\n+        self.register_buffer(\n+            \"softcap\",\n+            torch.tensor(self.attention_logits_soft_cap).float(),\n+            persistent=False,\n+        )\n+\n+    def create_local_causal_valid_mask(self):\n         lower_causal_mask = torch.tril(\n             torch.ones((self.context_size, self.chunk_size), dtype=torch.bool),\n             diagonal=0,\n@@ -892,13 +903,7 @@ def __init__(self, config: Gemma3nAudioConfig):\n         )\n         local_causal_valid_mask = torch.ones((self.chunk_size, self.context_size), dtype=torch.bool)\n         local_causal_valid_mask = local_causal_valid_mask * lower_causal_mask * upper_causal_mask\n-        self.register_buffer(\"local_causal_valid_mask\", local_causal_valid_mask, persistent=False)\n-\n-        self.register_buffer(\n-            \"softcap\",\n-            torch.tensor(self.attention_logits_soft_cap).float(),\n-            persistent=False,\n-        )\n+        return local_causal_valid_mask\n \n     def _pad_dim1(self, x: torch.Tensor, pad_left: int, pad_right: int) -> torch.Tensor:\n         batch, _, *tail_shape = x.shape\n@@ -1893,8 +1898,42 @@ def _init_weights(self, module):\n             init.ones_(module.weight)\n         elif isinstance(module, Gemma3nAudioAttention):\n             init.zeros_(module.per_dim_scale)\n+            q_scale = module.head_dim**-0.5\n+            r_softplus_0 = 1.0 / torch.nn.functional.softplus(torch.tensor(0.0))\n+            init.copy_(module.q_scale, q_scale * r_softplus_0)\n+            init.constant_(module.softcap, module.attention_logits_soft_cap)\n+            init.copy_(module.local_causal_valid_mask, module.create_local_causal_valid_mask())\n+        elif isinstance(module, Gemma3nTextScaledWordEmbedding):\n+            init.constant_(module.embed_scale, module.scalar_embed_scale)\n         elif isinstance(module, Gemma3nTextAltUp):\n             init.zeros_(module.correct_output_scale)\n+            init.constant_(module.router_input_scale, self.config.hidden_size**-1.0)\n+        elif isinstance(module, Gemma3nAudioRelativePositionEmbedding):\n+            min_timescale, max_timescale = 1.0, 1.0e4\n+            num_timescales = module.channels // 2\n+            log_timescale_increment = math.log(float(max_timescale) / float(min_timescale)) / max(\n+                num_timescales - 1, 1\n+            )\n+            inv_timescales = min_timescale * torch.exp(torch.arange(num_timescales) * -log_timescale_increment)\n+            init.copy_(module.inv_timescales, inv_timescales.float().unsqueeze(0).unsqueeze(0))\n+        elif isinstance(module, Gemma3nTextModel):\n+            init.constant_(module.per_layer_projection_scale, self.hidden_size**-0.5)\n+            init.constant_(module.per_layer_input_scale, 1 / math.sqrt(2.0))\n+        elif isinstance(module, Gemma3nRotaryEmbedding):\n+            for layer_type in module.layer_types:\n+                rope_init_fn = module.compute_default_rope_parameters\n+                if module.rope_type[layer_type] != \"default\":\n+                    rope_init_fn = ROPE_INIT_FUNCTIONS[module.rope_type[layer_type]]\n+                curr_inv_freq, _ = rope_init_fn(module.config, layer_type=layer_type)\n+                init.copy_(getattr(module, f\"{layer_type}_inv_freq\"), curr_inv_freq)\n+                init.copy_(getattr(module, f\"{layer_type}_original_inv_freq\"), curr_inv_freq)\n+\n+        if hasattr(module, \"gradient_clipping\"):\n+            init.constant_(module.gradient_clipping, self.config.gradient_clipping)\n+\n+\n+class Gemma3nRotaryEmbedding(Gemma3RotaryEmbedding):\n+    pass\n \n \n @auto_docstring(custom_intro=\"The base Gemma 3n language model without a language modeling head.\")"
        },
        {
            "sha": "9103794d230939c164740f45dbe2e152fb64e275",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -396,6 +396,7 @@ def _init_weights(self, module):\n             init.normal_(module.class_embedding, mean=0.0, std=self.config.initializer_range)\n             init.normal_(module.patch_embedding.weight, std=self.config.initializer_range)\n             init.normal_(module.position_embedding.weight, std=self.config.initializer_range)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         if isinstance(module, nn.Linear):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n@@ -408,6 +409,8 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+        elif isinstance(module, GitEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n # Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings with CLIP->Git"
        },
        {
            "sha": "6cef0e0a0b1a8917955556161bcd7e28ddcfa868",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -79,7 +79,7 @@ def __init__(self, config: GlmConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "bbbd6175e04151c9cf0e2d556ff4efd1e4c1b1eb",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -284,7 +284,7 @@ def __init__(self, config: Glm4Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "0578bb25cd963f1028a87ffb5785ada113acafd6",
            "filename": "src/transformers/models/glm46v/modular_glm46v.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fglm46v%2Fmodular_glm46v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fglm46v%2Fmodular_glm46v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm46v%2Fmodular_glm46v.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -110,6 +110,9 @@ class Glm46VPreTrainedModel(Glm4vPreTrainedModel):\n     _can_record_outputs = None\n     _no_split_modules = None\n \n+    def _init_weights(self, module):\n+        raise AttributeError(\"Not needed\")\n+\n \n class Glm46VModel(Glm4vModel):\n     _no_split_modules = None"
        },
        {
            "sha": "ec65d11f896a347fbcb263861c5b93749e5405c0",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -60,7 +60,7 @@ def __init__(self, config: Glm4MoeConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -499,6 +499,7 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Glm4MoeTopkRouter):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            init.zeros_(module.e_score_correction_bias)\n         elif isinstance(module, Glm4MoeNaiveMoe):\n             init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n             init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)"
        },
        {
            "sha": "00cc1d74bfd11941a5e428ad7fa9abeae1fb75cc",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -28,6 +28,7 @@\n import torch.nn.functional as F\n from torch.nn import LayerNorm\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -104,6 +105,8 @@ class Glm4vVisionRotaryEmbedding(nn.Module):\n \n     def __init__(self, dim: int, theta: float = 10000.0) -> None:\n         super().__init__()\n+        self.dim = dim\n+        self.theta = theta\n         inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n \n@@ -141,7 +144,6 @@ def __init__(self, config: Glm4vVisionConfig):\n         self.num_patches = (self.image_size // self.patch_size) ** 2\n         self.num_positions = self.num_patches\n         self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n-        self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n \n     def forward(self, embeddings, lengths, image_shapes, h_coords, w_coords) -> torch.Tensor:\n         \"\"\"\n@@ -403,7 +405,7 @@ def __init__(self, config: Glm4vTextConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -705,6 +707,12 @@ class Glm4vPreTrainedModel(PreTrainedModel):\n         \"attentions\": Glm4vTextAttention,\n     }\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, Glm4vVisionRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n+\n \n class Glm4vVisionModel(Glm4vPreTrainedModel):\n     config: Glm4vVisionConfig"
        },
        {
            "sha": "66bdc10d363bfae657291365b2df9dec3ad8df19",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -22,6 +22,7 @@\n import torch.nn.functional as F\n from torch.nn import LayerNorm\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PreTrainedConfig\n@@ -32,7 +33,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import RopeParameters\n-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n@@ -409,7 +410,6 @@ def __init__(self, config: Glm4vVisionConfig):\n         self.num_patches = (self.image_size // self.patch_size) ** 2\n         self.num_positions = self.num_patches\n         self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n-        self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n \n     def forward(self, embeddings, lengths, image_shapes, h_coords, w_coords) -> torch.Tensor:\n         \"\"\"\n@@ -725,6 +725,12 @@ class Glm4vPreTrainedModel(Qwen2_5_VLPreTrainedModel):\n         \"attentions\": Glm4vTextAttention,\n     }\n \n+    def _init_weights(self, module):\n+        PreTrainedModel._init_weights(self, module)\n+        if isinstance(module, Glm4vVisionRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n+\n \n class Glm4vVisionModel(Glm4vPreTrainedModel):\n     config: Glm4vVisionConfig"
        },
        {
            "sha": "21f05d2144f66db3254cccaeb25ded7fa958463b",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 21,
            "deletions": 16,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -107,7 +107,7 @@ def __init__(self, config: Glm4vMoeTextConfig, device=None, layer_type=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -602,9 +602,13 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Glm4vMoeTextTopkRouter):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            init.zeros_(module.e_score_correction_bias)\n         elif isinstance(module, Glm4vMoeTextNaiveMoe):\n             init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n             init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n+        if isinstance(module, Glm4vMoeVisionRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n \n \n @dataclass\n@@ -637,6 +641,22 @@ class Glm4vMoeCausalLMOutputWithPast(ModelOutput):\n     aux_loss: Optional[torch.FloatTensor] = None\n \n \n+class Glm4vMoeVisionRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, dim: int, theta: float = 10000.0) -> None:\n+        super().__init__()\n+        self.dim = dim\n+        self.theta = theta\n+        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+\n+    def forward(self, seqlen: int) -> torch.Tensor:\n+        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n+        freqs = torch.outer(seq, self.inv_freq)\n+        return freqs\n+\n+\n class Glm4vMoeisionMlp(nn.Module):\n     def __init__(self, config, bias: bool = False):\n         super().__init__()\n@@ -671,20 +691,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class Glm4vMoeVisionRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, dim: int, theta: float = 10000.0) -> None:\n-        super().__init__()\n-        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-    def forward(self, seqlen: int) -> torch.Tensor:\n-        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n-        freqs = torch.outer(seq, self.inv_freq)\n-        return freqs\n-\n-\n class Glm4vMoeVisionPatchMerger(nn.Module):\n     def __init__(self, dim: int, context_dim: int, hidden_act: str, bias: bool = False) -> None:\n         super().__init__()\n@@ -713,7 +719,6 @@ def __init__(self, config: Glm4vMoeVisionConfig):\n         self.num_patches = (self.image_size // self.patch_size) ** 2\n         self.num_positions = self.num_patches\n         self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n-        self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n \n     def forward(self, embeddings, lengths, image_shapes, h_coords, w_coords) -> torch.Tensor:\n         \"\"\""
        },
        {
            "sha": "990a30a77b71596319db9ab6a67e72cf285a9611",
            "filename": "src/transformers/models/glm4v_moe/modular_glm4v_moe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -18,6 +18,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PreTrainedConfig\n from ...masking_utils import create_causal_mask\n@@ -46,6 +47,7 @@\n     Glm4vTextModel,\n     Glm4vTextRotaryEmbedding,\n     Glm4vVisionModel,\n+    Glm4vVisionRotaryEmbedding,\n     rotate_half,\n )\n from ..qwen3_vl_moe.modeling_qwen3_vl_moe import (\n@@ -479,11 +481,21 @@ class Glm4vMoePreTrainedModel(Glm4MoePreTrainedModel):\n         \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"mlp.gate\", index=0),\n     }\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, Glm4vMoeVisionRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n+\n \n class Glm4vMoeCausalLMOutputWithPast(Qwen3VLMoeCausalLMOutputWithPast):\n     pass\n \n \n+class Glm4vMoeVisionRotaryEmbedding(Glm4vVisionRotaryEmbedding):\n+    pass\n+\n+\n @auto_docstring\n class Glm4vMoeVisionModel(Glm4vVisionModel):\n     pass"
        },
        {
            "sha": "19129a566d8cdd8e54ac097810d95b323e0ab594",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -103,7 +103,6 @@ def __init__(self, config, is_cross_attention=False, layer_idx=None):\n             ),\n             persistent=False,\n         )\n-        self.register_buffer(\"masked_bias\", torch.tensor(-1e4), persistent=False)\n \n         self.embed_dim = config.hidden_size\n         self.num_heads = config.num_attention_heads\n@@ -493,6 +492,14 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+        elif isinstance(module, GPT2Attention):\n+            max_positions = module.config.max_position_embeddings\n+            init.copy_(\n+                module.bias,\n+                torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(\n+                    1, 1, max_positions, max_positions\n+                ),\n+            )\n \n         # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n         #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale"
        },
        {
            "sha": "274e5b8530d2e83558f2eb72ad86fa60bf889168",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -369,6 +369,9 @@ def _init_weights(self, module):\n             init.normal_(\n                 module.c_proj.weight, mean=0.0, std=self.config.initializer_range / math.sqrt(2 * self.config.n_layer)\n             )\n+        elif isinstance(module, GPTBigCodeModel):\n+            max_positions = module.config.max_position_embeddings\n+            init.copy_(module.bias, torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)))\n \n \n @auto_docstring"
        },
        {
            "sha": "1e430acd159f9a77944498db795a6b8c4011d2c9",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -20,6 +20,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -70,11 +71,11 @@ def __init__(self, config, attention_type, layer_id=None):\n         # local causal self attention is a sliding window where each token can only attend to the previous\n         # window_size tokens. This is implemented by updating the causal mask such that for each token\n         # all other tokens are masked except the previous window_size tokens.\n+        self.attention_type = attention_type\n         if attention_type == \"local\":\n             bias = torch.bitwise_xor(bias, torch.tril(bias, -config.window_size))\n \n         self.register_buffer(\"bias\", bias, persistent=False)\n-        self.register_buffer(\"masked_bias\", torch.tensor(-1e9), persistent=False)\n \n         self.attn_dropout = nn.Dropout(float(config.attention_dropout))\n         self.resid_dropout = nn.Dropout(float(config.resid_dropout))\n@@ -382,6 +383,17 @@ class GPTNeoPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _can_compile_fullgraph = False  # TODO: needs a hybrid cache\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, GPTNeoSelfAttention):\n+            max_positions = module.config.max_position_embeddings\n+            bias = torch.tril(torch.ones((max_positions, max_positions), dtype=bool)).view(\n+                1, 1, max_positions, max_positions\n+            )\n+            if module.attention_type == \"local\":\n+                bias = torch.bitwise_xor(bias, torch.tril(bias, -module.config.window_size))\n+            init.copy_(module.bias, bias)\n+\n \n @auto_docstring\n class GPTNeoModel(GPTNeoPreTrainedModel):"
        },
        {
            "sha": "7bdbf7eb28208bbfc9c0cf1a8551fd2d0f94dd63",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -66,7 +66,7 @@ def __init__(self, config: GPTNeoXConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "e192344fa9c78902dd6066e6f080e1bba73d5163",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -78,7 +78,7 @@ def __init__(self, config: GPTNeoXJapaneseConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "ef11d63bd75d5f9116d31596916a932c108054c5",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -197,7 +197,7 @@ def __init__(self, config: GptOssConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "15dcc22adb3f032d54250dd9f6c99867ea7c991e",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -337,7 +337,7 @@ def __init__(self, config: GraniteConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "2ebddafdcc92d4686412e7f7d2df20c6dd9518ab",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -293,6 +293,12 @@ def _init_weights(self, module: nn.Module):\n         super()._init_weights(module)\n         if isinstance(module, GraniteSpeechEncoderProjector):\n             init.normal_(module.query)\n+        elif isinstance(module, GraniteSpeechCTCEncoder):\n+            context_size = module.config.context_size\n+            seq = torch.arange(context_size)\n+            relpos_dist = seq.view(-1, 1) - seq.view(1, -1)\n+            attention_dists = torch.clamp(relpos_dist, -context_size, context_size) + module.config.max_pos_emb\n+            init.copy_(module.attention_dists, attention_dists)\n \n \n @auto_docstring("
        },
        {
            "sha": "4a155a54e8413142e8bbdeefeb4ba019a65a8b5d",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -80,7 +80,7 @@ def __init__(self, config: GraniteMoeConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "3ab7ef9a2e5c020c262ab3e60d248a135748f491",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -919,7 +919,7 @@ def __init__(self, config: GraniteMoeHybridConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "75e88b6679ca6bdb2538e4231a1239dba539f364",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -494,7 +494,7 @@ def __init__(self, config: GraniteMoeSharedConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "0ee678f233b2ada1b97c516f0a6786635a2e5437",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -1415,7 +1415,7 @@ def _init_weights(self, module):\n         elif isinstance(module, GroundingDinoFusionLayer):\n             init.constant_(module.vision_param, 1e-4)\n             init.constant_(module.text_param, 1e-4)\n-        elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n+        elif isinstance(module, (nn.Linear, nn.Conv2d)):\n             init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n                 init.zeros_(module.bias)"
        },
        {
            "sha": "931cfd0423a9b985b99f1f9a66828b5ea08c96a0",
            "filename": "src/transformers/models/groupvit/modeling_groupvit.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -758,14 +758,19 @@ def _init_weights(self, module):\n             init.normal_(module.weight, mean=0.0, std=init_range)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n-        elif isinstance(module, nn.LayerNorm):\n+        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n \n         factor = self.config.initializer_factor\n         if isinstance(module, GroupViTTextEmbeddings):\n             init.normal_(module.token_embedding.weight, mean=0.0, std=factor * 0.02)\n             init.normal_(module.position_embedding.weight, mean=0.0, std=factor * 0.02)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         elif isinstance(module, GroupViTAttention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor"
        },
        {
            "sha": "cc517361766dcded605215dd05d0b146c430c902",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -79,7 +79,7 @@ def __init__(self, config: HeliumConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "a7723e74ed8c308f4ed0d5e6a73836c1efd7b493",
            "filename": "src/transformers/models/hgnet_v2/modeling_hgnet_v2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -26,6 +26,7 @@\n import torch.nn.functional as F\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_outputs import BackboneOutput, BaseModelOutputWithNoAttention, ImageClassifierOutputWithNoAttention\n from ...modeling_utils import PreTrainedModel\n@@ -45,6 +46,15 @@ class HGNetV2PreTrainedModel(PreTrainedModel):\n     input_modalities = (\"image\",)\n     _no_split_modules = [\"HGNetV2BasicLayer\"]\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        # We need to check it like that as d_fine models replace the BatchNorm2d by their own\n+        if \"BatchNorm\" in module.__class__.__name__:\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n+            init.zeros_(module.running_mean)\n+            init.ones_(module.running_var)\n+\n \n class HGNetV2LearnableAffineBlock(nn.Module):\n     def __init__(self, scale_value: float = 1.0, bias_value: float = 0.0):"
        },
        {
            "sha": "25cc420ccc307a1fa64e9e7749ae77d1ba689643",
            "filename": "src/transformers/models/hgnet_v2/modular_hgnet_v2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -20,6 +20,7 @@\n import torch.nn.functional as F\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...configuration_utils import PreTrainedConfig\n from ...modeling_outputs import (\n     BackboneOutput,\n@@ -170,6 +171,15 @@ class HGNetV2PreTrainedModel(PreTrainedModel):\n     input_modalities = (\"image\",)\n     _no_split_modules = [\"HGNetV2BasicLayer\"]\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        # We need to check it like that as d_fine models replace the BatchNorm2d by their own\n+        if \"BatchNorm\" in module.__class__.__name__:\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n+            init.zeros_(module.running_mean)\n+            init.ones_(module.running_var)\n+\n \n class HGNetV2LearnableAffineBlock(nn.Module):\n     def __init__(self, scale_value: float = 1.0, bias_value: float = 0.0):"
        },
        {
            "sha": "b33961c4dc1b285320ccdf3b9d3a97c37afcb12c",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -648,6 +648,10 @@ def _init_weights(self, module):\n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm1d)):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n         elif isinstance(module, nn.Conv1d):\n             if is_deepspeed_zero3_enabled():\n                 import deepspeed"
        },
        {
            "sha": "5d41c547d0e05221a54270d37253939b2499f09c",
            "filename": "src/transformers/models/hubert/modular_hubert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -145,6 +145,10 @@ def _init_weights(self, module):\n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm1d)):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n         elif isinstance(module, nn.Conv1d):\n             if is_deepspeed_zero3_enabled():\n                 import deepspeed"
        },
        {
            "sha": "378a1e30529390c99702a27bcb2f01d7bd76f850",
            "filename": "src/transformers/models/hunyuan_v1_dense/modeling_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -320,7 +320,7 @@ def __init__(self, config: HunYuanDenseV1Config, device=None):\n             inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "62551c08c66f80e25a30ab35a840ff273725e47e",
            "filename": "src/transformers/models/hunyuan_v1_dense/modular_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -148,7 +148,7 @@ def __init__(self, config: HunYuanDenseV1Config, device=None):\n             inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n \n class HunYuanDenseV1Model(LlamaModel):"
        },
        {
            "sha": "77691f9596f9bdcf7495621ac4cef051742197e5",
            "filename": "src/transformers/models/hunyuan_v1_moe/modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -413,7 +413,7 @@ def __init__(self, config: HunYuanMoEV1Config, device=None):\n             inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "09795deabc3878a7a1cd797e961751f580bd19d8",
            "filename": "src/transformers/models/ibert/modeling_ibert.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -593,16 +593,32 @@ def _init_weights(self, module):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n+            if getattr(module, \"weight_integer\", None) is not None:\n+                init.zeros_(module.weight_integer)\n+                init.zeros_(module.fc_scaling_factor)\n+            if getattr(module, \"bias_integer\", None) is not None:\n+                init.zeros_(module.bias_integer)\n         elif isinstance(module, (QuantEmbedding, nn.Embedding)):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n             if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n                 init.zeros_(module.weight[module.padding_idx])\n+            if getattr(module, \"weight_scaling_factor\", None) is not None:\n+                init.zeros_(module.weight_scaling_factor)\n+                init.zeros_(module.weight_integer)\n         elif isinstance(module, (IntLayerNorm, nn.LayerNorm)):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+            if getattr(module, \"shift\", None) is not None:\n+                init.zeros_(module.shift)\n         elif isinstance(module, IBertLMHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, IBertEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+        elif isinstance(module, QuantAct):\n+            init.constant_(module.x_min, -1e-5)\n+            init.constant_(module.x_max, 1e-5)\n+            init.zeros_(module.act_scaling_factor)\n \n     def resize_token_embeddings(self, new_num_tokens=None):\n         raise NotImplementedError(\"`resize_token_embeddings` is not supported for I-BERT.\")"
        },
        {
            "sha": "6887f36e917f3e2c267d5219d2e012cd3013195b",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -840,6 +840,7 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, IdeficsVisionEmbeddings):\n             init.normal_(module.class_embedding)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         elif isinstance(module, IdeficsGatedCrossAttentionLayer):\n             if self.config.alpha_initializer == \"zeros\":\n                 init.zeros_(module.alpha_cross_attn)\n@@ -852,6 +853,15 @@ def _init_weights(self, module):\n                 init.normal_(module.alpha_dense, mean=0.0, std=self.config.alphas_initializer_range)\n         elif isinstance(module, IdeficsPerceiverResampler):\n             init.normal_(module.latents)\n+        elif isinstance(module, IdeficsEmbedding):\n+            inv_freq = 1.0 / (module.base ** (torch.arange(0, module.dim, 2) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n+            t = torch.arange(module.max_position_embeddings).type_as(inv_freq)\n+            freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n+            # Different from paper, but it uses a different permutation in order to obtain the same calculation\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            init.copy_(module.cos_cached, emb.cos())\n+            init.copy_(module.sin_cached, emb.sin())\n \n \n @auto_docstring"
        },
        {
            "sha": "d4fb4cd55c64a56e52ea5e44b32d76629a18be7d",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -61,7 +61,7 @@ def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n class ImageGPTAttention(nn.Module):\n     def __init__(self, config, is_cross_attention: Optional[bool] = False, layer_idx: Optional[int] = None):\n         super().__init__()\n-\n+        self.config = config\n         max_positions = config.max_position_embeddings\n         self.register_buffer(\n             \"bias\",\n@@ -70,7 +70,6 @@ def __init__(self, config, is_cross_attention: Optional[bool] = False, layer_idx\n             ),\n             persistent=False,\n         )\n-        self.register_buffer(\"masked_bias\", torch.tensor(-1e4), persistent=False)\n \n         self.embed_dim = config.hidden_size\n         self.num_heads = config.num_attention_heads\n@@ -384,6 +383,14 @@ def _init_weights(self, module):\n                 if \"c_proj\" in name and \"weight\" in name:\n                     # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n                     init.normal_(p, mean=0.0, std=self.config.initializer_range / math.sqrt(2 * self.config.n_layer))\n+        elif isinstance(module, ImageGPTAttention):\n+            max_positions = module.config.max_position_embeddings\n+            init.copy_(\n+                module.bias,\n+                torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(\n+                    1, 1, max_positions, max_positions\n+                ),\n+            )\n \n \n @auto_docstring"
        },
        {
            "sha": "fd2edb3d35d8b3ed2dbdae5665b0bc72f72b1609",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -335,6 +335,8 @@ def _init_weights(self, module):\n             init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n         elif isinstance(module, (InstructBlipForConditionalGeneration, InstructBlipModel)):\n             init.zeros_(module.query_tokens)\n+        elif isinstance(module, InstructBlipQFormerEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n # Copied from transformers.models.blip.modeling_blip.BlipEncoder with Blip->InstructBlip"
        },
        {
            "sha": "8b03b40c66ad0b75c1ebecc3259fd01d99fed2b9",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 52,
            "deletions": 50,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -128,6 +128,56 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: boo\n         return embeddings\n \n \n+class InstructBlipVideoQFormerEmbeddings(nn.Module):\n+    \"\"\"Construct the embeddings from word and position embeddings.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n+        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n+\n+        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+\n+        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n+        self.register_buffer(\n+            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n+        )\n+\n+        self.config = config\n+\n+    def forward(\n+        self,\n+        input_ids=None,\n+        position_ids=None,\n+        query_embeds=None,\n+        past_key_values_length=0,\n+    ):\n+        if input_ids is not None:\n+            seq_length = input_ids.size()[1]\n+        else:\n+            seq_length = 0\n+\n+        if position_ids is None:\n+            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length].clone()\n+\n+        if input_ids is not None:\n+            embeddings = self.word_embeddings(input_ids)\n+\n+            position_embeddings = self.position_embeddings(position_ids.to(embeddings.device))\n+            embeddings = embeddings + position_embeddings\n+\n+            if query_embeds is not None:\n+                embeddings = torch.cat((query_embeds, embeddings), dim=1)\n+        else:\n+            embeddings = query_embeds\n+\n+        embeddings = embeddings.to(self.layernorm.weight.dtype)\n+        embeddings = self.layernorm(embeddings)\n+        embeddings = self.dropout(embeddings)\n+        return embeddings\n+\n+\n @auto_docstring\n class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n     config: InstructBlipVideoConfig\n@@ -158,6 +208,8 @@ def _init_weights(self, module):\n             init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n         elif isinstance(module, (InstructBlipVideoForConditionalGeneration, InstructBlipVideoModel)):\n             init.zeros_(module.query_tokens)\n+        elif isinstance(module, InstructBlipVideoQFormerEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n # Adapted from transformers.models.siglip.modeling_siglip.eager_attention_forward -> InstructBlipVideo doesn't cast attn weights to fp32\n@@ -677,56 +729,6 @@ def forward(\n         )\n \n \n-class InstructBlipVideoQFormerEmbeddings(nn.Module):\n-    \"\"\"Construct the embeddings from word and position embeddings.\"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n-        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n-\n-        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-\n-        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.register_buffer(\n-            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n-        )\n-\n-        self.config = config\n-\n-    def forward(\n-        self,\n-        input_ids=None,\n-        position_ids=None,\n-        query_embeds=None,\n-        past_key_values_length=0,\n-    ):\n-        if input_ids is not None:\n-            seq_length = input_ids.size()[1]\n-        else:\n-            seq_length = 0\n-\n-        if position_ids is None:\n-            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length].clone()\n-\n-        if input_ids is not None:\n-            embeddings = self.word_embeddings(input_ids)\n-\n-            position_embeddings = self.position_embeddings(position_ids.to(embeddings.device))\n-            embeddings = embeddings + position_embeddings\n-\n-            if query_embeds is not None:\n-                embeddings = torch.cat((query_embeds, embeddings), dim=1)\n-        else:\n-            embeddings = query_embeds\n-\n-        embeddings = embeddings.to(self.layernorm.weight.dtype)\n-        embeddings = self.layernorm(embeddings)\n-        embeddings = self.dropout(embeddings)\n-        return embeddings\n-\n-\n class InstructBlipVideoQFormerModel(InstructBlipVideoPreTrainedModel):\n     \"\"\"\n     Querying Transformer (Q-Former), used in InstructBlipVideo. Slightly modified from BLIP-2 as it also takes the"
        },
        {
            "sha": "5206eed446973974f7c1a6b726fd200c5f283901",
            "filename": "src/transformers/models/jais2/modeling_jais2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fjais2%2Fmodeling_jais2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fjais2%2Fmodeling_jais2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjais2%2Fmodeling_jais2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -279,7 +279,7 @@ def __init__(self, config: Jais2Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "aaa0ca0e4fcc267bbbbda2282da8a71c8c67316c",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -28,6 +28,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import ClassifierFreeGuidanceLogitsProcessor, GenerationMixin, GenerationMode, LogitsProcessorList\n@@ -58,6 +59,11 @@ class JanusPreTrainedModel(PreTrainedModel):\n \n     _can_compile_fullgraph = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, JanusVisionEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+\n \n @dataclass\n @auto_docstring("
        },
        {
            "sha": "b77c213ddd9fe540def887bdedd667467b1b32e9",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -24,8 +24,7 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from transformers.models.blip.image_processing_blip import BlipImageProcessor\n-\n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...configuration_utils import PreTrainedConfig\n@@ -58,6 +57,7 @@\n     logging,\n )\n from ..auto import CONFIG_MAPPING, AutoConfig, AutoModel\n+from ..blip.image_processing_blip import BlipImageProcessor\n from ..blip_2.modeling_blip_2 import Blip2VisionModel\n from ..chameleon.configuration_chameleon import ChameleonVQVAEConfig\n from ..chameleon.modeling_chameleon import (\n@@ -391,6 +391,11 @@ class JanusPreTrainedModel(PreTrainedModel):\n \n     _can_compile_fullgraph = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, JanusVisionEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+\n \n @dataclass\n @auto_docstring("
        },
        {
            "sha": "b1424298e6b9f72726344c9937f83ecf9b242d94",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -83,7 +83,7 @@ def __init__(self, config: JetMoeConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "84d961104e849d461454a216e3f9284a742fed23",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -559,6 +559,7 @@ class Kosmos2TextSinusoidalPositionalEmbedding(nn.Module):\n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None):\n         super().__init__()\n         self.offset = 2\n+        self.num_positions = num_positions\n         self.embedding_dim = embedding_dim\n         self.padding_idx = padding_idx\n         self.make_weights(num_positions + self.offset, embedding_dim, padding_idx)\n@@ -1138,6 +1139,7 @@ def _init_weights(self, module: nn.Module):\n             init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n             init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n             init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         elif isinstance(module, Kosmos2VisionAttention):\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             out_proj_std = (module.embed_dim**-0.5) * factor\n@@ -1170,6 +1172,11 @@ def _init_weights(self, module: nn.Module):\n         elif isinstance(module, nn.LayerNorm):\n             init.ones_(module.weight)\n             init.zeros_(module.bias)\n+        elif isinstance(module, Kosmos2TextSinusoidalPositionalEmbedding):\n+            emb_weights = module.get_embedding(\n+                module.num_positions + module.offset, module.embedding_dim, module.padding_idx\n+            )\n+            init.copy_(module.weights, emb_weights)\n \n         if isinstance(module, nn.Linear) and module.bias is not None:\n             init.zeros_(module.bias)"
        },
        {
            "sha": "d4c70ccd0e731301eb256fed3df526fb0326b975",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -619,6 +619,7 @@ class Kosmos2_5TextSinusoidalPositionalEmbedding(nn.Module):\n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None):\n         super().__init__()\n         self.offset = 2\n+        self.num_positions = num_positions\n         self.embedding_dim = embedding_dim\n         self.padding_idx = padding_idx\n         self.make_weights(num_positions + self.offset, embedding_dim, padding_idx)\n@@ -1253,6 +1254,11 @@ def _init_weights(self, module):\n                 init.zeros_(module.bias)\n         elif isinstance(module, Kosmos2_5ImageToTextProjection):\n             init.normal_(module.latent_query, mean=0.0, std=1.0)\n+        elif isinstance(module, Kosmos2_5TextSinusoidalPositionalEmbedding):\n+            emb_weights = module.get_embedding(\n+                module.num_positions + module.offset, module.embedding_dim, module.padding_idx\n+            )\n+            init.copy_(module.weights, emb_weights)\n \n \n class Kosmos2_5VisionModel(Kosmos2_5PreTrainedModel):"
        },
        {
            "sha": "c358c0ae8f5829f0be28dcbff578c85c5bb71387",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -112,6 +112,11 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, KyutaiSpeechToTextFlexibleLinear):\n             init.normal_(module.weight)\n+        if isinstance(module, KyutaiSpeechToTextEmbeddings):\n+            audio_tokens_offsets = torch.arange(module.config.num_codebooks) * module.config.codebook_vocab_size\n+            audio_tokens_offsets += module.config.vocab_size\n+            audio_tokens_offsets = nn.functional.pad(audio_tokens_offsets, (1, 0))\n+            init.copy_(module.audio_tokens_offsets, audio_tokens_offsets)\n \n \n class KyutaiSpeechToTextConv1dPaddingCache:\n@@ -202,6 +207,7 @@ def update(self, hidden_states: torch.Tensor, layer_idx: int):\n class KyutaiSpeechToTextEmbeddings(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n+        self.config = config\n         self.embed_tokens = nn.Embedding(\n             config.vocab_size + (config.num_codebooks * config.codebook_vocab_size) + 1,\n             config.hidden_size,\n@@ -277,7 +283,7 @@ def __init__(self, config: KyutaiSpeechToTextConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "c5b561fc71daadfb988bb8a6ac823e89ee30806e",
            "filename": "src/transformers/models/kyutai_speech_to_text/modular_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -20,6 +20,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...feature_extraction_utils import BatchFeature\n from ...generation import GenerationConfig, GenerationMixin\n@@ -213,7 +214,13 @@ def __call__(\n \n \n class KyutaiSpeechToTextPreTrainedModel(MoshiPreTrainedModel):\n-    pass\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, KyutaiSpeechToTextEmbeddings):\n+            audio_tokens_offsets = torch.arange(module.config.num_codebooks) * module.config.codebook_vocab_size\n+            audio_tokens_offsets += module.config.vocab_size\n+            audio_tokens_offsets = nn.functional.pad(audio_tokens_offsets, (1, 0))\n+            init.copy_(module.audio_tokens_offsets, audio_tokens_offsets)\n \n \n class KyutaiSpeechToTextConv1dPaddingCache(MimiConv1dPaddingCache):\n@@ -223,6 +230,7 @@ class KyutaiSpeechToTextConv1dPaddingCache(MimiConv1dPaddingCache):\n class KyutaiSpeechToTextEmbeddings(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n+        self.config = config\n         self.embed_tokens = nn.Embedding(\n             config.vocab_size + (config.num_codebooks * config.codebook_vocab_size) + 1,\n             config.hidden_size,"
        },
        {
            "sha": "c90907a0a765385372f9e1b12569f5180d05c027",
            "filename": "src/transformers/models/lasr/modeling_lasr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flasr%2Fmodeling_lasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flasr%2Fmodeling_lasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flasr%2Fmodeling_lasr.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -84,7 +84,7 @@ def __init__(self, config: LasrEncoderConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "52bfe7f76643e83459334678d5210575817c5b72",
            "filename": "src/transformers/models/layoutlm/modeling_layoutlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -431,6 +431,8 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, LayoutLMLMPredictionHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, LayoutLMEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n @auto_docstring"
        },
        {
            "sha": "f6e14de38d37b09c07df08cfc60aa0330e2213d5",
            "filename": "src/transformers/models/layoutlmv2/modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -467,9 +467,21 @@ def _init_weights(self, module):\n             if self.config.fast_qkv:\n                 init.zeros_(module.q_bias)\n                 init.zeros_(module.v_bias)\n+        elif isinstance(module, LayoutLMv2Embeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+        elif isinstance(module, LayoutLMv2VisualBackbone):\n+            num_channels = len(module.cfg.MODEL.PIXEL_MEAN)\n+            init.copy_(module.pixel_mean, torch.Tensor(module.cfg.MODEL.PIXEL_MEAN).view(num_channels, 1, 1))\n+            init.copy_(module.pixel_std, torch.Tensor(module.cfg.MODEL.PIXEL_STD).view(num_channels, 1, 1))\n         elif isinstance(module, LayoutLMv2Model):\n             if hasattr(module, \"visual_segment_embedding\"):\n                 init.normal_(module.visual_segment_embedding, mean=0.0, std=self.config.initializer_range)\n+        # We check the existence of each one since detectron2 seems to do weird things\n+        elif isinstance(module, detectron2.layers.FrozenBatchNorm2d):\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n+            init.zeros_(module.running_mean)\n+            init.constant_(module.running_var, 1.0 - module.eps)\n \n \n def my_convert_sync_batchnorm(module, process_group=None):"
        },
        {
            "sha": "8c55fa4203596f6b144b067295bc82a0abf1ce0a",
            "filename": "src/transformers/models/layoutlmv3/modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -212,6 +212,8 @@ def _init_weights(self, module):\n             if self.config.visual_embed:\n                 init.zeros_(module.cls_token)\n                 init.zeros_(module.pos_embed)\n+        elif isinstance(module, LayoutLMv3TextEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n class LayoutLMv3SelfAttention(nn.Module):"
        },
        {
            "sha": "17ab07bdf276915a20c4e130d94d827df70f84e1",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -1077,6 +1078,11 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, LEDForConditionalGeneration):\n+            init.zeros_(module.final_logits_bias)\n+\n \n @dataclass\n @auto_docstring("
        },
        {
            "sha": "68d08e8cb1f0a51892ef4fce7fbb79bc2dc795c9",
            "filename": "src/transformers/models/levit/modeling_levit.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...modeling_outputs import (\n     BaseModelOutputWithNoAttention,\n     BaseModelOutputWithPoolingAndNoAttention,\n@@ -165,13 +166,15 @@ def __init__(self, hidden_sizes, key_dim, num_attention_heads, attention_ratio,\n \n         points = list(itertools.product(range(resolution), range(resolution)))\n         len_points = len(points)\n+        self.len_points = len_points\n         attention_offsets, indices = {}, []\n         for p1 in points:\n             for p2 in points:\n                 offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))\n                 if offset not in attention_offsets:\n                     attention_offsets[offset] = len(attention_offsets)\n                 indices.append(attention_offsets[offset])\n+        self.indices = indices\n \n         self.attention_bias_cache = {}\n         self.attention_biases = torch.nn.Parameter(torch.zeros(num_attention_heads, len(attention_offsets)))\n@@ -243,6 +246,8 @@ def __init__(\n         points = list(itertools.product(range(resolution_in), range(resolution_in)))\n         points_ = list(itertools.product(range(resolution_out), range(resolution_out)))\n         len_points, len_points_ = len(points), len(points_)\n+        self.len_points_ = len_points_\n+        self.len_points = len_points\n         attention_offsets, indices = {}, []\n         for p1 in points_:\n             for p2 in points:\n@@ -251,6 +256,7 @@ def __init__(\n                 if offset not in attention_offsets:\n                     attention_offsets[offset] = len(attention_offsets)\n                 indices.append(attention_offsets[offset])\n+        self.indices = indices\n \n         self.attention_biases = torch.nn.Parameter(torch.zeros(num_attention_heads, len(attention_offsets)))\n         self.register_buffer(\n@@ -472,6 +478,18 @@ class LevitPreTrainedModel(PreTrainedModel):\n     input_modalities = (\"image\",)\n     _no_split_modules = [\"LevitResidualLayer\"]\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, LevitAttention):\n+            init.copy_(\n+                module.attention_bias_idxs, torch.LongTensor(module.indices).view(module.len_points, module.len_points)\n+            )\n+        elif isinstance(module, LevitAttentionSubsample):\n+            init.copy_(\n+                module.attention_bias_idxs,\n+                torch.LongTensor(module.indices).view(module.len_points_, module.len_points),\n+            )\n+\n \n @auto_docstring\n class LevitModel(LevitPreTrainedModel):"
        },
        {
            "sha": "5d6d079b776baea0999580b3c5547286b0aa1f14",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -83,7 +83,7 @@ def __init__(self, config: Lfm2Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "c29a8ded468bacd4a30fce0eb6a29dd3d482d04c",
            "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -84,7 +84,7 @@ def __init__(self, config: Lfm2MoeConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -684,6 +684,9 @@ def _init_weights(self, module):\n         if isinstance(module, Lfm2MoeExperts):\n             init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n             init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, Lfm2MoeSparseMoeBlock):\n+            if module.use_expert_bias:\n+                init.zeros_(module.expert_bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "728fff507c0d7f269c06d319a2719cdab33e23c0",
            "filename": "src/transformers/models/lfm2_moe/modular_lfm2_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -168,6 +168,9 @@ def _init_weights(self, module):\n         if isinstance(module, Lfm2MoeExperts):\n             init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n             init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, Lfm2MoeSparseMoeBlock):\n+            if module.use_expert_bias:\n+                init.zeros_(module.expert_bias)\n \n \n class Lfm2MoeModel(MixtralModel):"
        },
        {
            "sha": "ee5350a719d86bf60efe968faa00be3ca99a7e91",
            "filename": "src/transformers/models/lilt/modeling_lilt.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -21,6 +21,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -498,6 +499,11 @@ class LiltPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = []\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, LiltTextEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+\n \n @auto_docstring\n class LiltModel(LiltPreTrainedModel):"
        },
        {
            "sha": "da67f0d9435632ac5fe820f75381613ffb3b280a",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -87,7 +87,7 @@ def __init__(self, config: LlamaConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "420467c7ce60fdcc31b76654b547ef4c808ff988",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -188,7 +188,7 @@ def __init__(self, config: Llama4TextConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "c9bc6d60290f9caafcd3d76d31b9f5b04043654c",
            "filename": "src/transformers/models/longcat_flash/modeling_longcat_flash.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -82,7 +82,7 @@ def __init__(self, config: LongcatFlashConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -563,6 +563,7 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, LongcatFlashTopkRouter):\n             init.normal_(module.classifier.weight, mean=0.0, std=self.config.initializer_range)\n+            init.zeros_(module.e_score_correction_bias)\n         if isinstance(module, LongcatFlashExperts):\n             if module.gate_up_proj is not None:\n                 init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)"
        },
        {
            "sha": "c49e18096a1529b994e9c3b4cbf43470f9c3f1c2",
            "filename": "src/transformers/models/longcat_flash/modular_longcat_flash.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -347,6 +347,7 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, LongcatFlashTopkRouter):\n             init.normal_(module.classifier.weight, mean=0.0, std=self.config.initializer_range)\n+            init.zeros_(module.e_score_correction_bias)\n         if isinstance(module, LongcatFlashExperts):\n             if module.gate_up_proj is not None:\n                 init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)"
        },
        {
            "sha": "dc72c2dfd6a6f483d020c8be7ae2f20e72155138",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -84,6 +85,7 @@ class M2M100SinusoidalPositionalEmbedding(nn.Module):\n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None):\n         super().__init__()\n         self.offset = 2\n+        self.num_positions = num_positions\n         self.embedding_dim = embedding_dim\n         self.padding_idx = padding_idx\n         self.make_weights(num_positions + self.offset, embedding_dim, padding_idx)\n@@ -515,6 +517,14 @@ class M2M100PreTrainedModel(PreTrainedModel):\n     # Doesn't support `compile` (dynamic control flow). Can be fixed but low usage model\n     _can_compile_fullgraph = False\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, M2M100SinusoidalPositionalEmbedding):\n+            emb_weights = module.get_embedding(\n+                module.num_positions + module.offset, module.embedding_dim, module.padding_idx\n+            )\n+            init.copy_(module.weights, emb_weights)\n+\n \n class M2M100Encoder(M2M100PreTrainedModel):\n     \"\"\""
        },
        {
            "sha": "02d3a716bbadab5bc2cde0cf13eb103f1f56fbf8",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -451,6 +451,8 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, MarianSinusoidalPositionalEmbedding):\n             init.copy_(module.weight, module.create_weight())\n+        elif isinstance(module, MarianMTModel):\n+            init.zeros_(module.final_logits_bias)\n \n     @property\n     def dummy_inputs(self):"
        },
        {
            "sha": "859b190b8eab0611909fc5d7b06a2ba0d3db35f8",
            "filename": "src/transformers/models/markuplm/modeling_markuplm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch MarkupLM model.\"\"\"\n \n-import os\n from collections.abc import Callable\n from typing import Optional, Union\n \n@@ -517,10 +516,8 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, MarkupLMLMPredictionHead):\n             init.zeros_(module.bias)\n-\n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n-        return super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n+        elif isinstance(module, MarkupLMEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n @auto_docstring"
        },
        {
            "sha": "af64e17bec1e06cf8078955af04388721c02c096",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -2149,6 +2149,10 @@ def _init_weights(self, module: nn.Module):\n             init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n \n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n             init.ones_(module.weight)\n@@ -2160,6 +2164,11 @@ def _init_weights(self, module: nn.Module):\n             if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n                 init.zeros_(module.weight[module.padding_idx])\n \n+        elif isinstance(module, Mask2FormerLoss):\n+            empty_weight = torch.ones(module.num_labels + 1)\n+            empty_weight[-1] = module.eos_coef\n+            init.copy_(module.empty_weight, empty_weight)\n+\n         if hasattr(module, \"reference_points\"):\n             init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n             init.constant_(module.reference_points.bias, 0.0)"
        },
        {
            "sha": "3b2fb2f4ba9bc863d04dcf09b8af3d792cabf9c9",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -1470,11 +1470,19 @@ def _init_weights(self, module: nn.Module):\n             init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n         elif isinstance(module, nn.Embedding):\n             init.normal_(module.weight, mean=0.0, std=std)\n             # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n             if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n                 init.zeros_(module.weight[module.padding_idx])\n+        elif isinstance(module, MaskFormerLoss):\n+            empty_weight = torch.ones(module.num_labels + 1)\n+            empty_weight[-1] = module.eos_coef\n+            init.copy_(module.empty_weight, empty_weight)\n \n \n @auto_docstring"
        },
        {
            "sha": "5628efca75920b16a2e84acdc9ecd017a989671f",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 16,
            "deletions": 12,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -331,18 +331,7 @@ def __init__(self, config, dim, num_heads, window_size):\n             torch.zeros((2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1), num_heads)\n         )\n \n-        # get pair-wise relative position index for each token inside the window\n-        coords_h = torch.arange(self.window_size[0])\n-        coords_w = torch.arange(self.window_size[1])\n-        coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))\n-        coords_flatten = torch.flatten(coords, 1)\n-        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n-        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n-        relative_coords[:, :, 0] += self.window_size[0] - 1\n-        relative_coords[:, :, 1] += self.window_size[1] - 1\n-        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n-        relative_position_index = relative_coords.sum(-1)\n-        self.register_buffer(\"relative_position_index\", relative_position_index)\n+        self.register_buffer(\"relative_position_index\", self.create_relative_position_index())\n \n         self.query = nn.Linear(self.all_head_size, self.all_head_size, bias=config.qkv_bias)\n         self.key = nn.Linear(self.all_head_size, self.all_head_size, bias=config.qkv_bias)\n@@ -401,6 +390,20 @@ def forward(\n \n         return outputs\n \n+    def create_relative_position_index(self):\n+        # get pair-wise relative position index for each token inside the window\n+        coords_h = torch.arange(self.window_size[0])\n+        coords_w = torch.arange(self.window_size[1])\n+        coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))\n+        coords_flatten = torch.flatten(coords, 1)\n+        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n+        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n+        relative_coords[:, :, 0] += self.window_size[0] - 1\n+        relative_coords[:, :, 1] += self.window_size[1] - 1\n+        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n+        relative_position_index = relative_coords.sum(-1)\n+        return relative_position_index\n+\n \n # Copied from transformers.models.swin.modeling_swin.SwinSelfOutput with Swin->MaskFormerSwin\n class MaskFormerSwinSelfOutput(nn.Module):\n@@ -711,6 +714,7 @@ def _init_weights(self, module):\n                 init.zeros_(module.position_embeddings)\n         elif isinstance(module, MaskFormerSwinSelfAttention):\n             init.zeros_(module.relative_position_bias_table)\n+            init.copy_(module.relative_position_index, module.create_relative_position_index())\n \n \n class MaskFormerSwinModel(MaskFormerSwinPreTrainedModel):"
        },
        {
            "sha": "62ab2a6f3dda577e3e0c37ade40b3c483ed7b4a2",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -478,6 +479,11 @@ class MBartPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _can_compile_fullgraph = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, MBartForConditionalGeneration):\n+            init.zeros_(module.final_logits_bias)\n+\n     @property\n     def dummy_inputs(self):\n         pad_token = self.config.pad_token_id"
        },
        {
            "sha": "0ee100197e37f254d95d41d64ebad50472dbb235",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -528,6 +528,8 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, MegatronBertLMPredictionHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, MegatronBertEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n @dataclass"
        },
        {
            "sha": "d126b860bd09956707b412b79e4d7ff0f882aa59",
            "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -306,11 +306,13 @@ def _init_weights(self, module):\n         if isinstance(module, MetaClip2TextEmbeddings):\n             init.normal_(module.token_embedding.weight, mean=0.0, std=factor * 0.02)\n             init.normal_(module.position_embedding.weight, mean=0.0, std=factor * 0.02)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         elif isinstance(module, MetaClip2VisionEmbeddings):\n             factor = self.config.initializer_factor\n             init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n             init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n             init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         elif isinstance(module, MetaClip2Attention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor"
        },
        {
            "sha": "5d90fbf741be93356b845ef563f3445fd0fea29c",
            "filename": "src/transformers/models/metaclip_2/modular_metaclip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -225,11 +225,13 @@ def _init_weights(self, module):\n         if isinstance(module, MetaClip2TextEmbeddings):\n             init.normal_(module.token_embedding.weight, mean=0.0, std=factor * 0.02)\n             init.normal_(module.position_embedding.weight, mean=0.0, std=factor * 0.02)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         elif isinstance(module, MetaClip2VisionEmbeddings):\n             factor = self.config.initializer_factor\n             init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n             init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n             init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         elif isinstance(module, MetaClip2Attention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor"
        },
        {
            "sha": "d97c63d1cf00ab3f40dc4b9c3958147a4b7a8876",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 22,
            "deletions": 1,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -521,7 +521,7 @@ def __init__(self, config: MimiConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -1404,6 +1404,27 @@ def _init_weights(self, module):\n                 init.uniform_(module.bias, a=-k, b=k)\n         elif isinstance(module, MimiLayerScale):\n             init.constant_(module.scale, self.config.layer_scale_initial_scale)\n+        elif isinstance(module, MimiConv1d):\n+            kernel_size = module.conv.kernel_size[0]\n+            stride = module.conv.stride[0]\n+            dilation = module.conv.dilation[0]\n+            kernel_size = (kernel_size - 1) * dilation + 1\n+            init.constant_(module.stride, stride)\n+            init.constant_(module.kernel_size, kernel_size)\n+            init.constant_(module.padding_total, kernel_size - stride)\n+        elif isinstance(module, MimiEuclideanCodebook):\n+            init.ones_(module.initialized)\n+            init.ones_(module.cluster_usage)\n+            init.zeros_(module.embed_sum)\n+        elif isinstance(module, MimiRotaryEmbedding):\n+            rope_fn = (\n+                ROPE_INIT_FUNCTIONS[module.rope_type]\n+                if module.rope_type != \"default\"\n+                else module.compute_default_rope_parameters\n+            )\n+            buffer_value, _ = rope_fn(module.config)\n+            init.copy_(module.inv_freq, buffer_value)\n+            init.copy_(module.original_inv_freq, buffer_value)\n \n \n @auto_docstring("
        },
        {
            "sha": "a4edd3b351c68d0d09124a067c85478f39a15c06",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -271,7 +271,7 @@ def __init__(self, config: MiniMaxConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -613,6 +613,13 @@ def _init_weights(self, module):\n             init.normal_(module.down_proj, mean=0.0, std=std)\n         elif isinstance(module, MiniMaxTopKRouter):\n             init.normal_(module.weight, mean=0.0, std=std)\n+        if isinstance(module, MiniMaxLightningAttention):\n+            slope_rate = module.get_slope_rate()\n+            query_decay, key_decay, diagonal_decay = module.decay_factors(slope_rate)\n+            init.copy_(module.slope_rate, slope_rate)\n+            init.copy_(module.query_decay, query_decay)\n+            init.copy_(module.key_decay, key_decay)\n+            init.copy_(module.diagonal_decay, diagonal_decay)\n \n \n @auto_docstring"
        },
        {
            "sha": "72a607ab29085b3a8a2b41d9fe812e30e4c3bfa6",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -21,6 +21,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n@@ -527,6 +528,16 @@ class MiniMaxPreTrainedModel(MixtralPreTrainedModel):\n         \"attentions\": [MiniMaxAttention, MiniMaxLightningAttention],\n     }\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, MiniMaxLightningAttention):\n+            slope_rate = module.get_slope_rate()\n+            query_decay, key_decay, diagonal_decay = module.decay_factors(slope_rate)\n+            init.copy_(module.slope_rate, slope_rate)\n+            init.copy_(module.query_decay, query_decay)\n+            init.copy_(module.key_decay, key_decay)\n+            init.copy_(module.diagonal_decay, diagonal_decay)\n+\n \n class MiniMaxModel(MixtralModel):\n     @check_model_inputs"
        },
        {
            "sha": "811cfd562c1e7ab6f95de4348acfa9f24931ce7c",
            "filename": "src/transformers/models/ministral/modeling_ministral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -289,7 +289,7 @@ def __init__(self, config: MinistralConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "7f0db772cd65e0e5bee908327efe0e3928956070",
            "filename": "src/transformers/models/ministral3/modeling_ministral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fministral3%2Fmodeling_ministral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fministral3%2Fmodeling_ministral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral3%2Fmodeling_ministral3.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -295,7 +295,7 @@ def __init__(self, config: Ministral3Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "5a544e3fa29813d101fb92145afa2a12d49c8247",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -285,7 +285,7 @@ def __init__(self, config: MistralConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "b3bc1bb12c2678ab0a2edb0ecfceda7c0c97084e",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -169,7 +169,7 @@ def __init__(self, config: MixtralConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "7fb9fc013c7b9a1316ac39d97862526d3c2d3f15",
            "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -55,6 +55,8 @@ class MLCDRotaryEmbedding(nn.Module):\n \n     def __init__(self, dim: int, theta: float = 10000.0) -> None:\n         super().__init__()\n+        self.dim = dim\n+        self.theta = theta\n         inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n \n@@ -424,6 +426,7 @@ def _init_weights(self, module):\n             factor = self.config.initializer_factor\n             init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n             init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         elif isinstance(module, MLCDAttention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n@@ -447,6 +450,9 @@ def _init_weights(self, module):\n             init.ones_(module.weight)\n         elif isinstance(module, nn.Linear) and module.bias is not None:\n             init.zeros_(module.bias)\n+        elif isinstance(module, MLCDRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n \n \n class MLCDVisionTransformer(nn.Module):"
        },
        {
            "sha": "2e4b7e939644c1c1a3316a1ae89843185946f7c8",
            "filename": "src/transformers/models/mlcd/modular_mlcd.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -363,6 +363,7 @@ def _init_weights(self, module):\n             factor = self.config.initializer_factor\n             init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n             init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         elif isinstance(module, MLCDAttention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n@@ -386,6 +387,9 @@ def _init_weights(self, module):\n             init.ones_(module.weight)\n         elif isinstance(module, nn.Linear) and module.bias is not None:\n             init.zeros_(module.bias)\n+        elif isinstance(module, MLCDRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n \n \n class MLCDVisionTransformer(CLIPVisionTransformer):"
        },
        {
            "sha": "723feacbfc19a93828df653cdc2ed966168b45ae",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -741,7 +741,7 @@ def __init__(self, config: MllamaTextConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -847,6 +847,15 @@ def _init_weights(self, module):\n         elif isinstance(module, MllamaPrecomputedAspectRatioEmbedding):\n             if module.is_gated:\n                 init.zeros_(module.gate)\n+        elif isinstance(module, MllamaRotaryEmbedding):\n+            rope_fn = (\n+                ROPE_INIT_FUNCTIONS[module.rope_type]\n+                if module.rope_type != \"default\"\n+                else module.compute_default_rope_parameters\n+            )\n+            buffer_value, _ = rope_fn(module.config)\n+            init.copy_(module.inv_freq, buffer_value)\n+            init.copy_(module.original_inv_freq, buffer_value)\n \n     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask("
        },
        {
            "sha": "4a2905c2855653e1e240acdd78f560bd4ca5b271",
            "filename": "src/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -552,7 +552,7 @@ def _init_weights(self, module):\n         elif isinstance(module, MMGroundingDinoFusionLayer):\n             init.constant_(module.vision_param, 1e-4)\n             init.constant_(module.text_param, 1e-4)\n-        elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n+        elif isinstance(module, (nn.Linear, nn.Conv2d)):\n             init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n                 init.zeros_(module.bias)"
        },
        {
            "sha": "7b636801b67708a91876b05b370fbc1165b4b3a9",
            "filename": "src/transformers/models/mobilebert/modeling_mobilebert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -556,6 +556,8 @@ def _init_weights(self, module):\n             init.ones_(module.weight)\n         elif isinstance(module, MobileBertLMPredictionHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, MobileBertEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n @dataclass"
        },
        {
            "sha": "83f5d4bae8ec4ce4470182067dd7c806d5ff0811",
            "filename": "src/transformers/models/mobilevit/modeling_mobilevit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -615,6 +615,10 @@ def _init_weights(self, module: nn.Module) -> None:\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n         elif isinstance(module, nn.LayerNorm):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)"
        },
        {
            "sha": "c1007c4cfd73da9008c0b297da37147cbb20d2bc",
            "filename": "src/transformers/models/mobilevitv2/modeling_mobilevitv2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -582,6 +582,10 @@ def _init_weights(self, module: nn.Module) -> None:\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n         elif isinstance(module, nn.GroupNorm):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)"
        },
        {
            "sha": "751b15042681f087f94c8dce8f5cb5476a2965ea",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -268,7 +268,7 @@ def __init__(self, config: ModernBertConfig, device=None):\n                 rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type[layer_type]]\n             curr_inv_freq, curr_attention_scaling = rope_init_fn(self.config, device, layer_type=layer_type)\n             self.register_buffer(f\"{layer_type}_inv_freq\", curr_inv_freq, persistent=False)\n-            setattr(self, f\"{layer_type}_original_inv_freq\", curr_inv_freq)\n+            self.register_buffer(f\"{layer_type}_original_inv_freq\", curr_inv_freq.clone(), persistent=False)\n             setattr(self, f\"{layer_type}_attention_scaling\", curr_attention_scaling)\n \n     @staticmethod\n@@ -677,6 +677,14 @@ def init_weight(module: nn.Module, std: float):\n             init.ones_(module.weight)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n+        elif isinstance(module, ModernBertRotaryEmbedding):\n+            for layer_type in module.layer_types:\n+                rope_init_fn = module.compute_default_rope_parameters\n+                if module.rope_type[layer_type] != \"default\":\n+                    rope_init_fn = ROPE_INIT_FUNCTIONS[module.rope_type[layer_type]]\n+                curr_inv_freq, _ = rope_init_fn(module.config, layer_type=layer_type)\n+                init.copy_(getattr(module, f\"{layer_type}_inv_freq\"), curr_inv_freq)\n+                init.copy_(getattr(module, f\"{layer_type}_original_inv_freq\"), curr_inv_freq)\n \n     def _check_and_adjust_attn_implementation(\n         self, attn_implementation: Optional[str], is_init_check: bool = False"
        },
        {
            "sha": "402872e68a2a05378cac6da5625260b39d035f2d",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -35,7 +35,7 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import RopeParameters\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, RopeParameters\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, is_flash_attn_2_available, logging\n from ...utils.import_utils import is_triton_available\n@@ -871,6 +871,14 @@ def init_weight(module: nn.Module, std: float):\n             init.ones_(module.weight)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n+        elif isinstance(module, ModernBertRotaryEmbedding):\n+            for layer_type in module.layer_types:\n+                rope_init_fn = module.compute_default_rope_parameters\n+                if module.rope_type[layer_type] != \"default\":\n+                    rope_init_fn = ROPE_INIT_FUNCTIONS[module.rope_type[layer_type]]\n+                curr_inv_freq, _ = rope_init_fn(module.config, layer_type=layer_type)\n+                init.copy_(getattr(module, f\"{layer_type}_inv_freq\"), curr_inv_freq)\n+                init.copy_(getattr(module, f\"{layer_type}_original_inv_freq\"), curr_inv_freq)\n \n     def _check_and_adjust_attn_implementation(\n         self, attn_implementation: Optional[str], is_init_check: bool = False"
        },
        {
            "sha": "5cfb3370d54b9abecfd99f3ed13495b7ce8eab4c",
            "filename": "src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -119,7 +119,7 @@ def __init__(self, config: ModernBertDecoderConfig, device=None):\n                 rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type[layer_type]]\n             curr_inv_freq, curr_attention_scaling = rope_init_fn(self.config, device, layer_type=layer_type)\n             self.register_buffer(f\"{layer_type}_inv_freq\", curr_inv_freq, persistent=False)\n-            setattr(self, f\"{layer_type}_original_inv_freq\", curr_inv_freq)\n+            self.register_buffer(f\"{layer_type}_original_inv_freq\", curr_inv_freq.clone(), persistent=False)\n             setattr(self, f\"{layer_type}_attention_scaling\", curr_attention_scaling)\n \n     @staticmethod\n@@ -443,6 +443,14 @@ def init_weight(module: nn.Module, std: float):\n             init.ones_(module.weight)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n+        elif isinstance(module, ModernBertDecoderRotaryEmbedding):\n+            for layer_type in module.layer_types:\n+                rope_init_fn = module.compute_default_rope_parameters\n+                if module.rope_type[layer_type] != \"default\":\n+                    rope_init_fn = ROPE_INIT_FUNCTIONS[module.rope_type[layer_type]]\n+                curr_inv_freq, _ = rope_init_fn(module.config, layer_type=layer_type)\n+                init.copy_(getattr(module, f\"{layer_type}_inv_freq\"), curr_inv_freq)\n+                init.copy_(getattr(module, f\"{layer_type}_original_inv_freq\"), curr_inv_freq)\n \n \n @auto_docstring"
        },
        {
            "sha": "39954aa5ad0432f5427b006307f1c684c720cfd2",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -28,7 +28,7 @@\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n-from ...modeling_rope_utils import RopeParameters\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, RopeParameters\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n@@ -482,6 +482,14 @@ def init_weight(module: nn.Module, std: float):\n             init.ones_(module.weight)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n+        elif isinstance(module, ModernBertDecoderRotaryEmbedding):\n+            for layer_type in module.layer_types:\n+                rope_init_fn = module.compute_default_rope_parameters\n+                if module.rope_type[layer_type] != \"default\":\n+                    rope_init_fn = ROPE_INIT_FUNCTIONS[module.rope_type[layer_type]]\n+                curr_inv_freq, _ = rope_init_fn(module.config, layer_type=layer_type)\n+                init.copy_(getattr(module, f\"{layer_type}_inv_freq\"), curr_inv_freq)\n+                init.copy_(getattr(module, f\"{layer_type}_original_inv_freq\"), curr_inv_freq)\n \n     def _check_and_adjust_attn_implementation(self, attn_implementation, is_init_check):\n         raise AttributeError(\"No need to inherit!\")"
        },
        {
            "sha": "c518343bea4dc5832a69ff4ead3aa58bb7ae74b1",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -98,7 +98,7 @@ def __init__(self, config: MoonshineConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "0721a715d4e8e55cd81260808ab8765aacc63c28",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -289,7 +289,7 @@ def __init__(self, config: MoshiConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "b5a80f86a627414359d549c710c828fa75233724",
            "filename": "src/transformers/models/mpnet/modeling_mpnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -52,6 +52,8 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, MPNetLMHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, MPNetEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n class MPNetEmbeddings(nn.Module):"
        },
        {
            "sha": "38e674cf71d193c3733a7a355f96f961d1107c30",
            "filename": "src/transformers/models/mra/modeling_mra.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -796,6 +796,9 @@ def _init_weights(self, module: nn.Module):\n         super()._init_weights(module)\n         if isinstance(module, MraLMPredictionHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, MraEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)) + 2)\n+            init.zeros_(module.token_type_ids)\n \n \n @auto_docstring"
        },
        {
            "sha": "485489f01aad63bdc5a002b7bbdf767443730103",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -117,6 +117,7 @@ class MusicgenSinusoidalPositionalEmbedding(nn.Module):\n     def __init__(self, num_positions: int, embedding_dim: int):\n         super().__init__()\n         self.embedding_dim = embedding_dim\n+        self.num_positions = num_positions\n         self.make_weights(num_positions, embedding_dim)\n \n     def make_weights(self, num_embeddings: int, embedding_dim: int):\n@@ -432,6 +433,9 @@ def _init_weights(self, module):\n             # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n             if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n                 init.zeros_(module.weight[module.padding_idx])\n+        elif isinstance(module, MusicgenSinusoidalPositionalEmbedding):\n+            emb_weights = module.get_embedding(module.num_positions, module.embedding_dim)\n+            init.copy_(module.weights, emb_weights)\n \n \n class MusicgenDecoder(MusicgenPreTrainedModel):"
        },
        {
            "sha": "9ba2376523ae4fa62cee50b5ca8bc4b138088eb4",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -122,6 +122,7 @@ class MusicgenMelodySinusoidalPositionalEmbedding(nn.Module):\n     def __init__(self, num_positions: int, embedding_dim: int):\n         super().__init__()\n         self.embedding_dim = embedding_dim\n+        self.num_positions = num_positions\n         self.make_weights(num_positions, embedding_dim)\n \n     def make_weights(self, num_embeddings: int, embedding_dim: int):\n@@ -403,6 +404,9 @@ def _init_weights(self, module):\n             # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n             if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n                 init.zeros_(module.weight[module.padding_idx])\n+        elif isinstance(module, MusicgenMelodySinusoidalPositionalEmbedding):\n+            emb_weights = module.get_embedding(module.num_positions, module.embedding_dim)\n+            init.copy_(module.weights, emb_weights)\n \n \n # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder with MUSICGEN->MUSICGEN_MELODY,Musicgen->MusicgenMelody"
        },
        {
            "sha": "f2fdb8d6e7497f8b8871d029e50b484e8b17bc9b",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -21,6 +21,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -469,6 +470,11 @@ class MvpPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, MvpForConditionalGeneration):\n+            init.zeros_(module.final_logits_bias)\n+\n     @property\n     def dummy_inputs(self):\n         pad_token = self.config.pad_token_id"
        },
        {
            "sha": "4777ac8bcb5cf32d66b54286d9d8da08b3b1fddb",
            "filename": "src/transformers/models/nanochat/modeling_nanochat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodeling_nanochat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodeling_nanochat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodeling_nanochat.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -74,7 +74,7 @@ def __init__(self, config: NanoChatConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "e7a2cd99cc3f978522520c27c3a3504d5b56630a",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -110,7 +110,7 @@ def __init__(self, config: NemotronConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     # Ignore copy"
        },
        {
            "sha": "c5dd60956c70720eddd5c5fd9c40735860b1bd5f",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -21,6 +21,7 @@\n import torch.nn as nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -66,6 +67,7 @@ class NllbMoeSinusoidalPositionalEmbedding(nn.Module):\n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None):\n         super().__init__()\n         self.offset = 2\n+        self.num_positions = num_positions\n         self.embedding_dim = embedding_dim\n         self.padding_idx = padding_idx\n         self.make_weights(num_positions + self.offset, embedding_dim, padding_idx)\n@@ -665,6 +667,14 @@ class NllbMoePreTrainedModel(PreTrainedModel):\n     _supports_sdpa = False\n     _supports_flex_attn = False\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, NllbMoeSinusoidalPositionalEmbedding):\n+            emb_weights = module.get_embedding(\n+                module.num_positions + module.offset, module.embedding_dim, module.padding_idx\n+            )\n+            init.copy_(module.weights, emb_weights)\n+\n \n class NllbMoeEncoder(NllbMoePreTrainedModel):\n     _can_record_outputs = {"
        },
        {
            "sha": "5f9775a57d94e6c9233c7613b9995e4e99a5c1a3",
            "filename": "src/transformers/models/nystromformer/modeling_nystromformer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -21,6 +21,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -413,6 +414,12 @@ class NystromformerPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"nystromformer\"\n     supports_gradient_checkpointing = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, NystromformerEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)) + 2)\n+            init.zeros_(module.token_type_ids)\n+\n \n @auto_docstring\n class NystromformerModel(NystromformerPreTrainedModel):"
        },
        {
            "sha": "930342d34cb640949a7ec4d0ddd52197ce55ae4d",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -93,7 +93,7 @@ def __init__(self, config: OlmoConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "ae8410830db8fe1c34edbfc9336cc7f8e1512c81",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -85,7 +85,7 @@ def __init__(self, config: Olmo2Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "5a95424eefab8100e23864f0a9e808d19f6a9578",
            "filename": "src/transformers/models/olmo3/modeling_olmo3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -293,7 +293,7 @@ def __init__(self, config: Olmo3Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "8983eb08ee4246924f18e6664f03a59bd8933f0e",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -77,7 +77,7 @@ def __init__(self, config: OlmoeConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "5c22d19134f2d6f470626b776e7939c34ea7da0a",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -1022,6 +1022,10 @@ def linear_init_(module_to_init):\n         elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d)):\n             init.ones_(module.weight)\n             init.zeros_(module.bias)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n \n     def _set_gradient_checkpointing(self, module, value=False):\n         if isinstance(module, OmDetTurboDecoder):"
        },
        {
            "sha": "4904c2362095e1a834f308bb9a570157a773668a",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 38,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -935,44 +935,6 @@ class OneFormerForUniversalSegmentationOutput(ModelOutput):\n     attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n \n \n-# Modified from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrFrozenBatchNorm2d with DeformableDetr->OneFormerPixelDecoder\n-class OneFormerPixelDecoderFrozenBatchNorm2d(nn.Module):\n-    \"\"\"\n-    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n-\n-    Copy-paste from torchvision.misc.ops with added eps before rqsrt, without which any other models than\n-    torchvision.models.resnet[18,34,50,101] produce nans.\n-    \"\"\"\n-\n-    def __init__(self, n):\n-        super().__init__()\n-        self.register_buffer(\"weight\", torch.ones(n))\n-        self.register_buffer(\"bias\", torch.zeros(n))\n-        self.register_buffer(\"running_mean\", torch.zeros(n))\n-        self.register_buffer(\"running_var\", torch.ones(n))\n-\n-    def _load_from_state_dict(\n-        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n-    ):\n-        num_batches_tracked_key = prefix + \"num_batches_tracked\"\n-        if num_batches_tracked_key in state_dict:\n-            del state_dict[num_batches_tracked_key]\n-\n-        super()._load_from_state_dict(\n-            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n-        )\n-\n-    def forward(self, x):\n-        weight = self.weight.reshape(1, -1, 1, 1)\n-        bias = self.bias.reshape(1, -1, 1, 1)\n-        running_var = self.running_var.reshape(1, -1, 1, 1)\n-        running_mean = self.running_mean.reshape(1, -1, 1, 1)\n-        epsilon = 1e-5\n-        scale = weight * (running_var + epsilon).rsqrt()\n-        bias = bias - running_mean * scale\n-        return x * scale + bias\n-\n-\n # Modified from transformers.models.detr.modeling_deformable_detr.DeformableDetrMultiscaleDeformableAttention with DeformableDetr->OneFormerPixelDecoderEncoder\n class OneFormerPixelDecoderEncoderMultiscaleDeformableAttention(nn.Module):\n     \"\"\"\n@@ -2833,6 +2795,10 @@ def _init_weights(self, module: nn.Module):\n             init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n             init.ones_(module.weight)\n             init.zeros_(module.bias)\n@@ -2843,6 +2809,9 @@ def _init_weights(self, module: nn.Module):\n                 init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, OneFormerLoss):\n             init.constant_(module.logit_scale, np.log(1 / self.config.contrastive_temperature))\n+            empty_weight = torch.ones(module.num_classes + 1)\n+            empty_weight[-1] = module.eos_coef\n+            init.copy_(module.empty_weight, empty_weight)\n \n \n @auto_docstring"
        },
        {
            "sha": "ba4085866ffabb2c7e88772ac102cd33fb3fdaa1",
            "filename": "src/transformers/models/openai/modeling_openai.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -24,6 +24,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import gelu_new, get_activation, silu\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n@@ -46,6 +47,7 @@\n class Attention(nn.Module):\n     def __init__(self, nx, n_positions, config, scale=False):\n         super().__init__()\n+        self.n_positions = n_positions\n         n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n         if n_state % config.n_head != 0:\n             raise ValueError(f\"Attention n_state shape: {n_state} must be divisible by config.n_head {config.n_head}\")\n@@ -259,6 +261,16 @@ class OpenAIGPTPreTrainedModel(PreTrainedModel):\n     config: OpenAIGPTConfig\n     base_model_prefix = \"transformer\"\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, Attention):\n+            n_positions = module.n_positions\n+            init.copy_(\n+                module.bias, torch.tril(torch.ones(n_positions, n_positions)).view(1, 1, n_positions, n_positions)\n+            )\n+        elif isinstance(module, OpenAIGPTModel):\n+            init.copy_(module.position_ids, torch.arange(module.config.n_positions))\n+\n \n @dataclass\n @auto_docstring("
        },
        {
            "sha": "2287f4cfee2c079e3dcaa1760adc3c3290507e44",
            "filename": "src/transformers/models/ovis2/modeling_ovis2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -27,6 +27,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n@@ -430,6 +431,11 @@ class Ovis2PreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = True\n     _supports_attention_backend = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, Ovis2VisionEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+\n \n def hard_softmax(logits: torch.Tensor, dim: int):\n     y_soft = logits.softmax(dim)"
        },
        {
            "sha": "b1260e19bff2016c56047539fda38a11e6c1ef5f",
            "filename": "src/transformers/models/ovis2/modular_ovis2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -19,6 +19,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput\n@@ -159,6 +160,11 @@ class Ovis2PreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = True\n     _supports_attention_backend = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, Ovis2VisionEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+\n \n class Ovis2VisionModel(Ovis2PreTrainedModel):\n     config: Ovis2VisionConfig"
        },
        {
            "sha": "a9fe5f625a3ad2b6b7acc9c07ba2350d78ac05fd",
            "filename": "src/transformers/models/owlv2/modeling_owlv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -575,10 +575,12 @@ def _init_weights(self, module: nn.Module):\n         if isinstance(module, Owlv2TextEmbeddings):\n             init.normal_(module.token_embedding.weight, mean=0.0, std=factor * 0.02)\n             init.normal_(module.position_embedding.weight, mean=0.0, std=factor * 0.02)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         elif isinstance(module, Owlv2VisionEmbeddings):\n             init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n             init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n             init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         elif isinstance(module, Owlv2Attention):\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             out_proj_std = (module.embed_dim**-0.5) * factor"
        },
        {
            "sha": "df9a921eae4c42a07161bdb5f5685ee164014057",
            "filename": "src/transformers/models/owlvit/modeling_owlvit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -562,10 +562,12 @@ def _init_weights(self, module: nn.Module):\n         if isinstance(module, OwlViTTextEmbeddings):\n             init.normal_(module.token_embedding.weight, mean=0.0, std=factor * 0.02)\n             init.normal_(module.position_embedding.weight, mean=0.0, std=factor * 0.02)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         elif isinstance(module, OwlViTVisionEmbeddings):\n             init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n             init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n             init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         elif isinstance(module, OwlViTAttention):\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             out_proj_std = (module.embed_dim**-0.5) * factor"
        },
        {
            "sha": "add3fce7809a3b14b8499b13c26db75dafeb43e8",
            "filename": "src/transformers/models/paddleocr_vl/modeling_paddleocr_vl.py",
            "status": "modified",
            "additions": 23,
            "deletions": 13,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fmodeling_paddleocr_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fmodeling_paddleocr_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fmodeling_paddleocr_vl.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -30,6 +30,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN, GELUActivation\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -90,6 +91,8 @@ class PaddleOCRVisionRotaryEmbedding(nn.Module):\n \n     def __init__(self, dim: int, theta: float = 10000.0) -> None:\n         super().__init__()\n+        self.dim = dim\n+        self.theta = theta\n         inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n \n@@ -116,7 +119,7 @@ def __init__(self, config: PaddleOCRVLConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -444,6 +447,14 @@ class PaddleOCRVLPreTrainedModel(PreTrainedModel):\n         \"attentions\": PaddleOCRAttention,\n     }\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, PaddleOCRVisionEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+        elif isinstance(module, PaddleOCRVisionRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n+\n \n @auto_docstring\n class PaddleOCRTextModel(PaddleOCRVLPreTrainedModel):\n@@ -859,18 +870,17 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         image_grid_thw: Optional[list[Union[tuple[int, int, int], list[tuple[int, int, int]]]]] = None,\n     ) -> BaseModelOutput:\n-        \"\"\"\n-        Args:\n-            inputs_embeds (`torch.FloatTensor` of shape `(sequence_length, hidden_size)`, *optional*):\n-                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-                than the model's internal embedding lookup matrix.\n-            cu_seqlens (`torch.Tensor` of shape `(num_images + 1,)`):\n-                The cumulative sequence lengths of each image or video feature.\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                The attention_mask used in forward function shape [batch_size X sequence_length] if not None.\n-            image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n-                The temporal, height and width of feature shape of each image in LLM.\n+        r\"\"\"\n+        inputs_embeds (`torch.FloatTensor` of shape `(sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n+            This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n+            than the model's internal embedding lookup matrix.\n+        cu_seqlens (`torch.Tensor` of shape `(num_images + 1,)`):\n+            The cumulative sequence lengths of each image or video feature.\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            The attention_mask used in forward function shape [batch_size X sequence_length] if not None.\n+        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each image in LLM.\n         \"\"\"\n         device = inputs_embeds.device\n         hidden_states = inputs_embeds"
        },
        {
            "sha": "a28acb5204f963afad6837aa492770ac2eafb0f2",
            "filename": "src/transformers/models/paddleocr_vl/modular_paddleocr_vl.py",
            "status": "modified",
            "additions": 20,
            "deletions": 12,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fmodular_paddleocr_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fmodular_paddleocr_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fmodular_paddleocr_vl.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -25,6 +25,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import GELUActivation\n from ...cache_utils import Cache, DynamicCache\n from ...image_processing_utils import BatchFeature\n@@ -776,6 +777,14 @@ class PaddleOCRVLPreTrainedModel(PreTrainedModel):\n         \"attentions\": PaddleOCRAttention,\n     }\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, PaddleOCRVisionEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+        elif isinstance(module, PaddleOCRVisionRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n+\n \n class PaddleOCRTextModel(PaddleOCRVLPreTrainedModel, Ernie4_5Model):\n     def __init__(self, config: PaddleOCRTextConfig):\n@@ -977,18 +986,17 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         image_grid_thw: Optional[list[Union[tuple[int, int, int], list[tuple[int, int, int]]]]] = None,\n     ) -> BaseModelOutput:\n-        \"\"\"\n-        Args:\n-            inputs_embeds (`torch.FloatTensor` of shape `(sequence_length, hidden_size)`, *optional*):\n-                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-                than the model's internal embedding lookup matrix.\n-            cu_seqlens (`torch.Tensor` of shape `(num_images + 1,)`):\n-                The cumulative sequence lengths of each image or video feature.\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                The attention_mask used in forward function shape [batch_size X sequence_length] if not None.\n-            image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n-                The temporal, height and width of feature shape of each image in LLM.\n+        r\"\"\"\n+        inputs_embeds (`torch.FloatTensor` of shape `(sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n+            This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n+            than the model's internal embedding lookup matrix.\n+        cu_seqlens (`torch.Tensor` of shape `(num_images + 1,)`):\n+            The cumulative sequence lengths of each image or video feature.\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            The attention_mask used in forward function shape [batch_size X sequence_length] if not None.\n+        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each image in LLM.\n         \"\"\"\n         device = inputs_embeds.device\n         hidden_states = inputs_embeds"
        },
        {
            "sha": "0c3740f12f50c40174e2af487111e7ff1ec4a8b1",
            "filename": "src/transformers/models/parakeet/modeling_parakeet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -510,6 +510,11 @@ def _init_weights(self, module):\n             # Initialize positional bias parameters\n             init.normal_(module.bias_u, mean=0.0, std=std)\n             init.normal_(module.bias_v, mean=0.0, std=std)\n+        elif isinstance(module, ParakeetEncoderRelPositionalEncoding):\n+            inv_freq = 1.0 / (\n+                10000.0 ** (torch.arange(0, self.config.hidden_size, 2, dtype=torch.int64) / self.config.hidden_size)\n+            )\n+            init.copy_(module.inv_freq, inv_freq)\n \n     def _get_subsampling_output_length(self, input_lengths: torch.Tensor):\n         encoder_config = self.config.encoder_config if isinstance(self.config, ParakeetCTCConfig) else self.config"
        },
        {
            "sha": "b0a9ec883300209eba5623d1f9e3b77228f69100",
            "filename": "src/transformers/models/parakeet/modular_parakeet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -346,6 +346,11 @@ def _init_weights(self, module):\n             # Initialize positional bias parameters\n             init.normal_(module.bias_u, mean=0.0, std=std)\n             init.normal_(module.bias_v, mean=0.0, std=std)\n+        elif isinstance(module, ParakeetEncoderRelPositionalEncoding):\n+            inv_freq = 1.0 / (\n+                10000.0 ** (torch.arange(0, self.config.hidden_size, 2, dtype=torch.int64) / self.config.hidden_size)\n+            )\n+            init.copy_(module.inv_freq, inv_freq)\n \n     def _get_subsampling_output_length(self, input_lengths: torch.Tensor):\n         encoder_config = self.config.encoder_config if isinstance(self.config, ParakeetCTCConfig) else self.config"
        },
        {
            "sha": "331810235e48538dbe14f4168f814cb3b089b61f",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -696,6 +696,10 @@ def _init_weights(self, module):\n         elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n         elif isinstance(module, PatchTSMixerBatchNorm):\n             init.zeros_(module.batchnorm.bias)\n             init.ones_(module.batchnorm.weight)"
        },
        {
            "sha": "14b2b9cfcda12c118660a4eda32eeb76718ea6e2",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -584,12 +584,13 @@ def _init_weights(self, module: nn.Module):\n                         init.copy_(module.position_enc, position_enc)\n             else:\n                 init.copy_(module.position_enc, position_enc)\n-        elif isinstance(module, nn.LayerNorm):\n+        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n-        elif isinstance(module, PatchTSTBatchNorm):\n-            init.zeros_(module.batchnorm.bias)\n-            init.ones_(module.batchnorm.weight)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n         elif isinstance(module, nn.Linear):\n             init.normal_(module.weight, mean=0.0, std=self.config.init_std)\n             if module.bias is not None:"
        },
        {
            "sha": "57c4fcba1920ca423063d06289687178d2d0f75b",
            "filename": "src/transformers/models/pe_audio/modeling_pe_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpe_audio%2Fmodeling_pe_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpe_audio%2Fmodeling_pe_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpe_audio%2Fmodeling_pe_audio.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -524,7 +524,7 @@ def _init_weights(self, module):\n \n         if isinstance(module, PeAudioEncoderPatchEmbedder):\n             embed_dim = module.class_embedding.shape[-1]\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=embed_dim**-0.5 * std)\n+            init.normal_(module.class_embedding, mean=0.0, std=embed_dim**-0.5 * std)\n         if isinstance(module, nn.Conv1d):\n             init.trunc_normal_(module.weight, std=0.02)\n             init.constant_(module.bias, 0)\n@@ -564,7 +564,7 @@ def __init__(self, config: PeAudioEncoderConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "c857c8bbc602bc99b5eb8d3c71c9a469c747cae6",
            "filename": "src/transformers/models/pe_audio_video/modeling_pe_audio_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpe_audio_video%2Fmodeling_pe_audio_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpe_audio_video%2Fmodeling_pe_audio_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpe_audio_video%2Fmodeling_pe_audio_video.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -25,6 +25,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n@@ -465,7 +466,7 @@ def __init__(self, config: PeAudioVideoEncoderConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -542,7 +543,7 @@ def _init_weights(self, module):\n \n         if isinstance(module, PeAudioVideoEncoderPatchEmbedder):\n             embed_dim = module.class_embedding.shape[-1]\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=embed_dim**-0.5 * std)\n+            init.normal_(module.class_embedding, mean=0.0, std=embed_dim**-0.5 * std)\n \n \n @dataclass"
        },
        {
            "sha": "330d7d1d426564353c13570012871b363135872b",
            "filename": "src/transformers/models/pe_audio_video/modular_pe_audio_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpe_audio_video%2Fmodular_pe_audio_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpe_audio_video%2Fmodular_pe_audio_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpe_audio_video%2Fmodular_pe_audio_video.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -19,6 +19,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_outputs import BaseModelOutputWithPooling, MaskedLMOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel, eager_attention_forward\n@@ -331,7 +332,7 @@ def _init_weights(self, module):\n \n         if isinstance(module, PeAudioVideoEncoderPatchEmbedder):\n             embed_dim = module.class_embedding.shape[-1]\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=embed_dim**-0.5 * std)\n+            init.normal_(module.class_embedding, mean=0.0, std=embed_dim**-0.5 * std)\n \n \n @dataclass"
        },
        {
            "sha": "65ccf45af24aef14732f7efa8494e91a9dae3bb8",
            "filename": "src/transformers/models/pe_video/modeling_pe_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpe_video%2Fmodeling_pe_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpe_video%2Fmodeling_pe_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpe_video%2Fmodeling_pe_video.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -26,6 +26,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n@@ -427,7 +428,7 @@ def _init_weights(self, module):\n \n         if isinstance(module, PeVideoEncoderPatchEmbedder):\n             embed_dim = module.class_embedding.shape[-1]\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=embed_dim**-0.5 * std)\n+            init.normal_(module.class_embedding, mean=0.0, std=embed_dim**-0.5 * std)\n \n \n class PeVideoEncoderRotaryEmbedding(nn.Module):\n@@ -447,7 +448,7 @@ def __init__(self, config: PeVideoEncoderConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "bde3aeb06d6837d13426272827ac0be255048e90",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -443,6 +443,8 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, PegasusSinusoidalPositionalEmbedding):\n             init.copy_(module.weight, module.create_weight())\n+        elif isinstance(module, PegasusForConditionalGeneration):\n+            init.zeros_(module.final_logits_bias)\n \n \n class PegasusEncoder(PegasusPreTrainedModel):"
        },
        {
            "sha": "98a68764ad8a39157e4417ca78ef22563eced481",
            "filename": "src/transformers/models/perceiver/modeling_perceiver.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -551,9 +551,13 @@ def _init_weights(self, module):\n             # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n             if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n                 init.zeros_(module.weight[module.padding_idx])\n-        elif isinstance(module, nn.LayerNorm):\n+        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d)):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n \n \n @auto_docstring("
        },
        {
            "sha": "2940b2a6f6b11e0a8a27af5896ddf812900849e2",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -77,7 +77,7 @@ def __init__(self, config: PersimmonConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     # Ignore copy"
        },
        {
            "sha": "9839f30af844d002f99e124a75ed5f0cbbce5c22",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -49,7 +49,7 @@ def __init__(self, config: PhiConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "e60b0980e0235c6c46762ecc7df02ca4d0256428",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -83,7 +83,7 @@ def __init__(self, config: Phi3Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "b39c3c284cd9039a69686ebe799a0f28268affa2",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -881,6 +881,9 @@ def _init_weights(self, module):\n         if isinstance(module, Phi4MultimodalAudioGluPointWiseConv):\n             init.zeros_(module.b1)\n             init.zeros_(module.b2)\n+        elif isinstance(module, Phi4MultimodalAudioMeanVarianceNormLayer):\n+            init.zeros_(module.global_mean)\n+            init.ones_(module.global_invstd)\n \n \n def unfold_tensor(tensor, max_seq_len):\n@@ -1459,7 +1462,7 @@ def __init__(self, config: Phi4MultimodalConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "00e8fbef7123da60173fcaa19b1e2fcd03d72970",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -1123,6 +1123,9 @@ def _init_weights(self, module):\n         if isinstance(module, Phi4MultimodalAudioGluPointWiseConv):\n             init.zeros_(module.b1)\n             init.zeros_(module.b2)\n+        elif isinstance(module, Phi4MultimodalAudioMeanVarianceNormLayer):\n+            init.zeros_(module.global_mean)\n+            init.ones_(module.global_invstd)\n \n \n class Phi4MultimodalAudioModel(Phi4MultimodalAudioPreTrainedModel):"
        },
        {
            "sha": "037208c22e03ab78ddde635656a85f6dd2f7b13b",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -59,7 +59,7 @@ def __init__(self, config: PhimoeConfig, device=None):\n         inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "fb5da07912541073ce130cdcd97bd11bfa9e6d11",
            "filename": "src/transformers/models/phimoe/modular_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodular_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodular_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodular_phimoe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -52,7 +52,7 @@ def __init__(self, config: PhimoeConfig, device=None):\n         inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     def forward(self, x, position_ids=None, layer_type=None):\n         if layer_type is not None:"
        },
        {
            "sha": "bcca1537d96482f5562f6f59141840955c44e8bb",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -74,7 +74,7 @@ def __init__(self, config: PixtralVisionConfig, device=None, layer_type=None):\n \n         inv_freq, attention_scaling = rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "c876513f1cc8500062937c2fe05dd8d4185f52d2",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -27,6 +27,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -73,6 +74,11 @@ class PLBartPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, PLBartForConditionalGeneration):\n+            init.zeros_(module.final_logits_bias)\n+\n \n class PLBartLearnedPositionalEmbedding(nn.Embedding):\n     \"\"\""
        },
        {
            "sha": "919d419e632fbf8d1852389b3788bd1d3012a049",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -21,6 +21,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_outputs import (\n@@ -56,6 +57,11 @@ class PLBartPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, PLBartForConditionalGeneration):\n+            init.zeros_(module.final_logits_bias)\n+\n \n class PLBartEncoder(BartEncoder):\n     pass"
        },
        {
            "sha": "34494f2c55b96802cee19077bda63aaafea229ce",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -64,7 +64,7 @@ def __init__(self, config: Qwen2Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "d2871dda9bbf123d2242167d8f421d6261e41bac",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 88,
            "deletions": 62,
            "changes": 150,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -31,6 +31,7 @@\n from torch import nn\n from torch.nn import Parameter\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -62,6 +63,52 @@\n logger = logging.get_logger(__name__)\n \n \n+def kaiser_sinc_filter1d(cutoff, half_width, kernel_size):\n+    \"\"\"Generates a 1D Kaiser-windowed sinc filter.\n+\n+    Args:\n+        cutoff (float): Normalized cutoff frequency (0 to 0.5).\n+        half_width (float): Transition bandwidth.\n+        kernel_size (int): Number of filter taps.\n+\n+    Returns:\n+        torch.Tensor: A tensor of shape (1, 1, kernel_size) representing the filter.\n+    \"\"\"\n+    is_even = kernel_size % 2 == 0\n+    half_size = kernel_size // 2\n+\n+    # Compute Kaiser window parameters\n+    delta_f = 4 * half_width\n+    attenuation = 2.285 * (half_size - 1) * math.pi * delta_f + 7.95\n+\n+    if attenuation > 50.0:\n+        beta = 0.1102 * (attenuation - 8.7)\n+    elif attenuation >= 21.0:\n+        beta = 0.5842 * (attenuation - 21) ** 0.4 + 0.07886 * (attenuation - 21.0)\n+    else:\n+        beta = 0.0\n+\n+    kaiser_window = torch.kaiser_window(kernel_size, beta=beta, periodic=False, dtype=torch.float32)\n+\n+    # Compute time indices\n+    if is_even:\n+        time_indices = torch.arange(-half_size, half_size) + 0.5\n+    else:\n+        time_indices = torch.arange(kernel_size) - half_size\n+\n+    # Compute sinc filter\n+    if cutoff == 0:\n+        return torch.zeros((1, 1, kernel_size), dtype=torch.float32)  # Ensures correct shape\n+\n+    sinc_filter = torch.sinc(2 * cutoff * time_indices)\n+    normalized_filter = 2 * cutoff * kaiser_window * sinc_filter\n+\n+    # Normalize to ensure sum = 1 (avoid leakage of constant component)\n+    normalized_filter /= normalized_filter.sum()\n+\n+    return normalized_filter.view(1, 1, kernel_size)\n+\n+\n @auto_docstring\n class Qwen2_5OmniPreTrainedModel(PreTrainedModel):\n     config: Qwen2_5OmniConfig\n@@ -75,6 +122,23 @@ class Qwen2_5OmniPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = False\n     _supports_attention_backend = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, SinusoidsPositionEmbedding):\n+            log_timescale_increment = np.log(module.max_timescale) / (module.channels // 2 - 1)\n+            inv_timescales = torch.exp(-log_timescale_increment * torch.arange(module.channels // 2).float())\n+            scaled_time = torch.arange(module.length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n+            init.copy_(module.positional_embedding, torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1))\n+        elif isinstance(module, UpSample1d):\n+            filter_tensor = kaiser_sinc_filter1d(0.5 / module.ratio, 0.6 / module.ratio, module.kernel_size)\n+            init.copy_(module.filter, filter_tensor)\n+        elif isinstance(module, DownSample1d):\n+            filter_tensor = kaiser_sinc_filter1d(module.cutoff, module.half_width, module.kernel_size)\n+            init.copy_(module.filter, filter_tensor)\n+        elif isinstance(module, Qwen2_5_VisionRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n+\n \n class Qwen2_5OmniPreTrainedModelForConditionalGeneration(Qwen2_5OmniPreTrainedModel):\n     input_modalities = (\"image\", \"video\", \"audio\", \"text\")\n@@ -686,6 +750,9 @@ def forward(\n class SinusoidsPositionEmbedding(nn.Module):\n     def __init__(self, length, channels, max_timescale=10000):\n         super().__init__()\n+        self.length = length\n+        self.channels = channels\n+        self.max_timescale = max_timescale\n         if channels % 2 != 0:\n             raise ValueError(\"SinusoidsPositionEmbedding needs even channels input\")\n         log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n@@ -1018,6 +1085,22 @@ def forward(\n         return hidden_states\n \n \n+class Qwen2_5_VisionRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, dim: int, theta: float = 10000.0) -> None:\n+        super().__init__()\n+        self.dim = dim\n+        self.theta = theta\n+        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+\n+    def forward(self, seqlen: int) -> torch.Tensor:\n+        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n+        freqs = torch.outer(seq, self.inv_freq)\n+        return freqs\n+\n+\n class Qwen2_5_VisionPatchEmbed(nn.Module):\n     def __init__(\n         self,\n@@ -1044,20 +1127,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class Qwen2_5_VisionRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, dim: int, theta: float = 10000.0) -> None:\n-        super().__init__()\n-        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-    def forward(self, seqlen: int) -> torch.Tensor:\n-        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n-        freqs = torch.outer(seq, self.inv_freq)\n-        return freqs\n-\n-\n class Qwen2_5OmniPatchMerger(nn.Module):\n     def __init__(self, dim: int, context_dim: int, spatial_merge_size: int = 2) -> None:\n         super().__init__()\n@@ -1254,7 +1323,7 @@ def __init__(self, config: Qwen2_5OmniThinkerConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -2523,7 +2592,7 @@ def __init__(self, config: Qwen2_5OmniDiTConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -3186,52 +3255,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-def kaiser_sinc_filter1d(cutoff, half_width, kernel_size):\n-    \"\"\"Generates a 1D Kaiser-windowed sinc filter.\n-\n-    Args:\n-        cutoff (float): Normalized cutoff frequency (0 to 0.5).\n-        half_width (float): Transition bandwidth.\n-        kernel_size (int): Number of filter taps.\n-\n-    Returns:\n-        torch.Tensor: A tensor of shape (1, 1, kernel_size) representing the filter.\n-    \"\"\"\n-    is_even = kernel_size % 2 == 0\n-    half_size = kernel_size // 2\n-\n-    # Compute Kaiser window parameters\n-    delta_f = 4 * half_width\n-    attenuation = 2.285 * (half_size - 1) * math.pi * delta_f + 7.95\n-\n-    if attenuation > 50.0:\n-        beta = 0.1102 * (attenuation - 8.7)\n-    elif attenuation >= 21.0:\n-        beta = 0.5842 * (attenuation - 21) ** 0.4 + 0.07886 * (attenuation - 21.0)\n-    else:\n-        beta = 0.0\n-\n-    kaiser_window = torch.kaiser_window(kernel_size, beta=beta, periodic=False, dtype=torch.float32)\n-\n-    # Compute time indices\n-    if is_even:\n-        time_indices = torch.arange(-half_size, half_size) + 0.5\n-    else:\n-        time_indices = torch.arange(kernel_size) - half_size\n-\n-    # Compute sinc filter\n-    if cutoff == 0:\n-        return torch.zeros((1, 1, kernel_size), dtype=torch.float32)  # Ensures correct shape\n-\n-    sinc_filter = torch.sinc(2 * cutoff * time_indices)\n-    normalized_filter = 2 * cutoff * kaiser_window * sinc_filter\n-\n-    # Normalize to ensure sum = 1 (avoid leakage of constant component)\n-    normalized_filter /= normalized_filter.sum()\n-\n-    return normalized_filter.view(1, 1, kernel_size)\n-\n-\n class UpSample1d(nn.Module):\n     def __init__(self, ratio=2, kernel_size=None):\n         super().__init__()\n@@ -3262,6 +3285,9 @@ def __init__(self, ratio=2, kernel_size=None):\n         super().__init__()\n         cutoff = 0.5 / ratio\n         half_width = 0.6 / ratio\n+        self.cutoff = cutoff\n+        self.half_width = half_width\n+        self.kernel_size = kernel_size\n \n         if cutoff < 0.0:\n             raise ValueError(\"Minimum cutoff must be larger than zero.\")"
        },
        {
            "sha": "d682b346f72a1f9fb1437dda14ff4cd2c93a3b16",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 44,
            "deletions": 16,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -26,27 +26,13 @@\n from torch import nn\n from torch.nn import Parameter\n \n-from transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, rotate_half\n-from transformers.models.qwen2_5_vl.configuration_qwen2_5_vl import Qwen2_5_VLVisionConfig\n-from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import (\n-    Qwen2_5_VisionTransformerPretrainedModel,\n-    Qwen2_5_VLAttention,\n-    Qwen2_5_VLMLP,\n-    Qwen2_5_VLPreTrainedModel,\n-    Qwen2_5_VLTextModel,\n-    Qwen2_5_VLVisionBlock,\n-    eager_attention_forward,\n-)\n-from transformers.models.qwen2_audio.configuration_qwen2_audio import Qwen2AudioEncoderConfig\n-from transformers.models.qwen2_audio.modeling_qwen2_audio import Qwen2AudioEncoderLayer\n-from transformers.models.qwen2_vl.modeling_qwen2_vl import Qwen2VLRotaryEmbedding\n-\n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_rope_utils import RopeParameters\n-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n     TransformersKwargs,\n@@ -56,6 +42,21 @@\n )\n from ...utils.deprecation import deprecate_kwarg\n from ...utils.hub import cached_file\n+from ..llama.modeling_llama import LlamaRotaryEmbedding, rotate_half\n+from ..qwen2_5_vl.configuration_qwen2_5_vl import Qwen2_5_VLVisionConfig\n+from ..qwen2_5_vl.modeling_qwen2_5_vl import (\n+    Qwen2_5_VisionRotaryEmbedding,\n+    Qwen2_5_VisionTransformerPretrainedModel,\n+    Qwen2_5_VLAttention,\n+    Qwen2_5_VLMLP,\n+    Qwen2_5_VLPreTrainedModel,\n+    Qwen2_5_VLTextModel,\n+    Qwen2_5_VLVisionBlock,\n+    eager_attention_forward,\n+)\n+from ..qwen2_audio.configuration_qwen2_audio import Qwen2AudioEncoderConfig\n+from ..qwen2_audio.modeling_qwen2_audio import Qwen2AudioEncoderLayer\n+from ..qwen2_vl.modeling_qwen2_vl import Qwen2VLRotaryEmbedding\n \n \n logger = logging.get_logger(__name__)\n@@ -1054,6 +1055,23 @@ class Qwen2_5OmniPreTrainedModel(Qwen2_5_VLPreTrainedModel):\n     input_modalities = (\"image\", \"video\", \"audio\", \"text\")\n     _can_compile_fullgraph = False\n \n+    def _init_weights(self, module):\n+        PreTrainedModel._init_weights(self, module)\n+        if isinstance(module, SinusoidsPositionEmbedding):\n+            log_timescale_increment = np.log(module.max_timescale) / (module.channels // 2 - 1)\n+            inv_timescales = torch.exp(-log_timescale_increment * torch.arange(module.channels // 2).float())\n+            scaled_time = torch.arange(module.length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n+            init.copy_(module.positional_embedding, torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1))\n+        elif isinstance(module, UpSample1d):\n+            filter_tensor = kaiser_sinc_filter1d(0.5 / module.ratio, 0.6 / module.ratio, module.kernel_size)\n+            init.copy_(module.filter, filter_tensor)\n+        elif isinstance(module, DownSample1d):\n+            filter_tensor = kaiser_sinc_filter1d(module.cutoff, module.half_width, module.kernel_size)\n+            init.copy_(module.filter, filter_tensor)\n+        elif isinstance(module, Qwen2_5_VisionRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n+\n \n class Qwen2_5OmniPreTrainedModelForConditionalGeneration(Qwen2_5OmniPreTrainedModel):\n     input_modalities = (\"image\", \"video\", \"audio\", \"text\")\n@@ -1610,6 +1628,9 @@ def forward(\n class SinusoidsPositionEmbedding(nn.Module):\n     def __init__(self, length, channels, max_timescale=10000):\n         super().__init__()\n+        self.length = length\n+        self.channels = channels\n+        self.max_timescale = max_timescale\n         if channels % 2 != 0:\n             raise ValueError(\"SinusoidsPositionEmbedding needs even channels input\")\n         log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n@@ -1918,6 +1939,10 @@ def forward(\n         return hidden_states\n \n \n+class Qwen2_5_VisionRotaryEmbedding(Qwen2_5_VisionRotaryEmbedding):\n+    pass\n+\n+\n class Qwen2_5OmniVisionEncoder(Qwen2_5_VisionTransformerPretrainedModel):\n     config: Qwen2_5OmniVisionEncoderConfig\n     input_modalities = (\"image\", \"video\")\n@@ -3419,6 +3444,9 @@ def __init__(self, ratio=2, kernel_size=None):\n         super().__init__()\n         cutoff = 0.5 / ratio\n         half_width = 0.6 / ratio\n+        self.cutoff = cutoff\n+        self.half_width = half_width\n+        self.kernel_size = kernel_size\n \n         if cutoff < 0.0:\n             raise ValueError(\"Minimum cutoff must be larger than zero.\")"
        },
        {
            "sha": "7a600e424a19f9753f30d197328832eb978c3dc6",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -32,6 +32,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -96,6 +97,8 @@ class Qwen2_5_VisionRotaryEmbedding(nn.Module):\n \n     def __init__(self, dim: int, theta: float = 10000.0) -> None:\n         super().__init__()\n+        self.dim = dim\n+        self.theta = theta\n         inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n \n@@ -304,6 +307,12 @@ class Qwen2_5_VLPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = True\n     _supports_attention_backend = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, Qwen2_5_VisionRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n+\n \n class Qwen2_5_VisionTransformerPretrainedModel(Qwen2_5_VLPreTrainedModel):\n     config: Qwen2_5_VLVisionConfig\n@@ -510,7 +519,7 @@ def __init__(self, config: Qwen2_5_VLConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "db96cf9bb63dc14fb3b6f928e2f0954b94281838",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -26,8 +26,20 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n-from transformers.models.qwen2_vl.configuration_qwen2_vl import Qwen2VLConfig, Qwen2VLTextConfig\n-from transformers.models.qwen2_vl.modeling_qwen2_vl import (\n+from ... import initialization as init\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...configuration_utils import PreTrainedConfig\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import MultiModalData, ProcessingKwargs, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import logging\n+from ...video_utils import VideoInput\n+from ..qwen2_vl.configuration_qwen2_vl import Qwen2VLConfig, Qwen2VLTextConfig\n+from ..qwen2_vl.modeling_qwen2_vl import (\n     PatchEmbed,\n     PatchMerger,\n     Qwen2RMSNorm,\n@@ -40,23 +52,7 @@\n     VisionAttention,\n     VisionRotaryEmbedding,\n )\n-from transformers.models.qwen2_vl.processing_qwen2_vl import Qwen2VLProcessor\n-\n-from ...activations import ACT2FN\n-from ...cache_utils import Cache\n-from ...configuration_utils import PreTrainedConfig\n-from ...feature_extraction_utils import BatchFeature\n-from ...image_utils import ImageInput\n-from ...modeling_flash_attention_utils import is_flash_attn_available\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...processing_utils import MultiModalData, ProcessingKwargs, Unpack\n-from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n-from ...video_utils import VideoInput\n-\n-\n-if is_flash_attn_available():\n-    pass\n+from ..qwen2_vl.processing_qwen2_vl import Qwen2VLProcessor\n \n \n logger = logging.get_logger(__name__)\n@@ -173,7 +169,11 @@ def forward(\n \n \n class Qwen2_5_VLPreTrainedModel(Qwen2VLPreTrainedModel):\n-    pass\n+    def _init_weights(self, module):\n+        PreTrainedModel._init_weights(self, module)\n+        if isinstance(module, Qwen2_5_VisionRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n \n \n class Qwen2_5_VisionTransformerPretrainedModel(Qwen2_5_VLPreTrainedModel):"
        },
        {
            "sha": "f8a3e366aac9e9de1314c830fe0248a673461b49",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -90,7 +90,7 @@ def __init__(self, config: Qwen2MoeConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "eea80504b05be961b9810053d827820589c87884",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -28,6 +28,7 @@\n import torch.nn.functional as F\n from torch.nn import LayerNorm\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -125,7 +126,7 @@ def __init__(self, config: Qwen2VLConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters(\n@@ -246,6 +247,8 @@ class VisionRotaryEmbedding(nn.Module):\n \n     def __init__(self, dim: int, theta: float = 10000.0) -> None:\n         super().__init__()\n+        self.dim = dim\n+        self.theta = theta\n         inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n \n@@ -665,6 +668,12 @@ class Qwen2VLPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = True\n     _supports_attention_backend = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, VisionRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n+\n \n @auto_docstring\n class Qwen2VisionTransformerPretrainedModel(Qwen2VLPreTrainedModel):"
        },
        {
            "sha": "c7f888468a572d1d305a349376bb52769d1dde3a",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -100,7 +100,7 @@ def __init__(self, config: Qwen3Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "fe6ad166d0e99e2b627af33f14d5b3dcd0f1f598",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -401,7 +401,7 @@ def __init__(self, config: Qwen3MoeConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "beda505fac4133f770cb710c41e435da36efee61",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -189,7 +189,7 @@ def __init__(self, config: Qwen3NextConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "af3c16bf63d1c908982486186aa5da3705021d4c",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 52,
            "deletions": 34,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -64,6 +64,27 @@\n )\n \n \n+class SinusoidsPositionEmbedding(nn.Module):\n+    def __init__(self, length, channels, max_timescale=10000):\n+        super().__init__()\n+        self.length = length\n+        self.channels = channels\n+        self.max_timescale = max_timescale\n+        if channels % 2 != 0:\n+            raise ValueError(\"SinusoidsPositionEmbedding needs even channels input\")\n+        log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n+        inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2).float())\n+        scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n+        self.register_buffer(\n+            \"positional_embedding\",\n+            torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1),\n+            persistent=False,\n+        )\n+\n+    def forward(self, seqlen: int):\n+        return self.positional_embedding[:seqlen, :]\n+\n+\n @auto_docstring\n class Qwen3OmniMoePreTrainedModel(PreTrainedModel):\n     config: Qwen3OmniMoeConfig\n@@ -85,6 +106,19 @@ def _init_weights(self, module):\n             init.normal_(module.experts.gate_up_proj, mean=0.0, std=std)\n             init.normal_(module.experts.down_proj, mean=0.0, std=std)\n             init.normal_(module.gate.weight, mean=0.0, std=std)\n+        elif isinstance(module, Qwen3OmniMoeCode2Wav):\n+            init.copy_(\n+                module.code_offset,\n+                torch.arange(module.config.num_quantizers).view(1, -1, 1) * module.config.codebook_size,\n+            )\n+        elif isinstance(module, SinusoidsPositionEmbedding):\n+            log_timescale_increment = np.log(module.max_timescale) / (module.channels // 2 - 1)\n+            inv_timescales = torch.exp(-log_timescale_increment * torch.arange(module.channels // 2).float())\n+            scaled_time = torch.arange(module.length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n+            init.copy_(module.positional_embedding, torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1))\n+        elif isinstance(module, Qwen3OmniMoeVisionRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n \n \n def _get_feat_extract_output_lengths(input_lengths):\n@@ -620,24 +654,6 @@ def forward(\n         return outputs\n \n \n-class SinusoidsPositionEmbedding(nn.Module):\n-    def __init__(self, length, channels, max_timescale=10000):\n-        super().__init__()\n-        if channels % 2 != 0:\n-            raise ValueError(\"SinusoidsPositionEmbedding needs even channels input\")\n-        log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n-        inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2).float())\n-        scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n-        self.register_buffer(\n-            \"positional_embedding\",\n-            torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1),\n-            persistent=False,\n-        )\n-\n-    def forward(self, seqlen: int):\n-        return self.positional_embedding[:seqlen, :]\n-\n-\n @auto_docstring(\n     custom_intro=\"\"\"\n     Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n@@ -960,6 +976,22 @@ def forward(self, hidden: torch.Tensor) -> torch.Tensor:\n         return hidden\n \n \n+class Qwen3OmniMoeVisionRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, dim: int, theta: float = 10000.0) -> None:\n+        super().__init__()\n+        self.dim = dim\n+        self.theta = theta\n+        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+\n+    def forward(self, seqlen: int) -> torch.Tensor:\n+        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n+        freqs = torch.outer(seq, self.inv_freq)\n+        return freqs\n+\n+\n class Qwen3OmniMoeVisionMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -993,20 +1025,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class Qwen3OmniMoeVisionRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, dim: int, theta: float = 10000.0) -> None:\n-        super().__init__()\n-        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-    def forward(self, seqlen: int) -> torch.Tensor:\n-        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n-        freqs = torch.outer(seq, self.inv_freq)\n-        return freqs\n-\n-\n class Qwen3OmniMoeVisionBlock(GradientCheckpointingLayer):\n     def __init__(self, config, attn_implementation: str = \"sdpa\") -> None:\n         super().__init__()\n@@ -1248,7 +1266,7 @@ def __init__(self, config: Qwen3OmniMoeTextConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n         self.mrope_section = config.rope_parameters.get(\"mrope_section\", [24, 20, 20])\n \n@@ -2479,7 +2497,7 @@ def __init__(self, config: Qwen3OmniMoeConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "c3a74347ef3a5bedba7414e175b9195c4638c508",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 23,
            "deletions": 1,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -62,7 +62,11 @@\n     Qwen2_5OmniThinkerForConditionalGeneration,\n     SnakeBeta,\n )\n-from ..qwen2_5_omni.processing_qwen2_5_omni import Qwen2_5OmniProcessor, Qwen2_5OmniProcessorKwargs\n+from ..qwen2_5_omni.processing_qwen2_5_omni import (\n+    Qwen2_5OmniProcessor,\n+    Qwen2_5OmniProcessorKwargs,\n+    SinusoidsPositionEmbedding,\n+)\n from ..qwen2_moe.modeling_qwen2_moe import Qwen2MoeSparseMoeBlock\n from ..qwen3.configuration_qwen3 import Qwen3Config\n from ..qwen3.modeling_qwen3 import (\n@@ -91,6 +95,7 @@\n     Qwen3VLMoeTextRotaryEmbedding,\n     Qwen3VLMoeVisionAttention,\n     Qwen3VLMoeVisionModel,\n+    Qwen3VLMoeVisionRotaryEmbedding,\n )\n \n \n@@ -900,6 +905,19 @@ def _init_weights(self, module):\n             init.normal_(module.experts.gate_up_proj, mean=0.0, std=std)\n             init.normal_(module.experts.down_proj, mean=0.0, std=std)\n             init.normal_(module.gate.weight, mean=0.0, std=std)\n+        elif isinstance(module, Qwen3OmniMoeCode2Wav):\n+            init.copy_(\n+                module.code_offset,\n+                torch.arange(module.config.num_quantizers).view(1, -1, 1) * module.config.codebook_size,\n+            )\n+        elif isinstance(module, SinusoidsPositionEmbedding):\n+            log_timescale_increment = np.log(module.max_timescale) / (module.channels // 2 - 1)\n+            inv_timescales = torch.exp(-log_timescale_increment * torch.arange(module.channels // 2).float())\n+            scaled_time = torch.arange(module.length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n+            init.copy_(module.positional_embedding, torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1))\n+        elif isinstance(module, Qwen3OmniMoeVisionRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n \n \n class Qwen3OmniMoePreTrainedModelForConditionalGeneration(Qwen2_5OmniPreTrainedModelForConditionalGeneration):\n@@ -1297,6 +1315,10 @@ def forward(self, hidden: torch.Tensor) -> torch.Tensor:\n         return hidden\n \n \n+class Qwen3OmniMoeVisionRotaryEmbedding(Qwen3VLMoeVisionRotaryEmbedding):\n+    pass\n+\n+\n class Qwen3OmniMoeVisionEncoder(Qwen3VLMoeVisionModel):\n     config: Qwen3OmniMoeVisionEncoderConfig\n     _no_split_modules = [\"Qwen3OmniMoeVisionBlock\"]"
        },
        {
            "sha": "266a4b50a0dfafb34c26093e0ef98c00cecb6807",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -27,6 +27,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -81,6 +82,8 @@ class Qwen3VLVisionRotaryEmbedding(nn.Module):\n \n     def __init__(self, dim: int, theta: float = 10000.0) -> None:\n         super().__init__()\n+        self.dim = dim\n+        self.theta = theta\n         inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n \n@@ -292,7 +295,7 @@ def __init__(self, config: Qwen3VLTextConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n         self.mrope_section = config.rope_parameters.get(\"mrope_section\", [24, 20, 20])\n \n@@ -592,6 +595,12 @@ class Qwen3VLPreTrainedModel(PreTrainedModel):\n         \"attentions\": Qwen3VLTextAttention,\n     }\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, Qwen3VLVisionRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n+\n \n class Qwen3VLVisionModel(Qwen3VLPreTrainedModel):\n     config: Qwen3VLVisionConfig"
        },
        {
            "sha": "04b33a4941dfef2251166ef278b1e1c4abc98064",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -22,6 +22,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PreTrainedConfig\n@@ -31,7 +32,7 @@\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import RopeParameters, dynamic_rope_update\n-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import ProcessingKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import auto_docstring, can_return_tuple, logging\n@@ -488,6 +489,12 @@ class Qwen3VLPreTrainedModel(Qwen2VLPreTrainedModel):\n         \"attentions\": Qwen3VLTextAttention,\n     }\n \n+    def _init_weights(self, module):\n+        PreTrainedModel._init_weights(self, module)\n+        if isinstance(module, Qwen3VLVisionRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n+\n \n class Qwen3VLVisionModel(Qwen3VLPreTrainedModel):\n     config: Qwen3VLVisionConfig"
        },
        {
            "sha": "ca1cf86684daeb712abae77798d5d04d55dfb651",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 20,
            "deletions": 15,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -418,6 +418,25 @@ def _init_weights(self, module):\n         if isinstance(module, Qwen3VLMoeTextExperts):\n             init.normal_(module.gate_up_proj, mean=0.0, std=std)\n             init.normal_(module.down_proj, mean=0.0, std=std)\n+        elif isinstance(module, Qwen3VLMoeVisionRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n+\n+\n+class Qwen3VLMoeVisionRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, dim: int, theta: float = 10000.0) -> None:\n+        super().__init__()\n+        self.dim = dim\n+        self.theta = theta\n+        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+\n+    def forward(self, seqlen: int) -> torch.Tensor:\n+        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n+        freqs = torch.outer(seq, self.inv_freq)\n+        return freqs\n \n \n class Qwen3VLMoeVisionMLP(nn.Module):\n@@ -453,20 +472,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class Qwen3VLMoeVisionRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, dim: int, theta: float = 10000.0) -> None:\n-        super().__init__()\n-        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-    def forward(self, seqlen: int) -> torch.Tensor:\n-        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n-        freqs = torch.outer(seq, self.inv_freq)\n-        return freqs\n-\n-\n class Qwen3VLMoeVisionPatchMerger(nn.Module):\n     def __init__(self, config: Qwen3VLMoeVisionConfig, use_postshuffle_norm=False) -> None:\n         super().__init__()\n@@ -817,7 +822,7 @@ def __init__(self, config: Qwen3VLMoeTextConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n         self.mrope_section = config.rope_parameters.get(\"mrope_section\", [24, 20, 20])\n "
        },
        {
            "sha": "7a957e115db4fb04dc24ea6c44b9e332b78e1f19",
            "filename": "src/transformers/models/qwen3_vl_moe/modular_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -41,6 +41,7 @@\n     Qwen3VLTextAttention,\n     Qwen3VLTextModel,\n     Qwen3VLVisionModel,\n+    Qwen3VLVisionRotaryEmbedding,\n )\n \n \n@@ -368,6 +369,13 @@ def _init_weights(self, module):\n         if isinstance(module, Qwen3VLMoeTextExperts):\n             init.normal_(module.gate_up_proj, mean=0.0, std=std)\n             init.normal_(module.down_proj, mean=0.0, std=std)\n+        elif isinstance(module, Qwen3VLMoeVisionRotaryEmbedding):\n+            inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n+            init.copy_(module.inv_freq, inv_freq)\n+\n+\n+class Qwen3VLMoeVisionRotaryEmbedding(Qwen3VLVisionRotaryEmbedding):\n+    pass\n \n \n class Qwen3VLMoeVisionModel(Qwen3VLVisionModel):"
        },
        {
            "sha": "d4a36a82083c0fe781ac8cdaa69d86f5d128496c",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -80,7 +80,7 @@ def __init__(self, config: RecurrentGemmaConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     # Ignore copy\n@@ -611,10 +611,11 @@ def _init_weights(self, module):\n             # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n             if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n                 init.zeros_(module.weight[module.padding_idx])\n-\n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         elif isinstance(module, RecurrentGemmaRMSNorm):\n             init.zeros_(module.weight)\n+        elif isinstance(module, RecurrentGemmaModel):\n+            init.constant_(module.normalizer, module.config.hidden_size**0.5)\n \n     def _setup_cache(self, config, batch, device, dtype):\n         layers = getattr(self, \"model\", self).layers"
        },
        {
            "sha": "bb39df429d4f1a511631b8fbc5f814c565ba93e9",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -1851,6 +1851,14 @@ def _init_weights(self, module):\n         if isinstance(module, AxialPositionEmbeddings):\n             for weight in module.weights:\n                 init.normal_(weight, std=self.config.axial_norm_std)\n+        elif isinstance(module, LSHSelfAttention):\n+            init.constant_(module.self_mask_value_float16, -1e3)\n+            init.constant_(module.self_mask_value_float32, -1e5)\n+            init.constant_(module.mask_value_float16, -1e4)\n+            init.constant_(module.mask_value_float32, -1e9)\n+        elif isinstance(module, LocalSelfAttention):\n+            init.constant_(module.mask_value_float16, -1e4)\n+            init.constant_(module.mask_value_float32, -1e9)\n \n \n @dataclass"
        },
        {
            "sha": "b4f3cf8fda913770d360ffc3dccbc9b26331bdf1",
            "filename": "src/transformers/models/regnet/modeling_regnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_regnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_regnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_regnet.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -278,6 +278,10 @@ def _init_weights(self, module):\n         elif isinstance(module, (nn.BatchNorm2d, nn.GroupNorm)):\n             init.constant_(module.weight, 1)\n             init.constant_(module.bias, 0)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n \n \n @auto_docstring"
        },
        {
            "sha": "cba0bbb280aa5162912f27999246a1f1ee9ccd22",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -21,6 +21,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -488,6 +489,11 @@ class RemBertPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"rembert\"\n     supports_gradient_checkpointing = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, RemBertEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "fe9d3b01e7e76b2b928f6ac42e5bd6351cda2808",
            "filename": "src/transformers/models/resnet/modeling_resnet.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -262,9 +262,14 @@ def _init_weights(self, module):\n                 fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(module.weight)\n                 bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n                 init.uniform_(module.bias, -bound, bound)\n-        elif isinstance(module, (nn.BatchNorm2d, nn.GroupNorm)):\n-            init.constant_(module.weight, 1)\n-            init.constant_(module.bias, 0)\n+        # We need to check it like that as some Detr models replace the BatchNorm2d by their own\n+        elif \"BatchNorm\" in module.__class__.__name__:\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n+            init.zeros_(module.running_mean)\n+            init.ones_(module.running_var)\n+            if getattr(module, \"num_batches_tracked\", None) is not None:\n+                init.zeros_(module.num_batches_tracked)\n \n \n @auto_docstring"
        },
        {
            "sha": "3d6e2ff94773da179a12d7dc6b0e2278739b003c",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -501,6 +501,9 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, RobertaLMHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, RobertaEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n \n \n class RobertaEncoder(nn.Module):"
        },
        {
            "sha": "cb509a26ced1c9af3922d944af33810f50354b7a",
            "filename": "src/transformers/models/roberta/modular_roberta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -172,6 +172,9 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, RobertaLMHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, RobertaEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n \n \n class RobertaModel(BertModel):"
        },
        {
            "sha": "8db3beb058a52847bebc0d5dd7452bc0a957da77",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -561,6 +561,9 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, RobertaPreLayerNormLMHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, RobertaPreLayerNormEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n \n \n @auto_docstring("
        },
        {
            "sha": "910ebe97cdf32878b1e7ebe77ce04d047918c34a",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -621,6 +621,9 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, RoCBertLMPredictionHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, RoCBertEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+            init.zeros_(module.token_type_ids)\n \n \n @auto_docstring("
        },
        {
            "sha": "069a35cac52ed620b1418fecebfe2aa8129a31f7",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -1059,6 +1059,10 @@ def _init_weights(self, module):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n \n         elif isinstance(module, nn.LayerNorm):\n             init.ones_(module.weight)"
        },
        {
            "sha": "1511d0963cc2f3a28560f7f66836c25a68abe6f7",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr_resnet.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr_resnet.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -316,9 +316,14 @@ def _init_weights(self, module):\n                 fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(module.weight)\n                 bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n                 init.uniform_(module.bias, -bound, bound)\n-        elif isinstance(module, (nn.BatchNorm2d, nn.GroupNorm)):\n-            init.constant_(module.weight, 1)\n-            init.constant_(module.bias, 0)\n+        # We need to check it like that as some Detr models replace the BatchNorm2d by their own\n+        elif \"BatchNorm\" in module.__class__.__name__:\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n+            init.zeros_(module.running_mean)\n+            init.ones_(module.running_var)\n+            if getattr(module, \"num_batches_tracked\", None) is not None:\n+                init.zeros_(module.num_batches_tracked)\n \n \n @auto_docstring("
        },
        {
            "sha": "ce2e61aa7587f6d34d3daf72e9733349ef81ce4c",
            "filename": "src/transformers/models/rt_detr_v2/configuration_rt_detr_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -18,7 +18,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments"
        },
        {
            "sha": "3240c2ca154a12d6c4b1e203076509686d45cf97",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -506,6 +506,10 @@ def _init_weights(self, module):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n \n         elif isinstance(module, nn.LayerNorm):\n             init.ones_(module.weight)\n@@ -515,6 +519,9 @@ def _init_weights(self, module):\n             init.xavier_uniform_(module.weight_embedding.weight)\n         if hasattr(module, \"denoising_class_embed\") and self.config.num_denoising > 0:\n             init.xavier_uniform_(module.denoising_class_embed.weight)\n+        if isinstance(module, RTDetrV2MultiscaleDeformableAttention):\n+            n_points_scale = [1 / n for n in module.n_points_list for _ in range(n)]\n+            init.copy_(module.n_points_scale, torch.tensor(n_points_scale, dtype=torch.float32))\n \n \n @dataclass"
        },
        {
            "sha": "671e57fbd6812c5751e9c8b4e6fda48cc94596b3",
            "filename": "src/transformers/models/rt_detr_v2/modular_rt_detr_v2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -19,6 +19,7 @@\n import torch.nn.functional as F\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...configuration_utils import PreTrainedConfig\n from ...utils import is_torchdynamo_compiling, logging\n from ...utils.backbone_utils import (\n@@ -564,7 +565,11 @@ def __init__(self, config: RTDetrV2Config):\n \n \n class RTDetrV2PreTrainedModel(RTDetrPreTrainedModel):\n-    pass\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, RTDetrV2MultiscaleDeformableAttention):\n+            n_points_scale = [1 / n for n in module.n_points_list for _ in range(n)]\n+            init.copy_(module.n_points_scale, torch.tensor(n_points_scale, dtype=torch.float32))\n \n \n class RTDetrV2Decoder(RTDetrDecoder):"
        },
        {
            "sha": "4f23cd1ea10c6759dcd8524db2b062172a119921",
            "filename": "src/transformers/models/sam/configuration_sam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam%2Fconfiguration_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam%2Fconfiguration_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fconfiguration_sam.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -249,6 +249,7 @@ def __init__(\n         self.global_attn_indexes = global_attn_indexes\n         self.num_pos_feats = num_pos_feats\n         self.mlp_dim = int(hidden_size * mlp_ratio) if mlp_dim is None else mlp_dim\n+        self.scale = self.hidden_size // 2\n \n \n class SamConfig(PreTrainedConfig):"
        },
        {
            "sha": "9fcedae3eb815f8a763fe8903bb31ae579a3e27f",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -548,7 +548,7 @@ def forward(\n class SamPositionalEmbedding(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.scale = config.hidden_size // 2\n+        self.scale = config.scale\n         self.register_buffer(\"positional_embedding\", self.scale * torch.randn((2, config.num_pos_feats)))\n \n     def forward(self, input_coords, input_shape=None):\n@@ -1014,6 +1014,8 @@ def _init_weights(self, module: nn.Module):\n         elif isinstance(module, SamVisionEncoder):\n             if self.config.use_abs_pos:\n                 init.zeros_(module.pos_embed)\n+        elif isinstance(module, SamPositionalEmbedding):\n+            init.normal_(module.positional_embedding, std=module.scale)\n \n \n class SamVisionEncoder(SamPreTrainedModel):"
        },
        {
            "sha": "f599a6efa1104e57514d5742c38e58aa655af2e9",
            "filename": "src/transformers/models/sam2/modeling_sam2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -565,7 +565,9 @@ def _init_weights(self, module):\n                 init.zeros_(module.pos_embed)\n             if module.pos_embed_window is not None:\n                 init.zeros_(module.pos_embed_window)\n-        if isinstance(module, Sam2Model):\n+        elif isinstance(module, Sam2PositionalEmbedding):\n+            init.normal_(module.positional_embedding, std=module.scale)\n+        elif isinstance(module, Sam2Model):\n             if module.no_memory_embedding is not None:\n                 init.zeros_(module.no_memory_embedding)\n "
        },
        {
            "sha": "f7f39eb59fd3e7e284f4f403b4108bdd84567728",
            "filename": "src/transformers/models/sam2/modular_sam2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -681,7 +681,9 @@ def _init_weights(self, module):\n                 init.zeros_(module.pos_embed)\n             if module.pos_embed_window is not None:\n                 init.zeros_(module.pos_embed_window)\n-        if isinstance(module, Sam2Model):\n+        elif isinstance(module, Sam2PositionalEmbedding):\n+            init.normal_(module.positional_embedding, std=module.scale)\n+        elif isinstance(module, Sam2Model):\n             if module.no_memory_embedding is not None:\n                 init.zeros_(module.no_memory_embedding)\n "
        },
        {
            "sha": "7c79956dc814558fd280314d89f2eaaa688dbc5f",
            "filename": "src/transformers/models/sam2_video/modeling_sam2_video.py",
            "status": "modified",
            "additions": 50,
            "deletions": 37,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -688,6 +688,12 @@ def _init_weights(self, module):\n         if isinstance(module, Sam2VideoMemoryFuserCXBlock):\n             if module.scale is not None:\n                 init.zeros_(module.scale)\n+        elif isinstance(module, Sam2VideoVisionRotaryEmbedding):\n+            inv_freq = module.create_inv_freq()\n+            init.copy_(module.rope_embeddings_cos, inv_freq.cos())\n+            init.copy_(module.rope_embeddings_sin, inv_freq.sin())\n+        elif isinstance(module, Sam2VideoPositionalEmbedding):\n+            init.normal_(module.positional_embedding, std=module.scale)\n \n \n class Sam2VideoVisionRotaryEmbedding(nn.Module):\n@@ -698,24 +704,17 @@ class Sam2VideoVisionRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: Sam2VideoConfig):\n         super().__init__()\n-        dim = config.memory_attention_hidden_size // (\n+        self.dim = config.memory_attention_hidden_size // (\n             config.memory_attention_downsample_rate * config.memory_attention_num_attention_heads\n         )\n         # Ensure even dimension for proper axial splitting\n-        if dim % 4 != 0:\n+        if self.dim % 4 != 0:\n             raise ValueError(\"Dimension must be divisible by 4 for axial RoPE\")\n-        end_x, end_y = config.memory_attention_rope_feat_sizes\n-        freqs = 1.0 / (config.memory_attention_rope_theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n+        self.end_x, self.end_y = config.memory_attention_rope_feat_sizes\n+        self.memory_attention_rope_theta = config.memory_attention_rope_theta\n \n-        # Generate 2D position indices for axial rotary embedding\n-        flattened_indices = torch.arange(end_x * end_y, dtype=torch.long)\n-        x_positions = flattened_indices % end_x\n-        y_positions = torch.div(flattened_indices, end_x, rounding_mode=\"floor\")\n-        freqs_x = torch.outer(x_positions, freqs).float()\n-        freqs_y = torch.outer(y_positions, freqs).float()\n-        inv_freq = torch.cat([freqs_x, freqs_y], dim=-1)\n-        inv_freq = inv_freq.repeat_interleave(2, dim=-1)\n         # directly register the cos and sin embeddings as we have a fixed feature shape\n+        inv_freq = self.create_inv_freq()\n         self.register_buffer(\"rope_embeddings_cos\", inv_freq.cos(), persistent=False)\n         self.register_buffer(\"rope_embeddings_sin\", inv_freq.sin(), persistent=False)\n \n@@ -724,6 +723,20 @@ def forward(self) -> tuple[torch.Tensor, torch.Tensor]:\n         # As the feature map size is fixed, we can just return the pre-computed embeddings.\n         return self.rope_embeddings_cos, self.rope_embeddings_sin\n \n+    def create_inv_freq(self):\n+        freqs = 1.0 / (\n+            self.memory_attention_rope_theta ** (torch.arange(0, self.dim, 4)[: (self.dim // 4)].float() / self.dim)\n+        )\n+        # Generate 2D position indices for axial rotary embedding\n+        flattened_indices = torch.arange(self.end_x * self.end_y, dtype=torch.long)\n+        x_positions = flattened_indices % self.end_x\n+        y_positions = torch.div(flattened_indices, self.end_x, rounding_mode=\"floor\")\n+        freqs_x = torch.outer(x_positions, freqs).float()\n+        freqs_y = torch.outer(y_positions, freqs).float()\n+        inv_freq = torch.cat([freqs_x, freqs_y], dim=-1)\n+        inv_freq = inv_freq.repeat_interleave(2, dim=-1)\n+        return inv_freq\n+\n \n def rotate_pairwise(x):\n     \"\"\"\n@@ -1101,6 +1114,31 @@ def forward(\n         return vision_features, vision_pos_enc\n \n \n+class Sam2VideoPositionalEmbedding(nn.Module):\n+    def __init__(self, config: Sam2VideoPromptEncoderConfig):\n+        super().__init__()\n+        self.scale = config.scale\n+        positional_embedding = self.scale * torch.randn((2, config.hidden_size // 2))\n+        self.register_buffer(\"positional_embedding\", positional_embedding)\n+\n+    def forward(self, input_coords, input_shape=None):\n+        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n+        coordinates = input_coords.clone()\n+\n+        if input_shape is not None:\n+            coordinates[:, :, :, 0] = coordinates[:, :, :, 0] / input_shape[1]\n+            coordinates[:, :, :, 1] = coordinates[:, :, :, 1] / input_shape[0]\n+        coordinates.to(torch.float32)\n+\n+        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n+        coordinates = 2 * coordinates - 1\n+        coordinates = coordinates.to(self.positional_embedding.dtype)\n+        coordinates = coordinates @ self.positional_embedding\n+        coordinates = 2 * np.pi * coordinates\n+        # outputs d_1 x ... x d_n x channel shape\n+        return torch.cat([torch.sin(coordinates), torch.cos(coordinates)], dim=-1)\n+\n+\n @dataclass\n @auto_docstring(custom_intro=\"Base class for the vision encoder's outputs.\")\n class Sam2VideoVisionEncoderOutput(ModelOutput):\n@@ -1130,31 +1168,6 @@ class Sam2VideoVisionEncoderOutput(ModelOutput):\n     attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n-class Sam2VideoPositionalEmbedding(nn.Module):\n-    def __init__(self, config: Sam2VideoPromptEncoderConfig):\n-        super().__init__()\n-        self.scale = config.scale\n-        positional_embedding = self.scale * torch.randn((2, config.hidden_size // 2))\n-        self.register_buffer(\"positional_embedding\", positional_embedding)\n-\n-    def forward(self, input_coords, input_shape=None):\n-        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n-        coordinates = input_coords.clone()\n-\n-        if input_shape is not None:\n-            coordinates[:, :, :, 0] = coordinates[:, :, :, 0] / input_shape[1]\n-            coordinates[:, :, :, 1] = coordinates[:, :, :, 1] / input_shape[0]\n-        coordinates.to(torch.float32)\n-\n-        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n-        coordinates = 2 * coordinates - 1\n-        coordinates = coordinates.to(self.positional_embedding.dtype)\n-        coordinates = coordinates @ self.positional_embedding\n-        coordinates = 2 * np.pi * coordinates\n-        # outputs d_1 x ... x d_n x channel shape\n-        return torch.cat([torch.sin(coordinates), torch.cos(coordinates)], dim=-1)\n-\n-\n class Sam2VideoMaskEmbedding(nn.Module):\n     def __init__(self, config: Sam2VideoPromptEncoderConfig):\n         super().__init__()"
        },
        {
            "sha": "26ecfaa8332d5884761cdef8ad2f159381b0346d",
            "filename": "src/transformers/models/sam2_video/modular_sam2_video.py",
            "status": "modified",
            "additions": 30,
            "deletions": 12,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -51,6 +51,7 @@\n     Sam2ImageSegmentationOutput,\n     Sam2LayerNorm,\n     Sam2Model,\n+    Sam2PositionalEmbedding,\n     Sam2SinePositionEmbedding,\n     Sam2TwoWayAttentionBlock,\n     eager_attention_forward,\n@@ -1013,6 +1014,12 @@ def _init_weights(self, module):\n         if isinstance(module, Sam2VideoMemoryFuserCXBlock):\n             if module.scale is not None:\n                 init.zeros_(module.scale)\n+        elif isinstance(module, Sam2VideoVisionRotaryEmbedding):\n+            inv_freq = module.create_inv_freq()\n+            init.copy_(module.rope_embeddings_cos, inv_freq.cos())\n+            init.copy_(module.rope_embeddings_sin, inv_freq.sin())\n+        elif isinstance(module, Sam2VideoPositionalEmbedding):\n+            init.normal_(module.positional_embedding, std=module.scale)\n \n \n class Sam2VideoVisionRotaryEmbedding(nn.Module):\n@@ -1023,24 +1030,17 @@ class Sam2VideoVisionRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: Sam2VideoConfig):\n         super().__init__()\n-        dim = config.memory_attention_hidden_size // (\n+        self.dim = config.memory_attention_hidden_size // (\n             config.memory_attention_downsample_rate * config.memory_attention_num_attention_heads\n         )\n         # Ensure even dimension for proper axial splitting\n-        if dim % 4 != 0:\n+        if self.dim % 4 != 0:\n             raise ValueError(\"Dimension must be divisible by 4 for axial RoPE\")\n-        end_x, end_y = config.memory_attention_rope_feat_sizes\n-        freqs = 1.0 / (config.memory_attention_rope_theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n+        self.end_x, self.end_y = config.memory_attention_rope_feat_sizes\n+        self.memory_attention_rope_theta = config.memory_attention_rope_theta\n \n-        # Generate 2D position indices for axial rotary embedding\n-        flattened_indices = torch.arange(end_x * end_y, dtype=torch.long)\n-        x_positions = flattened_indices % end_x\n-        y_positions = torch.div(flattened_indices, end_x, rounding_mode=\"floor\")\n-        freqs_x = torch.outer(x_positions, freqs).float()\n-        freqs_y = torch.outer(y_positions, freqs).float()\n-        inv_freq = torch.cat([freqs_x, freqs_y], dim=-1)\n-        inv_freq = inv_freq.repeat_interleave(2, dim=-1)\n         # directly register the cos and sin embeddings as we have a fixed feature shape\n+        inv_freq = self.create_inv_freq()\n         self.register_buffer(\"rope_embeddings_cos\", inv_freq.cos(), persistent=False)\n         self.register_buffer(\"rope_embeddings_sin\", inv_freq.sin(), persistent=False)\n \n@@ -1049,6 +1049,20 @@ def forward(self) -> tuple[torch.Tensor, torch.Tensor]:\n         # As the feature map size is fixed, we can just return the pre-computed embeddings.\n         return self.rope_embeddings_cos, self.rope_embeddings_sin\n \n+    def create_inv_freq(self):\n+        freqs = 1.0 / (\n+            self.memory_attention_rope_theta ** (torch.arange(0, self.dim, 4)[: (self.dim // 4)].float() / self.dim)\n+        )\n+        # Generate 2D position indices for axial rotary embedding\n+        flattened_indices = torch.arange(self.end_x * self.end_y, dtype=torch.long)\n+        x_positions = flattened_indices % self.end_x\n+        y_positions = torch.div(flattened_indices, self.end_x, rounding_mode=\"floor\")\n+        freqs_x = torch.outer(x_positions, freqs).float()\n+        freqs_y = torch.outer(y_positions, freqs).float()\n+        inv_freq = torch.cat([freqs_x, freqs_y], dim=-1)\n+        inv_freq = inv_freq.repeat_interleave(2, dim=-1)\n+        return inv_freq\n+\n \n def rotate_pairwise(x):\n     \"\"\"\n@@ -1426,6 +1440,10 @@ def forward(\n         return vision_features, vision_pos_enc\n \n \n+class Sam2VideoPositionalEmbedding(Sam2PositionalEmbedding):\n+    pass\n+\n+\n # a large negative value as a placeholder score for missing objects\n NO_OBJ_SCORE = -1024.0\n "
        },
        {
            "sha": "e481cf77fef847038779669f324565e55c0047c4",
            "filename": "src/transformers/models/sam3/modeling_sam3.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -417,6 +417,10 @@ def __init__(self, config: Sam3ViTConfig, end_x: int, end_y: int, scale: float =\n         # Ensure even dimension for proper axial splitting\n         if dim % 4 != 0:\n             raise ValueError(\"Dimension must be divisible by 4 for axial RoPE\")\n+        self.end_x, self.end_y = end_x, end_y\n+        self.dim = dim\n+        self.rope_theta = config.rope_theta\n+        self.scale = scale\n         freqs = 1.0 / (config.rope_theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n \n         flattened_indices = torch.arange(end_x * end_y, dtype=torch.long)\n@@ -776,6 +780,19 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Sam3ViTEmbeddings):\n             init.normal_(module.position_embeddings, mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, Sam3ViTRotaryEmbedding):\n+            end_x, end_y = module.end_x, module.end_y\n+            dim = module.dim\n+            freqs = 1.0 / (module.rope_theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n+            flattened_indices = torch.arange(end_x * end_y, dtype=torch.long)\n+            x_positions = (flattened_indices % end_x) * module.scale\n+            y_positions = torch.div(flattened_indices, end_x, rounding_mode=\"floor\") * module.scale\n+            freqs_x = torch.outer(x_positions, freqs).float()\n+            freqs_y = torch.outer(y_positions, freqs).float()\n+            inv_freq = torch.cat([freqs_x, freqs_y], dim=-1)\n+            inv_freq = inv_freq.repeat_interleave(2, dim=-1)\n+            init.copy_(module.rope_embeddings_cos, inv_freq.cos())\n+            init.copy_(module.rope_embeddings_sin, inv_freq.sin())\n \n \n @auto_docstring"
        },
        {
            "sha": "5dfee7753f97a57972d9db98e5eb8de63fe4a7c9",
            "filename": "src/transformers/models/sam3_tracker/modeling_sam3_tracker.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodeling_sam3_tracker.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodeling_sam3_tracker.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodeling_sam3_tracker.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -128,6 +128,8 @@ def _init_weights(self, module):\n         if isinstance(module, Sam3TrackerModel):\n             if module.no_memory_embedding is not None:\n                 init.zeros_(module.no_memory_embedding)\n+        elif isinstance(module, Sam3TrackerPositionalEmbedding):\n+            init.normal_(module.positional_embedding, std=module.scale)\n \n \n class Sam3TrackerPositionalEmbedding(nn.Module):"
        },
        {
            "sha": "82e273c31b3cf879d5429d435a1120d61e342615",
            "filename": "src/transformers/models/sam3_tracker/modular_sam3_tracker.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodular_sam3_tracker.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodular_sam3_tracker.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodular_sam3_tracker.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -149,6 +149,8 @@ def _init_weights(self, module):\n         if isinstance(module, Sam3TrackerModel):\n             if module.no_memory_embedding is not None:\n                 init.zeros_(module.no_memory_embedding)\n+        elif isinstance(module, Sam3TrackerPositionalEmbedding):\n+            init.normal_(module.positional_embedding, std=module.scale)\n \n \n class Sam3TrackerPositionalEmbedding(Sam2PositionalEmbedding):"
        },
        {
            "sha": "b1178a1c7c94078d2303c6212a7d64b78fa72f88",
            "filename": "src/transformers/models/sam3_tracker_video/modeling_sam3_tracker_video.py",
            "status": "modified",
            "additions": 25,
            "deletions": 12,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodeling_sam3_tracker_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodeling_sam3_tracker_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodeling_sam3_tracker_video.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -692,6 +692,12 @@ def _init_weights(self, module):\n         if isinstance(module, Sam3TrackerVideoMemoryFuserCXBlock):\n             if module.scale is not None:\n                 init.zeros_(module.scale)\n+        elif isinstance(module, Sam3TrackerVideoVisionRotaryEmbedding):\n+            inv_freq = module.create_inv_freq()\n+            init.copy_(module.rope_embeddings_cos, inv_freq.cos())\n+            init.copy_(module.rope_embeddings_sin, inv_freq.sin())\n+        elif isinstance(module, Sam3TrackerVideoPositionalEmbedding):\n+            init.normal_(module.positional_embedding, std=module.scale)\n \n \n class Sam3TrackerVideoVisionRotaryEmbedding(nn.Module):\n@@ -702,24 +708,17 @@ class Sam3TrackerVideoVisionRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: Sam3TrackerVideoConfig):\n         super().__init__()\n-        dim = config.memory_attention_hidden_size // (\n+        self.dim = config.memory_attention_hidden_size // (\n             config.memory_attention_downsample_rate * config.memory_attention_num_attention_heads\n         )\n         # Ensure even dimension for proper axial splitting\n-        if dim % 4 != 0:\n+        if self.dim % 4 != 0:\n             raise ValueError(\"Dimension must be divisible by 4 for axial RoPE\")\n-        end_x, end_y = config.memory_attention_rope_feat_sizes\n-        freqs = 1.0 / (config.memory_attention_rope_theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n+        self.end_x, self.end_y = config.memory_attention_rope_feat_sizes\n+        self.memory_attention_rope_theta = config.memory_attention_rope_theta\n \n-        # Generate 2D position indices for axial rotary embedding\n-        flattened_indices = torch.arange(end_x * end_y, dtype=torch.long)\n-        x_positions = flattened_indices % end_x\n-        y_positions = torch.div(flattened_indices, end_x, rounding_mode=\"floor\")\n-        freqs_x = torch.outer(x_positions, freqs).float()\n-        freqs_y = torch.outer(y_positions, freqs).float()\n-        inv_freq = torch.cat([freqs_x, freqs_y], dim=-1)\n-        inv_freq = inv_freq.repeat_interleave(2, dim=-1)\n         # directly register the cos and sin embeddings as we have a fixed feature shape\n+        inv_freq = self.create_inv_freq()\n         self.register_buffer(\"rope_embeddings_cos\", inv_freq.cos(), persistent=False)\n         self.register_buffer(\"rope_embeddings_sin\", inv_freq.sin(), persistent=False)\n \n@@ -728,6 +727,20 @@ def forward(self) -> tuple[torch.Tensor, torch.Tensor]:\n         # As the feature map size is fixed, we can just return the pre-computed embeddings.\n         return self.rope_embeddings_cos, self.rope_embeddings_sin\n \n+    def create_inv_freq(self):\n+        freqs = 1.0 / (\n+            self.memory_attention_rope_theta ** (torch.arange(0, self.dim, 4)[: (self.dim // 4)].float() / self.dim)\n+        )\n+        # Generate 2D position indices for axial rotary embedding\n+        flattened_indices = torch.arange(self.end_x * self.end_y, dtype=torch.long)\n+        x_positions = flattened_indices % self.end_x\n+        y_positions = torch.div(flattened_indices, self.end_x, rounding_mode=\"floor\")\n+        freqs_x = torch.outer(x_positions, freqs).float()\n+        freqs_y = torch.outer(y_positions, freqs).float()\n+        inv_freq = torch.cat([freqs_x, freqs_y], dim=-1)\n+        inv_freq = inv_freq.repeat_interleave(2, dim=-1)\n+        return inv_freq\n+\n \n def rotate_pairwise(x):\n     \"\"\""
        },
        {
            "sha": "e9b337928ccdc5845235ee39b4b78167d84217af",
            "filename": "src/transformers/models/sam_hq/configuration_sam_hq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fconfiguration_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fconfiguration_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fconfiguration_sam_hq.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -188,6 +188,7 @@ def __init__(\n         self.global_attn_indexes = global_attn_indexes\n         self.num_pos_feats = num_pos_feats\n         self.mlp_dim = int(hidden_size * mlp_ratio) if mlp_dim is None else mlp_dim\n+        self.scale = self.hidden_size // 2\n \n \n class SamHQMaskDecoderConfig(PreTrainedConfig):"
        },
        {
            "sha": "1d93469c6c318a35f4835889e9f930a7c613a9d9",
            "filename": "src/transformers/models/sam_hq/modeling_sam_hq.py",
            "status": "modified",
            "additions": 25,
            "deletions": 23,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -413,6 +413,29 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.FloatTensor]:\n         return hidden_states\n \n \n+class SamHQPositionalEmbedding(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.scale = config.scale\n+        self.register_buffer(\"positional_embedding\", self.scale * torch.randn((2, config.num_pos_feats)))\n+\n+    def forward(self, input_coords, input_shape=None):\n+        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n+        coordinates = input_coords.clone()\n+\n+        if input_shape is not None:\n+            coordinates[:, :, :, 0] = coordinates[:, :, :, 0] / input_shape[1]\n+            coordinates[:, :, :, 1] = coordinates[:, :, :, 1] / input_shape[0]\n+\n+        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n+        coordinates = 2 * coordinates - 1\n+        coordinates = coordinates.to(self.positional_embedding.dtype)\n+        coordinates = coordinates @ self.positional_embedding\n+        coordinates = 2 * np.pi * coordinates\n+        # outputs d_1 x ... x d_n x channel shape\n+        return torch.cat([torch.sin(coordinates), torch.cos(coordinates)], dim=-1)\n+\n+\n @auto_docstring\n class SamHQPreTrainedModel(PreTrainedModel):\n     config: SamHQConfig\n@@ -433,6 +456,8 @@ def _init_weights(self, module: nn.Module):\n         elif isinstance(module, SamHQVisionEncoder):\n             if self.config.use_abs_pos:\n                 init.zeros_(module.pos_embed)\n+        elif isinstance(module, SamHQPositionalEmbedding):\n+            init.normal_(module.positional_embedding, std=module.scale)\n \n \n class SamHQPatchEmbeddings(nn.Module):\n@@ -1070,29 +1095,6 @@ def forward(\n         return self.vision_encoder(pixel_values, **kwargs)\n \n \n-class SamHQPositionalEmbedding(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.scale = config.hidden_size // 2\n-        self.register_buffer(\"positional_embedding\", self.scale * torch.randn((2, config.num_pos_feats)))\n-\n-    def forward(self, input_coords, input_shape=None):\n-        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n-        coordinates = input_coords.clone()\n-\n-        if input_shape is not None:\n-            coordinates[:, :, :, 0] = coordinates[:, :, :, 0] / input_shape[1]\n-            coordinates[:, :, :, 1] = coordinates[:, :, :, 1] / input_shape[0]\n-\n-        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n-        coordinates = 2 * coordinates - 1\n-        coordinates = coordinates.to(self.positional_embedding.dtype)\n-        coordinates = coordinates @ self.positional_embedding\n-        coordinates = 2 * np.pi * coordinates\n-        # outputs d_1 x ... x d_n x channel shape\n-        return torch.cat([torch.sin(coordinates), torch.cos(coordinates)], dim=-1)\n-\n-\n class SamHQMaskEmbedding(nn.Module):\n     def __init__(self, config: SamHQPromptEncoderConfig):\n         super().__init__()"
        },
        {
            "sha": "1d579ff719495e251f2644b648e4109ee95ee8a0",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 16,
            "deletions": 1,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -884,13 +884,14 @@ def forward(self, input_ids: torch.Tensor):\n         return super().forward(input_ids) * self.embed_scale\n \n \n-# Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding\n+# Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding with M2M100->SeamlessM4T\n class SeamlessM4TSinusoidalPositionalEmbedding(nn.Module):\n     \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n \n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None):\n         super().__init__()\n         self.offset = 2\n+        self.num_positions = num_positions\n         self.embedding_dim = embedding_dim\n         self.padding_idx = padding_idx\n         self.make_weights(num_positions + self.offset, embedding_dim, padding_idx)\n@@ -1375,11 +1376,25 @@ def _init_weights(self, module: nn.Module):\n         elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n         elif isinstance(module, nn.Conv1d):\n             init.kaiming_normal_(module.weight)\n             if module.bias is not None:\n                 k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n                 init.uniform_(module.bias, a=-k, b=k)\n+        elif isinstance(module, SeamlessM4TSinusoidalPositionalEmbedding):\n+            emb_weights = module.get_embedding(\n+                module.num_positions + module.offset, module.embedding_dim, module.padding_idx\n+            )\n+            init.copy_(module.weights, emb_weights)\n+        elif isinstance(module, SeamlessM4TConformerRotaryPositionalEmbedding):\n+            dim = self.config.hidden_size // self.config.speech_encoder_attention_heads\n+            base = self.config.rotary_embedding_base\n+            inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n+            init.copy_(module.inv_freq, inv_freq)\n \n     def _compute_sub_sample_lengths_from_attention_mask(self, attention_mask):\n         kernel_size, stride = self.config.adaptor_kernel_size, self.config.adaptor_stride"
        },
        {
            "sha": "25d8e0c35758505456fcbb3aaa0b2aaff1039b63",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -762,6 +762,7 @@ class SeamlessM4Tv2SinusoidalPositionalEmbedding(nn.Module):\n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None):\n         super().__init__()\n         self.offset = 2\n+        self.num_positions = num_positions\n         self.embedding_dim = embedding_dim\n         self.padding_idx = padding_idx\n         self.make_weights(num_positions + self.offset, embedding_dim, padding_idx)\n@@ -1292,6 +1293,11 @@ def _init_weights(self, module: nn.Module):\n             if module.bias is not None:\n                 k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n                 init.uniform_(module.bias, a=-k, b=k)\n+        elif isinstance(module, SeamlessM4Tv2SinusoidalPositionalEmbedding):\n+            emb_weights = module.get_embedding(\n+                module.num_positions + module.offset, module.embedding_dim, module.padding_idx\n+            )\n+            init.copy_(module.weights, emb_weights)\n \n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel._compute_sub_sample_lengths_from_attention_mask\n     def _compute_sub_sample_lengths_from_attention_mask(self, attention_mask):"
        },
        {
            "sha": "fc4ab3578b98eee6f7f91523d7b1bd17ad2005e3",
            "filename": "src/transformers/models/seed_oss/modeling_seed_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -311,7 +311,7 @@ def __init__(self, config: SeedOssConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "c3393247a0fc79c6863631d78f90ee69f235f94c",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -430,6 +430,8 @@ def _init_weights(self, module):\n                 else self.config.hidden_size\n             )\n             init.normal_(module.position_embedding.weight, std=1 / np.sqrt(width))\n+            if hasattr(module, \"position_ids\"):\n+                init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         elif isinstance(module, nn.Embedding):\n             default_flax_embed_init(module.weight)\n         elif isinstance(module, SiglipAttention):\n@@ -465,6 +467,8 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+        elif isinstance(module, SiglipTextEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoder with AltCLIP->Siglip"
        },
        {
            "sha": "3926cb7fb1e737e6a2554b005d20961df8fa138b",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 44,
            "deletions": 40,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -215,6 +215,46 @@ def forward(self, pixel_values: torch.FloatTensor, spatial_shapes: torch.LongTen\n         return embeddings\n \n \n+class Siglip2TextEmbeddings(nn.Module):\n+    def __init__(self, config: Siglip2TextConfig):\n+        super().__init__()\n+        embed_dim = config.hidden_size\n+\n+        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)\n+        self.position_embedding = nn.Embedding(config.max_position_embeddings, embed_dim)\n+\n+        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n+        self.register_buffer(\n+            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n+        )\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+    ) -> torch.Tensor:\n+        seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n+        max_position_embedding = self.position_embedding.weight.shape[0]\n+\n+        if seq_length > max_position_embedding:\n+            raise ValueError(\n+                f\"Sequence length must be less than max_position_embeddings (got `sequence length`: \"\n+                f\"{seq_length} and max_position_embeddings: {max_position_embedding}\"\n+            )\n+\n+        if position_ids is None:\n+            position_ids = self.position_ids[:, :seq_length]\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.token_embedding(input_ids)\n+\n+        position_embeddings = self.position_embedding(position_ids)\n+        embeddings = inputs_embeds + position_embeddings\n+\n+        return embeddings\n+\n+\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -412,6 +452,8 @@ def _init_weights(self, module):\n                 else self.config.hidden_size\n             )\n             init.normal_(module.position_embedding.weight, std=1 / np.sqrt(width))\n+            if hasattr(module, \"position_ids\"):\n+                init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n         elif isinstance(module, nn.Embedding):\n             default_flax_embed_init(module.weight)\n         elif isinstance(module, Siglip2Attention):\n@@ -447,6 +489,8 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+        elif isinstance(module, Siglip2TextEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n class Siglip2Encoder(nn.Module):\n@@ -552,46 +596,6 @@ def forward(\n         )\n \n \n-class Siglip2TextEmbeddings(nn.Module):\n-    def __init__(self, config: Siglip2TextConfig):\n-        super().__init__()\n-        embed_dim = config.hidden_size\n-\n-        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)\n-        self.position_embedding = nn.Embedding(config.max_position_embeddings, embed_dim)\n-\n-        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.register_buffer(\n-            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n-        )\n-\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-    ) -> torch.Tensor:\n-        seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n-        max_position_embedding = self.position_embedding.weight.shape[0]\n-\n-        if seq_length > max_position_embedding:\n-            raise ValueError(\n-                f\"Sequence length must be less than max_position_embeddings (got `sequence length`: \"\n-                f\"{seq_length} and max_position_embeddings: {max_position_embedding}\"\n-            )\n-\n-        if position_ids is None:\n-            position_ids = self.position_ids[:, :seq_length]\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.token_embedding(input_ids)\n-\n-        position_embeddings = self.position_embedding(position_ids)\n-        embeddings = inputs_embeds + position_embeddings\n-\n-        return embeddings\n-\n-\n class Siglip2TextTransformer(Siglip2PreTrainedModel):\n     _input_embed_layer = \"token_embedding\"\n "
        },
        {
            "sha": "5fa88d96b5ec963883ace9e15c8c7ff9c1dd801c",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -63,7 +63,7 @@ def __init__(self, config: SmolLM3Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "9fc7d1ffc73d0fae1a910d65cebd733a7c60ea7d",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -105,6 +106,7 @@ class Speech2TextSinusoidalPositionalEmbedding(nn.Module):\n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None):\n         super().__init__()\n         self.offset = 2\n+        self.num_positions = num_positions\n         self.embedding_dim = embedding_dim\n         self.padding_idx = padding_idx\n         self.make_weights(num_positions + self.offset, embedding_dim, padding_idx)\n@@ -495,6 +497,14 @@ class Speech2TextPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = False\n     _supports_flex_attn = False\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, Speech2TextSinusoidalPositionalEmbedding):\n+            emb_weights = module.get_embedding(\n+                module.num_positions + module.offset, module.embedding_dim, module.padding_idx\n+            )\n+            init.copy_(module.weights, emb_weights)\n+\n     def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):\n         \"\"\"\n         Computes the output length of the convolutional layers"
        },
        {
            "sha": "79ad1f23bed00c1c11552334dabff8604222bcc9",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -290,6 +290,7 @@ class SpeechT5SinusoidalPositionalEmbedding(nn.Module):\n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None):\n         super().__init__()\n         self.offset = 2\n+        self.num_positions = num_positions\n         self.embedding_dim = embedding_dim\n         self.padding_idx = padding_idx\n         self.make_weights(num_positions + self.offset, embedding_dim, padding_idx)\n@@ -414,6 +415,7 @@ def __init__(self, dropout, dim, max_len=5000):\n         self.register_buffer(\"pe\", pe, persistent=False)\n         self.dropout = nn.Dropout(p=dropout)\n         self.dim = dim\n+        self.max_len = max_len\n         self.alpha = nn.Parameter(torch.tensor(1.0))\n \n     def forward(self, emb):\n@@ -1184,6 +1186,14 @@ def _init_weights(self, module: nn.Module):\n             init.constant_(module.conv.bias, 0)\n         elif isinstance(module, SpeechT5ScaledPositionalEncoding):\n             init.ones_(module.alpha)\n+            dim, max_len = module.dim, module.max_len\n+            pe = torch.zeros(max_len, dim)\n+            position = torch.arange(0, max_len).unsqueeze(1)\n+            div_term = torch.exp(torch.arange(0, dim, 2, dtype=torch.int64).float() * -(math.log(10000.0) / dim))\n+            pe[:, 0::2] = torch.sin(position.float() * div_term)\n+            pe[:, 1::2] = torch.cos(position.float() * div_term)\n+            pe = pe.unsqueeze(0)\n+            init.copy_(module.pe, pe)\n         elif isinstance(module, SpeechT5FeatureProjection):\n             k = math.sqrt(1 / module.projection.in_features)\n             init.uniform_(module.projection.weight, a=-k, b=k)\n@@ -1195,6 +1205,10 @@ def _init_weights(self, module: nn.Module):\n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm1d)):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n         elif isinstance(module, nn.Conv1d):\n             init.kaiming_normal_(module.weight)\n             if module.bias is not None:\n@@ -1205,6 +1219,14 @@ def _init_weights(self, module: nn.Module):\n             # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n             if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n                 init.zeros_(module.weight[module.padding_idx])\n+        elif isinstance(module, SpeechT5SinusoidalPositionalEmbedding):\n+            emb_weights = module.get_embedding(\n+                module.num_positions + module.offset, module.embedding_dim, module.padding_idx\n+            )\n+            init.copy_(module.weights, emb_weights)\n+        elif isinstance(module, SpeechT5HifiGan):\n+            init.zeros_(module.mean)\n+            init.ones_(module.scale)\n \n         if hasattr(module, \"masked_spec_embed\"):\n             init.uniform_(module.masked_spec_embed)\n@@ -3008,6 +3030,12 @@ def __init__(self, config: SpeechT5HifiGanConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, SpeechT5HifiGan):\n+            init.zeros_(module.mean)\n+            init.ones_(module.scale)\n+\n     def apply_weight_norm(self):\n         weight_norm = nn.utils.weight_norm\n         if hasattr(nn.utils.parametrizations, \"weight_norm\"):"
        },
        {
            "sha": "004328cd122be00c9bfbe4883b468ef7c538a87a",
            "filename": "src/transformers/models/splinter/modeling_splinter.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, ModelOutput, QuestionAnsweringModelOutput\n@@ -331,6 +332,11 @@ class SplinterPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"splinter\"\n     supports_gradient_checkpointing = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, SplinterEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n+\n \n @auto_docstring\n class SplinterModel(SplinterPreTrainedModel):"
        },
        {
            "sha": "6b4ad2ea62eb4c9dcb823e52cf4f84297236ded0",
            "filename": "src/transformers/models/squeezebert/modeling_squeezebert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fmodeling_squeezebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fmodeling_squeezebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fmodeling_squeezebert.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -412,6 +412,8 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, SqueezeBertLMPredictionHead):\n             init.zeros_(module.bias)\n+        elif isinstance(module, SqueezeBertEmbeddings):\n+            init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n \n @auto_docstring"
        },
        {
            "sha": "9d5138c5eb4cad07e7eaf8c50074520cd9b5a176",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -76,7 +76,7 @@ def __init__(self, config: StableLmConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     # Ignore copy"
        },
        {
            "sha": "560a4d4c9807efbd9e23f745f791bad216a4798b",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -289,7 +289,7 @@ def __init__(self, config: Starcoder2Config, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "3a5042d07542a6128549868d3662eacbe0f35d08",
            "filename": "src/transformers/models/swiftformer/modeling_swiftformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -400,6 +400,10 @@ def _init_weights(self, module: nn.Module) -> None:\n         elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d)):\n             init.constant_(module.bias, 0)\n             init.constant_(module.weight, 1.0)\n+            if getattr(module, \"running_mean\", None) is not None:\n+                init.zeros_(module.running_mean)\n+                init.ones_(module.running_var)\n+                init.zeros_(module.num_batches_tracked)\n         elif isinstance(module, (SwiftFormerConvEncoder, SwiftFormerLocalRepresentation)):\n             init.ones_(module.layer_scale)\n         elif isinstance(module, SwiftFormerEncoderBlock):"
        },
        {
            "sha": "28506c2fff0cfb75f18943a67ec79dfee5abdf53",
            "filename": "src/transformers/models/swin/modeling_swin.py",
            "status": "modified",
            "additions": 16,
            "deletions": 12,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -411,18 +411,7 @@ def __init__(self, config, dim, num_heads, window_size):\n             torch.zeros((2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1), num_heads)\n         )\n \n-        # get pair-wise relative position index for each token inside the window\n-        coords_h = torch.arange(self.window_size[0])\n-        coords_w = torch.arange(self.window_size[1])\n-        coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))\n-        coords_flatten = torch.flatten(coords, 1)\n-        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n-        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n-        relative_coords[:, :, 0] += self.window_size[0] - 1\n-        relative_coords[:, :, 1] += self.window_size[1] - 1\n-        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n-        relative_position_index = relative_coords.sum(-1)\n-        self.register_buffer(\"relative_position_index\", relative_position_index)\n+        self.register_buffer(\"relative_position_index\", self.create_relative_position_index())\n \n         self.query = nn.Linear(self.all_head_size, self.all_head_size, bias=config.qkv_bias)\n         self.key = nn.Linear(self.all_head_size, self.all_head_size, bias=config.qkv_bias)\n@@ -481,6 +470,20 @@ def forward(\n \n         return outputs\n \n+    def create_relative_position_index(self):\n+        # get pair-wise relative position index for each token inside the window\n+        coords_h = torch.arange(self.window_size[0])\n+        coords_w = torch.arange(self.window_size[1])\n+        coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))\n+        coords_flatten = torch.flatten(coords, 1)\n+        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n+        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n+        relative_coords[:, :, 0] += self.window_size[0] - 1\n+        relative_coords[:, :, 1] += self.window_size[1] - 1\n+        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n+        relative_position_index = relative_coords.sum(-1)\n+        return relative_position_index\n+\n \n class SwinSelfOutput(nn.Module):\n     def __init__(self, config, dim):\n@@ -823,6 +826,7 @@ def _init_weights(self, module):\n                 init.zeros_(module.position_embeddings)\n         elif isinstance(module, SwinSelfAttention):\n             init.zeros_(module.relative_position_bias_table)\n+            init.copy_(module.relative_position_index, module.create_relative_position_index())\n \n \n @auto_docstring"
        },
        {
            "sha": "662580e1cda8033084bc865b4abc07c5e5fb6c4d",
            "filename": "src/transformers/models/swin2sr/modeling_swin2sr.py",
            "status": "modified",
            "additions": 49,
            "deletions": 33,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -250,40 +250,8 @@ def __init__(self, config, dim, num_heads, window_size, pretrained_window_size=[\n             nn.Linear(2, 512, bias=True), nn.ReLU(inplace=True), nn.Linear(512, num_heads, bias=False)\n         )\n \n-        # get relative_coords_table\n-        relative_coords_h = torch.arange(-(self.window_size[0] - 1), self.window_size[0], dtype=torch.int64).float()\n-        relative_coords_w = torch.arange(-(self.window_size[1] - 1), self.window_size[1], dtype=torch.int64).float()\n-        relative_coords_table = (\n-            torch.stack(meshgrid([relative_coords_h, relative_coords_w], indexing=\"ij\"))\n-            .permute(1, 2, 0)\n-            .contiguous()\n-            .unsqueeze(0)\n-        )  # [1, 2*window_height - 1, 2*window_width - 1, 2]\n-        if pretrained_window_size[0] > 0:\n-            relative_coords_table[:, :, :, 0] /= pretrained_window_size[0] - 1\n-            relative_coords_table[:, :, :, 1] /= pretrained_window_size[1] - 1\n-        elif window_size > 1:\n-            relative_coords_table[:, :, :, 0] /= self.window_size[0] - 1\n-            relative_coords_table[:, :, :, 1] /= self.window_size[1] - 1\n-        relative_coords_table *= 8  # normalize to -8, 8\n-        relative_coords_table = (\n-            torch.sign(relative_coords_table) * torch.log2(torch.abs(relative_coords_table) + 1.0) / math.log2(8)\n-        )\n-        # set to same dtype as mlp weight\n-        relative_coords_table = relative_coords_table.to(next(self.continuous_position_bias_mlp.parameters()).dtype)\n+        relative_coords_table, relative_position_index = self.create_coords_table_and_index()\n         self.register_buffer(\"relative_coords_table\", relative_coords_table, persistent=False)\n-\n-        # get pair-wise relative position index for each token inside the window\n-        coords_h = torch.arange(self.window_size[0])\n-        coords_w = torch.arange(self.window_size[1])\n-        coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))\n-        coords_flatten = torch.flatten(coords, 1)\n-        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n-        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n-        relative_coords[:, :, 0] += self.window_size[0] - 1\n-        relative_coords[:, :, 1] += self.window_size[1] - 1\n-        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n-        relative_position_index = relative_coords.sum(-1)\n         self.register_buffer(\"relative_position_index\", relative_position_index, persistent=False)\n \n         self.query = nn.Linear(self.all_head_size, self.all_head_size, bias=config.qkv_bias)\n@@ -359,6 +327,43 @@ def forward(\n \n         return outputs\n \n+    def create_coords_table_and_index(self):\n+        # get relative_coords_table\n+        relative_coords_h = torch.arange(-(self.window_size[0] - 1), self.window_size[0], dtype=torch.int64).float()\n+        relative_coords_w = torch.arange(-(self.window_size[1] - 1), self.window_size[1], dtype=torch.int64).float()\n+        relative_coords_table = (\n+            torch.stack(meshgrid([relative_coords_h, relative_coords_w], indexing=\"ij\"))\n+            .permute(1, 2, 0)\n+            .contiguous()\n+            .unsqueeze(0)\n+        )  # [1, 2*window_height - 1, 2*window_width - 1, 2]\n+        if self.pretrained_window_size[0] > 0:\n+            relative_coords_table[:, :, :, 0] /= self.pretrained_window_size[0] - 1\n+            relative_coords_table[:, :, :, 1] /= self.pretrained_window_size[1] - 1\n+        elif self.window_size[0] > 1:\n+            relative_coords_table[:, :, :, 0] /= self.window_size[0] - 1\n+            relative_coords_table[:, :, :, 1] /= self.window_size[1] - 1\n+        relative_coords_table *= 8  # normalize to -8, 8\n+        relative_coords_table = (\n+            torch.sign(relative_coords_table) * torch.log2(torch.abs(relative_coords_table) + 1.0) / math.log2(8)\n+        )\n+        # set to same dtype as mlp weight\n+        relative_coords_table = relative_coords_table.to(next(self.continuous_position_bias_mlp.parameters()).dtype)\n+\n+        # get pair-wise relative position index for each token inside the window\n+        coords_h = torch.arange(self.window_size[0])\n+        coords_w = torch.arange(self.window_size[1])\n+        coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))\n+        coords_flatten = torch.flatten(coords, 1)\n+        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n+        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n+        relative_coords[:, :, 0] += self.window_size[0] - 1\n+        relative_coords[:, :, 1] += self.window_size[1] - 1\n+        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n+        relative_position_index = relative_coords.sum(-1)\n+\n+        return relative_coords_table, relative_position_index\n+\n \n # Copied from transformers.models.swin.modeling_swin.SwinSelfOutput with Swin->Swin2SR\n class Swin2SRSelfOutput(nn.Module):\n@@ -702,6 +707,17 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n+        elif isinstance(module, Swin2SRSelfAttention):\n+            init.constant_(module.logit_scale, math.log(10))\n+            relative_coords_table, relative_position_index = module.create_coords_table_and_index()\n+            init.copy_(module.relative_coords_table, relative_coords_table)\n+            init.copy_(module.relative_position_index, relative_position_index)\n+        elif isinstance(module, Swin2SRModel):\n+            if module.config.num_channels == 3 and module.config.num_channels_out == 3:\n+                mean = torch.tensor([0.4488, 0.4371, 0.4040]).view(1, 3, 1, 1)\n+            else:\n+                mean = torch.zeros(1, 1, 1, 1)\n+            init.copy_(module.mean, mean)\n \n \n @auto_docstring"
        },
        {
            "sha": "306617301954b0c7fb3bbe6f47e54cb35b039f5f",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 41,
            "deletions": 33,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -421,40 +421,8 @@ def __init__(self, config, dim, num_heads, window_size, pretrained_window_size=[\n             nn.Linear(2, 512, bias=True), nn.ReLU(inplace=True), nn.Linear(512, num_heads, bias=False)\n         )\n \n-        # get relative_coords_table\n-        relative_coords_h = torch.arange(-(self.window_size[0] - 1), self.window_size[0], dtype=torch.int64).float()\n-        relative_coords_w = torch.arange(-(self.window_size[1] - 1), self.window_size[1], dtype=torch.int64).float()\n-        relative_coords_table = (\n-            torch.stack(meshgrid([relative_coords_h, relative_coords_w], indexing=\"ij\"))\n-            .permute(1, 2, 0)\n-            .contiguous()\n-            .unsqueeze(0)\n-        )  # [1, 2*window_height - 1, 2*window_width - 1, 2]\n-        if pretrained_window_size[0] > 0:\n-            relative_coords_table[:, :, :, 0] /= pretrained_window_size[0] - 1\n-            relative_coords_table[:, :, :, 1] /= pretrained_window_size[1] - 1\n-        elif window_size > 1:\n-            relative_coords_table[:, :, :, 0] /= self.window_size[0] - 1\n-            relative_coords_table[:, :, :, 1] /= self.window_size[1] - 1\n-        relative_coords_table *= 8  # normalize to -8, 8\n-        relative_coords_table = (\n-            torch.sign(relative_coords_table) * torch.log2(torch.abs(relative_coords_table) + 1.0) / math.log2(8)\n-        )\n-        # set to same dtype as mlp weight\n-        relative_coords_table = relative_coords_table.to(next(self.continuous_position_bias_mlp.parameters()).dtype)\n+        relative_coords_table, relative_position_index = self.create_coords_table_and_index()\n         self.register_buffer(\"relative_coords_table\", relative_coords_table, persistent=False)\n-\n-        # get pair-wise relative position index for each token inside the window\n-        coords_h = torch.arange(self.window_size[0])\n-        coords_w = torch.arange(self.window_size[1])\n-        coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))\n-        coords_flatten = torch.flatten(coords, 1)\n-        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n-        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n-        relative_coords[:, :, 0] += self.window_size[0] - 1\n-        relative_coords[:, :, 1] += self.window_size[1] - 1\n-        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n-        relative_position_index = relative_coords.sum(-1)\n         self.register_buffer(\"relative_position_index\", relative_position_index, persistent=False)\n \n         self.query = nn.Linear(self.all_head_size, self.all_head_size, bias=config.qkv_bias)\n@@ -530,6 +498,43 @@ def forward(\n \n         return outputs\n \n+    def create_coords_table_and_index(self):\n+        # get relative_coords_table\n+        relative_coords_h = torch.arange(-(self.window_size[0] - 1), self.window_size[0], dtype=torch.int64).float()\n+        relative_coords_w = torch.arange(-(self.window_size[1] - 1), self.window_size[1], dtype=torch.int64).float()\n+        relative_coords_table = (\n+            torch.stack(meshgrid([relative_coords_h, relative_coords_w], indexing=\"ij\"))\n+            .permute(1, 2, 0)\n+            .contiguous()\n+            .unsqueeze(0)\n+        )  # [1, 2*window_height - 1, 2*window_width - 1, 2]\n+        if self.pretrained_window_size[0] > 0:\n+            relative_coords_table[:, :, :, 0] /= self.pretrained_window_size[0] - 1\n+            relative_coords_table[:, :, :, 1] /= self.pretrained_window_size[1] - 1\n+        elif self.window_size[0] > 1:\n+            relative_coords_table[:, :, :, 0] /= self.window_size[0] - 1\n+            relative_coords_table[:, :, :, 1] /= self.window_size[1] - 1\n+        relative_coords_table *= 8  # normalize to -8, 8\n+        relative_coords_table = (\n+            torch.sign(relative_coords_table) * torch.log2(torch.abs(relative_coords_table) + 1.0) / math.log2(8)\n+        )\n+        # set to same dtype as mlp weight\n+        relative_coords_table = relative_coords_table.to(next(self.continuous_position_bias_mlp.parameters()).dtype)\n+\n+        # get pair-wise relative position index for each token inside the window\n+        coords_h = torch.arange(self.window_size[0])\n+        coords_w = torch.arange(self.window_size[1])\n+        coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))\n+        coords_flatten = torch.flatten(coords, 1)\n+        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n+        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n+        relative_coords[:, :, 0] += self.window_size[0] - 1\n+        relative_coords[:, :, 1] += self.window_size[1] - 1\n+        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n+        relative_position_index = relative_coords.sum(-1)\n+\n+        return relative_coords_table, relative_position_index\n+\n \n # Copied from transformers.models.swin.modeling_swin.SwinSelfOutput with Swin->Swinv2\n class Swinv2SelfOutput(nn.Module):\n@@ -904,6 +909,9 @@ def _init_weights(self, module):\n                 init.zeros_(module.position_embeddings)\n         elif isinstance(module, Swinv2SelfAttention):\n             init.constant_(module.logit_scale, math.log(10))\n+            relative_coords_table, relative_position_index = module.create_coords_table_and_index()\n+            init.copy_(module.relative_coords_table, relative_coords_table)\n+            init.copy_(module.relative_position_index, relative_position_index)\n \n \n @auto_docstring"
        },
        {
            "sha": "c49c684bc3be5d0e477009e21a9ce2a06c48dd38",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -108,7 +108,7 @@ def __init__(self, config: T5GemmaConfig, device=None):\n         inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n \n     @staticmethod\n     def compute_default_rope_parameters("
        },
        {
            "sha": "0365c5169a3f6736da771750546c688bfa5c157d",
            "filename": "src/transformers/models/t5gemma2/modeling_t5gemma2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -113,7 +113,7 @@ def __init__(self, config: T5Gemma2TextConfig, device=None):\n                 rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type[layer_type]]\n             curr_inv_freq, curr_attention_scaling = rope_init_fn(self.config, device, layer_type=layer_type)\n             self.register_buffer(f\"{layer_type}_inv_freq\", curr_inv_freq, persistent=False)\n-            setattr(self, f\"{layer_type}_original_inv_freq\", curr_inv_freq)\n+            self.register_buffer(f\"{layer_type}_original_inv_freq\", curr_inv_freq.clone(), persistent=False)\n             setattr(self, f\"{layer_type}_attention_scaling\", curr_attention_scaling)\n \n     @staticmethod\n@@ -648,6 +648,7 @@ def __init__(\n         eoi_token_index: int = 256_000,\n     ):\n         super().__init__(num_embeddings, embedding_dim, padding_idx)\n+        self.scalar_embed_scale = embed_scale\n         self.register_buffer(\"embed_scale\", torch.tensor(embed_scale), persistent=False)\n         self.eoi_token_index = eoi_token_index\n         self.eoi_embedding = nn.Parameter(torch.zeros(self.embedding_dim))\n@@ -699,6 +700,7 @@ def _init_weights(self, module):\n             init.zeros_(module.mm_input_projection_weight)\n         elif isinstance(module, T5Gemma2TextScaledWordEmbedding):\n             init.zeros_(module.eoi_embedding)\n+            init.constant_(module.embed_scale, module.scalar_embed_scale)\n         elif isinstance(module, T5Gemma2ClassificationHead):\n             scale = module.out_proj.weight.shape[0] ** -0.5\n             init.normal_(module.out_proj.weight, mean=0.0, std=self.config.initializer_range * scale)\n@@ -707,6 +709,14 @@ def _init_weights(self, module):\n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         elif \"RMSNorm\" in module.__class__.__name__:\n             init.zeros_(module.weight)\n+        elif isinstance(module, T5Gemma2RotaryEmbedding):\n+            for layer_type in module.layer_types:\n+                rope_init_fn = module.compute_default_rope_parameters\n+                if module.rope_type[layer_type] != \"default\":\n+                    rope_init_fn = ROPE_INIT_FUNCTIONS[module.rope_type[layer_type]]\n+                curr_inv_freq, _ = rope_init_fn(module.config, layer_type=layer_type)\n+                init.copy_(getattr(module, f\"{layer_type}_inv_freq\"), curr_inv_freq)\n+                init.copy_(getattr(module, f\"{layer_type}_original_inv_freq\"), curr_inv_freq)\n \n     def prepare_decoder_input_ids_from_labels(self, input_ids):\n         \"\"\""
        },
        {
            "sha": "9f23077a16efac12ce61ad143337101df0a9e674",
            "filename": "src/transformers/models/t5gemma2/modular_t5gemma2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodular_t5gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/537c2e3d7e4e2df84e0f3c1148b85fffcf684110/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodular_t5gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodular_t5gemma2.py?ref=537c2e3d7e4e2df84e0f3c1148b85fffcf684110",
            "patch": "@@ -34,7 +34,7 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import RopeParameters\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, RopeParameters\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -774,6 +774,7 @@ def _init_weights(self, module):\n             init.zeros_(module.mm_input_projection_weight)\n         elif isinstance(module, T5Gemma2TextScaledWordEmbedding):\n             init.zeros_(module.eoi_embedding)\n+            init.constant_(module.embed_scale, module.scalar_embed_scale)\n         elif isinstance(module, T5Gemma2ClassificationHead):\n             scale = module.out_proj.weight.shape[0] ** -0.5\n             init.normal_(module.out_proj.weight, mean=0.0, std=self.config.initializer_range * scale)\n@@ -782,6 +783,14 @@ def _init_weights(self, module):\n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         elif \"RMSNorm\" in module.__class__.__name__:\n             init.zeros_(module.weight)\n+        elif isinstance(module, T5Gemma2RotaryEmbedding):\n+            for layer_type in module.layer_types:\n+                rope_init_fn = module.compute_default_rope_parameters\n+                if module.rope_type[layer_type] != \"default\":\n+                    rope_init_fn = ROPE_INIT_FUNCTIONS[module.rope_type[layer_type]]\n+                curr_inv_freq, _ = rope_init_fn(module.config, layer_type=layer_type)\n+                init.copy_(getattr(module, f\"{layer_type}_inv_freq\"), curr_inv_freq)\n+                init.copy_(getattr(module, f\"{layer_type}_original_inv_freq\"), curr_inv_freq)\n \n     def prepare_decoder_input_ids_from_labels(self, input_ids):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 3978,
        "additions": 2801,
        "deletions": 1177
    }
}