{
    "author": "ManukyanD",
    "message": "Fix synced multi-GPU generation with LLMs and VLMs (#35893)\n\n* Fix synced multi-GPU generation\r\n\r\n* fix copies\r\n\r\n---------\r\n\r\nCo-authored-by: Davit Manukyan <ManukyanD>\r\nCo-authored-by: Raushan Turganbay <raushan@huggingface.co>",
    "sha": "d8080d55c789acea91c40300da6deee849cd8f77",
    "files": [
        {
            "sha": "2a118b7a36750a438649882aafde76e0fbc34c1e",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 13,
            "deletions": 2,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=d8080d55c789acea91c40300da6deee849cd8f77",
            "patch": "@@ -40,7 +40,13 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_torchdynamo_compiling,\n+    logging,\n+    replace_return_docstrings,\n+)\n from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import (\n     is_causal_conv1d_available,\n@@ -1578,8 +1584,13 @@ def prepare_inputs_for_generation(\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+        #              (we can't check exception 3 while compiling)\n         if not empty_past_kv:\n-            if inputs_embeds is not None:  # Exception 1\n+            if (\n+                inputs_embeds is not None  # Exception 1\n+                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+            ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]"
        },
        {
            "sha": "57e04b77dda467ba475f33eaddf97baf2444f546",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=d8080d55c789acea91c40300da6deee849cd8f77",
            "patch": "@@ -51,6 +51,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1268,8 +1269,13 @@ def prepare_inputs_for_generation(\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+        #              (we can't check exception 3 while compiling)\n         if not empty_past_kv:\n-            if inputs_embeds is not None:  # Exception 1\n+            if (\n+                inputs_embeds is not None  # Exception 1\n+                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+            ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]"
        },
        {
            "sha": "5a4ace462344820aa09fb2d3ffb6c807689118d3",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=d8080d55c789acea91c40300da6deee849cd8f77",
            "patch": "@@ -36,7 +36,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import logging\n+from ...utils import is_torchdynamo_compiling, logging\n from .configuration_bloom import BloomConfig\n \n \n@@ -893,8 +893,13 @@ def prepare_inputs_for_generation(\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+        #              (we can't check exception 3 while compiling)\n         if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n+            if (\n+                inputs_embeds is not None  # Exception 1\n+                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+            ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]"
        },
        {
            "sha": "dd0a92907a14ea515357592b2425f2cfccc31e61",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=d8080d55c789acea91c40300da6deee849cd8f77",
            "patch": "@@ -41,6 +41,7 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1651,8 +1652,13 @@ def prepare_inputs_for_generation(\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+        #              (we can't check exception 3 while compiling)\n         if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n+            if (\n+                inputs_embeds is not None  # Exception 1\n+                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+            ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]"
        },
        {
            "sha": "e900413740cc76f97ad5cd07daee5904657fa8a2",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=d8080d55c789acea91c40300da6deee849cd8f77",
            "patch": "@@ -36,6 +36,7 @@\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -921,8 +922,13 @@ def prepare_inputs_for_generation(\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+        #              (we can't check exception 3 while compiling)\n         if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n+            if (\n+                inputs_embeds is not None  # Exception 1\n+                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+            ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]"
        },
        {
            "sha": "f24e1378ecc7351ed6f8ebe7a46573b333850d06",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=d8080d55c789acea91c40300da6deee849cd8f77",
            "patch": "@@ -29,6 +29,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import (\n+    is_torchdynamo_compiling,\n     logging,\n )\n from ..cohere.modeling_cohere import (\n@@ -590,8 +591,13 @@ def prepare_inputs_for_generation(\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+        #              (we can't check exception 3 while compiling)\n         if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n+            if (\n+                inputs_embeds is not None  # Exception 1\n+                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+            ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]"
        },
        {
            "sha": "320fbdf3593c165837f94ba4e398bc4e5b7140e1",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=d8080d55c789acea91c40300da6deee849cd8f77",
            "patch": "@@ -42,6 +42,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1566,8 +1567,13 @@ def prepare_inputs_for_generation(\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+        #              (we can't check exception 3 while compiling)\n         if not empty_past_kv:\n-            if inputs_embeds is not None:  # Exception 1\n+            if (\n+                inputs_embeds is not None  # Exception 1\n+                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+            ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]"
        },
        {
            "sha": "d9b0d9039169fb67c6767a18565edca896678b1e",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=d8080d55c789acea91c40300da6deee849cd8f77",
            "patch": "@@ -32,6 +32,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -2174,8 +2175,13 @@ def prepare_inputs_for_generation(\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+        #              (we can't check exception 3 while compiling)\n         if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n+            if (\n+                inputs_embeds is not None  # Exception 1\n+                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+            ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]"
        },
        {
            "sha": "242622d293a28a945d4350a3a4ba4a46084acfd8",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=d8080d55c789acea91c40300da6deee849cd8f77",
            "patch": "@@ -45,6 +45,7 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1869,8 +1870,13 @@ def prepare_inputs_for_generation(\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+        #              (we can't check exception 3 while compiling)\n         if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n+            if (\n+                inputs_embeds is not None  # Exception 1\n+                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+            ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]"
        },
        {
            "sha": "87216988b717ac2a058cbb21f63a8a878453b7df",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=d8080d55c789acea91c40300da6deee849cd8f77",
            "patch": "@@ -51,7 +51,7 @@\n from ...image_utils import ImageInput, VideoInput\n from ...processing_utils import ProcessingKwargs, Unpack, VideosKwargs\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import is_flash_attn_2_available\n+from ...utils import is_flash_attn_2_available, is_torchdynamo_compiling\n \n \n if is_flash_attn_2_available():\n@@ -768,8 +768,13 @@ def prepare_inputs_for_generation(\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+        #              (we can't check exception 3 while compiling)\n         if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n+            if (\n+                inputs_embeds is not None  # Exception 1\n+                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+            ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]"
        },
        {
            "sha": "a766f05ee52c54d04ff52503cb8f0efeb1e06e18",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=d8080d55c789acea91c40300da6deee849cd8f77",
            "patch": "@@ -41,6 +41,7 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1732,8 +1733,13 @@ def prepare_inputs_for_generation(\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+        #              (we can't check exception 3 while compiling)\n         if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n+            if (\n+                inputs_embeds is not None  # Exception 1\n+                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+            ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]"
        },
        {
            "sha": "a9a0b64abad5af66331fc38d5dfa0814b8e67f85",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=d8080d55c789acea91c40300da6deee849cd8f77",
            "patch": "@@ -45,6 +45,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1321,7 +1322,12 @@ def prepare_inputs_for_generation(\n             # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n             # Exception 1: when passing input_embeds, input_ids may be missing entries\n             # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-            if inputs_embeds is not None:  # Exception 1\n+            # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+            #              (we can't check exception 3 while compiling)\n+            if (\n+                inputs_embeds is not None  # Exception 1\n+                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+            ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]"
        },
        {
            "sha": "f2d7d21a743ed2733066a965820e6cc76c9271b5",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 2,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8080d55c789acea91c40300da6deee849cd8f77/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=d8080d55c789acea91c40300da6deee849cd8f77",
            "patch": "@@ -37,7 +37,13 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_torchdynamo_compiling,\n+    logging,\n+    replace_return_docstrings,\n+)\n from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available\n from .configuration_zamba2 import Zamba2Config\n@@ -1753,7 +1759,12 @@ def prepare_inputs_for_generation(\n             # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n             # Exception 1: when passing input_embeds, input_ids may be missing entries\n             # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-            if inputs_embeds is not None:  # Exception 1\n+            # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+            #              (we can't check exception 3 while compiling)\n+            if (\n+                inputs_embeds is not None  # Exception 1\n+                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+            ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]"
        }
    ],
    "stats": {
        "total": 120,
        "additions": 103,
        "deletions": 17
    }
}