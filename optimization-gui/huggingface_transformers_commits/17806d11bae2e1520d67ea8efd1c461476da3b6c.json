{
    "author": "Cyrilvallez",
    "message": "Improve modular converter (#33991)\n\n* improve modular\r\n\r\n* style\r\n\r\n* Update modular_model_converter.py\r\n\r\n* pretty print warning\r\n\r\n* style\r\n\r\n* Support to remove unused classes as part of added dependencies as well\r\n\r\n* nits\r\n\r\n* correct bug\r\n\r\n* add example\r\n\r\n* style\r\n\r\n* Add documentation",
    "sha": "17806d11bae2e1520d67ea8efd1c461476da3b6c",
    "files": [
        {
            "sha": "1516233ec4d6e1eb62f7493bc4761a913c19d1b1",
            "filename": "docs/source/en/modular_transformers.md",
            "status": "modified",
            "additions": 21,
            "deletions": 1,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/17806d11bae2e1520d67ea8efd1c461476da3b6c/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17806d11bae2e1520d67ea8efd1c461476da3b6c/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodular_transformers.md?ref=17806d11bae2e1520d67ea8efd1c461476da3b6c",
            "patch": "@@ -52,6 +52,7 @@ For example:\n   reference it (in case of addition) or completely remove it (in case of deletion).\n - If a class inherits from another, for example: class GemmaModel(LlamaModel):, dependencies are automatically \n   inferred. All submodules will be automatically inferred from the superclass.\n+- If you define new functions in the `modular` and use them inside classes, the linter will automatically infer the \n \n You should be able to write everything (the tokenizer, the image processor, the model, the config) in this `modular` \n file, and the corresponding files will be created for you. \n@@ -158,6 +159,25 @@ class GemmaTokenizer(LlamaTokenizer):\n         raise AttributeError(\"Not needed for Gemma\")\n ```\n \n+### Define new functions\n+\n+If you define a new function in the `modular` file to be used inside a class, say\n+\n+```python\n+def my_new_function(*args, **kwargs):\n+  # Do something here\n+  pass\n+\n+class GemmaModel(LlamaModel):\n+    def forward(*args, **kwargs):\n+      # Call the function\n+      example = my_new_function(*args, **kwargs)\n+      # continue here\n+```\n+\n+the `my_new_function` function (and, recursively, any other new functions called in its body) will be automatically copy-pasted \n+in the file where it is used.\n+\n ### Calling `super()`\n We recently shipped a few features that allow you to go from:\n ```python\n@@ -174,4 +194,4 @@ We now also support special cases like\n class GemmaVisionModel(CLIPModel):                                 \n     pass\n ```\n-where the name of your class `GemmaVision` is not the same as the modular `Gemma`. This is super useful for composite models\n\\ No newline at end of file\n+where the name of your class `GemmaVision` is not the same as the modular `Gemma`. This is super useful for composite models.\n\\ No newline at end of file"
        },
        {
            "sha": "e170803cccab7043646a49bd31445ea5de603fe5",
            "filename": "src/transformers/models/gemma/configuration_gemma.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py?ref=17806d11bae2e1520d67ea8efd1c461476da3b6c",
            "patch": "@@ -1,9 +1,9 @@\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_modular_file.py>.\n-#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the modular. If any change should be done, please apply the change to the\n-#                           modular_xxx.py file directly. One of our CI enforces this\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from src/transformers/models/gemma/modular_gemma.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_gemma.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n # coding=utf-8\n # Copyright 2024 Google Inc. HuggingFace Inc. team. All rights reserved.\n #"
        },
        {
            "sha": "ff206a470bc3fa29d7c0eb70d85f63e378ec2287",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=17806d11bae2e1520d67ea8efd1c461476da3b6c",
            "patch": "@@ -1,9 +1,9 @@\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_modular_file.py>.\n-#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the modular. If any change should be done, please apply the change to the\n-#                           modular_xxx.py file directly. One of our CI enforces this\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from src/transformers/models/gemma/modular_gemma.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_gemma.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n # coding=utf-8\n # Copyright 2024 Google Inc. HuggingFace Inc. team. All rights reserved.\n #"
        },
        {
            "sha": "ff0d1d034c2238f3911aad76d1ca99e6674dcb13",
            "filename": "src/transformers/models/gemma/tokenization_gemma.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py?ref=17806d11bae2e1520d67ea8efd1c461476da3b6c",
            "patch": "@@ -1,9 +1,9 @@\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_modular_file.py>.\n-#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the modular. If any change should be done, please apply the change to the\n-#                           modular_xxx.py file directly. One of our CI enforces this\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from src/transformers/models/gemma/modular_gemma.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_gemma.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n # coding=utf-8\n # Copyright 2024 Google Inc. HuggingFace Inc. team. All rights reserved.\n #"
        },
        {
            "sha": "74976bdd340f41bc68f1b3dffc7802340e8ef876",
            "filename": "src/transformers/models/gemma2/configuration_gemma2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py?ref=17806d11bae2e1520d67ea8efd1c461476da3b6c",
            "patch": "@@ -1,9 +1,9 @@\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_modular_file.py>.\n-#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the modular. If any change should be done, please apply the change to the\n-#                           modular_xxx.py file directly. One of our CI enforces this\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from src/transformers/models/gemma2/modular_gemma2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_gemma2.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n # coding=utf-8\n # Copyright 2024 Google Inc. HuggingFace Inc. team. All rights reserved.\n #"
        },
        {
            "sha": "d8c758719064226f6967a1970b832ae997e9055e",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=17806d11bae2e1520d67ea8efd1c461476da3b6c",
            "patch": "@@ -1,9 +1,9 @@\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_modular_file.py>.\n-#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the modular. If any change should be done, please apply the change to the\n-#                           modular_xxx.py file directly. One of our CI enforces this\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from src/transformers/models/gemma2/modular_gemma2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_gemma2.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n # coding=utf-8\n # Copyright 2024 Google Inc. HuggingFace Inc. team. All rights reserved.\n #"
        },
        {
            "sha": "e7c8eeccef98b4e432836f02fb186e896977ab9d",
            "filename": "src/transformers/models/instructblipvideo/configuration_instructblipvideo.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py?ref=17806d11bae2e1520d67ea8efd1c461476da3b6c",
            "patch": "@@ -1,9 +1,9 @@\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_modular_file.py>.\n-#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the modular. If any change should be done, please apply the change to the\n-#                           modular_xxx.py file directly. One of our CI enforces this\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from src/transformers/models/instructblipvideo/modular_instructblipvideo.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_instructblipvideo.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n # coding=utf-8\n # Copyright 2024 HuggingFace Inc. team. All rights reserved.\n #"
        },
        {
            "sha": "a300268ed713273a159d5b6ebf4ae37b4386b68a",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=17806d11bae2e1520d67ea8efd1c461476da3b6c",
            "patch": "@@ -1,9 +1,9 @@\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_modular_file.py>.\n-#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the modular. If any change should be done, please apply the change to the\n-#                           modular_xxx.py file directly. One of our CI enforces this\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from src/transformers/models/instructblipvideo/modular_instructblipvideo.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_instructblipvideo.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n # coding=utf-8\n # Copyright 2024 HuggingFace Inc. team. All rights reserved.\n #"
        },
        {
            "sha": "0e4e39b4b3ab53807ef51d7d68f735411b4315e5",
            "filename": "src/transformers/models/llava_next_video/configuration_llava_next_video.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py?ref=17806d11bae2e1520d67ea8efd1c461476da3b6c",
            "patch": "@@ -1,9 +1,9 @@\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_modular_file.py>.\n-#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the modular. If any change should be done, please apply the change to the\n-#                           modular_xxx.py file directly. One of our CI enforces this\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from src/transformers/models/llava_next_video/modular_llava_next_video.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_llava_next_video.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n # coding=utf-8\n # Copyright 2024 HuggingFace Inc. team. All rights reserved.\n #"
        },
        {
            "sha": "58fed1832670f52d24b448f82dad5988a2222502",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17806d11bae2e1520d67ea8efd1c461476da3b6c/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=17806d11bae2e1520d67ea8efd1c461476da3b6c",
            "patch": "@@ -1,9 +1,9 @@\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_modular_file.py>.\n-#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the modular. If any change should be done, please apply the change to the\n-#                           modular_xxx.py file directly. One of our CI enforces this\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from src/transformers/models/llava_next_video/modular_llava_next_video.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_llava_next_video.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n # coding=utf-8\n # Copyright 2024 HuggingFace Inc. team. All rights reserved.\n #"
        },
        {
            "sha": "599dc70e17e877c077c23065b956743c0880b2c3",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 224,
            "deletions": 16,
            "changes": 240,
            "blob_url": "https://github.com/huggingface/transformers/blob/17806d11bae2e1520d67ea8efd1c461476da3b6c/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17806d11bae2e1520d67ea8efd1c461476da3b6c/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=17806d11bae2e1520d67ea8efd1c461476da3b6c",
            "patch": "@@ -15,8 +15,9 @@\n import argparse\n import glob\n import importlib\n+import os\n import re\n-from collections import defaultdict\n+from collections import defaultdict, deque\n from typing import Dict, List, Set\n \n import libcst as cst\n@@ -33,12 +34,19 @@\n logger = logging.get_logger(__name__)\n \n \n-AUTO_GENERATED_MESSAGE = \"\"\"#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_modular_file.py>.\n-#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the modular. If any change should be done, please apply the change to the\n-#                           modular_xxx.py file directly. One of our CI enforces this\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+# This is used to avoid overwriting these top-level assignments even if they are in the dependency graph. Otherwise, the\n+# value from the dependency is used, then mapped to current name convention, resulting in wrong value.\n+# The corresponding mapped value is used to define the file target for the assignment\n+ASSIGNMENTS_TO_KEEP = {\n+    \"_CHECKPOINT_FOR_DOC\": \"modeling\",\n+}\n+\n+AUTO_GENERATED_MESSAGE = \"\"\"#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from {relative_path}.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          {short_name} file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n \"\"\"\n \n \n@@ -114,11 +122,14 @@ def visit_SimpleStatementLine(self, node):\n         if m.matches(node, m.SimpleStatementLine(body=[m.Assign()])) and m.matches(\n             self.get_metadata(cst.metadata.ParentNodeProvider, node), m.Module()\n         ):\n-            if hasattr(node.body[0].targets[0].target, \"value\"):\n-                self.assignments[node.body[0].targets[0].target.value] = node\n+            left_hand_side = node.body[0].targets[0].target\n+            if hasattr(left_hand_side, \"value\"):\n+                if left_hand_side.value not in ASSIGNMENTS_TO_KEEP.keys():\n+                    self.assignments[left_hand_side.value] = node\n             else:\n-                for idx, target in enumerate(list(node.body[0].targets[0].target.elements)):\n-                    self.assignments[target.value.value] = node.body[0].value.elements[idx].value\n+                for idx, target in enumerate(list(left_hand_side.elements)):\n+                    if target.value.value not in ASSIGNMENTS_TO_KEEP.keys():\n+                        self.assignments[target.value.value] = node.body[0].value.elements[idx].value\n         if m.matches(node, m.SimpleStatementLine(body=[m.Import() | m.ImportFrom()])):\n             self.imports[node.body[0].names] = node\n \n@@ -612,6 +623,99 @@ def get_new_part(class_name, base_class):\n     return snake_case\n \n \n+def find_all_dependencies(function: str, dependency_mapping: dict[str, set]):\n+    \"\"\"Return all the dependencies of the given top-level function. Given the following structure in the `modular_xxx.py` file:\n+    ```\n+    def foo1():\n+        pass\n+\n+    def foo2():\n+        pass\n+\n+    def bar():\n+        foo1()\n+\n+    def foobar():\n+        bar()\n+        foo2()\n+\n+    class MyLayer(SomeOtherModelLayer):\n+        def forward(...):\n+            foobar()\n+    ```\n+    and the `dependency_mapping` created when visiting the `modular_xxx.py` file, we get:\n+    ```\n+    dependency_mapping = {'bar': {'foo1'}, 'foobar': {'bar', 'foo2'}}\n+    find_all_dependencies('foobar', dependency_mapping)\n+    >>> [('bar', 'foobar'), ('foo2', 'foobar'), ('foo1', 'bar')]\n+    ```\n+    That is, all the functions needed (and their immediate parent) so that the function to be added in MyLayer (`foobar`) can\n+    work correctly.\n+    \"\"\"\n+    all_dependencies = deque(dependency_mapping[function])\n+    all_dependencies_with_parent = [(dep, function) for dep in dependency_mapping[function]]\n+    checked_dependencies = set(function)\n+    while len(all_dependencies) > 0:\n+        # Pick element to visit\n+        parent = all_dependencies.popleft()\n+        if parent not in checked_dependencies:\n+            # Update dependencies\n+            all_dependencies.extend(dependency_mapping[parent])\n+            all_dependencies_with_parent += [(dependency, parent) for dependency in dependency_mapping[parent]]\n+            # add visited node to the list\n+            checked_dependencies.add(parent)\n+\n+    # no child can ever appear before its parent thanks to the queue (needed to add them at the correct location in the body later)\n+    return all_dependencies_with_parent\n+\n+\n+class PostModularConverterCleaner(CSTTransformer):\n+    \"\"\"Allow simple cleaning after conversion. Remove top-level functions/classes without any calls (they may arise due\n+    to dependency mapping, even if code parts with those functions/classes were overwritten)\"\"\"\n+\n+    METADATA_DEPENDENCIES = (ParentNodeProvider,)\n+\n+    def __init__(self, added_dependencies: set):\n+        super().__init__()\n+        self.top_level_functions_or_classes = {}\n+        self.all_used_functions_or_classes = set()\n+        self.added_dependencies = added_dependencies\n+\n+    def visit_FunctionDef(self, node):\n+        parent_node = self.get_metadata(cst.metadata.ParentNodeProvider, node)\n+        if m.matches(parent_node, m.Module()):\n+            self.top_level_functions_or_classes[node.name.value] = node\n+\n+    def visit_ClassDef(self, node):\n+        parent_node = self.get_metadata(cst.metadata.ParentNodeProvider, node)\n+        if m.matches(parent_node, m.Module()):\n+            self.top_level_functions_or_classes[node.name.value] = node\n+\n+    def visit_Name(self, node: cst.Name):\n+        \"\"\"This is used to find any mention of a top-level function or class except its own definition.\n+        It will contain other names as well, but those will not be used. This is the most general way to do it\n+        since mentions may appear in a lot of different contexts (apart from simple Call to the function/class).\n+        e.g. Attention classes are only mentionned by their name in a dict assignment.\n+        \"\"\"\n+        parent_node = self.get_metadata(cst.metadata.ParentNodeProvider, node)\n+\n+        if not (\n+            (m.matches(parent_node, m.ClassDef()) and parent_node.name.value == node.value)\n+            or (m.matches(parent_node, m.FunctionDef()) and parent_node.name.value == node.value)\n+        ):\n+            self.all_used_functions_or_classes.add(node.value)\n+\n+    def leave_Module(self, original_node: cst.Module, node):\n+        # Find any class/function that was mistakenly added as part of the dependencies and remove it\n+        unused = self.added_dependencies - self.all_used_functions_or_classes\n+        nodes_to_remove = [\n+            self.top_level_functions_or_classes[name] for name in unused if name in self.top_level_functions_or_classes\n+        ]\n+        new_body = [node_ for node_ in original_node.body if node_ not in nodes_to_remove]\n+        # Return a new module with the updated body\n+        return node.with_changes(body=new_body)\n+\n+\n class ModularConverterTransformer(CSTTransformer):\n     METADATA_DEPENDENCIES = (ParentNodeProvider, ScopeProvider, PositionProvider)\n \n@@ -643,6 +747,13 @@ def __init__(self, python_module, new_name, given_old_name=None, given_new_name=\n         self.match_patterns = \"|\".join(self.files.keys())\n         self.all_definitions = {}\n         self.class_to_file_type = {}\n+        self.current_class = None  # keep track of current top-level class during visit\n+        self.current_top_level_function = None  # keep track of current top-level function during visit\n+        # Mapping from top-level functions to classes using them\n+        self.function_call_class_mapping = defaultdict(lambda: set())\n+        # Mapping from top-level functions to other top-level functions dependencies\n+        self.function_call_dependency_mapping = defaultdict(lambda: set())\n+        self.added_dependencies = set()\n \n     def visit_ImportFrom(self, node: cst.ImportFrom) -> None:\n         \"\"\"When visiting imports from `transformers.models.xxx` we need to:\n@@ -692,9 +803,20 @@ def leave_SimpleStatementLine(self, original_node, updated_node):\n                 if updated_node not in self.all_imports:\n                     self.all_imports.append(updated_node)\n                 return updated_node\n+            elif m.matches(original_node, m.SimpleStatementLine(body=[m.Assign()])):\n+                if original_node.body[0].targets[0].target.value in ASSIGNMENTS_TO_KEEP.keys():\n+                    file_ = ASSIGNMENTS_TO_KEEP[original_node.body[0].targets[0].target.value]\n+                    self.files[file_][original_node.body[0].targets[0].target.value] = {\n+                        \"node\": original_node,\n+                        \"insert_idx\": self.global_scope_index,\n+                    }\n             self.global_scope_index += 100\n         return updated_node\n \n+    def visit_ClassDef(self, node: cst.ClassDef):\n+        \"\"\"Used to keep track of current class\"\"\"\n+        self.current_class = node.name.value\n+\n     def leave_ClassDef(self, original_node, updated_node):\n         \"\"\"\n         1. Filter the `base` classes of this class\n@@ -772,9 +894,10 @@ def leave_ClassDef(self, original_node, updated_node):\n                 node = class_finder.global_nodes.get(dependency, None)\n                 if node is not None:\n                     if dependency not in file_to_update:\n-                        node = self.all_definitions.get(dependency, node)\n+                        node = self.all_definitions.pop(dependency, node)\n                         start_insert_idx -= 1\n                         file_to_update[dependency] = {\"insert_idx\": start_insert_idx, \"node\": node}\n+                        self.added_dependencies.add(dependency)\n                     elif dependency not in self.inserted_deps:\n                         # make sure the node is written after its dependencies\n                         start_insert_idx = file_to_update[dependency][\"insert_idx\"] - 1\n@@ -811,8 +934,15 @@ def leave_ClassDef(self, original_node, updated_node):\n         else:\n             self.class_to_file_type[class_name] = \"modeling\"\n             self.files[\"modeling\"][class_name] = {\"insert_idx\": self.global_scope_index, \"node\": updated_node}\n+\n+        self.current_class = None\n         return updated_node\n \n+    def visit_FunctionDef(self, node):\n+        parent_node = self.get_metadata(cst.metadata.ParentNodeProvider, node)\n+        if m.matches(parent_node, m.Module()):\n+            self.current_top_level_function = node.name.value\n+\n     def leave_FunctionDef(self, original_node, node):\n         parent_node = self.get_metadata(cst.metadata.ParentNodeProvider, original_node)\n         if m.matches(parent_node, m.Module()):\n@@ -852,7 +982,71 @@ def leave_If(self, original_node, node):\n                 logger.warning(f\"one import is protected with `if`. Hard guess where it's used {full_statement}\")\n         return node\n \n-    def leave_Module(self, original_node: cst.Assign, node):\n+    def visit_Call(self, node: cst.Call):\n+        \"\"\"This is used to create a mapping from functions to class calling them, and from top-level functions to functions called inside them.\n+        Important note: we only rely on direct Call to the functions here, not indirect mentions (such as assigning a variable with the function,\n+        add calling the variable later). This should be enough as the `modular_xxx` and `modeling_xxx` structures should be as simple as possible.\"\"\"\n+        # Only map function calls if we're inside a class (i.e., current_class is set)\n+        if self.current_class is not None:\n+            # Simple function calls such as foo()\n+            if isinstance(node.func, cst.Name):\n+                self.function_call_class_mapping[node.func.value].add(self.current_class)\n+        elif self.current_top_level_function is not None:\n+            # Simple function calls such as foo()\n+            if isinstance(node.func, cst.Name):\n+                self.function_call_dependency_mapping[self.current_top_level_function].add(node.func.value)\n+\n+    def _maybe_add_function_to_body(\n+        self,\n+        top_level_function: str,\n+        body: dict,\n+        function_node: cst.FunctionDef,\n+        matching_callers: set | None = None,\n+        parent: str | None = None,\n+    ) -> bool:\n+        \"\"\"Check if the `top_level_function` should be added to the body (i.e. it is not already present, and `matching_callers`\n+        is not empy, or `parent`is provided). If it should be added, do it (in the correct location, just before its caller) and return\n+        `True`. Return `False` otherwise.\n+        \"\"\"\n+        if matching_callers is None and parent is None:\n+            raise ValueError(\"Cannot add function if both the parent and the matching callers are None.\")\n+        if matching_callers is None:\n+            matching_callers = {parent}\n+        if len(matching_callers) > 0 and top_level_function not in body.keys():\n+            # Add the function just before the first class using it\n+            new_idx = min([body[element][\"insert_idx\"] for element in matching_callers])\n+            # Reorder the elements\n+            for element in body.keys():\n+                if body[element][\"insert_idx\"] >= new_idx:\n+                    body[element][\"insert_idx\"] += 1\n+            # Assign new element to body (after changing the count to avoid messing it)\n+            body[top_level_function] = {\"insert_idx\": new_idx, \"node\": function_node}\n+            return True\n+        return False\n+\n+    def _recursively_add_all_new_needed_functions_in_files(self):\n+        \"\"\"For all top-level functions which were newly defined in the `modular_xxx.py`, check if they are used in a class in\n+        the different files, and add them to the file if it is the case (also recursively adding all other functions that\n+        may be needed in that function body).\"\"\"\n+        # At this point, `self.all_definitions` only contains newly defined top-level functions in the `modualr_xxx.py`\n+        for top_level_function, function_node in self.all_definitions.items():\n+            calling_entities = self.function_call_class_mapping[top_level_function]\n+            # The function may be needed in different files, we need to iterate on them\n+            for file, body in self.files.items():\n+                file_elements = set(body.keys())\n+                # If the intersection is not null, top_level_func must be added to file\n+                matching_callers = calling_entities & file_elements\n+                added = self._maybe_add_function_to_body(top_level_function, body, function_node, matching_callers)\n+                # If the function was added, we need to recursively add all its dependencies\n+                if added:\n+                    for dependency, parent in find_all_dependencies(\n+                        top_level_function, self.function_call_dependency_mapping\n+                    ):\n+                        self._maybe_add_function_to_body(\n+                            dependency, body, self.all_definitions[dependency], parent=parent\n+                        )\n+\n+    def leave_Module(self, original_node: cst.Module, node):\n         imports = {self.python_module.code_for_node(k): k for k in self.all_imports}\n         dependency_imports = {file_type: imports.copy() for file_type in self.files}\n         for super_file_name, visiter in self.visited_module.items():\n@@ -861,12 +1055,19 @@ def leave_Module(self, original_node: cst.Assign, node):\n                 {self.python_module.code_for_node(k): k for k in visiter.imports.values()}\n             )\n \n+        # Check if any new top-level function from the `modular_xxx.py` should be added to the different files\n+        # (if it is called in a class in the file, then it will be copy pasted from `modular.py` to that file).\n+        self._recursively_add_all_new_needed_functions_in_files()\n+\n         for file, body in self.files.items():\n             new_body = [k[1][\"node\"] for k in sorted(body.items(), key=lambda x: x[1][\"insert_idx\"])]\n             if len(new_body) > 0:\n                 if file in dependency_imports.keys():\n                     new_body = list(dependency_imports[file].values()) + new_body\n-                self.files[file] = cst.Module(body=[*new_body], header=node.header)\n+                new_module = cst.Module(body=[*new_body], header=node.header)\n+                # Final cleanup\n+                new_module = MetadataWrapper(new_module).visit(PostModularConverterCleaner(self.added_dependencies))\n+                self.files[file] = new_module\n         return node\n \n \n@@ -885,7 +1086,14 @@ def convert_modular_file(modular_file, old_model_name=None, new_model_name=None,\n         wrapper.visit(cst_transformers)\n         for file, node in cst_transformers.files.items():\n             if node != {}:\n-                ruffed_code = run_ruff(AUTO_GENERATED_MESSAGE + node.code, True)\n+                # Get relative path starting from src/transformers/\n+                relative_path = re.search(\n+                    f\"{os.sep}(src{os.sep}transformers{os.sep}.*)\", os.path.abspath(modular_file)\n+                ).group(1)\n+                header = AUTO_GENERATED_MESSAGE.format(\n+                    relative_path=relative_path, short_name=os.path.basename(relative_path)\n+                )\n+                ruffed_code = run_ruff(header + node.code, True)\n                 formatted_code = run_ruff(ruffed_code, False)\n                 output[file] = [formatted_code, ruffed_code]\n         return output\n@@ -916,7 +1124,7 @@ def save_modeling_file(modular_file, converted_file):\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\n         \"--files_to_parse\",\n-        default=[\"src/transformers/models/gemma/modular_gemma.py\"],\n+        default=[\"src/transformers/models/roberta/modular_roberta.py\"],\n         nargs=\"+\",\n         help=\"A list of `modular_xxxx` files that should be converted to single model file\",\n     )"
        }
    ],
    "stats": {
        "total": 370,
        "additions": 299,
        "deletions": 71
    }
}