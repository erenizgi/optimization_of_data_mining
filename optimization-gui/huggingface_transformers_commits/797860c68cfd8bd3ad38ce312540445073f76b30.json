{
    "author": "hamza-hcompany",
    "message": "feat: add flexible Liger Kernel configuration to TrainingArguments (#38911)\n\n* feat: add flexible Liger Kernel configuration to TrainingArguments\n\nAdd support for granular Liger Kernel configuration through a new\n`liger_kernel_config` parameter in TrainingArguments. This allows users\nto selectively enable/disable specific kernels (rope, swiglu, cross_entropy,\netc.) instead of the current approach that rely on default configuration.\n\nFeatures:\n- Add `liger_kernel_config` dict parameter to TrainingArguments\n- Support selective kernel application for all supported models\n- Maintain full backward compatibility with existing `use_liger_kernel` flag\n\nExample usage:\n```python\nTrainingArguments(\n    use_liger_kernel=True,\n    liger_kernel_config={\n        \"rope\": True,\n        \"swiglu\": True,\n        \"cross_entropy\": False,\n        \"fused_linear_cross_entropy\": True\n    }\n)\nCloses #38905\n\n* Address comments and update Liger section in Trainer docs",
    "sha": "797860c68cfd8bd3ad38ce312540445073f76b30",
    "files": [
        {
            "sha": "3572fb4385d5346ad624b43ec8e8580fe9d7a5d8",
            "filename": "docs/source/en/trainer.md",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/797860c68cfd8bd3ad38ce312540445073f76b30/docs%2Fsource%2Fen%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/797860c68cfd8bd3ad38ce312540445073f76b30/docs%2Fsource%2Fen%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftrainer.md?ref=797860c68cfd8bd3ad38ce312540445073f76b30",
            "patch": "@@ -493,6 +493,33 @@ training_args = TrainingArguments(\n )\n ```\n \n+You can also configure which specific kernels to apply using the `liger_kernel_config` parameter. This dict is passed as keyword arguments to the `_apply_liger_kernel_to_instance` function, allowing fine-grained control over kernel usage. Available options vary by model but typically include: `rope`, `swiglu`, `cross_entropy`, `fused_linear_cross_entropy`, `rms_norm`, etc.\n+\n+```py\n+from transformers import TrainingArguments\n+\n+# Apply only specific kernels\n+training_args = TrainingArguments(\n+    output_dir=\"your-model\",\n+    learning_rate=2e-5,\n+    per_device_train_batch_size=16,\n+    per_device_eval_batch_size=16,\n+    num_train_epochs=2,\n+    weight_decay=0.01,\n+    eval_strategy=\"epoch\",\n+    save_strategy=\"epoch\",\n+    load_best_model_at_end=True,\n+    push_to_hub=True,\n+    use_liger_kernel=True,\n+    liger_kernel_config={\n+        \"rope\": True,\n+        \"cross_entropy\": True,\n+        \"rms_norm\": False,  # Don't apply Liger's RMSNorm kernel\n+        \"swiglu\": True,\n+    }\n+)\n+```\n+\n ### NEFTune\n \n [NEFTune](https://hf.co/papers/2310.05914) adds noise to the embedding vectors during training to improve model performance. Enable it in [`Trainer`] with the `neftune_noise_alpha` parameter in [`TrainingArguments`] to control how much noise is added."
        },
        {
            "sha": "39448cd5ab0f8a5c46169e16722559c03899c548",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/797860c68cfd8bd3ad38ce312540445073f76b30/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/797860c68cfd8bd3ad38ce312540445073f76b30/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=797860c68cfd8bd3ad38ce312540445073f76b30",
            "patch": "@@ -526,12 +526,15 @@ def __init__(\n             if is_liger_kernel_available():\n                 from liger_kernel.transformers import _apply_liger_kernel_to_instance\n \n+                # Prepare kernel config - use provided config or default (empty dict for default behavior)\n+                kernel_config = self.args.liger_kernel_config if self.args.liger_kernel_config is not None else {}\n+\n                 if isinstance(model, PreTrainedModel):\n-                    # Patch the model with liger kernels. Use the default kernel configurations.\n-                    _apply_liger_kernel_to_instance(model=model)\n+                    # Patch the model with liger kernels. Use the the specified or default kernel configurations.\n+                    _apply_liger_kernel_to_instance(model=model, **kernel_config)\n                 elif hasattr(model, \"get_base_model\") and isinstance(model.get_base_model(), PreTrainedModel):\n-                    # Patch the base model with liger kernels where model is a PeftModel. Use the default kernel configurations.\n-                    _apply_liger_kernel_to_instance(model=model.get_base_model())\n+                    # Patch the base model with liger kernels where model is a PeftModel. Use the specified or default kernel configurations.\n+                    _apply_liger_kernel_to_instance(model=model.get_base_model(), **kernel_config)\n                 else:\n                     logger.warning(\n                         \"The model is not an instance of PreTrainedModel. No liger kernels will be applied.\""
        },
        {
            "sha": "ceb5a6b132a8dae0f12f79162685c811731a3542",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/797860c68cfd8bd3ad38ce312540445073f76b30/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/797860c68cfd8bd3ad38ce312540445073f76b30/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=797860c68cfd8bd3ad38ce312540445073f76b30",
            "patch": "@@ -793,6 +793,11 @@ class TrainingArguments:\n             It can effectively increase multi-GPU training throughput by ~20% and reduces memory usage by ~60%, works out of the box with\n             flash attention, PyTorch FSDP, and Microsoft DeepSpeed. Currently, it supports llama, mistral, mixtral and gemma models.\n \n+        liger_kernel_config (`Optional[dict]`, *optional*):\n+            Configuration to be used for Liger Kernel. When use_liger_kernel=True, this dict is passed as keyword arguments to the\n+            `_apply_liger_kernel_to_instance` function, which specifies which kernels to apply. Available options vary by model but typically\n+            include: 'rope', 'swiglu', 'cross_entropy', 'fused_linear_cross_entropy', 'rms_norm', etc. If `None`, use the default kernel configurations.\n+\n         average_tokens_across_devices (`bool`, *optional*, defaults to `False`):\n             Whether or not to average tokens across devices. If enabled, will use all_reduce to synchronize\n             num_tokens_in_batch for precise loss calculation. Reference:\n@@ -1525,6 +1530,19 @@ class TrainingArguments:\n         metadata={\"help\": \"Whether or not to enable the Liger Kernel for model training.\"},\n     )\n \n+    liger_kernel_config: Optional[dict[str, bool]] = field(\n+        default=None,\n+        metadata={\n+            \"help\": (\n+                \"Configuration to be used for Liger Kernel. When use_liger_kernel=True, \"\n+                \"this dict is passed as keyword arguments to the `_apply_liger_kernel_to_instance` function, \"\n+                \"which specifies which kernels to apply. Available options vary by model \"\n+                \"but typically include: 'rope', 'swiglu', 'cross_entropy', 'fused_linear_cross_entropy', \"\n+                \"'rms_norm', etc. If None, use the default kernel configurations.\"\n+            )\n+        },\n+    )\n+\n     eval_use_gather_object: Optional[bool] = field(\n         default=False,\n         metadata={"
        },
        {
            "sha": "2594edcdef8c53497440a60cba08f55984e7aaac",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 42,
            "deletions": 0,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/797860c68cfd8bd3ad38ce312540445073f76b30/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/797860c68cfd8bd3ad38ce312540445073f76b30/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=797860c68cfd8bd3ad38ce312540445073f76b30",
            "patch": "@@ -1792,6 +1792,25 @@ def test_use_liger_kernel_patching(self):\n             self.assertEqual(modeling_llama.apply_rotary_pos_emb, liger_rotary_pos_emb)\n             self.assertTrue(isinstance(tiny_llama.model.norm, LigerRMSNorm))\n \n+    @require_liger_kernel\n+    def test_use_liger_kernel_custom_config_patching(self):\n+        # Ensure any monkey patching is cleaned up for subsequent tests\n+        with patch(\"transformers.models.llama.modeling_llama\"):\n+            from liger_kernel.transformers import LigerRMSNorm\n+\n+            config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+            tiny_llama = LlamaForCausalLM(config)\n+\n+            args = TrainingArguments(\n+                self.get_auto_remove_tmp_dir(),\n+                use_liger_kernel=True,\n+                liger_kernel_config={\"rms_norm\": False},  # Don't apply Liger's RMSNorm\n+            )\n+            Trainer(tiny_llama, args)\n+\n+            # Check that the RMSNorm kernel is not applied as specified in the config\n+            self.assertFalse(isinstance(tiny_llama.model.norm, LigerRMSNorm))\n+\n     @require_liger_kernel\n     @require_torch_accelerator\n     def test_use_liger_kernel_trainer(self):\n@@ -1810,6 +1829,29 @@ def test_use_liger_kernel_trainer(self):\n         # Check this works\n         _ = trainer.train()\n \n+    @require_liger_kernel\n+    @require_torch_accelerator\n+    def test_use_liger_kernel_custom_config_trainer(self):\n+        # Check that trainer still works with liger kernel applied when using a custom config\n+        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+        tiny_llama = LlamaForCausalLM(config)\n+\n+        x = torch.randint(0, 100, (128,))\n+        train_dataset = RepeatDataset(x)\n+\n+        args = TrainingArguments(\n+            self.get_auto_remove_tmp_dir(),\n+            learning_rate=1e-2,\n+            logging_steps=5,\n+            max_steps=20,\n+            use_liger_kernel=True,\n+            liger_kernel_config={\"rms_norm\": False, \"cross_entropy\": True, \"fused_linear_cross_entropy\": False},\n+        )\n+        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+\n+        # Check this works\n+        _ = trainer.train()\n+\n     @require_lomo\n     @require_torch_accelerator\n     def test_lomo(self):"
        }
    ],
    "stats": {
        "total": 98,
        "additions": 94,
        "deletions": 4
    }
}