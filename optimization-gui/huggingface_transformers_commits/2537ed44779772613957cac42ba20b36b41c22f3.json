{
    "author": "qubvel",
    "message": "Update `get_*_features` methods + update doc snippets (#40555)\n\n* siglip\n\n* clip\n\n* aimv2\n\n* metaclip_2\n\n* align\n\n* align fixup\n\n* altclip\n\n* blip2 (make consistent)\n\n* chineese clip\n\n* clipseg\n\n* flava\n\n* groupvit\n\n* owlv2\n\n* owlvit\n\n* vision_encoder\n\n* clap\n\n* x_clip\n\n* fixup\n\n* fix siglip2\n\n* blip2\n\n* fix blip2 tests (revert to original)\n\n* fix docs",
    "sha": "2537ed44779772613957cac42ba20b36b41c22f3",
    "files": [
        {
            "sha": "f99fcbd67b615bbecef8d7d3c74d11651ce536ae",
            "filename": "src/transformers/models/aimv2/modeling_aimv2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 30,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -34,7 +34,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs\n from .configuration_aimv2 import Aimv2Config, Aimv2TextConfig, Aimv2VisionConfig\n \n \n@@ -650,14 +650,13 @@ def __init__(self, config: Aimv2Config):\n \n         self.post_init()\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -667,39 +666,32 @@ def get_text_features(\n         Examples:\n \n         ```python\n+        >>> import torch\n         >>> from transformers import AutoTokenizer, Aimv2Model\n \n         >>> model = Aimv2Model.from_pretrained(\"openai/aimv2-vit-base-patch32\")\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"openai/aimv2-vit-base-patch32\")\n \n         >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n-        >>> text_features = model.get_text_features(**inputs)\n-        ```\"\"\"\n-        # Use AIMV2 model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n \n+        >>> with torch.inference_mode():\n+        ...     text_features = model.get_text_features(**inputs)\n+        ```\"\"\"\n         text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n         )\n-\n         pooled_output = text_outputs.pooler_output\n         text_features = self.text_projection(pooled_output)\n \n         return text_features\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_image_features(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        pixel_values: torch.FloatTensor,\n         interpolate_pos_encoding: bool = False,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -710,33 +702,25 @@ def get_image_features(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n         >>> from transformers import AutoProcessor, Aimv2Model\n+        >>> from transformers.image_utils import load_image\n \n         >>> model = Aimv2Model.from_pretrained(\"openai/aimv2-vit-base-patch32\")\n         >>> processor = AutoProcessor.from_pretrained(\"openai/aimv2-vit-base-patch32\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n \n-        >>> image_features = model.get_image_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     image_features = model.get_image_features(**inputs)\n         ```\"\"\"\n-        # Use AIMV2 model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n-\n         pooled_output = vision_outputs.pooler_output\n         image_features = self.visual_projection(pooled_output)\n "
        },
        {
            "sha": "063b3312ccc56a80b8182d995d9c41d691c106f3",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 19,
            "deletions": 45,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -32,7 +32,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs, logging\n from .configuration_align import AlignConfig, AlignTextConfig, AlignVisionConfig\n \n \n@@ -1125,6 +1125,7 @@ def __init__(self, config: AlignConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n         self,\n@@ -1134,9 +1135,6 @@ def get_text_features(\n         position_ids: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1146,45 +1144,32 @@ def get_text_features(\n         Examples:\n \n         ```python\n+        >>> import torch\n         >>> from transformers import AutoTokenizer, AlignModel\n \n         >>> model = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"kakaobrain/align-base\")\n \n         >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n-        >>> text_features = model.get_text_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     text_features = model.get_text_features(**inputs)\n         ```\"\"\"\n-        # Use ALIGN model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         text_outputs = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-\n         last_hidden_state = text_outputs[0][:, 0, :]\n         text_features = self.text_projection(last_hidden_state)\n \n         return text_features\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n-    def get_image_features(\n-        self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> torch.FloatTensor:\n+    def get_image_features(self, pixel_values: torch.FloatTensor) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n             image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\n@@ -1193,34 +1178,22 @@ def get_image_features(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n         >>> from transformers import AutoProcessor, AlignModel\n+        >>> from transformers.image_utils import load_image\n \n         >>> model = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n         >>> processor = AutoProcessor.from_pretrained(\"kakaobrain/align-base\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n-\n-        >>> image_features = model.get_image_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     image_features = model.get_image_features(**inputs)\n         ```\"\"\"\n-        # Use ALIGN model's config for some fields (if specified) instead of those of vision & text components.\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        vision_outputs = self.vision_model(\n-            pixel_values=pixel_values,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        image_features = vision_outputs[1]  # pooled_output\n-\n+        vision_outputs = self.vision_model(pixel_values=pixel_values)\n+        image_features = vision_outputs.pooler_output\n         return image_features\n \n     @can_return_tuple\n@@ -1246,21 +1219,22 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n         >>> from transformers import AutoProcessor, AlignModel\n+        >>> from transformers.image_utils import load_image\n \n         >>> model = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n         >>> processor = AutoProcessor.from_pretrained(\"kakaobrain/align-base\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = processor(\n         ...     images=image, text=[\"a photo of a cat\", \"a photo of a dog\"], return_tensors=\"pt\", padding=True\n         ... )\n \n-        >>> outputs = model(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     outputs = model(**inputs)\n         >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n         >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n         ```\"\"\""
        },
        {
            "sha": "6c0196c48a9e3011edfbc5caf80fda782d4fe283",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 19,
            "deletions": 38,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -32,7 +32,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs, logging, torch_int\n from .configuration_altclip import AltCLIPConfig, AltCLIPTextConfig, AltCLIPVisionConfig\n \n \n@@ -1204,16 +1204,14 @@ def __init__(self, config: AltCLIPConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        token_type_ids=None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        token_type_ids: Optional[torch.Tensor] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1223,42 +1221,33 @@ def get_text_features(\n         Examples:\n \n         ```python\n+        >>> import torch\n         >>> from transformers import AutoProcessor, AltCLIPModel\n \n         >>> model = AltCLIPModel.from_pretrained(\"BAAI/AltCLIP\")\n         >>> processor = AutoProcessor.from_pretrained(\"BAAI/AltCLIP\")\n+\n         >>> inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n-        >>> text_features = model.get_text_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     text_features = model.get_text_features(**inputs)\n         ```\"\"\"\n-        # Use AltCLIP model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         text_outputs = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             token_type_ids=token_type_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        pooled_output = text_outputs[1]\n+        pooled_output = text_outputs.pooler_output\n         text_features = self.text_projection(pooled_output)\n \n         return text_features\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_image_features(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        pixel_values: torch.FloatTensor,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1268,33 +1257,25 @@ def get_image_features(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n         >>> from transformers import AutoProcessor, AltCLIPModel\n+        >>> from transformers.image_utils import load_image\n \n         >>> model = AltCLIPModel.from_pretrained(\"BAAI/AltCLIP\")\n         >>> processor = AutoProcessor.from_pretrained(\"BAAI/AltCLIP\")\n+\n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n+\n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n-        >>> image_features = model.get_image_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     image_features = model.get_image_features(**inputs)\n         ```\"\"\"\n-        # Use AltCLIP model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         vision_outputs = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n-            return_dict=return_dict,\n         )\n-\n-        pooled_output = vision_outputs[1]  # pooled_output\n+        pooled_output = vision_outputs.pooler_output\n         image_features = self.visual_projection(pooled_output)\n \n         return image_features"
        },
        {
            "sha": "65a936b84567bbe3cb451851767b17bfbaab54cb",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 92,
            "deletions": 85,
            "changes": 177,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -15,6 +15,7 @@\n \"\"\"PyTorch BLIP-2 model.\"\"\"\n \n import math\n+import warnings\n from dataclasses import dataclass\n from typing import Any, Callable, Optional, Union\n \n@@ -31,11 +32,20 @@\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPooling,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n+    CausalLMOutputWithPast,\n+    Seq2SeqLMOutput,\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import ModelOutput, TransformersKwargs, auto_docstring, logging, torch_int\n+from ...utils import (\n+    ModelOutput,\n+    TransformersKwargs,\n+    auto_docstring,\n+    filter_out_non_signature_kwargs,\n+    logging,\n+    torch_int,\n+)\n from ..auto import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n from .configuration_blip_2 import Blip2Config, Blip2QFormerConfig, Blip2VisionConfig\n \n@@ -1269,19 +1279,17 @@ def _tie_weights(self):\n             self.language_model.encoder.embed_tokens = self.language_model.shared\n             self.language_model.decoder.embed_tokens = self.language_model.shared\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.Tensor] = None,\n         decoder_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ):\n+        legacy_output: bool = True,\n+    ) -> Union[torch.FloatTensor, CausalLMOutputWithPast]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -1299,161 +1307,160 @@ def get_text_features(\n         decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n+        legacy_output (`bool`, *optional*, defaults to `True`):\n+            Whether to return a model output object or a tensor of features.\n \n         Returns:\n-            text_outputs (`CausalLMOutputWithPast`, or `tuple(torch.FloatTensor)` if `return_dict=False`):\n-                The language model outputs. If `return_dict=True`, the output is a [`CausalLMOutputWithPast`] that\n-                contains the language model logits, the past key values and the hidden states if\n-                `output_hidden_states=True`.\n+            text_outputs (`CausalLMOutputWithPast` or `torch.FloatTensor`):\n+                The language model outputs. If `legacy_output=False`, the output is a `torch.FloatTensor`.\n+\n         Examples:\n         ```python\n         >>> import torch\n         >>> from transformers import AutoTokenizer, Blip2Model\n \n         >>> model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n-\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n+\n         >>> inputs = tokenizer([\"a photo of a cat\"], padding=True, return_tensors=\"pt\")\n-        >>> text_features = model.get_text_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     text_features = model.get_text_features(**inputs)\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if legacy_output:\n+            warnings.warn(\n+                \"Deprecation notice: In Transformers v4.59, the default return value of `get_text_features` will change. \"\n+                \"Currently, this method returns a model output object, but starting in v4.59, it will return a tensor instead. \"\n+                \"To opt in to the new behavior now, set `legacy_output=False`.\"\n+            )\n \n         if self.config.use_decoder_only_language_model:\n-            text_outputs = self.language_model(\n+            text_outputs: CausalLMOutputWithPast = self.language_model(\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-                output_hidden_states=output_hidden_states,\n-                return_dict=return_dict,\n-                **kwargs,\n+                return_dict=True,\n             )\n         else:\n             inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n-\n-            text_outputs = self.language_model(\n+            text_outputs: Seq2SeqLMOutput = self.language_model(\n                 inputs_embeds=inputs_embeds,\n                 attention_mask=attention_mask,\n                 decoder_input_ids=decoder_input_ids,\n                 decoder_attention_mask=decoder_attention_mask,\n-                output_attentions=output_attentions,\n-                output_hidden_states=output_hidden_states,\n-                return_dict=return_dict,\n                 labels=labels,\n-                **kwargs,\n+                return_dict=True,\n             )\n \n-        return text_outputs\n+        return text_outputs if legacy_output else text_outputs.logits\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_image_features(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        pixel_values: torch.FloatTensor,\n         interpolate_pos_encoding: bool = False,\n-    ):\n+        legacy_output: bool = True,\n+    ) -> Union[torch.FloatTensor, CausalLMOutputWithPast]:\n         r\"\"\"\n+        legacy_output (`bool`, *optional*, defaults to `True`):\n+            Whether to return a model output object or a tensor of features.\n+\n         Returns:\n-            vision_outputs (`BaseModelOutputWithPooling` or tuple of `torch.FloatTensor`):\n-                The vision model outputs. If `return_dict=True`, the output is a [`BaseModelOutputWithPooling`] that\n-                contains the image features, the pooled image features and the hidden states if\n-                `output_hidden_states=True`.\n+            vision_outputs (`BaseModelOutputWithPooling` or `torch.FloatTensor`):\n+                The vision model outputs. If `legacy_output=False`, the output is a `torch.FloatTensor`.\n+\n         Examples:\n         ```python\n         >>> import torch\n-        >>> from PIL import Image\n-        >>> import requests\n         >>> from transformers import AutoProcessor, Blip2Model\n+        >>> from transformers.image_utils import load_image\n \n         >>> model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n \n         >>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n+\n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n-        >>> image_outputs = model.get_image_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     image_outputs = model.get_image_features(**inputs)\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        if legacy_output:\n+            warnings.warn(\n+                \"Deprecation notice: In Transformers v4.59, the default return value of `get_text_features` will change. \"\n+                \"Currently, this method returns a model output object, but starting in v4.59, it will return a tensor instead. \"\n+                \"To opt in to the new behavior now, set `legacy_output=False`.\"\n+            )\n \n         vision_outputs = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            return_dict=True,\n         )\n \n-        return vision_outputs\n+        return vision_outputs if legacy_output else vision_outputs.pooler_output\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_qformer_features(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        pixel_values: torch.FloatTensor,\n         interpolate_pos_encoding: bool = False,\n-    ):\n+        legacy_output: bool = True,\n+    ) -> Union[torch.FloatTensor, BaseModelOutputWithPooling]:\n         r\"\"\"\n+        legacy_output (`bool`, *optional*, defaults to `True`):\n+            Whether to return a model output object or a tensor of features.\n+\n         Returns:\n-            vision_outputs (`BaseModelOutputWithPooling` or tuple of `torch.FloatTensor`):\n-                The vision model outputs. If `return_dict=True`, the output is a [`BaseModelOutputWithPooling`] that\n-                contains the image features, the pooled image features and the hidden states if\n-                `output_hidden_states=True`.\n+            qformer_outputs (`BaseModelOutputWithPooling` or `torch.FloatTensor`):\n+                The Q-Former outputs. If `legacy_output=False`, the output is a `torch.FloatTensor`.\n+\n         Examples:\n+\n         ```python\n         >>> import torch\n-        >>> from PIL import Image\n-        >>> import requests\n-        >>> from transformers import Blip2Processor, Blip2Model\n+        >>> from transformers import AutoProcessor, Blip2Model\n+        >>> from transformers.image_utils import load_image\n \n         >>> processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n         >>> model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n+\n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n-        >>> qformer_outputs = model.get_qformer_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     qformer_outputs = model.get_qformer_features(**inputs)\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        vision_outputs = self.vision_model(\n+        if legacy_output:\n+            warnings.warn(\n+                \"Deprecation notice: In Transformers v4.59, the default return value of `get_qformer_features` will change. \"\n+                \"Currently, this method returns a model output object, but starting in v4.59, it will return a tensor instead. \"\n+                \"To opt in to the new behavior now, set `legacy_output=False`.\"\n+            )\n+\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            return_dict=True,\n         )\n \n-        image_embeds = vision_outputs[0]\n+        image_embeds = vision_outputs.last_hidden_state\n \n         # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n         image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n \n         query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n-        query_outputs = self.qformer(\n+        query_outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.qformer(\n             query_embeds=query_tokens,\n             encoder_hidden_states=image_embeds,\n             encoder_attention_mask=image_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n         )\n \n-        return query_outputs\n+        return query_outputs if legacy_output else query_outputs.last_hidden_state\n \n     def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor):\n         \"\"\"\n@@ -1747,9 +1754,8 @@ def forward(\n \n         ```python\n         >>> import torch\n-        >>> from PIL import Image\n-        >>> import requests\n         >>> from transformers import AutoProcessor, Blip2VisionModelWithProjection\n+        >>> from transformers.image_utils import load_image\n \n         >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n \n@@ -1760,11 +1766,12 @@ def forward(\n         >>> model.to(device)  # doctest: +IGNORE_RESULT\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n \n-        >>> outputs = model(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     outputs = model(**inputs)\n         >>> image_embeds = outputs.image_embeds\n         >>> print(image_embeds.shape)\n         torch.Size([1, 32, 256])"
        },
        {
            "sha": "3af6e01a34d58e1a8aa244760dd4d97f53c5940b",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 22,
            "deletions": 43,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -30,7 +30,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs, logging, torch_int\n from .configuration_chinese_clip import ChineseCLIPConfig, ChineseCLIPTextConfig, ChineseCLIPVisionConfig\n \n \n@@ -1036,16 +1036,14 @@ def __init__(self, config: ChineseCLIPConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1055,45 +1053,35 @@ def get_text_features(\n         Examples:\n \n         ```python\n+        >>> import torch\n         >>> from transformers import AutoTokenizer, ChineseCLIPModel\n \n         >>> model = ChineseCLIPModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n \n         >>> inputs = tokenizer([\"杰尼龟\", \"妙蛙种子\", \"小火龙\", \"皮卡丘\"], padding=True, return_tensors=\"pt\")\n-        >>> text_features = model.get_text_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     text_features = model.get_text_features(**inputs)\n         >>> text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n         ```\"\"\"\n-        # Use CHINESE_CLIP model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        text_outputs = self.text_model(\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        pooled_output = text_outputs[0][:, 0, :]\n+        pooled_output = text_outputs.pooler_output\n         text_features = self.text_projection(pooled_output)\n \n         return text_features\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_image_features(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        pixel_values: torch.FloatTensor,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1103,37 +1091,28 @@ def get_image_features(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n         >>> from transformers import AutoProcessor, ChineseCLIPModel\n+        >>> from transformers.image_utils import load_image\n \n         >>> model = ChineseCLIPModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n         >>> processor = AutoProcessor.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n \n         >>> url = \"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n \n-        >>> image_features = model.get_image_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     image_features = model.get_image_features(**inputs)\n         >>> image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n         ```\"\"\"\n-        # Use CHINESE_CLIP model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        vision_outputs = self.vision_model(\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n-            return_dict=return_dict,\n         )\n \n-        pooled_output = vision_outputs[1]  # pooled_output\n+        pooled_output = vision_outputs.pooler_output\n         image_features = self.visual_projection(pooled_output)\n \n         return image_features\n@@ -1160,19 +1139,20 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n         >>> from transformers import AutoProcessor, ChineseCLIPModel\n+        >>> from transformers.image_utils import load_image\n \n         >>> model = ChineseCLIPModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n         >>> processor = AutoProcessor.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n \n         >>> url = \"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = processor(text=[\"杰尼龟\", \"妙蛙种子\", \"小火龙\", \"皮卡丘\"], images=image, return_tensors=\"pt\", padding=True)\n \n-        >>> outputs = model(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     outputs = model(**inputs)\n         >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n         >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n         ```\"\"\"\n@@ -1181,7 +1161,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         vision_outputs = self.vision_model(\n             pixel_values=pixel_values,"
        },
        {
            "sha": "1d5c35fbaa1926a2280c66023962f7662d937bfd",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 18,
            "deletions": 43,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -32,7 +32,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs, logging, torch_int\n from .configuration_clap import ClapAudioConfig, ClapConfig, ClapTextConfig\n \n \n@@ -1611,15 +1611,13 @@ def __init__(self, config: ClapConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1629,45 +1627,31 @@ def get_text_features(\n         Examples:\n \n         ```python\n+        >>> import torch\n         >>> from transformers import AutoTokenizer, ClapModel\n \n         >>> model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"laion/clap-htsat-unfused\")\n \n         >>> inputs = tokenizer([\"the sound of a cat\", \"the sound of a dog\"], padding=True, return_tensors=\"pt\")\n-        >>> text_features = model.get_text_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     text_features = model.get_text_features(**inputs)\n         ```\"\"\"\n-        # Use CLAP model's config for some fields (if specified) instead of those of audio & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        text_outputs = self.text_model(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n+            input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids\n         )\n-\n-        pooled_output = text_outputs[1] if return_dict is not None else text_outputs.pooler_output\n-        text_features = self.text_projection(pooled_output)\n+        text_features = self.text_projection(text_outputs.pooler_output)\n         text_features = F.normalize(text_features, dim=-1)\n \n         return text_features\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_audio_features(\n         self,\n-        input_features: Optional[torch.Tensor] = None,\n+        input_features: torch.Tensor,\n         is_longer: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n@@ -1681,30 +1665,21 @@ def get_audio_features(\n         Examples:\n \n         ```python\n-        >>> from transformers import AutoFeatureExtractor, ClapModel\n         >>> import torch\n+        >>> from transformers import AutoFeatureExtractor, ClapModel\n \n         >>> model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n         >>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"laion/clap-htsat-unfused\")\n         >>> random_audio = torch.rand((16_000))\n+\n         >>> inputs = feature_extractor(random_audio, return_tensors=\"pt\")\n-        >>> audio_features = model.get_audio_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     audio_features = model.get_audio_features(**inputs)\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        audio_outputs: BaseModelOutputWithPooling = self.audio_model(\n+            input_features=input_features, is_longer=is_longer\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        audio_outputs = self.audio_model(\n-            input_features=input_features,\n-            is_longer=is_longer,\n-            return_dict=return_dict,\n-        )\n-\n-        pooled_output = audio_outputs[1] if not return_dict else audio_outputs.pooler_output\n-\n-        audio_features = self.audio_projection(pooled_output)\n+        audio_features = self.audio_projection(audio_outputs.pooler_output)\n         audio_features = F.normalize(audio_features, dim=-1)\n \n         return audio_features"
        },
        {
            "sha": "a5f079b6e31796efaad5e6a6de1424b9ee176d50",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 27,
            "deletions": 39,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -26,7 +26,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs, logging, torch_int\n from .configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n \n \n@@ -868,14 +868,13 @@ def __init__(self, config: CLIPConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -885,39 +884,32 @@ def get_text_features(\n         Examples:\n \n         ```python\n+        >>> import torch\n         >>> from transformers import AutoTokenizer, CLIPModel\n \n         >>> model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n \n         >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n-        >>> text_features = model.get_text_features(**inputs)\n-        ```\"\"\"\n-        # Use CLIP model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n \n+        >>> with torch.inference_mode():\n+        ...     text_features = model.get_text_features(**inputs)\n+        ```\"\"\"\n         text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n         )\n-\n         pooled_output = text_outputs.pooler_output\n         text_features = self.text_projection(pooled_output)\n \n         return text_features\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_image_features(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        pixel_values: torch.FloatTensor,\n         interpolate_pos_encoding: bool = False,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -928,33 +920,25 @@ def get_image_features(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n         >>> from transformers import AutoProcessor, CLIPModel\n+        >>> from transformers.image_utils import load_image\n \n         >>> model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n         >>> processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n \n-        >>> image_features = model.get_image_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     image_features = model.get_image_features(**inputs)\n         ```\"\"\"\n-        # Use CLIP model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n-\n         pooled_output = vision_outputs.pooler_output\n         image_features = self.visual_projection(pooled_output)\n \n@@ -980,21 +964,22 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n         >>> from transformers import AutoProcessor, CLIPModel\n+        >>> from transformers.image_utils import load_image\n \n         >>> model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n         >>> processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = processor(\n         ...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n         ... )\n \n-        >>> outputs = model(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     outputs = model(**inputs)\n         >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n         >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n         ```\"\"\"\n@@ -1087,14 +1072,16 @@ def forward(\n         Examples:\n \n         ```python\n+        >>> import torch\n         >>> from transformers import AutoTokenizer, CLIPTextModelWithProjection\n \n         >>> model = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n \n         >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n \n-        >>> outputs = model(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     outputs = model(**inputs)\n         >>> text_embeds = outputs.text_embeds\n         ```\"\"\"\n \n@@ -1148,19 +1135,20 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n         >>> from transformers import AutoProcessor, CLIPVisionModelWithProjection\n+        >>> from transformers.image_utils import load_image\n \n         >>> model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n         >>> processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n \n-        >>> outputs = model(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     outputs = model(**inputs)\n         >>> image_embeds = outputs.image_embeds\n         ```\"\"\"\n "
        },
        {
            "sha": "603a5e842bed677bccb519be38706c8bbf0991ad",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 28,
            "deletions": 48,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -28,7 +28,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs, logging, torch_int\n from .configuration_clipseg import CLIPSegConfig, CLIPSegTextConfig, CLIPSegVisionConfig\n \n \n@@ -847,15 +847,13 @@ def __init__(self, config: CLIPSegConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -865,43 +863,32 @@ def get_text_features(\n         Examples:\n \n         ```python\n+        >>> import torch\n         >>> from transformers import AutoTokenizer, CLIPSegModel\n \n         >>> tokenizer = AutoTokenizer.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n         >>> model = CLIPSegModel.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n \n         >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n-        >>> text_features = model.get_text_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     text_features = model.get_text_features(**inputs)\n         ```\"\"\"\n-        # Use CLIPSEG model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        text_outputs = self.text_model(\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-\n-        pooled_output = text_outputs[1]\n+        pooled_output = text_outputs.pooler_output\n         text_features = self.text_projection(pooled_output)\n \n         return text_features\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_image_features(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        pixel_values: torch.FloatTensor,\n         interpolate_pos_encoding: bool = True,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -911,36 +898,26 @@ def get_image_features(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n         >>> from transformers import AutoProcessor, CLIPSegModel\n+        >>> from transformers.image_utils import load_image\n \n         >>> processor = AutoProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n         >>> model = CLIPSegModel.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n \n-        >>> image_features = model.get_image_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     image_features = model.get_image_features(**inputs)\n         ```\"\"\"\n-        # Use CLIPSEG model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        vision_outputs = self.vision_model(\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n-            return_dict=return_dict,\n         )\n-\n-        pooled_output = vision_outputs[1]  # pooled_output\n+        pooled_output = vision_outputs.pooler_output\n         image_features = self.visual_projection(pooled_output)\n \n         return image_features\n@@ -965,21 +942,22 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n         >>> from transformers import AutoProcessor, CLIPSegModel\n+        >>> from transformers.image_utils import load_image\n \n         >>> processor = AutoProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n         >>> model = CLIPSegModel.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = processor(\n         ...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n         ... )\n \n-        >>> outputs = model(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     outputs = model(**inputs)\n         >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n         >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n         ```\"\"\"\n@@ -1277,19 +1255,21 @@ def forward(\n         Examples:\n \n         ```python\n+        >>> import torch\n         >>> from transformers import AutoProcessor, CLIPSegForImageSegmentation\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> from transformers.image_utils import load_image\n \n         >>> processor = AutoProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n         >>> model = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n+\n         >>> texts = [\"a cat\", \"a remote\", \"a blanket\"]\n         >>> inputs = processor(text=texts, images=[image] * len(texts), padding=True, return_tensors=\"pt\")\n \n-        >>> outputs = model(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     outputs = model(**inputs)\n \n         >>> logits = outputs.logits\n         >>> print(logits.shape)"
        },
        {
            "sha": "cefaa6f95f682b17cab711e6d4e950703758f2eb",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 17,
            "deletions": 26,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -29,7 +29,7 @@\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import ModelOutput, auto_docstring, logging, torch_int\n+from ...utils import ModelOutput, auto_docstring, filter_out_non_signature_kwargs, logging, torch_int\n from .configuration_flava import (\n     FlavaConfig,\n     FlavaImageCodebookConfig,\n@@ -1081,16 +1081,14 @@ def __init__(self, config: FlavaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, text_seq_length)`):\n@@ -1111,6 +1109,7 @@ def get_text_features(\n         Examples:\n \n         ```python\n+        >>> import torch\n         >>> from transformers import AutoProcessor, FlavaModel\n \n         >>> model = FlavaModel.from_pretrained(\"{0}\")\n@@ -1119,35 +1118,30 @@ def get_text_features(\n         >>> inputs = processor(\n         ...     text=[\"a photo of a cat\", \"a photo of a dog\"], max_length=77, padding=\"max_length\", return_tensors=\"pt\"\n         ... )\n-        >>> text_features = model.get_text_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     text_features = model.get_text_features(**inputs)\n         ```\n         \"\"\"\n-        text_outputs = self.text_model(\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-\n-        pooled_output = text_outputs[0]  # last_hidden_state\n+        pooled_output = text_outputs.last_hidden_state\n         text_features = self.text_projection(pooled_output)\n \n         return text_features\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_image_features(\n         self,\n-        pixel_values: Optional[torch.Tensor] = None,\n+        pixel_values: torch.Tensor,\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n         interpolate_pos_encoding: Optional[bool] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, image_num_patches)`):\n@@ -1160,33 +1154,30 @@ def get_image_features(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n         >>> from transformers import AutoProcessor, FlavaModel\n+        >>> from transformers.image_utils import load_image\n \n         >>> model = FlavaModel.from_pretrained(\"{0}\")\n         >>> processor = AutoProcessor.from_pretrained(\"{0}\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n \n-        >>> image_features = model.get_image_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     image_features = model.get_image_features(**inputs)\n         ```\n         \"\"\"\n-        image_outputs = self.image_model(\n+        image_outputs: BaseModelOutputWithPooling = self.image_model(\n             pixel_values=pixel_values,\n             bool_masked_pos=bool_masked_pos,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n-            return_dict=return_dict,\n         )\n-\n-        pooled_output = image_outputs[0]  # last_hidden_state\n+        pooled_output = image_outputs.last_hidden_state\n         image_features = self.image_projection(pooled_output)\n \n         return image_features"
        },
        {
            "sha": "7d705d98e6ed790c51f8efdd834c11a5e5de74fe",
            "filename": "src/transformers/models/groupvit/modeling_groupvit.py",
            "status": "modified",
            "additions": 17,
            "deletions": 49,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -28,7 +28,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, logging, torch_int\n+from ...utils import ModelOutput, auto_docstring, filter_out_non_signature_kwargs, logging, torch_int\n from .configuration_groupvit import GroupViTConfig, GroupViTTextConfig, GroupViTVisionConfig\n \n \n@@ -1219,15 +1219,13 @@ def __init__(self, config: GroupViTConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1237,43 +1235,27 @@ def get_text_features(\n         Examples:\n \n         ```python\n+        >>> import torch\n         >>> from transformers import CLIPTokenizer, GroupViTModel\n \n         >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n         >>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n \n         >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n-        >>> text_features = model.get_text_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     text_features = model.get_text_features(**inputs)\n         ```\"\"\"\n-        # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        text_outputs = self.text_model(\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-\n-        pooled_output = text_outputs[1]\n-        text_features = self.text_projection(pooled_output)\n-\n+        text_features = self.text_projection(text_outputs.pooler_output)\n         return text_features\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n-    def get_image_features(\n-        self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> torch.FloatTensor:\n+    def get_image_features(self, pixel_values: torch.Tensor) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n             image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\n@@ -1282,37 +1264,23 @@ def get_image_features(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n         >>> from transformers import AutoProcessor, GroupViTModel\n+        >>> from transformers.image_utils import load_image\n \n         >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n         >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n \n-        >>> image_features = model.get_image_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     image_features = model.get_image_features(**inputs)\n         ```\"\"\"\n-        # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        vision_outputs = self.vision_model(\n-            pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        pooled_output = vision_outputs[1]  # pooled_output\n-        image_features = self.visual_projection(pooled_output)\n-\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(pixel_values)\n+        image_features = self.visual_projection(vision_outputs.pooler_output)\n         return image_features\n \n     @auto_docstring"
        },
        {
            "sha": "a8befe57611e87b2275e348fa47cb2399e0d4abc",
            "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
            "status": "modified",
            "additions": 35,
            "deletions": 39,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -17,7 +17,15 @@\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils import (\n+    ModelOutput,\n+    TransformersKwargs,\n+    auto_docstring,\n+    can_return_tuple,\n+    filter_out_non_signature_kwargs,\n+    logging,\n+    torch_int,\n+)\n from ...utils.generic import check_model_inputs\n from .configuration_metaclip_2 import MetaClip2Config, MetaClip2TextConfig, MetaClip2VisionConfig\n \n@@ -649,14 +657,16 @@ def forward(\n         Examples:\n \n         ```python\n+        >>> import torch\n         >>> from transformers import AutoTokenizer, MetaClip2TextModelWithProjection\n \n         >>> model = MetaClip2TextModelWithProjection.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n \n         >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n \n-        >>> outputs = model(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     outputs = model(**inputs)\n         >>> text_embeds = outputs.text_embeds\n         ```\"\"\"\n \n@@ -779,14 +789,13 @@ def __init__(self, config: MetaClip2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -796,39 +805,32 @@ def get_text_features(\n         Examples:\n \n         ```python\n+        >>> import torch\n         >>> from transformers import AutoTokenizer, MetaClip2Model\n \n         >>> model = MetaClip2Model.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n \n         >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n-        >>> text_features = model.get_text_features(**inputs)\n-        ```\"\"\"\n-        # Use METACLIP_2 model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n \n+        >>> with torch.inference_mode():\n+        ...     text_features = model.get_text_features(**inputs)\n+        ```\"\"\"\n         text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n         )\n-\n         pooled_output = text_outputs.pooler_output\n         text_features = self.text_projection(pooled_output)\n \n         return text_features\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_image_features(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        pixel_values: torch.FloatTensor,\n         interpolate_pos_encoding: bool = False,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -839,33 +841,25 @@ def get_image_features(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n         >>> from transformers import AutoProcessor, MetaClip2Model\n+        >>> from transformers.image_utils import load_image\n \n         >>> model = MetaClip2Model.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n         >>> processor = AutoProcessor.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n \n-        >>> image_features = model.get_image_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     image_features = model.get_image_features(**inputs)\n         ```\"\"\"\n-        # Use METACLIP_2 model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n-\n         pooled_output = vision_outputs.pooler_output\n         image_features = self.visual_projection(pooled_output)\n \n@@ -891,21 +885,22 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n         >>> from transformers import AutoProcessor, MetaClip2Model\n+        >>> from transformers.image_utils import load_image\n \n         >>> model = MetaClip2Model.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n         >>> processor = AutoProcessor.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = processor(\n         ...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n         ... )\n \n-        >>> outputs = model(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     outputs = model(**inputs)\n         >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n         >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n         ```\"\"\"\n@@ -1116,19 +1111,20 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n         >>> from transformers import AutoProcessor, MetaClip2VisionModelWithProjection\n+        >>> from transformers.image_utils import load_image\n \n         >>> model = MetaClip2VisionModelWithProjection.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n         >>> processor = AutoProcessor.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n \n-        >>> outputs = model(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     outputs = model(**inputs)\n         >>> image_embeds = outputs.image_embeds\n         ```\"\"\"\n "
        },
        {
            "sha": "47c62f2f53708d758be5bc5a6fdd2cc57bf1ea56",
            "filename": "src/transformers/models/owlv2/modeling_owlv2.py",
            "status": "modified",
            "additions": 26,
            "deletions": 34,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -27,7 +27,14 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, is_vision_available, logging, torch_int\n+from ...utils import (\n+    ModelOutput,\n+    auto_docstring,\n+    filter_out_non_signature_kwargs,\n+    is_vision_available,\n+    logging,\n+    torch_int,\n+)\n from .configuration_owlv2 import Owlv2Config, Owlv2TextConfig, Owlv2VisionConfig\n \n \n@@ -957,14 +964,12 @@ def __init__(self, config: Owlv2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size * num_max_text_queries, sequence_length)`):\n@@ -978,33 +983,29 @@ def get_text_features(\n \n         Examples:\n         ```python\n+        >>> import torch\n         >>> from transformers import AutoProcessor, Owlv2Model\n \n         >>> model = Owlv2Model.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n         >>> processor = AutoProcessor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n         >>> inputs = processor(\n         ...     text=[[\"a photo of a cat\", \"a photo of a dog\"], [\"photo of a astranaut\"]], return_tensors=\"pt\"\n         ... )\n-        >>> text_features = model.get_text_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     text_features = model.get_text_features(**inputs)\n         ```\"\"\"\n-        # Use OWLv2 model's config for some fields (if specified) instead of those of vision & text components.\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         # Get embeddings for all text queries in all batch samples\n-        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask, return_dict=return_dict)\n-        pooled_output = text_output[1]\n-        text_features = self.text_projection(pooled_output)\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n+        text_features = self.text_projection(text_outputs.pooler_output)\n \n         return text_features\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_image_features(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        pixel_values: torch.Tensor,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1013,34 +1014,25 @@ def get_image_features(\n \n         Examples:\n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n+        >>> from transformers.image_utils import load_image\n         >>> from transformers import AutoProcessor, Owlv2Model\n \n         >>> model = Owlv2Model.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n         >>> processor = AutoProcessor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n+\n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n+\n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n-        >>> image_features = model.get_image_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     image_features = model.get_image_features(**inputs)\n         ```\"\"\"\n-        # Use OWLv2 model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        vision_outputs = self.vision_model(\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n-            return_dict=return_dict,\n         )\n-\n-        pooled_output = vision_outputs[1]\n-        image_features = self.visual_projection(pooled_output)\n+        image_features = self.visual_projection(vision_outputs.pooler_output)\n \n         return image_features\n "
        },
        {
            "sha": "352bdd07dbf57be78608b41297a7a8883ae8dca1",
            "filename": "src/transformers/models/owlvit/modeling_owlvit.py",
            "status": "modified",
            "additions": 26,
            "deletions": 34,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -27,7 +27,14 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, is_vision_available, logging, torch_int\n+from ...utils import (\n+    ModelOutput,\n+    auto_docstring,\n+    filter_out_non_signature_kwargs,\n+    is_vision_available,\n+    logging,\n+    torch_int,\n+)\n from .configuration_owlvit import OwlViTConfig, OwlViTTextConfig, OwlViTVisionConfig\n \n \n@@ -938,14 +945,12 @@ def __init__(self, config: OwlViTConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size * num_max_text_queries, sequence_length)`):\n@@ -959,33 +964,29 @@ def get_text_features(\n \n         Examples:\n         ```python\n+        >>> import torch\n         >>> from transformers import AutoProcessor, OwlViTModel\n \n         >>> model = OwlViTModel.from_pretrained(\"google/owlvit-base-patch32\")\n         >>> processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n         >>> inputs = processor(\n         ...     text=[[\"a photo of a cat\", \"a photo of a dog\"], [\"photo of a astranaut\"]], return_tensors=\"pt\"\n         ... )\n-        >>> text_features = model.get_text_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     text_features = model.get_text_features(**inputs)\n         ```\"\"\"\n-        # Use OWL-ViT model's config for some fields (if specified) instead of those of vision & text components.\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         # Get embeddings for all text queries in all batch samples\n-        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask, return_dict=return_dict)\n-        pooled_output = text_output[1]\n-        text_features = self.text_projection(pooled_output)\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n+        text_features = self.text_projection(text_outputs.pooler_output)\n \n         return text_features\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_image_features(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        pixel_values: torch.Tensor,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -994,34 +995,25 @@ def get_image_features(\n \n         Examples:\n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n+        >>> from transformers.image_utils import load_image\n         >>> from transformers import AutoProcessor, OwlViTModel\n \n         >>> model = OwlViTModel.from_pretrained(\"google/owlvit-base-patch32\")\n         >>> processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n+\n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n+\n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n-        >>> image_features = model.get_image_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     image_features = model.get_image_features(**inputs)\n         ```\"\"\"\n-        # Use OWL-ViT model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        vision_outputs = self.vision_model(\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n-            return_dict=return_dict,\n         )\n-\n-        pooled_output = vision_outputs[1]\n-        image_features = self.visual_projection(pooled_output)\n+        image_features = self.visual_projection(vision_outputs.pooler_output)\n \n         return image_features\n "
        },
        {
            "sha": "9a0a3666006983bda4f7cc9366d554fe996d4a39",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 8,
            "deletions": 29,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -30,7 +30,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple, torch_int\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs, torch_int\n from .configuration_siglip import SiglipConfig, SiglipTextConfig, SiglipVisionConfig\n \n \n@@ -899,14 +899,13 @@ def __init__(self, config: SiglipConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -927,30 +926,20 @@ def get_text_features(\n         >>> with torch.no_grad():\n         ...     text_features = model.get_text_features(**inputs)\n         ```\"\"\"\n-        # Use SigLIP model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n         )\n-\n         pooled_output = text_outputs.pooler_output\n \n         return pooled_output\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_image_features(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        pixel_values: torch.FloatTensor,\n         interpolate_pos_encoding: bool = False,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -961,35 +950,25 @@ def get_image_features(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n-        >>> from transformers import AutoProcessor, AutoModel\n         >>> import torch\n+        >>> from transformers import AutoProcessor, AutoModel\n+        >>> from transformers.image_utils import load_image\n \n         >>> model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\")\n         >>> processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n \n         >>> with torch.no_grad():\n         ...     image_features = model.get_image_features(**inputs)\n         ```\"\"\"\n-        # Use SiglipModel's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n-\n         pooled_output = vision_outputs.pooler_output\n \n         return pooled_output"
        },
        {
            "sha": "337b4b5a4cf4ad124ba0e4b36cd6eb5d03beb6b4",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 30,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -35,7 +35,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs\n from .configuration_siglip2 import Siglip2Config, Siglip2TextConfig, Siglip2VisionConfig\n \n \n@@ -953,14 +953,13 @@ def __init__(self, config: Siglip2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -981,32 +980,22 @@ def get_text_features(\n         >>> with torch.no_grad():\n         ...     text_features = model.get_text_features(**inputs)\n         ```\"\"\"\n-        # Use Siglip2 model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n         )\n-\n         pooled_output = text_outputs.pooler_output\n \n         return pooled_output\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_image_features(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         pixel_attention_mask: Optional[torch.Tensor] = None,\n         spatial_shapes: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         pixel_attention_mask (`torch.Tensor` of shape `(batch_size, image_size, image_size)`, *optional*):\n@@ -1021,37 +1010,27 @@ def get_image_features(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n-        >>> from transformers import AutoProcessor, AutoModel\n         >>> import torch\n+        >>> from transformers import AutoProcessor, AutoModel\n+        >>> from transformers.image_utils import load_image\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = load_image(url)\n \n         >>> model = AutoModel.from_pretrained(\"google/siglip2-base-patch16-224\")\n         >>> processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-224\")\n \n-        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n-\n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n \n         >>> with torch.no_grad():\n         ...     image_features = model.get_image_features(**inputs)\n         ```\n         \"\"\"\n-        # Use Siglip2Model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n             attention_mask=pixel_attention_mask,\n             spatial_shapes=spatial_shapes,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n         )\n-\n         pooled_output = vision_outputs.pooler_output\n \n         return pooled_output"
        },
        {
            "sha": "beef34d7f12da18440f57919253cde15dd94656a",
            "filename": "src/transformers/models/siglip2/modular_siglip2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 17,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -37,6 +37,7 @@\n )\n \n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...utils import auto_docstring, filter_out_non_signature_kwargs\n \n \n class Siglip2TextConfig(SiglipTextConfig):\n@@ -359,13 +360,13 @@ def forward(\n \n class Siglip2Model(SiglipModel):\n     # Update: add `spatial_shapes` and `pixel_attention_mask`\n+    @filter_out_non_signature_kwargs()\n+    @auto_docstring\n     def get_image_features(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         pixel_attention_mask: Optional[torch.Tensor] = None,\n         spatial_shapes: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         pixel_attention_mask (`torch.Tensor` of shape `(batch_size, image_size, image_size)`, *optional*):\n@@ -380,37 +381,27 @@ def get_image_features(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n-        >>> from transformers import AutoProcessor, AutoModel\n         >>> import torch\n+        >>> from transformers import AutoProcessor, AutoModel\n+        >>> from transformers.image_utils import load_image\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = load_image(url)\n \n         >>> model = AutoModel.from_pretrained(\"google/siglip2-base-patch16-224\")\n         >>> processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-224\")\n \n-        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n-\n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n \n         >>> with torch.no_grad():\n         ...     image_features = model.get_image_features(**inputs)\n         ```\n         \"\"\"\n-        # Use Siglip2Model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n             attention_mask=pixel_attention_mask,\n             spatial_shapes=spatial_shapes,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n         )\n-\n         pooled_output = vision_outputs.pooler_output\n \n         return pooled_output"
        },
        {
            "sha": "039f9fa9e9c5ae247f25f8e54ed03dc4656376fc",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 22,
            "deletions": 37,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -19,8 +19,9 @@\n import torch\n from torch import nn\n \n+from ...modeling_outputs import BaseModelOutputWithPooling\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...utils import auto_docstring, filter_out_non_signature_kwargs, logging\n from ..auto.configuration_auto import AutoConfig\n from ..auto.modeling_auto import AutoModel\n from ..clip.modeling_clip import CLIPOutput, CLIPVisionConfig, CLIPVisionModel\n@@ -100,17 +101,15 @@ def __init__(\n         self.text_projection = nn.Linear(self.text_embed_dim, self.projection_dim, bias=False)\n         self.logit_scale = nn.Parameter(torch.tensor(self.config.logit_scale_init_value))\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids=None,\n-        attention_mask=None,\n-        position_ids=None,\n-        token_type_ids=None,\n-        output_attentions=None,\n-        output_hidden_states=None,\n-        return_dict=None,\n-    ):\n+        input_ids: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        token_type_ids: Optional[torch.Tensor] = None,\n+    ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n             text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The text embeddings obtained by\n@@ -119,37 +118,29 @@ def get_text_features(\n         Examples:\n \n         ```python\n+        >>> import torch\n         >>> from transformers import VisionTextDualEncoderModel, AutoTokenizer\n \n         >>> model = VisionTextDualEncoderModel.from_pretrained(\"clip-italian/clip-italian\")\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"clip-italian/clip-italian\")\n \n         >>> inputs = tokenizer([\"una foto di un gatto\", \"una foto di un cane\"], padding=True, return_tensors=\"pt\")\n-        >>> text_features = model.get_text_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     text_features = model.get_text_features(**inputs)\n         ```\"\"\"\n-        text_outputs = self.text_model(\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             token_type_ids=token_type_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-\n-        pooled_output = text_outputs[1]\n-        text_features = self.text_projection(pooled_output)\n+        text_features = self.text_projection(text_outputs.pooler_output)\n \n         return text_features\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n-    def get_image_features(\n-        self,\n-        pixel_values=None,\n-        output_attentions=None,\n-        output_hidden_states=None,\n-        return_dict=None,\n-    ):\n+    def get_image_features(self, pixel_values: torch.Tensor) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n             image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\n@@ -158,29 +149,23 @@ def get_image_features(\n         Examples:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n+        >>> import torch\n         >>> from transformers import VisionTextDualEncoderModel, AutoImageProcessor\n+        >>> from transformers.image_utils import load_image\n \n         >>> model = VisionTextDualEncoderModel.from_pretrained(\"clip-italian/clip-italian\")\n         >>> image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> image = load_image(url)\n \n         >>> inputs = image_processor(images=image, return_tensors=\"pt\")\n \n-        >>> image_features = model.get_image_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     image_features = model.get_image_features(**inputs)\n         ```\"\"\"\n-        vision_outputs = self.vision_model(\n-            pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        pooled_output = vision_outputs[1]  # pooled_output\n-        image_features = self.visual_projection(pooled_output)\n+        vision_outputs = self.vision_model(pixel_values=pixel_values)\n+        image_features = self.visual_projection(vision_outputs.pooler_output)\n \n         return image_features\n "
        },
        {
            "sha": "31b5b5c752859dfd84bf58f042d07308a79fcb6d",
            "filename": "src/transformers/models/x_clip/modeling_x_clip.py",
            "status": "modified",
            "additions": 14,
            "deletions": 45,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2537ed44779772613957cac42ba20b36b41c22f3/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py?ref=2537ed44779772613957cac42ba20b36b41c22f3",
            "patch": "@@ -31,6 +31,7 @@\n     ModelOutput,\n     auto_docstring,\n     can_return_tuple,\n+    filter_out_non_signature_kwargs,\n     logging,\n     torch_int,\n )\n@@ -1203,15 +1204,13 @@ def __init__(self, config: XCLIPConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1221,42 +1220,29 @@ def get_text_features(\n         Examples:\n \n         ```python\n+        >>> import torch\n         >>> from transformers import AutoTokenizer, AutoModel\n \n         >>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/xclip-base-patch32\")\n         >>> model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n \n         >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n-        >>> text_features = model.get_text_features(**inputs)\n+        >>> with torch.inference_mode():\n+        ...     text_features = model.get_text_features(**inputs)\n         ```\"\"\"\n-        # Use X_CLIP model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        text_outputs = self.text_model(\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n+        text_features = self.text_projection(text_outputs.pooler_output)\n+        return text_features\n \n-        text_embeds = text_outputs[1]\n-        text_embeds = self.text_projection(text_embeds)\n-\n-        return text_embeds\n-\n+    @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_video_features(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        pixel_values: torch.Tensor,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1333,35 +1319,18 @@ def get_video_features(\n \n         >>> video_features = model.get_video_features(**inputs)\n         ```\"\"\"\n-        # Use X_CLIP model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         batch_size, num_frames, num_channels, height, width = pixel_values.shape\n         pixel_values = pixel_values.reshape(-1, num_channels, height, width)\n \n-        vision_outputs = self.vision_model(\n-            pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(pixel_values=pixel_values)\n \n-        video_embeds = vision_outputs[1]\n+        video_embeds = vision_outputs.pooler_output\n         video_embeds = self.visual_projection(video_embeds)\n \n         cls_features = video_embeds.view(batch_size, num_frames, -1)\n \n-        mit_outputs = self.mit(\n-            cls_features,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-        video_embeds = mit_outputs[1]\n+        mit_outputs: BaseModelOutputWithPooling = self.mit(cls_features)\n+        video_embeds = mit_outputs.pooler_output\n \n         return video_embeds\n "
        }
    ],
    "stats": {
        "total": 1132,
        "additions": 421,
        "deletions": 711
    }
}