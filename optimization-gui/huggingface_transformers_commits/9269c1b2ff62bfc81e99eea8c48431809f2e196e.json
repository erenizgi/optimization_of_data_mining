{
    "author": "raimbekovm",
    "message": "Fix spelling typos in comments and code (#43046)\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "9269c1b2ff62bfc81e99eea8c48431809f2e196e",
    "files": [
        {
            "sha": "9e43baf498b1bf2387c4e65ef239f0d260445438",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9269c1b2ff62bfc81e99eea8c48431809f2e196e/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9269c1b2ff62bfc81e99eea8c48431809f2e196e/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=9269c1b2ff62bfc81e99eea8c48431809f2e196e",
            "patch": "@@ -145,7 +145,7 @@ def convert(\n     ) -> dict[str, torch.Tensor]:\n         target_pattern = self.get_target_pattern(target_patterns)\n         all_tensors = []\n-        # Very important to keep the relative order of the source patterms here, so we iterate over them not the\n+        # Very important to keep the relative order of the source patterns here, so we iterate over them not the\n         # input directly as it's unordered!\n         for source_pattern in source_patterns:\n             tensors = input_dict[source_pattern]"
        },
        {
            "sha": "6386506bb53ac11c58fdbcd31731e1c53b1d43fc",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9269c1b2ff62bfc81e99eea8c48431809f2e196e/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9269c1b2ff62bfc81e99eea8c48431809f2e196e/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=9269c1b2ff62bfc81e99eea8c48431809f2e196e",
            "patch": "@@ -2415,7 +2415,7 @@ def generate(\n                 raise NotImplementedError(\n                     f\"assistant_model is not supported for continuous batching. Got {assistant_model = }\"\n                 )\n-            if streamer is not None:  # TODO: actualy this could be supported\n+            if streamer is not None:  # TODO: actually this could be supported\n                 raise NotImplementedError(f\"streaming is not supported for continuous batching. Got {streamer = }\")\n             if negative_prompt_ids is not None:\n                 raise NotImplementedError("
        },
        {
            "sha": "407c84391ac6f1e9cc33a2422e46a9e68847d6ba",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9269c1b2ff62bfc81e99eea8c48431809f2e196e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9269c1b2ff62bfc81e99eea8c48431809f2e196e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=9269c1b2ff62bfc81e99eea8c48431809f2e196e",
            "patch": "@@ -3937,7 +3937,7 @@ def generate(\n                 - **Text** (`torch.Tensor`): Generated text token sequence.\n                 - **Audio waveform** (`torch.Tensor`): Generated audio waveform.\n         \"\"\"\n-        # check `False` on purpose because the paramter can be `str/bool`. This is needed for BC\n+        # check `False` on purpose because the parameter can be `str/bool`. This is needed for BC\n         generation_mode = kwargs.pop(\"generation_mode\", None)\n         return_audio = generation_mode != \"text\" and generation_mode is not False\n "
        },
        {
            "sha": "514e9decd13175d7ef948ab428ddfc39112b6d7a",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9269c1b2ff62bfc81e99eea8c48431809f2e196e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9269c1b2ff62bfc81e99eea8c48431809f2e196e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=9269c1b2ff62bfc81e99eea8c48431809f2e196e",
            "patch": "@@ -4096,7 +4096,7 @@ def generate(\n                 - **Text** (`torch.Tensor`): Generated text token sequence.\n                 - **Audio waveform** (`torch.Tensor`): Generated audio waveform.\n         \"\"\"\n-        # check `False` on purpose because the paramter can be `str/bool`. This is needed for BC\n+        # check `False` on purpose because the parameter can be `str/bool`. This is needed for BC\n         generation_mode = kwargs.pop(\"generation_mode\", None)\n         return_audio = generation_mode != \"text\" and generation_mode is not False\n "
        },
        {
            "sha": "d3d5b365fc49d867c6a3f61d0377cd1c88f940e3",
            "filename": "src/transformers/models/xcodec/modeling_xcodec.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9269c1b2ff62bfc81e99eea8c48431809f2e196e/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9269c1b2ff62bfc81e99eea8c48431809f2e196e/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py?ref=9269c1b2ff62bfc81e99eea8c48431809f2e196e",
            "patch": "@@ -516,7 +516,7 @@ def encode(\n         e_semantic_input = self._extract_semantic_features(input_values).detach()\n         e_semantic = self.encoder_semantic(e_semantic_input.transpose(1, 2))\n \n-        # orignal codebase infer to get the output length, but we can directly infer it\n+        # original codebase infer to get the output length, but we can directly infer it\n         # from the model and know whether we should pad\n         if self._get_conv1d_output_lengths(input_values.shape[2], self.acoustic_encoder) != e_semantic.shape[2]:\n             e_acoustic = self.acoustic_encoder(F.pad(input_values, (self.pad, self.pad)))"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 5,
        "deletions": 5
    }
}