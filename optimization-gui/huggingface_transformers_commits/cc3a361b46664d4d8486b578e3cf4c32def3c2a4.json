{
    "author": "SunMarc",
    "message": "[quants] refactor logic for modules_to_not_convert (#36672)",
    "sha": "cc3a361b46664d4d8486b578e3cf4c32def3c2a4",
    "files": [
        {
            "sha": "5193670815999fce60de5ee4a55d944bfac8df3c",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=cc3a361b46664d4d8486b578e3cf4c32def3c2a4",
            "patch": "@@ -247,6 +247,25 @@ def _dequantize(self, model):\n             f\"{self.quantization_config.quant_method} has no implementation of `dequantize`, please raise an issue on GitHub.\"\n         )\n \n+    @staticmethod\n+    def get_modules_to_not_convert(\n+        model: \"PreTrainedModel\",\n+        skip_modules: Optional[List[str]] = None,\n+        keep_in_fp32_modules: Optional[List[str]] = None,\n+    ):\n+        from ..integrations import get_keys_to_not_convert\n+\n+        modules_to_not_convert = []\n+        if skip_modules is None:\n+            modules_to_not_convert = get_keys_to_not_convert(model)\n+        else:\n+            modules_to_not_convert = skip_modules\n+\n+        if keep_in_fp32_modules is not None:\n+            modules_to_not_convert.extend(keep_in_fp32_modules)\n+\n+        return modules_to_not_convert\n+\n     @property\n     def is_qat_trainable(self) -> bool:\n         \"\"\"Flag indicating whether the quantized model can carry out quantization aware training\"\"\""
        },
        {
            "sha": "28460ac38edb5e899689df7259d95b5d733bce6d",
            "filename": "src/transformers/quantizers/quantizer_awq.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py?ref=cc3a361b46664d4d8486b578e3cf4c32def3c2a4",
            "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import importlib.metadata\n-from typing import TYPE_CHECKING\n+from typing import TYPE_CHECKING, List, Optional\n \n from packaging import version\n \n@@ -96,13 +96,14 @@ def update_torch_dtype(self, torch_dtype):\n             logger.warning(\"We suggest you to set `torch_dtype=torch.float16` for better efficiency with AWQ.\")\n         return torch_dtype\n \n-    def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n-        from ..integrations import get_keys_to_not_convert, replace_quantization_scales, replace_with_awq_linear\n+    def _process_model_before_weight_loading(\n+        self, model: \"PreTrainedModel\", keep_in_fp32_modules: Optional[List[str]] = None, **kwargs\n+    ):\n+        from ..integrations import replace_quantization_scales, replace_with_awq_linear\n \n-        self.modules_to_not_convert = get_keys_to_not_convert(model)\n-\n-        if self.quantization_config.modules_to_not_convert is not None:\n-            self.modules_to_not_convert.extend(self.quantization_config.modules_to_not_convert)\n+        self.modules_to_not_convert = self.get_modules_to_not_convert(\n+            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+        )\n \n         model, has_been_replaced = replace_with_awq_linear(\n             model, quantization_config=self.quantization_config, modules_to_not_convert=self.modules_to_not_convert"
        },
        {
            "sha": "e56bb161ac449853f0fb12bc86aed1a9c75b88ee",
            "filename": "src/transformers/quantizers/quantizer_bitnet.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py?ref=cc3a361b46664d4d8486b578e3cf4c32def3c2a4",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Dict, List, Union\n+from typing import TYPE_CHECKING, Dict, List, Optional, Union\n \n from .base import HfQuantizer\n \n@@ -81,16 +81,14 @@ def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        device_map,\n-        keep_in_fp32_modules: List[str] = [],\n+        keep_in_fp32_modules: Optional[List[str]] = None,\n         **kwargs,\n     ):\n-        from ..integrations import get_keys_to_not_convert, replace_with_bitnet_linear\n+        from ..integrations import replace_with_bitnet_linear\n \n-        self.modules_to_not_convert = get_keys_to_not_convert(model)\n-\n-        if self.quantization_config.modules_to_not_convert is not None:\n-            self.modules_to_not_convert.extend(self.quantization_config.modules_to_not_convert)\n+        self.modules_to_not_convert = self.get_modules_to_not_convert(\n+            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+        )\n \n         model = replace_with_bitnet_linear(\n             model,"
        },
        {
            "sha": "7fb9176c4673a009914600ae47068c22826739d2",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 5,
            "deletions": 12,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=cc3a361b46664d4d8486b578e3cf4c32def3c2a4",
            "patch": "@@ -288,23 +288,16 @@ def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n         device_map,\n-        keep_in_fp32_modules: List[str] = [],\n+        keep_in_fp32_modules: Optional[List[str]] = None,\n         **kwargs,\n     ):\n-        from ..integrations import get_keys_to_not_convert, replace_with_bnb_linear\n+        from ..integrations import replace_with_bnb_linear\n \n         llm_int8_enable_fp32_cpu_offload = self.quantization_config.llm_int8_enable_fp32_cpu_offload\n \n-        # We keep some modules such as the lm_head in their original dtype for numerical stability reasons\n-        if self.quantization_config.llm_int8_skip_modules is None:\n-            self.modules_to_not_convert = get_keys_to_not_convert(model)\n-        else:\n-            self.modules_to_not_convert = self.quantization_config.llm_int8_skip_modules\n-\n-        if not isinstance(self.modules_to_not_convert, list):\n-            self.modules_to_not_convert = [self.modules_to_not_convert]\n-\n-        self.modules_to_not_convert.extend(keep_in_fp32_modules)\n+        self.modules_to_not_convert = self.get_modules_to_not_convert(\n+            model, self.quantization_config.llm_int8_skip_modules, keep_in_fp32_modules\n+        )\n \n         # Extend `self.modules_to_not_convert` to keys that are supposed to be offloaded to `cpu` or `disk`\n         if isinstance(device_map, dict) and len(device_map.keys()) > 1:"
        },
        {
            "sha": "cac339b16b9a099f52cc8715eb0b0ecddcff54fd",
            "filename": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "status": "modified",
            "additions": 5,
            "deletions": 12,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py?ref=cc3a361b46664d4d8486b578e3cf4c32def3c2a4",
            "patch": "@@ -245,23 +245,16 @@ def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n         device_map,\n-        keep_in_fp32_modules: List[str] = [],\n+        keep_in_fp32_modules: Optional[List[str]] = None,\n         **kwargs,\n     ):\n-        from ..integrations import get_keys_to_not_convert, replace_with_bnb_linear\n+        from ..integrations import replace_with_bnb_linear\n \n         llm_int8_enable_fp32_cpu_offload = self.quantization_config.llm_int8_enable_fp32_cpu_offload\n \n-        # We keep some modules such as the lm_head in their original dtype for numerical stability reasons\n-        if self.quantization_config.llm_int8_skip_modules is None:\n-            self.modules_to_not_convert = get_keys_to_not_convert(model)\n-        else:\n-            self.modules_to_not_convert = self.quantization_config.llm_int8_skip_modules\n-\n-        if not isinstance(self.modules_to_not_convert, list):\n-            self.modules_to_not_convert = [self.modules_to_not_convert]\n-\n-        self.modules_to_not_convert.extend(keep_in_fp32_modules)\n+        self.modules_to_not_convert = self.get_modules_to_not_convert(\n+            model, self.quantization_config.llm_int8_skip_modules, keep_in_fp32_modules\n+        )\n \n         # Extend `self.modules_to_not_convert` to keys that are supposed to be offloaded to `cpu` or `disk`\n         if isinstance(device_map, dict) and len(device_map.keys()) > 1:"
        },
        {
            "sha": "988f90789ac3751ca27a9ce4d1ed71762fd7c434",
            "filename": "src/transformers/quantizers/quantizer_eetq.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py?ref=cc3a361b46664d4d8486b578e3cf4c32def3c2a4",
            "patch": "@@ -155,16 +155,14 @@ def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        device_map,\n-        keep_in_fp32_modules: List[str] = [],\n+        keep_in_fp32_modules: Optional[List[str]] = None,\n         **kwargs,\n     ):\n-        from ..integrations import get_keys_to_not_convert, replace_with_eetq_linear\n+        from ..integrations import replace_with_eetq_linear\n \n-        self.modules_to_not_convert = get_keys_to_not_convert(model)\n-\n-        if self.quantization_config.modules_to_not_convert is not None:\n-            self.modules_to_not_convert.extend(self.quantization_config.modules_to_not_convert)\n+        self.modules_to_not_convert = self.get_modules_to_not_convert(\n+            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+        )\n \n         model = replace_with_eetq_linear(\n             model,"
        },
        {
            "sha": "dd0927765d19e7be1aa5e5f1cc22262587ec1e1a",
            "filename": "src/transformers/quantizers/quantizer_fbgemm_fp8.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py?ref=cc3a361b46664d4d8486b578e3cf4c32def3c2a4",
            "patch": "@@ -161,16 +161,14 @@ def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        device_map,\n-        keep_in_fp32_modules: List[str] = [],\n+        keep_in_fp32_modules: Optional[List[str]] = None,\n         **kwargs,\n     ):\n-        from ..integrations import get_keys_to_not_convert, replace_with_fbgemm_fp8_linear\n+        from ..integrations import replace_with_fbgemm_fp8_linear\n \n-        self.modules_to_not_convert = get_keys_to_not_convert(model)\n-\n-        if self.quantization_config.modules_to_not_convert is not None:\n-            self.modules_to_not_convert.extend(self.quantization_config.modules_to_not_convert)\n+        self.modules_to_not_convert = self.get_modules_to_not_convert(\n+            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+        )\n \n         model = replace_with_fbgemm_fp8_linear(\n             model,"
        },
        {
            "sha": "ac6b73551215e9bbed4e42271fe74dada892e83e",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=cc3a361b46664d4d8486b578e3cf4c32def3c2a4",
            "patch": "@@ -162,16 +162,14 @@ def check_quantized_param(\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        device_map,\n-        modules_to_not_convert: List[str] = [],\n+        keep_in_fp32_modules: Optional[List[str]] = None,\n         **kwargs,\n     ):\n         from ..integrations.finegrained_fp8 import replace_with_fp8_linear\n \n-        self.modules_to_not_convert = [\"lm_head\"] + modules_to_not_convert\n-\n-        if self.quantization_config.modules_to_not_convert:\n-            self.modules_to_not_convert.extend(self.quantization_config.modules_to_not_convert)\n+        self.modules_to_not_convert = self.get_modules_to_not_convert(\n+            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+        )\n \n         model = replace_with_fp8_linear(\n             model,"
        },
        {
            "sha": "93ab958a30c5146a3a63c2431e1f9bc2ebe0d34c",
            "filename": "src/transformers/quantizers/quantizer_hqq.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py?ref=cc3a361b46664d4d8486b578e3cf4c32def3c2a4",
            "patch": "@@ -273,12 +273,8 @@ def forward_with_device(self, x):\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        device_map,\n-        keep_in_fp32_modules: List[str] = None,\n         **kwargs,\n     ):\n-        keep_in_fp32_modules = keep_in_fp32_modules if keep_in_fp32_modules is not None else []\n-\n         # Add the corresponding quant_config to each valid module. This allows us to do the actual nn.Linear -> HQQLinear conversion in create_quantized_param().\n         # prepare_for_hqq_linear() also sets the right quantization config inside the model (model.config.quantization_config) and the layers (hqq_layer.quant_config)\n         model = prepare_for_hqq_linear(model, quantization_config=self.quantization_config)"
        },
        {
            "sha": "be760f0d430c16c5a81036972689ebca553bb20f",
            "filename": "src/transformers/quantizers/quantizer_quanto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 12,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py?ref=cc3a361b46664d4d8486b578e3cf4c32def3c2a4",
            "patch": "@@ -177,20 +177,13 @@ def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n             )\n \n     def _process_model_before_weight_loading(\n-        self, model: \"PreTrainedModel\", keep_in_fp32_modules: List[str] = [], **kwargs\n+        self, model: \"PreTrainedModel\", keep_in_fp32_modules: Optional[List[str]] = None, **kwargs\n     ):\n-        from ..integrations import get_keys_to_not_convert, replace_with_quanto_layers\n+        from ..integrations import replace_with_quanto_layers\n \n-        # We keep some modules such as the lm_head in their original dtype for numerical stability reasons\n-        if self.quantization_config.modules_to_not_convert is None:\n-            self.modules_to_not_convert = get_keys_to_not_convert(model)\n-        else:\n-            self.modules_to_not_convert = self.quantization_config.modules_to_not_convert\n-\n-        if not isinstance(self.modules_to_not_convert, list):\n-            self.modules_to_not_convert = [self.modules_to_not_convert]\n-\n-        self.modules_to_not_convert.extend(keep_in_fp32_modules)\n+        self.modules_to_not_convert = self.get_modules_to_not_convert(\n+            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+        )\n \n         model, _ = replace_with_quanto_layers(\n             model, modules_to_not_convert=self.modules_to_not_convert, quantization_config=self.quantization_config"
        },
        {
            "sha": "4cf1193edbf2cc676704db06fda7505c824548c0",
            "filename": "src/transformers/quantizers/quantizer_spqr.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py?ref=cc3a361b46664d4d8486b578e3cf4c32def3c2a4",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING, List, Optional\n \n from .base import HfQuantizer\n \n@@ -65,12 +65,17 @@ def update_torch_dtype(self, torch_dtype: \"torch.dtype\") -> \"torch.dtype\":\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n+        keep_in_fp32_modules: Optional[List[str]] = None,\n         **kwargs,\n     ):\n+        self.modules_to_not_convert = self.get_modules_to_not_convert(\n+            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+        )\n+\n         replace_with_spqr_linear(\n             model,\n             quantization_config=self.quantization_config,\n-            modules_to_not_convert=self.quantization_config.modules_to_not_convert,\n+            modules_to_not_convert=self.modules_to_not_convert,\n         )\n         model.config.quantization_config = self.quantization_config\n "
        },
        {
            "sha": "e233f0689aa68ccd708de9e8b23a16ed422b1c76",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=cc3a361b46664d4d8486b578e3cf4c32def3c2a4",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n import importlib\n import types\n-from typing import TYPE_CHECKING, Union\n+from typing import TYPE_CHECKING, Optional, Union\n \n from packaging import version\n \n@@ -144,14 +144,12 @@ def adjust_max_memory(self, max_memory: Dict[str, Union[int, str]]) -> Dict[str,\n         max_memory = {key: val * 0.9 for key, val in max_memory.items()}\n         return max_memory\n \n-    def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n-        from ..integrations import get_keys_to_not_convert\n-\n-        self.modules_to_not_convert = get_keys_to_not_convert(model)\n-\n-        if self.quantization_config.modules_to_not_convert is not None:\n-            self.modules_to_not_convert.extend(self.quantization_config.modules_to_not_convert)\n-\n+    def _process_model_before_weight_loading(\n+        self, model: \"PreTrainedModel\", keep_in_fp32_modules: Optional[List[str]] = None, **kwargs\n+    ):\n+        self.modules_to_not_convert = self.get_modules_to_not_convert(\n+            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+        )\n         return\n \n     def check_quantized_param("
        },
        {
            "sha": "85483357448f15df66dc77b4e0e9158970ab1764",
            "filename": "src/transformers/quantizers/quantizer_vptq.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py?ref=cc3a361b46664d4d8486b578e3cf4c32def3c2a4",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING, List, Optional\n \n from .base import HfQuantizer\n \n@@ -68,6 +68,7 @@ def update_torch_dtype(self, torch_dtype: \"torch.dtype\") -> \"torch.dtype\":\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n+        keep_in_fp32_modules: Optional[List[str]] = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -76,14 +77,14 @@ def _process_model_before_weight_loading(\n         \"\"\"\n         from ..integrations import replace_with_vptq_linear\n \n-        modules_to_not_convert = kwargs.get(\"modules_to_not_convert\", []) + (\n-            self.quantization_config.modules_to_not_convert or []\n+        self.modules_to_not_convert = self.get_modules_to_not_convert(\n+            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n         )\n \n         replace_with_vptq_linear(\n             model,\n             quantization_config=self.quantization_config,\n-            modules_to_not_convert=modules_to_not_convert,\n+            modules_to_not_convert=self.modules_to_not_convert,\n         )\n         model.config.quantization_config = self.quantization_config\n "
        },
        {
            "sha": "0988d8ac1474714bc03ad3e127bdb13e90c92e53",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc3a361b46664d4d8486b578e3cf4c32def3c2a4/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=cc3a361b46664d4d8486b578e3cf4c32def3c2a4",
            "patch": "@@ -1424,8 +1424,6 @@ def __init__(\n         tune_metadata: Optional[Dict[str, Any]] = None,\n         **kwargs,\n     ):\n-        if modules_to_not_convert is None:\n-            modules_to_not_convert = [\"lm_head\"]\n         if tune_metadata is None:\n             tune_metadata = {}\n         self.quant_method = QuantizationMethod.HIGGS\n@@ -1652,8 +1650,6 @@ def __init__(\n         self.bits = bits\n         self.beta1 = beta1\n         self.beta2 = beta2\n-        if modules_to_not_convert is None:\n-            modules_to_not_convert = []\n         self.modules_to_not_convert = modules_to_not_convert\n         self.post_init()\n \n@@ -1674,10 +1670,6 @@ def post_init(self):\n             raise ValueError(\"SpQR currently only supports beta1 = 16\")\n         if self.beta2 != 16:\n             raise ValueError(\"SpQR currently only supports beta2 = 16\")\n-\n-        if self.modules_to_not_convert is not None and not isinstance(self.modules_to_not_convert, list):\n-            raise ValueError(\"modules_to_not_convert must be a list of strings\")\n-\n         if not isinstance(self.shapes, dict):\n             raise TypeError(\"shapes must be a dict\")\n "
        }
    ],
    "stats": {
        "total": 179,
        "additions": 81,
        "deletions": 98
    }
}