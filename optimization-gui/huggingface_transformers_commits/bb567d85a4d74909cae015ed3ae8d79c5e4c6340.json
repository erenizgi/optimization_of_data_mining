{
    "author": "itazap",
    "message": "refactor can_save_slow_tokenizer (#37722)\n\n* refactor to rm property can_save_slow_tokenizer, it can be done within the if of save_vocab\n\n* move property to fast\n\n* revert if\n\n* check if vocab_file is attr\n\n* fix check for sp\n\n* fix if condition\n\n* fix if condition\n\n* fix if condition",
    "sha": "bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
    "files": [
        {
            "sha": "05712eeb6ebc3f88daa972194c3bfcac980a5629",
            "filename": "src/transformers/models/albert/tokenization_albert_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -130,10 +130,6 @@ def __init__(\n         self.keep_accents = keep_accents\n         self.vocab_file = vocab_file\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     def build_inputs_with_special_tokens(\n         self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n     ) -> List[int]:"
        },
        {
            "sha": "70c301eee91887d1963324ab995aad09e220e3e7",
            "filename": "src/transformers/models/barthez/tokenization_barthez_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -122,10 +122,6 @@ def __init__(\n \n         self.vocab_file = vocab_file\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     def build_inputs_with_special_tokens(\n         self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n     ) -> List[int]:"
        },
        {
            "sha": "18383a7ddb1802f245046fc504c54d0422b6e63e",
            "filename": "src/transformers/models/big_bird/tokenization_big_bird_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -119,10 +119,6 @@ def __init__(\n \n         self.vocab_file = vocab_file\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     def build_inputs_with_special_tokens(\n         self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n     ) -> List[int]:"
        },
        {
            "sha": "05d0073da6b377a1a37c76f175a2ec8b8695b277",
            "filename": "src/transformers/models/camembert/tokenization_camembert_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -125,10 +125,6 @@ def __init__(\n \n         self.vocab_file = vocab_file\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     def build_inputs_with_special_tokens(\n         self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n     ) -> List[int]:"
        },
        {
            "sha": "089c5c066e73da374f43e841352750bb3a44adca",
            "filename": "src/transformers/models/code_llama/tokenization_code_llama_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -168,10 +168,6 @@ def __init__(\n         self._eot_token = eot_token\n         self.fill_token = fill_token\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     # Copied from transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast.update_post_processor\n     def update_post_processor(self):\n         \"\"\""
        },
        {
            "sha": "48caf28c0a122c29be3ae7df13e476ecbabf0e79",
            "filename": "src/transformers/models/cpm/tokenization_cpm_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -144,10 +144,6 @@ def __init__(\n         self.jieba = jieba\n         self.translator = str.maketrans(\" \\n\", \"\\u2582\\u2583\")\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     # Copied from transformers.models.xlnet.tokenization_xlnet_fast.XLNetTokenizerFast.build_inputs_with_special_tokens\n     def build_inputs_with_special_tokens(\n         self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None"
        },
        {
            "sha": "096b4b239c6c0de2095610cb1e6b1de8e4a928df",
            "filename": "src/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -119,10 +119,6 @@ def __init__(\n         self.split_by_punct = split_by_punct\n         self.vocab_file = vocab_file\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and"
        },
        {
            "sha": "4b452554ea078cda8feba831c922689568397cf4",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/tokenization_xlm_prophetnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Ftokenization_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Ftokenization_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Ftokenization_xlm_prophetnet.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -169,10 +169,6 @@ def __init__(\n             **kwargs,\n         )\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     def __getstate__(self):\n         state = self.__dict__.copy()\n         state[\"sp_model\"] = None"
        },
        {
            "sha": "9550bcbb4ae1427eefdb5a5d23ad9b91bb5036c7",
            "filename": "src/transformers/models/fnet/tokenization_fnet_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -113,10 +113,6 @@ def __init__(\n         self.keep_accents = keep_accents\n         self.vocab_file = vocab_file\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     def build_inputs_with_special_tokens(\n         self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n     ) -> List[int]:"
        },
        {
            "sha": "bc6e0c8ba7c5d75c7a145d23f54309d438c04ea6",
            "filename": "src/transformers/models/gemma/tokenization_gemma_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -114,10 +114,6 @@ def __init__(\n         self.update_post_processor()\n         self.vocab_file = vocab_file\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     # Copied from transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast.update_post_processor\n     def update_post_processor(self):\n         \"\"\""
        },
        {
            "sha": "7cf8655cc67f053989d33c25fb6adf90fb63c37a",
            "filename": "src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -260,10 +260,6 @@ def __init__(\n         self.pad_token_label = pad_token_label\n         self.only_label_first_subword = only_label_first_subword\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     @add_end_docstrings(LAYOUTXLM_ENCODE_KWARGS_DOCSTRING)\n     def __call__(\n         self,"
        },
        {
            "sha": "a0fff2589ca57dc57ec5e07fb741b0ae6f14d67a",
            "filename": "src/transformers/models/llama/tokenization_llama_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -171,10 +171,6 @@ def __init__(\n         self.use_default_system_prompt = use_default_system_prompt\n         self.vocab_file = vocab_file\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     def update_post_processor(self):\n         \"\"\"\n         Updates the underlying post processor with the current `bos_token` and `eos_token`."
        },
        {
            "sha": "1996a9ecd2ec1e269afcf24b2f584732a565e384",
            "filename": "src/transformers/models/mbart/tokenization_mbart_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fmbart%2Ftokenization_mbart_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fmbart%2Ftokenization_mbart_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Ftokenization_mbart_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -123,10 +123,6 @@ def __init__(\n         self.tgt_lang = tgt_lang\n         self.set_src_lang_special_tokens(self._src_lang)\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     @property\n     def src_lang(self) -> str:\n         return self._src_lang"
        },
        {
            "sha": "01980a2a822802c618ed34523503b723c4e2b1ba",
            "filename": "src/transformers/models/mbart50/tokenization_mbart50_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fmbart50%2Ftokenization_mbart50_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fmbart50%2Ftokenization_mbart50_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart50%2Ftokenization_mbart50_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -137,10 +137,6 @@ def __init__(\n         self.cur_lang_code_id = self.lang_code_to_id[self._src_lang]\n         self.set_src_lang_special_tokens(self._src_lang)\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     @property\n     def src_lang(self) -> str:\n         return self._src_lang"
        },
        {
            "sha": "a5592dc96ed260724c6a580fc31ad22fb420f950",
            "filename": "src/transformers/models/nllb/tokenization_nllb_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fnllb%2Ftokenization_nllb_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fnllb%2Ftokenization_nllb_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb%2Ftokenization_nllb_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -166,10 +166,6 @@ def __init__(\n         self.tgt_lang = tgt_lang\n         self.set_src_lang_special_tokens(self._src_lang)\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     @property\n     def src_lang(self) -> str:\n         return self._src_lang"
        },
        {
            "sha": "657390ec770901b62ce4f02b0d639bed59a81f7d",
            "filename": "src/transformers/models/pegasus/tokenization_pegasus_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fpegasus%2Ftokenization_pegasus_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fpegasus%2Ftokenization_pegasus_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Ftokenization_pegasus_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -148,10 +148,6 @@ def __init__(\n         )\n         self.vocab_file = vocab_file\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     def _special_token_mask(self, seq):\n         all_special_ids = set(self.all_special_ids)  # call it once instead of inside list comp\n         all_special_ids.remove(self.unk_token_id)  # <unk> is only sometimes special"
        },
        {
            "sha": "f6d8c7c60050ddd45917cf104649502e79abfe6d",
            "filename": "src/transformers/models/reformer/tokenization_reformer_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Freformer%2Ftokenization_reformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Freformer%2Ftokenization_reformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Ftokenization_reformer_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -91,10 +91,6 @@ def __init__(\n \n         self.vocab_file = vocab_file\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         if not self.can_save_slow_tokenizer:\n             raise ValueError("
        },
        {
            "sha": "9c489773c2367dd0b627c5f2f2078a5846f87bb9",
            "filename": "src/transformers/models/rembert/tokenization_rembert_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -125,10 +125,6 @@ def __init__(\n         self.keep_accents = keep_accents\n         self.vocab_file = vocab_file\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     def build_inputs_with_special_tokens(\n         self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n     ) -> List[int]:"
        },
        {
            "sha": "a3ab631560a9e2e2f7aa0cee77d78e543ed20c48",
            "filename": "src/transformers/models/seamless_m4t/tokenization_seamless_m4t_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -151,10 +151,6 @@ def __init__(\n         self.set_src_lang_special_tokens(self._src_lang)\n         self.set_tgt_lang_special_tokens(self._tgt_lang)\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     @property\n     # Copied from transformers.models.nllb.tokenization_nllb.NllbTokenizer.src_lang\n     def src_lang(self) -> str:"
        },
        {
            "sha": "e4b3fe570958b054e855b0dfaed6ce91af12b65a",
            "filename": "src/transformers/models/t5/tokenization_t5_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -131,10 +131,6 @@ def __init__(\n         self.vocab_file = vocab_file\n         self._extra_ids = extra_ids\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     @staticmethod\n     def _eventually_correct_t5_max_length(pretrained_model_name_or_path, max_model_length, init_max_model_length):\n         if pretrained_model_name_or_path in T5TokenizerFast.max_model_input_sizes:"
        },
        {
            "sha": "941bb8d6f730a5fe7ff9f62309f72db96bba901d",
            "filename": "src/transformers/models/udop/tokenization_udop_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -234,10 +234,6 @@ def __init__(\n         self.pad_token_label = pad_token_label\n         self.only_label_first_subword = only_label_first_subword\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     @add_end_docstrings(UDOP_ENCODE_KWARGS_DOCSTRING)\n     def __call__(\n         self,"
        },
        {
            "sha": "77b9a8bb8eaa9860ba35749d04be9a584de2d797",
            "filename": "src/transformers/models/xglm/tokenization_xglm_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fxglm%2Ftokenization_xglm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fxglm%2Ftokenization_xglm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Ftokenization_xglm_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -120,10 +120,6 @@ def __init__(\n \n         self.vocab_file = vocab_file\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     def build_inputs_with_special_tokens(\n         self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n     ) -> List[int]:"
        },
        {
            "sha": "a7bee70f1d6d544a2439fe485f79c65b08f42fba",
            "filename": "src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Ftokenization_xlm_roberta_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Ftokenization_xlm_roberta_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Ftokenization_xlm_roberta_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -120,10 +120,6 @@ def __init__(\n \n         self.vocab_file = vocab_file\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     def build_inputs_with_special_tokens(\n         self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n     ) -> List[int]:"
        },
        {
            "sha": "054ba0c30549c36a6035932d8798240ac00a05eb",
            "filename": "src/transformers/models/xlnet/tokenization_xlnet_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -152,10 +152,6 @@ def __init__(\n         self.keep_accents = keep_accents\n         self.vocab_file = vocab_file\n \n-    @property\n-    def can_save_slow_tokenizer(self) -> bool:\n-        return os.path.isfile(self.vocab_file) if self.vocab_file else False\n-\n     def build_inputs_with_special_tokens(\n         self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n     ) -> List[int]:"
        },
        {
            "sha": "3636a1e67c15c065e1475b772900df1d3e63be4d",
            "filename": "src/transformers/tokenization_utils_fast.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb567d85a4d74909cae015ed3ae8d79c5e4c6340/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_fast.py?ref=bb567d85a4d74909cae015ed3ae8d79c5e4c6340",
            "patch": "@@ -226,10 +226,16 @@ def is_fast(self) -> bool:\n     @property\n     def can_save_slow_tokenizer(self) -> bool:\n         \"\"\"\n-        `bool`: Whether or not the slow tokenizer can be saved. Usually for sentencepiece based slow tokenizer, this\n+        `bool`: Whether or not the slow tokenizer can be saved. For a sentencepiece based slow tokenizer, this\n         can only be `True` if the original `\"sentencepiece.model\"` was not deleted.\n         \"\"\"\n-        return True\n+        if \"vocab_file\" in self.vocab_files_names and self.vocab_files_names[\"vocab_file\"].endswith(\".model\"):\n+            if hasattr(self, \"vocab_file\") and self.vocab_file:\n+                # If the vocab file is a sentencepiece model, we can save it\n+                return os.path.isfile(self.vocab_file)\n+            return False\n+        else:\n+            return True\n \n     @property\n     def vocab_size(self) -> int:"
        }
    ],
    "stats": {
        "total": 106,
        "additions": 8,
        "deletions": 98
    }
}