{
    "author": "McPatate",
    "message": "fix(ci): unexpected keyword argument `streaming` (#42102)\n\n* debug(ci): run `pwd` to check what we're working with\n\n* fix(ci): `ls -lR`\n\n* fix(ci): remove working directory which should not be there?\n\n* fix(cb): make sure memory is freed when calling `stop`\n\n* fix(ci): effectively clear cache\n\n* fix(ci): reduce memory safety margin\n\n* refactor(cb): add fixme note on default safety margin value",
    "sha": "ba938fa59049e365714c8dad0d12b7dbe59646b8",
    "files": [
        {
            "sha": "9a4e4fc3504e178fd14d8086931805c10a374f90",
            "filename": ".github/workflows/benchmark.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba938fa59049e365714c8dad0d12b7dbe59646b8/.github%2Fworkflows%2Fbenchmark.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba938fa59049e365714c8dad0d12b7dbe59646b8/.github%2Fworkflows%2Fbenchmark.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fbenchmark.yml?ref=ba938fa59049e365714c8dad0d12b7dbe59646b8",
            "patch": "@@ -40,7 +40,6 @@ jobs:\n         run: python3 -m pip install -r benchmark_v2/requirements.txt kernels\r\n \r\n       - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\r\n-        working-directory: /transformers\r\n         run: python3 -m pip uninstall -y transformers && python3 -m pip install -e \".[torch]\"\r\n \r\n       - name: Run benchmark\r"
        },
        {
            "sha": "69fa2b51b576892849edbd0fd93d482f81ce3198",
            "filename": "benchmark_v2/framework/benchmark_runner.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba938fa59049e365714c8dad0d12b7dbe59646b8/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba938fa59049e365714c8dad0d12b7dbe59646b8/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_runner.py?ref=ba938fa59049e365714c8dad0d12b7dbe59646b8",
            "patch": "@@ -117,8 +117,6 @@ def flush_memory():\n     # Clear CUDA cache\n     if torch.cuda.is_available():\n         torch.cuda.empty_cache()\n-        torch.cuda.reset_max_memory_allocated()\n-        torch.cuda.reset_peak_memory_stats()\n         torch.cuda.synchronize()\n     gc.collect()\n "
        },
        {
            "sha": "780da4ce9b1512d94469d4ce225344228825da1a",
            "filename": "src/transformers/generation/continuous_batching/cache.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba938fa59049e365714c8dad0d12b7dbe59646b8/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba938fa59049e365714c8dad0d12b7dbe59646b8/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py?ref=ba938fa59049e365714c8dad0d12b7dbe59646b8",
            "patch": "@@ -189,7 +189,9 @@ def __init__(\n         num_blocks, max_batch_tokens = memory_handler.infer_num_blocks_and_max_batch_tokens(\n             num_blocks=getattr(generation_config, \"num_blocks\", None),\n             max_batch_tokens=getattr(generation_config, \"max_batch_tokens\", None),\n-            max_memory_percent=getattr(generation_config, \"max_memory\", 0.9),\n+            max_memory_percent=getattr(\n+                generation_config, \"max_memory\", 0.8\n+            ),  # FIXME: it seems we overcommit memory, was changed from 0.9 which caused OOMs in our benchmarking CI\n             cache_dtype=self.dtype,\n         )\n \n@@ -414,7 +416,7 @@ def infer_num_blocks_and_max_batch_tokens(\n         self,\n         num_blocks: Optional[int] = None,\n         max_batch_tokens: Optional[int] = None,\n-        max_memory_percent: float = 0.9,\n+        max_memory_percent: float = 0.8,  # FIXME: it seems we overcommit memory, was changed from 0.9 which caused OOMs in our benchmarking CI\n         cache_dtype: torch.dtype = torch.float16,\n     ) -> tuple[int, int]:\n         \"\"\"Determine optimal number of blocks and maximum number of tokens per batch based on available memory and\n@@ -454,7 +456,7 @@ def infer_num_blocks_and_max_batch_tokens(\n \n     def compute_num_blocks_and_max_batch_tokens(\n         self,\n-        max_memory_percent: float = 0.9,\n+        max_memory_percent: float,\n         cache_dtype: torch.dtype = torch.float16,\n         m: float = 0.01,\n     ) -> tuple[int, int]:\n@@ -503,7 +505,7 @@ def compute_num_blocks_and_max_batch_tokens(\n     def compute_max_batch_tokens(\n         self,\n         num_blocks: int,\n-        max_memory_percent: float = 0.9,\n+        max_memory_percent: float,\n         cache_dtype: torch.dtype = torch.float16,\n     ) -> int:\n         \"\"\"Calculate maximum batch tokens M given a fixed number of cache blocks. The formula for M is given by:\n@@ -531,7 +533,7 @@ def compute_max_batch_tokens(\n     def compute_num_blocks(\n         self,\n         max_batch_tokens: int,\n-        max_memory_percent: float = 0.9,\n+        max_memory_percent: float,\n         cache_dtype: torch.dtype = torch.float16,\n     ) -> int:\n         \"\"\"Calculate number of cache blocks N given a fixed maximum token per token M. The formula for N is given by:"
        },
        {
            "sha": "407a66f775d73cf084adead8e9cff13350c6fdd8",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba938fa59049e365714c8dad0d12b7dbe59646b8/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba938fa59049e365714c8dad0d12b7dbe59646b8/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=ba938fa59049e365714c8dad0d12b7dbe59646b8",
            "patch": "@@ -826,6 +826,8 @@ def stop(self, block: bool = True, timeout: Optional[float] = None) -> None:\n         if block:\n             self.join(stop_trigger_time, timeout)\n \n+        self.batch_processor = None\n+\n     def join(self, stop_trigger_time: float, timeout: Optional[float] = None) -> None:\n         \"\"\"Wait for the background thread to finish.\n "
        }
    ],
    "stats": {
        "total": 17,
        "additions": 9,
        "deletions": 8
    }
}