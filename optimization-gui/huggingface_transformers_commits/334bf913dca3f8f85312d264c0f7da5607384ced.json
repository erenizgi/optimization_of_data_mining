{
    "author": "Flink-ddd",
    "message": "Fix(informer): Correct tensor shape for input_size=1 (#38856)\n\n* Fix(time_series): Correct scaler tensor shape in base model\n\nThe create_network_inputs function in TimeSeriesTransformerModel\nhandled the scaler's loc and scale tensors inconsistently.\nWhen input_size=1, the tensors were not squeezed, leading to\ndownstream dimension errors for models like Informer.\n\nThis commit refactors the logic to unconditionally apply .squeeze(1),\nwhich correctly handles all input_size cases and fixes the bug at its source.\n\nFixes #38745\n\n* Fix(time_series): Correct scaler tensor shape in base model\n\nThe create_network_inputs function in TimeSeriesTransformerModel\nhandled the scaler's loc and scale tensors inconsistently.\nWhen input_size=1, the tensors were not squeezed, leading to\ndownstream dimension errors for models like Informer.\n\nThis commit refactors the logic to unconditionally apply .squeeze(1),\nwhich correctly handles all input_size cases and fixes the bug at its source.\n\nFixes #38745\n\n---------\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>",
    "sha": "334bf913dca3f8f85312d264c0f7da5607384ced",
    "files": [
        {
            "sha": "98859e7534cb53c015e8e3d7ae93ef6e6b1e55b0",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/334bf913dca3f8f85312d264c0f7da5607384ced/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/334bf913dca3f8f85312d264c0f7da5607384ced/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=334bf913dca3f8f85312d264c0f7da5607384ced",
            "patch": "@@ -1461,8 +1461,14 @@ def create_network_inputs(\n         )\n \n         # static features\n-        log_abs_loc = loc.abs().log1p() if self.config.input_size == 1 else loc.squeeze(1).abs().log1p()\n-        log_scale = scale.log() if self.config.input_size == 1 else scale.squeeze(1).log()\n+        if loc.ndim == 3:\n+            squeezed_loc = loc.squeeze(1)\n+            squeezed_scale = scale.squeeze(1)\n+        else:\n+            squeezed_loc = loc\n+            squeezed_scale = scale\n+        log_abs_loc = squeezed_loc.abs().log1p()\n+        log_scale = squeezed_scale.log()\n         static_feat = torch.cat((log_abs_loc, log_scale), dim=1)\n \n         if static_real_features is not None:"
        },
        {
            "sha": "8672900e76a5e0f343b9f74a4f37e664d50fa94b",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/334bf913dca3f8f85312d264c0f7da5607384ced/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/334bf913dca3f8f85312d264c0f7da5607384ced/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=334bf913dca3f8f85312d264c0f7da5607384ced",
            "patch": "@@ -1228,8 +1228,14 @@ def create_network_inputs(\n         )\n \n         # static features\n-        log_abs_loc = loc.abs().log1p() if self.config.input_size == 1 else loc.squeeze(1).abs().log1p()\n-        log_scale = scale.log() if self.config.input_size == 1 else scale.squeeze(1).log()\n+        if loc.ndim == 3:\n+            squeezed_loc = loc.squeeze(1)\n+            squeezed_scale = scale.squeeze(1)\n+        else:\n+            squeezed_loc = loc\n+            squeezed_scale = scale\n+        log_abs_loc = squeezed_loc.abs().log1p()\n+        log_scale = squeezed_scale.log()\n         static_feat = torch.cat((log_abs_loc, log_scale), dim=1)\n \n         if static_real_features is not None:"
        }
    ],
    "stats": {
        "total": 20,
        "additions": 16,
        "deletions": 4
    }
}