{
    "author": "muellerzr",
    "message": "Add more rigerous non-slow grad accum tests (#35668)\n\n* Add more rigerous non-slow grad accum tests\r\n\r\n* Further nits\r\n\r\n* Re-add space\r\n\r\n* Readbility\r\n\r\n* Use tinystories instead\r\n\r\n* Revert transformer diff\r\n\r\n* tweak threshs",
    "sha": "1fae54c7216e144b426e753400abdc1299d4fc74",
    "files": [
        {
            "sha": "a8b8e8af1165ecd33f3d9f54997f11fc7936812f",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 21,
            "deletions": 22,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fae54c7216e144b426e753400abdc1299d4fc74/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fae54c7216e144b426e753400abdc1299d4fc74/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=1fae54c7216e144b426e753400abdc1299d4fc74",
            "patch": "@@ -793,35 +793,34 @@ def test_model_init(self):\n             trainer.train()\n             self.check_trained_model(trainer.model, alternate_seed=True)\n \n-    @slow\n     def test_gradient_accumulation_loss_alignment_with_model_loss(self):\n         set_seed(42)\n         import datasets\n \n-        model_name = \"nickypro/tinyllama-110M\"\n+        model_name = \"nickypro/tinyllama-15M\"\n         dataset_name = \"wikitext\"\n         dataset_config = \"wikitext-2-raw-v1\"\n-        dataset = datasets.load_dataset(dataset_name, dataset_config, split=\"train[:500]\")\n-        dataset = dataset.train_test_split(test_size=0.2)\n+        dataset = datasets.load_dataset(dataset_name, dataset_config, split=\"train[:40]\")\n         tokenizer = AutoTokenizer.from_pretrained(model_name)\n \n         tokenizer.pad_token = tokenizer.eos_token\n \n         def tokenize_function(examples):\n-            return tokenizer(examples[\"text\"], max_length=128, padding=\"max_length\", truncation=True)\n+            return tokenizer(examples[\"text\"], max_length=16, padding=\"max_length\", truncation=True)\n \n-        tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n+        tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n \n         data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n \n         model = AutoModelForCausalLM.from_pretrained(model_name)\n+        state_dict = model.state_dict()\n \n         base_loss_callback = StoreLossCallback()\n \n         args_kwargs = {\n             \"report_to\": \"none\",\n             \"logging_steps\": 1,\n-            \"max_steps\": 20,\n+            \"max_steps\": 5,\n             \"learning_rate\": 3e-4,\n             \"disable_tqdm\": True,\n         }\n@@ -834,7 +833,7 @@ def tokenize_function(examples):\n             trainer = Trainer(\n                 model,\n                 args,\n-                train_dataset=tokenized_dataset[\"train\"],\n+                train_dataset=tokenized_dataset,\n                 callbacks=[base_loss_callback],\n                 data_collator=data_collator,\n             )\n@@ -854,19 +853,19 @@ def tokenize_function(examples):\n             trainer = Trainer(\n                 model,\n                 args,\n-                train_dataset=tokenized_dataset[\"train\"],\n+                train_dataset=tokenized_dataset,\n                 callbacks=[grad_accum_loss_callback],\n                 data_collator=data_collator,\n             )\n             trainer.train()\n \n             set_seed(42)\n-            model = AutoModelForCausalLM.from_pretrained(model_name)\n+            model.load_state_dict(state_dict)\n             broken_loss_callback = StoreLossCallback()\n             trainer = Trainer(\n                 model,\n                 args,\n-                train_dataset=tokenized_dataset[\"train\"],\n+                train_dataset=tokenized_dataset,\n                 callbacks=[broken_loss_callback],\n                 data_collator=data_collator,\n             )\n@@ -886,31 +885,31 @@ def tokenize_function(examples):\n             self.assertLess(max(diff_truth), 0.01, f\"Difference {max(diff_truth)} is not within 0.01\")\n \n             # max diff broken should be very off\n-            self.assertGreater(max(diff_broken), 2, f\"Difference {max(diff_broken)} is not greater than 2\")\n+            self.assertGreater(max(diff_broken), 1.5, f\"Difference {max(diff_broken)} is not greater than 2\")\n \n             loss_base = sum(base_loss_callback.losses)\n             loss_broken = sum(broken_loss_callback.losses)\n \n             # mean/sum loss should not vary too much.\n             relative_diff = abs(loss_base - loss_broken) / max(loss_base, loss_broken)\n-            self.assertLess(relative_diff, 0.1, f\"Relative difference {relative_diff} is not within 0.1\")\n+            self.assertLess(relative_diff, 0.2, f\"Relative difference {relative_diff} is not within 0.2\")\n \n-    @slow\n     def test_gradient_accumulation_loss_alignment_with_loss_func(self):\n         set_seed(42)\n         import datasets\n \n         model_name = \"roneneldan/TinyStories-33M\"\n         dataset_name = \"wikitext\"\n         dataset_config = \"wikitext-2-raw-v1\"\n-        dataset = datasets.load_dataset(dataset_name, dataset_config, split=\"train[:500]\")\n-        dataset = dataset.train_test_split(test_size=0.2)\n+        dataset = datasets.load_dataset(dataset_name, dataset_config, split=\"train[:40]\")\n         tokenizer = AutoTokenizer.from_pretrained(model_name)\n \n+        tokenizer.pad_token = tokenizer.eos_token\n+\n         def tokenize_function(examples):\n-            return tokenizer(examples[\"text\"])\n+            return tokenizer(examples[\"text\"], max_length=16, padding=\"max_length\", truncation=True)\n \n-        tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n+        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n \n         tokenizer.pad_token = tokenizer.eos_token\n         data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n@@ -929,7 +928,7 @@ def compute_loss(logits, labels, vocab_size, num_items_in_batch, disable_num_ite\n         args_kwargs = {\n             \"report_to\": \"none\",\n             \"logging_steps\": 1,\n-            \"max_steps\": 20,\n+            \"max_steps\": 5,\n             \"learning_rate\": 3e-4,\n             \"disable_tqdm\": True,\n         }\n@@ -942,7 +941,7 @@ def compute_loss(logits, labels, vocab_size, num_items_in_batch, disable_num_ite\n             trainer = Trainer(\n                 model,\n                 args,\n-                train_dataset=tokenized_dataset[\"train\"],\n+                train_dataset=tokenized_dataset,\n                 callbacks=[base_loss_callback],\n                 compute_loss_func=loss_fn,\n                 data_collator=data_collator,\n@@ -962,7 +961,7 @@ def compute_loss(logits, labels, vocab_size, num_items_in_batch, disable_num_ite\n             trainer = Trainer(\n                 model,\n                 args,\n-                train_dataset=tokenized_dataset[\"train\"],\n+                train_dataset=tokenized_dataset,\n                 callbacks=[grad_accum_loss_callback],\n                 compute_loss_func=loss_fn,\n                 data_collator=data_collator,\n@@ -976,7 +975,7 @@ def compute_loss(logits, labels, vocab_size, num_items_in_batch, disable_num_ite\n             trainer = Trainer(\n                 model,\n                 args,\n-                train_dataset=tokenized_dataset[\"train\"],\n+                train_dataset=tokenized_dataset,\n                 callbacks=[broken_loss_callback],\n                 compute_loss_func=loss_fn,\n                 data_collator=data_collator,"
        }
    ],
    "stats": {
        "total": 43,
        "additions": 21,
        "deletions": 22
    }
}