{
    "author": "yonigozlan",
    "message": "Allow fallback to loading from Auto\"SubProcessor\".from_pretrained when model_type can't be inferred from config (#42402)\n\n* fix raise error early\n\n* add back feature extractor saving logic",
    "sha": "675e8763cafd74894ce4699d7583bd387beac14e",
    "files": [
        {
            "sha": "6d08bf37ebab66bcdc4d34c162d642f61038f2d8",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 17,
            "deletions": 10,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/675e8763cafd74894ce4699d7583bd387beac14e/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/675e8763cafd74894ce4699d7583bd387beac14e/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=675e8763cafd74894ce4699d7583bd387beac14e",
            "patch": "@@ -317,7 +317,6 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n                     processor_class = config_dict.get(\"processor_class\", None)\n                     if \"AutoProcessor\" in config_dict.get(\"auto_map\", {}):\n                         processor_auto_map = config_dict[\"auto_map\"][\"AutoProcessor\"]\n-\n             # Saved as feature extractor\n             if preprocessor_config_file is None:\n                 preprocessor_config_file = cached_file(\n@@ -345,16 +344,24 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n                     processor_auto_map = config_dict[\"auto_map\"][\"AutoProcessor\"]\n \n         if processor_class is None:\n-            # Otherwise, load config, if it can be loaded.\n-            if not isinstance(config, PreTrainedConfig):\n-                config = AutoConfig.from_pretrained(\n-                    pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n-                )\n+            # Last resort: try loading the model config to get processor_class.\n+            # This handles cases where processor info is only in config.json (not in any\n+            # preprocessor/tokenizer config files). AutoConfig.from_pretrained may raise\n+            # ValueError if the model_type is unrecognized or the config is invalid -\n+            # we catch and ignore this to allow fallback to AutoTokenizer/AutoImageProcessor.\n+            try:\n+                if not isinstance(config, PreTrainedConfig):\n+                    config = AutoConfig.from_pretrained(\n+                        pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n+                    )\n \n-            # And check if the config contains the processor class.\n-            processor_class = getattr(config, \"processor_class\", None)\n-            if hasattr(config, \"auto_map\") and \"AutoProcessor\" in config.auto_map:\n-                processor_auto_map = config.auto_map[\"AutoProcessor\"]\n+                processor_class = getattr(config, \"processor_class\", None)\n+                if hasattr(config, \"auto_map\") and \"AutoProcessor\" in config.auto_map:\n+                    processor_auto_map = config.auto_map[\"AutoProcessor\"]\n+            except ValueError:\n+                # Config loading failed (unrecognized model_type, invalid config, etc.)\n+                # Continue to fallback logic below (AutoTokenizer, AutoImageProcessor, etc.)\n+                pass\n \n         if processor_class is not None:\n             processor_class = processor_class_from_name(processor_class)"
        }
    ],
    "stats": {
        "total": 27,
        "additions": 17,
        "deletions": 10
    }
}