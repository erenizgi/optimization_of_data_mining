{
    "author": "zucchini-nlp",
    "message": "Ignore keys on `validate_rope` (#33753)\n\n* ignore keys on check rope\r\n\r\n* add tests\r\n\r\n* fix tests, so maybe better leave at logger lvl",
    "sha": "061c2c4c387f05350aa63b284f7910d69588c79f",
    "files": [
        {
            "sha": "150c402c5e13717150d0987af16d503f6291f4a3",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 25,
            "deletions": 15,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/061c2c4c387f05350aa63b284f7910d69588c79f/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/061c2c4c387f05350aa63b284f7910d69588c79f/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=061c2c4c387f05350aa63b284f7910d69588c79f",
            "patch": "@@ -360,13 +360,23 @@ def _compute_llama3_parameters(\n }\n \n \n-def _check_received_keys(rope_type: str, received_keys: set, required_keys: set, optional_keys: Optional[set] = None):\n+def _check_received_keys(\n+    rope_type: str,\n+    received_keys: set,\n+    required_keys: set,\n+    optional_keys: Optional[set] = None,\n+    ignore_keys: Optional[set] = None,\n+):\n     \"\"\"Compare the received keys in `config.rope_scaling` against the expected and optional keys\"\"\"\n     # BC: \"rope_type\" was originally \"type\" -- let's check for \"rope_type\" when \"type\" is present\n     if \"type\" in received_keys:\n         received_keys -= {\"type\"}\n         required_keys.add(\"rope_type\")\n \n+    # Some models need to store model-specific keys, and we don't want to throw warning at them\n+    if ignore_keys is not None:\n+        received_keys -= ignore_keys\n+\n     missing_keys = required_keys - received_keys\n     if missing_keys:\n         raise KeyError(f\"Missing required keys in `rope_scaling` for 'rope_type'='{rope_type}': {missing_keys}\")\n@@ -379,47 +389,47 @@ def _check_received_keys(rope_type: str, received_keys: set, required_keys: set,\n         logger.warning(f\"Unrecognized keys in `rope_scaling` for 'rope_type'='{rope_type}': {unused_keys}\")\n \n \n-def _validate_default_rope_parameters(config: PretrainedConfig):\n+def _validate_default_rope_parameters(config: PretrainedConfig, ignore_keys: Optional[set] = None):\n     rope_scaling = config.rope_scaling\n     rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n     required_keys = {\"rope_type\"}\n     received_keys = set(rope_scaling.keys())\n-    _check_received_keys(rope_type, received_keys, required_keys)\n+    _check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)\n \n \n-def _validate_linear_scaling_rope_parameters(config: PretrainedConfig):\n+def _validate_linear_scaling_rope_parameters(config: PretrainedConfig, ignore_keys: Optional[set] = None):\n     rope_scaling = config.rope_scaling\n     rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n     required_keys = {\"rope_type\", \"factor\"}\n     received_keys = set(rope_scaling.keys())\n-    _check_received_keys(rope_type, received_keys, required_keys)\n+    _check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)\n \n     factor = rope_scaling[\"factor\"]\n     if factor is None or not isinstance(factor, float) or factor < 1.0:\n         logger.warning(f\"`rope_scaling`'s factor field must be a float >= 1, got {factor}\")\n \n \n-def _validate_dynamic_scaling_rope_parameters(config: PretrainedConfig):\n+def _validate_dynamic_scaling_rope_parameters(config: PretrainedConfig, ignore_keys: Optional[set] = None):\n     rope_scaling = config.rope_scaling\n     rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n     required_keys = {\"rope_type\", \"factor\"}\n     # TODO (joao): update logic for the inclusion of `original_max_position_embeddings`\n     optional_keys = {\"original_max_position_embeddings\"}\n     received_keys = set(rope_scaling.keys())\n-    _check_received_keys(rope_type, received_keys, required_keys, optional_keys)\n+    _check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n \n     factor = rope_scaling[\"factor\"]\n     if factor is None or not isinstance(factor, float) or factor < 1.0:\n         logger.warning(f\"`rope_scaling`'s factor field must be a float >= 1, got {factor}\")\n \n \n-def _validate_yarn_parameters(config: PretrainedConfig):\n+def _validate_yarn_parameters(config: PretrainedConfig, ignore_keys: Optional[set] = None):\n     rope_scaling = config.rope_scaling\n     rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n     required_keys = {\"rope_type\", \"factor\"}\n     optional_keys = {\"attention_factor\", \"beta_fast\", \"beta_slow\"}\n     received_keys = set(rope_scaling.keys())\n-    _check_received_keys(rope_type, received_keys, required_keys, optional_keys)\n+    _check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n \n     factor = rope_scaling[\"factor\"]\n     if factor is None or not isinstance(factor, float) or factor < 1.0:\n@@ -444,14 +454,14 @@ def _validate_yarn_parameters(config: PretrainedConfig):\n         )\n \n \n-def _validate_longrope_parameters(config: PretrainedConfig):\n+def _validate_longrope_parameters(config: PretrainedConfig, ignore_keys: Optional[set] = None):\n     rope_scaling = config.rope_scaling\n     rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n     required_keys = {\"rope_type\", \"short_factor\", \"long_factor\"}\n     # TODO (joao): update logic for the inclusion of `original_max_position_embeddings`\n     optional_keys = {\"attention_factor\", \"factor\", \"original_max_position_embeddings\"}\n     received_keys = set(rope_scaling.keys())\n-    _check_received_keys(rope_type, received_keys, required_keys, optional_keys)\n+    _check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n \n     partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n     head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n@@ -494,12 +504,12 @@ def _validate_longrope_parameters(config: PretrainedConfig):\n                 )\n \n \n-def _validate_llama3_parameters(config: PretrainedConfig):\n+def _validate_llama3_parameters(config: PretrainedConfig, ignore_keys: Optional[set] = None):\n     rope_scaling = config.rope_scaling\n     rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n     required_keys = {\"rope_type\", \"factor\", \"original_max_position_embeddings\", \"low_freq_factor\", \"high_freq_factor\"}\n     received_keys = set(rope_scaling.keys())\n-    _check_received_keys(rope_type, received_keys, required_keys)\n+    _check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)\n \n     factor = rope_scaling[\"factor\"]\n     if factor is None or not isinstance(factor, float) or factor < 1.0:\n@@ -541,7 +551,7 @@ def _validate_llama3_parameters(config: PretrainedConfig):\n }\n \n \n-def rope_config_validation(config: PretrainedConfig):\n+def rope_config_validation(config: PretrainedConfig, ignore_keys: Optional[set] = None):\n     \"\"\"\n     Validate the RoPE config arguments, given a `PretrainedConfig` object\n     \"\"\"\n@@ -553,7 +563,7 @@ def rope_config_validation(config: PretrainedConfig):\n     rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", \"default\"))\n     validation_fn = ROPE_VALIDATION_FUNCTIONS.get(rope_type)\n     if validation_fn is not None:\n-        validation_fn(config)\n+        validation_fn(config, ignore_keys=ignore_keys)\n     else:\n         logger.warning(\n             f\"Missing validation function mapping in `ROPE_VALIDATION_FUNCTIONS` for 'rope_type'='{rope_type}'\""
        },
        {
            "sha": "1349006e768cd49f15cb4991123bbe87b053d681",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/061c2c4c387f05350aa63b284f7910d69588c79f/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/061c2c4c387f05350aa63b284f7910d69588c79f/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=061c2c4c387f05350aa63b284f7910d69588c79f",
            "patch": "@@ -235,11 +235,13 @@ def __init__(\n \n         # Validate the correctness of rotary position embeddings parameters\n         # BC: if there is a 'type' field, move it to 'rope_type'.\n-        # and change type from 'mrope' to 'default'\n+        # and change type from 'mrope' to 'default' because `mrope` does defeault RoPE calculations\n+        # one can set it to \"linear\"/\"dynamic\" etc. to have scaled RoPE\n+        # TODO: @raushan update config in the hub\n         if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n             if self.rope_scaling[\"type\"] == \"mrope\":\n                 self.rope_scaling[\"type\"] = \"default\"\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n-        rope_config_validation(self)\n+        rope_config_validation(self, ignore_keys={\"mrope_section\"})\n \n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)"
        },
        {
            "sha": "d51f534055872ae526e3b721e5164b70392573a3",
            "filename": "tests/utils/test_modeling_rope_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/061c2c4c387f05350aa63b284f7910d69588c79f/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/061c2c4c387f05350aa63b284f7910d69588c79f/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_rope_utils.py?ref=061c2c4c387f05350aa63b284f7910d69588c79f",
            "patch": "@@ -65,6 +65,19 @@ def test_rope_validation(self):\n                     with self.assertRaises(KeyError):\n                         rope_config_validation(config)\n \n+        # Any other parameters passed to RoPE will raise a warning that a particular key is not used\n+        # But sometimes we can have model-specific RoPE kwargs and bypass warning with `ignore_keys`\n+        model_specific_kwarg = \"mrope_sections\"  # e,g in Qwen2-VL\n+\n+        for rope_type in all_rope_types:\n+            if rope_type == \"default\":\n+                config.rope_scaling = {\"rope_type\": rope_type, model_specific_kwarg: True}\n+                rope_config_validation(config, ignore_keys={model_specific_kwarg})\n+                with self.assertLogs(\"transformers.modeling_rope_utils\", level=\"WARNING\") as logs:\n+                    rope_config_validation(config)\n+                    self.assertEqual(len(logs.output), 1)\n+                    self.assertIn(model_specific_kwarg, logs.output[0])\n+\n     def test_default_rope_function_bc(self):\n         config = LlamaConfig()\n         device = torch_device"
        }
    ],
    "stats": {
        "total": 59,
        "additions": 42,
        "deletions": 17
    }
}