{
    "author": "LiheYoung",
    "message": "Add Pixio pre-trained models (#42795)\n\n* Add Pixo\n\n* Add Pixo\n\n* Add test\n\n* Add model_doc\n\n* Add model_doc\n\n* modularize\n\n* modularize more\n\n* Add Pixo\n\n* Add Pixo\n\n* Add test\n\n* Add model_doc\n\n* Add model_doc\n\n* Use modular for Pixo\n\n* missing backbone autodoc\n\n* cleanup\n\n* cleanup\n\n* Revise converting\n\n* rename\n\n* rename\n\n* cleanup\n\n* small test update\n\n* address core review comments\n\n* also docs\n\n* fix\n\n* better with the toctree :eyes:\n\n---------\n\nCo-authored-by: Pablo Montalvo <pablo.montalvo.leroux@gmail.com>\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>",
    "sha": "a05e0e27ca1277a062344b93d4b6d2481ad1f601",
    "files": [
        {
            "sha": "efb46c3410a654ce82321bcb98d521122e283645",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a05e0e27ca1277a062344b93d4b6d2481ad1f601/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/a05e0e27ca1277a062344b93d4b6d2481ad1f601/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=a05e0e27ca1277a062344b93d4b6d2481ad1f601",
            "patch": "@@ -819,6 +819,8 @@\n         title: MobileViT\n       - local: model_doc/mobilevitv2\n         title: MobileViTV2\n+      - local: model_doc/pixio\n+        title: Pixio\n       - local: model_doc/poolformer\n         title: PoolFormer\n       - local: model_doc/prompt_depth_anything"
        },
        {
            "sha": "9a02d494058ee1c147d3c310b76a8a3fc8eb0615",
            "filename": "docs/source/en/model_doc/pixio.md",
            "status": "added",
            "additions": 121,
            "deletions": 0,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/a05e0e27ca1277a062344b93d4b6d2481ad1f601/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixio.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a05e0e27ca1277a062344b93d4b6d2481ad1f601/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixio.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixio.md?ref=a05e0e27ca1277a062344b93d4b6d2481ad1f601",
            "patch": "@@ -0,0 +1,121 @@\n+<!--Copyright 2025 Meta AI and The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+-->\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-12-16.*\n+*This model is to be announced*\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n+# Pixio\n+\n+[Pixio]() is a vision foundation model that uses [ViT](./vit) as a feature extractor for multiple downstream tasks like depth estimation, semantic segmentation, feed-forward 3D reconstruction, robotics, and image classification. It is built on the Masked Autoencoder (MAE) pre-training framework, with four minimal yet critical updates: 1) deeper decoder, 2) larger masking granularity, 3) more class tokens, and 4) web-scale curated training data.\n+\n+You can find all the original Pixio checkpoints under the [Pixio]() collection.\n+\n+The example below demonstrates how to obtain an image embedding with the [`AutoModel`] class.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import requests\n+from transformers import AutoImageProcessor, AutoModel\n+from PIL import Image\n+\n+url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+\n+processor = AutoImageProcessor.from_pretrained(\"facebook/pixio-vith16\")\n+model = AutoModel.from_pretrained(\"facebook/pixio-vith16\")\n+\n+inputs = processor(images=image, return_tensors=\"pt\")\n+outputs = model(**inputs)\n+features_norm = outputs.last_hidden_state # class tokens + patch tokens after last LayerNorm\n+features = outputs.hidden_states[-1] # class tokens + patch tokens before last LayerNorm\n+```\n+\n+## Notes\n+\n+- The example below shows how to split the output tensor into:\n+  - a set of global embeddings for the whole image, commonly referred to as `CLS` token,\n+    useful for classification and retrieval. \n+    You can either average them (recommended) or concatenate them along the channel dimension.\n+  - a set of local embeddings, one for each `16x16` patch of the input image,\n+    useful for dense tasks, such as depth estimation and semantic segmentation.\n+\n+  ```py\n+  from transformers import AutoImageProcessor, AutoModel\n+  from PIL import Image\n+  import requests\n+\n+  url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n+  image = Image.open(requests.get(url, stream=True).raw)\n+  print(image.height, image.width)  # [480, 640]\n+\n+  processor = AutoImageProcessor.from_pretrained('facebook/pixio-vith16')\n+  model = AutoModel.from_pretrained('facebook/pixio-vith16')\n+  patch_size = model.config.patch_size\n+\n+  inputs = processor(images=image, return_tensors=\"pt\")\n+  print(inputs.pixel_values.shape)  # [1, 3, 256, 256]\n+  batch_size, rgb, img_height, img_width = inputs.pixel_values.shape\n+  num_patches_height, num_patches_width = img_height // patch_size, img_width // patch_size\n+  num_patches_flat = num_patches_height * num_patches_width\n+\n+  outputs = model(**inputs)\n+  last_hidden_states = outputs.last_hidden_state\n+  print(last_hidden_states.shape)  # [1, 8 + 256, 1280]\n+  assert last_hidden_states.shape == (batch_size, model.config.n_cls_tokens + num_patches_flat, model.config.hidden_size)\n+\n+  cls_tokens = last_hidden_states[:, :model.config.n_cls_tokens, :]\n+  patch_features = last_hidden_states[:, model.config.n_cls_tokens:, :].unflatten(1, (num_patches_height, num_patches_width))\n+  ```\n+\n+- Use [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) to speedup inference.\n+\n+  ```py\n+  import torch\n+  from transformers import AutoImageProcessor, AutoModel\n+  from PIL import Image\n+  import requests\n+\n+  url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n+  image = Image.open(requests.get(url, stream=True).raw)\n+\n+  processor = AutoImageProcessor.from_pretrained('facebook/pixio-vith16')\n+  model = AutoModel.from_pretrained('facebook/pixio-vith16')\n+\n+  compiled_model = torch.compile(model)\n+\n+  inputs = processor(images=image, return_tensors=\"pt\")\n+  outputs = compiled_model(**inputs)\n+  last_hidden_states = outputs.last_hidden_state\n+  ```\n+\n+## PixioConfig\n+\n+[[autodoc]] PixioConfig\n+\n+## PixioModel\n+\n+[[autodoc]] PixioModel\n+    - forward\n+\n+## PixioBackbone\n+\n+[[autodoc]] PixioBackbone\n+    - forward"
        },
        {
            "sha": "37f50eb4ad568c13e533455a8a9a53676863c542",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=a05e0e27ca1277a062344b93d4b6d2481ad1f601",
            "patch": "@@ -286,6 +286,7 @@\n     from .phimoe import *\n     from .phobert import *\n     from .pix2struct import *\n+    from .pixio import *\n     from .pixtral import *\n     from .plbart import *\n     from .poolformer import *"
        },
        {
            "sha": "9ba2b9915ffb72da42c9af14b78b47066236ad38",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=a05e0e27ca1277a062344b93d4b6d2481ad1f601",
            "patch": "@@ -323,6 +323,7 @@\n         (\"phi4_multimodal\", \"Phi4MultimodalConfig\"),\n         (\"phimoe\", \"PhimoeConfig\"),\n         (\"pix2struct\", \"Pix2StructConfig\"),\n+        (\"pixio\", \"PixioConfig\"),\n         (\"pixtral\", \"PixtralVisionConfig\"),\n         (\"plbart\", \"PLBartConfig\"),\n         (\"poolformer\", \"PoolFormerConfig\"),\n@@ -787,6 +788,7 @@\n         (\"phimoe\", \"Phimoe\"),\n         (\"phobert\", \"PhoBERT\"),\n         (\"pix2struct\", \"Pix2Struct\"),\n+        (\"pixio\", \"Pixio\"),\n         (\"pixtral\", \"Pixtral\"),\n         (\"plbart\", \"PLBart\"),\n         (\"poolformer\", \"PoolFormer\"),"
        },
        {
            "sha": "2b24c5893f9ba50e243c6fc788e9f2e97c3ea918",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=a05e0e27ca1277a062344b93d4b6d2481ad1f601",
            "patch": "@@ -159,6 +159,7 @@\n             (\"perception_lm\", (None, \"PerceptionLMImageProcessorFast\")),\n             (\"phi4_multimodal\", (None, \"Phi4MultimodalImageProcessorFast\")),\n             (\"pix2struct\", (\"Pix2StructImageProcessor\", \"Pix2StructImageProcessorFast\")),\n+            (\"pixio\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n             (\"pixtral\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n             (\"poolformer\", (\"PoolFormerImageProcessor\", \"PoolFormerImageProcessorFast\")),\n             (\"prompt_depth_anything\", (\"PromptDepthAnythingImageProcessor\", \"PromptDepthAnythingImageProcessorFast\")),"
        },
        {
            "sha": "9a01876acec0ab69cb467ba667e4cc145263d27e",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=a05e0e27ca1277a062344b93d4b6d2481ad1f601",
            "patch": "@@ -320,6 +320,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"phi3\", \"Phi3Model\"),\n         (\"phi4_multimodal\", \"Phi4MultimodalModel\"),\n         (\"phimoe\", \"PhimoeModel\"),\n+        (\"pixio\", \"PixioModel\"),\n         (\"pixtral\", \"PixtralVisionModel\"),\n         (\"plbart\", \"PLBartModel\"),\n         (\"poolformer\", \"PoolFormerModel\"),\n@@ -806,6 +807,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mobilenet_v2\", \"MobileNetV2Model\"),\n         (\"mobilevit\", \"MobileViTModel\"),\n         (\"mobilevitv2\", \"MobileViTV2Model\"),\n+        (\"pixio\", \"PixioModel\"),\n         (\"poolformer\", \"PoolFormerModel\"),\n         (\"pvt\", \"PvtModel\"),\n         (\"regnet\", \"RegNetModel\"),\n@@ -1688,6 +1690,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"hgnet_v2\", \"HGNetV2Backbone\"),\n         (\"hiera\", \"HieraBackbone\"),\n         (\"maskformer-swin\", \"MaskFormerSwinBackbone\"),\n+        (\"pixio\", \"PixioBackbone\"),\n         (\"pvt_v2\", \"PvtV2Backbone\"),\n         (\"resnet\", \"ResNetBackbone\"),\n         (\"rt_detr_resnet\", \"RTDetrResNetBackbone\"),"
        },
        {
            "sha": "75fb6567900a792e9846a05d6cb818a3799a194a",
            "filename": "src/transformers/models/pixio/__init__.py",
            "status": "added",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2Fpixio%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2Fpixio%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixio%2F__init__.py?ref=a05e0e27ca1277a062344b93d4b6d2481ad1f601",
            "patch": "@@ -0,0 +1,30 @@\n+# coding=utf-8\n+# Copyright 2025 Meta AI and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Pixio model configuration\"\"\"\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_pixio import *\n+    from .modeling_pixio import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "b274ec8aef184618bfd116c88a6efb5b196dceac",
            "filename": "src/transformers/models/pixio/configuration_pixio.py",
            "status": "added",
            "additions": 151,
            "deletions": 0,
            "changes": 151,
            "blob_url": "https://github.com/huggingface/transformers/blob/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2Fpixio%2Fconfiguration_pixio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2Fpixio%2Fconfiguration_pixio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixio%2Fconfiguration_pixio.py?ref=a05e0e27ca1277a062344b93d4b6d2481ad1f601",
            "patch": "@@ -0,0 +1,151 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/pixio/modular_pixio.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_pixio.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Meta AI and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PreTrainedConfig\n+from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n+\n+\n+class PixioConfig(BackboneConfigMixin, PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`PixioModel`]. It is used to instantiate a\n+    Pixio model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the ViT\n+    [facebook/pixio-huge](https://huggingface.co/facebook/pixio-huge) architecture.\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1280):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        mlp_ratio (`int`, *optional*, defaults to 4):\n+            Ratio of the hidden size of the MLPs relative to the `hidden_size`.\n+        n_cls_tokens (`int`, *optional*, defaults to 8):\n+            Number of class tokens in the Transformer encoder.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n+        hidden_dropout_prob (`float`, *optional*, defaults to 0.0):\n+            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n+        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        image_size (`int`, *optional*, defaults to 256):\n+            The size (resolution) of each image.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        qkv_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to add a bias to the queries, keys and values.\n+        drop_path_rate (`float`, *optional*, defaults to 0.0):\n+            Stochastic depth rate per sample (when applied in the main path of residual layers).\n+        out_features (`list[str]`, *optional*):\n+            If used as backbone, list of features to output. Can be any of `\"stem\"`, `\"stage1\"`, `\"stage2\"`, etc.\n+            (depending on how many stages the model has). If unset and `out_indices` is set, will default to the\n+            corresponding stages. If unset and `out_indices` is unset, will default to the last stage. Must be in the\n+            same order as defined in the `stage_names` attribute.\n+        out_indices (`list[int]`, *optional*):\n+            If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how\n+            many stages the model has). If unset and `out_features` is set, will default to the corresponding stages.\n+            If unset and `out_features` is unset, will default to the last stage. Must be in the\n+            same order as defined in the `stage_names` attribute.\n+        apply_layernorm (`bool`, *optional*, defaults to `True`):\n+            Whether to apply layer normalization to the feature maps in case the model is used as backbone.\n+        reshape_hidden_states (`bool`, *optional*, defaults to `True`):\n+            Whether to reshape the feature maps to 4D tensors of shape `(batch_size, hidden_size, height, width)` in\n+            case the model is used as backbone. If `False`, the feature maps will be 3D tensors of shape `(batch_size,\n+            seq_len, hidden_size)`.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import PixioConfig, PixioModel\n+\n+    >>> # Initializing a Pixio pixio-huge style configuration\n+    >>> configuration = PixioConfig()\n+\n+    >>> # Initializing a model (with random weights) from the pixio-huge style configuration\n+    >>> model = PixioModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"pixio\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=1280,\n+        num_hidden_layers=32,\n+        num_attention_heads=16,\n+        mlp_ratio=4,\n+        n_cls_tokens=8,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.0,\n+        attention_probs_dropout_prob=0.0,\n+        initializer_range=0.02,\n+        layer_norm_eps=1e-6,\n+        image_size=256,\n+        patch_size=16,\n+        num_channels=3,\n+        qkv_bias=True,\n+        drop_path_rate=0.0,\n+        out_features=None,\n+        out_indices=None,\n+        apply_layernorm=True,\n+        reshape_hidden_states=True,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.mlp_ratio = mlp_ratio\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.initializer_range = initializer_range\n+        self.layer_norm_eps = layer_norm_eps\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.qkv_bias = qkv_bias\n+        self.drop_path_rate = drop_path_rate\n+        self.stage_names = [\"stem\"] + [f\"stage{idx}\" for idx in range(1, num_hidden_layers + 1)]\n+        self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n+            out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n+        )\n+        self.apply_layernorm = apply_layernorm\n+        self.reshape_hidden_states = reshape_hidden_states\n+\n+        self.n_cls_tokens = n_cls_tokens\n+\n+\n+__all__ = [\"PixioConfig\"]"
        },
        {
            "sha": "a75e151e681c7d5aafde31ff3800f39eb8755230",
            "filename": "src/transformers/models/pixio/convert_pixio_to_pytorch.py",
            "status": "added",
            "additions": 229,
            "deletions": 0,
            "changes": 229,
            "blob_url": "https://github.com/huggingface/transformers/blob/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2Fpixio%2Fconvert_pixio_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2Fpixio%2Fconvert_pixio_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixio%2Fconvert_pixio_to_pytorch.py?ref=a05e0e27ca1277a062344b93d4b6d2481ad1f601",
            "patch": "@@ -0,0 +1,229 @@\n+# coding=utf-8\n+# Copyright 2025 Meta AI and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Convert Pixio checkpoints from the original repository.\n+\n+URL: https://github.com/facebookresearch/pixio/tree/main\n+\"\"\"\n+\n+import argparse\n+from pathlib import Path\n+\n+import requests\n+import torch\n+from PIL import Image\n+\n+from transformers import BitImageProcessor, PixioConfig, PixioModel\n+from transformers.image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling\n+from transformers.utils import logging\n+\n+\n+logging.set_verbosity_info()\n+logger = logging.get_logger(__name__)\n+\n+\n+def get_pixio_config(model_name):\n+    if \"vitb16\" in model_name:\n+        kwargs = {\n+            \"hidden_size\": 768,\n+            \"num_hidden_layers\": 12,\n+            \"num_attention_heads\": 12,\n+        }\n+    elif \"vitl16\" in model_name:\n+        kwargs = {\n+            \"hidden_size\": 1024,\n+            \"num_hidden_layers\": 24,\n+            \"num_attention_heads\": 16,\n+        }\n+    elif \"vith16\" in model_name:\n+        kwargs = {\n+            \"hidden_size\": 1280,\n+            \"num_hidden_layers\": 32,\n+            \"num_attention_heads\": 16,\n+        }\n+    elif \"vit1b16\" in model_name:\n+        kwargs = {\n+            \"hidden_size\": 1536,\n+            \"num_hidden_layers\": 48,\n+            \"num_attention_heads\": 24,\n+        }\n+    elif \"vit5b16\" in model_name:\n+        kwargs = {\n+            \"hidden_size\": 3072,\n+            \"num_hidden_layers\": 48,\n+            \"num_attention_heads\": 32,\n+        }\n+    else:\n+        raise ValueError(f\"Model '{model_name}' not supported\")\n+\n+    config = PixioConfig(**kwargs)\n+\n+    return config\n+\n+\n+def create_rename_keys(config):\n+    rename_keys = []\n+    # fmt: off\n+\n+    # patch embedding layer\n+    rename_keys.append((\"cls_token\", \"embeddings.cls_token\"))\n+    rename_keys.append((\"pos_embed\", \"embeddings.position_embeddings\"))\n+    rename_keys.append((\"patch_embed.proj.weight\", \"embeddings.patch_embeddings.projection.weight\"))\n+    rename_keys.append((\"patch_embed.proj.bias\", \"embeddings.patch_embeddings.projection.bias\"))\n+\n+    for i in range(config.num_hidden_layers):\n+        # layernorms\n+        rename_keys.append((f\"blocks.{i}.norm1.weight\", f\"encoder.layer.{i}.norm1.weight\"))\n+        rename_keys.append((f\"blocks.{i}.norm1.bias\", f\"encoder.layer.{i}.norm1.bias\"))\n+        rename_keys.append((f\"blocks.{i}.norm2.weight\", f\"encoder.layer.{i}.norm2.weight\"))\n+        rename_keys.append((f\"blocks.{i}.norm2.bias\", f\"encoder.layer.{i}.norm2.bias\"))\n+        # MLP\n+        rename_keys.append((f\"blocks.{i}.mlp.fc1.weight\", f\"encoder.layer.{i}.mlp.fc1.weight\"))\n+        rename_keys.append((f\"blocks.{i}.mlp.fc1.bias\", f\"encoder.layer.{i}.mlp.fc1.bias\"))\n+        rename_keys.append((f\"blocks.{i}.mlp.fc2.weight\", f\"encoder.layer.{i}.mlp.fc2.weight\"))\n+        rename_keys.append((f\"blocks.{i}.mlp.fc2.bias\", f\"encoder.layer.{i}.mlp.fc2.bias\"))\n+        # attention projection layer\n+        rename_keys.append((f\"blocks.{i}.attn.proj.weight\", f\"encoder.layer.{i}.attention.output.dense.weight\"))\n+        rename_keys.append((f\"blocks.{i}.attn.proj.bias\", f\"encoder.layer.{i}.attention.output.dense.bias\"))\n+\n+    # final layernorm\n+    rename_keys.append((\"norm.weight\", \"layernorm.weight\"))\n+    rename_keys.append((\"norm.bias\", \"layernorm.bias\"))\n+\n+    # fmt: on\n+    return rename_keys\n+\n+\n+def rename_key(dct, old, new):\n+    val = dct.pop(old)\n+    dct[new] = val\n+\n+\n+# we split up the matrix of each encoder layer into queries, keys and values\n+def read_in_q_k_v(state_dict, config):\n+    for i in range(config.num_hidden_layers):\n+        # read in weights + bias of input projection layer (in timm, this is a single matrix + bias)\n+        in_proj_weight = state_dict.pop(f\"blocks.{i}.attn.qkv.weight\")\n+        in_proj_bias = state_dict.pop(f\"blocks.{i}.attn.qkv.bias\")\n+        # next, add query, keys and values (in that order) to the state dict\n+        state_dict[f\"encoder.layer.{i}.attention.attention.query.weight\"] = in_proj_weight[: config.hidden_size, :]\n+        state_dict[f\"encoder.layer.{i}.attention.attention.query.bias\"] = in_proj_bias[: config.hidden_size]\n+        state_dict[f\"encoder.layer.{i}.attention.attention.key.weight\"] = in_proj_weight[\n+            config.hidden_size : config.hidden_size * 2, :\n+        ]\n+        state_dict[f\"encoder.layer.{i}.attention.attention.key.bias\"] = in_proj_bias[\n+            config.hidden_size : config.hidden_size * 2\n+        ]\n+        state_dict[f\"encoder.layer.{i}.attention.attention.value.weight\"] = in_proj_weight[-config.hidden_size :, :]\n+        state_dict[f\"encoder.layer.{i}.attention.attention.value.bias\"] = in_proj_bias[-config.hidden_size :]\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n+    return image\n+\n+\n+@torch.no_grad()\n+def convert_pixio_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path, push_to_hub=False):\n+    \"\"\"\n+    Copy/paste/tweak model's weights to our Pixio structure.\n+    \"\"\"\n+\n+    # define default Pixio configuration\n+    config = get_pixio_config(model_name)\n+    state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n+    rename_keys = create_rename_keys(config)\n+    for src, dest in rename_keys:\n+        rename_key(state_dict, src, dest)\n+    read_in_q_k_v(state_dict, config)\n+\n+    # load HuggingFace model\n+    model = PixioModel(config).eval()\n+    model.load_state_dict(state_dict)\n+\n+    # load image\n+    image = prepare_img()\n+    processor = BitImageProcessor(\n+        size={\"height\": 256, \"width\": 256},\n+        do_center_crop=False,\n+        crop_size={\"height\": 256, \"width\": 256},\n+        resample=PILImageResampling.BICUBIC,\n+        image_mean=IMAGENET_DEFAULT_MEAN,\n+        image_std=IMAGENET_DEFAULT_STD,\n+    )\n+    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n+\n+    with torch.no_grad():\n+        outputs = model(pixel_values, output_hidden_states=True)\n+\n+    print(\"last layer class embeddings w/ LayerNorm:\")\n+    print(outputs.last_hidden_state[:, : config.n_cls_tokens])\n+    print(\"last layer patch embeddings w/ LayerNorm:\")\n+    print(outputs.last_hidden_state[:, config.n_cls_tokens :])\n+    print(\"last layer class embeddings w/o LayerNorm:\")\n+    print(outputs.hidden_states[-1][:, : config.n_cls_tokens])\n+    print(\"last layer patch embeddings w/o LayerNorm:\")\n+    print(outputs.hidden_states[-1][:, config.n_cls_tokens :])\n+\n+    if pytorch_dump_folder_path is not None:\n+        Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n+        print(f\"Saving model {model_name} to {pytorch_dump_folder_path}\")\n+        model.save_pretrained(pytorch_dump_folder_path)\n+        print(f\"Saving image processor to {pytorch_dump_folder_path}\")\n+        processor.save_pretrained(pytorch_dump_folder_path)\n+\n+    if push_to_hub:\n+        name = model_name.replace(\"_\", \"-\")\n+        model.push_to_hub(f\"facebook/{name}\")\n+        processor.push_to_hub(f\"facebook/{name}\")\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    # Required parameters\n+    parser.add_argument(\n+        \"--model_name\",\n+        default=\"pixio_vith16\",\n+        type=str,\n+        choices=[\n+            \"pixio_vitb16\",\n+            \"pixio_vitl16\",\n+            \"pixio_vith16\",\n+            \"pixio_vit1b16\",\n+            \"pixio_vit5b16\",\n+        ],\n+        help=\"Name of the model you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--checkpoint_path\",\n+        required=True,\n+        type=str,\n+        help=\"Path of the checkpoint you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\",\n+        default=None,\n+        type=str,\n+        help=\"Path to the output PyTorch model directory.\",\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n+    )\n+\n+    args = parser.parse_args()\n+    convert_pixio_checkpoint(args.model_name, args.checkpoint_path, args.pytorch_dump_folder_path, args.push_to_hub)"
        },
        {
            "sha": "3ea3ca37c6e5a4a89e0e896338720b65c828c3ad",
            "filename": "src/transformers/models/pixio/modeling_pixio.py",
            "status": "added",
            "additions": 507,
            "deletions": 0,
            "changes": 507,
            "blob_url": "https://github.com/huggingface/transformers/blob/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2Fpixio%2Fmodeling_pixio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2Fpixio%2Fmodeling_pixio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixio%2Fmodeling_pixio.py?ref=a05e0e27ca1277a062344b93d4b6d2481ad1f601",
            "patch": "@@ -0,0 +1,507 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/pixio/modular_pixio.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_pixio.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Meta AI and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import collections.abc\n+from collections.abc import Callable\n+from typing import Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ... import initialization as init\n+from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BackboneOutput, BaseModelOutput, BaseModelOutputWithPooling\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, is_tracing\n+from ...utils.backbone_utils import BackboneMixin\n+from ...utils.generic import check_model_inputs\n+from .configuration_pixio import PixioConfig\n+\n+\n+class PixioPatchEmbeddings(nn.Module):\n+    \"\"\"\n+    This class turns `pixel_values` of shape `(batch_size, num_channels, height, width)` into the initial\n+    `hidden_states` (patch embeddings) of shape `(batch_size, seq_length, hidden_size)` to be consumed by a\n+    Transformer.\n+    \"\"\"\n+\n+    def __init__(self, config: PixioConfig):\n+        super().__init__()\n+        image_size, patch_size = config.image_size, config.patch_size\n+        num_channels, hidden_size = config.num_channels, config.hidden_size\n+\n+        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n+        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n+        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.num_patches = num_patches\n+\n+        self.projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n+\n+    def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n+        batch_size, num_channels, height, width = pixel_values.shape\n+        if num_channels != self.num_channels:\n+            raise ValueError(\n+                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n+                f\" Expected {self.num_channels} but got {num_channels}.\"\n+            )\n+        if not interpolate_pos_encoding:\n+            if height != self.image_size[0] or width != self.image_size[1]:\n+                raise ValueError(\n+                    f\"Input image size ({height}*{width}) doesn't match model\"\n+                    f\" ({self.image_size[0]}*{self.image_size[1]}).\"\n+                )\n+        embeddings = self.projection(pixel_values).flatten(2).transpose(1, 2)\n+        return embeddings\n+\n+\n+class PixioEmbeddings(nn.Module):\n+    \"\"\"\n+    Construct the CLS tokens, position and patch embeddings.\n+    \"\"\"\n+\n+    def __init__(self, config: PixioConfig) -> None:\n+        super().__init__()\n+\n+        self.cls_token = nn.Parameter(torch.randn(1, config.n_cls_tokens, config.hidden_size))\n+        self.mask_token = None\n+        self.patch_embeddings = PixioPatchEmbeddings(config)\n+        num_patches = self.patch_embeddings.num_patches\n+        self.position_embeddings = nn.Parameter(torch.randn(1, num_patches + config.n_cls_tokens, config.hidden_size))\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.n_cls_tokens = config.n_cls_tokens\n+        self.patch_size = config.patch_size\n+        self.config = config\n+\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support tracing and interpolation at torch.float32 precision.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+        num_patches = embeddings.shape[1] - self.n_cls_tokens\n+        num_positions = self.position_embeddings.shape[1] - self.n_cls_tokens\n+\n+        if not is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n+\n+        class_pos_embed = self.position_embeddings[:, : self.n_cls_tokens]\n+        patch_pos_embed = self.position_embeddings[:, self.n_cls_tokens :]\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+        target_dtype = patch_pos_embed.dtype\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed.to(torch.float32),\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        ).to(dtype=target_dtype)\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n+\n+    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n+        target_dtype = self.patch_embeddings.projection.weight.dtype\n+        embeddings = self.patch_embeddings(pixel_values.to(dtype=target_dtype))\n+\n+        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n+        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n+\n+        embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+\n+        embeddings = self.dropout(embeddings)\n+\n+        return embeddings\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class PixioSelfAttention(nn.Module):\n+    def __init__(self, config: PixioConfig):\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size {config.hidden_size} is not a multiple of the number of attention \"\n+                f\"heads {config.num_attention_heads}.\"\n+            )\n+\n+        self.config = config\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.scaling = self.attention_head_size**-0.5\n+        self.is_causal = False\n+\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        batch_size = hidden_states.shape[0]\n+        new_shape = batch_size, -1, self.num_attention_heads, self.attention_head_size\n+\n+        key_layer = self.key(hidden_states).view(*new_shape).transpose(1, 2)\n+        value_layer = self.value(hidden_states).view(*new_shape).transpose(1, 2)\n+        query_layer = self.query(hidden_states).view(*new_shape).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        context_layer, attention_probs = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            None,\n+            is_causal=self.is_causal,\n+            scaling=self.scaling,\n+            dropout=0.0 if not self.training else self.dropout_prob,\n+        )\n+\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.reshape(new_context_layer_shape)\n+\n+        return context_layer, attention_probs\n+\n+\n+class PixioSelfOutput(nn.Module):\n+    \"\"\"\n+    The residual connection is defined in PixioLayer instead of here (as is the case with other models), due to the\n+    layernorm applied before each block.\n+    \"\"\"\n+\n+    def __init__(self, config: PixioConfig):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+\n+    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        return hidden_states\n+\n+\n+class PixioAttention(nn.Module):\n+    def __init__(self, config: PixioConfig):\n+        super().__init__()\n+        self.attention = PixioSelfAttention(config)\n+        self.output = PixioSelfOutput(config)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        self_attn_output, _ = self.attention(hidden_states)\n+        output = self.output(self_attn_output, hidden_states)\n+        return output\n+\n+\n+def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:\n+    \"\"\"\n+    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n+\n+    \"\"\"\n+    if drop_prob == 0.0 or not training:\n+        return input\n+    keep_prob = 1 - drop_prob\n+    shape = (input.shape[0],) + (1,) * (input.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n+    random_tensor = keep_prob + torch.rand(shape, dtype=input.dtype, device=input.device)\n+    random_tensor.floor_()  # binarize\n+    output = input.div(keep_prob) * random_tensor\n+    return output\n+\n+\n+class PixioDropPath(nn.Module):\n+    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n+\n+    def __init__(self, drop_prob: Optional[float] = None) -> None:\n+        super().__init__()\n+        self.drop_prob = drop_prob\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        return drop_path(hidden_states, self.drop_prob, self.training)\n+\n+    def extra_repr(self) -> str:\n+        return f\"p={self.drop_prob}\"\n+\n+\n+class PixioMLP(nn.Module):\n+    def __init__(self, config) -> None:\n+        super().__init__()\n+        in_features = out_features = config.hidden_size\n+        hidden_features = int(config.hidden_size * config.mlp_ratio)\n+        self.fc1 = nn.Linear(in_features, hidden_features, bias=True)\n+        if isinstance(config.hidden_act, str):\n+            self.activation = ACT2FN[config.hidden_act]\n+        else:\n+            self.activation = config.hidden_act\n+        self.fc2 = nn.Linear(hidden_features, out_features, bias=True)\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        hidden_state = self.fc1(hidden_state)\n+        hidden_state = self.activation(hidden_state)\n+        hidden_state = self.fc2(hidden_state)\n+        return hidden_state\n+\n+\n+class PixioLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: PixioConfig) -> None:\n+        super().__init__()\n+\n+        self.norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.attention = PixioAttention(config)\n+        self.drop_path = PixioDropPath(config.drop_path_rate) if config.drop_path_rate > 0.0 else nn.Identity()\n+\n+        self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.mlp = PixioMLP(config)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states_norm = self.norm1(hidden_states)\n+        self_attention_output = self.attention(hidden_states_norm)\n+\n+        hidden_states = self.drop_path(self_attention_output) + hidden_states\n+\n+        layer_output = self.norm2(hidden_states)\n+        layer_output = self.mlp(layer_output)\n+\n+        layer_output = self.drop_path(layer_output) + hidden_states\n+\n+        return layer_output\n+\n+\n+class PixioEncoder(nn.Module):\n+    def __init__(self, config: PixioConfig):\n+        super().__init__()\n+        self.config = config\n+        self.layer = nn.ModuleList([PixioLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    def forward(self, hidden_states: torch.Tensor, output_hidden_states: bool = False) -> BaseModelOutput:\n+        all_hidden_states = [hidden_states] if output_hidden_states else None\n+        for i, layer_module in enumerate(self.layer):\n+            hidden_states = layer_module(hidden_states)\n+            if all_hidden_states:\n+                all_hidden_states.append(hidden_states)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=tuple(all_hidden_states) if all_hidden_states else None,\n+        )\n+\n+\n+@auto_docstring\n+class PixioPreTrainedModel(PreTrainedModel):\n+    config: PixioConfig\n+    base_model_prefix = \"pixio\"\n+    main_input_name = \"pixel_values\"\n+    input_modalities = (\"image\",)\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"PixioEmbeddings\", \"PixioLayer\"]\n+    _supports_sdpa = True\n+    _supports_flash_attn = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": PixioLayer,\n+        \"attentions\": PixioSelfAttention,\n+    }\n+\n+    @torch.no_grad()\n+    def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                init.zeros_(module.bias)\n+        elif isinstance(module, nn.LayerNorm):\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n+        elif isinstance(module, PixioEmbeddings):\n+            init.trunc_normal_(module.position_embeddings, mean=0.0, std=self.config.initializer_range)\n+            init.trunc_normal_(module.cls_token, mean=0.0, std=self.config.initializer_range)\n+            if module.mask_token is not None:\n+                init.zeros_(module.mask_token)\n+\n+\n+@auto_docstring\n+class PixioModel(PixioPreTrainedModel):\n+    def __init__(self, config: PixioConfig):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.embeddings = PixioEmbeddings(config)\n+        self.encoder = PixioEncoder(config)\n+\n+        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> PixioPatchEmbeddings:\n+        return self.embeddings.patch_embeddings\n+\n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n+    ) -> BaseModelOutputWithPooling:\n+        if output_hidden_states is None:\n+            output_hidden_states = self.config.output_hidden_states\n+\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        embedding_output = self.embeddings(pixel_values)\n+\n+        encoder_outputs: BaseModelOutput = self.encoder(embedding_output, output_hidden_states=output_hidden_states)\n+        sequence_output = encoder_outputs.last_hidden_state\n+        sequence_output = self.layernorm(sequence_output)\n+        pooled_output = sequence_output[:, : self.embeddings.n_cls_tokens, :].mean(dim=1)\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=sequence_output,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Pixio backbone, to be used with frameworks like DETR and MaskFormer.\n+    \"\"\"\n+)\n+class PixioBackbone(PixioPreTrainedModel, BackboneMixin):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        super()._init_backbone(config)\n+\n+        self.num_features = [config.hidden_size for _ in range(config.num_hidden_layers + 1)]\n+        self.embeddings = PixioEmbeddings(config)\n+        self.encoder = PixioEncoder(config)\n+\n+        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> PixioPatchEmbeddings:\n+        return self.embeddings.patch_embeddings\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self, pixel_values: torch.Tensor, output_hidden_states: Optional[bool] = None, **kwargs\n+    ) -> BackboneOutput:\n+        r\"\"\"\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoImageProcessor, AutoBackbone\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> import requests\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> processor = AutoImageProcessor.from_pretrained(\"facebook/pixio-huge\")\n+        >>> model = AutoBackbone.from_pretrained(\n+        ...     \"facebook/pixio-huge\", out_features=[\"stage7\", \"stage15\", \"stage23\", \"stage31\"]\n+        ... )\n+\n+        >>> inputs = processor(image, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> feature_maps = outputs.feature_maps\n+        >>> list(feature_maps[-1].shape)\n+        [1, 1280, 16, 16]\n+        ```\"\"\"\n+        if output_hidden_states is None:\n+            output_hidden_states = self.config.output_hidden_states\n+\n+        embedding_output = self.embeddings(pixel_values)\n+        output: BaseModelOutput = self.encoder(embedding_output, output_hidden_states=True)\n+        hidden_states = output.hidden_states\n+\n+        feature_maps = []\n+        for stage, hidden_state in zip(self.stage_names, hidden_states):\n+            if stage in self.out_features:\n+                if self.config.apply_layernorm:\n+                    hidden_state = self.layernorm(hidden_state)\n+                if self.config.reshape_hidden_states:\n+                    hidden_state = hidden_state[:, self.embeddings.n_cls_tokens :]\n+                    batch_size, _, height, width = pixel_values.shape\n+                    patch_size = self.config.patch_size\n+                    hidden_state = hidden_state.reshape(batch_size, height // patch_size, width // patch_size, -1)\n+                    hidden_state = hidden_state.permute(0, 3, 1, 2).contiguous()\n+                feature_maps.append(hidden_state)\n+\n+        return BackboneOutput(\n+            feature_maps=tuple(feature_maps),\n+            hidden_states=hidden_states if output_hidden_states else None,\n+        )\n+\n+\n+__all__ = [\"PixioModel\", \"PixioPreTrainedModel\", \"PixioBackbone\"]"
        },
        {
            "sha": "6697aa51eff8f49f220b1018fdb63e96da1bccb8",
            "filename": "src/transformers/models/pixio/modular_pixio.py",
            "status": "added",
            "additions": 404,
            "deletions": 0,
            "changes": 404,
            "blob_url": "https://github.com/huggingface/transformers/blob/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2Fpixio%2Fmodular_pixio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a05e0e27ca1277a062344b93d4b6d2481ad1f601/src%2Ftransformers%2Fmodels%2Fpixio%2Fmodular_pixio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixio%2Fmodular_pixio.py?ref=a05e0e27ca1277a062344b93d4b6d2481ad1f601",
            "patch": "@@ -0,0 +1,404 @@\n+# coding=utf-8\n+# Copyright 2025 Meta AI and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Pixio model.\"\"\"\n+\n+from typing import Optional\n+\n+import torch\n+from torch import nn\n+\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BackboneOutput, BaseModelOutput, BaseModelOutputWithPooling\n+from ...utils import auto_docstring, is_tracing, logging\n+from ...utils.generic import check_model_inputs\n+from ..dinov2.configuration_dinov2 import Dinov2Config\n+from ..dinov2.modeling_dinov2 import (\n+    Dinov2Backbone,\n+    Dinov2DropPath,\n+    Dinov2MLP,\n+)\n+from ..vit.modeling_vit import ViTAttention, ViTPatchEmbeddings, ViTPreTrainedModel\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class PixioConfig(Dinov2Config):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`PixioModel`]. It is used to instantiate a\n+    Pixio model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the ViT\n+    [facebook/pixio-huge](https://huggingface.co/facebook/pixio-huge) architecture.\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1280):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        mlp_ratio (`int`, *optional*, defaults to 4):\n+            Ratio of the hidden size of the MLPs relative to the `hidden_size`.\n+        n_cls_tokens (`int`, *optional*, defaults to 8):\n+            Number of class tokens in the Transformer encoder.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n+        hidden_dropout_prob (`float`, *optional*, defaults to 0.0):\n+            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n+        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        image_size (`int`, *optional*, defaults to 256):\n+            The size (resolution) of each image.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        qkv_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to add a bias to the queries, keys and values.\n+        drop_path_rate (`float`, *optional*, defaults to 0.0):\n+            Stochastic depth rate per sample (when applied in the main path of residual layers).\n+        out_features (`list[str]`, *optional*):\n+            If used as backbone, list of features to output. Can be any of `\"stem\"`, `\"stage1\"`, `\"stage2\"`, etc.\n+            (depending on how many stages the model has). If unset and `out_indices` is set, will default to the\n+            corresponding stages. If unset and `out_indices` is unset, will default to the last stage. Must be in the\n+            same order as defined in the `stage_names` attribute.\n+        out_indices (`list[int]`, *optional*):\n+            If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how\n+            many stages the model has). If unset and `out_features` is set, will default to the corresponding stages.\n+            If unset and `out_features` is unset, will default to the last stage. Must be in the\n+            same order as defined in the `stage_names` attribute.\n+        apply_layernorm (`bool`, *optional*, defaults to `True`):\n+            Whether to apply layer normalization to the feature maps in case the model is used as backbone.\n+        reshape_hidden_states (`bool`, *optional*, defaults to `True`):\n+            Whether to reshape the feature maps to 4D tensors of shape `(batch_size, hidden_size, height, width)` in\n+            case the model is used as backbone. If `False`, the feature maps will be 3D tensors of shape `(batch_size,\n+            seq_len, hidden_size)`.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import PixioConfig, PixioModel\n+\n+    >>> # Initializing a Pixio pixio-huge style configuration\n+    >>> configuration = PixioConfig()\n+\n+    >>> # Initializing a model (with random weights) from the pixio-huge style configuration\n+    >>> model = PixioModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"pixio\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=1280,\n+        num_hidden_layers=32,\n+        num_attention_heads=16,\n+        mlp_ratio=4,\n+        n_cls_tokens=8,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.0,\n+        attention_probs_dropout_prob=0.0,\n+        initializer_range=0.02,\n+        layer_norm_eps=1e-6,\n+        image_size=256,\n+        patch_size=16,\n+        num_channels=3,\n+        qkv_bias=True,\n+        drop_path_rate=0.0,\n+        out_features=None,\n+        out_indices=None,\n+        apply_layernorm=True,\n+        reshape_hidden_states=True,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            hidden_size=hidden_size,\n+            num_hidden_layers=num_hidden_layers,\n+            num_attention_heads=num_attention_heads,\n+            mlp_ratio=mlp_ratio,\n+            hidden_act=hidden_act,\n+            hidden_dropout_prob=hidden_dropout_prob,\n+            attention_probs_dropout_prob=attention_probs_dropout_prob,\n+            initializer_range=initializer_range,\n+            layer_norm_eps=layer_norm_eps,\n+            image_size=image_size,\n+            patch_size=patch_size,\n+            num_channels=num_channels,\n+            qkv_bias=qkv_bias,\n+            drop_path_rate=drop_path_rate,\n+            apply_layernorm=apply_layernorm,\n+            reshape_hidden_states=reshape_hidden_states,\n+        )\n+\n+        self.n_cls_tokens = n_cls_tokens\n+\n+        del self.layerscale_value\n+        del self.use_swiglu_ffn\n+        del self.use_mask_token\n+\n+\n+class PixioPatchEmbeddings(ViTPatchEmbeddings):\n+    pass\n+\n+\n+class PixioEmbeddings(nn.Module):\n+    \"\"\"\n+    Construct the CLS tokens, position and patch embeddings.\n+    \"\"\"\n+\n+    def __init__(self, config: PixioConfig) -> None:\n+        super().__init__()\n+\n+        self.cls_token = nn.Parameter(torch.randn(1, config.n_cls_tokens, config.hidden_size))\n+        self.mask_token = None\n+        self.patch_embeddings = PixioPatchEmbeddings(config)\n+        num_patches = self.patch_embeddings.num_patches\n+        self.position_embeddings = nn.Parameter(torch.randn(1, num_patches + config.n_cls_tokens, config.hidden_size))\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.n_cls_tokens = config.n_cls_tokens\n+        self.patch_size = config.patch_size\n+        self.config = config\n+\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support tracing and interpolation at torch.float32 precision.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+        num_patches = embeddings.shape[1] - self.n_cls_tokens\n+        num_positions = self.position_embeddings.shape[1] - self.n_cls_tokens\n+\n+        if not is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n+\n+        class_pos_embed = self.position_embeddings[:, : self.n_cls_tokens]\n+        patch_pos_embed = self.position_embeddings[:, self.n_cls_tokens :]\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+        target_dtype = patch_pos_embed.dtype\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed.to(torch.float32),\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        ).to(dtype=target_dtype)\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n+\n+    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n+        target_dtype = self.patch_embeddings.projection.weight.dtype\n+        embeddings = self.patch_embeddings(pixel_values.to(dtype=target_dtype))\n+\n+        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n+        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n+\n+        embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+\n+        embeddings = self.dropout(embeddings)\n+\n+        return embeddings\n+\n+\n+class PixioAttention(ViTAttention):\n+    pass\n+\n+\n+class PixioDropPath(Dinov2DropPath):\n+    pass\n+\n+\n+class PixioMLP(Dinov2MLP):\n+    pass\n+\n+\n+class PixioLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: PixioConfig) -> None:\n+        super().__init__()\n+\n+        self.norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.attention = PixioAttention(config)\n+        self.drop_path = PixioDropPath(config.drop_path_rate) if config.drop_path_rate > 0.0 else nn.Identity()\n+\n+        self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.mlp = PixioMLP(config)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states_norm = self.norm1(hidden_states)\n+        self_attention_output = self.attention(hidden_states_norm)\n+\n+        hidden_states = self.drop_path(self_attention_output) + hidden_states\n+\n+        layer_output = self.norm2(hidden_states)\n+        layer_output = self.mlp(layer_output)\n+\n+        layer_output = self.drop_path(layer_output) + hidden_states\n+\n+        return layer_output\n+\n+\n+class PixioEncoder(nn.Module):\n+    def __init__(self, config: PixioConfig):\n+        super().__init__()\n+        self.config = config\n+        self.layer = nn.ModuleList([PixioLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    def forward(self, hidden_states: torch.Tensor, output_hidden_states: bool = False) -> BaseModelOutput:\n+        all_hidden_states = [hidden_states] if output_hidden_states else None\n+        for i, layer_module in enumerate(self.layer):\n+            hidden_states = layer_module(hidden_states)\n+            if all_hidden_states:\n+                all_hidden_states.append(hidden_states)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=tuple(all_hidden_states) if all_hidden_states else None,\n+        )\n+\n+\n+class PixioPreTrainedModel(ViTPreTrainedModel):\n+    pass\n+\n+\n+@auto_docstring\n+class PixioModel(PixioPreTrainedModel):\n+    def __init__(self, config: PixioConfig):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.embeddings = PixioEmbeddings(config)\n+        self.encoder = PixioEncoder(config)\n+\n+        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> PixioPatchEmbeddings:\n+        return self.embeddings.patch_embeddings\n+\n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n+    ) -> BaseModelOutputWithPooling:\n+        if output_hidden_states is None:\n+            output_hidden_states = self.config.output_hidden_states\n+\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        embedding_output = self.embeddings(pixel_values)\n+\n+        encoder_outputs: BaseModelOutput = self.encoder(embedding_output, output_hidden_states=output_hidden_states)\n+        sequence_output = encoder_outputs.last_hidden_state\n+        sequence_output = self.layernorm(sequence_output)\n+        pooled_output = sequence_output[:, : self.embeddings.n_cls_tokens, :].mean(dim=1)\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=sequence_output,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Pixio backbone, to be used with frameworks like DETR and MaskFormer.\n+    \"\"\"\n+)\n+class PixioBackbone(Dinov2Backbone):\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self, pixel_values: torch.Tensor, output_hidden_states: Optional[bool] = None, **kwargs\n+    ) -> BackboneOutput:\n+        r\"\"\"\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoImageProcessor, AutoBackbone\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> import requests\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> processor = AutoImageProcessor.from_pretrained(\"facebook/pixio-huge\")\n+        >>> model = AutoBackbone.from_pretrained(\n+        ...     \"facebook/pixio-huge\", out_features=[\"stage7\", \"stage15\", \"stage23\", \"stage31\"]\n+        ... )\n+\n+        >>> inputs = processor(image, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> feature_maps = outputs.feature_maps\n+        >>> list(feature_maps[-1].shape)\n+        [1, 1280, 16, 16]\n+        ```\"\"\"\n+        if output_hidden_states is None:\n+            output_hidden_states = self.config.output_hidden_states\n+\n+        embedding_output = self.embeddings(pixel_values)\n+        output: BaseModelOutput = self.encoder(embedding_output, output_hidden_states=True)\n+        hidden_states = output.hidden_states\n+\n+        feature_maps = []\n+        for stage, hidden_state in zip(self.stage_names, hidden_states):\n+            if stage in self.out_features:\n+                if self.config.apply_layernorm:\n+                    hidden_state = self.layernorm(hidden_state)\n+                if self.config.reshape_hidden_states:\n+                    hidden_state = hidden_state[:, self.embeddings.n_cls_tokens :]\n+                    batch_size, _, height, width = pixel_values.shape\n+                    patch_size = self.config.patch_size\n+                    hidden_state = hidden_state.reshape(batch_size, height // patch_size, width // patch_size, -1)\n+                    hidden_state = hidden_state.permute(0, 3, 1, 2).contiguous()\n+                feature_maps.append(hidden_state)\n+\n+        return BackboneOutput(\n+            feature_maps=tuple(feature_maps),\n+            hidden_states=hidden_states if output_hidden_states else None,\n+        )\n+\n+\n+__all__ = [\"PixioConfig\", \"PixioModel\", \"PixioPreTrainedModel\", \"PixioBackbone\"]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/pixio/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/a05e0e27ca1277a062344b93d4b6d2481ad1f601/tests%2Fmodels%2Fpixio%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a05e0e27ca1277a062344b93d4b6d2481ad1f601/tests%2Fmodels%2Fpixio%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixio%2F__init__.py?ref=a05e0e27ca1277a062344b93d4b6d2481ad1f601"
        },
        {
            "sha": "a1c1ac3aa43297f992d44ad3196f54abd4bba8ba",
            "filename": "tests/models/pixio/test_modeling_pixio.py",
            "status": "added",
            "additions": 288,
            "deletions": 0,
            "changes": 288,
            "blob_url": "https://github.com/huggingface/transformers/blob/a05e0e27ca1277a062344b93d4b6d2481ad1f601/tests%2Fmodels%2Fpixio%2Ftest_modeling_pixio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a05e0e27ca1277a062344b93d4b6d2481ad1f601/tests%2Fmodels%2Fpixio%2Ftest_modeling_pixio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixio%2Ftest_modeling_pixio.py?ref=a05e0e27ca1277a062344b93d4b6d2481ad1f601",
            "patch": "@@ -0,0 +1,288 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Pixio model.\"\"\"\n+\n+import unittest\n+from functools import cached_property\n+\n+from transformers import PixioConfig\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_backbone_common import BackboneTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+    from transformers import PixioBackbone, PixioModel\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import AutoImageProcessor\n+\n+\n+class PixioModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        image_size=30,\n+        patch_size=2,\n+        num_channels=3,\n+        is_training=True,\n+        use_labels=True,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=37,\n+        n_cls_tokens=1,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.1,\n+        attention_probs_dropout_prob=0.1,\n+        type_sequence_label_size=10,\n+        initializer_range=0.02,\n+        scope=None,\n+        attn_implementation=\"eager\",\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.is_training = is_training\n+        self.use_labels = use_labels\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.n_cls_tokens = n_cls_tokens\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.type_sequence_label_size = type_sequence_label_size\n+        self.initializer_range = initializer_range\n+        self.scope = scope\n+        self.attn_implementation = attn_implementation\n+\n+        # in Pixio, the seq length equals the number of patches + class tokens\n+        num_patches = (image_size // patch_size) ** 2\n+        self.seq_length = num_patches + n_cls_tokens\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+\n+        labels = None\n+        if self.use_labels:\n+            labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n+\n+        config = self.get_config()\n+\n+        return config, pixel_values, labels\n+\n+    def get_config(self):\n+        return PixioConfig(\n+            image_size=self.image_size,\n+            patch_size=self.patch_size,\n+            num_channels=self.num_channels,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            n_cls_tokens=self.n_cls_tokens,\n+            hidden_act=self.hidden_act,\n+            hidden_dropout_prob=self.hidden_dropout_prob,\n+            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n+            initializer_range=self.initializer_range,\n+            attn_implementation=self.attn_implementation,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values, labels):\n+        model = PixioModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+\n+    def create_and_check_backbone(self, config, pixel_values, labels):\n+        model = PixioBackbone(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+\n+        # verify hidden states\n+        self.parent.assertEqual(len(result.feature_maps), len(config.out_features))\n+        expected_size = self.image_size // config.patch_size\n+        self.parent.assertListEqual(\n+            list(result.feature_maps[0].shape), [self.batch_size, model.channels[0], expected_size, expected_size]\n+        )\n+\n+        # verify channels\n+        self.parent.assertEqual(len(model.channels), len(config.out_features))\n+\n+        # verify backbone works with out_features=None\n+        config.out_features = None\n+        model = PixioBackbone(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+\n+        # verify feature maps\n+        self.parent.assertEqual(len(result.feature_maps), 1)\n+        self.parent.assertListEqual(\n+            list(result.feature_maps[0].shape), [self.batch_size, model.channels[0], expected_size, expected_size]\n+        )\n+\n+        # verify channels\n+        self.parent.assertEqual(len(model.channels), 1)\n+\n+        # verify backbone works with apply_layernorm=False and reshape_hidden_states=False\n+        config.apply_layernorm = False\n+        config.reshape_hidden_states = False\n+\n+        model = PixioBackbone(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+\n+        # verify feature maps\n+        self.parent.assertEqual(len(result.feature_maps), 1)\n+        self.parent.assertListEqual(\n+            list(result.feature_maps[0].shape), [self.batch_size, self.seq_length, self.hidden_size]\n+        )\n+\n+    def create_and_check_for_image_classification(self, config, pixel_values, labels):\n+        self.parent.skipTest(reason=\"Pixio currently exposes only the base model and backbone.\")\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (\n+            config,\n+            pixel_values,\n+            labels,\n+        ) = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class PixioModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as Pixio does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    test_torch_exportable = True\n+\n+    all_model_classes = (\n+        (\n+            PixioModel,\n+            PixioBackbone,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    pipeline_model_mapping = {\"image-feature-extraction\": PixioModel} if is_torch_available() else {}\n+\n+    test_resize_embeddings = False\n+\n+    def setUp(self):\n+        self.model_tester = PixioModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=PixioConfig, has_text_modality=False, hidden_size=37)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_backbone(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_backbone(*config_and_inputs)\n+\n+    def test_for_image_classification(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_for_image_classification(*config_and_inputs)\n+\n+    def test_batching_equivalence(self, atol=1e-4, rtol=1e-4):\n+        super().test_batching_equivalence(atol=atol, rtol=rtol)\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+    return image\n+\n+\n+@require_torch\n+@require_vision\n+class PixioModelIntegrationTest(unittest.TestCase):\n+    @cached_property\n+    def default_image_processor(self):\n+        return AutoImageProcessor.from_pretrained(\"LiheYoung/pixio-vith16\") if is_vision_available() else None\n+\n+    @slow\n+    def test_inference_no_head(self):\n+        model = PixioModel.from_pretrained(\"LiheYoung/pixio-vith16\").to(torch_device)\n+\n+        image_processor = self.default_image_processor\n+        image = prepare_img()\n+        inputs = image_processor(image, return_tensors=\"pt\").to(torch_device)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        # verify the last hidden states\n+        expected_shape = torch.Size((1, 264, 1280))\n+        self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n+\n+        expected_slice = torch.tensor(\n+            [[0.7420, -1.4220, 0.1580], [0.3938, -1.4386, 0.2878], [0.2898, -1.4012, 0.3667]],\n+            device=torch_device,\n+        )\n+        # valid the first three patch tokens\n+        torch.testing.assert_close(outputs.last_hidden_state[0, 8:11, :3], expected_slice, rtol=1e-4, atol=1e-4)\n+\n+\n+@require_torch\n+class PixioBackboneTest(unittest.TestCase, BackboneTesterMixin):\n+    all_model_classes = (PixioBackbone,) if is_torch_available() else ()\n+    config_class = PixioConfig\n+\n+    has_attentions = False\n+\n+    def setUp(self):\n+        self.model_tester = PixioModelTester(self)"
        }
    ],
    "stats": {
        "total": 1739,
        "additions": 1739,
        "deletions": 0
    }
}