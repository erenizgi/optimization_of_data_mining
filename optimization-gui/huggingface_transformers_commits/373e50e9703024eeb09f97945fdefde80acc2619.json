{
    "author": "zucchini-nlp",
    "message": "Init cache on meta device (#35164)\n\n* init cache on meta device\r\n\r\n* offloaded static + enable tests\r\n\r\n* tests weren't running before  :(\r\n\r\n* update\r\n\r\n* fix mamba\r\n\r\n* fix copies\r\n\r\n* update\r\n\r\n* address comments and fix tests\r\n\r\n* fix copies\r\n\r\n* Update src/transformers/cache_utils.py\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* update\r\n\r\n* mamba fix\r\n\r\n---------\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "373e50e9703024eeb09f97945fdefde80acc2619",
    "files": [
        {
            "sha": "b2be3f238d0cf1a89d52542750832d23ff75e29f",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 94,
            "deletions": 37,
            "changes": 131,
            "blob_url": "https://github.com/huggingface/transformers/blob/373e50e9703024eeb09f97945fdefde80acc2619/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/373e50e9703024eeb09f97945fdefde80acc2619/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=373e50e9703024eeb09f97945fdefde80acc2619",
            "patch": "@@ -1069,12 +1069,15 @@ class StaticCache(Cache):\n             The maximum sequence length with which the model will be used.\n         device (`torch.device` or `str`):\n             The device on which the cache should be initialized. Should be the same as the layer.\n+            The recommended way however is not not indicate any `device`, in that case cache will be initialized on `meta`\n+            device by default, and then moved to input device when updating.\n         dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n         layer_device_map(`Dict[int, Union[str, torch.device, int]]]`, `optional`):\n             Mapping between the layers and its device. This is required when you are manually initializing the cache and the model is splitted between differents gpus.\n             You can know which layers mapped to which device by checking the associated device_map: `model.hf_device_map`.\n \n+\n     Example:\n \n         ```python\n@@ -1096,6 +1099,7 @@ class StaticCache(Cache):\n     \"\"\"\n \n     # TODO (joao): remove `=None` in non-optional arguments in v4.46. Remove from `OBJECTS_TO_IGNORE` as well.\n+    @deprecate_kwarg(\"layer_device_map\", version=\"4.52.0\")\n     def __init__(\n         self,\n         config: PretrainedConfig,\n@@ -1122,6 +1126,7 @@ def __init__(\n         )\n \n         self.dtype = dtype\n+        self.device = torch.device(device) if device is not None else torch.device(\"meta\")\n         self.num_key_value_heads = (\n             config.num_attention_heads\n             if getattr(config, \"num_key_value_heads\", None) is None\n@@ -1136,7 +1141,7 @@ def __init__(\n             if layer_device_map is not None:\n                 layer_device = layer_device_map[idx]\n             else:\n-                layer_device = device\n+                layer_device = self.device\n             new_layer_key_cache = torch.zeros(cache_shape, dtype=self.dtype, device=layer_device)\n             new_layer_value_cache = torch.zeros(cache_shape, dtype=self.dtype, device=layer_device)\n             # Notes:\n@@ -1181,6 +1186,9 @@ def update(\n         \"\"\"\n \n         cache_position = cache_kwargs.get(\"cache_position\")\n+        if self.key_cache[layer_idx].device.type == \"meta\":\n+            self.key_cache[layer_idx] = torch.zeros_like(self.key_cache[layer_idx], device=key_states.device)\n+            self.value_cache[layer_idx] = torch.zeros_like(self.value_cache[layer_idx], device=value_states.device)\n \n         k_out = self.key_cache[layer_idx]\n         v_out = self.value_cache[layer_idx]\n@@ -1209,6 +1217,8 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         # Occupied cache == any slot in the 3rd dim (sequence length) holds a non-zero value. To save on compute, let's\n         # limit the check to the first batch member and head dimension.\n         # TODO: deprecate this function in favor of `cache_position`\n+        if self.key_cache[layer_idx].device.type == \"meta\":\n+            return 0\n         return (self.key_cache[layer_idx][0, 0].any(dim=-1)).sum()\n \n     def get_max_cache_shape(self) -> Optional[int]:\n@@ -1217,9 +1227,10 @@ def get_max_cache_shape(self) -> Optional[int]:\n     def reset(self):\n         \"\"\"Resets the cache values while preserving the objects\"\"\"\n         for layer_idx in range(len(self.key_cache)):\n-            # In-place ops prevent breaking the static address\n-            self.key_cache[layer_idx].zero_()\n-            self.value_cache[layer_idx].zero_()\n+            if self.key_cache[layer_idx].device.type != \"meta\":\n+                # In-place ops prevent breaking the static address\n+                self.key_cache[layer_idx].zero_()\n+                self.value_cache[layer_idx].zero_()\n \n     @property\n     def batch_size(self):\n@@ -1257,6 +1268,8 @@ class SlidingWindowCache(StaticCache):\n             The maximum sequence length with which the model will be used.\n         device (`torch.device` or `str`):\n             The device on which the cache should be initialized. Should be the same as the layer.\n+            The recommended way however is not not indicate any `device`, in that case cache will be initialized on `meta`\n+            device by default, and then moved to input device when updating.\n         dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n         layer_device_map(`Dict[int, Union[str, torch.device, int]]]`, `optional`):\n@@ -1321,8 +1334,15 @@ def update(\n         cache_kwargs: Optional[Dict[str, Any]] = None,\n     ) -> Tuple[torch.Tensor]:\n         cache_position = cache_kwargs.get(\"cache_position\")\n+\n+        if self.key_cache[layer_idx].device.type == \"meta\":\n+            self.key_cache[layer_idx] = torch.zeros_like(self.key_cache[layer_idx], device=key_states.device)\n+            self.value_cache[layer_idx] = torch.zeros_like(self.value_cache[layer_idx], device=value_states.device)\n+\n         k_out = self.key_cache[layer_idx]\n         v_out = self.value_cache[layer_idx]\n+        key_states = key_states.to(k_out.dtype)\n+        value_states = value_states.to(v_out.dtype)\n \n         # assume this only happens in prefill phase when prompt length > sliding_window_size (= max_cache_len)\n         if cache_position.shape[0] > self.max_cache_len:\n@@ -1365,9 +1385,10 @@ def get_max_cache_shape(self) -> Optional[int]:\n \n     def reset(self):\n         for layer_idx in range(len(self.key_cache)):\n-            # In-place ops prevent breaking the static address\n-            self.key_cache[layer_idx].zero_()\n-            self.value_cache[layer_idx].zero_()\n+            if self.key_cache[layer_idx].device.type != \"meta\":\n+                # In-place ops prevent breaking the static address\n+                self.key_cache[layer_idx].zero_()\n+                self.value_cache[layer_idx].zero_()\n \n \n class EncoderDecoderCache(Cache):\n@@ -1561,8 +1582,10 @@ class HybridCache(Cache):\n             smaller batch size is used.\n         max_cache_len (`int`):\n             The maximum sequence length with which the model will be used.\n-        device (`torch.device` or `str`, *optional*, defaults to `\"cpu\"`):\n+        device (`torch.device` or `str`, *optional*):\n             The device on which the cache should be initialized. Should be the same as the layer.\n+            The recommended way however is not not indicate any `device`, in that case cache will be initialized on `meta`\n+            device by default, and then moved to input device when updating.\n         dtype (torch.dtype, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n         layer_device_map(`Dict[int, Union[str, torch.device, int]]]`, `optional`):\n@@ -1590,12 +1613,13 @@ class HybridCache(Cache):\n     \"\"\"\n \n     # TODO (joao): remove `=None` in non-optional arguments in v4.46. Remove from `OBJECTS_TO_IGNORE` as well.\n+    @deprecate_kwarg(\"layer_device_map\", version=\"4.52.0\")\n     def __init__(\n         self,\n         config: PretrainedConfig,\n         batch_size: int = None,\n         max_cache_len: int = None,\n-        device: Union[torch.device, str] = \"cpu\",\n+        device: Union[torch.device, str] = None,\n         dtype: torch.dtype = torch.float32,\n         max_batch_size: Optional[int] = None,\n         layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n@@ -1623,9 +1647,11 @@ def __init__(\n         self.num_key_value_heads = (\n             config.num_attention_heads if config.num_key_value_heads is None else config.num_key_value_heads\n         )\n+\n+        self.device = torch.device(device) if device is not None else torch.device(\"meta\")\n         layer_switch = config.sliding_window_pattern if hasattr(config, \"sliding_window_pattern\") else 2  # 2 is for BC\n         self.is_sliding = torch.tensor(\n-            [bool((i + 1) % layer_switch) for i in range(config.num_hidden_layers)], dtype=torch.bool, device=device\n+            [bool((i + 1) % layer_switch) for i in range(config.num_hidden_layers)], dtype=torch.bool\n         )\n         self.key_cache: List[torch.Tensor] = []\n         self.value_cache: List[torch.Tensor] = []\n@@ -1640,7 +1666,7 @@ def __init__(\n             if layer_device_map is not None:\n                 layer_device = layer_device_map[i]\n             else:\n-                layer_device = device\n+                layer_device = self.device\n             # Note: `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\n             # breaks when updating the cache.\n             cache_shape = global_cache_shape if not self.is_sliding[i] else sliding_cache_shape\n@@ -1696,8 +1722,16 @@ def update(\n     ) -> Tuple[torch.Tensor]:\n         cache_position = cache_kwargs.get(\"cache_position\")\n         sliding_window = cache_kwargs.get(\"sliding_window\")\n+\n+        if self.key_cache[layer_idx].device.type == \"meta\":\n+            self.key_cache[layer_idx] = torch.zeros_like(self.key_cache[layer_idx], device=key_states.device)\n+            self.value_cache[layer_idx] = torch.zeros_like(self.value_cache[layer_idx], device=value_states.device)\n+\n         k_out = self.key_cache[layer_idx]\n         v_out = self.value_cache[layer_idx]\n+        key_states = key_states.to(k_out.dtype)\n+        value_states = value_states.to(v_out.dtype)\n+\n         if sliding_window:\n             update_fn = self._sliding_update\n         else:\n@@ -1725,14 +1759,18 @@ def get_seq_length(self, layer_idx: Optional[int] = 0):\n                 \"`get_seq_length` on `HybridCache` may get inconsistent results depending on the layer index. \"\n                 \"Using the `layer_idx` argument is not supported.\"\n             )\n+\n+        if self.key_cache[layer_idx].device.type == \"meta\":\n+            return 0\n         return (self.key_cache[layer_idx][0, 0].any(dim=-1)).sum()\n \n     def reset(self):\n         \"\"\"Resets the cache values while preserving the objects\"\"\"\n         for layer_idx in range(len(self.key_cache)):\n-            # In-place ops prevent breaking the static address\n-            self.key_cache[layer_idx].zero_()\n-            self.value_cache[layer_idx].zero_()\n+            if self.key_cache[layer_idx].device.type != \"meta\":\n+                # In-place ops prevent breaking the static address\n+                self.key_cache[layer_idx].zero_()\n+                self.value_cache[layer_idx].zero_()\n \n     @property\n     def batch_size(self):\n@@ -1757,10 +1795,14 @@ class MambaCache:\n             The default `dtype` to use when initializing the layer.\n         device (`torch.device` or `str`, *optional*):\n             The device on which the cache should be initialized. Should be the same as the layer.\n+            The recommended way however is not not indicate any `device`, in that case cache will be initialized on `meta`\n+            device by default, and then moved to input device when updating.\n \n     Attributes:\n         dtype: (`torch.dtype`):\n             The default `dtype` used to initializing the cache.\n+        device (`torch.device`):\n+            The default device on which the cache was initialized.\n         intermediate_size: (`int`):\n             Model's intermediate_size taken from config.\n         ssm_state_size: (`int`):\n@@ -1809,30 +1851,40 @@ def __init__(\n         self.intermediate_size = config.intermediate_size\n         self.ssm_state_size = config.state_size\n         self.conv_kernel_size = config.conv_kernel\n+        self.device = torch.device(device) if device is not None else torch.device(\"meta\")\n+\n+        self.conv_states: List[torch.Tensor] = []\n+        self.ssm_states: List[torch.Tensor] = []\n+        for _ in range(config.num_hidden_layers):\n+            conv_state: torch.Tensor = torch.zeros(\n+                self.max_batch_size,\n+                self.intermediate_size,\n+                self.conv_kernel_size,\n+                device=self.device,\n+                dtype=dtype,\n+            )\n+            ssm_state: torch.Tensor = torch.zeros(\n+                self.max_batch_size,\n+                self.intermediate_size,\n+                self.ssm_state_size,\n+                device=self.device,\n+                dtype=dtype,\n+            )\n \n-        self.conv_states: torch.Tensor = torch.zeros(\n-            config.num_hidden_layers,\n-            self.max_batch_size,\n-            self.intermediate_size,\n-            self.conv_kernel_size,\n-            device=device,\n-            dtype=dtype,\n-        )\n-        self.ssm_states: torch.Tensor = torch.zeros(\n-            config.num_hidden_layers,\n-            self.max_batch_size,\n-            self.intermediate_size,\n-            self.ssm_state_size,\n-            device=device,\n-            dtype=dtype,\n-        )\n-\n-        torch._dynamo.mark_static_address(self.conv_states)\n-        torch._dynamo.mark_static_address(self.ssm_states)\n+            torch._dynamo.mark_static_address(conv_state)\n+            torch._dynamo.mark_static_address(ssm_state)\n+            self.conv_states.append(conv_state)\n+            self.ssm_states.append(ssm_state)\n \n     def update_conv_state(\n         self, layer_idx: int, new_conv_state: torch.Tensor, cache_position: torch.LongTensor\n     ) -> torch.Tensor:\n+        if self.conv_states[layer_idx].device.type == \"meta\":\n+            self.conv_states[layer_idx] = torch.zeros_like(\n+                self.conv_states[layer_idx],\n+                device=new_conv_state.device,\n+            )\n+\n         conv_state = self.conv_states[layer_idx]\n         cache_position = cache_position.clamp(0, self.conv_kernel_size - 1)\n \n@@ -1843,12 +1895,15 @@ def update_conv_state(\n         return self.conv_states[layer_idx]\n \n     def update_ssm_state(self, layer_idx: int, new_ssm_state: torch.Tensor):\n-        self.ssm_states[layer_idx] = new_ssm_state.to(self.ssm_states.device)\n+        self.ssm_states[layer_idx] = new_ssm_state.to(self.ssm_states[layer_idx].device)\n         return self.ssm_states[layer_idx]\n \n     def reset(self):\n-        self.conv_states.zero_()\n-        self.ssm_states.zero_()\n+        for layer_idx in range(len(self.conv_states)):\n+            if self.conv_states[layer_idx].device.type != \"meta\":\n+                # In-place ops prevent breaking the static address\n+                self.conv_states[layer_idx].zero_()\n+                self.ssm_states[layer_idx].zero_()\n \n     @property\n     def batch_size(self):\n@@ -1920,6 +1975,7 @@ class OffloadedStaticCache(StaticCache):\n         ```\n     \"\"\"\n \n+    @deprecate_kwarg(\"layer_device_map\", version=\"4.52.0\")\n     def __init__(\n         self,\n         config: PretrainedConfig,\n@@ -1930,9 +1986,10 @@ def __init__(\n         offload_device: Union[str, torch.device] = torch.device(\"cpu\"),\n         layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n     ) -> None:\n+        super(Cache, self).__init__()\n         self.max_batch_size = max_batch_size\n         self.max_cache_len = config.max_position_embeddings if max_cache_len is None else max_cache_len\n-        self.device = torch.device(device) if layer_device_map is None else layer_device_map[0]\n+        self.device = torch.device(device) if layer_device_map is None else torch.device(layer_device_map[0])\n         self.offload_device = torch.device(offload_device)\n         self.dtype = dtype if dtype is not None else torch.float32\n "
        },
        {
            "sha": "461d7e1215810189b3e6b1bb421c12922ea7c4dc",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 34,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/373e50e9703024eeb09f97945fdefde80acc2619/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/373e50e9703024eeb09f97945fdefde80acc2619/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=373e50e9703024eeb09f97945fdefde80acc2619",
            "patch": "@@ -1633,45 +1633,12 @@ def _get_cache(\n                     # models. May cause trobles with non-text modalities.\n                     cache_dtype = self.get_output_embeddings().weight.dtype\n \n-            def get_layer_device_map(execution_device_map: Optional[dict] = None):\n-                num_hidden_layers = self.config.get_text_config().num_hidden_layers\n-                if execution_device_map is None:\n-                    return None\n-                elif len(execution_device_map) == 1 and \"\" in execution_device_map:\n-                    return {idx: execution_device_map[\"\"] for idx in range(num_hidden_layers)}\n-                layer_device_map = {}\n-                for layer in execution_device_map:\n-                    for idx in range(num_hidden_layers):\n-                        if f\".{idx}.\" in f\"{layer}.\":\n-                            layer_device_map[idx] = execution_device_map[layer]\n-                            break\n-                for idx in range(num_hidden_layers):\n-                    if idx not in layer_device_map:\n-                        raise RuntimeError(f\"layer {idx} has not been mapped to a device.\")\n-                return layer_device_map\n-\n-            execution_device_map = None\n-            # Taken from dispatch_model from accelerate.\n-            # This is needed here if we don't want to make changes in accelerate in order to save execution_device\n-            # For offloaded case, we need to get the execution device, not just the device where it is offloaded\n-            if hasattr(self, \"hf_device_map\"):\n-                if set(self.hf_device_map.values()) == {\"cpu\"} or set(self.hf_device_map.values()) == {\"cpu\", \"disk\"}:\n-                    main_device = \"cpu\"\n-                else:\n-                    main_device = [d for d in self.hf_device_map.values() if d not in [\"cpu\", \"disk\"]][0]\n-                execution_device_map = {\n-                    name: main_device if device in [\"cpu\", \"disk\"] else device\n-                    for name, device in self.hf_device_map.items()\n-                }\n-            layer_device_map = get_layer_device_map(execution_device_map)\n-\n             cache_kwargs = {\n                 \"config\": self.config.get_text_config(),\n                 \"max_batch_size\": batch_size,\n                 \"max_cache_len\": max_cache_len,\n-                \"device\": device,\n                 \"dtype\": cache_dtype,\n-                \"layer_device_map\": layer_device_map,\n+                \"device\": device if cache_implementation == \"offloaded_static\" else None,\n             }\n             self._cache = cache_cls(**cache_kwargs)\n             if requires_cross_attention_cache:"
        },
        {
            "sha": "a0cbc8ba4e78d15fb669c57edd716cee6d89b631",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/373e50e9703024eeb09f97945fdefde80acc2619/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/373e50e9703024eeb09f97945fdefde80acc2619/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=373e50e9703024eeb09f97945fdefde80acc2619",
            "patch": "@@ -73,6 +73,7 @@ def __init__(self, model: PreTrainedModel):\n             batch_size=self.model.generation_config.cache_config.batch_size,\n             max_cache_len=self.model.generation_config.cache_config.max_cache_len,\n             dtype=self.model.dtype,\n+            device=self.model.generation_config.cache_config.device,\n         )\n         self.is_causal = any(\"CausalLM\" in arch for arch in self.model.config.architectures)\n         if self.is_causal:"
        },
        {
            "sha": "15469577fb41ac4147848e1a94fd18aa5cea83b5",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/373e50e9703024eeb09f97945fdefde80acc2619/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/373e50e9703024eeb09f97945fdefde80acc2619/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=373e50e9703024eeb09f97945fdefde80acc2619",
            "patch": "@@ -582,7 +582,6 @@ def forward(\n                 self.config,\n                 max_batch_size=batch_size,\n                 max_cache_len=seq_len,\n-                device=self.device,\n                 dtype=inputs_embeds.dtype,\n             )\n "
        },
        {
            "sha": "7020df27021ff8f56b2ce2ceba013d571ce0209c",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/373e50e9703024eeb09f97945fdefde80acc2619/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/373e50e9703024eeb09f97945fdefde80acc2619/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=373e50e9703024eeb09f97945fdefde80acc2619",
            "patch": "@@ -461,7 +461,6 @@ def forward(\n                 self.config,\n                 max_batch_size=batch_size,\n                 max_cache_len=seq_len,\n-                device=self.device,\n                 dtype=inputs_embeds.dtype,\n             )\n "
        },
        {
            "sha": "fb7e59051a836f4d27ad155f3d132f06a61bb052",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/373e50e9703024eeb09f97945fdefde80acc2619/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/373e50e9703024eeb09f97945fdefde80acc2619/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=373e50e9703024eeb09f97945fdefde80acc2619",
            "patch": "@@ -579,7 +579,6 @@ def forward(\n                 self.config,\n                 max_batch_size=batch_size,\n                 max_cache_len=seq_len,\n-                device=self.device,\n                 dtype=inputs_embeds.dtype,\n             )\n "
        },
        {
            "sha": "53a947eb95b30b99abc78a229ff69176c15c2a9c",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/373e50e9703024eeb09f97945fdefde80acc2619/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/373e50e9703024eeb09f97945fdefde80acc2619/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=373e50e9703024eeb09f97945fdefde80acc2619",
            "patch": "@@ -405,7 +405,6 @@ def forward(\n                 self.config,\n                 max_batch_size=batch_size,\n                 max_cache_len=seq_len,\n-                device=self.device,\n                 dtype=inputs_embeds.dtype,\n             )\n "
        },
        {
            "sha": "8d492ce673dafabd40f5562c50a46242217205c0",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/373e50e9703024eeb09f97945fdefde80acc2619/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/373e50e9703024eeb09f97945fdefde80acc2619/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=373e50e9703024eeb09f97945fdefde80acc2619",
            "patch": "@@ -728,22 +728,13 @@ def test_compile_static_cache(self):\n         dynamic_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, dynamic_text)\n \n-        # Static Cache\n+        # Static Cache + compile (`generate()` internally compiles each decoding step when static cache is used)\n         generated_ids = model.generate(\n             **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"static\"\n         )\n         static_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, static_text)\n \n-        # Static Cache + compile\n-        model._cache = None  # clear cache object, initialized when we pass `cache_implementation=\"static\"`\n-        model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n-        generated_ids = model.generate(\n-            **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"static\"\n-        )\n-        static_compiled_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n-        self.assertEqual(EXPECTED_TEXT_COMPLETION, static_compiled_text)\n-\n     @slow\n     @require_read_token\n     def test_export_static_cache(self):\n@@ -795,6 +786,7 @@ def test_export_static_cache(self):\n                     cache_config={\n                         \"batch_size\": batch_size,\n                         \"max_cache_len\": max_generation_length,\n+                        \"device\": device,\n                     },\n                 ),\n             )"
        },
        {
            "sha": "d361378503fa9653df72c6036a9a9fbcadf76a0b",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/373e50e9703024eeb09f97945fdefde80acc2619/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/373e50e9703024eeb09f97945fdefde80acc2619/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=373e50e9703024eeb09f97945fdefde80acc2619",
            "patch": "@@ -4635,6 +4635,11 @@ def test_flash_attn_2_from_config(self):\n                     fa2_correctly_converted = True\n                     break\n \n+            fa2_correctly_converted = (\n+                fa2_correctly_converted\n+                if not model_class._supports_flex_attn\n+                else fa2_model.config._attn_implementation == \"flash_attention_2\"\n+            )\n             self.assertTrue(fa2_correctly_converted)\n \n             _ = fa2_model(input_ids=dummy_input, attention_mask=dummy_attention_mask)\n@@ -4653,6 +4658,11 @@ def test_flash_attn_2_from_config(self):\n                         fa2_correctly_converted = True\n                         break\n \n+                fa2_correctly_converted = (\n+                    fa2_correctly_converted\n+                    if not model_class._supports_flex_attn\n+                    else model_from_pretrained.config._attn_implementation == \"flash_attention_2\"\n+                )\n                 self.assertFalse(fa2_correctly_converted)\n \n     def _get_custom_4d_mask_test_data(self):"
        },
        {
            "sha": "d67b026638e9721d2d979d1da9c28a6c70e7d110",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 27,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/373e50e9703024eeb09f97945fdefde80acc2619/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/373e50e9703024eeb09f97945fdefde80acc2619/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=373e50e9703024eeb09f97945fdefde80acc2619",
            "patch": "@@ -198,6 +198,7 @@ def test_static_cache_exportability(self):\n                 cache_config={\n                     \"batch_size\": batch_size,\n                     \"max_cache_len\": max_cache_len,\n+                    \"device\": device,\n                 },\n             ),\n         )\n@@ -310,11 +311,12 @@ def test_hybrid_cache_n_sequences(self):\n             do_sample=False,\n             max_new_tokens=20,\n             num_return_sequences=2,\n+            num_beams=2,\n         )\n         decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n         expected_text = [\n-            \"Hello I am doing a project on the 1918 flu pandemic and I am trying to find out how many\",\n-            \"Hello I am doing a project on the 1918 flu pandemic and I am trying to find out how many\",\n+            \"Hello I am doing a project for my school and I am trying to make a program that will allow me to input a\",\n+            \"Hello I am doing a project for my school and I am trying to make a program that will allow me to use a\",\n         ]\n         self.assertListEqual(decoded, expected_text)\n \n@@ -380,8 +382,6 @@ def test_sink_cache_iterative_prompts(self):\n         [\n             (\"eager\", \"static\"),\n             (\"sdpa\", \"static\"),\n-            (\"eager\", \"offloaded-static\"),\n-            (\"sdpa\", \"offloaded-static\"),\n         ]\n     )\n     def test_static_cache_greedy_decoding_pad_left(self, attn_implementation, cache_implementation):\n@@ -427,8 +427,6 @@ def test_static_cache_greedy_decoding_pad_left(self, attn_implementation, cache_\n         [\n             (\"eager\", \"static\"),\n             (\"sdpa\", \"static\"),\n-            (\"eager\", \"offloaded-static\"),\n-            (\"sdpa\", \"offloaded-static\"),\n         ]\n     )\n     def test_static_cache_greedy_decoding_pad_right(self, attn_implementation, cache_implementation):\n@@ -462,26 +460,6 @@ def test_static_cache_greedy_decoding_pad_right(self, attn_implementation, cache\n         with self.subTest(f\"{attn_implementation}, static, eager\"):\n             self.assertListEqual(decoded, EXPECTED_GENERATION)\n \n-        set_seed(0)\n-        model._forward = model.forward\n-        compiled_forward = torch.compile(model.forward)\n-\n-        def compiled(func, input_ids, **kwargs):\n-            return func(input_ids, **kwargs)\n-\n-        def call(input_ids, **kwargs):\n-            if input_ids.shape[-1] == 1:\n-                return compiled(compiled_forward, input_ids, **kwargs)\n-\n-            return model._forward(input_ids, **kwargs)\n-\n-        model.forward = call\n-\n-        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n-        decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n-        with self.subTest(f\"{attn_implementation}, static, compiled\"):\n-            self.assertListEqual(decoded, EXPECTED_GENERATION)\n-\n     def test_dynamic_cache_extra_left_padding(self):\n         \"\"\"Tests that adding extra left-padding does not affect the generation with the dynamic cache\"\"\"\n         EXPECTED_GENERATION = [\n@@ -519,7 +497,6 @@ def test_dynamic_cache_extra_left_padding(self):\n     @parameterized.expand(\n         [\n             \"static\",\n-            \"offloaded-static\",\n         ]\n     )\n     def test_static_cache_extra_left_padding(self, cache_implementation):"
        }
    ],
    "stats": {
        "total": 224,
        "additions": 112,
        "deletions": 112
    }
}