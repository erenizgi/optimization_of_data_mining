{
    "author": "zucchini-nlp",
    "message": "[ESM] support attention API (#40370)\n\n* ESM supports attention API\n\n* supports flags\n\n* fix tests\n\n* fix copiees\n\n* another fixup needed after fixing tests\n\n* fix tests and make sure Evolla copied everything\n\n* fix\n\n* order\n\n* forgot about \"is_causal\" for fa2\n\n* cross attention can't be causal",
    "sha": "ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4",
    "files": [
        {
            "sha": "aaea1614bb75303a42c494b24f5fe2cd7777b8b2",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 149,
            "deletions": 314,
            "changes": 463,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4",
            "patch": "@@ -16,14 +16,13 @@\n \"\"\"PyTorch ESM model.\"\"\"\n \n import math\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n import torch.utils.checkpoint\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithCrossAttentions,\n@@ -32,15 +31,18 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import auto_docstring, can_return_tuple, logging\n+from ...modeling_utils import (\n+    ALL_ATTENTION_FUNCTIONS,\n+    PreTrainedModel,\n+    find_pruneable_heads_and_indices,\n+    prune_linear_layer,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_esm import EsmConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -219,7 +221,7 @@ def forward(\n         if self.token_dropout and input_ids is not None:\n             embeddings = embeddings.masked_fill((input_ids == self.mask_token_id).unsqueeze(-1), 0.0)\n             mask_ratio_train = 0.15 * 0.8  # Hardcoded as the ratio used in all ESM model training runs\n-            src_lengths = attention_mask.sum(-1)\n+            src_lengths = attention_mask.sum(-1) if attention_mask is not None else input_ids.shape[1]\n             mask_ratio_observed = (input_ids == self.mask_token_id).sum(-1).float() / src_lengths\n             embeddings = (embeddings * (1 - mask_ratio_train) / (1 - mask_ratio_observed)[:, None, None]).to(\n                 embeddings.dtype\n@@ -255,8 +257,58 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n         return position_ids.unsqueeze(0).expand(input_shape)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    # ESM applies relative position embeddings and we don't copy from Llama\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n+    if hasattr(module, \"position_embedding_type\") and module.position_embedding_type in [\n+        \"relative_key\",\n+        \"relative_key_query\",\n+    ]:\n+        seq_length = query.shape[2]\n+        position_ids_l = torch.arange(seq_length, dtype=torch.long, device=attn_weights.device).view(-1, 1)\n+        position_ids_r = torch.arange(seq_length, dtype=torch.long, device=attn_weights.device).view(1, -1)\n+        distance = position_ids_l - position_ids_r\n+        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n+        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n+\n+        if module.position_embedding_type == \"relative_key\":\n+            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+        elif module.position_embedding_type == \"relative_key_query\":\n+            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n+            relative_position_scores = relative_position_scores_query + relative_position_scores_key\n+\n+        attn_weights = attn_weights + relative_position_scores\n+\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class EsmSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.config = config\n \n@@ -274,7 +326,7 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.key = nn.Linear(config.hidden_size, self.all_head_size)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.dropout = config.attention_probs_dropout_prob\n         self.position_embedding_type = position_embedding_type or getattr(\n             config, \"position_embedding_type\", \"absolute\"\n         )\n@@ -285,8 +337,10 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         elif self.position_embedding_type == \"rotary\":\n             self.rotary_embeddings = RotaryEmbedding(dim=self.attention_head_size)\n \n+        self.scaling = 1.0  # For BC we apply scaling before RoPE\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n+        self.is_causal = self.is_decoder and not is_cross_attention\n \n     def forward(\n         self,\n@@ -295,24 +349,18 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        hidden_shape = (hidden_states.shape[0], -1, self.num_attention_heads, self.attention_head_size)\n+        batch_size, seq_length = hidden_states.shape[:-1]\n+        hidden_shape = (batch_size, seq_length, -1, self.attention_head_size)\n \n         query_layer = self.query(hidden_states).view(hidden_shape).transpose(1, 2)\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-\n-        if is_cross_attention:\n-            key_layer = self.key(encoder_hidden_states).view(hidden_shape).transpose(1, 2)\n-            value_layer = self.value(encoder_hidden_states).view(hidden_shape).transpose(1, 2)\n-            attention_mask = encoder_attention_mask\n-        else:\n-            key_layer = self.key(hidden_states).view(hidden_shape).transpose(1, 2)\n-            value_layer = self.value(hidden_states).view(hidden_shape).transpose(1, 2)\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+        key_layer = self.key(current_states).view(hidden_shape).transpose(1, 2)\n+        value_layer = self.value(current_states).view(hidden_shape).transpose(1, 2)\n \n         # Matt: Our BERT model (which this code was derived from) scales attention logits down by sqrt(head_dim).\n         # ESM scales the query down by the same factor instead. Modulo numerical stability these are equivalent,\n@@ -323,51 +371,29 @@ def forward(\n         if self.position_embedding_type == \"rotary\":\n             query_layer, key_layer = self.rotary_embeddings(query_layer, key_layer)\n \n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            seq_length = hidden_states.size()[1]\n-            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in EsmModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n-        context_layer = torch.matmul(attention_probs.to(value_layer.dtype), value_layer)\n-\n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type in [\"relative_key\", \"relative_key_query\"]:\n+                raise ValueError(\n+                    f\"ESM {self.config._attn_implementation} attention does not support {self.position_embedding_type} embeddings. \"\n+                    \"Set attention explicitly to 'eager' with `model.set_attn_implementation('eager')`\"\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            **kwargs,\n+        )\n \n-        if self.is_decoder:\n-            outputs = outputs + (None,)\n-        return outputs\n+        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()\n+        return attn_output, attn_weights\n \n \n class EsmSelfOutput(nn.Module):\n@@ -383,129 +409,10 @@ def forward(self, hidden_states, input_tensor):\n         return hidden_states\n \n \n-class EsmFlashAttention2(EsmSelfAttention):\n-    \"\"\"\n-    ESM flash attention module. This module inherits from `EsmSelfAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n-        super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-        self.dropout_prob = config.attention_probs_dropout_prob\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n-        # Flash attention doesn't support output_attentions or cross attention\n-        if output_attentions or head_mask is not None or encoder_hidden_states is not None:\n-            logger.warning_once(\n-                \"EsmFlashAttention2 does not support output_attentions, head_mask, or cross_attention. \"\n-                \"Falling back to the manual attention implementation. This warning can be removed using \"\n-                'the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                output_attentions,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        hidden_shape = (hidden_states.shape[0], -1, self.num_attention_heads, self.attention_head_size)\n-        query_layer = self.query(hidden_states).view(hidden_shape).transpose(1, 2)\n-        key_layer = self.key(hidden_states).view(hidden_shape).transpose(1, 2)\n-        value_layer = self.value(hidden_states).view(hidden_shape).transpose(1, 2)\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32.\n-        input_dtype = query_layer.dtype\n-        device_type = query_layer.device.type if query_layer.device.type != \"mps\" else \"cpu\"\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = (\n-                    torch.get_autocast_dtype(device_type)\n-                    if hasattr(torch, \"get_autocast_dtype\")\n-                    else torch.get_autocast_gpu_dtype()\n-                )\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.query.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_layer = query_layer.to(target_dtype)\n-            key_layer = key_layer.to(target_dtype)\n-            value_layer = value_layer.to(target_dtype)\n-\n-        # Matt: Our BERT model (which this code was derived from) scales attention logits down by sqrt(head_dim).\n-        # ESM scales the query down by the same factor instead. Modulo numerical stability these are equivalent,\n-        # but not when rotary embeddings get involved. Therefore, we scale the query here to match the original\n-        # ESM code and fix rotary embeddings.\n-        query_layer = query_layer * self.attention_head_size**-0.5\n-\n-        if self.position_embedding_type == \"rotary\":\n-            query_layer, key_layer = self.rotary_embeddings(query_layer, key_layer)\n-        elif self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            raise ValueError(f\"ESM flash attention does not support {self.position_embedding_type} embeddings\")\n-\n-        # It would likely be faster to change self.transpose_for_scores to output the correct\n-        # dimensions for flash_attention_2, but that would also mean changing the rotary embedding\n-        # functions. Here we just permute the dimensions to match the expected input.\n-        attn_output = _flash_attention_forward(\n-            query_layer.permute(0, 2, 1, 3),\n-            key_layer.permute(0, 2, 1, 3),\n-            value_layer.permute(0, 2, 1, 3),\n-            attention_mask,\n-            query_length=q_len,\n-            is_causal=self.is_decoder,\n-            softmax_scale=1.0,\n-            dropout=self.dropout_prob if self.training else 0.0,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-\n-        outputs = (attn_output, None)\n-        if self.is_decoder:\n-            outputs = outputs + (None,)\n-\n-        return outputs\n-\n-\n-ESM_ATTENTION_CLASSES = {\n-    \"eager\": EsmSelfAttention,\n-    \"flash_attention_2\": EsmFlashAttention2,\n-}\n-\n-\n class EsmAttention(nn.Module):\n-    def __init__(self, config, layer_idx=None):\n+    def __init__(self, config, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n-        self.self = ESM_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx=layer_idx)\n+        self.self = EsmSelfAttention(config, layer_idx=layer_idx, is_cross_attention=is_cross_attention)\n         self.output = EsmSelfOutput(config)\n         self.pruned_heads = set()\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -535,21 +442,19 @@ def forward(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        output_attentions=False,\n-        cache_position=None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n         hidden_states_ln = self.LayerNorm(hidden_states)\n-        self_outputs = self.self(\n+        attn_output, _ = self.self(\n             hidden_states_ln,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attn_output = self.output(attn_output, hidden_states)\n+        return attn_output\n \n \n class EsmIntermediate(nn.Module):\n@@ -587,7 +492,7 @@ def __init__(self, config):\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise RuntimeError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = EsmAttention(config)\n+            self.crossattention = EsmAttention(config, is_cross_attention=True)\n         self.intermediate = EsmIntermediate(config)\n         self.output = EsmOutput(config)\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -599,22 +504,14 @@ def forward(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        output_attentions=False,\n-        cache_position=None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n-        self_attention_outputs = self.attention(\n+        attention_output = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n-        attention_output = self_attention_outputs[0]\n-\n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n@@ -623,25 +520,17 @@ def forward(\n                     \" with cross-attention layers by setting `config.add_cross_attention=True`\"\n                 )\n \n-            cross_attention_outputs = self.crossattention(\n+            attention_output = self.crossattention(\n                 attention_output,\n                 attention_mask=attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n-                output_attentions=output_attentions,\n+                **kwargs,\n             )\n-            attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n \n         layer_output = self.feed_forward_chunk(attention_output)\n-\n-        outputs = (layer_output,) + outputs\n-\n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (None,)\n-        return outputs\n+        return layer_output\n \n     def feed_forward_chunk(self, attention_output):\n         attention_output_ln = self.LayerNorm(attention_output)\n@@ -666,48 +555,23 @@ def forward(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        output_attentions=False,\n-        output_hidden_states=False,\n-        return_dict=True,\n-        cache_position=None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n-\n         for i, layer_module in enumerate(self.layer):\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n-            layer_outputs = layer_module(\n-                hidden_states=hidden_states,\n+            hidden_states = layer_module(\n+                hidden_states,\n                 attention_mask=attention_mask,\n                 head_mask=layer_head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n-                output_attentions=output_attentions,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n-\n         if self.emb_layer_norm_after:\n             hidden_states = self.emb_layer_norm_after(hidden_states)\n \n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        return BaseModelOutputWithCrossAttentions(\n-            last_hidden_state=hidden_states,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-            cross_attentions=all_cross_attentions,\n-        )\n+        return BaseModelOutputWithCrossAttentions(last_hidden_state=hidden_states)\n \n \n # Copied from transformers.models.bert.modeling_bert.BertPooler\n@@ -734,6 +598,17 @@ class EsmPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"EsmLayer\", \"EsmFoldTriangularSelfAttentionBlock\", \"EsmEmbeddings\"]\n     _keys_to_ignore_on_load_unexpected = [\"position_embeddings.weight\"]\n     _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+\n+    _can_record_outputs = {\n+        \"hidden_states\": EsmLayer,\n+        \"attentions\": [OutputRecorder(EsmSelfAttention, index=1, layer_name=\"attention\")],\n+        \"cross_attentions\": [\n+            OutputRecorder(EsmSelfAttention, index=1, layer_name=\"crossattention\"),\n+        ],\n+    }\n \n     # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights with BertLMPredictionHead->EsmLMHead\n     def _init_weights(self, module):\n@@ -808,7 +683,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -819,9 +694,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `((batch_size, sequence_length))`):\n@@ -841,43 +714,31 @@ def forward(\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix.\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        batch_size, seq_length = input_shape\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n-\n-        if attention_mask is None:\n-            attention_mask = torch.ones(((batch_size, seq_length)), device=device)\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embeddings(\n+                input_ids=input_ids,\n+                position_ids=position_ids,\n+            )\n \n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            extended_attention_mask = attention_mask\n+        if self.config._attn_implementation != \"flash_attention_2\":\n+            batch_size, seq_length = inputs_embeds.shape[:-1]\n+            if attention_mask is None:\n+                attention_mask = torch.ones(((batch_size, seq_length)), device=inputs_embeds.device)\n \n-        else:\n-            # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-            # ourselves in which case we just need to make it broadcastable to all heads.\n-            extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n+            attention_mask: torch.Tensor = self.get_extended_attention_mask(\n+                attention_mask, input_shape=(batch_size, seq_length)\n+            )\n \n         # If a 2D or 3D attention mask is provided for the cross-attention\n         # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n         if self.config.is_decoder and encoder_hidden_states is not None:\n             encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n             encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n             if encoder_attention_mask is None:\n-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n+                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=inputs_embeds.device)\n             encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n         else:\n             encoder_extended_attention_mask = None\n@@ -889,31 +750,20 @@ def forward(\n         # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n \n-        embedding_output = self.embeddings(\n-            input_ids=input_ids,\n-            position_ids=position_ids,\n-            attention_mask=attention_mask,\n-            inputs_embeds=inputs_embeds,\n-        )\n         encoder_outputs = self.encoder(\n-            embedding_output,\n-            attention_mask=extended_attention_mask,\n+            inputs_embeds,\n+            attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n+            **kwargs,\n         )\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-            cross_attentions=encoder_outputs.cross_attentions,\n         )\n \n     def predict_contacts(self, tokens, attention_mask):\n@@ -966,17 +816,14 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n             config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n             loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         outputs = self.esm(\n             input_ids,\n@@ -986,9 +833,7 @@ def forward(\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n+            **kwargs,\n         )\n         sequence_output = outputs[0]\n         prediction_scores = self.lm_head(sequence_output)\n@@ -1061,27 +906,22 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         outputs = self.esm(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n+            **kwargs,\n         )\n         sequence_output = outputs[0]\n         logits = self.classifier(sequence_output)\n@@ -1143,25 +983,20 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         outputs = self.esm(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]"
        },
        {
            "sha": "dbff29fade87b7a656da760beb8dfda4a6a14830",
            "filename": "src/transformers/models/esm/modeling_esmfold.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py?ref=ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4",
            "patch": "@@ -1991,6 +1991,10 @@ def distogram(coords, min_bin, max_bin, num_bins):\n class EsmForProteinFolding(EsmPreTrainedModel):\n     _no_split_modules = [\"EsmFoldStructureModule\", \"EsmFoldTriangularSelfAttentionBlock\"]\n     _supports_flash_attn = False\n+    _supports_sdpa = False\n+    _supports_attention_backend = False\n+\n+    _can_record_outputs = None\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "f6d0361e95fb996075f2c14e7531b14164d5545e",
            "filename": "src/transformers/models/evolla/configuration_evolla.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py?ref=ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4",
            "patch": "@@ -76,7 +76,6 @@ def __init__(\n         initializer_range=0.02,\n         layer_norm_eps=1e-05,\n         position_embedding_type=\"rotary\",\n-        use_cache=True,\n         emb_layer_norm_before=False,\n         token_dropout=True,\n         **kwargs,\n@@ -94,7 +93,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n         self.position_embedding_type = position_embedding_type\n-        self.use_cache = use_cache\n         self.emb_layer_norm_before = emb_layer_norm_before\n         self.token_dropout = token_dropout\n "
        },
        {
            "sha": "feb8cd07f603ad7911369102bead0b64583759d5",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 118,
            "deletions": 279,
            "changes": 397,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4",
            "patch": "@@ -32,7 +32,6 @@\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithCrossAttentions,\n@@ -51,19 +50,12 @@\n     prune_linear_layer,\n )\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.deprecation import deprecate_kwarg\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_evolla import EvollaConfig, SaProtConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n def create_position_ids_from_input_ids(input_ids, padding_idx):\n     \"\"\"\n     Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n@@ -141,7 +133,7 @@ def forward(\n         if self.token_dropout and input_ids is not None:\n             embeddings = embeddings.masked_fill((input_ids == self.mask_token_id).unsqueeze(-1), 0.0)\n             mask_ratio_train = 0.15 * 0.8  # Hardcoded as the ratio used in all EVOLLA_SA_PROT model training runs\n-            src_lengths = attention_mask.sum(-1)\n+            src_lengths = attention_mask.sum(-1) if attention_mask is not None else input_ids.shape[1]\n             mask_ratio_observed = (input_ids == self.mask_token_id).sum(-1).float() / src_lengths\n             embeddings = (embeddings * (1 - mask_ratio_train) / (1 - mask_ratio_observed)[:, None, None]).to(\n                 embeddings.dtype\n@@ -229,13 +221,63 @@ def forward(self, q: torch.Tensor, k: torch.Tensor) -> tuple[torch.Tensor, torch\n         self._cos_cached, self._sin_cached = self._update_cos_sin_tables(k, seq_dimension=-2)\n \n         return (\n-            apply_rotary_pos_emb_esm(q, self._cos_cached, self._sin_cached),\n-            apply_rotary_pos_emb_esm(k, self._cos_cached, self._sin_cached),\n+            apply_rotary_pos_emb_esm(q, self._cos_cached, self._sin_cached).to(dtype=q.dtype),\n+            apply_rotary_pos_emb_esm(k, self._cos_cached, self._sin_cached).to(dtype=k.dtype),\n         )\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    # EVOLLA_SA_PROT applies relative position embeddings and we don't copy from Llama\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n+    if hasattr(module, \"position_embedding_type\") and module.position_embedding_type in [\n+        \"relative_key\",\n+        \"relative_key_query\",\n+    ]:\n+        seq_length = query.shape[2]\n+        position_ids_l = torch.arange(seq_length, dtype=torch.long, device=attn_weights.device).view(-1, 1)\n+        position_ids_r = torch.arange(seq_length, dtype=torch.long, device=attn_weights.device).view(1, -1)\n+        distance = position_ids_l - position_ids_r\n+        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n+        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n+\n+        if module.position_embedding_type == \"relative_key\":\n+            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+        elif module.position_embedding_type == \"relative_key_query\":\n+            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n+            relative_position_scores = relative_position_scores_query + relative_position_scores_key\n+\n+        attn_weights = attn_weights + relative_position_scores\n+\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class EvollaSaProtSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.config = config\n \n@@ -253,7 +295,7 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.key = nn.Linear(config.hidden_size, self.all_head_size)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.dropout = config.attention_probs_dropout_prob\n         self.position_embedding_type = position_embedding_type or getattr(\n             config, \"position_embedding_type\", \"absolute\"\n         )\n@@ -266,6 +308,8 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n \n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n+        self.scaling = 1.0\n+        self.is_causal = self.is_decoder and not is_cross_attention\n \n     def forward(\n         self,\n@@ -274,24 +318,18 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        hidden_shape = (hidden_states.shape[0], -1, self.num_attention_heads, self.attention_head_size)\n+        batch_size, seq_length = hidden_states.shape[:-1]\n+        hidden_shape = (batch_size, seq_length, -1, self.attention_head_size)\n \n         query_layer = self.query(hidden_states).view(hidden_shape).transpose(1, 2)\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-\n-        if is_cross_attention:\n-            key_layer = self.key(encoder_hidden_states).view(hidden_shape).transpose(1, 2)\n-            value_layer = self.value(encoder_hidden_states).view(hidden_shape).transpose(1, 2)\n-            attention_mask = encoder_attention_mask\n-        else:\n-            key_layer = self.key(hidden_states).view(hidden_shape).transpose(1, 2)\n-            value_layer = self.value(hidden_states).view(hidden_shape).transpose(1, 2)\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+        key_layer = self.key(current_states).view(hidden_shape).transpose(1, 2)\n+        value_layer = self.value(current_states).view(hidden_shape).transpose(1, 2)\n \n         # Matt: Our BERT model (which this code was derived from) scales attention logits down by sqrt(head_dim).\n         # EVOLLA_SA_PROT scales the query down by the same factor instead. Modulo numerical stability these are equivalent,\n@@ -302,51 +340,29 @@ def forward(\n         if self.position_embedding_type == \"rotary\":\n             query_layer, key_layer = self.rotary_embeddings(query_layer, key_layer)\n \n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            seq_length = hidden_states.size()[1]\n-            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in EvollaSaProtModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n-        context_layer = torch.matmul(attention_probs.to(value_layer.dtype), value_layer)\n-\n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type in [\"relative_key\", \"relative_key_query\"]:\n+                raise ValueError(\n+                    f\"ESM {self.config._attn_implementation} attention does not support {self.position_embedding_type} embeddings. \"\n+                    \"Set attention explicitly to 'eager' with `model.set_attn_implementation('eager')`\"\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            **kwargs,\n+        )\n \n-        if self.is_decoder:\n-            outputs = outputs + (None,)\n-        return outputs\n+        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()\n+        return attn_output, attn_weights\n \n \n class EvollaSaProtSelfOutput(nn.Module):\n@@ -362,129 +378,10 @@ def forward(self, hidden_states, input_tensor):\n         return hidden_states\n \n \n-class EvollaSaProtFlashAttention2(EvollaSaProtSelfAttention):\n-    \"\"\"\n-    EVOLLA_SA_PROT flash attention module. This module inherits from `EvollaSaProtSelfAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n-        super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-        self.dropout_prob = config.attention_probs_dropout_prob\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n-        # Flash attention doesn't support output_attentions or cross attention\n-        if output_attentions or head_mask is not None or encoder_hidden_states is not None:\n-            logger.warning_once(\n-                \"EvollaSaProtFlashAttention2 does not support output_attentions, head_mask, or cross_attention. \"\n-                \"Falling back to the manual attention implementation. This warning can be removed using \"\n-                'the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                output_attentions,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        hidden_shape = (hidden_states.shape[0], -1, self.num_attention_heads, self.attention_head_size)\n-        query_layer = self.query(hidden_states).view(hidden_shape).transpose(1, 2)\n-        key_layer = self.key(hidden_states).view(hidden_shape).transpose(1, 2)\n-        value_layer = self.value(hidden_states).view(hidden_shape).transpose(1, 2)\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32.\n-        input_dtype = query_layer.dtype\n-        device_type = query_layer.device.type if query_layer.device.type != \"mps\" else \"cpu\"\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = (\n-                    torch.get_autocast_dtype(device_type)\n-                    if hasattr(torch, \"get_autocast_dtype\")\n-                    else torch.get_autocast_gpu_dtype()\n-                )\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.query.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_layer = query_layer.to(target_dtype)\n-            key_layer = key_layer.to(target_dtype)\n-            value_layer = value_layer.to(target_dtype)\n-\n-        # Matt: Our BERT model (which this code was derived from) scales attention logits down by sqrt(head_dim).\n-        # EVOLLA_SA_PROT scales the query down by the same factor instead. Modulo numerical stability these are equivalent,\n-        # but not when rotary embeddings get involved. Therefore, we scale the query here to match the original\n-        # EVOLLA_SA_PROT code and fix rotary embeddings.\n-        query_layer = query_layer * self.attention_head_size**-0.5\n-\n-        if self.position_embedding_type == \"rotary\":\n-            query_layer, key_layer = self.rotary_embeddings(query_layer, key_layer)\n-        elif self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            raise ValueError(f\"ESM flash attention does not support {self.position_embedding_type} embeddings\")\n-\n-        # It would likely be faster to change self.transpose_for_scores to output the correct\n-        # dimensions for flash_attention_2, but that would also mean changing the rotary embedding\n-        # functions. Here we just permute the dimensions to match the expected input.\n-        attn_output = _flash_attention_forward(\n-            query_layer.permute(0, 2, 1, 3),\n-            key_layer.permute(0, 2, 1, 3),\n-            value_layer.permute(0, 2, 1, 3),\n-            attention_mask,\n-            query_length=q_len,\n-            is_causal=self.is_decoder,\n-            softmax_scale=1.0,\n-            dropout=self.dropout_prob if self.training else 0.0,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-\n-        outputs = (attn_output, None)\n-        if self.is_decoder:\n-            outputs = outputs + (None,)\n-\n-        return outputs\n-\n-\n-EVOLLA_SA_PROT_ATTENTION_CLASSES = {\n-    \"eager\": EvollaSaProtSelfAttention,\n-    \"flash_attention_2\": EvollaSaProtFlashAttention2,\n-}\n-\n-\n class EvollaSaProtAttention(nn.Module):\n-    def __init__(self, config, layer_idx=None):\n+    def __init__(self, config, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n-        self.self = EVOLLA_SA_PROT_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx=layer_idx)\n+        self.self = EvollaSaProtSelfAttention(config, layer_idx=layer_idx, is_cross_attention=is_cross_attention)\n         self.output = EvollaSaProtSelfOutput(config)\n         self.pruned_heads = set()\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -514,21 +411,19 @@ def forward(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        output_attentions=False,\n-        cache_position=None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n         hidden_states_ln = self.LayerNorm(hidden_states)\n-        self_outputs = self.self(\n+        attn_output, _ = self.self(\n             hidden_states_ln,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attn_output = self.output(attn_output, hidden_states)\n+        return attn_output\n \n \n def gelu(x):\n@@ -573,7 +468,7 @@ def __init__(self, config):\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise RuntimeError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = EvollaSaProtAttention(config)\n+            self.crossattention = EvollaSaProtAttention(config, is_cross_attention=True)\n         self.intermediate = EvollaSaProtIntermediate(config)\n         self.output = EvollaSaProtOutput(config)\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -585,22 +480,14 @@ def forward(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        output_attentions=False,\n-        cache_position=None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n-        self_attention_outputs = self.attention(\n+        attention_output = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n-        attention_output = self_attention_outputs[0]\n-\n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n@@ -609,25 +496,17 @@ def forward(\n                     \" with cross-attention layers by setting `config.add_cross_attention=True`\"\n                 )\n \n-            cross_attention_outputs = self.crossattention(\n+            attention_output = self.crossattention(\n                 attention_output,\n                 attention_mask=attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n-                output_attentions=output_attentions,\n+                **kwargs,\n             )\n-            attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n \n         layer_output = self.feed_forward_chunk(attention_output)\n-\n-        outputs = (layer_output,) + outputs\n-\n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (None,)\n-        return outputs\n+        return layer_output\n \n     def feed_forward_chunk(self, attention_output):\n         attention_output_ln = self.LayerNorm(attention_output)\n@@ -652,48 +531,23 @@ def forward(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        output_attentions=False,\n-        output_hidden_states=False,\n-        return_dict=True,\n-        cache_position=None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n-\n         for i, layer_module in enumerate(self.layer):\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n-            layer_outputs = layer_module(\n-                hidden_states=hidden_states,\n+            hidden_states = layer_module(\n+                hidden_states,\n                 attention_mask=attention_mask,\n                 head_mask=layer_head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n-                output_attentions=output_attentions,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n-\n         if self.emb_layer_norm_after:\n             hidden_states = self.emb_layer_norm_after(hidden_states)\n \n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        return BaseModelOutputWithCrossAttentions(\n-            last_hidden_state=hidden_states,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-            cross_attentions=all_cross_attentions,\n-        )\n+        return BaseModelOutputWithCrossAttentions(last_hidden_state=hidden_states)\n \n \n class EvollaSaProtPooler(nn.Module):\n@@ -715,6 +569,17 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n class EvollaSaProtPreTrainedModel(PreTrainedModel):\n     config: SaProtConfig\n     _no_split_modules = [\"EvollaSaProtLayer\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_attention_backend = True\n+\n+    _can_record_outputs = {\n+        \"hidden_states\": EvollaSaProtLayer,\n+        \"attentions\": [OutputRecorder(EvollaSaProtSelfAttention, index=1, layer_name=\"attention\")],\n+        \"cross_attentions\": [\n+            OutputRecorder(EvollaSaProtSelfAttention, index=1, layer_name=\"crossattention\"),\n+        ],\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -752,7 +617,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @can_return_tuple\n+    @check_model_inputs\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor],\n@@ -1329,32 +1194,6 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n-def eager_attention_forward(\n-    module: nn.Module,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n-    dropout: float = 0.0,\n-    **kwargs: Unpack[TransformersKwargs],\n-):\n-    key_states = repeat_kv(key, module.num_key_value_groups)\n-    value_states = repeat_kv(value, module.num_key_value_groups)\n-\n-    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n-    if attention_mask is not None:\n-        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-        attn_weights = attn_weights + causal_mask\n-\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n-    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n-    attn_output = torch.matmul(attn_weights, value_states)\n-    attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-    return attn_output, attn_weights\n-\n-\n class EvollaAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n "
        },
        {
            "sha": "630c99a382f9c1d6eda45e419ab0a89aab230c34",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 19,
            "deletions": 6,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4",
            "patch": "@@ -37,7 +37,7 @@\n     logging,\n )\n from ...utils.deprecation import deprecate_kwarg\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from ..esm.modeling_esm import (\n     EsmAttention,\n     EsmEmbeddings,\n@@ -122,13 +122,13 @@ def forward(self, q: torch.Tensor, k: torch.Tensor) -> tuple[torch.Tensor, torch\n         self._cos_cached, self._sin_cached = self._update_cos_sin_tables(k, seq_dimension=-2)\n \n         return (\n-            apply_rotary_pos_emb_esm(q, self._cos_cached, self._sin_cached),\n-            apply_rotary_pos_emb_esm(k, self._cos_cached, self._sin_cached),\n+            apply_rotary_pos_emb_esm(q, self._cos_cached, self._sin_cached).to(dtype=q.dtype),\n+            apply_rotary_pos_emb_esm(k, self._cos_cached, self._sin_cached).to(dtype=k.dtype),\n         )\n \n \n class EvollaSaProtSelfAttention(EsmSelfAttention, nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None, is_cross_attention=False):\n         nn.Module.__init__(self)\n         self.config = config\n \n@@ -146,7 +146,7 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.key = nn.Linear(config.hidden_size, self.all_head_size)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.dropout = config.attention_probs_dropout_prob\n         self.position_embedding_type = position_embedding_type or getattr(\n             config, \"position_embedding_type\", \"absolute\"\n         )\n@@ -159,6 +159,8 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n \n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n+        self.scaling = 1.0\n+        self.is_causal = self.is_decoder and not is_cross_attention\n \n \n class EvollaSaProtSelfOutput(EsmSelfOutput):\n@@ -193,6 +195,17 @@ class EvollaSaProtPooler(EsmPooler):\n class EvollaSaProtPreTrainedModel(PreTrainedModel):\n     config: SaProtConfig\n     _no_split_modules = [\"EvollaSaProtLayer\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_attention_backend = True\n+\n+    _can_record_outputs = {\n+        \"hidden_states\": EvollaSaProtLayer,\n+        \"attentions\": [OutputRecorder(EvollaSaProtSelfAttention, index=1, layer_name=\"attention\")],\n+        \"cross_attentions\": [\n+            OutputRecorder(EvollaSaProtSelfAttention, index=1, layer_name=\"crossattention\"),\n+        ],\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -230,7 +243,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @can_return_tuple\n+    @check_model_inputs\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor],"
        },
        {
            "sha": "890df968bd0a2ff55babde6d6a609c535dc61ac1",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4",
            "patch": "@@ -974,22 +974,22 @@ def check_model_inputs(func):\n \n     @wraps(func)\n     def wrapper(self, *args, **kwargs):\n-        use_cache = kwargs.get(\"use_cache\")\n-        if use_cache is None:\n-            use_cache = getattr(self.config, \"use_cache\", False)\n+        use_cache = (\n+            kwargs[\"use_cache\"] if kwargs.get(\"use_cache\") is not None else getattr(self.config, \"use_cache\", None)\n+        )\n+        if use_cache is not None:\n+            if getattr(self, \"gradient_checkpointing\", False) and self.training and use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+                )\n+                use_cache = False\n+\n+            kwargs[\"use_cache\"] = use_cache\n \n         return_dict = kwargs.pop(\"return_dict\", None)\n         if return_dict is None:\n             return_dict = getattr(self.config, \"return_dict\", True)\n \n-        if getattr(self, \"gradient_checkpointing\", False) and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n-\n-        kwargs[\"use_cache\"] = use_cache\n-\n         all_args = kwargs.copy()\n         if \"kwargs\" in all_args:\n             for k, v in all_args[\"kwargs\"].items():"
        },
        {
            "sha": "72ef77c88c0a3840ba09d26b047fe70ddb6f3703",
            "filename": "tests/models/esm/test_modeling_esm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py?ref=ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4",
            "patch": "@@ -238,6 +238,7 @@ def test_model_various_embeddings(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n             config_and_inputs[0].position_embedding_type = type\n+            config_and_inputs[0]._attn_implementation = \"eager\"\n             self.model_tester.create_and_check_model(*config_and_inputs)\n \n     def test_for_masked_lm(self):"
        },
        {
            "sha": "242e578eaba8f5d729e0d21f57cb714a4179ebe1",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=ed5dd2999cb2ba93e9f30c0cc09ac045c2e379a4",
            "patch": "@@ -4391,7 +4391,6 @@ def test_flash_attention_2_continue_generate_with_position_ids(self):\n                 next_token_logits_from_generate = generation_out.logits[-1]\n \n                 # acceptable numerical instability\n-                # print(next_token_logits_from_generate, next_token_logits)\n                 tol = torch.finfo(torch.bfloat16).eps\n                 torch.testing.assert_close(next_token_logits_from_generate, next_token_logits, rtol=tol, atol=tol)\n "
        }
    ],
    "stats": {
        "total": 915,
        "additions": 302,
        "deletions": 613
    }
}