{
    "author": "vasqu",
    "message": "[`T5Gemma2`] Fix bidirectional mask for encoder (#42820)\n\n* fix\n\n* add test and fix copies\n\n* comments\n\n* fixup\n\n* fix docstrings",
    "sha": "33c948e494ecbfe5237000e9c8c1cc4b1fc848e6",
    "files": [
        {
            "sha": "8db715d3f09645fc37a42c329799e3ad2f22767d",
            "filename": "src/transformers/models/t5gemma2/configuration_t5gemma2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 42,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/33c948e494ecbfe5237000e9c8c1cc4b1fc848e6/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fconfiguration_t5gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33c948e494ecbfe5237000e9c8c1cc4b1fc848e6/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fconfiguration_t5gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fconfiguration_t5gemma2.py?ref=33c948e494ecbfe5237000e9c8c1cc4b1fc848e6",
            "patch": "@@ -32,9 +32,9 @@\n \n class T5Gemma2TextConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`T5Gemma2TextModel`]. It is used to instantiate an T5Gemma2Text\n-    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n-    defaults will yield a similar configuration to that of the T5Gemma2Text-7B.\n+    This is the configuration class to store the configuration of a [`T5Gemma2TextModel`]. It is used to instantiate the encoder's\n+    text model portion of the T5Gemma2 Model according to the specified arguments, defining the model architecture. Instantiating\n+    a configuration with the defaults will yield a similar configuration to that of the T5Gemma2Text-7B.\n     e.g. [google/t5gemma2_text-7b](https://huggingface.co/google/t5gemma2_text-7b)\n     Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PreTrainedConfig`] for more information.\n@@ -99,19 +99,6 @@ class T5Gemma2TextConfig(PreTrainedConfig):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n-        use_bidirectional_attention (`bool`, *optional*, defaults to `False`):\n-            If True, the model will attend to all text tokens instead of using a causal mask. This does not change\n-            behavior for vision tokens.\n-\n-    ```python\n-    >>> from transformers import T5Gemma2TextModel, T5Gemma2TextConfig\n-    >>> # Initializing a T5Gemma2Text t5gemma2_text-7b style configuration\n-    >>> configuration = T5Gemma2TextConfig()\n-    >>> # Initializing a model from the t5gemma2_text-7b style configuration\n-    >>> model = T5Gemma2TextModel(configuration)\n-    >>> # Accessing the model configuration\n-    >>> configuration = model.config\n-    ```\n     \"\"\"\n \n     model_type = \"t5gemma2_text\"\n@@ -158,7 +145,6 @@ def __init__(\n         final_logit_softcapping: Optional[float] = None,\n         attn_logit_softcapping: Optional[float] = None,\n         rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        use_bidirectional_attention: Optional[bool] = False,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -181,10 +167,6 @@ def __init__(\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n \n-        self.use_bidirectional_attention = use_bidirectional_attention\n-        if use_bidirectional_attention:\n-            self.sliding_window = (self.sliding_window // 2) + 1  # due to fa we set exclusive bounds\n-\n         # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n         self._sliding_window_pattern = kwargs.get(\"sliding_window_pattern\", 6)\n \n@@ -326,9 +308,9 @@ def __init__(\n \n class T5Gemma2DecoderConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`T5Gemma2DecoderModel`]. It is used to instantiate an T5Gemma2Decoder\n-    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n-    defaults will yield a similar configuration to that of the T5Gemma2Decoder-7B.\n+    This is the configuration class to store the configuration of a [`T5Gemma2DecoderModel`]. It is used to instantiate the decoder\n+    text model portion of the T5Gemma2 Model according to the specified arguments, defining the model architecture. Instantiating\n+    a configuration with the defaults will yield a similar configuration to that of the T5Gemma2Decoder-7B.\n     e.g. [google/t5gemma2_text-7b](https://huggingface.co/google/t5gemma2_text-7b)\n     Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PreTrainedConfig`] for more information.\n@@ -393,19 +375,6 @@ class T5Gemma2DecoderConfig(PreTrainedConfig):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n-        use_bidirectional_attention (`bool`, *optional*, defaults to `False`):\n-            If True, the model will attend to all text tokens instead of using a causal mask. This does not change\n-            behavior for vision tokens.\n-\n-    ```python\n-    >>> from transformers import T5Gemma2DecoderModel, T5Gemma2DecoderConfig\n-    >>> # Initializing a T5Gemma2Decoder t5gemma2_text-7b style configuration\n-    >>> configuration = T5Gemma2DecoderConfig()\n-    >>> # Initializing a model from the t5gemma2_text-7b style configuration\n-    >>> model = T5Gemma2DecoderModel(configuration)\n-    >>> # Accessing the model configuration\n-    >>> configuration = model.config\n-    ```\n     \"\"\"\n \n     model_type = \"t5gemma2_decoder\"\n@@ -452,7 +421,6 @@ def __init__(\n         final_logit_softcapping: Optional[float] = None,\n         attn_logit_softcapping: Optional[float] = None,\n         rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        use_bidirectional_attention: Optional[bool] = False,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -475,10 +443,6 @@ def __init__(\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n \n-        self.use_bidirectional_attention = use_bidirectional_attention\n-        if use_bidirectional_attention:\n-            self.sliding_window = (self.sliding_window // 2) + 1  # due to fa we set exclusive bounds\n-\n         # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n         self._sliding_window_pattern = kwargs.get(\"sliding_window_pattern\", 6)\n "
        },
        {
            "sha": "bd12b5aac15d30132c508ec18873185c87abac4d",
            "filename": "src/transformers/models/t5gemma2/modeling_t5gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/33c948e494ecbfe5237000e9c8c1cc4b1fc848e6/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33c948e494ecbfe5237000e9c8c1cc4b1fc848e6/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py?ref=33c948e494ecbfe5237000e9c8c1cc4b1fc848e6",
            "patch": "@@ -266,7 +266,7 @@ def __init__(self, config: T5Gemma2TextConfig, layer_idx: int):\n         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.scaling = config.query_pre_attn_scalar**-0.5\n         self.attention_dropout = self.config.attention_dropout\n-        self.is_causal = not self.config.use_bidirectional_attention\n+        self.is_causal = False  # Only used by the encoder\n \n         self.q_proj = nn.Linear(\n             config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n@@ -348,7 +348,7 @@ def __init__(self, config: T5Gemma2TextConfig, layer_idx: int):\n         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.scaling = config.query_pre_attn_scalar**-0.5\n         self.attention_dropout = self.config.attention_dropout\n-        self.is_causal = not self.config.use_bidirectional_attention\n+        self.is_causal = False  # Fused causal and encoder mask\n \n         self.q_proj = nn.Linear(\n             config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n@@ -446,7 +446,6 @@ def forward(\n             merged_attention_mask,\n             dropout=self.attention_dropout if self.training else 0.0,\n             scaling=self.scaling,\n-            is_causal=False,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "ce31cc40c1e70ab350b182d5b022f65a24d4e99e",
            "filename": "src/transformers/models/t5gemma2/modular_t5gemma2.py",
            "status": "modified",
            "additions": 280,
            "deletions": 4,
            "changes": 284,
            "blob_url": "https://github.com/huggingface/transformers/blob/33c948e494ecbfe5237000e9c8c1cc4b1fc848e6/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodular_t5gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33c948e494ecbfe5237000e9c8c1cc4b1fc848e6/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodular_t5gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodular_t5gemma2.py?ref=33c948e494ecbfe5237000e9c8c1cc4b1fc848e6",
            "patch": "@@ -22,7 +22,7 @@\n \n from ... import initialization as init\n from ...cache_utils import DynamicCache, EncoderDecoderCache, StaticCache\n-from ...configuration_utils import PreTrainedConfig\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...generation import GenerationConfig, GenerationMixin, GenerationMode\n from ...masking_utils import create_bidirectional_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -34,6 +34,7 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n+from ...modeling_rope_utils import RopeParameters\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -70,9 +71,146 @@\n logger = logging.get_logger(__name__)\n \n \n-class T5Gemma2TextConfig(Gemma3TextConfig):\n+class T5Gemma2TextConfig(Gemma3TextConfig, PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`T5Gemma2TextModel`]. It is used to instantiate the encoder's\n+    text model portion of the T5Gemma2 Model according to the specified arguments, defining the model architecture. Instantiating\n+    a configuration with the defaults will yield a similar configuration to that of the T5Gemma2Text-7B.\n+    e.g. [google/t5gemma2_text-7b](https://huggingface.co/google/t5gemma2_text-7b)\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 262208):\n+            Vocabulary size of the T5Gemma2Text model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`T5Gemma2TextModel`]\n+        hidden_size (`int`, *optional*, defaults to 2304):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 9216):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 26):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 4):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        head_dim (`int`, *optional*, defaults to 256):\n+            The attention head dimension.\n+        hidden_activation (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+            The non-linear activation function (function or string) in the decoder. Will default to `\"gelu_pytorch_tanh\"`\n+            if not specified. `\"gelu_pytorch_tanh\"` uses an approximation of the `\"gelu\"` activation function.\n+        max_position_embeddings (`int`, *optional*, defaults to 131072):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        eos_token_id (`int`, *optional*, defaults to 1):\n+            End of stream token id.\n+        bos_token_id (`int`, *optional*, defaults to 2):\n+            Beginning of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to tie weight embeddings\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        query_pre_attn_scalar (`float`, *optional*, defaults to 256):\n+            Scaling factor used on the attention scores\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            In T5Gemma2Text, every other layer uses sliding window attention. This is the size of the sliding window.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n+        final_logit_softcapping (`float`, *optional*):\n+            Scaling factor when applying tanh softcapping on the logits.\n+        attn_logit_softcapping (`float`, *optional*):\n+            Scaling factor when applying tanh softcapping on the attention scores.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n+    \"\"\"\n+\n     model_type = \"t5gemma2_text\"\n \n+    def __init__(\n+        self,\n+        vocab_size: Optional[int] = 262_208,\n+        hidden_size: Optional[int] = 2304,\n+        intermediate_size: Optional[int] = 9216,\n+        num_hidden_layers: Optional[int] = 26,\n+        num_attention_heads: Optional[int] = 8,\n+        num_key_value_heads: Optional[int] = 4,\n+        head_dim: Optional[int] = 256,\n+        hidden_activation: Optional[str] = \"gelu_pytorch_tanh\",\n+        max_position_embeddings: Optional[int] = 131_072,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 0,\n+        eos_token_id: Optional[int] = 1,\n+        bos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = True,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        query_pre_attn_scalar: Optional[int] = 256,\n+        sliding_window: Optional[int] = 4096,\n+        layer_types: Optional[list[str]] = None,\n+        final_logit_softcapping: Optional[float] = None,\n+        attn_logit_softcapping: Optional[float] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.head_dim = head_dim\n+        self.num_key_value_heads = num_key_value_heads\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.hidden_activation = hidden_activation\n+        self.query_pre_attn_scalar = query_pre_attn_scalar\n+        self.sliding_window = sliding_window\n+        self.final_logit_softcapping = final_logit_softcapping\n+        self.attn_logit_softcapping = attn_logit_softcapping\n+        self.layer_types = layer_types\n+\n+        # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n+        self._sliding_window_pattern = kwargs.get(\"sliding_window_pattern\", 6)\n+\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\" if bool((i + 1) % self._sliding_window_pattern) else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n+\n+        self.rope_parameters = rope_parameters\n+        PreTrainedConfig.__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n \n class T5Gemma2EncoderConfig(Gemma3Config):\n     model_type = \"t5gemma2_encoder\"\n@@ -83,9 +221,146 @@ class T5Gemma2EncoderConfig(Gemma3Config):\n     }\n \n \n-class T5Gemma2DecoderConfig(Gemma3TextConfig):\n+class T5Gemma2DecoderConfig(Gemma3TextConfig, PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`T5Gemma2DecoderModel`]. It is used to instantiate the decoder\n+    text model portion of the T5Gemma2 Model according to the specified arguments, defining the model architecture. Instantiating\n+    a configuration with the defaults will yield a similar configuration to that of the T5Gemma2Decoder-7B.\n+    e.g. [google/t5gemma2_text-7b](https://huggingface.co/google/t5gemma2_text-7b)\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 262208):\n+            Vocabulary size of the T5Gemma2Decoder model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`T5Gemma2DecoderModel`]\n+        hidden_size (`int`, *optional*, defaults to 2304):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 9216):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 26):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 4):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        head_dim (`int`, *optional*, defaults to 256):\n+            The attention head dimension.\n+        hidden_activation (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+            The non-linear activation function (function or string) in the decoder. Will default to `\"gelu_pytorch_tanh\"`\n+            if not specified. `\"gelu_pytorch_tanh\"` uses an approximation of the `\"gelu\"` activation function.\n+        max_position_embeddings (`int`, *optional*, defaults to 131072):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        eos_token_id (`int`, *optional*, defaults to 1):\n+            End of stream token id.\n+        bos_token_id (`int`, *optional*, defaults to 2):\n+            Beginning of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to tie weight embeddings\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        query_pre_attn_scalar (`float`, *optional*, defaults to 256):\n+            Scaling factor used on the attention scores\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            In T5Gemma2Decoder, every other layer uses sliding window attention. This is the size of the sliding window.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n+        final_logit_softcapping (`float`, *optional*):\n+            Scaling factor when applying tanh softcapping on the logits.\n+        attn_logit_softcapping (`float`, *optional*):\n+            Scaling factor when applying tanh softcapping on the attention scores.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n+    \"\"\"\n+\n     model_type = \"t5gemma2_decoder\"\n \n+    def __init__(\n+        self,\n+        vocab_size: Optional[int] = 262_208,\n+        hidden_size: Optional[int] = 2304,\n+        intermediate_size: Optional[int] = 9216,\n+        num_hidden_layers: Optional[int] = 26,\n+        num_attention_heads: Optional[int] = 8,\n+        num_key_value_heads: Optional[int] = 4,\n+        head_dim: Optional[int] = 256,\n+        hidden_activation: Optional[str] = \"gelu_pytorch_tanh\",\n+        max_position_embeddings: Optional[int] = 131_072,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 0,\n+        eos_token_id: Optional[int] = 1,\n+        bos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = True,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        query_pre_attn_scalar: Optional[int] = 256,\n+        sliding_window: Optional[int] = 4096,\n+        layer_types: Optional[list[str]] = None,\n+        final_logit_softcapping: Optional[float] = None,\n+        attn_logit_softcapping: Optional[float] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.head_dim = head_dim\n+        self.num_key_value_heads = num_key_value_heads\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.hidden_activation = hidden_activation\n+        self.query_pre_attn_scalar = query_pre_attn_scalar\n+        self.sliding_window = sliding_window\n+        self.final_logit_softcapping = final_logit_softcapping\n+        self.attn_logit_softcapping = attn_logit_softcapping\n+        self.layer_types = layer_types\n+\n+        # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n+        self._sliding_window_pattern = kwargs.get(\"sliding_window_pattern\", 6)\n+\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\" if bool((i + 1) % self._sliding_window_pattern) else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n+\n+        self.rope_parameters = rope_parameters\n+        PreTrainedConfig.__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n \n class T5Gemma2Config(PreTrainedConfig):\n     r\"\"\"\n@@ -257,13 +532,15 @@ def compute_default_rope_parameters(\n class T5Gemma2SelfAttention(Gemma3Attention):\n     def __init__(self, config: T5Gemma2TextConfig, layer_idx: int):\n         super().__init__(config, layer_idx)\n+        self.is_causal = False  # Only used by the encoder\n \n \n class T5Gemma2MergedAttention(Gemma3Attention):\n     \"\"\"Merged self-attention and cross-attention for decoder.\"\"\"\n \n     def __init__(self, config: T5Gemma2TextConfig, layer_idx: int):\n         super().__init__(config, layer_idx)\n+        self.is_causal = False  # Fused causal and encoder mask\n \n     def forward(\n         self,\n@@ -342,7 +619,6 @@ def forward(\n             merged_attention_mask,\n             dropout=self.attention_dropout if self.training else 0.0,\n             scaling=self.scaling,\n-            is_causal=False,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "de6fbc2c7699a763db07cd946a5b2d5b7ad6d028",
            "filename": "tests/models/t5gemma2/test_modeling_t5gemma2.py",
            "status": "modified",
            "additions": 48,
            "deletions": 0,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/33c948e494ecbfe5237000e9c8c1cc4b1fc848e6/tests%2Fmodels%2Ft5gemma2%2Ftest_modeling_t5gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33c948e494ecbfe5237000e9c8c1cc4b1fc848e6/tests%2Fmodels%2Ft5gemma2%2Ftest_modeling_t5gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma2%2Ftest_modeling_t5gemma2.py?ref=33c948e494ecbfe5237000e9c8c1cc4b1fc848e6",
            "patch": "@@ -106,6 +106,7 @@ def __init__(\n         hidden_dropout_prob=0.1,\n         attention_probs_dropout_prob=0.1,\n         max_position_embeddings=512,\n+        layer_types=[\"full_attention\", \"sliding_attention\"],\n         type_vocab_size=16,\n         type_sequence_label_size=2,\n         initializer_range=0.02,\n@@ -150,6 +151,7 @@ def __init__(\n         self.hidden_dropout_prob = hidden_dropout_prob\n         self.attention_probs_dropout_prob = attention_probs_dropout_prob\n         self.max_position_embeddings = max_position_embeddings\n+        self.layer_types = layer_types\n         self.type_vocab_size = type_vocab_size\n         self.type_sequence_label_size = type_sequence_label_size\n         self.initializer_range = initializer_range\n@@ -175,6 +177,7 @@ def get_encoder_config(self):\n                 hidden_dropout_prob=self.hidden_dropout_prob,\n                 attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n                 max_position_embeddings=self.max_position_embeddings,\n+                layer_types=self.layer_types,\n                 type_vocab_size=self.type_vocab_size,\n                 is_decoder=False,\n                 initializer_range=self.initializer_range,\n@@ -205,6 +208,7 @@ def get_decoder_config(self):\n             hidden_dropout_prob=self.hidden_dropout_prob,\n             attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n             max_position_embeddings=self.max_position_embeddings,\n+            layer_types=self.layer_types,\n             type_vocab_size=self.type_vocab_size,\n             is_decoder=True,\n             initializer_range=self.initializer_range,\n@@ -616,6 +620,46 @@ def create_and_check_model_fp16_forward(\n         )[\"last_hidden_state\"]\n         self.parent.assertFalse(torch.isnan(output).any().item())\n \n+    def create_and_create_and_check_forward_full_mask(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+        pixel_values,\n+    ):\n+        \"\"\"\n+        Checks whether we can use the shortcuts in our mask generation (SDPA) properly,\n+        these rely on the `is_causal` flag to function properly\n+        \"\"\"\n+        model = self.model_class(config=config).to(torch_device).eval()\n+\n+        # Force full mask (all true) which can be shortcircuited to `None`\n+        attention_mask = torch.ones_like(attention_mask)\n+        decoder_attention_mask = torch.ones_like(decoder_attention_mask)\n+\n+        output_full_mask = model(\n+            input_ids,\n+            pixel_values=pixel_values,\n+            decoder_input_ids=decoder_input_ids,\n+            attention_mask=attention_mask,\n+            decoder_attention_mask=decoder_attention_mask,\n+        )[\"last_hidden_state\"]\n+\n+        # Compile forces the mask creation to happen at any time\n+        model.forward = torch.compile(model.forward)\n+        output_full_mask_no_shortcut = model(\n+            input_ids,\n+            pixel_values=pixel_values,\n+            decoder_input_ids=decoder_input_ids,\n+            attention_mask=attention_mask,\n+            decoder_attention_mask=decoder_attention_mask,\n+        )[\"last_hidden_state\"]\n+\n+        self.parent.assertTrue(torch.allclose(output_full_mask, output_full_mask_no_shortcut, atol=1e-3, rtol=1e-3))\n+\n \n @require_torch\n class T5Gemma2ModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n@@ -724,6 +768,10 @@ def test_model_fp16_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model_fp16_forward(*config_and_inputs)\n \n+    def test_forward_full_mask(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_create_and_check_forward_full_mask(*config_and_inputs)\n+\n     # Based on tests.models.gemma.test_modeling_gemma.GemmaModelTest.test_Gemma_sequence_classification_model with Gemma -> T5Gemma2 (Add is_encoder_decoder option)\n     def test_T5Gemma2_sequence_classification_model(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        }
    ],
    "stats": {
        "total": 385,
        "additions": 336,
        "deletions": 49
    }
}