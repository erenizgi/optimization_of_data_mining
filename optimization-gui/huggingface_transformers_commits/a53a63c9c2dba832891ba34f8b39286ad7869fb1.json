{
    "author": "keetrap",
    "message": "Add Fast Mobilenet-V2 Processor (#37113)\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "a53a63c9c2dba832891ba34f8b39286ad7869fb1",
    "files": [
        {
            "sha": "ffe830ac8d91e89687ddff0f76fa6437bd2153a8",
            "filename": "docs/source/en/model_doc/mobilenet_v2.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a53a63c9c2dba832891ba34f8b39286ad7869fb1/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a53a63c9c2dba832891ba34f8b39286ad7869fb1/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md?ref=a53a63c9c2dba832891ba34f8b39286ad7869fb1",
            "patch": "@@ -84,6 +84,11 @@ If you're interested in submitting a resource to be included here, please feel f\n \n [[autodoc]] MobileNetV2ImageProcessor\n     - preprocess\n+\n+## MobileNetV2ImageProcessorFast\n+\n+[[autodoc]] MobileNetV2ImageProcessorFast\n+    - preprocess\n     - post_process_semantic_segmentation\n \n ## MobileNetV2Model"
        },
        {
            "sha": "3389261575cba1174801e8cc09bf677984e3cd42",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a53a63c9c2dba832891ba34f8b39286ad7869fb1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a53a63c9c2dba832891ba34f8b39286ad7869fb1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=a53a63c9c2dba832891ba34f8b39286ad7869fb1",
            "patch": "@@ -116,7 +116,7 @@\n             (\"mistral3\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n             (\"mllama\", (\"MllamaImageProcessor\",)),\n             (\"mobilenet_v1\", (\"MobileNetV1ImageProcessor\",)),\n-            (\"mobilenet_v2\", (\"MobileNetV2ImageProcessor\",)),\n+            (\"mobilenet_v2\", (\"MobileNetV2ImageProcessor\", \"MobileNetV2ImageProcessorFast\")),\n             (\"mobilevit\", (\"MobileViTImageProcessor\",)),\n             (\"mobilevitv2\", (\"MobileViTImageProcessor\",)),\n             (\"nat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),"
        },
        {
            "sha": "0a5dbc3ce4c84e179d028b297ba1d3b75c249c3c",
            "filename": "src/transformers/models/mobilenet_v2/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a53a63c9c2dba832891ba34f8b39286ad7869fb1/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a53a63c9c2dba832891ba34f8b39286ad7869fb1/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2F__init__.py?ref=a53a63c9c2dba832891ba34f8b39286ad7869fb1",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_mobilenet_v2 import *\n     from .feature_extraction_mobilenet_v2 import *\n     from .image_processing_mobilenet_v2 import *\n+    from .image_processing_mobilenet_v2_fast import *\n     from .modeling_mobilenet_v2 import *\n else:\n     import sys"
        },
        {
            "sha": "52421103b3ab8e65883108cc7fec6c86db512d74",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2_fast.py",
            "status": "added",
            "additions": 89,
            "deletions": 0,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/a53a63c9c2dba832891ba34f8b39286ad7869fb1/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a53a63c9c2dba832891ba34f8b39286ad7869fb1/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py?ref=a53a63c9c2dba832891ba34f8b39286ad7869fb1",
            "patch": "@@ -0,0 +1,89 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for MobileNetV2.\"\"\"\n+\n+from typing import List, Tuple\n+\n+from ...image_processing_utils_fast import BASE_IMAGE_PROCESSOR_FAST_DOCSTRING, BaseImageProcessorFast\n+from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, PILImageResampling\n+from ...utils import add_start_docstrings, is_torch_available, is_torch_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast MobileNetV2 image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+)\n+class MobileNetV2ImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"shortest_edge\": 256}\n+    default_to_square = False\n+    crop_size = {\"height\": 224, \"width\": 224}\n+    do_resize = True\n+    do_center_crop = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = None\n+\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: List[Tuple] = None):\n+        \"\"\"\n+        Converts the output of [`MobileNetV2ForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`MobileNetV2ForSemanticSegmentation`]):\n+                Raw outputs of the model.\n+            target_sizes (`List[Tuple]` of length `batch_size`, *optional*):\n+                List of tuples corresponding to the requested final size (height, width) of each prediction. If unset,\n+                predictions will not be resized.\n+\n+        Returns:\n+            semantic_segmentation: `List[torch.Tensor]` of length `batch_size`, where each item is a semantic\n+            segmentation map of shape (height, width) corresponding to the target_sizes entry (if `target_sizes` is\n+            specified). Each entry of each `torch.Tensor` correspond to a semantic class id.\n+        \"\"\"\n+        # TODO: add support for other frameworks\n+        logits = outputs.logits\n+\n+        # Resize logits and compute semantic segmentation maps\n+        if target_sizes is not None:\n+            if len(logits) != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+            if is_torch_tensor(target_sizes):\n+                target_sizes = target_sizes.numpy()\n+\n+            semantic_segmentation = []\n+\n+            for idx in range(len(logits)):\n+                resized_logits = torch.nn.functional.interpolate(\n+                    logits[idx].unsqueeze(dim=0), size=target_sizes[idx], mode=\"bilinear\", align_corners=False\n+                )\n+                semantic_map = resized_logits[0].argmax(dim=0)\n+                semantic_segmentation.append(semantic_map)\n+        else:\n+            semantic_segmentation = logits.argmax(dim=1)\n+            semantic_segmentation = [semantic_segmentation[i] for i in range(semantic_segmentation.shape[0])]\n+\n+        return semantic_segmentation\n+\n+\n+__all__ = [\"MobileNetV2ImageProcessorFast\"]"
        },
        {
            "sha": "526fe04738bf4bc653205168b828645746fe609a",
            "filename": "tests/models/mobilenet_v2/test_image_processing_mobilenet_v2.py",
            "status": "modified",
            "additions": 19,
            "deletions": 13,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/a53a63c9c2dba832891ba34f8b39286ad7869fb1/tests%2Fmodels%2Fmobilenet_v2%2Ftest_image_processing_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a53a63c9c2dba832891ba34f8b39286ad7869fb1/tests%2Fmodels%2Fmobilenet_v2%2Ftest_image_processing_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilenet_v2%2Ftest_image_processing_mobilenet_v2.py?ref=a53a63c9c2dba832891ba34f8b39286ad7869fb1",
            "patch": "@@ -16,14 +16,17 @@\n import unittest\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_vision_available\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n if is_vision_available():\n     from transformers import MobileNetV2ImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import MobileNetV2ImageProcessorFast\n+\n \n class MobileNetV2ImageProcessingTester:\n     def __init__(\n@@ -79,6 +82,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class MobileNetV2ImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = MobileNetV2ImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = MobileNetV2ImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -89,17 +93,19 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processor, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processor, \"size\"))\n-        self.assertTrue(hasattr(image_processor, \"do_center_crop\"))\n-        self.assertTrue(hasattr(image_processor, \"crop_size\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processor, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processor, \"size\"))\n+            self.assertTrue(hasattr(image_processor, \"do_center_crop\"))\n+            self.assertTrue(hasattr(image_processor, \"crop_size\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 20})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n-\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 20})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n+\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})"
        }
    ],
    "stats": {
        "total": 129,
        "additions": 115,
        "deletions": 14
    }
}