{
    "author": "SangbumChoi",
    "message": "Mask2former & Maskformer Fast Image Processor (#35685)\n\n* add maskformerfast\n\n* test\n\n* revert do_reduce_labels and add testing\n\n* make style & fix-copies\n\n* add mask2former and make fix-copies\nTO DO:\n\tadd test for mask2former\n\n* make fix-copies\n\n* fill docstring\n\n* enable mask2former fast processor\n\n* python utils/custom_init_isort.py\n\n* make fix-copies\n\n* fix PR's comments\n\n* modular file update\n\n* add license\n\n* make style\n\n* modular file\n\n* make fix-copies\n\n* merge\n\n* temp commit\n\n* finish up maskformer mask2former\n\n* remove zero shot examples\n\n---------\n\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "d9b35c635eb16e837a7b563c519c5231d4e09265",
    "files": [
        {
            "sha": "04968e27e348a586efdfc08e2276f51da31157b6",
            "filename": "docs/source/en/model_doc/mask2former.md",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d9b35c635eb16e837a7b563c519c5231d4e09265/docs%2Fsource%2Fen%2Fmodel_doc%2Fmask2former.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d9b35c635eb16e837a7b563c519c5231d4e09265/docs%2Fsource%2Fen%2Fmodel_doc%2Fmask2former.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmask2former.md?ref=d9b35c635eb16e837a7b563c519c5231d4e09265",
            "patch": "@@ -77,4 +77,12 @@ The resource should ideally demonstrate something new instead of duplicating an\n     - encode_inputs\n     - post_process_semantic_segmentation\n     - post_process_instance_segmentation\n+    - post_process_panoptic_segmentation\n+\n+## Mask2FormerImageProcessorFast\n+\n+[[autodoc]] Mask2FormerImageProcessorFast\n+    - preprocess\n+    - post_process_semantic_segmentation\n+    - post_process_instance_segmentation\n     - post_process_panoptic_segmentation\n\\ No newline at end of file"
        },
        {
            "sha": "cd84cd9ffd2731e3aecdfe0e6dbf079e3af604cc",
            "filename": "docs/source/en/model_doc/maskformer.md",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d9b35c635eb16e837a7b563c519c5231d4e09265/docs%2Fsource%2Fen%2Fmodel_doc%2Fmaskformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d9b35c635eb16e837a7b563c519c5231d4e09265/docs%2Fsource%2Fen%2Fmodel_doc%2Fmaskformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmaskformer.md?ref=d9b35c635eb16e837a7b563c519c5231d4e09265",
            "patch": "@@ -76,6 +76,14 @@ This model was contributed by [francesco](https://huggingface.co/francesco). The\n     - post_process_instance_segmentation\n     - post_process_panoptic_segmentation\n \n+## MaskFormerImageProcessorFast\n+\n+[[autodoc]] MaskFormerImageProcessorFast\n+    - preprocess\n+    - post_process_semantic_segmentation\n+    - post_process_instance_segmentation\n+    - post_process_panoptic_segmentation\n+\n ## MaskFormerFeatureExtractor\n \n [[autodoc]] MaskFormerFeatureExtractor"
        },
        {
            "sha": "f169cf413a6f2e8b9d5c026fa2474b70aaf851cd",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d9b35c635eb16e837a7b563c519c5231d4e09265/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d9b35c635eb16e837a7b563c519c5231d4e09265/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=d9b35c635eb16e837a7b563c519c5231d4e09265",
            "patch": "@@ -118,8 +118,8 @@\n             (\"llava_next\", (\"LlavaNextImageProcessor\", \"LlavaNextImageProcessorFast\")),\n             (\"llava_next_video\", (\"LlavaNextVideoImageProcessor\",)),\n             (\"llava_onevision\", (\"LlavaOnevisionImageProcessor\", \"LlavaOnevisionImageProcessorFast\")),\n-            (\"mask2former\", (\"Mask2FormerImageProcessor\",)),\n-            (\"maskformer\", (\"MaskFormerImageProcessor\",)),\n+            (\"mask2former\", (\"Mask2FormerImageProcessor\", \"Mask2FormerImageProcessorFast\")),\n+            (\"maskformer\", (\"MaskFormerImageProcessor\", \"MaskFormerImageProcessorFast\")),\n             (\"mgp-str\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"mistral3\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n             (\"mlcd\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),"
        },
        {
            "sha": "a4281e77dfdc571012bc4b9e1052832966ad3712",
            "filename": "src/transformers/models/mask2former/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d9b35c635eb16e837a7b563c519c5231d4e09265/src%2Ftransformers%2Fmodels%2Fmask2former%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d9b35c635eb16e837a7b563c519c5231d4e09265/src%2Ftransformers%2Fmodels%2Fmask2former%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2F__init__.py?ref=d9b35c635eb16e837a7b563c519c5231d4e09265",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_mask2former import *\n     from .image_processing_mask2former import *\n+    from .image_processing_mask2former_fast import *\n     from .modeling_mask2former import *\n else:\n     import sys"
        },
        {
            "sha": "20610e7c7744071feb66584d7ce90f6beb418e15",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former.py",
            "status": "modified",
            "additions": 73,
            "deletions": 9,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/d9b35c635eb16e837a7b563c519c5231d4e09265/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d9b35c635eb16e837a7b563c519c5231d4e09265/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py?ref=d9b35c635eb16e837a7b563c519c5231d4e09265",
            "patch": "@@ -35,8 +35,8 @@\n     PILImageResampling,\n     get_image_size,\n     infer_channel_dimension_format,\n-    is_batched,\n     is_scaled_image,\n+    make_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -61,6 +61,46 @@\n     from torch import nn\n \n \n+# Copied from transformers.models.detr.image_processing_detr.get_size_with_aspect_ratio\n+def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n+    \"\"\"\n+    Computes the output image size given the input image size and the desired output size.\n+\n+    Args:\n+        image_size (`tuple[int, int]`):\n+            The input image size.\n+        size (`int`):\n+            The desired output size.\n+        max_size (`int`, *optional*):\n+            The maximum allowed output size.\n+    \"\"\"\n+    height, width = image_size\n+    raw_size = None\n+    if max_size is not None:\n+        min_original_size = float(min((height, width)))\n+        max_original_size = float(max((height, width)))\n+        if max_original_size / min_original_size * size > max_size:\n+            raw_size = max_size * min_original_size / max_original_size\n+            size = int(round(raw_size))\n+\n+    if (height <= width and height == size) or (width <= height and width == size):\n+        oh, ow = height, width\n+    elif width < height:\n+        ow = size\n+        if max_size is not None and raw_size is not None:\n+            oh = int(raw_size * height / width)\n+        else:\n+            oh = int(size * height / width)\n+    else:\n+        oh = size\n+        if max_size is not None and raw_size is not None:\n+            ow = int(raw_size * width / height)\n+        else:\n+            ow = int(size * width / height)\n+\n+    return (oh, ow)\n+\n+\n # Copied from transformers.models.detr.image_processing_detr.max_across_indices\n def max_across_indices(values: Iterable[Any]) -> list[Any]:\n     \"\"\"\n@@ -394,6 +434,10 @@ class Mask2FormerImageProcessor(BaseImageProcessor):\n             The background label will be replaced by `ignore_index`.\n         num_labels (`int`, *optional*):\n             The number of labels in the segmentation map.\n+        pad_size (`Dict[str, int]`, *optional*):\n+            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+            height and width in the batch.\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n@@ -416,6 +460,7 @@ def __init__(\n         ignore_index: Optional[int] = None,\n         do_reduce_labels: bool = False,\n         num_labels: Optional[int] = None,\n+        pad_size: Optional[dict[str, int]] = None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -439,6 +484,7 @@ def __init__(\n         self.ignore_index = ignore_index\n         self.do_reduce_labels = do_reduce_labels\n         self.num_labels = num_labels\n+        self.pad_size = pad_size\n \n     @classmethod\n     def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n@@ -697,6 +743,7 @@ def preprocess(\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        pad_size: Optional[dict[str, int]] = None,\n     ) -> BatchFeature:\n         do_resize = do_resize if do_resize is not None else self.do_resize\n         size = size if size is not None else self.size\n@@ -710,6 +757,7 @@ def preprocess(\n         image_std = image_std if image_std is not None else self.image_std\n         ignore_index = ignore_index if ignore_index is not None else self.ignore_index\n         do_reduce_labels = do_reduce_labels if do_reduce_labels is not None else self.do_reduce_labels\n+        pad_size = self.pad_size if pad_size is None else pad_size\n \n         if not valid_images(images):\n             raise ValueError(\n@@ -734,9 +782,9 @@ def preprocess(\n                 \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n             )\n \n-        if not is_batched(images):\n-            images = [images]\n-            segmentation_maps = [segmentation_maps] if segmentation_maps is not None else None\n+        images = make_list_of_images(images)\n+        if segmentation_maps is not None:\n+            segmentation_maps = make_list_of_images(segmentation_maps, expected_ndims=2)\n \n         if segmentation_maps is not None and len(images) != len(segmentation_maps):\n             raise ValueError(\"Images and segmentation maps must have the same length.\")\n@@ -774,6 +822,7 @@ def preprocess(\n             do_reduce_labels,\n             return_tensors,\n             input_data_format=data_format,\n+            pad_size=pad_size,\n         )\n         return encoded_inputs\n \n@@ -805,7 +854,7 @@ def _pad_image(\n         )\n         return padded_image\n \n-    # Copied from transformers.models.vilt.image_processing_vilt.ViltImageProcessor.pad\n+    # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.pad\n     def pad(\n         self,\n         images: list[np.ndarray],\n@@ -814,6 +863,7 @@ def pad(\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        pad_size: Optional[dict[str, int]] = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n@@ -837,13 +887,21 @@ def pad(\n                 The channel dimension format of the image. If not provided, it will be the same as the input image.\n             input_data_format (`ChannelDimension` or `str`, *optional*):\n                 The channel dimension format of the input image. If not provided, it will be inferred.\n+            pad_size (`Dict[str, int]`, *optional*):\n+                The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+                provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+                height and width in the batch.\n         \"\"\"\n-        pad_size = get_max_height_width(images, input_data_format=input_data_format)\n+        pad_size = pad_size if pad_size is not None else self.pad_size\n+        if pad_size is not None:\n+            padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+        else:\n+            padded_size = get_max_height_width(images, input_data_format=input_data_format)\n \n         padded_images = [\n             self._pad_image(\n                 image,\n-                pad_size,\n+                padded_size,\n                 constant_values=constant_values,\n                 data_format=data_format,\n                 input_data_format=input_data_format,\n@@ -854,7 +912,7 @@ def pad(\n \n         if return_pixel_mask:\n             masks = [\n-                make_pixel_mask(image=image, output_size=pad_size, input_data_format=input_data_format)\n+                make_pixel_mask(image=image, output_size=padded_size, input_data_format=input_data_format)\n                 for image in images\n             ]\n             data[\"pixel_mask\"] = masks\n@@ -870,6 +928,7 @@ def encode_inputs(\n         do_reduce_labels: bool = False,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        pad_size: Optional[dict[str, int]] = None,\n     ):\n         \"\"\"\n         Pad images up to the largest image in a batch and create a corresponding `pixel_mask`.\n@@ -909,6 +968,11 @@ def encode_inputs(\n             input_data_format (`ChannelDimension` or `str`, *optional*):\n                 The channel dimension format of the input image. If not provided, it will be inferred.\n \n+            pad_size (`Dict[str, int]`, *optional*):\n+                The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+                provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+                height and width in the batch.\n+\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n \n@@ -930,7 +994,7 @@ def encode_inputs(\n             input_data_format = infer_channel_dimension_format(pixel_values_list[0])\n \n         encoded_inputs = self.pad(\n-            pixel_values_list, return_tensors=return_tensors, input_data_format=input_data_format\n+            pixel_values_list, return_tensors=return_tensors, input_data_format=input_data_format, pad_size=pad_size\n         )\n \n         if segmentation_maps is not None:"
        },
        {
            "sha": "1f61e9b0cdb911c0a9d596cc26d513ce33707e9e",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former_fast.py",
            "status": "added",
            "additions": 737,
            "deletions": 0,
            "changes": 737,
            "blob_url": "https://github.com/huggingface/transformers/blob/d9b35c635eb16e837a7b563c519c5231d4e09265/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d9b35c635eb16e837a7b563c519c5231d4e09265/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py?ref=d9b35c635eb16e837a7b563c519c5231d4e09265",
            "patch": "@@ -0,0 +1,737 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/mask2former/modular_mask2former.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_mask2former.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+from typing import Any, Optional, Union\n+\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    SizeDict,\n+    get_image_size_for_max_height_width,\n+    get_max_height_width,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    logging,\n+)\n+from ...utils.deprecation import deprecate_kwarg\n+from .image_processing_mask2former import (\n+    compute_segments,\n+    convert_segmentation_to_rle,\n+    get_size_with_aspect_ratio,\n+    remove_low_and_no_objects,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.transforms import functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Mask2FormerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    r\"\"\"\n+    size_divisor (`int`, *optional*, defaults to 32):\n+        Some backbones need images divisible by a certain number. If not passed, it defaults to the value used in\n+        Swin Transformer.\n+    ignore_index (`int`, *optional*):\n+        Label to be assigned to background pixels in segmentation maps. If provided, segmentation map pixels\n+        denoted with 0 (background) will be replaced with `ignore_index`.\n+    do_reduce_labels (`bool`, *optional*, defaults to `False`):\n+        Whether or not to decrement all label values of segmentation maps by 1. Usually used for datasets where 0\n+        is used for background, and background itself is not included in all classes of a dataset (e.g. ADE20k).\n+        The background label will be replaced by `ignore_index`.\n+    num_labels (`int`, *optional*):\n+        The number of labels in the segmentation map.\n+    do_pad (`bool`, *optional*, defaults to `True`):\n+        Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n+        method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n+        If `pad_size` is provided, the image will be padded to the specified dimensions.\n+        Otherwise, the image will be padded to the maximum height and width of the batch.\n+    pad_size (`Dict[str, int]`, *optional*):\n+        The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+        provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+        height and width in the batch.\n+    \"\"\"\n+\n+    size_divisor: Optional[int]\n+    ignore_index: Optional[int]\n+    do_reduce_labels: Optional[bool]\n+    num_labels: Optional[int]\n+    do_pad: Optional[bool]\n+    pad_size: Optional[dict[str, int]]\n+\n+\n+def convert_segmentation_map_to_binary_masks_fast(\n+    segmentation_map: \"torch.Tensor\",\n+    instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n+    ignore_index: Optional[int] = None,\n+    do_reduce_labels: bool = False,\n+):\n+    if do_reduce_labels and ignore_index is None:\n+        raise ValueError(\"If `do_reduce_labels` is True, `ignore_index` must be provided.\")\n+\n+    if do_reduce_labels:\n+        segmentation_map = torch.where(segmentation_map == 0, ignore_index, segmentation_map - 1)\n+\n+    all_labels = torch.unique(segmentation_map)\n+\n+    if ignore_index is not None:\n+        all_labels = all_labels[all_labels != ignore_index]  # drop background label if applicable\n+\n+    binary_masks = [(segmentation_map == i) for i in all_labels]\n+    if binary_masks:\n+        binary_masks = torch.stack(binary_masks, dim=0)\n+    else:\n+        binary_masks = torch.zeros((0, *segmentation_map.shape), device=segmentation_map.device)\n+\n+    # Convert instance ids to class ids\n+    if instance_id_to_semantic_id is not None:\n+        labels = torch.zeros(all_labels.shape[0], device=segmentation_map.device)\n+\n+        for i, label in enumerate(all_labels):\n+            class_id = instance_id_to_semantic_id[(label.item() + 1 if do_reduce_labels else label.item())]\n+            labels[i] = class_id - 1 if do_reduce_labels else class_id\n+    else:\n+        labels = all_labels\n+    return binary_masks.float(), labels.long()\n+\n+\n+@auto_docstring\n+class Mask2FormerImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"shortest_edge\": 800, \"longest_edge\": 1333}\n+    default_to_square = False\n+    do_resize = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    do_normalize = True\n+    do_pad = True\n+    model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+    size_divisor = 32\n+    do_reduce_labels = False\n+    valid_kwargs = Mask2FormerFastImageProcessorKwargs\n+\n+    @deprecate_kwarg(\"reduce_labels\", new_name=\"do_reduce_labels\", version=\"4.44.0\")\n+    @deprecate_kwarg(\"size_divisibility\", new_name=\"size_divisor\", version=\"4.41.0\")\n+    @deprecate_kwarg(\"max_size\", version=\"4.27.0\", warn_if_greater_or_equal_version=True)\n+    def __init__(self, **kwargs: Unpack[Mask2FormerFastImageProcessorKwargs]) -> None:\n+        if \"pad_and_return_pixel_mask\" in kwargs:\n+            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+\n+        size = kwargs.pop(\"size\", None)\n+        max_size = kwargs.pop(\"max_size\", None)\n+\n+        if size is None and max_size is not None:\n+            size = self.size\n+            size[\"longest_edge\"] = max_size\n+        elif size is None:\n+            size = self.size\n+\n+        self.size = get_size_dict(size, max_size=max_size, default_to_square=False)\n+\n+        super().__init__(**kwargs)\n+\n+    @classmethod\n+    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n+        \"\"\"\n+        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n+        created using from_dict and kwargs e.g. `Mask2FormerImageProcessor.from_pretrained(checkpoint, max_size=800)`\n+        \"\"\"\n+        image_processor_dict = image_processor_dict.copy()\n+        if \"max_size\" in kwargs:\n+            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n+        if \"size_divisibility\" in kwargs:\n+            image_processor_dict[\"size_divisor\"] = kwargs.pop(\"size_divisibility\")\n+        if \"reduce_labels\" in image_processor_dict:\n+            image_processor_dict[\"do_reduce_labels\"] = image_processor_dict.pop(\"reduce_labels\")\n+        return super().from_dict(image_processor_dict, **kwargs)\n+\n+    def to_dict(self) -> dict[str, Any]:\n+        \"\"\"\n+        Serializes this instance to a Python dictionary. This method calls the superclass method and then removes the\n+        `_max_size` attribute from the dictionary.\n+        \"\"\"\n+        image_processor_dict = super().to_dict()\n+        image_processor_dict.pop(\"_max_size\", None)\n+        return image_processor_dict\n+\n+    def reduce_label(self, labels: list[\"torch.Tensor\"]):\n+        for idx in range(len(labels)):\n+            label = labels[idx]\n+            label = torch.where(label == 0, torch.tensor(255, dtype=label.dtype), label)\n+            label = label - 1\n+            label = torch.where(label == 254, torch.tensor(255, dtype=label.dtype), label)\n+            labels[idx] = label\n+\n+    def resize(\n+        self,\n+        image: torch.Tensor,\n+        size: SizeDict,\n+        size_divisor: int = 0,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Resize the image to the given size. Size can be `min_size` (scalar) or `(height, width)` tuple. If size is an\n+        int, smaller edge of the image will be matched to this number.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Size of the image's `(height, width)` dimensions after resizing. Available options are:\n+                    - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n+                        Do NOT keep the aspect ratio.\n+                    - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n+                        the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n+                        less or equal to `longest_edge`.\n+                    - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n+                        aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n+                        `max_width`.\n+            size_divisor (`int`, *optional*, defaults to 0):\n+                If `size_divisor` is given, the output image size will be divisible by the number.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                Resampling filter to use if resizing the image.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+        if size.shortest_edge and size.longest_edge:\n+            # Resize the image so that the shortest edge or the longest edge is of the given size\n+            # while maintaining the aspect ratio of the original image.\n+            new_size = get_size_with_aspect_ratio(\n+                image.size()[-2:],\n+                size[\"shortest_edge\"],\n+                size[\"longest_edge\"],\n+            )\n+        elif size.max_height and size.max_width:\n+            new_size = get_image_size_for_max_height_width(image.size()[-2:], size[\"max_height\"], size[\"max_width\"])\n+        elif size.height and size.width:\n+            new_size = (size[\"height\"], size[\"width\"])\n+        else:\n+            raise ValueError(\n+                \"Size must contain 'height' and 'width' keys or 'shortest_edge' and 'longest_edge' keys. Got\"\n+                f\" {size.keys()}.\"\n+            )\n+        if size_divisor > 0:\n+            height, width = new_size\n+            height = int(math.ceil(height / size_divisor) * size_divisor)\n+            width = int(math.ceil(width / size_divisor) * size_divisor)\n+            new_size = (height, width)\n+\n+        image = F.resize(\n+            image,\n+            size=new_size,\n+            interpolation=interpolation,\n+            **kwargs,\n+        )\n+        return image\n+\n+    def pad(\n+        self,\n+        images: torch.Tensor,\n+        padded_size: tuple[int, int],\n+        segmentation_maps: Optional[torch.Tensor] = None,\n+        fill: int = 0,\n+        ignore_index: int = 255,\n+    ) -> BatchFeature:\n+        original_size = images.size()[-2:]\n+        padding_bottom = padded_size[0] - original_size[0]\n+        padding_right = padded_size[1] - original_size[1]\n+        if padding_bottom < 0 or padding_right < 0:\n+            raise ValueError(\n+                f\"Padding dimensions are negative. Please make sure that the padded size is larger than the \"\n+                f\"original size. Got padded size: {padded_size}, original size: {original_size}.\"\n+            )\n+        if original_size != padded_size:\n+            padding = [0, 0, padding_right, padding_bottom]\n+            images = F.pad(images, padding, fill=fill)\n+            if segmentation_maps is not None:\n+                segmentation_maps = F.pad(segmentation_maps, padding, fill=ignore_index)\n+\n+        # Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n+        pixel_mask = torch.zeros((images.shape[0], *padded_size), dtype=torch.int64, device=images.device)\n+        pixel_mask[:, : original_size[0], : original_size[1]] = 1\n+\n+        return images, pixel_mask, segmentation_maps\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n+        **kwargs: Unpack[Mask2FormerFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps.\n+        instance_id_to_semantic_id (`Union[list[dict[int, int]], dict[int, int]]`, *optional*):\n+            A mapping from instance IDs to semantic IDs.\n+        \"\"\"\n+        return super().preprocess(\n+            images,\n+            segmentation_maps,\n+            instance_id_to_semantic_id,\n+            **kwargs,\n+        )\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: ImageInput,\n+        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]],\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[Mask2FormerFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        To be overriden by subclasses when image-like inputs other than images should be processed.\n+        It can be used for segmentation maps, depth maps, etc.\n+        \"\"\"\n+        # Prepare input images\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        if segmentation_maps is not None:\n+            segmentation_maps = self._prepare_image_like_inputs(\n+                images=segmentation_maps,\n+                expected_ndims=2,\n+                do_convert_rgb=False,\n+                input_data_format=ChannelDimension.FIRST,\n+            )\n+        return self._preprocess(images, segmentation_maps, instance_id_to_semantic_id, **kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        segmentation_maps: Optional[\"torch.Tensor\"],\n+        instance_id_to_semantic_id: Optional[dict[int, int]],\n+        do_resize: Optional[bool],\n+        size: Optional[dict[str, int]],\n+        pad_size: Optional[dict[str, int]],\n+        size_divisor: Optional[int],\n+        interpolation: Optional[Union[\"PILImageResampling\", \"F.InterpolationMode\"]],\n+        do_rescale: Optional[bool],\n+        rescale_factor: Optional[float],\n+        do_normalize: Optional[bool],\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        ignore_index: Optional[int],\n+        do_reduce_labels: Optional[bool],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        if segmentation_maps is not None and len(images) != len(segmentation_maps):\n+            raise ValueError(\"Images and segmentation maps must have the same length.\")\n+\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+        if segmentation_maps is not None:\n+            grouped_segmentation_maps, grouped_segmentation_maps_index = group_images_by_shape(\n+                segmentation_maps, disable_grouping=disable_grouping\n+            )\n+            resized_segmentation_maps_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(\n+                    image=stacked_images, size=size, size_divisor=size_divisor, interpolation=interpolation\n+                )\n+                if segmentation_maps is not None:\n+                    stacked_segmentation_maps = self.resize(\n+                        image=grouped_segmentation_maps[shape],\n+                        size=size,\n+                        size_divisor=size_divisor,\n+                        interpolation=F.InterpolationMode.NEAREST_EXACT,\n+                    )\n+            resized_images_grouped[shape] = stacked_images\n+            if segmentation_maps is not None:\n+                resized_segmentation_maps_grouped[shape] = stacked_segmentation_maps\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+        if segmentation_maps is not None:\n+            resized_segmentation_maps = reorder_images(\n+                resized_segmentation_maps_grouped, grouped_segmentation_maps_index\n+            )\n+        if pad_size is not None:\n+            padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+        else:\n+            padded_size = get_max_height_width(resized_images)\n+\n+        if segmentation_maps is not None:\n+            mask_labels = []\n+            class_labels = []\n+            # Convert to list of binary masks and labels\n+            for idx, segmentation_map in enumerate(resized_segmentation_maps):\n+                if isinstance(instance_id_to_semantic_id, list):\n+                    instance_id = instance_id_to_semantic_id[idx]\n+                else:\n+                    instance_id = instance_id_to_semantic_id\n+                # Use instance2class_id mapping per image\n+                masks, classes = convert_segmentation_map_to_binary_masks_fast(\n+                    segmentation_map.squeeze(0),\n+                    instance_id,\n+                    ignore_index=ignore_index,\n+                    do_reduce_labels=do_reduce_labels,\n+                )\n+                mask_labels.append(masks)\n+                class_labels.append(classes)\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        processed_pixel_masks_grouped = {}\n+        if segmentation_maps is not None:\n+            grouped_segmentation_maps, grouped_segmentation_maps_index = group_images_by_shape(\n+                mask_labels, disable_grouping=disable_grouping\n+            )\n+            processed_segmentation_maps_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            padded_images, pixel_masks, padded_segmentation_maps = self.pad(\n+                images=stacked_images,\n+                segmentation_maps=grouped_segmentation_maps[shape] if segmentation_maps is not None else None,\n+                padded_size=padded_size,\n+                ignore_index=ignore_index,\n+            )\n+            processed_images_grouped[shape] = padded_images\n+            processed_pixel_masks_grouped[shape] = pixel_masks\n+            if segmentation_maps is not None:\n+                processed_segmentation_maps_grouped[shape] = padded_segmentation_maps.squeeze(1)\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_pixel_masks = reorder_images(processed_pixel_masks_grouped, grouped_images_index)\n+        encoded_inputs = BatchFeature(\n+            data={\n+                \"pixel_values\": torch.stack(processed_images, dim=0) if return_tensors else processed_images,\n+                \"pixel_mask\": torch.stack(processed_pixel_masks, dim=0) if return_tensors else processed_pixel_masks,\n+            },\n+            tensor_type=return_tensors,\n+        )\n+        if segmentation_maps is not None:\n+            mask_labels = reorder_images(processed_segmentation_maps_grouped, grouped_segmentation_maps_index)\n+            # we cannot batch them since they don't share a common class size\n+            encoded_inputs[\"mask_labels\"] = mask_labels\n+            encoded_inputs[\"class_labels\"] = class_labels\n+\n+        return encoded_inputs\n+\n+    def post_process_semantic_segmentation(\n+        self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Converts the output of [`Mask2FormerForUniversalSegmentation`] into semantic segmentation maps. Only supports\n+        PyTorch.\n+\n+        Args:\n+            outputs ([`Mask2FormerForUniversalSegmentation`]):\n+                Raw outputs of the model.\n+            target_sizes (`List[Tuple[int, int]]`, *optional*):\n+                List of length (batch_size), where each list item (`Tuple[int, int]]`) corresponds to the requested\n+                final size (height, width) of each prediction. If left to None, predictions will not be resized.\n+        Returns:\n+            `List[torch.Tensor]`:\n+                A list of length `batch_size`, where each item is a semantic segmentation map of shape (height, width)\n+                corresponding to the target_sizes entry (if `target_sizes` is specified). Each entry of each\n+                `torch.Tensor` correspond to a semantic class id.\n+        \"\"\"\n+        class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n+        masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n+\n+        # Scale back to preprocessed image size - (384, 384) for all models\n+        masks_queries_logits = torch.nn.functional.interpolate(\n+            masks_queries_logits, size=(384, 384), mode=\"bilinear\", align_corners=False\n+        )\n+\n+        # Remove the null class `[..., :-1]`\n+        masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]\n+        masks_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n+\n+        # Semantic segmentation logits of shape (batch_size, num_classes, height, width)\n+        segmentation = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)\n+        batch_size = class_queries_logits.shape[0]\n+\n+        # Resize logits and compute semantic segmentation maps\n+        if target_sizes is not None:\n+            if batch_size != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+            semantic_segmentation = []\n+            for idx in range(batch_size):\n+                resized_logits = torch.nn.functional.interpolate(\n+                    segmentation[idx].unsqueeze(dim=0), size=target_sizes[idx], mode=\"bilinear\", align_corners=False\n+                )\n+                semantic_map = resized_logits[0].argmax(dim=0)\n+                semantic_segmentation.append(semantic_map)\n+        else:\n+            semantic_segmentation = segmentation.argmax(dim=1)\n+            semantic_segmentation = [semantic_segmentation[i] for i in range(semantic_segmentation.shape[0])]\n+\n+        return semantic_segmentation\n+\n+    def post_process_instance_segmentation(\n+        self,\n+        outputs,\n+        threshold: float = 0.5,\n+        mask_threshold: float = 0.5,\n+        overlap_mask_area_threshold: float = 0.8,\n+        target_sizes: Optional[list[tuple[int, int]]] = None,\n+        return_coco_annotation: Optional[bool] = False,\n+        return_binary_maps: Optional[bool] = False,\n+    ) -> list[dict]:\n+        \"\"\"\n+        Converts the output of [`Mask2FormerForUniversalSegmentationOutput`] into instance segmentation predictions.\n+        Only supports PyTorch. If instances could overlap, set either return_coco_annotation or return_binary_maps\n+        to `True` to get the correct segmentation result.\n+\n+        Args:\n+            outputs ([`Mask2FormerForUniversalSegmentation`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The probability score threshold to keep predicted instance masks.\n+            mask_threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold to use when turning the predicted masks into binary values.\n+            overlap_mask_area_threshold (`float`, *optional*, defaults to 0.8):\n+                The overlap mask area threshold to merge or discard small disconnected parts within each binary\n+                instance mask.\n+            target_sizes (`List[Tuple]`, *optional*):\n+                List of length (batch_size), where each list item (`Tuple[int, int]]`) corresponds to the requested\n+                final size (height, width) of each prediction. If left to None, predictions will not be resized.\n+            return_coco_annotation (`bool`, *optional*, defaults to `False`):\n+                If set to `True`, segmentation maps are returned in COCO run-length encoding (RLE) format.\n+            return_binary_maps (`bool`, *optional*, defaults to `False`):\n+                If set to `True`, segmentation maps are returned as a concatenated tensor of binary segmentation maps\n+                (one per detected instance).\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n+            - **segmentation** -- A tensor of shape `(height, width)` where each pixel represents a `segment_id`, or\n+              `List[List]` run-length encoding (RLE) of the segmentation map if return_coco_annotation is set to\n+              `True`, or a tensor of shape `(num_instances, height, width)` if return_binary_maps is set to `True`.\n+              Set to `None` if no mask if found above `threshold`.\n+            - **segments_info** -- A dictionary that contains additional information on each segment.\n+                - **id** -- An integer representing the `segment_id`.\n+                - **label_id** -- An integer representing the label / semantic class id corresponding to `segment_id`.\n+                - **score** -- Prediction score of segment with `segment_id`.\n+        \"\"\"\n+        if return_coco_annotation and return_binary_maps:\n+            raise ValueError(\"return_coco_annotation and return_binary_maps can not be both set to True.\")\n+\n+        # [batch_size, num_queries, num_classes+1]\n+        class_queries_logits = outputs.class_queries_logits\n+        # [batch_size, num_queries, height, width]\n+        masks_queries_logits = outputs.masks_queries_logits\n+\n+        # Scale back to preprocessed image size - (384, 384) for all models\n+        masks_queries_logits = torch.nn.functional.interpolate(\n+            masks_queries_logits, size=(384, 384), mode=\"bilinear\", align_corners=False\n+        )\n+\n+        device = masks_queries_logits.device\n+        num_classes = class_queries_logits.shape[-1] - 1\n+        num_queries = class_queries_logits.shape[-2]\n+\n+        # Loop over items in batch size\n+        results: list[dict[str, TensorType]] = []\n+\n+        for i in range(class_queries_logits.shape[0]):\n+            mask_pred = masks_queries_logits[i]\n+            mask_cls = class_queries_logits[i]\n+\n+            scores = torch.nn.functional.softmax(mask_cls, dim=-1)[:, :-1]\n+            labels = torch.arange(num_classes, device=device).unsqueeze(0).repeat(num_queries, 1).flatten(0, 1)\n+\n+            scores_per_image, topk_indices = scores.flatten(0, 1).topk(num_queries, sorted=False)\n+            labels_per_image = labels[topk_indices]\n+\n+            topk_indices = torch.div(topk_indices, num_classes, rounding_mode=\"floor\")\n+            mask_pred = mask_pred[topk_indices]\n+            pred_masks = (mask_pred > 0).float()\n+\n+            # Calculate average mask prob\n+            mask_scores_per_image = (mask_pred.sigmoid().flatten(1) * pred_masks.flatten(1)).sum(1) / (\n+                pred_masks.flatten(1).sum(1) + 1e-6\n+            )\n+            pred_scores = scores_per_image * mask_scores_per_image\n+            pred_classes = labels_per_image\n+\n+            segmentation = torch.zeros((384, 384)) - 1\n+            if target_sizes is not None:\n+                segmentation = torch.zeros(target_sizes[i]) - 1\n+                pred_masks = torch.nn.functional.interpolate(\n+                    pred_masks.unsqueeze(0), size=target_sizes[i], mode=\"nearest\"\n+                )[0]\n+\n+            instance_maps, segments = [], []\n+            current_segment_id = 0\n+            for j in range(num_queries):\n+                score = pred_scores[j].item()\n+\n+                if not torch.all(pred_masks[j] == 0) and score >= threshold:\n+                    segmentation[pred_masks[j] == 1] = current_segment_id\n+                    segments.append(\n+                        {\n+                            \"id\": current_segment_id,\n+                            \"label_id\": pred_classes[j].item(),\n+                            \"was_fused\": False,\n+                            \"score\": round(score, 6),\n+                        }\n+                    )\n+                    current_segment_id += 1\n+                    instance_maps.append(pred_masks[j])\n+\n+            # Return segmentation map in run-length encoding (RLE) format\n+            if return_coco_annotation:\n+                segmentation = convert_segmentation_to_rle(segmentation)\n+\n+            # Return a concatenated tensor of binary instance maps\n+            if return_binary_maps and len(instance_maps) != 0:\n+                segmentation = torch.stack(instance_maps, dim=0)\n+\n+            results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n+        return results\n+\n+    def post_process_panoptic_segmentation(\n+        self,\n+        outputs,\n+        threshold: float = 0.5,\n+        mask_threshold: float = 0.5,\n+        overlap_mask_area_threshold: float = 0.8,\n+        label_ids_to_fuse: Optional[set[int]] = None,\n+        target_sizes: Optional[list[tuple[int, int]]] = None,\n+    ) -> list[dict]:\n+        \"\"\"\n+        Converts the output of [`Mask2FormerForUniversalSegmentationOutput`] into image panoptic segmentation\n+        predictions. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`Mask2FormerForUniversalSegmentationOutput`]):\n+                The outputs from [`Mask2FormerForUniversalSegmentation`].\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The probability score threshold to keep predicted instance masks.\n+            mask_threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold to use when turning the predicted masks into binary values.\n+            overlap_mask_area_threshold (`float`, *optional*, defaults to 0.8):\n+                The overlap mask area threshold to merge or discard small disconnected parts within each binary\n+                instance mask.\n+            label_ids_to_fuse (`Set[int]`, *optional*):\n+                The labels in this state will have all their instances be fused together. For instance we could say\n+                there can only be one sky in an image, but several persons, so the label ID for sky would be in that\n+                set, but not the one for person.\n+            target_sizes (`List[Tuple]`, *optional*):\n+                List of length (batch_size), where each list item (`Tuple[int, int]]`) corresponds to the requested\n+                final size (height, width) of each prediction in batch. If left to None, predictions will not be\n+                resized.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n+            - **segmentation** -- a tensor of shape `(height, width)` where each pixel represents a `segment_id`, set\n+              to `None` if no mask if found above `threshold`. If `target_sizes` is specified, segmentation is resized\n+              to the corresponding `target_sizes` entry.\n+            - **segments_info** -- A dictionary that contains additional information on each segment.\n+                - **id** -- an integer representing the `segment_id`.\n+                - **label_id** -- An integer representing the label / semantic class id corresponding to `segment_id`.\n+                - **was_fused** -- a boolean, `True` if `label_id` was in `label_ids_to_fuse`, `False` otherwise.\n+                  Multiple instances of the same class / label were fused and assigned a single `segment_id`.\n+                - **score** -- Prediction score of segment with `segment_id`.\n+        \"\"\"\n+\n+        if label_ids_to_fuse is None:\n+            logger.warning(\"`label_ids_to_fuse` unset. No instance will be fused.\")\n+            label_ids_to_fuse = set()\n+\n+        class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n+        masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n+\n+        # Scale back to preprocessed image size - (384, 384) for all models\n+        masks_queries_logits = torch.nn.functional.interpolate(\n+            masks_queries_logits, size=(384, 384), mode=\"bilinear\", align_corners=False\n+        )\n+\n+        batch_size = class_queries_logits.shape[0]\n+        num_labels = class_queries_logits.shape[-1] - 1\n+\n+        mask_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n+\n+        # Predicted label and score of each query (batch_size, num_queries)\n+        pred_scores, pred_labels = nn.functional.softmax(class_queries_logits, dim=-1).max(-1)\n+\n+        # Loop over items in batch size\n+        results: list[dict[str, TensorType]] = []\n+\n+        for i in range(batch_size):\n+            mask_probs_item, pred_scores_item, pred_labels_item = remove_low_and_no_objects(\n+                mask_probs[i], pred_scores[i], pred_labels[i], threshold, num_labels\n+            )\n+\n+            # No mask found\n+            if mask_probs_item.shape[0] <= 0:\n+                height, width = target_sizes[i] if target_sizes is not None else mask_probs_item.shape[1:]\n+                segmentation = torch.zeros((height, width)) - 1\n+                results.append({\"segmentation\": segmentation, \"segments_info\": []})\n+                continue\n+\n+            # Get segmentation map and segment information of batch item\n+            target_size = target_sizes[i] if target_sizes is not None else None\n+            segmentation, segments = compute_segments(\n+                mask_probs=mask_probs_item,\n+                pred_scores=pred_scores_item,\n+                pred_labels=pred_labels_item,\n+                mask_threshold=mask_threshold,\n+                overlap_mask_area_threshold=overlap_mask_area_threshold,\n+                label_ids_to_fuse=label_ids_to_fuse,\n+                target_size=target_size,\n+            )\n+\n+            results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n+        return results\n+\n+\n+__all__ = [\"Mask2FormerImageProcessorFast\"]"
        },
        {
            "sha": "eed1e7adc9ceaa0abbebb5d479057e424d7295be",
            "filename": "src/transformers/models/mask2former/modular_mask2former.py",
            "status": "added",
            "additions": 315,
            "deletions": 0,
            "changes": 315,
            "blob_url": "https://github.com/huggingface/transformers/blob/d9b35c635eb16e837a7b563c519c5231d4e09265/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodular_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d9b35c635eb16e837a7b563c519c5231d4e09265/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodular_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodular_mask2former.py?ref=d9b35c635eb16e837a7b563c519c5231d4e09265",
            "patch": "@@ -0,0 +1,315 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional\n+\n+from transformers.models.maskformer.image_processing_maskformer_fast import MaskFormerImageProcessorFast\n+\n+from ...utils import (\n+    TensorType,\n+    is_torch_available,\n+    logging,\n+)\n+from .image_processing_mask2former import (\n+    compute_segments,\n+    convert_segmentation_to_rle,\n+    remove_low_and_no_objects,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Mask2FormerImageProcessorFast(MaskFormerImageProcessorFast):\n+    def post_process_semantic_segmentation(\n+        self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Converts the output of [`Mask2FormerForUniversalSegmentation`] into semantic segmentation maps. Only supports\n+        PyTorch.\n+\n+        Args:\n+            outputs ([`Mask2FormerForUniversalSegmentation`]):\n+                Raw outputs of the model.\n+            target_sizes (`List[Tuple[int, int]]`, *optional*):\n+                List of length (batch_size), where each list item (`Tuple[int, int]]`) corresponds to the requested\n+                final size (height, width) of each prediction. If left to None, predictions will not be resized.\n+        Returns:\n+            `List[torch.Tensor]`:\n+                A list of length `batch_size`, where each item is a semantic segmentation map of shape (height, width)\n+                corresponding to the target_sizes entry (if `target_sizes` is specified). Each entry of each\n+                `torch.Tensor` correspond to a semantic class id.\n+        \"\"\"\n+        class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n+        masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n+\n+        # Scale back to preprocessed image size - (384, 384) for all models\n+        masks_queries_logits = torch.nn.functional.interpolate(\n+            masks_queries_logits, size=(384, 384), mode=\"bilinear\", align_corners=False\n+        )\n+\n+        # Remove the null class `[..., :-1]`\n+        masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]\n+        masks_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n+\n+        # Semantic segmentation logits of shape (batch_size, num_classes, height, width)\n+        segmentation = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)\n+        batch_size = class_queries_logits.shape[0]\n+\n+        # Resize logits and compute semantic segmentation maps\n+        if target_sizes is not None:\n+            if batch_size != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+            semantic_segmentation = []\n+            for idx in range(batch_size):\n+                resized_logits = torch.nn.functional.interpolate(\n+                    segmentation[idx].unsqueeze(dim=0), size=target_sizes[idx], mode=\"bilinear\", align_corners=False\n+                )\n+                semantic_map = resized_logits[0].argmax(dim=0)\n+                semantic_segmentation.append(semantic_map)\n+        else:\n+            semantic_segmentation = segmentation.argmax(dim=1)\n+            semantic_segmentation = [semantic_segmentation[i] for i in range(semantic_segmentation.shape[0])]\n+\n+        return semantic_segmentation\n+\n+    def post_process_instance_segmentation(\n+        self,\n+        outputs,\n+        threshold: float = 0.5,\n+        mask_threshold: float = 0.5,\n+        overlap_mask_area_threshold: float = 0.8,\n+        target_sizes: Optional[list[tuple[int, int]]] = None,\n+        return_coco_annotation: Optional[bool] = False,\n+        return_binary_maps: Optional[bool] = False,\n+    ) -> list[dict]:\n+        \"\"\"\n+        Converts the output of [`Mask2FormerForUniversalSegmentationOutput`] into instance segmentation predictions.\n+        Only supports PyTorch. If instances could overlap, set either return_coco_annotation or return_binary_maps\n+        to `True` to get the correct segmentation result.\n+\n+        Args:\n+            outputs ([`Mask2FormerForUniversalSegmentation`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The probability score threshold to keep predicted instance masks.\n+            mask_threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold to use when turning the predicted masks into binary values.\n+            overlap_mask_area_threshold (`float`, *optional*, defaults to 0.8):\n+                The overlap mask area threshold to merge or discard small disconnected parts within each binary\n+                instance mask.\n+            target_sizes (`List[Tuple]`, *optional*):\n+                List of length (batch_size), where each list item (`Tuple[int, int]]`) corresponds to the requested\n+                final size (height, width) of each prediction. If left to None, predictions will not be resized.\n+            return_coco_annotation (`bool`, *optional*, defaults to `False`):\n+                If set to `True`, segmentation maps are returned in COCO run-length encoding (RLE) format.\n+            return_binary_maps (`bool`, *optional*, defaults to `False`):\n+                If set to `True`, segmentation maps are returned as a concatenated tensor of binary segmentation maps\n+                (one per detected instance).\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n+            - **segmentation** -- A tensor of shape `(height, width)` where each pixel represents a `segment_id`, or\n+              `List[List]` run-length encoding (RLE) of the segmentation map if return_coco_annotation is set to\n+              `True`, or a tensor of shape `(num_instances, height, width)` if return_binary_maps is set to `True`.\n+              Set to `None` if no mask if found above `threshold`.\n+            - **segments_info** -- A dictionary that contains additional information on each segment.\n+                - **id** -- An integer representing the `segment_id`.\n+                - **label_id** -- An integer representing the label / semantic class id corresponding to `segment_id`.\n+                - **score** -- Prediction score of segment with `segment_id`.\n+        \"\"\"\n+        if return_coco_annotation and return_binary_maps:\n+            raise ValueError(\"return_coco_annotation and return_binary_maps can not be both set to True.\")\n+\n+        # [batch_size, num_queries, num_classes+1]\n+        class_queries_logits = outputs.class_queries_logits\n+        # [batch_size, num_queries, height, width]\n+        masks_queries_logits = outputs.masks_queries_logits\n+\n+        # Scale back to preprocessed image size - (384, 384) for all models\n+        masks_queries_logits = torch.nn.functional.interpolate(\n+            masks_queries_logits, size=(384, 384), mode=\"bilinear\", align_corners=False\n+        )\n+\n+        device = masks_queries_logits.device\n+        num_classes = class_queries_logits.shape[-1] - 1\n+        num_queries = class_queries_logits.shape[-2]\n+\n+        # Loop over items in batch size\n+        results: list[dict[str, TensorType]] = []\n+\n+        for i in range(class_queries_logits.shape[0]):\n+            mask_pred = masks_queries_logits[i]\n+            mask_cls = class_queries_logits[i]\n+\n+            scores = torch.nn.functional.softmax(mask_cls, dim=-1)[:, :-1]\n+            labels = torch.arange(num_classes, device=device).unsqueeze(0).repeat(num_queries, 1).flatten(0, 1)\n+\n+            scores_per_image, topk_indices = scores.flatten(0, 1).topk(num_queries, sorted=False)\n+            labels_per_image = labels[topk_indices]\n+\n+            topk_indices = torch.div(topk_indices, num_classes, rounding_mode=\"floor\")\n+            mask_pred = mask_pred[topk_indices]\n+            pred_masks = (mask_pred > 0).float()\n+\n+            # Calculate average mask prob\n+            mask_scores_per_image = (mask_pred.sigmoid().flatten(1) * pred_masks.flatten(1)).sum(1) / (\n+                pred_masks.flatten(1).sum(1) + 1e-6\n+            )\n+            pred_scores = scores_per_image * mask_scores_per_image\n+            pred_classes = labels_per_image\n+\n+            segmentation = torch.zeros((384, 384)) - 1\n+            if target_sizes is not None:\n+                segmentation = torch.zeros(target_sizes[i]) - 1\n+                pred_masks = torch.nn.functional.interpolate(\n+                    pred_masks.unsqueeze(0), size=target_sizes[i], mode=\"nearest\"\n+                )[0]\n+\n+            instance_maps, segments = [], []\n+            current_segment_id = 0\n+            for j in range(num_queries):\n+                score = pred_scores[j].item()\n+\n+                if not torch.all(pred_masks[j] == 0) and score >= threshold:\n+                    segmentation[pred_masks[j] == 1] = current_segment_id\n+                    segments.append(\n+                        {\n+                            \"id\": current_segment_id,\n+                            \"label_id\": pred_classes[j].item(),\n+                            \"was_fused\": False,\n+                            \"score\": round(score, 6),\n+                        }\n+                    )\n+                    current_segment_id += 1\n+                    instance_maps.append(pred_masks[j])\n+\n+            # Return segmentation map in run-length encoding (RLE) format\n+            if return_coco_annotation:\n+                segmentation = convert_segmentation_to_rle(segmentation)\n+\n+            # Return a concatenated tensor of binary instance maps\n+            if return_binary_maps and len(instance_maps) != 0:\n+                segmentation = torch.stack(instance_maps, dim=0)\n+\n+            results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n+        return results\n+\n+    def post_process_panoptic_segmentation(\n+        self,\n+        outputs,\n+        threshold: float = 0.5,\n+        mask_threshold: float = 0.5,\n+        overlap_mask_area_threshold: float = 0.8,\n+        label_ids_to_fuse: Optional[set[int]] = None,\n+        target_sizes: Optional[list[tuple[int, int]]] = None,\n+    ) -> list[dict]:\n+        \"\"\"\n+        Converts the output of [`Mask2FormerForUniversalSegmentationOutput`] into image panoptic segmentation\n+        predictions. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`Mask2FormerForUniversalSegmentationOutput`]):\n+                The outputs from [`Mask2FormerForUniversalSegmentation`].\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The probability score threshold to keep predicted instance masks.\n+            mask_threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold to use when turning the predicted masks into binary values.\n+            overlap_mask_area_threshold (`float`, *optional*, defaults to 0.8):\n+                The overlap mask area threshold to merge or discard small disconnected parts within each binary\n+                instance mask.\n+            label_ids_to_fuse (`Set[int]`, *optional*):\n+                The labels in this state will have all their instances be fused together. For instance we could say\n+                there can only be one sky in an image, but several persons, so the label ID for sky would be in that\n+                set, but not the one for person.\n+            target_sizes (`List[Tuple]`, *optional*):\n+                List of length (batch_size), where each list item (`Tuple[int, int]]`) corresponds to the requested\n+                final size (height, width) of each prediction in batch. If left to None, predictions will not be\n+                resized.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n+            - **segmentation** -- a tensor of shape `(height, width)` where each pixel represents a `segment_id`, set\n+              to `None` if no mask if found above `threshold`. If `target_sizes` is specified, segmentation is resized\n+              to the corresponding `target_sizes` entry.\n+            - **segments_info** -- A dictionary that contains additional information on each segment.\n+                - **id** -- an integer representing the `segment_id`.\n+                - **label_id** -- An integer representing the label / semantic class id corresponding to `segment_id`.\n+                - **was_fused** -- a boolean, `True` if `label_id` was in `label_ids_to_fuse`, `False` otherwise.\n+                  Multiple instances of the same class / label were fused and assigned a single `segment_id`.\n+                - **score** -- Prediction score of segment with `segment_id`.\n+        \"\"\"\n+\n+        if label_ids_to_fuse is None:\n+            logger.warning(\"`label_ids_to_fuse` unset. No instance will be fused.\")\n+            label_ids_to_fuse = set()\n+\n+        class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n+        masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n+\n+        # Scale back to preprocessed image size - (384, 384) for all models\n+        masks_queries_logits = torch.nn.functional.interpolate(\n+            masks_queries_logits, size=(384, 384), mode=\"bilinear\", align_corners=False\n+        )\n+\n+        batch_size = class_queries_logits.shape[0]\n+        num_labels = class_queries_logits.shape[-1] - 1\n+\n+        mask_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n+\n+        # Predicted label and score of each query (batch_size, num_queries)\n+        pred_scores, pred_labels = nn.functional.softmax(class_queries_logits, dim=-1).max(-1)\n+\n+        # Loop over items in batch size\n+        results: list[dict[str, TensorType]] = []\n+\n+        for i in range(batch_size):\n+            mask_probs_item, pred_scores_item, pred_labels_item = remove_low_and_no_objects(\n+                mask_probs[i], pred_scores[i], pred_labels[i], threshold, num_labels\n+            )\n+\n+            # No mask found\n+            if mask_probs_item.shape[0] <= 0:\n+                height, width = target_sizes[i] if target_sizes is not None else mask_probs_item.shape[1:]\n+                segmentation = torch.zeros((height, width)) - 1\n+                results.append({\"segmentation\": segmentation, \"segments_info\": []})\n+                continue\n+\n+            # Get segmentation map and segment information of batch item\n+            target_size = target_sizes[i] if target_sizes is not None else None\n+            segmentation, segments = compute_segments(\n+                mask_probs=mask_probs_item,\n+                pred_scores=pred_scores_item,\n+                pred_labels=pred_labels_item,\n+                mask_threshold=mask_threshold,\n+                overlap_mask_area_threshold=overlap_mask_area_threshold,\n+                label_ids_to_fuse=label_ids_to_fuse,\n+                target_size=target_size,\n+            )\n+\n+            results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n+        return results\n+\n+    def post_process_segmentation():\n+        raise NotImplementedError(\"Segmentation post-processing is not implemented for Mask2Former yet.\")\n+\n+\n+__all__ = [\"Mask2FormerImageProcessorFast\"]"
        },
        {
            "sha": "3a91c136c2f32c434a52161342fb8df6f7b0011d",
            "filename": "src/transformers/models/maskformer/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d9b35c635eb16e837a7b563c519c5231d4e09265/src%2Ftransformers%2Fmodels%2Fmaskformer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d9b35c635eb16e837a7b563c519c5231d4e09265/src%2Ftransformers%2Fmodels%2Fmaskformer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2F__init__.py?ref=d9b35c635eb16e837a7b563c519c5231d4e09265",
            "patch": "@@ -22,6 +22,7 @@\n     from .configuration_maskformer_swin import *\n     from .feature_extraction_maskformer import *\n     from .image_processing_maskformer import *\n+    from .image_processing_maskformer_fast import *\n     from .modeling_maskformer import *\n     from .modeling_maskformer_swin import *\n else:"
        },
        {
            "sha": "52dc99b4c374619c1e39150e30a39861cb340226",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer.py",
            "status": "modified",
            "additions": 68,
            "deletions": 5,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/d9b35c635eb16e837a7b563c519c5231d4e09265/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d9b35c635eb16e837a7b563c519c5231d4e09265/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py?ref=d9b35c635eb16e837a7b563c519c5231d4e09265",
            "patch": "@@ -67,6 +67,46 @@\n     from torch import nn\n \n \n+# Copied from transformers.models.detr.image_processing_detr.get_size_with_aspect_ratio\n+def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n+    \"\"\"\n+    Computes the output image size given the input image size and the desired output size.\n+\n+    Args:\n+        image_size (`tuple[int, int]`):\n+            The input image size.\n+        size (`int`):\n+            The desired output size.\n+        max_size (`int`, *optional*):\n+            The maximum allowed output size.\n+    \"\"\"\n+    height, width = image_size\n+    raw_size = None\n+    if max_size is not None:\n+        min_original_size = float(min((height, width)))\n+        max_original_size = float(max((height, width)))\n+        if max_original_size / min_original_size * size > max_size:\n+            raw_size = max_size * min_original_size / max_original_size\n+            size = int(round(raw_size))\n+\n+    if (height <= width and height == size) or (width <= height and width == size):\n+        oh, ow = height, width\n+    elif width < height:\n+        ow = size\n+        if max_size is not None and raw_size is not None:\n+            oh = int(raw_size * height / width)\n+        else:\n+            oh = int(size * height / width)\n+    else:\n+        oh = size\n+        if max_size is not None and raw_size is not None:\n+            ow = int(raw_size * width / height)\n+        else:\n+            ow = int(size * width / height)\n+\n+    return (oh, ow)\n+\n+\n # Copied from transformers.models.detr.image_processing_detr.max_across_indices\n def max_across_indices(values: Iterable[Any]) -> list[Any]:\n     \"\"\"\n@@ -399,6 +439,10 @@ class MaskFormerImageProcessor(BaseImageProcessor):\n             The background label will be replaced by `ignore_index`.\n         num_labels (`int`, *optional*):\n             The number of labels in the segmentation map.\n+        pad_size (`Dict[str, int]`, *optional*):\n+            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+            height and width in the batch.\n \n     \"\"\"\n \n@@ -422,6 +466,7 @@ def __init__(\n         ignore_index: Optional[int] = None,\n         do_reduce_labels: bool = False,\n         num_labels: Optional[int] = None,\n+        pad_size: Optional[dict[str, int]] = None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -445,6 +490,7 @@ def __init__(\n         self.ignore_index = ignore_index\n         self.do_reduce_labels = do_reduce_labels\n         self.num_labels = num_labels\n+        self.pad_size = pad_size\n \n     @classmethod\n     def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n@@ -700,6 +746,7 @@ def preprocess(\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        pad_size: Optional[dict[str, int]] = None,\n     ) -> BatchFeature:\n         do_resize = do_resize if do_resize is not None else self.do_resize\n         size = size if size is not None else self.size\n@@ -713,6 +760,7 @@ def preprocess(\n         image_std = image_std if image_std is not None else self.image_std\n         ignore_index = ignore_index if ignore_index is not None else self.ignore_index\n         do_reduce_labels = do_reduce_labels if do_reduce_labels is not None else self.do_reduce_labels\n+        pad_size = self.pad_size if pad_size is None else pad_size\n \n         if not valid_images(images):\n             raise ValueError(\n@@ -777,6 +825,7 @@ def preprocess(\n             do_reduce_labels,\n             return_tensors,\n             input_data_format=data_format,\n+            pad_size=pad_size,\n         )\n         return encoded_inputs\n \n@@ -808,7 +857,6 @@ def _pad_image(\n         )\n         return padded_image\n \n-    # Copied from transformers.models.vilt.image_processing_vilt.ViltImageProcessor.pad\n     def pad(\n         self,\n         images: list[np.ndarray],\n@@ -817,6 +865,7 @@ def pad(\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        pad_size: Optional[dict[str, int]] = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n@@ -840,13 +889,21 @@ def pad(\n                 The channel dimension format of the image. If not provided, it will be the same as the input image.\n             input_data_format (`ChannelDimension` or `str`, *optional*):\n                 The channel dimension format of the input image. If not provided, it will be inferred.\n+            pad_size (`Dict[str, int]`, *optional*):\n+                The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+                provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+                height and width in the batch.\n         \"\"\"\n-        pad_size = get_max_height_width(images, input_data_format=input_data_format)\n+        pad_size = pad_size if pad_size is not None else self.pad_size\n+        if pad_size is not None:\n+            padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+        else:\n+            padded_size = get_max_height_width(images, input_data_format=input_data_format)\n \n         padded_images = [\n             self._pad_image(\n                 image,\n-                pad_size,\n+                padded_size,\n                 constant_values=constant_values,\n                 data_format=data_format,\n                 input_data_format=input_data_format,\n@@ -857,7 +914,7 @@ def pad(\n \n         if return_pixel_mask:\n             masks = [\n-                make_pixel_mask(image=image, output_size=pad_size, input_data_format=input_data_format)\n+                make_pixel_mask(image=image, output_size=padded_size, input_data_format=input_data_format)\n                 for image in images\n             ]\n             data[\"pixel_mask\"] = masks\n@@ -873,6 +930,7 @@ def encode_inputs(\n         do_reduce_labels: bool = False,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        pad_size: Optional[dict[str, int]] = None,\n     ):\n         \"\"\"\n         Pad images up to the largest image in a batch and create a corresponding `pixel_mask`.\n@@ -909,6 +967,11 @@ def encode_inputs(\n                 If set, will return tensors instead of NumPy arrays. If set to `'pt'`, return PyTorch `torch.Tensor`\n                 objects.\n \n+            pad_size (`Dict[str, int]`, *optional*):\n+                The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+                provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+                height and width in the batch.\n+\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n \n@@ -930,7 +993,7 @@ def encode_inputs(\n             input_data_format = infer_channel_dimension_format(pixel_values_list[0])\n \n         encoded_inputs = self.pad(\n-            pixel_values_list, return_tensors=return_tensors, input_data_format=input_data_format\n+            pixel_values_list, return_tensors=return_tensors, input_data_format=input_data_format, pad_size=pad_size\n         )\n \n         if segmentation_maps is not None:"
        },
        {
            "sha": "bdd13afbe6116be51795660858c6e4487d27bb64",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer_fast.py",
            "status": "added",
            "additions": 775,
            "deletions": 0,
            "changes": 775,
            "blob_url": "https://github.com/huggingface/transformers/blob/d9b35c635eb16e837a7b563c519c5231d4e09265/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d9b35c635eb16e837a7b563c519c5231d4e09265/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py?ref=d9b35c635eb16e837a7b563c519c5231d4e09265",
            "patch": "@@ -0,0 +1,775 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for MaskFormer.\"\"\"\n+\n+import math\n+import warnings\n+from typing import TYPE_CHECKING, Any, Optional, Union\n+\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    SizeDict,\n+    get_image_size_for_max_height_width,\n+    get_max_height_width,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    logging,\n+)\n+from ...utils.deprecation import deprecate_kwarg\n+from .image_processing_maskformer import (\n+    compute_segments,\n+    convert_segmentation_to_rle,\n+    get_size_with_aspect_ratio,\n+    remove_low_and_no_objects,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+if TYPE_CHECKING:\n+    from transformers import MaskFormerForInstanceSegmentationOutput\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.transforms import functional as F\n+\n+\n+def convert_segmentation_map_to_binary_masks_fast(\n+    segmentation_map: \"torch.Tensor\",\n+    instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n+    ignore_index: Optional[int] = None,\n+    do_reduce_labels: bool = False,\n+):\n+    if do_reduce_labels and ignore_index is None:\n+        raise ValueError(\"If `do_reduce_labels` is True, `ignore_index` must be provided.\")\n+\n+    if do_reduce_labels:\n+        segmentation_map = torch.where(segmentation_map == 0, ignore_index, segmentation_map - 1)\n+\n+    all_labels = torch.unique(segmentation_map)\n+\n+    if ignore_index is not None:\n+        all_labels = all_labels[all_labels != ignore_index]  # drop background label if applicable\n+\n+    binary_masks = [(segmentation_map == i) for i in all_labels]\n+    if binary_masks:\n+        binary_masks = torch.stack(binary_masks, dim=0)\n+    else:\n+        binary_masks = torch.zeros((0, *segmentation_map.shape), device=segmentation_map.device)\n+\n+    # Convert instance ids to class ids\n+    if instance_id_to_semantic_id is not None:\n+        labels = torch.zeros(all_labels.shape[0], device=segmentation_map.device)\n+\n+        for i, label in enumerate(all_labels):\n+            class_id = instance_id_to_semantic_id[(label.item() + 1 if do_reduce_labels else label.item())]\n+            labels[i] = class_id - 1 if do_reduce_labels else class_id\n+    else:\n+        labels = all_labels\n+    return binary_masks.float(), labels.long()\n+\n+\n+class MaskFormerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    r\"\"\"\n+    size_divisor (`int`, *optional*, defaults to 32):\n+        Some backbones need images divisible by a certain number. If not passed, it defaults to the value used in\n+        Swin Transformer.\n+    ignore_index (`int`, *optional*):\n+        Label to be assigned to background pixels in segmentation maps. If provided, segmentation map pixels\n+        denoted with 0 (background) will be replaced with `ignore_index`.\n+    do_reduce_labels (`bool`, *optional*, defaults to `False`):\n+        Whether or not to decrement all label values of segmentation maps by 1. Usually used for datasets where 0\n+        is used for background, and background itself is not included in all classes of a dataset (e.g. ADE20k).\n+        The background label will be replaced by `ignore_index`.\n+    num_labels (`int`, *optional*):\n+        The number of labels in the segmentation map.\n+    do_pad (`bool`, *optional*, defaults to `True`):\n+        Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n+        method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n+        If `pad_size` is provided, the image will be padded to the specified dimensions.\n+        Otherwise, the image will be padded to the maximum height and width of the batch.\n+    pad_size (`Dict[str, int]`, *optional*):\n+        The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+        provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+        height and width in the batch.\n+    \"\"\"\n+\n+    size_divisor: Optional[int]\n+    ignore_index: Optional[int]\n+    do_reduce_labels: Optional[bool]\n+    num_labels: Optional[int]\n+    do_pad: Optional[bool]\n+    pad_size: Optional[dict[str, int]]\n+\n+\n+@auto_docstring\n+class MaskFormerImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"shortest_edge\": 800, \"longest_edge\": 1333}\n+    default_to_square = False\n+    do_resize = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    do_normalize = True\n+    do_pad = True\n+    model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+    size_divisor = 32\n+    do_reduce_labels = False\n+    valid_kwargs = MaskFormerFastImageProcessorKwargs\n+\n+    @deprecate_kwarg(\"reduce_labels\", new_name=\"do_reduce_labels\", version=\"4.44.0\")\n+    @deprecate_kwarg(\"size_divisibility\", new_name=\"size_divisor\", version=\"4.41.0\")\n+    @deprecate_kwarg(\"max_size\", version=\"4.27.0\", warn_if_greater_or_equal_version=True)\n+    def __init__(self, **kwargs: Unpack[MaskFormerFastImageProcessorKwargs]) -> None:\n+        if \"pad_and_return_pixel_mask\" in kwargs:\n+            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+\n+        size = kwargs.pop(\"size\", None)\n+        max_size = kwargs.pop(\"max_size\", None)\n+\n+        if size is None and max_size is not None:\n+            size = self.size\n+            size[\"longest_edge\"] = max_size\n+        elif size is None:\n+            size = self.size\n+\n+        self.size = get_size_dict(size, max_size=max_size, default_to_square=False)\n+\n+        super().__init__(**kwargs)\n+\n+    @classmethod\n+    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n+        \"\"\"\n+        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n+        created using from_dict and kwargs e.g. `MaskFormerImageProcessor.from_pretrained(checkpoint, max_size=800)`\n+        \"\"\"\n+        image_processor_dict = image_processor_dict.copy()\n+        if \"max_size\" in kwargs:\n+            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n+        if \"size_divisibility\" in kwargs:\n+            image_processor_dict[\"size_divisor\"] = kwargs.pop(\"size_divisibility\")\n+        if \"reduce_labels\" in image_processor_dict:\n+            image_processor_dict[\"do_reduce_labels\"] = image_processor_dict.pop(\"reduce_labels\")\n+        return super().from_dict(image_processor_dict, **kwargs)\n+\n+    def to_dict(self) -> dict[str, Any]:\n+        \"\"\"\n+        Serializes this instance to a Python dictionary. This method calls the superclass method and then removes the\n+        `_max_size` attribute from the dictionary.\n+        \"\"\"\n+        image_processor_dict = super().to_dict()\n+        image_processor_dict.pop(\"_max_size\", None)\n+        return image_processor_dict\n+\n+    def reduce_label(self, labels: list[\"torch.Tensor\"]):\n+        for idx in range(len(labels)):\n+            label = labels[idx]\n+            label = torch.where(label == 0, torch.tensor(255, dtype=label.dtype), label)\n+            label = label - 1\n+            label = torch.where(label == 254, torch.tensor(255, dtype=label.dtype), label)\n+            labels[idx] = label\n+\n+    def resize(\n+        self,\n+        image: torch.Tensor,\n+        size: SizeDict,\n+        size_divisor: int = 0,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Resize the image to the given size. Size can be `min_size` (scalar) or `(height, width)` tuple. If size is an\n+        int, smaller edge of the image will be matched to this number.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Size of the image's `(height, width)` dimensions after resizing. Available options are:\n+                    - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n+                        Do NOT keep the aspect ratio.\n+                    - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n+                        the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n+                        less or equal to `longest_edge`.\n+                    - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n+                        aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n+                        `max_width`.\n+            size_divisor (`int`, *optional*, defaults to 0):\n+                If `size_divisor` is given, the output image size will be divisible by the number.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                Resampling filter to use if resizing the image.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+        if size.shortest_edge and size.longest_edge:\n+            # Resize the image so that the shortest edge or the longest edge is of the given size\n+            # while maintaining the aspect ratio of the original image.\n+            new_size = get_size_with_aspect_ratio(\n+                image.size()[-2:],\n+                size[\"shortest_edge\"],\n+                size[\"longest_edge\"],\n+            )\n+        elif size.max_height and size.max_width:\n+            new_size = get_image_size_for_max_height_width(image.size()[-2:], size[\"max_height\"], size[\"max_width\"])\n+        elif size.height and size.width:\n+            new_size = (size[\"height\"], size[\"width\"])\n+        else:\n+            raise ValueError(\n+                \"Size must contain 'height' and 'width' keys or 'shortest_edge' and 'longest_edge' keys. Got\"\n+                f\" {size.keys()}.\"\n+            )\n+        if size_divisor > 0:\n+            height, width = new_size\n+            height = int(math.ceil(height / size_divisor) * size_divisor)\n+            width = int(math.ceil(width / size_divisor) * size_divisor)\n+            new_size = (height, width)\n+\n+        image = F.resize(\n+            image,\n+            size=new_size,\n+            interpolation=interpolation,\n+            **kwargs,\n+        )\n+        return image\n+\n+    def pad(\n+        self,\n+        images: torch.Tensor,\n+        padded_size: tuple[int, int],\n+        segmentation_maps: Optional[torch.Tensor] = None,\n+        fill: int = 0,\n+        ignore_index: int = 255,\n+    ) -> BatchFeature:\n+        original_size = images.size()[-2:]\n+        padding_bottom = padded_size[0] - original_size[0]\n+        padding_right = padded_size[1] - original_size[1]\n+        if padding_bottom < 0 or padding_right < 0:\n+            raise ValueError(\n+                f\"Padding dimensions are negative. Please make sure that the padded size is larger than the \"\n+                f\"original size. Got padded size: {padded_size}, original size: {original_size}.\"\n+            )\n+        if original_size != padded_size:\n+            padding = [0, 0, padding_right, padding_bottom]\n+            images = F.pad(images, padding, fill=fill)\n+            if segmentation_maps is not None:\n+                segmentation_maps = F.pad(segmentation_maps, padding, fill=ignore_index)\n+\n+        # Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n+        pixel_mask = torch.zeros((images.shape[0], *padded_size), dtype=torch.int64, device=images.device)\n+        pixel_mask[:, : original_size[0], : original_size[1]] = 1\n+\n+        return images, pixel_mask, segmentation_maps\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n+        **kwargs: Unpack[MaskFormerFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps.\n+        instance_id_to_semantic_id (`Union[list[dict[int, int]], dict[int, int]]`, *optional*):\n+            A mapping from instance IDs to semantic IDs.\n+        \"\"\"\n+        return super().preprocess(\n+            images,\n+            segmentation_maps,\n+            instance_id_to_semantic_id,\n+            **kwargs,\n+        )\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: ImageInput,\n+        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]],\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[MaskFormerFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        To be overriden by subclasses when image-like inputs other than images should be processed.\n+        It can be used for segmentation maps, depth maps, etc.\n+        \"\"\"\n+        # Prepare input images\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        if segmentation_maps is not None:\n+            segmentation_maps = self._prepare_image_like_inputs(\n+                images=segmentation_maps,\n+                expected_ndims=2,\n+                do_convert_rgb=False,\n+                input_data_format=ChannelDimension.FIRST,\n+            )\n+        return self._preprocess(images, segmentation_maps, instance_id_to_semantic_id, **kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        segmentation_maps: Optional[\"torch.Tensor\"],\n+        instance_id_to_semantic_id: Optional[dict[int, int]],\n+        do_resize: Optional[bool],\n+        size: Optional[dict[str, int]],\n+        pad_size: Optional[dict[str, int]],\n+        size_divisor: Optional[int],\n+        interpolation: Optional[Union[\"PILImageResampling\", \"F.InterpolationMode\"]],\n+        do_rescale: Optional[bool],\n+        rescale_factor: Optional[float],\n+        do_normalize: Optional[bool],\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        ignore_index: Optional[int],\n+        do_reduce_labels: Optional[bool],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        if segmentation_maps is not None and len(images) != len(segmentation_maps):\n+            raise ValueError(\"Images and segmentation maps must have the same length.\")\n+\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+        if segmentation_maps is not None:\n+            grouped_segmentation_maps, grouped_segmentation_maps_index = group_images_by_shape(\n+                segmentation_maps, disable_grouping=disable_grouping\n+            )\n+            resized_segmentation_maps_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(\n+                    image=stacked_images, size=size, size_divisor=size_divisor, interpolation=interpolation\n+                )\n+                if segmentation_maps is not None:\n+                    stacked_segmentation_maps = self.resize(\n+                        image=grouped_segmentation_maps[shape],\n+                        size=size,\n+                        size_divisor=size_divisor,\n+                        interpolation=F.InterpolationMode.NEAREST_EXACT,\n+                    )\n+            resized_images_grouped[shape] = stacked_images\n+            if segmentation_maps is not None:\n+                resized_segmentation_maps_grouped[shape] = stacked_segmentation_maps\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+        if segmentation_maps is not None:\n+            resized_segmentation_maps = reorder_images(\n+                resized_segmentation_maps_grouped, grouped_segmentation_maps_index\n+            )\n+        if pad_size is not None:\n+            padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+        else:\n+            padded_size = get_max_height_width(resized_images)\n+\n+        if segmentation_maps is not None:\n+            mask_labels = []\n+            class_labels = []\n+            # Convert to list of binary masks and labels\n+            for idx, segmentation_map in enumerate(resized_segmentation_maps):\n+                if isinstance(instance_id_to_semantic_id, list):\n+                    instance_id = instance_id_to_semantic_id[idx]\n+                else:\n+                    instance_id = instance_id_to_semantic_id\n+                # Use instance2class_id mapping per image\n+                masks, classes = convert_segmentation_map_to_binary_masks_fast(\n+                    segmentation_map.squeeze(0),\n+                    instance_id,\n+                    ignore_index=ignore_index,\n+                    do_reduce_labels=do_reduce_labels,\n+                )\n+                mask_labels.append(masks)\n+                class_labels.append(classes)\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        processed_pixel_masks_grouped = {}\n+        if segmentation_maps is not None:\n+            grouped_segmentation_maps, grouped_segmentation_maps_index = group_images_by_shape(\n+                mask_labels, disable_grouping=disable_grouping\n+            )\n+            processed_segmentation_maps_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            padded_images, pixel_masks, padded_segmentation_maps = self.pad(\n+                images=stacked_images,\n+                segmentation_maps=grouped_segmentation_maps[shape] if segmentation_maps is not None else None,\n+                padded_size=padded_size,\n+                ignore_index=ignore_index,\n+            )\n+            processed_images_grouped[shape] = padded_images\n+            processed_pixel_masks_grouped[shape] = pixel_masks\n+            if segmentation_maps is not None:\n+                processed_segmentation_maps_grouped[shape] = padded_segmentation_maps.squeeze(1)\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_pixel_masks = reorder_images(processed_pixel_masks_grouped, grouped_images_index)\n+        encoded_inputs = BatchFeature(\n+            data={\n+                \"pixel_values\": torch.stack(processed_images, dim=0) if return_tensors else processed_images,\n+                \"pixel_mask\": torch.stack(processed_pixel_masks, dim=0) if return_tensors else processed_pixel_masks,\n+            },\n+            tensor_type=return_tensors,\n+        )\n+        if segmentation_maps is not None:\n+            mask_labels = reorder_images(processed_segmentation_maps_grouped, grouped_segmentation_maps_index)\n+            # we cannot batch them since they don't share a common class size\n+            encoded_inputs[\"mask_labels\"] = mask_labels\n+            encoded_inputs[\"class_labels\"] = class_labels\n+\n+        return encoded_inputs\n+\n+    # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.post_process_segmentation\n+    def post_process_segmentation(\n+        self, outputs: \"MaskFormerForInstanceSegmentationOutput\", target_size: Optional[tuple[int, int]] = None\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Converts the output of [`MaskFormerForInstanceSegmentationOutput`] into image segmentation predictions. Only\n+        supports PyTorch.\n+\n+        Args:\n+            outputs ([`MaskFormerForInstanceSegmentationOutput`]):\n+                The outputs from [`MaskFormerForInstanceSegmentation`].\n+\n+            target_size (`tuple[int, int]`, *optional*):\n+                If set, the `masks_queries_logits` will be resized to `target_size`.\n+\n+        Returns:\n+            `torch.Tensor`:\n+                A tensor of shape (`batch_size, num_class_labels, height, width`).\n+        \"\"\"\n+        warnings.warn(\n+            \"`post_process_segmentation` is deprecated and will be removed in v5 of Transformers, please use\"\n+            \" `post_process_instance_segmentation`\",\n+            FutureWarning,\n+        )\n+\n+        # class_queries_logits has shape [BATCH, QUERIES, CLASSES + 1]\n+        class_queries_logits = outputs.class_queries_logits\n+        # masks_queries_logits has shape [BATCH, QUERIES, HEIGHT, WIDTH]\n+        masks_queries_logits = outputs.masks_queries_logits\n+        if target_size is not None:\n+            masks_queries_logits = torch.nn.functional.interpolate(\n+                masks_queries_logits,\n+                size=target_size,\n+                mode=\"bilinear\",\n+                align_corners=False,\n+            )\n+        # remove the null class `[..., :-1]`\n+        masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]\n+        # mask probs has shape [BATCH, QUERIES, HEIGHT, WIDTH]\n+        masks_probs = masks_queries_logits.sigmoid()\n+        # now we want to sum over the queries,\n+        # $ out_{c,h,w} =  \\sum_q p_{q,c} * m_{q,h,w} $\n+        # where $ softmax(p) \\in R^{q, c} $ is the mask classes\n+        # and $ sigmoid(m) \\in R^{q, h, w}$ is the mask probabilities\n+        # b(atch)q(uery)c(lasses), b(atch)q(uery)h(eight)w(idth)\n+        segmentation = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)\n+\n+        return segmentation\n+\n+    # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.post_process_semantic_segmentation\n+    def post_process_semantic_segmentation(\n+        self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Converts the output of [`MaskFormerForInstanceSegmentation`] into semantic segmentation maps. Only supports\n+        PyTorch.\n+\n+        Args:\n+            outputs ([`MaskFormerForInstanceSegmentation`]):\n+                Raw outputs of the model.\n+            target_sizes (`list[tuple[int, int]]`, *optional*):\n+                List of length (batch_size), where each list item (`tuple[int, int]]`) corresponds to the requested\n+                final size (height, width) of each prediction. If left to None, predictions will not be resized.\n+        Returns:\n+            `list[torch.Tensor]`:\n+                A list of length `batch_size`, where each item is a semantic segmentation map of shape (height, width)\n+                corresponding to the target_sizes entry (if `target_sizes` is specified). Each entry of each\n+                `torch.Tensor` correspond to a semantic class id.\n+        \"\"\"\n+        class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n+        masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n+\n+        # Remove the null class `[..., :-1]`\n+        masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]\n+        masks_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n+\n+        # Semantic segmentation logits of shape (batch_size, num_classes, height, width)\n+        segmentation = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)\n+        batch_size = class_queries_logits.shape[0]\n+\n+        # Resize logits and compute semantic segmentation maps\n+        if target_sizes is not None:\n+            if batch_size != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+            semantic_segmentation = []\n+            for idx in range(batch_size):\n+                resized_logits = torch.nn.functional.interpolate(\n+                    segmentation[idx].unsqueeze(dim=0), size=target_sizes[idx], mode=\"bilinear\", align_corners=False\n+                )\n+                semantic_map = resized_logits[0].argmax(dim=0)\n+                semantic_segmentation.append(semantic_map)\n+        else:\n+            semantic_segmentation = segmentation.argmax(dim=1)\n+            semantic_segmentation = [semantic_segmentation[i] for i in range(semantic_segmentation.shape[0])]\n+\n+        return semantic_segmentation\n+\n+    # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.post_process_instance_segmentation\n+    def post_process_instance_segmentation(\n+        self,\n+        outputs,\n+        threshold: float = 0.5,\n+        mask_threshold: float = 0.5,\n+        overlap_mask_area_threshold: float = 0.8,\n+        target_sizes: Optional[list[tuple[int, int]]] = None,\n+        return_coco_annotation: Optional[bool] = False,\n+        return_binary_maps: Optional[bool] = False,\n+    ) -> list[dict]:\n+        \"\"\"\n+        Converts the output of [`MaskFormerForInstanceSegmentationOutput`] into instance segmentation predictions. Only\n+        supports PyTorch. If instances could overlap, set either return_coco_annotation or return_binary_maps\n+        to `True` to get the correct segmentation result.\n+\n+        Args:\n+            outputs ([`MaskFormerForInstanceSegmentation`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The probability score threshold to keep predicted instance masks.\n+            mask_threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold to use when turning the predicted masks into binary values.\n+            overlap_mask_area_threshold (`float`, *optional*, defaults to 0.8):\n+                The overlap mask area threshold to merge or discard small disconnected parts within each binary\n+                instance mask.\n+            target_sizes (`list[Tuple]`, *optional*):\n+                List of length (batch_size), where each list item (`tuple[int, int]]`) corresponds to the requested\n+                final size (height, width) of each prediction. If left to None, predictions will not be resized.\n+            return_coco_annotation (`bool`, *optional*, defaults to `False`):\n+                If set to `True`, segmentation maps are returned in COCO run-length encoding (RLE) format.\n+            return_binary_maps (`bool`, *optional*, defaults to `False`):\n+                If set to `True`, segmentation maps are returned as a concatenated tensor of binary segmentation maps\n+                (one per detected instance).\n+        Returns:\n+            `list[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n+            - **segmentation** -- A tensor of shape `(height, width)` where each pixel represents a `segment_id`, or\n+              `list[List]` run-length encoding (RLE) of the segmentation map if return_coco_annotation is set to\n+              `True`, or a tensor of shape `(num_instances, height, width)` if return_binary_maps is set to `True`.\n+              Set to `None` if no mask if found above `threshold`.\n+            - **segments_info** -- A dictionary that contains additional information on each segment.\n+                - **id** -- An integer representing the `segment_id`.\n+                - **label_id** -- An integer representing the label / semantic class id corresponding to `segment_id`.\n+                - **score** -- Prediction score of segment with `segment_id`.\n+        \"\"\"\n+        if return_coco_annotation and return_binary_maps:\n+            raise ValueError(\"return_coco_annotation and return_binary_maps can not be both set to True.\")\n+\n+        # [batch_size, num_queries, num_classes+1]\n+        class_queries_logits = outputs.class_queries_logits\n+        # [batch_size, num_queries, height, width]\n+        masks_queries_logits = outputs.masks_queries_logits\n+\n+        device = masks_queries_logits.device\n+        num_classes = class_queries_logits.shape[-1] - 1\n+        num_queries = class_queries_logits.shape[-2]\n+\n+        # Loop over items in batch size\n+        results: list[dict[str, TensorType]] = []\n+\n+        for i in range(class_queries_logits.shape[0]):\n+            mask_pred = masks_queries_logits[i]\n+            mask_cls = class_queries_logits[i]\n+\n+            scores = torch.nn.functional.softmax(mask_cls, dim=-1)[:, :-1]\n+            labels = torch.arange(num_classes, device=device).unsqueeze(0).repeat(num_queries, 1).flatten(0, 1)\n+\n+            scores_per_image, topk_indices = scores.flatten(0, 1).topk(num_queries, sorted=False)\n+            labels_per_image = labels[topk_indices]\n+\n+            topk_indices = torch.div(topk_indices, num_classes, rounding_mode=\"floor\")\n+            mask_pred = mask_pred[topk_indices]\n+            pred_masks = (mask_pred > 0).float()\n+\n+            # Calculate average mask prob\n+            mask_scores_per_image = (mask_pred.sigmoid().flatten(1) * pred_masks.flatten(1)).sum(1) / (\n+                pred_masks.flatten(1).sum(1) + 1e-6\n+            )\n+            pred_scores = scores_per_image * mask_scores_per_image\n+            pred_classes = labels_per_image\n+\n+            segmentation = torch.zeros(masks_queries_logits.shape[2:]) - 1\n+            if target_sizes is not None:\n+                segmentation = torch.zeros(target_sizes[i]) - 1\n+                pred_masks = torch.nn.functional.interpolate(\n+                    pred_masks.unsqueeze(0), size=target_sizes[i], mode=\"nearest\"\n+                )[0]\n+\n+            instance_maps, segments = [], []\n+            current_segment_id = 0\n+            for j in range(num_queries):\n+                score = pred_scores[j].item()\n+\n+                if not torch.all(pred_masks[j] == 0) and score >= threshold:\n+                    segmentation[pred_masks[j] == 1] = current_segment_id\n+                    segments.append(\n+                        {\n+                            \"id\": current_segment_id,\n+                            \"label_id\": pred_classes[j].item(),\n+                            \"was_fused\": False,\n+                            \"score\": round(score, 6),\n+                        }\n+                    )\n+                    current_segment_id += 1\n+                    instance_maps.append(pred_masks[j])\n+\n+            # Return segmentation map in run-length encoding (RLE) format\n+            if return_coco_annotation:\n+                segmentation = convert_segmentation_to_rle(segmentation)\n+\n+            # Return a concatenated tensor of binary instance maps\n+            if return_binary_maps and len(instance_maps) != 0:\n+                segmentation = torch.stack(instance_maps, dim=0)\n+\n+            results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n+        return results\n+\n+    # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.post_process_panoptic_segmentation\n+    def post_process_panoptic_segmentation(\n+        self,\n+        outputs,\n+        threshold: float = 0.5,\n+        mask_threshold: float = 0.5,\n+        overlap_mask_area_threshold: float = 0.8,\n+        label_ids_to_fuse: Optional[set[int]] = None,\n+        target_sizes: Optional[list[tuple[int, int]]] = None,\n+    ) -> list[dict]:\n+        \"\"\"\n+        Converts the output of [`MaskFormerForInstanceSegmentationOutput`] into image panoptic segmentation\n+        predictions. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`MaskFormerForInstanceSegmentationOutput`]):\n+                The outputs from [`MaskFormerForInstanceSegmentation`].\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The probability score threshold to keep predicted instance masks.\n+            mask_threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold to use when turning the predicted masks into binary values.\n+            overlap_mask_area_threshold (`float`, *optional*, defaults to 0.8):\n+                The overlap mask area threshold to merge or discard small disconnected parts within each binary\n+                instance mask.\n+            label_ids_to_fuse (`Set[int]`, *optional*):\n+                The labels in this state will have all their instances be fused together. For instance we could say\n+                there can only be one sky in an image, but several persons, so the label ID for sky would be in that\n+                set, but not the one for person.\n+            target_sizes (`list[Tuple]`, *optional*):\n+                List of length (batch_size), where each list item (`tuple[int, int]]`) corresponds to the requested\n+                final size (height, width) of each prediction in batch. If left to None, predictions will not be\n+                resized.\n+\n+        Returns:\n+            `list[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n+            - **segmentation** -- a tensor of shape `(height, width)` where each pixel represents a `segment_id`, set\n+              to `None` if no mask if found above `threshold`. If `target_sizes` is specified, segmentation is resized\n+              to the corresponding `target_sizes` entry.\n+            - **segments_info** -- A dictionary that contains additional information on each segment.\n+                - **id** -- an integer representing the `segment_id`.\n+                - **label_id** -- An integer representing the label / semantic class id corresponding to `segment_id`.\n+                - **was_fused** -- a boolean, `True` if `label_id` was in `label_ids_to_fuse`, `False` otherwise.\n+                  Multiple instances of the same class / label were fused and assigned a single `segment_id`.\n+                - **score** -- Prediction score of segment with `segment_id`.\n+        \"\"\"\n+\n+        if label_ids_to_fuse is None:\n+            logger.warning(\"`label_ids_to_fuse` unset. No instance will be fused.\")\n+            label_ids_to_fuse = set()\n+\n+        class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n+        masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n+\n+        batch_size = class_queries_logits.shape[0]\n+        num_labels = class_queries_logits.shape[-1] - 1\n+\n+        mask_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n+\n+        # Predicted label and score of each query (batch_size, num_queries)\n+        pred_scores, pred_labels = nn.functional.softmax(class_queries_logits, dim=-1).max(-1)\n+\n+        # Loop over items in batch size\n+        results: list[dict[str, TensorType]] = []\n+\n+        for i in range(batch_size):\n+            mask_probs_item, pred_scores_item, pred_labels_item = remove_low_and_no_objects(\n+                mask_probs[i], pred_scores[i], pred_labels[i], threshold, num_labels\n+            )\n+\n+            # No mask found\n+            if mask_probs_item.shape[0] <= 0:\n+                height, width = target_sizes[i] if target_sizes is not None else mask_probs_item.shape[1:]\n+                segmentation = torch.zeros((height, width)) - 1\n+                results.append({\"segmentation\": segmentation, \"segments_info\": []})\n+                continue\n+\n+            # Get segmentation map and segment information of batch item\n+            target_size = target_sizes[i] if target_sizes is not None else None\n+            segmentation, segments = compute_segments(\n+                mask_probs=mask_probs_item,\n+                pred_scores=pred_scores_item,\n+                pred_labels=pred_labels_item,\n+                mask_threshold=mask_threshold,\n+                overlap_mask_area_threshold=overlap_mask_area_threshold,\n+                label_ids_to_fuse=label_ids_to_fuse,\n+                target_size=target_size,\n+            )\n+\n+            results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n+        return results\n+\n+\n+__all__ = [\"MaskFormerImageProcessorFast\"]"
        },
        {
            "sha": "c2685df19fe768619a54d93c1dd90ba2c610a2ff",
            "filename": "tests/models/mask2former/test_image_processing_mask2former.py",
            "status": "modified",
            "additions": 169,
            "deletions": 91,
            "changes": 260,
            "blob_url": "https://github.com/huggingface/transformers/blob/d9b35c635eb16e837a7b563c519c5231d4e09265/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d9b35c635eb16e837a7b563c519c5231d4e09265/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py?ref=d9b35c635eb16e837a7b563c519c5231d4e09265",
            "patch": "@@ -21,7 +21,7 @@\n \n from transformers.image_utils import ChannelDimension\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -34,6 +34,9 @@\n         from transformers.models.mask2former.image_processing_mask2former import binary_mask_to_rle\n         from transformers.models.mask2former.modeling_mask2former import Mask2FormerForUniversalSegmentationOutput\n \n+        if is_torchvision_available():\n+            from transformers import Mask2FormerImageProcessorFast\n+\n if is_vision_available():\n     from PIL import Image\n \n@@ -54,6 +57,7 @@ def __init__(\n         num_labels=10,\n         do_reduce_labels=True,\n         ignore_index=255,\n+        pad_size=None,\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -66,6 +70,7 @@ def __init__(\n         self.image_mean = image_mean\n         self.image_std = image_std\n         self.size_divisor = 0\n+        self.pad_size = pad_size\n         # for the post_process_functions\n         self.batch_size = 2\n         self.num_queries = 3\n@@ -87,6 +92,7 @@ def prepare_image_processor_dict(self):\n             \"num_labels\": self.num_labels,\n             \"do_reduce_labels\": self.do_reduce_labels,\n             \"ignore_index\": self.ignore_index,\n+            \"pad_size\": self.pad_size,\n         }\n \n     def get_expected_values(self, image_inputs, batched=False):\n@@ -145,10 +151,26 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n         )\n \n \n+# Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_single_inputs\n+def prepare_semantic_single_inputs():\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    example = ds[0]\n+    return example[\"image\"], example[\"map\"]\n+\n+\n+# Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_batch_inputs\n+def prepare_semantic_batch_inputs():\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    return list(ds[\"image\"][:2]), list(ds[\"map\"][:2])\n+\n+\n @require_torch\n @require_vision\n class Mask2FormerImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = Mask2FormerImageProcessor if (is_vision_available() and is_torch_available()) else None\n+    fast_image_processing_class = (\n+        Mask2FormerImageProcessorFast if (is_vision_available() and is_torchvision_available()) else None\n+    )\n \n     def setUp(self):\n         super().setUp()\n@@ -159,25 +181,27 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"ignore_index\"))\n-        self.assertTrue(hasattr(image_processing, \"num_labels\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"ignore_index\"))\n+            self.assertTrue(hasattr(image_processing, \"num_labels\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 32, \"longest_edge\": 1333})\n-        self.assertEqual(image_processor.size_divisor, 0)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 32, \"longest_edge\": 1333})\n+            self.assertEqual(image_processor.size_divisor, 0)\n \n-        image_processor = self.image_processing_class.from_dict(\n-            self.image_processor_dict, size=42, max_size=84, size_divisibility=8\n-        )\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n-        self.assertEqual(image_processor.size_divisor, 8)\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, size=42, max_size=84, size_divisibility=8\n+            )\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n+            self.assertEqual(image_processor.size_divisor, 8)\n \n     def comm_get_image_processing_inputs(\n         self,\n@@ -225,15 +249,16 @@ def comm_get_image_processing_inputs(\n     def test_with_size_divisor(self):\n         size_divisors = [8, 16, 32]\n         weird_input_sizes = [(407, 802), (582, 1094)]\n-        for size_divisor in size_divisors:\n-            image_processor_dict = {**self.image_processor_dict, **{\"size_divisor\": size_divisor}}\n-            image_processing = self.image_processing_class(**image_processor_dict)\n-            for weird_input_size in weird_input_sizes:\n-                inputs = image_processing([np.ones((3, *weird_input_size))], return_tensors=\"pt\")\n-                pixel_values = inputs[\"pixel_values\"]\n-                # check if divisible\n-                self.assertTrue((pixel_values.shape[-1] % size_divisor) == 0)\n-                self.assertTrue((pixel_values.shape[-2] % size_divisor) == 0)\n+        for image_processing_class in self.image_processor_list:\n+            for size_divisor in size_divisors:\n+                image_processor_dict = {**self.image_processor_dict, **{\"size_divisor\": size_divisor}}\n+                image_processing = image_processing_class(**image_processor_dict)\n+                for weird_input_size in weird_input_sizes:\n+                    inputs = image_processing([np.ones((3, *weird_input_size))], return_tensors=\"pt\")\n+                    pixel_values = inputs[\"pixel_values\"]\n+                    # check if divisible\n+                    self.assertTrue((pixel_values.shape[-1] % size_divisor) == 0)\n+                    self.assertTrue((pixel_values.shape[-2] % size_divisor) == 0)\n \n     def test_call_with_segmentation_maps(self):\n         def common(\n@@ -463,91 +488,144 @@ def test_binary_mask_to_rle(self):\n         self.assertEqual(rle[1], 45)\n \n     def test_post_process_semantic_segmentation(self):\n-        fature_extractor = self.image_processing_class(num_labels=self.image_processor_tester.num_classes)\n-        outputs = self.image_processor_tester.get_fake_mask2former_outputs()\n+        for image_processing_class in self.image_processor_list:\n+            fature_extractor = image_processing_class(num_labels=self.image_processor_tester.num_classes)\n+            outputs = self.image_processor_tester.get_fake_mask2former_outputs()\n \n-        segmentation = fature_extractor.post_process_semantic_segmentation(outputs)\n+            segmentation = fature_extractor.post_process_semantic_segmentation(outputs)\n \n-        self.assertEqual(len(segmentation), self.image_processor_tester.batch_size)\n-        self.assertEqual(segmentation[0].shape, (384, 384))\n+            self.assertEqual(len(segmentation), self.image_processor_tester.batch_size)\n+            self.assertEqual(segmentation[0].shape, (384, 384))\n \n-        target_sizes = [(1, 4) for i in range(self.image_processor_tester.batch_size)]\n-        segmentation = fature_extractor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n+            target_sizes = [(1, 4) for i in range(self.image_processor_tester.batch_size)]\n+            segmentation = fature_extractor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n \n-        self.assertEqual(segmentation[0].shape, target_sizes[0])\n+            self.assertEqual(segmentation[0].shape, target_sizes[0])\n \n     def test_post_process_instance_segmentation(self):\n-        image_processor = self.image_processing_class(num_labels=self.image_processor_tester.num_classes)\n-        outputs = self.image_processor_tester.get_fake_mask2former_outputs()\n-        segmentation = image_processor.post_process_instance_segmentation(outputs, threshold=0)\n-\n-        self.assertTrue(len(segmentation) == self.image_processor_tester.batch_size)\n-        for el in segmentation:\n-            self.assertTrue(\"segmentation\" in el)\n-            self.assertTrue(\"segments_info\" in el)\n-            self.assertEqual(type(el[\"segments_info\"]), list)\n-            self.assertEqual(el[\"segmentation\"].shape, (384, 384))\n-\n-        segmentation = image_processor.post_process_instance_segmentation(\n-            outputs, threshold=0, return_binary_maps=True\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(num_labels=self.image_processor_tester.num_classes)\n+            outputs = self.image_processor_tester.get_fake_mask2former_outputs()\n+            segmentation = image_processor.post_process_instance_segmentation(outputs, threshold=0)\n+\n+            self.assertTrue(len(segmentation) == self.image_processor_tester.batch_size)\n+            for el in segmentation:\n+                self.assertTrue(\"segmentation\" in el)\n+                self.assertTrue(\"segments_info\" in el)\n+                self.assertEqual(type(el[\"segments_info\"]), list)\n+                self.assertEqual(el[\"segmentation\"].shape, (384, 384))\n+\n+            segmentation = image_processor.post_process_instance_segmentation(\n+                outputs, threshold=0, return_binary_maps=True\n+            )\n \n-        self.assertTrue(len(segmentation) == self.image_processor_tester.batch_size)\n-        for el in segmentation:\n-            self.assertTrue(\"segmentation\" in el)\n-            self.assertTrue(\"segments_info\" in el)\n-            self.assertEqual(type(el[\"segments_info\"]), list)\n-            self.assertEqual(len(el[\"segmentation\"].shape), 3)\n-            self.assertEqual(el[\"segmentation\"].shape[1:], (384, 384))\n+            self.assertTrue(len(segmentation) == self.image_processor_tester.batch_size)\n+            for el in segmentation:\n+                self.assertTrue(\"segmentation\" in el)\n+                self.assertTrue(\"segments_info\" in el)\n+                self.assertEqual(type(el[\"segments_info\"]), list)\n+                self.assertEqual(len(el[\"segmentation\"].shape), 3)\n+                self.assertEqual(el[\"segmentation\"].shape[1:], (384, 384))\n \n     def test_post_process_panoptic_segmentation(self):\n-        image_processing = self.image_processing_class(num_labels=self.image_processor_tester.num_classes)\n-        outputs = self.image_processor_tester.get_fake_mask2former_outputs()\n-        segmentation = image_processing.post_process_panoptic_segmentation(outputs, threshold=0)\n-\n-        self.assertTrue(len(segmentation) == self.image_processor_tester.batch_size)\n-        for el in segmentation:\n-            self.assertTrue(\"segmentation\" in el)\n-            self.assertTrue(\"segments_info\" in el)\n-            self.assertEqual(type(el[\"segments_info\"]), list)\n-            self.assertEqual(el[\"segmentation\"].shape, (384, 384))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(num_labels=self.image_processor_tester.num_classes)\n+            outputs = self.image_processor_tester.get_fake_mask2former_outputs()\n+            segmentation = image_processing.post_process_panoptic_segmentation(outputs, threshold=0)\n+\n+            self.assertTrue(len(segmentation) == self.image_processor_tester.batch_size)\n+            for el in segmentation:\n+                self.assertTrue(\"segmentation\" in el)\n+                self.assertTrue(\"segments_info\" in el)\n+                self.assertEqual(type(el[\"segments_info\"]), list)\n+                self.assertEqual(el[\"segmentation\"].shape, (384, 384))\n \n     def test_post_process_label_fusing(self):\n-        image_processor = self.image_processing_class(num_labels=self.image_processor_tester.num_classes)\n-        outputs = self.image_processor_tester.get_fake_mask2former_outputs()\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(num_labels=self.image_processor_tester.num_classes)\n+            outputs = self.image_processor_tester.get_fake_mask2former_outputs()\n \n-        segmentation = image_processor.post_process_panoptic_segmentation(\n-            outputs, threshold=0, mask_threshold=0, overlap_mask_area_threshold=0\n-        )\n-        unfused_segments = [el[\"segments_info\"] for el in segmentation]\n+            segmentation = image_processor.post_process_panoptic_segmentation(\n+                outputs, threshold=0, mask_threshold=0, overlap_mask_area_threshold=0\n+            )\n+            unfused_segments = [el[\"segments_info\"] for el in segmentation]\n \n-        fused_segmentation = image_processor.post_process_panoptic_segmentation(\n-            outputs, threshold=0, mask_threshold=0, overlap_mask_area_threshold=0, label_ids_to_fuse={1}\n-        )\n-        fused_segments = [el[\"segments_info\"] for el in fused_segmentation]\n+            fused_segmentation = image_processor.post_process_panoptic_segmentation(\n+                outputs, threshold=0, mask_threshold=0, overlap_mask_area_threshold=0, label_ids_to_fuse={1}\n+            )\n+            fused_segments = [el[\"segments_info\"] for el in fused_segmentation]\n \n-        for el_unfused, el_fused in zip(unfused_segments, fused_segments):\n-            if len(el_unfused) == 0:\n-                self.assertEqual(len(el_unfused), len(el_fused))\n-                continue\n+            for el_unfused, el_fused in zip(unfused_segments, fused_segments):\n+                if len(el_unfused) == 0:\n+                    self.assertEqual(len(el_unfused), len(el_fused))\n+                    continue\n \n-            # Get number of segments to be fused\n-            fuse_targets = [1 for el in el_unfused if el[\"label_id\"] in {1}]\n-            num_to_fuse = 0 if len(fuse_targets) == 0 else sum(fuse_targets) - 1\n-            # Expected number of segments after fusing\n-            expected_num_segments = max([el[\"id\"] for el in el_unfused]) - num_to_fuse\n-            num_segments_fused = max([el[\"id\"] for el in el_fused])\n-            self.assertEqual(num_segments_fused, expected_num_segments)\n+                # Get number of segments to be fused\n+                fuse_targets = [1 for el in el_unfused if el[\"label_id\"] in {1}]\n+                num_to_fuse = 0 if len(fuse_targets) == 0 else sum(fuse_targets) - 1\n+                # Expected number of segments after fusing\n+                expected_num_segments = max([el[\"id\"] for el in el_unfused]) - num_to_fuse\n+                num_segments_fused = max([el[\"id\"] for el in el_fused])\n+                self.assertEqual(num_segments_fused, expected_num_segments)\n \n     def test_removed_deprecated_kwargs(self):\n         image_processor_dict = dict(self.image_processor_dict)\n         image_processor_dict.pop(\"do_reduce_labels\", None)\n         image_processor_dict[\"reduce_labels\"] = True\n \n         # test we are able to create the image processor with the deprecated kwargs\n-        image_processor = self.image_processing_class(**image_processor_dict)\n-        self.assertEqual(image_processor.do_reduce_labels, True)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**image_processor_dict)\n+            self.assertEqual(image_processor.do_reduce_labels, True)\n+\n+            # test we still support reduce_labels with config\n+            image_processor = image_processing_class.from_dict(image_processor_dict)\n+            self.assertEqual(image_processor.do_reduce_labels, True)\n+\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image, dummy_map = prepare_semantic_single_inputs()\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        image_encoding_slow = image_processor_slow(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n+        image_encoding_fast = image_processor_fast(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(image_encoding_slow.pixel_values, image_encoding_fast.pixel_values)\n+        for mask_label_slow, mask_label_fast in zip(image_encoding_slow.mask_labels, image_encoding_fast.mask_labels):\n+            self._assert_slow_fast_tensors_equivalence(mask_label_slow, mask_label_fast)\n+        for class_label_slow, class_label_fast in zip(\n+            image_encoding_slow.class_labels, image_encoding_fast.class_labels\n+        ):\n+            self._assert_slow_fast_tensors_equivalence(class_label_slow.float(), class_label_fast.float())\n+\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images, dummy_maps = prepare_semantic_batch_inputs()\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, segmentation_maps=dummy_maps, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, segmentation_maps=dummy_maps, return_tensors=\"pt\")\n \n-        # test we still support reduce_labels with config\n-        image_processor = self.image_processing_class.from_dict(image_processor_dict)\n-        self.assertEqual(image_processor.do_reduce_labels, True)\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        for mask_label_slow, mask_label_fast in zip(encoding_slow.mask_labels, encoding_fast.mask_labels):\n+            self._assert_slow_fast_tensors_equivalence(mask_label_slow, mask_label_fast)\n+        for class_label_slow, class_label_fast in zip(encoding_slow.class_labels, encoding_fast.class_labels):\n+            self._assert_slow_fast_tensors_equivalence(class_label_slow.float(), class_label_fast.float())"
        },
        {
            "sha": "01a9f0d0867ee23472ea9b0ee19ee9a07b8da6a9",
            "filename": "tests/models/maskformer/test_image_processing_maskformer.py",
            "status": "modified",
            "additions": 192,
            "deletions": 117,
            "changes": 309,
            "blob_url": "https://github.com/huggingface/transformers/blob/d9b35c635eb16e837a7b563c519c5231d4e09265/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d9b35c635eb16e837a7b563c519c5231d4e09265/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py?ref=d9b35c635eb16e837a7b563c519c5231d4e09265",
            "patch": "@@ -20,7 +20,7 @@\n from huggingface_hub import hf_hub_download\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -33,6 +33,9 @@\n         from transformers.models.maskformer.image_processing_maskformer import binary_mask_to_rle\n         from transformers.models.maskformer.modeling_maskformer import MaskFormerForInstanceSegmentationOutput\n \n+        if is_torchvision_available():\n+            from transformers import MaskFormerImageProcessorFast\n+\n if is_vision_available():\n     from PIL import Image\n \n@@ -144,10 +147,26 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n         )\n \n \n+# Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_single_inputs\n+def prepare_semantic_single_inputs():\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    example = ds[0]\n+    return example[\"image\"], example[\"map\"]\n+\n+\n+# Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_batch_inputs\n+def prepare_semantic_batch_inputs():\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    return list(ds[\"image\"][:2]), list(ds[\"map\"][:2])\n+\n+\n @require_torch\n @require_vision\n class MaskFormerImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = MaskFormerImageProcessor if (is_vision_available() and is_torch_available()) else None\n+    fast_image_processing_class = (\n+        MaskFormerImageProcessorFast if (is_vision_available() and is_torchvision_available()) else None\n+    )\n \n     def setUp(self):\n         super().setUp()\n@@ -158,25 +177,27 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"ignore_index\"))\n-        self.assertTrue(hasattr(image_processing, \"num_labels\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"ignore_index\"))\n+            self.assertTrue(hasattr(image_processing, \"num_labels\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 32, \"longest_edge\": 1333})\n-        self.assertEqual(image_processor.size_divisor, 0)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 32, \"longest_edge\": 1333})\n+            self.assertEqual(image_processor.size_divisor, 0)\n \n-        image_processor = self.image_processing_class.from_dict(\n-            self.image_processor_dict, size=42, max_size=84, size_divisibility=8\n-        )\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n-        self.assertEqual(image_processor.size_divisor, 8)\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, size=42, max_size=84, size_divisibility=8\n+            )\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n+            self.assertEqual(image_processor.size_divisor, 8)\n \n     def comm_get_image_processing_inputs(\n         self, with_segmentation_maps=False, is_instance_map=False, segmentation_type=\"np\"\n@@ -211,15 +232,16 @@ def comm_get_image_processing_inputs(\n     def test_with_size_divisor(self):\n         size_divisors = [8, 16, 32]\n         weird_input_sizes = [(407, 802), (582, 1094)]\n-        for size_divisor in size_divisors:\n-            image_processor_dict = {**self.image_processor_dict, **{\"size_divisor\": size_divisor}}\n-            image_processing = self.image_processing_class(**image_processor_dict)\n-            for weird_input_size in weird_input_sizes:\n-                inputs = image_processing([np.ones((3, *weird_input_size))], return_tensors=\"pt\")\n-                pixel_values = inputs[\"pixel_values\"]\n-                # check if divisible\n-                self.assertTrue((pixel_values.shape[-1] % size_divisor) == 0)\n-                self.assertTrue((pixel_values.shape[-2] % size_divisor) == 0)\n+        for image_processing_class in self.image_processor_list:\n+            for size_divisor in size_divisors:\n+                image_processor_dict = {**self.image_processor_dict, **{\"size_divisor\": size_divisor}}\n+                image_processing = image_processing_class(**image_processor_dict)\n+                for weird_input_size in weird_input_sizes:\n+                    inputs = image_processing([np.ones((3, *weird_input_size))], return_tensors=\"pt\")\n+                    pixel_values = inputs[\"pixel_values\"]\n+                    # check if divisible\n+                    self.assertTrue((pixel_values.shape[-1] % size_divisor) == 0)\n+                    self.assertTrue((pixel_values.shape[-2] % size_divisor) == 0)\n \n     def test_call_with_segmentation_maps(self):\n         def common(is_instance_map=False, segmentation_type=None):\n@@ -417,116 +439,122 @@ def test_binary_mask_to_rle(self):\n         self.assertEqual(rle[1], 45)\n \n     def test_post_process_segmentation(self):\n-        fature_extractor = self.image_processing_class(num_labels=self.image_processor_tester.num_classes)\n-        outputs = self.image_processor_tester.get_fake_maskformer_outputs()\n-        segmentation = fature_extractor.post_process_segmentation(outputs)\n-\n-        self.assertEqual(\n-            segmentation.shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.num_classes,\n-                self.image_processor_tester.height,\n-                self.image_processor_tester.width,\n-            ),\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            feature_extractor = image_processing_class(num_labels=self.image_processor_tester.num_classes)\n+            outputs = self.image_processor_tester.get_fake_maskformer_outputs()\n+            segmentation = feature_extractor.post_process_segmentation(outputs)\n \n-        target_size = (1, 4)\n-        segmentation = fature_extractor.post_process_segmentation(outputs, target_size=target_size)\n+            self.assertEqual(\n+                segmentation.shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_classes,\n+                    self.image_processor_tester.height,\n+                    self.image_processor_tester.width,\n+                ),\n+            )\n \n-        self.assertEqual(\n-            segmentation.shape,\n-            (self.image_processor_tester.batch_size, self.image_processor_tester.num_classes, *target_size),\n-        )\n+            target_size = (1, 4)\n+            segmentation = feature_extractor.post_process_segmentation(outputs, target_size=target_size)\n \n-    def test_post_process_semantic_segmentation(self):\n-        fature_extractor = self.image_processing_class(num_labels=self.image_processor_tester.num_classes)\n-        outputs = self.image_processor_tester.get_fake_maskformer_outputs()\n-\n-        segmentation = fature_extractor.post_process_semantic_segmentation(outputs)\n-\n-        self.assertEqual(len(segmentation), self.image_processor_tester.batch_size)\n-        self.assertEqual(\n-            segmentation[0].shape,\n-            (\n-                self.image_processor_tester.height,\n-                self.image_processor_tester.width,\n-            ),\n-        )\n+            self.assertEqual(\n+                segmentation.shape,\n+                (self.image_processor_tester.batch_size, self.image_processor_tester.num_classes, *target_size),\n+            )\n \n-        target_sizes = [(1, 4) for i in range(self.image_processor_tester.batch_size)]\n-        segmentation = fature_extractor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n+    def test_post_process_semantic_segmentation(self):\n+        for image_processing_class in self.image_processor_list:\n+            feature_extractor = image_processing_class(num_labels=self.image_processor_tester.num_classes)\n+            outputs = self.image_processor_tester.get_fake_maskformer_outputs()\n \n-        self.assertEqual(segmentation[0].shape, target_sizes[0])\n+            segmentation = feature_extractor.post_process_semantic_segmentation(outputs)\n \n-    def test_post_process_instance_segmentation(self):\n-        image_processor = self.image_processing_class(num_labels=self.image_processor_tester.num_classes)\n-        outputs = self.image_processor_tester.get_fake_maskformer_outputs()\n-        segmentation = image_processor.post_process_instance_segmentation(outputs, threshold=0)\n-\n-        self.assertTrue(len(segmentation) == self.image_processor_tester.batch_size)\n-        for el in segmentation:\n-            self.assertTrue(\"segmentation\" in el)\n-            self.assertTrue(\"segments_info\" in el)\n-            self.assertEqual(type(el[\"segments_info\"]), list)\n+            self.assertEqual(len(segmentation), self.image_processor_tester.batch_size)\n             self.assertEqual(\n-                el[\"segmentation\"].shape, (self.image_processor_tester.height, self.image_processor_tester.width)\n+                segmentation[0].shape,\n+                (\n+                    self.image_processor_tester.height,\n+                    self.image_processor_tester.width,\n+                ),\n             )\n \n-        segmentation = image_processor.post_process_instance_segmentation(\n-            outputs, threshold=0, return_binary_maps=True\n-        )\n+            target_sizes = [(1, 4) for i in range(self.image_processor_tester.batch_size)]\n+            segmentation = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n \n-        self.assertTrue(len(segmentation) == self.image_processor_tester.batch_size)\n-        for el in segmentation:\n-            self.assertTrue(\"segmentation\" in el)\n-            self.assertTrue(\"segments_info\" in el)\n-            self.assertEqual(type(el[\"segments_info\"]), list)\n-            self.assertEqual(len(el[\"segmentation\"].shape), 3)\n-            self.assertEqual(\n-                el[\"segmentation\"].shape[1:], (self.image_processor_tester.height, self.image_processor_tester.width)\n+            self.assertEqual(segmentation[0].shape, target_sizes[0])\n+\n+    def test_post_process_instance_segmentation(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(num_labels=self.image_processor_tester.num_classes)\n+            outputs = self.image_processor_tester.get_fake_maskformer_outputs()\n+            segmentation = image_processor.post_process_instance_segmentation(outputs, threshold=0)\n+\n+            self.assertTrue(len(segmentation) == self.image_processor_tester.batch_size)\n+            for el in segmentation:\n+                self.assertTrue(\"segmentation\" in el)\n+                self.assertTrue(\"segments_info\" in el)\n+                self.assertEqual(type(el[\"segments_info\"]), list)\n+                self.assertEqual(\n+                    el[\"segmentation\"].shape, (self.image_processor_tester.height, self.image_processor_tester.width)\n+                )\n+\n+            segmentation = image_processor.post_process_instance_segmentation(\n+                outputs, threshold=0, return_binary_maps=True\n             )\n \n+            self.assertTrue(len(segmentation) == self.image_processor_tester.batch_size)\n+            for el in segmentation:\n+                self.assertTrue(\"segmentation\" in el)\n+                self.assertTrue(\"segments_info\" in el)\n+                self.assertEqual(type(el[\"segments_info\"]), list)\n+                self.assertEqual(len(el[\"segmentation\"].shape), 3)\n+                self.assertEqual(\n+                    el[\"segmentation\"].shape[1:],\n+                    (self.image_processor_tester.height, self.image_processor_tester.width),\n+                )\n+\n     def test_post_process_panoptic_segmentation(self):\n-        image_processing = self.image_processing_class(num_labels=self.image_processor_tester.num_classes)\n-        outputs = self.image_processor_tester.get_fake_maskformer_outputs()\n-        segmentation = image_processing.post_process_panoptic_segmentation(outputs, threshold=0)\n-\n-        self.assertTrue(len(segmentation) == self.image_processor_tester.batch_size)\n-        for el in segmentation:\n-            self.assertTrue(\"segmentation\" in el)\n-            self.assertTrue(\"segments_info\" in el)\n-            self.assertEqual(type(el[\"segments_info\"]), list)\n-            self.assertEqual(\n-                el[\"segmentation\"].shape, (self.image_processor_tester.height, self.image_processor_tester.width)\n-            )\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(num_labels=self.image_processor_tester.num_classes)\n+            outputs = self.image_processor_tester.get_fake_maskformer_outputs()\n+            segmentation = image_processing.post_process_panoptic_segmentation(outputs, threshold=0)\n+\n+            self.assertTrue(len(segmentation) == self.image_processor_tester.batch_size)\n+            for el in segmentation:\n+                self.assertTrue(\"segmentation\" in el)\n+                self.assertTrue(\"segments_info\" in el)\n+                self.assertEqual(type(el[\"segments_info\"]), list)\n+                self.assertEqual(\n+                    el[\"segmentation\"].shape, (self.image_processor_tester.height, self.image_processor_tester.width)\n+                )\n \n     def test_post_process_label_fusing(self):\n-        image_processor = self.image_processing_class(num_labels=self.image_processor_tester.num_classes)\n-        outputs = self.image_processor_tester.get_fake_maskformer_outputs()\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = self.image_processing_class(num_labels=self.image_processor_tester.num_classes)\n+            outputs = self.image_processor_tester.get_fake_maskformer_outputs()\n \n-        segmentation = image_processor.post_process_panoptic_segmentation(\n-            outputs, threshold=0, mask_threshold=0, overlap_mask_area_threshold=0\n-        )\n-        unfused_segments = [el[\"segments_info\"] for el in segmentation]\n+            segmentation = image_processor.post_process_panoptic_segmentation(\n+                outputs, threshold=0, mask_threshold=0, overlap_mask_area_threshold=0\n+            )\n+            unfused_segments = [el[\"segments_info\"] for el in segmentation]\n \n-        fused_segmentation = image_processor.post_process_panoptic_segmentation(\n-            outputs, threshold=0, mask_threshold=0, overlap_mask_area_threshold=0, label_ids_to_fuse={1}\n-        )\n-        fused_segments = [el[\"segments_info\"] for el in fused_segmentation]\n+            fused_segmentation = image_processor.post_process_panoptic_segmentation(\n+                outputs, threshold=0, mask_threshold=0, overlap_mask_area_threshold=0, label_ids_to_fuse={1}\n+            )\n+            fused_segments = [el[\"segments_info\"] for el in fused_segmentation]\n \n-        for el_unfused, el_fused in zip(unfused_segments, fused_segments):\n-            if len(el_unfused) == 0:\n-                self.assertEqual(len(el_unfused), len(el_fused))\n-                continue\n+            for el_unfused, el_fused in zip(unfused_segments, fused_segments):\n+                if len(el_unfused) == 0:\n+                    self.assertEqual(len(el_unfused), len(el_fused))\n+                    continue\n \n-            # Get number of segments to be fused\n-            fuse_targets = [1 for el in el_unfused if el[\"label_id\"] in {1}]\n-            num_to_fuse = 0 if len(fuse_targets) == 0 else sum(fuse_targets) - 1\n-            # Expected number of segments after fusing\n-            expected_num_segments = max([el[\"id\"] for el in el_unfused]) - num_to_fuse\n-            num_segments_fused = max([el[\"id\"] for el in el_fused])\n-            self.assertEqual(num_segments_fused, expected_num_segments)\n+                # Get number of segments to be fused\n+                fuse_targets = [1 for el in el_unfused if el[\"label_id\"] in {1}]\n+                num_to_fuse = 0 if len(fuse_targets) == 0 else sum(fuse_targets) - 1\n+                # Expected number of segments after fusing\n+                expected_num_segments = max([el[\"id\"] for el in el_unfused]) - num_to_fuse\n+                num_segments_fused = max([el[\"id\"] for el in el_fused])\n+                self.assertEqual(num_segments_fused, expected_num_segments)\n \n     def test_removed_deprecated_kwargs(self):\n         image_processor_dict = dict(self.image_processor_dict)\n@@ -540,3 +568,50 @@ def test_removed_deprecated_kwargs(self):\n         # test we still support reduce_labels with config\n         image_processor = self.image_processing_class.from_dict(image_processor_dict)\n         self.assertEqual(image_processor.do_reduce_labels, True)\n+\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image, dummy_map = prepare_semantic_single_inputs()\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        image_encoding_slow = image_processor_slow(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n+        image_encoding_fast = image_processor_fast(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(image_encoding_slow.pixel_values, image_encoding_fast.pixel_values)\n+        for mask_label_slow, mask_label_fast in zip(image_encoding_slow.mask_labels, image_encoding_fast.mask_labels):\n+            self._assert_slow_fast_tensors_equivalence(mask_label_slow, mask_label_fast)\n+        for class_label_slow, class_label_fast in zip(\n+            image_encoding_slow.class_labels, image_encoding_fast.class_labels\n+        ):\n+            self._assert_slow_fast_tensors_equivalence(class_label_slow.float(), class_label_fast.float())\n+\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images, dummy_maps = prepare_semantic_batch_inputs()\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, segmentation_maps=dummy_maps, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, segmentation_maps=dummy_maps, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        for mask_label_slow, mask_label_fast in zip(encoding_slow.mask_labels, encoding_fast.mask_labels):\n+            self._assert_slow_fast_tensors_equivalence(mask_label_slow, mask_label_fast)\n+        for class_label_slow, class_label_fast in zip(encoding_slow.class_labels, encoding_fast.class_labels):\n+            self._assert_slow_fast_tensors_equivalence(class_label_slow.float(), class_label_fast.float())"
        }
    ],
    "stats": {
        "total": 2573,
        "additions": 2349,
        "deletions": 224
    }
}