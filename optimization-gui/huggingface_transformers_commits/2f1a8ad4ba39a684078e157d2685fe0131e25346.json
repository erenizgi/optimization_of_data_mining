{
    "author": "zucchini-nlp",
    "message": "Fix setting attention for multimodal models (#39984)\n\n* fix\n\n* use non-explicit `None`\n\n* keep previously set attn if exists",
    "sha": "2f1a8ad4ba39a684078e157d2685fe0131e25346",
    "files": [
        {
            "sha": "442b19a7c9aa6cd7f8f30fe4b4663ded2fe31d29",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f1a8ad4ba39a684078e157d2685fe0131e25346/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f1a8ad4ba39a684078e157d2685fe0131e25346/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=2f1a8ad4ba39a684078e157d2685fe0131e25346",
            "patch": "@@ -410,15 +410,17 @@ def _attn_implementation(self):\n     def _attn_implementation(self, value: Optional[Union[str, dict]]):\n         \"\"\"We set it recursively on the sub-configs as well\"\"\"\n         # Set if for current config\n-        attn_implementation = value if not isinstance(value, dict) else value.get(\"\", self._attn_implementation)\n+        current_attn = getattr(self, \"_attn_implementation\", None)\n+        attn_implementation = value if not isinstance(value, dict) else value.get(\"\", current_attn)\n         self._attn_implementation_internal = attn_implementation\n \n         # Set it recursively on the subconfigs\n         for subconfig_key in self.sub_configs:\n             subconfig = getattr(self, subconfig_key, None)\n             if subconfig is not None:\n+                current_subconfig_attn = getattr(subconfig, \"_attn_implementation\", None)\n                 sub_implementation = (\n-                    value if not isinstance(value, dict) else value.get(subconfig_key, subconfig._attn_implementation)\n+                    value if not isinstance(value, dict) else value.get(subconfig_key, current_subconfig_attn)\n                 )\n                 subconfig._attn_implementation = sub_implementation\n "
        },
        {
            "sha": "fff4bb896312c7b77b5349a0355e76546a7bdc1f",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f1a8ad4ba39a684078e157d2685fe0131e25346/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f1a8ad4ba39a684078e157d2685fe0131e25346/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=2f1a8ad4ba39a684078e157d2685fe0131e25346",
            "patch": "@@ -3680,6 +3680,20 @@ def test_attn_implementation_composite_models(self):\n             model = model_class(config)\n             self.assertTrue(model.config.get_text_config(decoder=True)._attn_implementation == \"eager\")\n \n+            # Test that using `dict` atttention implementation works with `from_pretrained`\n+            #  Set all backbones to \"eager\" because \"eager\" attention is always available\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                new_model = model.from_pretrained(tmpdirname, attn_implementation=attn_implementation_per_subconfig)\n+                self.assertTrue(new_model.config._attn_implementation == \"eager\")\n+                for submodule in new_model.modules():\n+                    if (\n+                        submodule is not new_model\n+                        and isinstance(submodule, PreTrainedModel)\n+                        and submodule.config.__class__ != new_model.config.__class__\n+                    ):\n+                        self.assertTrue(submodule.config._attn_implementation == \"eager\")\n+\n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_non_composite_models(self):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 20,
        "additions": 18,
        "deletions": 2
    }
}