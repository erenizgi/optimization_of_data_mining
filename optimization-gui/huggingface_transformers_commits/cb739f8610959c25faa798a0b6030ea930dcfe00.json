{
    "author": "MekkCyber",
    "message": "[core] fix mxfp4 (#42382)\n\n* initial commit\n\n* fix import\n\n* fix\n\n* add ops\n\n* style\n\n* decouple dequantize & deserialize logic\n\n* up",
    "sha": "cb739f8610959c25faa798a0b6030ea930dcfe00",
    "files": [
        {
            "sha": "6430c0d9d57dd87a364d494a4b583c422ffea4ae",
            "filename": "src/transformers/integrations/mxfp4.py",
            "status": "modified",
            "additions": 186,
            "deletions": 10,
            "changes": 196,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb739f8610959c25faa798a0b6030ea930dcfe00/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb739f8610959c25faa798a0b6030ea930dcfe00/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmxfp4.py?ref=cb739f8610959c25faa798a0b6030ea930dcfe00",
            "patch": "@@ -18,13 +18,19 @@\n if is_torch_available():\n     import torch\n     from torch import nn\n+from typing import Optional\n+\n+from ..core_model_loading import ConversionOps\n+\n \n if is_accelerate_available():\n     from accelerate import init_empty_weights\n \n import re\n from contextlib import contextmanager\n \n+from ..quantizers.quantizers_utils import get_module_from_name\n+\n \n logger = logging.get_logger(__name__)\n \n@@ -70,6 +76,126 @@ def on_device(dev):\n     yield\n \n \n+class Mxfp4Quantize(ConversionOps):\n+    def __init__(self, hf_quantizer):\n+        self.hf_quantizer = hf_quantizer\n+\n+    def convert(\n+        self,\n+        input_dict: dict[str, torch.Tensor],\n+        model: Optional[torch.nn.Module] = None,\n+        missing_keys: Optional[list[str]] = None,\n+        full_layer_name: str | None = None,\n+        **kwargs,\n+    ) -> dict[str, torch.Tensor]:\n+        _, value = tuple(input_dict.items())[0]\n+        value = value[0] if isinstance(value, list) else value\n+\n+        module, _ = get_module_from_name(model, full_layer_name)\n+\n+        with torch.device(value.device):\n+            if isinstance(module, Mxfp4GptOssExperts):\n+                triton_weight_tensor, weight_scale = quantize_to_mxfp4(value.transpose(-1, -2), triton_kernels_hub)\n+                PrecisionConfig, FlexCtx, InFlexData = (\n+                    triton_kernels_hub.matmul_ogs.PrecisionConfig,\n+                    triton_kernels_hub.matmul_ogs.FlexCtx,\n+                    triton_kernels_hub.matmul_ogs.InFlexData,\n+                )\n+                triton_weight_tensor, weight_scale = swizzle_mxfp4(\n+                    triton_weight_tensor, weight_scale, triton_kernels_hub\n+                )\n+\n+                proj = \"gate_up_proj\" if \"gate_up_proj\" in full_layer_name else \"down_proj\"\n+\n+                if proj in module._parameters:\n+                    # Remove the nn.Parameter registration so we can attach the Triton tensor\n+                    del module._parameters[proj]\n+\n+                setattr(module, proj, triton_weight_tensor)\n+                setattr(\n+                    module,\n+                    f\"{proj}_precision_config\",\n+                    PrecisionConfig(weight_scale=weight_scale, flex_ctx=FlexCtx(rhs_data=InFlexData())),\n+                )\n+\n+                missing_keys.discard(f\"{full_layer_name}\")\n+                module._is_hf_initialized = True\n+\n+                return {}\n+\n+\n+class Mxfp4Dequantize(ConversionOps):\n+    def __init__(self, hf_quantizer):\n+        self.hf_quantizer = hf_quantizer\n+\n+    def convert(\n+        self,\n+        input_dict: dict[str, torch.Tensor],\n+        model: Optional[torch.nn.Module] = None,\n+        full_layer_name: str | None = None,\n+        missing_keys=None,\n+        **kwargs,\n+    ) -> dict[str, torch.Tensor]:\n+        param_data = {}\n+        if \"_blocks\" in input_dict.keys():\n+            if isinstance(input_dict[\"_blocks\"], list):\n+                param_data[\"_blocks\"] = input_dict[\"_blocks\"][0]\n+            else:\n+                param_data[\"_blocks\"] = input_dict[\"_blocks\"]\n+        if \"_scales\" in input_dict.keys():\n+            if isinstance(input_dict[\"_scales\"], list):\n+                param_data[\"_scales\"] = input_dict[\"_scales\"][0]\n+            else:\n+                param_data[\"_scales\"] = input_dict[\"_scales\"]\n+\n+        # Here we are dequantizing the weights\n+        dequantized = dequantize_convertops(param_data[\"_blocks\"], param_data[\"_scales\"], param_data[\"_blocks\"].device)\n+        return {full_layer_name: dequantized}\n+\n+\n+class Mxfp4Deserialize(ConversionOps):\n+    def __init__(self, hf_quantizer):\n+        self.hf_quantizer = hf_quantizer\n+\n+    def convert(\n+        self,\n+        input_dict: dict[str, torch.Tensor],\n+        model: Optional[torch.nn.Module] = None,\n+        full_layer_name: str | None = None,\n+        missing_keys: Optional[list[str]] = None,\n+        **kwargs,\n+    ) -> dict[str, torch.Tensor]:\n+        param_data = {}\n+        if \"_blocks\" in input_dict.keys():\n+            if isinstance(input_dict[\"_blocks\"], list):\n+                param_data[\"_blocks\"] = input_dict[\"_blocks\"][0]\n+            else:\n+                param_data[\"_blocks\"] = input_dict[\"_blocks\"]\n+        if \"_scales\" in input_dict.keys():\n+            if isinstance(input_dict[\"_scales\"], list):\n+                param_data[\"_scales\"] = input_dict[\"_scales\"][0]\n+            else:\n+                param_data[\"_scales\"] = input_dict[\"_scales\"]\n+\n+        # Eagerly set tensors on the module and perform swizzle\n+        module, _ = get_module_from_name(model, full_layer_name)\n+        proj = \"gate_up_proj\" if \"gate_up_proj\" in full_layer_name else \"down_proj\"\n+        swizzle_mxfp4_convertops(\n+            param_data[\"_blocks\"],\n+            param_data[\"_scales\"],\n+            module,\n+            proj,\n+            param_data[\"_blocks\"].device,\n+            triton_kernels_hub,\n+        )\n+        missing_keys.discard(f\"{full_layer_name}\")\n+        module._is_hf_initialized = True\n+        # We return an empty mapping since the module was updated in-place. This prevents\n+        # the loader from trying to materialize the original meta-parameter names again.\n+        # We don't use set_param_for_module since it expects mainly a torch.nn.Parameter or a safetensors pointer\n+        return {}\n+\n+\n # Copied from GPT_OSS repo and vllm\n def quantize_to_mxfp4(w, triton_kernels_hub):\n     downcast_to_mxfp_torch = triton_kernels_hub.numerics_details.mxfp.downcast_to_mxfp_torch\n@@ -110,6 +236,7 @@ def convert_moe_packed_tensors(\n     \"\"\"\n     import math\n \n+    blocks = blocks.to(torch.uint8)\n     # Check if blocks and scales are on CPU, and move to GPU if so\n     if not blocks.is_cuda and torch.cuda.is_available():\n         blocks = blocks.cuda()\n@@ -162,26 +289,20 @@ def __init__(self, config):\n         self.intermediate_size = config.intermediate_size\n         self.hidden_size = config.hidden_size\n \n-        self.gate_up_proj_blocks = nn.Parameter(\n+        self.gate_up_proj = nn.Parameter(\n             torch.zeros(self.num_experts, 2 * self.intermediate_size, self.hidden_size // 32, 16, dtype=torch.uint8),\n             requires_grad=False,\n         )\n-        self.gate_up_proj_scales = nn.Parameter(\n-            torch.zeros(self.num_experts, 2 * self.intermediate_size, self.hidden_size // 32, dtype=torch.uint8),\n-            requires_grad=False,\n-        )\n+\n         self.gate_up_proj_bias = nn.Parameter(\n             torch.zeros(self.num_experts, 2 * self.intermediate_size, dtype=torch.float32), requires_grad=False\n         )\n \n-        self.down_proj_blocks = nn.Parameter(\n+        self.down_proj = nn.Parameter(\n             torch.zeros((self.num_experts, self.hidden_size, self.intermediate_size // 32, 16), dtype=torch.uint8),\n             requires_grad=False,\n         )\n-        self.down_proj_scales = nn.Parameter(\n-            torch.zeros(self.num_experts, self.hidden_size, self.intermediate_size // 32, dtype=torch.uint8),\n-            requires_grad=False,\n-        )\n+\n         self.down_proj_bias = nn.Parameter(\n             torch.zeros(self.num_experts, self.hidden_size, dtype=torch.float32), requires_grad=False\n         )\n@@ -361,6 +482,14 @@ def dequantize(module, param_name, param_value, target_device, dq_param_name, **\n                 delattr(module, scales_attr)\n \n \n+def dequantize_convertops(blocks, scales, target_device):\n+    dequantized = convert_moe_packed_tensors(blocks, scales)\n+    if target_device == \"cpu\" and torch.cuda.is_available():\n+        torch.cuda.empty_cache()\n+    dequantized = torch.nn.Parameter(dequantized.to(target_device))\n+    return dequantized\n+\n+\n def load_and_swizzle_mxfp4(module, param_name, param_value, target_device, triton_kernels_hub, **kwargs):\n     \"\"\"\n     This transforms the weights obtained using `convert_gpt_oss.py` to load them into `Mxfp4GptOssExperts`.\n@@ -428,6 +557,53 @@ def load_and_swizzle_mxfp4(module, param_name, param_value, target_device, trito\n         del blocks\n \n \n+def swizzle_mxfp4_convertops(blocks, scales, module, proj, target_device, triton_kernels_hub):\n+    \"\"\"\n+    This transforms the weights obtained using `convert_gpt_oss.py` to load them into `Mxfp4GptOssExperts`.\n+    \"\"\"\n+    PrecisionConfig, FlexCtx, InFlexData = (\n+        triton_kernels_hub.matmul_ogs.PrecisionConfig,\n+        triton_kernels_hub.matmul_ogs.FlexCtx,\n+        triton_kernels_hub.matmul_ogs.InFlexData,\n+    )\n+\n+    local_experts = blocks.size(0)\n+    if getattr(target_device, \"type\", target_device) == \"cpu\":\n+        target_device = torch.accelerator.current_accelerator().type if hasattr(torch, \"accelerator\") else \"cuda\"\n+\n+    blocks = blocks.to(target_device).contiguous()\n+    scales = scales.to(target_device).contiguous()\n+\n+    if proj == \"gate_up_proj\":\n+        blocks = blocks.reshape(local_experts, module.intermediate_size * 2, -1)\n+    else:\n+        blocks = blocks.reshape(local_experts, -1, module.intermediate_size // 2)\n+    if getattr(target_device, \"type\", target_device) == \"cpu\":\n+        target_device = \"cuda\"\n+\n+    with on_device(target_device):\n+        triton_weight_tensor, weight_scale = swizzle_mxfp4(\n+            blocks.transpose(-2, -1), scales.transpose(-2, -1), triton_kernels_hub\n+        )\n+    # need to overwrite the shapes for the kernels\n+    if proj == \"gate_up_proj\":\n+        triton_weight_tensor.shape = torch.Size([local_experts, module.hidden_size, module.intermediate_size * 2])\n+    else:\n+        triton_weight_tensor.shape = torch.Size([local_experts, module.intermediate_size, module.hidden_size])\n+\n+    # triton_weight_tensor is what needs to be passed in oai kernels. It stores the data, the shapes and any more objects. It's like a subtensor\n+    # Since the Experts module registers gate_up_proj and down_proj as nn.Parameters, we need to remove them so we can attach the Triton tensor\n+    if proj in module._parameters:\n+        # Remove the nn.Parameter registration so we can attach the Triton tensor\n+        del module._parameters[proj]\n+    setattr(module, proj, triton_weight_tensor)\n+    setattr(\n+        module,\n+        f\"{proj}_precision_config\",\n+        PrecisionConfig(weight_scale=weight_scale, flex_ctx=FlexCtx(rhs_data=InFlexData())),\n+    )\n+\n+\n def _replace_with_mxfp4_linear(\n     model,\n     modules_to_not_convert=None,"
        },
        {
            "sha": "ae91138653d2a8c0560189f82f778ba4a0e670af",
            "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb739f8610959c25faa798a0b6030ea930dcfe00/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb739f8610959c25faa798a0b6030ea930dcfe00/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py?ref=cb739f8610959c25faa798a0b6030ea930dcfe00",
            "patch": "@@ -32,6 +32,8 @@\n if is_torch_available():\n     import torch\n \n+    from ..core_model_loading import WeightConverter\n+\n logger = logging.get_logger(__name__)\n triton_kernels_hub = None\n \n@@ -157,6 +159,8 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n         from ..integrations import Mxfp4GptOssExperts\n         from ..models.gpt_oss.modeling_gpt_oss import GptOssExperts\n \n+        if self.pre_quantized:\n+            return False\n         # if we are dequantizing, the model doesn't have scales, and blocks only params like gate_up_proj and down_proj so we need to handle this case differently\n         if self.quantization_config.dequantize and (\"blocks\" in param_name or \"scales\" in param_name):\n             module, tensor_name = get_module_from_name(model, param_name[: -len(\"_blocks\")])\n@@ -426,3 +430,30 @@ def is_trainable(self) -> bool:\n             \"MXFP4 quantization don't support training, please consider dequantizing the model first by passing quantization_config=Mxfp4Config(dequantize=True) to .from_pretrained()\"\n         )\n         return False\n+\n+    def get_quantize_ops(self):\n+        from ..integrations.mxfp4 import Mxfp4Quantize\n+\n+        return Mxfp4Quantize(self)\n+\n+    def get_weight_conversions(self):\n+        from ..integrations.mxfp4 import Mxfp4Dequantize, Mxfp4Deserialize\n+\n+        if self.pre_quantized:\n+            if self.quantization_config.dequantize:\n+                return [\n+                    WeightConverter(\n+                        source_keys=[\"_blocks\", \"_scales\"],\n+                        target_keys=\"\",\n+                        operations=[Mxfp4Dequantize(self)],\n+                    )\n+                ]\n+            else:\n+                return [\n+                    WeightConverter(\n+                        source_keys=[\"_blocks\", \"_scales\"],\n+                        target_keys=\"\",\n+                        operations=[Mxfp4Deserialize(self)],\n+                    )\n+                ]\n+        return []"
        }
    ],
    "stats": {
        "total": 227,
        "additions": 217,
        "deletions": 10
    }
}