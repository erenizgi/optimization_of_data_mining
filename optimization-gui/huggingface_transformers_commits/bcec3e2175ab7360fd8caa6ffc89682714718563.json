{
    "author": "yao-matrix",
    "message": "fix TrainerIntegrationDeepSpeed UT failures (#41236)\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>",
    "sha": "bcec3e2175ab7360fd8caa6ffc89682714718563",
    "files": [
        {
            "sha": "4a378c78682c59bd2ac96fdc39bb81f2e243ec36",
            "filename": "src/transformers/integrations/deepspeed.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bcec3e2175ab7360fd8caa6ffc89682714718563/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bcec3e2175ab7360fd8caa6ffc89682714718563/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py?ref=bcec3e2175ab7360fd8caa6ffc89682714718563",
            "patch": "@@ -356,7 +356,7 @@ def deepspeed_optim_sched(trainer, hf_deepspeed_config, args, num_training_steps\n \n     optimizer = None\n     if \"optimizer\" in config:\n-        if args.adafactor:\n+        if args.optim == \"adafactor\":\n             raise ValueError(\n                 \"--adafactor was passed, but also found `optimizer` configured in the DeepSpeed config. \"\n                 \"Only one optimizer can be configured.\""
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}