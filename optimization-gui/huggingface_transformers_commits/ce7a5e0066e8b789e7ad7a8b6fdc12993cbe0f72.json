{
    "author": "Cyrilvallez",
    "message": "Correctly create tied key mapping in post_init, and dynamic tie weight (#42270)\n\n* add dynamic\n\n* improve\n\n* doc\n\n* true dynamic\n\n* everywhere\n\n* improve\n\n* fix\n\n* more\n\n* small fix\n\n* small fix\n\n* fix duplicates\n\n* fix\n\n* doc\n\n* fix\n\n* improve doc\n\n* comment\n\n* more doc\n\n* style",
    "sha": "ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72",
    "files": [
        {
            "sha": "3dc522efe251698e8aa384e19aebc70633545c32",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 147,
            "deletions": 100,
            "changes": 247,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72",
            "patch": "@@ -1297,11 +1297,8 @@ def post_init(self):\n         # Attach the different parallel plans and tied weight keys to the top-most model, so that everything is\n         # easily available\n         self._tp_plan, self._ep_plan, self._pp_plan = {}, {}, {}\n-        # Current submodel should register its tied weights keys only if the config is asking for it\n-        if not self.config.tie_word_embeddings and not self.config.tie_encoder_decoder:\n-            self.all_tied_weights_keys = {}\n-        else:\n-            self.all_tied_weights_keys = self._tied_weights_keys.copy() if self._tied_weights_keys is not None else {}\n+        # Current submodel should register its tied weights\n+        self.all_tied_weights_keys = self.get_expanded_tied_weights_keys(all_submodels=False)\n         # If current model is a base model, attach `base_model_tp_plan` and `base_model_pp_plan` from config\n         if self.base_model is self:\n             self._pp_plan = self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else {}\n@@ -2180,99 +2177,155 @@ def smart_apply(self, fn):\n         # Let the magic happen with this simple call\n         self.smart_apply(self._initialize_weights)\n \n-    def tie_weights(self, missing_keys: Optional[set[str]] = None):\n-        \"\"\"\n-        If set in the config, tie the weights between the input embeddings and the output embeddings,\n-        and the encoder and decoder. This relies on the `_tied_weights_keys` dict.\n-\n-        This is very sensible! For many reasons and especially this one:\n-        ```python\n-        from torch import nn\n-        import torch\n-        class MyClass(nn.Module):\n-            def __init__(self):\n-                super().__init__()\n-                self.proj = nn.Linear(8,8)\n-                self.bias = nn.Parameter(torch.empty(8))\n-                self.proj.bias = self.bias\n-\n-        c = MyClass()\n-        print(list(c.named_parameters()))\n+    def get_expanded_tied_weights_keys(self, all_submodels: bool = False) -> dict:\n+        r\"\"\"\n+        Return the expanded tied weight keys (in case they contain modules or regex patterns) for only the current\n+        model, or recursively for all submodels if `all_submodels=True` (i.e. it will re-check the config values for all\n+        submodels).\n+\n+        For almost all models, we only require to tie the embeddings, so the model has an internal property\n+        `_tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}`. In this case, the mapping is already\n+        \"expanded\", i.e. it already contains full parameters, and this function will simply return a copy of the property.\n+        For more complex patterns, e.g. for `DFineForObjectDetection`, we have the following attribute\n         ```\n-        That's for a parameter, for a module, it will just remove the ones that are \"shared\" (that makes sense) and overwrite getattr for it.\n-\n-        ```python\n-        from torch import nn\n-        import torch\n-        class Decoder(nn.Module):\n-            def __init__(self):\n-                super().__init__()\n-                self.embedding = nn.Embedding(8,8)\n-\n-        class Encoder(nn.Module):\n-            def __init__(self):\n-                super().__init__()\n-                self.embedding = nn.Embedding(8,8)\n-\n-        class EncoderDecoder(nn.Module):\n-            def __init__(self):\n-                super().__init__()\n-                self.encoder = Encoder()\n-                self.decoder = Decoder()\n-                self.encoder.embedding = self.decoder.embedding # setattr is convenient\n-\n-        c = EncoderDecoder()\n-        print(list(c.named_parameters()))\n+        _tied_weights_keys = {\n+            r\"bbox_embed.(?![0])\\d+\": \"bbox_embed.0\",\n+            r\"class_embed.(?![0])\\d+\": \"class_embed.0\",\n+            \"model.decoder.class_embed\": \"class_embed\",\n+            \"model.decoder.bbox_embed\": \"bbox_embed\",\n+        }\n         ```\n-        Thus the order of the keys matters. If you tie `self.decoder.embedding` you can no longer tie anything inside it.\n-\n-        If you call this function, it will always tie. There is only 1 tricky case, if all weights are missing, you still want to mention that\n-        the ones you tied were missing.\n+        In this case, the function looks up all the model's parameters and buffers, and matches all the params,\n+        returning the following:\n+        ```\n+        {\n+            'bbox_embed.1.layers.0.bias': 'bbox_embed.0.layers.0.bias',\n+            'bbox_embed.1.layers.0.weight': 'bbox_embed.0.layers.0.weight',\n+            'bbox_embed.1.layers.1.bias': 'bbox_embed.0.layers.1.bias',\n+            'bbox_embed.1.layers.1.weight': 'bbox_embed.0.layers.1.weight',\n+            'bbox_embed.1.layers.2.bias': 'bbox_embed.0.layers.2.bias',\n+            'bbox_embed.1.layers.2.weight': 'bbox_embed.0.layers.2.weight',\n+            'bbox_embed.2.layers.0.bias': 'bbox_embed.0.layers.0.bias',\n+            'bbox_embed.2.layers.0.weight': 'bbox_embed.0.layers.0.weight',\n+            ...\n+            'class_embed.1.bias': 'class_embed.0.bias',\n+            'class_embed.1.weight': 'class_embed.0.weight',\n+            'class_embed.2.bias': 'class_embed.0.bias',\n+            'class_embed.2.weight': 'class_embed.0.weight',\n+            ...\n+            'model.decoder.class_embed.0.bias': 'class_embed.0.bias',\n+            'model.decoder.class_embed.0.weight': 'class_embed.0.weight',\n+            'model.decoder.class_embed.1.bias': 'class_embed.0.bias',\n+            'model.decoder.class_embed.1.weight': 'class_embed.0.weight',\n+            ...\n+            'model.decoder.bbox_embed.0.layers.0.bias': 'bbox_embed.0.layers.0.bias',\n+            'model.decoder.bbox_embed.0.layers.0.weight': 'bbox_embed.0.layers.0.weight',\n+            'model.decoder.bbox_embed.0.layers.1.bias': 'bbox_embed.0.layers.1.bias',\n+            'model.decoder.bbox_embed.0.layers.1.weight': 'bbox_embed.0.layers.1.weight',\n+            ...\n+        }\n+        ```\n+        i.e. all the parameters matching the regex and modules patterns in `_tied_weights_keys`\n         \"\"\"\n-        # TODO Cyril: using this fixed set of keys (set in post_init()) does not allow to switch the config flag and re-tie\n-        mapping = getattr(self, \"all_tied_weights_keys\", None)\n-        if not isinstance(mapping, dict):\n-            return\n+        if all_submodels:\n+            expanded_tied_weights = {}\n+            for prefix, submodule in self.named_modules(remove_duplicate=False):\n+                if isinstance(submodule, PreTrainedModel):\n+                    # Will dynamically check the config if it has changed\n+                    submodel_tied_weights = submodule.get_expanded_tied_weights_keys(all_submodels=False)\n+                    if prefix != \"\":\n+                        submodel_tied_weights = {\n+                            f\"{prefix}.{k}\": f\"{prefix}.{v}\" for k, v in submodel_tied_weights.items()\n+                        }\n+                    expanded_tied_weights.update(submodel_tied_weights)\n+            return expanded_tied_weights\n \n-        # TODO let's pray this is not too slow :)\n-        top_level_params = dict(self.named_parameters(remove_duplicate=False)) | dict(\n-            self.named_buffers(remove_duplicate=False)\n-        )\n-        for target_name, source_name in mapping.items():\n-            source_name = \"^\" + source_name\n+        tied_mapping = self._tied_weights_keys\n+        # If the config does not specify any tying, return empty dict\n+        if not self.config.tie_word_embeddings and not self.config.tie_encoder_decoder:\n+            return {}\n+        # If None, return empty dict\n+        elif tied_mapping is None:\n+            return {}\n+        # Short-cut for the most common cases: if the tied weights mapping only contains already expanded params,\n+        # return it directly (the regex matches names containing only letters, numbers, dots, and underscores to make\n+        # sure it does not contain a regex pattern, and finishing by \"bias\" or \"weight\" to make sure it's not a module)\n+        common_case_regex = re.compile(r\"^[A-Za-z0-9_\\.]+(weight)|(bias)$\")\n+        if all(common_case_regex.match(k) for k in tied_mapping.keys() | tied_mapping.values()):\n+            return tied_mapping.copy()\n+\n+        # We need to expand the regex patterns or the modules into proper parameters\n+        expanded_tied_weights = {}\n+        all_param_names = {k for k, _ in self.named_parameters(remove_duplicate=False)} | {\n+            k for k, _ in self.named_buffers(remove_duplicate=False)\n+        }\n+        for target_name, source_name in tied_mapping.items():\n             target_name = \"^\" + target_name\n+            source_name = \"^\" + source_name\n \n-            source_is_there = bool(missing_keys) and not re.search(\n-                source_name, \"\\n\".join(missing_keys), flags=re.MULTILINE\n-            )\n-            source_params = sorted(filter(lambda x: re.search(source_name, x), top_level_params.keys()))\n-            target_params = sorted(filter(lambda x: re.search(target_name, x), top_level_params.keys()))\n-            if not len(source_params) > 0 or len(target_params) % len(source_params) != 0:\n+            source_params = sorted(filter(lambda x: re.search(source_name, x), all_param_names))\n+            target_params = sorted(filter(lambda x: re.search(target_name, x), all_param_names))\n+            if (\n+                not len(source_params) > 0\n+                or not len(target_params) > 0\n+                or len(target_params) % len(source_params) != 0\n+            ):\n                 raise ValueError(\n-                    f\"There is an issue with your definition of `tie_weights_keys` for {source_name}:{target_name}. We found {source_params} to tie into {target_params}\"\n+                    f\"There is an issue with your definition of `tie_weights_keys` for {source_name}:{target_name}. \"\n+                    f\"We found {source_params} to tie into {target_params}\"\n                 )\n-            if len(target_params) > 0:\n-                # we cycle source as it should be dispatch in many target if regex\n-                for target_n, source_n in zip(target_params, cycle(source_params)):\n-                    if \".\" in target_n:\n-                        parent_path, last = target_n.rsplit(\".\", 1)\n-                        parent = self.get_submodule(parent_path)\n-                    else:\n-                        parent_path, last = \"\", target_n\n-                        parent = self  # top-level\n-                    setattr(parent, last, top_level_params[source_n])\n-                    self._adjust_bias(parent, top_level_params[source_n])\n-                    if missing_keys and source_is_there:  # test_model_weights_reload_no_missing_tied_weights\n-                        missing_keys.discard(target_n)\n+            # we cycle source as it should be dispatch in many target if regex\n+            for target_n, source_n in zip(target_params, cycle(source_params)):\n+                # If the source is already registed as a target, use the original corresponding source. This should never\n+                # happen in general, but some models such as `d_fine` have complicated regex patterns, so it end up being\n+                # the case for simplicity of the regexes. Fix it silently here\n+                if source_n in expanded_tied_weights.keys():\n+                    # Use original source instead of having keys both as source and targets\n+                    expanded_tied_weights[target_n] = expanded_tied_weights[source_n]\n+                # Usual case, everything is already correct\n+                else:\n+                    expanded_tied_weights[target_n] = source_n\n+\n+        return expanded_tied_weights\n+\n+    def tie_weights(self, missing_keys: Optional[set[str]] = None, recompute_mapping: bool = True):\n+        \"\"\"\n+        Tie the model weights. If `recompute_mapping=False` (default when called internally), it will rely on the\n+        `model.all_tied_weights_keys` attribute, containing the `{target: source}` mapping for the tied params.\n+        If `recompute_mapping=True`, it will re-check all internal submodels and their config to determine the params\n+        that need to be tied. This is the default when `model.tie_weights()` is called on its own, outside of\n+        `__init__`, and `from_pretrained`, in case the config values were changed somewhere.\n+        \"\"\"\n+        # In this case, the keys stored in `all_tied_weights_keys` are already correct\n+        if not recompute_mapping:\n+            tied_keys = self.all_tied_weights_keys\n+        else:\n+            tied_keys = self.get_expanded_tied_weights_keys(all_submodels=True)\n+\n+        for target_param_name, source_param_name in tied_keys.items():\n+            source_param = self.get_parameter_or_buffer(source_param_name)\n+            if \".\" in target_param_name:\n+                parent_name, name = target_param_name.rsplit(\".\", 1)\n+                parent = self.get_submodule(parent_name)\n             else:\n-                target_is_not_there = missing_keys and re.search(\n-                    target_name, \"\\n\".join(missing_keys), flags=re.MULTILINE\n-                )\n-                raise ValueError(\n-                    \"There is a problem in the way you tie your keys or the way they were saved.\\n\"\n-                    f\"source_is_there={source_is_there}, target_is_there={not target_is_not_there}, missing_keys={missing_keys},\"\n-                    \"tie_word_embeddings/tie_encoder_decoder={(self.config.tie_word_embeddings or self.config.tie_encoder_decoder)}\"\n-                )\n+                name = target_param_name\n+                parent = self\n+            setattr(parent, name, source_param)\n+            self._adjust_bias(parent, source_param)\n+            if missing_keys is not None:\n+                source_is_there = source_param_name not in missing_keys\n+                target_is_there = target_param_name not in missing_keys\n+                # If we tied correctly, remove the target from the missing keys\n+                if source_is_there:\n+                    missing_keys.discard(target_param_name)\n+                # If the source is not present, but the target is, the checkpoint is corrupted\n+                # TODO: maybe we could simply tie in the opposite direction here instead of error?\n+                elif target_is_there:\n+                    raise ValueError(\n+                        f\"This checkpoint seem corrupted. The tied weights mapping for this model specifies to tie \"\n+                        f\"{source_param_name} (which should be present and is not), to {target_param_name} (which is \"\n+                        f\"present).\"\n+                    )\n \n     def _adjust_bias(self, output_embeddings, input_embeddings):\n         if getattr(output_embeddings, \"bias\", None) is not None and hasattr(output_embeddings, \"weight\"):\n@@ -2788,9 +2841,8 @@ def init_weights(self):\n         if _init_weights:\n             # Initialize weights\n             self.initialize_weights()\n-            # Tie weights needs to be called as it figures out recursively if sub modules\n-            # need to tie\n-            self.tie_weights()\n+            # Tie weights needs to be called here, but it can use the pre-computed `all_tied_weights_keys`\n+            self.tie_weights(recompute_mapping=False)\n \n     def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n         \"\"\"\n@@ -4079,7 +4131,7 @@ def _load_pretrained_model(\n         model._initialize_missing_keys(is_quantized)\n \n         # Tie the weights\n-        model.tie_weights(missing_keys)\n+        model.tie_weights(missing_keys=missing_keys, recompute_mapping=False)\n \n         # Adjust missing and unexpected keys\n         missing_keys, unexpected_keys = model._adjust_missing_and_unexpected_keys(missing_keys, unexpected_keys)\n@@ -4396,13 +4448,8 @@ def mark_tied_weights_as_initialized(self):\n         This is very important as most embeddings are tied, and they are huge params (vocabularies are often 256k), so\n         running inits on them is very costly.\"\"\"\n         for tied_param in self.all_tied_weights_keys.keys():\n-            # It's always a proper weight except for 2 or 3 old models where it's a regex or module set to None\n-            # -> just skip it in those cases (they will just re-init before tying, so they loose the added optimization)\n-            try:\n-                param = self.get_parameter(tied_param)\n-                param._is_hf_initialized = True\n-            except AttributeError:\n-                pass\n+            param = self.get_parameter(tied_param)\n+            param._is_hf_initialized = True\n \n     def get_parameter_or_buffer(self, target: str):\n         \"\"\""
        },
        {
            "sha": "a64684eddc182ba4e186e624ae0aad1b5f3f2bde",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72",
            "patch": "@@ -732,8 +732,6 @@ def __init__(self, config):\n         self.esm = EsmModel(config, add_pooling_layer=False)\n         self.lm_head = EsmLMHead(config)\n \n-        self.init_weights()\n-\n         self.post_init()\n \n     def get_output_embeddings(self):\n@@ -828,8 +826,6 @@ def __init__(self, config):\n         self.esm = EsmModel(config, add_pooling_layer=False)\n         self.classifier = EsmClassificationHead(config)\n \n-        self.init_weights()\n-\n         self.post_init()\n \n     @can_return_tuple\n@@ -903,8 +899,6 @@ def __init__(self, config):\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n \n-        self.init_weights()\n-\n         self.post_init()\n \n     @can_return_tuple"
        },
        {
            "sha": "8adc51426fefb35a435587f77579c5808906690a",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72",
            "patch": "@@ -993,7 +993,7 @@ def __init__(self, config, target_lang: Optional[str] = None):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def tie_weights(self, missing_keys=None):\n+    def tie_weights(self, **kwargs):\n         \"\"\"\n         This method overwrites [`~PreTrainedModel.tie_weights`] so that adapter weights can be correctly loaded when\n         passing `target_lang=...` to `from_pretrained(...)`."
        },
        {
            "sha": "fee52071b4c0bd13d2df77258974172a11586688",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72",
            "patch": "@@ -1111,7 +1111,7 @@ def __init__(self, config, vision_model=None):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def tie_weights(self, missing_keys=None):\n+    def tie_weights(self, **kwargs):\n         \"\"\"\n         Overwrite `transformers.modeling_utils.PreTrainedModel.tie_weights` to handle the case of\n         IdeficsDecoupledLinear and IdeficsDecoupledEmbedding."
        },
        {
            "sha": "58d5c80b5bc5d8d00da59286155042771f686fca",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72",
            "patch": "@@ -857,7 +857,7 @@ def __init__(self, config, target_lang: Optional[str] = None):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def tie_weights(self, missing_keys=None):\n+    def tie_weights(self, **kwargs):\n         \"\"\"\n         This method overwrites [`~PreTrainedModel.tie_weights`] so that adapter weights can be correctly loaded when\n         passing `target_lang=...` to `from_pretrained(...)`."
        },
        {
            "sha": "acbd0ab16b8236a0c4ae6efc87a2821e1351a5af",
            "filename": "src/transformers/models/sew_d/modeling_sew_d.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py?ref=ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72",
            "patch": "@@ -1400,7 +1400,7 @@ def __init__(self, config, target_lang: Optional[str] = None):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def tie_weights(self, missing_keys=None):\n+    def tie_weights(self, **kwargs):\n         \"\"\"\n         This method overwrites [`~PreTrainedModel.tie_weights`] so that adapter weights can be correctly loaded when\n         passing `target_lang=...` to `from_pretrained(...)`."
        },
        {
            "sha": "30951a1f03ada796ceb6d6fce92fd1f034b7dfe5",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72",
            "patch": "@@ -1210,7 +1210,7 @@ def __init__(self, config, target_lang: Optional[str] = None):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def tie_weights(self, missing_keys=None):\n+    def tie_weights(self, **kwargs):\n         \"\"\"\n         This method overwrites [`~PreTrainedModel.tie_weights`] so that adapter weights can be correctly loaded when\n         passing `target_lang=...` to `from_pretrained(...)`."
        },
        {
            "sha": "d3a405b758cfd24ee0e8c26cdeb04f04d9cbdeb8",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72",
            "patch": "@@ -1206,7 +1206,7 @@ def __init__(self, config, target_lang: Optional[str] = None):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def tie_weights(self, missing_keys=None):\n+    def tie_weights(self, **kwargs):\n         \"\"\"\n         This method overwrites [`~PreTrainedModel.tie_weights`] so that adapter weights can be correctly loaded when\n         passing `target_lang=...` to `from_pretrained(...)`."
        },
        {
            "sha": "60b8f7f04c9ee87f9af525677110c34a081b8e5c",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72",
            "patch": "@@ -1684,7 +1684,7 @@ def __init__(self, config, target_lang: Optional[str] = None):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def tie_weights(self, missing_keys=None):\n+    def tie_weights(self, **kwargs):\n         \"\"\"\n         This method overwrites [`~PreTrainedModel.tie_weights`] so that adapter weights can be correctly loaded when\n         passing `target_lang=...` to `from_pretrained(...)`."
        },
        {
            "sha": "06c571e0bf459285511f6082aa78456ba0481178",
            "filename": "src/transformers/models/wavlm/modeling_wavlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py?ref=ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72",
            "patch": "@@ -1135,7 +1135,7 @@ def __init__(self, config, target_lang: Optional[str] = None):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def tie_weights(self, missing_keys=None):\n+    def tie_weights(self, **kwargs):\n         \"\"\"\n         This method overwrites [`~PreTrainedModel.tie_weights`] so that adapter weights can be correctly loaded when\n         passing `target_lang=...` to `from_pretrained(...)`."
        },
        {
            "sha": "094dc816233e5f0ac02d081ab12f0252c5f38c55",
            "filename": "tests/models/openai/test_modeling_openai.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/tests%2Fmodels%2Fopenai%2Ftest_modeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/tests%2Fmodels%2Fopenai%2Ftest_modeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopenai%2Ftest_modeling_openai.py?ref=ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72",
            "patch": "@@ -269,6 +269,13 @@ def test_model_from_pretrained(self):\n         model = OpenAIGPTModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n+    @unittest.skip(\"Tied weights mapping is reversed, so this is supposed to error out\")\n+    def test_correct_missing_keys(self):\n+        # openai defines `_tied_weights_keys = {\"transformer.tokens_embed.weight\": \"lm_head.weight\"}` instead\n+        # of the usual `_tied_weights_keys = {\"lm_head.weight\": \"transformer.tokens_embed.weight\"}`, so removing\n+        # the head parameters actually removes the source weight, so this test is supposed to fail\n+        pass\n+\n \n @require_torch\n class OPENAIGPTModelLanguageGenerationTest(unittest.TestCase):"
        },
        {
            "sha": "9579e23d35ac0a66a15510e664a5d4e53319aa25",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=ce7a5e0066e8b789e7ad7a8b6fdc12993cbe0f72",
            "patch": "@@ -183,7 +183,7 @@ def __init__(self, config):\n         def forward(self, x):\n             return self.linear_2(self.linear(x))\n \n-        def tie_weights(self, missing_keys=None):\n+        def tie_weights(self, missing_keys=None, **kwargs):\n             self.linear_2.weight = self.linear.weight\n             if missing_keys is not None:\n                 missing_keys.discard(\"linear_2.weight\")\n@@ -257,7 +257,7 @@ def __init__(self, config):\n         def forward(self, x):\n             return self.decoder(self.base(x))\n \n-        def tie_weights(self, missing_keys=None):\n+        def tie_weights(self, missing_keys=None, **kwargs):\n             self.decoder.weight = self.base.linear.weight\n             if missing_keys is not None:\n                 missing_keys.discard(\"decoder.weight\")"
        }
    ],
    "stats": {
        "total": 280,
        "additions": 164,
        "deletions": 116
    }
}