{
    "author": "guangy10",
    "message": "Fix passing str dtype to static cache (#33741)\n\nCo-authored-by: Guang Yang <guangyang@fb.com>",
    "sha": "808997a6343c46fbd7e6c92d507805d51750baf4",
    "files": [
        {
            "sha": "4adfcc39a4b1fa5568dda0a498ae543cf7f29829",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/808997a6343c46fbd7e6c92d507805d51750baf4/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/808997a6343c46fbd7e6c92d507805d51750baf4/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=808997a6343c46fbd7e6c92d507805d51750baf4",
            "patch": "@@ -68,7 +68,7 @@ def __init__(self, model: PreTrainedModel):\n             config=self.model.config,\n             batch_size=self.model.generation_config.cache_config.batch_size,\n             max_cache_len=self.model.generation_config.cache_config.max_cache_len,\n-            dtype=self.model.config.torch_dtype,\n+            dtype=self.model.dtype,\n         )\n         self.is_causal = any(\"CausalLM\" in arch for arch in self.model.config.architectures)\n         if self.is_causal:"
        },
        {
            "sha": "8392ed18b7166552c71ccce454db3ca045363f78",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/808997a6343c46fbd7e6c92d507805d51750baf4/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/808997a6343c46fbd7e6c92d507805d51750baf4/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=808997a6343c46fbd7e6c92d507805d51750baf4",
            "patch": "@@ -181,7 +181,7 @@ def test_static_cache_exportability(self):\n \n         set_seed(0)\n         device = \"cpu\"\n-        dtype = torch.float32\n+        dtype = \"bfloat16\"\n         cache_implementation = \"static\"\n         attn_implementation = \"sdpa\"  # Export and ExecuTorch only works for SdpaAttention\n         batch_size = 1"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}