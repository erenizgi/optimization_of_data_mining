{
    "author": "zucchini-nlp",
    "message": "VLMs: `patch_size` -> `num_image_tokens` in processing (#33424)\n\n* use num additional tokens\r\n\r\n* fix copies + docs\r\n\r\n* another fix copies :)\r\n\r\n* add docs\r\n\r\n* move order for BC",
    "sha": "1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
    "files": [
        {
            "sha": "4125d372d55ad5562c2c8123dd2765c8a4ece085",
            "filename": "docs/source/en/model_doc/blip-2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md?ref=1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
            "patch": "@@ -40,6 +40,10 @@ The original code can be found [here](https://github.com/salesforce/LAVIS/tree/5\n - BLIP-2 can be used for conditional text generation given an image and an optional text prompt. At inference time, it's recommended to use the [`generate`] method.\n - One can use [`Blip2Processor`] to prepare images for the model, and decode the predicted tokens ID's back to text.\n \n+> [!NOTE]\n+> BLIP models after release v4.46 will raise warnings about adding `processor.num_query_tokens = {{num_query_tokens}}` and expand model embeddings layer to add special `<image>` token. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you. Adding these attributes means that BLIP will add the number of query tokens required per image and expand the text with as many `<image>` placeholders as there will be query tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there wil be failure when merging the embeddings.\n+The attributes can be obtained from model config, as `model.config.num_query_tokens` and model embeddings expansion can be done by following [this link](https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042).\n+\n ## Resources\n \n A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with BLIP-2."
        },
        {
            "sha": "904a96bc786f075d32ab19cf85afaf1e4e6727f4",
            "filename": "docs/source/en/model_doc/instructblip.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md?ref=1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
            "patch": "@@ -33,6 +33,10 @@ The original code can be found [here](https://github.com/salesforce/LAVIS/tree/m\n \n InstructBLIP uses the same architecture as [BLIP-2](blip2) with a tiny but important difference: it also feeds the text prompt (instruction) to the Q-Former.\n \n+> [!NOTE]\n+> BLIP models after release v4.46 will raise warnings about adding `processor.num_query_tokens = {{num_query_tokens}}` and expand model embeddings layer to add special `<image>` token. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you. Adding these attributes means that BLIP will add the number of query tokens required per image and expand the text with as many `<image>` placeholders as there will be query tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there wil be failure when merging the embeddings.\n+The attributes can be obtained from model config, as `model.config.num_query_tokens` and model embeddings expansion can be done by following [this link](https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042).\n+\n ## InstructBlipConfig\n \n [[autodoc]] InstructBlipConfig"
        },
        {
            "sha": "8b2207ce176566afa58f9f3074ec6ce982352d2a",
            "filename": "docs/source/en/model_doc/instructblipvideo.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md?ref=1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
            "patch": "@@ -35,6 +35,10 @@ The original code can be found [here](https://github.com/salesforce/LAVIS/tree/m\n \n - The model was trained by sampling 4 frames per video, so it's recommended to sample 4 frames\n \n+> [!NOTE]\n+> BLIP models after release v4.46 will raise warnings about adding `processor.num_query_tokens = {{num_query_tokens}}` and expand model embeddings layer to add special `<image>` token. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you. Adding these attributes means that BLIP will add the number of query tokens required per image and expand the text with as many `<image>` placeholders as there will be query tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there wil be failure when merging the embeddings.\n+The attributes can be obtained from model config, as `model.config.num_query_tokens` and model embeddings expansion can be done by following [this link](https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042).\n+\n ## InstructBlipVideoConfig\n \n [[autodoc]] InstructBlipVideoConfig"
        },
        {
            "sha": "dec19ca5ef45db8edf1df12ce7ef8e44c9add9d2",
            "filename": "docs/source/en/model_doc/llava.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md?ref=1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
            "patch": "@@ -40,6 +40,13 @@ The original code can be found [here](https://github.com/haotian-liu/LLaVA/tree/\n \n - Note the model has not been explicitly trained to process multiple images in the same prompt, although this is technically possible, you may experience inaccurate results.\n \n+\n+> [!NOTE]\n+> LLaVA models after release v4.46 will raise warnings about adding `processor.patch_size = {{patch_size}}`, `processor.num_additional_image_tokens = {{num_additional_image_tokens}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you.\n+Adding these attributes means that LLaVA will try to infer the number of image tokens required per image and expand the text with as many `<image>` placeholders as there will be tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.\n+The attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n+\n+\n ### Single image inference\n \n For best results, we recommend users to use the processor's `apply_chat_template()` method to format your prompt correctly. For that you need to construct a conversation history, passing in a plain string will not format your prompt. Each message in the conversation history for chat templates is a dictionary with keys \"role\" and \"content\". The \"content\" should be a list of dictionaries, for \"text\" and \"image\" modalities, as follows:"
        },
        {
            "sha": "88bd63e7101f1793dab78d3608e851c3de096ebd",
            "filename": "docs/source/en/model_doc/llava_next.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md?ref=1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
            "patch": "@@ -53,6 +53,12 @@ The original code can be found [here](https://github.com/haotian-liu/LLaVA/tree/\n </Tip>\n \n \n+> [!NOTE]\n+> LLaVA models after release v4.46 will raise warnings about adding `processor.patch_size = {{patch_size}}`, `processor.num_additional_image_tokens = {{num_additional_image_tokens}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you.\n+Adding these attributes means that LLaVA will try to infer the number of image tokens required per image and expand the text with as many `<image>` placeholders as there will be tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.\n+The attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n+\n+\n - Note that each checkpoint has been trained with a specific prompt format, depending on which large language model (LLM) was used. You can use the processor's `apply_chat_template` to format your prompts correctly. For that you have to construct a conversation history, passing a plain string will not format your prompt. Each message in the conversation history for chat templates is a dictionary with keys \"role\" and \"content\". The \"content\" should be a list of dictionaries, for \"text\" and \"image\" modalities. Below is an example of how to do that and the list of formats accepted by each checkpoint.\n \n We will use [llava-v1.6-mistral-7b-hf](https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf) and a conversation history of text and image. Each content field has to be a list of dicts, as follows:"
        },
        {
            "sha": "f8a149f12b67790e67acf0e0ee3efda3b423509f",
            "filename": "docs/source/en/model_doc/llava_next_video.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md?ref=1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
            "patch": "@@ -50,6 +50,12 @@ The original code can be found [here](https://github.com/LLaVA-VL/LLaVA-NeXT/tre\n </Tip>\n \n \n+> [!NOTE]\n+> LLaVA models after release v4.46 will raise warnings about adding `processor.patch_size = {{patch_size}}`, `processor.num_additional_image_tokens = {{num_additional_image_tokens}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you.\n+Adding these attributes means that LLaVA will try to infer the number of image tokens required per image and expand the text with as many `<image>` placeholders as there will be tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.\n+The attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n+\n+\n - Note that each checkpoint has been trained with a specific prompt format, depending on which large language model (LLM) was used. You can use tokenizer's `apply_chat_template` to format your prompts correctly. Below is an example of how to do that.\n \n We will use [LLaVA-NeXT-Video-7B-hf](https://huggingface.co/llava-hf/LLaVA-NeXT-Video-7B-hf) and a conversation history of videos and images. Each content field has to be a list of dicts, as follows:"
        },
        {
            "sha": "105307196effd037a6ae055b84096439c0d11176",
            "filename": "docs/source/en/model_doc/video_llava.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md?ref=1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
            "patch": "@@ -54,6 +54,12 @@ This model was contributed by [RaushanTurganbay](https://huggingface.co/RaushanT\n The original code can be found [here](https://github.com/PKU-YuanGroup/Video-LLaVA).\n \n \n+> [!NOTE]\n+> LLaVA models after release v4.46 will raise warnings about adding `processor.patch_size = {{patch_size}}`, `processor.num_additional_image_tokens = {{num_additional_image_tokens}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you.\n+Adding these attributes means that LLaVA will try to infer the number of image tokens required per image and expand the text with as many `<image>` placeholders as there will be tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.\n+The attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n+\n+\n ## Usage example\n \n ### Single Media Mode"
        },
        {
            "sha": "328310f3e26b770d13c3f11e11b598139d07a5f1",
            "filename": "docs/source/en/model_doc/vipllava.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/docs%2Fsource%2Fen%2Fmodel_doc%2Fvipllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/docs%2Fsource%2Fen%2Fmodel_doc%2Fvipllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvipllava.md?ref=1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
            "patch": "@@ -39,6 +39,12 @@ This model was contributed by [Younes Belkada](https://huggingface.co/ybelkada)\n \n - Note the model has not been explicitly trained to process multiple images in the same prompt, although this is technically possible, you may experience inaccurate results.\n \n+> [!NOTE]\n+> LLaVA models after release v4.46 will raise warnings about adding `processor.patch_size = {{patch_size}}`, `processor.num_additional_image_tokens = {{num_additional_image_tokens}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you.\n+Adding these attributes means that LLaVA will try to infer the number of image tokens required per image and expand the text with as many `<image>` placeholders as there will be tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.\n+The attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n+\n+\n - For better results, we recommend users to use the processor's `apply_chat_template()` method to format your prompt correctly. For that you need to construct a conversation history, passing in a plain string will not format your prompt. Each message in the conversation history for chat templates is a dictionary with keys \"role\" and \"content\". The \"content\" should be a list of dictionaries, for \"text\" and \"image\" modalities, as follows:\n \n ```python"
        },
        {
            "sha": "08caa3d1d8a75ab29fdc0c1f00abab41a13d97e4",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 16,
            "deletions": 3,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
            "patch": "@@ -58,10 +58,19 @@ class LlavaProcessor(ProcessorMixin):\n             in a chat into a tokenizable string.\n         image_token (`str`, *optional*, defaults to `\"<image>\"`):\n             Special token used to denote image location.\n+        num_additional_image_tokens (`int`, *optional*, defaults to 0):\n+            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n+            extra tokens appended, no need to set this arg.\n     \"\"\"\n \n     attributes = [\"image_processor\", \"tokenizer\"]\n-    valid_kwargs = [\"chat_template\", \"patch_size\", \"vision_feature_select_strategy\", \"image_token\"]\n+    valid_kwargs = [\n+        \"chat_template\",\n+        \"patch_size\",\n+        \"vision_feature_select_strategy\",\n+        \"image_token\",\n+        \"num_additional_image_tokens\",\n+    ]\n     image_processor_class = \"AutoImageProcessor\"\n     tokenizer_class = \"AutoTokenizer\"\n \n@@ -73,9 +82,11 @@ def __init__(\n         vision_feature_select_strategy=None,\n         chat_template=None,\n         image_token=\"<image>\",  # set the default and let users change if they have peculiar special tokens in rare cases\n+        num_additional_image_tokens=0,\n         **kwargs,\n     ):\n         self.patch_size = patch_size\n+        self.num_additional_image_tokens = num_additional_image_tokens\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n         self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n@@ -147,9 +158,11 @@ def __call__(\n                 # Replace the image token with the expanded image token sequence\n                 pixel_values = image_inputs[\"pixel_values\"]\n                 height, width = get_image_size(to_numpy_array(pixel_values[0]))\n-                num_image_tokens = (height // self.patch_size) * (width // self.patch_size) + 1\n+                num_image_tokens = (height // self.patch_size) * (\n+                    width // self.patch_size\n+                ) + self.num_additional_image_tokens\n                 if self.vision_feature_select_strategy == \"default\":\n-                    num_image_tokens -= 1\n+                    num_image_tokens -= self.num_additional_image_tokens\n \n                 prompt_strings = []\n                 for sample in text:"
        },
        {
            "sha": "09f9e621a5873ebdc592c20520208198d866798b",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 14,
            "deletions": 3,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
            "patch": "@@ -61,10 +61,19 @@ class LlavaNextProcessor(ProcessorMixin):\n             in a chat into a tokenizable string.\n         image_token (`str`, *optional*, defaults to `\"<image>\"`):\n             Special token used to denote image location.\n+        num_additional_image_tokens (`int`, *optional*, defaults to 0):\n+            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n+            extra tokens appended, no need to set this arg.\n     \"\"\"\n \n     attributes = [\"image_processor\", \"tokenizer\"]\n-    valid_kwargs = [\"chat_template\", \"patch_size\", \"vision_feature_select_strategy\", \"image_token\"]\n+    valid_kwargs = [\n+        \"chat_template\",\n+        \"patch_size\",\n+        \"vision_feature_select_strategy\",\n+        \"image_token\",\n+        \"num_additional_image_tokens\",\n+    ]\n     image_processor_class = \"AutoImageProcessor\"\n     tokenizer_class = \"AutoTokenizer\"\n \n@@ -76,9 +85,11 @@ def __init__(\n         vision_feature_select_strategy=None,\n         chat_template=None,\n         image_token=\"<image>\",  # set the default and let users change if they have peculiar special tokens in rare cases\n+        num_additional_image_tokens=0,\n         **kwargs,\n     ):\n         self.patch_size = patch_size\n+        self.num_additional_image_tokens = num_additional_image_tokens\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n         self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n@@ -155,7 +166,7 @@ def __call__(\n                         orig_height, orig_width = image_size\n                         num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n                         if self.vision_feature_select_strategy == \"default\":\n-                            num_image_tokens -= 1\n+                            num_image_tokens -= self.num_additional_image_tokens\n                         sample = sample.replace(self.image_token, \"<placeholder>\" * num_image_tokens, 1)\n                     prompt_strings.append(sample)\n                 prompt_strings = [sample.replace(\"<placeholder>\", self.image_token) for sample in prompt_strings]\n@@ -178,7 +189,7 @@ def _get_number_of_features(self, orig_height: int, orig_width: int, height: int\n             orig_height, orig_width, patches_height, patches_width, scale_height, scale_width\n         )\n         # The base patch covers the entire image (+1 for the CLS)\n-        base_features = patches_height * patches_width + 1\n+        base_features = patches_height * patches_width + self.num_additional_image_tokens\n         num_image_tokens = unpadded_features + newline_features + base_features\n         return num_image_tokens\n "
        },
        {
            "sha": "db4999a2a8ae04edb7eb45611e7af7d0d786b0a8",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 20,
            "deletions": 5,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
            "patch": "@@ -58,12 +58,22 @@ class LlavaNextVideoProcessor(ProcessorMixin):\n             Special token used to denote video location.\n         image_token (`str`, *optional*, defaults to `\"<image>\"`):\n             Special token used to denote image location.\n+        num_additional_image_tokens (`int`, *optional*, defaults to 0):\n+            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n+            extra tokens appended, no need to set this arg.\n     \"\"\"\n \n     # video and image processor share same args, but have different processing logic\n     # only image processor config is saved in the hub\n     attributes = [\"video_processor\", \"image_processor\", \"tokenizer\"]\n-    valid_kwargs = [\"chat_template\", \"patch_size\", \"vision_feature_select_strategy\", \"image_token\", \"video_token\"]\n+    valid_kwargs = [\n+        \"chat_template\",\n+        \"patch_size\",\n+        \"vision_feature_select_strategy\",\n+        \"image_token\",\n+        \"video_token\",\n+        \"num_additional_image_tokens\",\n+    ]\n     image_processor_class = \"LlavaNextImageProcessor\"\n     video_processor_class = \"LlavaNextVideoImageProcessor\"\n     tokenizer_class = (\"LlamaTokenizer\", \"LlamaTokenizerFast\")\n@@ -78,9 +88,11 @@ def __init__(\n         vision_feature_select_strategy=None,\n         video_token=\"<video>\",\n         image_token=\"<image>\",\n+        num_additional_image_tokens=0,\n         **kwargs,\n     ):\n         self.patch_size = patch_size\n+        self.num_additional_image_tokens = num_additional_image_tokens\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n         self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n         self.video_token = tokenizer.video_token if hasattr(tokenizer, \"video_token\") else video_token\n@@ -164,8 +176,9 @@ def __call__(\n         if self.patch_size is None or self.vision_feature_select_strategy is None:\n             logger.warning_once(\n                 \"Expanding inputs for image/video tokens in LLaVa-NeXT-Video should be done in processing. \"\n-                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n-                \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n+                \"Please add `patch_size`, `num_additional_image_tokens` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n+                \"with `processor.patch_size = {{patch_size}}`, `processor.num_additional_image_tokens = {{num_additional_image_tokens}}` \"\n+                \"and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n                 \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n             )\n         else:\n@@ -180,7 +193,7 @@ def __call__(\n                         orig_height, orig_width = image_size\n                         num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n                         if self.vision_feature_select_strategy == \"default\":\n-                            num_image_tokens -= 1\n+                            num_image_tokens -= self.num_additional_image_tokens\n                         sample = sample.replace(self.image_token, \"<placeholder>\" * num_image_tokens, 1)\n                     prompt_strings.append(sample)\n                 text = [sample.replace(\"<placeholder>\", self.image_token) for sample in prompt_strings]\n@@ -190,6 +203,8 @@ def __call__(\n                 one_video = to_numpy_array(videos_inputs.get(\"pixel_values_videos\")[0])\n                 height, width = get_image_size(one_video[0])\n                 num_frames = one_video.shape[0]  # frame dim is always after batch dim\n+\n+                # no `self.num_additional_image_tokens` added because video always has a default feature selection strategy\n                 num_image_tokens = (height // self.patch_size) * (width // self.patch_size)\n                 num_video_tokens = num_image_tokens // 4 * num_frames  # divide by 4 needed for avg pooling layer\n                 prompt_strings = []\n@@ -222,7 +237,7 @@ def _get_number_of_features(self, orig_height: int, orig_width: int, height: int\n             orig_height, orig_width, patches_height, patches_width, scale_height, scale_width\n         )\n         # The base patch covers the entire image (+1 for the CLS)\n-        base_features = patches_height * patches_width + 1\n+        base_features = patches_height * patches_width + self.num_additional_image_tokens\n         num_image_tokens = unpadded_features + newline_features + base_features\n         return num_image_tokens\n "
        },
        {
            "sha": "5d04d7b613445545d8b2fe56c43831a948f455c5",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 20,
            "deletions": 4,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
            "patch": "@@ -51,10 +51,20 @@ class VideoLlavaProcessor(ProcessorMixin):\n             Special token used to denote video location.\n         chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n             in a chat into a tokenizable string.\n+        num_additional_image_tokens (`int`, *optional*, defaults to 0):\n+            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n+            extra tokens appended, no need to set this arg.\n     \"\"\"\n \n     attributes = [\"image_processor\", \"tokenizer\"]\n-    valid_kwargs = [\"chat_template\", \"patch_size\", \"vision_feature_select_strategy\", \"image_token\", \"video_token\"]\n+    valid_kwargs = [\n+        \"chat_template\",\n+        \"patch_size\",\n+        \"vision_feature_select_strategy\",\n+        \"image_token\",\n+        \"video_token\",\n+        \"num_additional_image_tokens\",\n+    ]\n     image_processor_class = \"VideoLlavaImageProcessor\"\n     tokenizer_class = \"AutoTokenizer\"\n \n@@ -67,9 +77,11 @@ def __init__(\n         image_token=\"<image>\",  # set the default and let users change if they have peculiar special tokens in rare cases\n         video_token=\"<video>\",\n         chat_template=None,\n+        num_additional_image_tokens=0,\n         **kwargs,\n     ):\n         self.patch_size = patch_size\n+        self.num_additional_image_tokens = num_additional_image_tokens\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n         self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n         self.video_token = tokenizer.video_token if hasattr(tokenizer, \"video_token\") else video_token\n@@ -165,13 +177,17 @@ def __call__(\n                 height, width = get_image_size(one_video[0])\n                 num_frames = one_video.shape[0]  # frame dim is always after batch dim\n \n-            num_image_tokens = (height // self.patch_size) * (width // self.patch_size) + 1\n+            num_image_tokens = (height // self.patch_size) * (\n+                width // self.patch_size\n+            ) + self.num_additional_image_tokens\n             num_video_tokens = num_image_tokens * num_frames\n \n-            num_image_tokens = (height // self.patch_size) * (width // self.patch_size) + 1\n+            num_image_tokens = (height // self.patch_size) * (\n+                width // self.patch_size\n+            ) + self.num_additional_image_tokens\n             num_video_tokens = num_image_tokens * num_frames\n             if self.vision_feature_select_strategy == \"default\":\n-                num_image_tokens -= 1\n+                num_image_tokens -= self.num_additional_image_tokens\n \n             prompt_strings = []\n             for sample in text:"
        },
        {
            "sha": "3d08ab35e0f6307d7e6d7262753257b8830785c4",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
            "patch": "@@ -607,13 +607,15 @@ def test_expansion_in_processing(self):\n \n         # check processing with expansion of inputs\n         processor.vision_feature_select_strategy = \"default\"\n+        processor.num_additional_image_tokens = 1\n         processor.patch_size = 14\n         inputs_expanded = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n         self.assertTrue(inputs_expanded.input_ids.shape[-1] == 593)\n \n         # check processing without expansion of inputs (legacy behavior)\n         processor.vision_feature_select_strategy = None\n         processor.patch_size = None\n+        processor.num_additional_image_tokens = None\n         inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n         self.assertTrue(inputs.input_ids.shape[-1] == 18)\n "
        },
        {
            "sha": "c258ce96b94e487bae612f1b523d1b1004ec385e",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
            "patch": "@@ -622,6 +622,7 @@ def test_expansion_in_processing_multiimage(self):\n         # check processing with expansion of inputs\n         processor.vision_feature_select_strategy = \"default\"\n         processor.patch_size = 14\n+        processor.num_additional_image_tokens = 1\n         inputs_expanded = processor(text=prompt, images=[raw_image, deer_image], return_tensors=\"pt\").to(\n             torch_device, torch.float16\n         )\n@@ -630,6 +631,7 @@ def test_expansion_in_processing_multiimage(self):\n         # check processing without expansion of inputs (legacy behavior)\n         processor.vision_feature_select_strategy = None\n         processor.patch_size = None\n+        processor.num_additional_image_tokens = None\n         inputs = processor(text=prompt, images=[raw_image, deer_image], return_tensors=\"pt\").to(\n             torch_device, torch.float16\n         )\n@@ -656,12 +658,14 @@ def test_expansion_in_processing(self):\n         # check processing with expansion of inputs\n         processor.vision_feature_select_strategy = \"default\"\n         processor.patch_size = 14\n+        processor.num_additional_image_tokens = 1\n         inputs_expanded = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n         self.assertTrue(inputs_expanded.input_ids.shape[-1] == 2356)\n \n         # check processing without expansion of inputs (legacy behavior)\n         processor.vision_feature_select_strategy = None\n         processor.patch_size = None\n+        processor.num_additional_image_tokens = None\n         inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n         self.assertTrue(inputs.input_ids.shape[-1] == 17)\n "
        },
        {
            "sha": "a6fb341ff9bf56541ae1efce29fd80db672d4c73",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
            "patch": "@@ -558,12 +558,14 @@ def test_expansion_in_processing(self):\n         # check processing with expansion of inputs\n         processor.vision_feature_select_strategy = \"default\"\n         processor.patch_size = 14\n+        processor.num_additional_image_tokens = 1\n         inputs_expanded = processor(self.prompt_video, videos=[self.video], return_tensors=\"pt\").to(torch_device)\n         self.assertTrue(inputs_expanded.input_ids.shape[-1] == 1170)\n \n         # check processing without expansion of inputs (legacy behavior)\n         processor.vision_feature_select_strategy = None\n         processor.patch_size = None\n+        processor.num_additional_image_tokens = None\n         inputs = processor(self.prompt_video, videos=[self.video], return_tensors=\"pt\").to(torch_device)\n         self.assertTrue(inputs.input_ids.shape[-1] == 19)\n \n@@ -586,12 +588,14 @@ def test_expansion_in_processing_images(self):\n         # check processing with expansion of inputs\n         processor.vision_feature_select_strategy = \"default\"\n         processor.patch_size = 14\n+        processor.num_additional_image_tokens = 1\n         inputs_expanded = processor(self.prompt_image, images=[self.image], return_tensors=\"pt\").to(torch_device)\n         self.assertTrue(inputs_expanded.input_ids.shape[-1] == 2652)\n \n         # check processing without expansion of inputs (legacy behavior)\n         processor.vision_feature_select_strategy = None\n         processor.patch_size = None\n+        processor.num_additional_image_tokens = None\n         inputs = processor(self.prompt_image, images=[self.image], return_tensors=\"pt\").to(torch_device)\n         self.assertTrue(inputs.input_ids.shape[-1] == 19)\n \n@@ -624,6 +628,7 @@ def test_expansion_in_processing_multiimage(self):\n         # check processing with expansion of inputs\n         processor.vision_feature_select_strategy = \"default\"\n         processor.patch_size = 14\n+        processor.num_additional_image_tokens = 1\n         inputs_expanded = processor(text=prompt, images=[raw_image, deer_image], return_tensors=\"pt\").to(\n             torch_device, torch.float16\n         )\n@@ -632,6 +637,7 @@ def test_expansion_in_processing_multiimage(self):\n         # check processing without expansion of inputs (legacy behavior)\n         processor.vision_feature_select_strategy = None\n         processor.patch_size = None\n+        processor.num_additional_image_tokens = None\n         inputs = processor(text=prompt, images=[raw_image, deer_image], return_tensors=\"pt\").to(\n             torch_device, torch.float16\n         )"
        },
        {
            "sha": "14b079665ab6d6eabf231bede3873527974bac5d",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
            "patch": "@@ -625,12 +625,14 @@ def test_expansion_in_processing_images(self):\n         # check processing with expansion of inputs\n         processor.vision_feature_select_strategy = \"default\"\n         processor.patch_size = 14\n+        processor.num_additional_image_tokens = 1\n         inputs_expanded = processor(prompt, images=image, return_tensors=\"pt\").to(torch_device, torch.float16)\n         self.assertTrue(inputs_expanded.input_ids.shape[-1] == 274)\n \n         # check processing without expansion of inputs (legacy behavior)\n         processor.vision_feature_select_strategy = None\n         processor.patch_size = None\n+        processor.num_additional_image_tokens = None\n         inputs = processor(prompt, images=image, return_tensors=\"pt\").to(torch_device, torch.float16)\n         self.assertTrue(inputs.input_ids.shape[-1] == 19)\n \n@@ -657,12 +659,14 @@ def test_expansion_in_processing(self):\n         # check processing with expansion of inputs\n         processor.vision_feature_select_strategy = \"default\"\n         processor.patch_size = 14\n+        processor.num_additional_image_tokens = 1\n         inputs_expanded = processor(prompt, videos=video_file, return_tensors=\"pt\").to(torch_device, torch.float16)\n         self.assertTrue(inputs_expanded.input_ids.shape[-1] == 2074)\n \n         # check processing without expansion of inputs (legacy behavior)\n         processor.vision_feature_select_strategy = None\n         processor.patch_size = None\n+        processor.num_additional_image_tokens = None\n         inputs = processor(prompt, videos=video_file, return_tensors=\"pt\").to(torch_device, torch.float16)\n         self.assertTrue(inputs.input_ids.shape[-1] == 19)\n "
        },
        {
            "sha": "4f501fc10a028fba31cb96b68737874ec9a66dcd",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1646ffb4d19b6777fd45ac727c7a7c323d51e7f8/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=1646ffb4d19b6777fd45ac727c7a7c323d51e7f8",
            "patch": "@@ -374,12 +374,14 @@ def test_expansion_in_processing(self):\n         # check processing with expansion of inputs\n         processor.vision_feature_select_strategy = \"default\"\n         processor.patch_size = 14\n+        processor.num_additional_image_tokens = 1\n         inputs_expanded = processor(prompt, raw_image, return_tensors=\"pt\").to(torch_device, torch.float16)\n         self.assertTrue(inputs_expanded.input_ids.shape[-1] == 593)\n \n         # check processing without expansion of inputs (legacy behavior)\n         processor.vision_feature_select_strategy = None\n         processor.patch_size = None\n+        processor.num_additional_image_tokens = None\n         inputs = processor(prompt, raw_image, return_tensors=\"pt\").to(torch_device, torch.float16)\n         self.assertTrue(inputs.input_ids.shape[-1] == 18)\n "
        }
    ],
    "stats": {
        "total": 146,
        "additions": 131,
        "deletions": 15
    }
}