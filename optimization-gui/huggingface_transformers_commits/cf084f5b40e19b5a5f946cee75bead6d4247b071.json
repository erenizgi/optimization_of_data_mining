{
    "author": "vasqu",
    "message": "[`Jetmoe`] Fix RoPE (#40819)\n\n* fix\n\n* remove prints\n\n* why was this there...",
    "sha": "cf084f5b40e19b5a5f946cee75bead6d4247b071",
    "files": [
        {
            "sha": "118e734143f42ecff966882ad8f77814bf691f21",
            "filename": "src/transformers/models/jetmoe/configuration_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf084f5b40e19b5a5f946cee75bead6d4247b071/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf084f5b40e19b5a5f946cee75bead6d4247b071/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py?ref=cf084f5b40e19b5a5f946cee75bead6d4247b071",
            "patch": "@@ -94,6 +94,7 @@ class JetMoeConfig(PretrainedConfig):\n \n     model_type = \"jetmoe\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    attribute_map = {\"head_dim\": \"kv_channels\"}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "3171612eb4929617ef23dcee54d3e990ec23d174",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf084f5b40e19b5a5f946cee75bead6d4247b071/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf084f5b40e19b5a5f946cee75bead6d4247b071/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=cf084f5b40e19b5a5f946cee75bead6d4247b071",
            "patch": "@@ -932,15 +932,6 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        if attention_mask is not None and self._attn_implementation == \"flash_attention_2\" and use_cache:\n-            batch_size = inputs_embeds.shape[0]\n-            is_padding_right = attention_mask[:, -1].sum().item() != batch_size\n-            if is_padding_right:\n-                raise ValueError(\n-                    \"You are attempting to perform batched generation with padding_side='right'\"\n-                    \" this may lead to unexpected behaviour for Flash Attention version of JetMoe. Make sure to \"\n-                    \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n-                )\n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )"
        },
        {
            "sha": "41b78b9fcf4736bb3181ee6c30cfa2f45cdc71db",
            "filename": "tests/models/jetmoe/test_modeling_jetmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf084f5b40e19b5a5f946cee75bead6d4247b071/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf084f5b40e19b5a5f946cee75bead6d4247b071/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py?ref=cf084f5b40e19b5a5f946cee75bead6d4247b071",
            "patch": "@@ -184,12 +184,10 @@ def test_model_8b_batched_generation(self):\n         tokenizer = AutoTokenizer.from_pretrained(\"jetmoe/jetmoe-8b\", use_fast=False)\n         model = JetMoeForCausalLM.from_pretrained(\"jetmoe/jetmoe-8b\")\n         input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.model.embed_tokens.weight.device)\n-        print(input_ids)\n \n         # greedy generation outputs\n         generated_ids = model.generate(**input_ids, max_new_tokens=10, temperature=0)\n         text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n-        print(text)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n \n         del model"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 1,
        "deletions": 11
    }
}