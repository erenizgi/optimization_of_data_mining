{
    "author": "eginhard",
    "message": "Fix BatchEncoding.to() for nested elements (#38985)",
    "sha": "561a79a2f4b5929f71266fb0ec1e2493c419d808",
    "files": [
        {
            "sha": "200bf478edd8091e20a5e27b463b266dac510517",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/561a79a2f4b5929f71266fb0ec1e2493c419d808/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/561a79a2f4b5929f71266fb0ec1e2493c419d808/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=561a79a2f4b5929f71266fb0ec1e2493c419d808",
            "patch": "@@ -801,14 +801,13 @@ def to(self, device: Union[str, \"torch.device\"], *, non_blocking: bool = False)\n             [`BatchEncoding`]: The same instance after modification.\n         \"\"\"\n         requires_backends(self, [\"torch\"])\n-        import torch\n \n         # This check catches things like APEX blindly calling \"to\" on all inputs to a module\n         # Otherwise it passes the casts down and casts the LongTensor containing the token idxs\n         # into a HalfTensor\n         if isinstance(device, str) or is_torch_device(device) or isinstance(device, int):\n             self.data = {\n-                k: v.to(device=device, non_blocking=non_blocking) if isinstance(v, torch.Tensor) else v\n+                k: v.to(device=device, non_blocking=non_blocking) if hasattr(v, \"to\") and callable(v.to) else v\n                 for k, v in self.data.items()\n             }\n         else:"
        }
    ],
    "stats": {
        "total": 3,
        "additions": 1,
        "deletions": 2
    }
}