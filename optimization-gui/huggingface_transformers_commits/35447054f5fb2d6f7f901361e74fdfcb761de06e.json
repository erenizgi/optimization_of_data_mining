{
    "author": "Cyrilvallez",
    "message": "Update Mistral conversion script (#34829)\n\n* Update convert_mistral_weights_to_hf.py\r\n\r\n* Update convert_mistral_weights_to_hf.py\r\n\r\n* Update convert_mistral_weights_to_hf.py",
    "sha": "35447054f5fb2d6f7f901361e74fdfcb761de06e",
    "files": [
        {
            "sha": "1a89ade8fa6dbde03289d53328a17bac3942af41",
            "filename": "src/transformers/models/mistral/convert_mistral_weights_to_hf.py",
            "status": "modified",
            "additions": 194,
            "deletions": 208,
            "changes": 402,
            "blob_url": "https://github.com/huggingface/transformers/blob/35447054f5fb2d6f7f901361e74fdfcb761de06e/src%2Ftransformers%2Fmodels%2Fmistral%2Fconvert_mistral_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/35447054f5fb2d6f7f901361e74fdfcb761de06e/src%2Ftransformers%2Fmodels%2Fmistral%2Fconvert_mistral_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fconvert_mistral_weights_to_hf.py?ref=35447054f5fb2d6f7f901361e74fdfcb761de06e",
            "patch": "@@ -12,20 +12,15 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import argparse\n-import gc\n import json\n import os\n-import shutil\n+import re\n import warnings\n \n import torch\n-from safetensors.torch import load_file as safe_load_file\n+from safetensors.torch import load_file\n \n-from transformers import (\n-    LlamaTokenizer,\n-    MistralConfig,\n-    MistralForCausalLM,\n-)\n+from transformers import LlamaTokenizer, MistralConfig, MistralForCausalLM\n \n \n try:\n@@ -39,251 +34,242 @@\n     )\n     tokenizer_class = LlamaTokenizer\n \n-\"\"\"\n-Sample usage:\n+# fmt: off\n+STATE_DICT_MAPPING = {\n+    # CausalLM keys\n+    r\"^output.weight\":                            r\"lm_head.weight\",\n \n-```\n-python src/transformers/models/mistral/convert_mistral_weights_to_hf.py \\\n-    --input_dir /path/to/downloaded/mistral/weights --model_size 7B --output_dir /output/path\n-```\n+    # Model keys\n+    r\"^norm.weight\":                              r\"model.norm.weight\",\n+    r\"^tok_embeddings.weight\":                    r\"model.embed_tokens.weight\",\n \n-Thereafter, models can be loaded via:\n+    # Layers keys\n+    r\"^layers.(\\d+).attention_norm.weight\":       r\"model.layers.\\1.input_layernorm.weight\",\n+    r\"^layers.(\\d+).ffn_norm.weight\":             r\"model.layers.\\1.post_attention_layernorm.weight\",\n \n-```py\n-from transformers import MistralForCausalLM, LlamaTokenizer\n+    # Attention keys\n+    r\"^layers.(\\d+).attention.w(q|k|v|o).weight\": r\"model.layers.\\1.self_attn.\\2_proj.weight\",\n \n-model = MistralForCausalLM.from_pretrained(\"/output/path\")\n-tokenizer = LlamaTokenizer.from_pretrained(\"/output/path\")\n-```\n \n-Important note: you need to be able to host the whole model in RAM to execute this script (even if the biggest versions\n-come in several checkpoints they each contain a part of each weight of the model, so we need to load them all in RAM).\n-\"\"\"\n+    # MLP keys\n+    r\"^layers.(\\d+).feed_forward.w1.weight\":      r\"model.layers.\\1.mlp.gate_proj.weight\",\n+    r\"^layers.(\\d+).feed_forward.w2.weight\":      r\"model.layers.\\1.mlp.down_proj.weight\",\n+    r\"^layers.(\\d+).feed_forward.w3.weight\":      r\"model.layers.\\1.mlp.up_proj.weight\",\n+}\n+# fmt: on\n \n-NUM_SHARDS = {\"7B\": 1}\n \n+def map_old_key_to_new(old_key):\n+    \"\"\"Map of a key of the original state dict to the equivalent key in HF format\"\"\"\n+    for pattern, replacement in STATE_DICT_MAPPING.items():\n+        new_key, n_replace = re.subn(pattern, replacement, old_key)\n+        # Early exit of the loop\n+        if n_replace > 0:\n+            return new_key\n \n-def compute_intermediate_size(n, ffn_dim_multiplier=1, multiple_of=256):\n-    return multiple_of * ((int(ffn_dim_multiplier * int(8 * n / 3)) + multiple_of - 1) // multiple_of)\n+    raise ValueError(f\"Key: {old_key} could not be mapped (check the mapping).\")\n \n \n def read_json(path):\n     with open(path, \"r\") as f:\n         return json.load(f)\n \n \n-def write_json(text, path):\n-    with open(path, \"w\") as f:\n-        json.dump(text, f)\n+def permute_for_rope(tensor, n_heads, dim1, dim2):\n+    \"\"\"Permute the weights for the ROPE formulation.\"\"\"\n+    tensor = tensor.view(n_heads, dim1 // n_heads // 2, 2, dim2)\n+    tensor = tensor.transpose(1, 2)\n+    tensor = tensor.reshape(dim1, dim2)\n+    return tensor\n \n \n-def write_model(model_path, input_base_path, model_size, tokenizer_path=None, safe_serialization=True, is_v3=False):\n-    # for backward compatibility, before you needed the repo to be called `my_repo/model_size`\n-    if not os.path.isfile(os.path.join(input_base_path, \"params.json\")):\n-        input_base_path = os.path.join(input_base_path, model_size)\n+def convert_state_dict(original_state_dict: dict, config: MistralConfig):\n+    \"\"\"Convert a state dict file, when a single `nn.Module` is never sharded in different files (usual case).\"\"\"\n+    new_dict = {}\n \n-    os.makedirs(model_path, exist_ok=True)\n-    tmp_model_path = os.path.join(model_path, \"tmp\")\n-    os.makedirs(tmp_model_path, exist_ok=True)\n+    n_heads = config.num_attention_heads\n+    dim = config.hidden_size\n+    dims_per_head = dim // n_heads\n+    num_key_value_heads = config.num_key_value_heads\n+    key_value_dim = dims_per_head * num_key_value_heads\n \n-    params = read_json(os.path.join(input_base_path, \"params.json\"))\n-    num_shards = NUM_SHARDS[model_size]\n+    for old_key, tensor in original_state_dict.items():\n+        new_key = map_old_key_to_new(old_key)\n \n-    sliding_window = params.get(\"sliding_window\", None)\n+        if \"q_proj\" in new_key:\n+            tensor = tensor.view(n_heads, dims_per_head, dim).reshape(dim, dim)\n+            tensor = permute_for_rope(tensor, n_heads, dim, dim)\n+        elif \"k_proj\" in new_key:\n+            tensor = tensor.view(num_key_value_heads, dims_per_head, dim).reshape(key_value_dim, dim)\n+            tensor = permute_for_rope(tensor, num_key_value_heads, key_value_dim, dim)\n+        elif \"v_proj\" in new_key:\n+            tensor = tensor.view(num_key_value_heads, dims_per_head, dim).reshape(key_value_dim, dim)\n \n-    # For some reason this is a string in the params.json\n-    if sliding_window is not None:\n-        sliding_window = int(sliding_window)\n+        new_dict[new_key] = tensor\n+    return new_dict\n \n-    n_layers = params[\"n_layers\"]\n-    n_heads = params[\"n_heads\"]\n-    n_heads_per_shard = n_heads // num_shards\n-    dim = params[\"dim\"]\n+\n+def get_concat_dim(key):\n+    \"\"\"Return the dimension to concatenate the weights on.\"\"\"\n+    concat_dim_1 = [\n+        r\"model.embed_tokens.weight\",\n+        r\"model.layers.(\\d+).self_attn.o_proj.weight\",\n+        r\"model.layers.(\\d+).mlp.down_proj.weight\",\n+    ]\n+    if any(re.search(pattern, key) for pattern in concat_dim_1):\n+        return 1\n+    return 0\n+\n+\n+def convert_state_dict_sharded(loaded_shards: list[dict], config: MistralConfig):\n+    \"\"\"Convert the state dict, when a single `nn.Module` is sharded accross different files.\"\"\"\n+    new_dict = {}\n+\n+    num_shards = len(loaded_shards)\n+\n+    n_heads = config.num_attention_heads\n+    dim = config.hidden_size\n     dims_per_head = dim // n_heads\n-    base = params.get(\"rope_theta\", 10000.0)\n-    inv_freq = 1.0 / (base ** (torch.arange(0, dims_per_head, 2).float() / dims_per_head))\n-    max_position_embeddings = 4096 * 8\n-\n-    if tokenizer_path is not None:\n-        tokenizer = tokenizer_class(tokenizer_path + \".v3\" if is_v3 else \"\")\n-        tokenizer.save_pretrained(model_path)\n-    vocab_size = tokenizer.vocab_size if tokenizer_path is not None else 32000\n-\n-    if \"n_kv_heads\" in params:\n-        num_key_value_heads = params[\"n_kv_heads\"]  # for GQA / MQA\n-        num_local_key_value_heads = num_key_value_heads // num_shards\n-        key_value_dim = dims_per_head * num_local_key_value_heads\n-    else:  # compatibility with other checkpoints\n-        num_key_value_heads = n_heads\n-        num_local_key_value_heads = n_heads_per_shard\n-        key_value_dim = dim\n-\n-    # permute for sliced rotary\n-    def permute(w, n_heads=n_heads, dim1=dim, dim2=dim):\n-        return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n-\n-    print(f\"Fetching all parameters from the checkpoint at {input_base_path}.\")\n-\n-    # Load weights - for v3 models the consolidated weights are in a single file format in safetensors\n-    if is_v3:\n-        loaded = [safe_load_file(os.path.join(input_base_path, \"consolidated.safetensors\"))]\n-    else:\n-        loaded = [\n-            torch.load(os.path.join(input_base_path, f\"consolidated.{i:02d}.pth\"), map_location=\"cpu\")\n-            for i in range(num_shards)\n-        ]\n-    param_count = 0\n-    index_dict = {\"weight_map\": {}}\n-    for layer_i in range(n_layers):\n-        filename = f\"pytorch_model-{layer_i + 1}-of-{n_layers + 1}.bin\"\n-\n-        # Sharded\n-        # Note that attention.w{q,k,v,o}, feed_fordward.w[1,2,3], attention_norm.weight and ffn_norm.weight share\n-        # the same storage object, saving attention_norm and ffn_norm will save other weights too, which is\n-        # redundant as other weights will be stitched from multiple shards. To avoid that, they are cloned.\n-\n-        state_dict = {\n-            f\"model.layers.{layer_i}.input_layernorm.weight\": loaded[0][\n-                f\"layers.{layer_i}.attention_norm.weight\"\n-            ].clone(),\n-            f\"model.layers.{layer_i}.post_attention_layernorm.weight\": loaded[0][\n-                f\"layers.{layer_i}.ffn_norm.weight\"\n-            ].clone(),\n-        }\n-        state_dict[f\"model.layers.{layer_i}.self_attn.q_proj.weight\"] = permute(\n-            torch.cat(\n-                [\n-                    loaded[i][f\"layers.{layer_i}.attention.wq.weight\"].view(n_heads_per_shard, dims_per_head, dim)\n-                    for i in range(num_shards)\n-                ],\n-                dim=0,\n+    num_key_value_heads = config.num_key_value_heads\n+    n_heads_per_shard = n_heads // num_shards\n+    num_local_key_value_heads = num_key_value_heads // num_shards\n+    key_value_dim = dim if n_heads == num_key_value_heads else dims_per_head * num_local_key_value_heads\n+\n+    original_keys = loaded_shards[0].keys()\n+    for old_key in original_keys:\n+        new_key = map_old_key_to_new(old_key)\n+        cat_dim = get_concat_dim(new_key)\n+\n+        if \"q_proj\" in new_key:\n+            tensor = torch.cat(\n+                [shard.pop(old_key).view(n_heads_per_shard, dims_per_head, dim) for shard in loaded_shards],\n+                dim=cat_dim,\n             ).reshape(dim, dim)\n-        )\n-        state_dict[f\"model.layers.{layer_i}.self_attn.k_proj.weight\"] = permute(\n-            torch.cat(\n-                [\n-                    loaded[i][f\"layers.{layer_i}.attention.wk.weight\"].view(\n-                        num_local_key_value_heads, dims_per_head, dim\n-                    )\n-                    for i in range(num_shards)\n-                ],\n-                dim=0,\n-            ).reshape(key_value_dim, dim),\n-            num_key_value_heads,\n-            key_value_dim,\n-            dim,\n-        )\n-        state_dict[f\"model.layers.{layer_i}.self_attn.v_proj.weight\"] = torch.cat(\n-            [\n-                loaded[i][f\"layers.{layer_i}.attention.wv.weight\"].view(num_local_key_value_heads, dims_per_head, dim)\n-                for i in range(num_shards)\n-            ],\n-            dim=0,\n-        ).reshape(key_value_dim, dim)\n-\n-        state_dict[f\"model.layers.{layer_i}.self_attn.o_proj.weight\"] = torch.cat(\n-            [loaded[i][f\"layers.{layer_i}.attention.wo.weight\"] for i in range(num_shards)], dim=1\n-        )\n-        state_dict[f\"model.layers.{layer_i}.mlp.gate_proj.weight\"] = torch.cat(\n-            [loaded[i][f\"layers.{layer_i}.feed_forward.w1.weight\"] for i in range(num_shards)], dim=0\n-        )\n-        state_dict[f\"model.layers.{layer_i}.mlp.down_proj.weight\"] = torch.cat(\n-            [loaded[i][f\"layers.{layer_i}.feed_forward.w2.weight\"] for i in range(num_shards)], dim=1\n-        )\n-        state_dict[f\"model.layers.{layer_i}.mlp.up_proj.weight\"] = torch.cat(\n-            [loaded[i][f\"layers.{layer_i}.feed_forward.w3.weight\"] for i in range(num_shards)], dim=0\n-        )\n-\n-        state_dict[f\"model.layers.{layer_i}.self_attn.rotary_emb.inv_freq\"] = inv_freq\n-        for k, v in state_dict.items():\n-            index_dict[\"weight_map\"][k] = filename\n-            param_count += v.numel()\n-        torch.save(state_dict, os.path.join(tmp_model_path, filename))\n-\n-    filename = f\"pytorch_model-{n_layers + 1}-of-{n_layers + 1}.bin\"\n-    state_dict = {\n-        \"model.norm.weight\": loaded[0][\"norm.weight\"],\n-        \"model.embed_tokens.weight\": torch.cat([loaded[i][\"tok_embeddings.weight\"] for i in range(num_shards)], dim=1),\n-        \"lm_head.weight\": torch.cat([loaded[i][\"output.weight\"] for i in range(num_shards)], dim=0),\n+            tensor = permute_for_rope(tensor, n_heads, dim, dim)\n+        elif \"k_proj\" in new_key:\n+            tensor = torch.cat(\n+                [shard.pop(old_key).view(num_local_key_value_heads, dims_per_head, dim) for shard in loaded_shards],\n+                dim=cat_dim,\n+            ).reshape(key_value_dim, dim)\n+            tensor = permute_for_rope(tensor, num_key_value_heads, key_value_dim, dim)\n+        elif \"v_proj\" in new_key:\n+            tensor = torch.cat(\n+                [shard.pop(old_key).view(num_local_key_value_heads, dims_per_head, dim) for shard in loaded_shards],\n+                dim=cat_dim,\n+            ).reshape(key_value_dim, dim)\n+        elif \"input_layernorm\" in new_key or \"post_attention_layernorm\" in new_key:\n+            tensor = loaded_shards[0][old_key].clone()\n+        elif \"model.norm.weight\" in new_key:\n+            tensor = loaded_shards[0][old_key]\n+        else:\n+            tensor = torch.cat([shard.pop(old_key) for shard in loaded_shards], dim=cat_dim)\n+\n+        new_dict[new_key] = tensor\n+\n+    return new_dict\n+\n+\n+def convert_config(original_config: dict, max_position_embeddings: int):\n+    key_mapping = {\n+        \"hidden_size\": \"dim\",\n+        \"num_hidden_layers\": \"n_layers\",\n+        \"intermediate_size\": \"hidden_dim\",\n+        \"num_attention_heads\": \"n_heads\",\n+        \"rms_norm_eps\": \"norm_eps\",\n     }\n-\n-    for k, v in state_dict.items():\n-        index_dict[\"weight_map\"][k] = filename\n-        param_count += v.numel()\n-    torch.save(state_dict, os.path.join(tmp_model_path, filename))\n-\n-    # Write configs\n-    index_dict[\"metadata\"] = {\"total_size\": param_count * 2}\n-    write_json(index_dict, os.path.join(tmp_model_path, \"pytorch_model.bin.index.json\"))\n-    config = MistralConfig(\n-        hidden_size=dim,\n-        intermediate_size=params[\"hidden_dim\"],\n-        num_attention_heads=params[\"n_heads\"],\n-        num_hidden_layers=params[\"n_layers\"],\n-        rms_norm_eps=params[\"norm_eps\"],\n-        num_key_value_heads=num_key_value_heads,\n-        vocab_size=vocab_size,\n-        rope_theta=base,\n-        max_position_embeddings=max_position_embeddings,\n-        sliding_window=sliding_window,\n+    similar_keys_to_keep = [\n+        \"head_dim\",\n+        \"vocab_size\",\n+    ]\n+\n+    new_config_kwargs = {k: original_config[v] for k, v in key_mapping.items()}\n+    new_config_kwargs.update({k: v for k, v in original_config.items() if k in similar_keys_to_keep})\n+\n+    # These are not always defined depending on `params.json`\n+    new_config_kwargs[\"sliding_window\"] = original_config.get(\"sliding_window\", None)\n+    new_config_kwargs[\"num_key_value_heads\"] = original_config.get(\n+        \"n_kv_heads\", new_config_kwargs[\"num_attention_heads\"]\n     )\n-    config.save_pretrained(tmp_model_path)\n+    new_config_kwargs[\"rope_theta\"] = original_config.get(\"rope_theta\", 10000.0)\n+\n+    # This is never provided in `params.json`, we provide it manually\n+    new_config_kwargs[\"max_position_embeddings\"] = max_position_embeddings\n \n-    # Make space so we can load the model properly now.\n-    del state_dict\n-    del loaded\n-    gc.collect()\n+    # This may sometimes be a string in `params.json`\n+    if new_config_kwargs[\"sliding_window\"] is not None:\n+        new_config_kwargs[\"sliding_window\"] = int(new_config_kwargs[\"sliding_window\"])\n \n-    print(\"Loading the checkpoint in a Mistral model.\")\n-    model = MistralForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n-    # Avoid saving this as part of the config.\n-    del model.config._name_or_path\n-    model.config.torch_dtype = torch.float16\n-    print(\"Saving in the Transformers format.\")\n+    new_config = MistralConfig(**new_config_kwargs)\n+    return new_config\n \n-    model.save_pretrained(model_path, safe_serialization=safe_serialization)\n-    shutil.rmtree(tmp_model_path)\n \n+def convert_and_write_model(input_dir: str, output_dir: str, max_position_embeddings: int, modules_are_split: bool):\n+    \"\"\"Convert the model and save it (this implicitly save the config as well).\"\"\"\n+    params = read_json(os.path.join(input_dir, \"params.json\"))\n+    config = convert_config(params, max_position_embeddings)\n+\n+    full_state_dict = {}\n+    # The model may be split between different files, but a single nn.Module is always fully present in a single file\n+    if not modules_are_split:\n+        shards = [file for file in os.listdir(input_dir) if file.endswith(\".safetensors\")]\n+        for shard_file in shards:\n+            original_state_dict = load_file(os.path.join(input_dir, shard_file))\n+            new_dict = convert_state_dict(original_state_dict, config)\n+            full_state_dict.update(new_dict)\n+    # A single nn.Module is split between different checkpoint files\n+    else:\n+        shards = [file for file in os.listdir(input_dir) if re.match(r\"consolidated.\\d+.pth\", file)]\n+        shards = sorted(shards, key=lambda x: int(x.split(\".\")[1]))\n+        loaded_shards = [torch.load(os.path.join(input_dir, file), map_location=\"cpu\") for file in shards]\n+        full_state_dict = convert_state_dict_sharded(loaded_shards, config)\n+\n+    # Load weights into model and resave them\n+    with torch.device(\"meta\"):\n+        model = MistralForCausalLM(config)\n+    model.load_state_dict(full_state_dict, strict=True, assign=True)\n+    model.save_pretrained(output_dir)\n \n-def write_tokenizer(tokenizer_path, input_tokenizer_path):\n-    # Initialize the tokenizer based on the `spm` model\n-    print(f\"Saving a {tokenizer_class.__name__} to {tokenizer_path}.\")\n-    tokenizer = tokenizer_class(input_tokenizer_path)\n-    tokenizer.save_pretrained(tokenizer_path)\n+\n+def convert_and_write_tokenizer(input_dir: str, output_dir: str):\n+    \"\"\"Convert the tokenizer and save it.\"\"\"\n+    # May have .v3 or .v7 at the end\n+    tokenizer_file = [file for file in os.listdir(input_dir) if \"tokenizer.model\" in file][0]\n+    tokenizer = tokenizer_class(os.path.join(input_dir, tokenizer_file))\n+    tokenizer.save_pretrained(output_dir)\n \n \n def main():\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\n-        \"--input_dir\",\n+        \"input_dir\",\n         help=\"Location of Mistral weights, which contains tokenizer.model and model folders\",\n     )\n     parser.add_argument(\n-        \"--model_size\",\n-        choices=[\"7B\", \"tokenizer_only\"],\n-        help=\"'f' models correspond to the finetuned versions, and are specific to the Mistral2 official release. For more details on Mistral2, checkout the original repo: https://huggingface.co/meta-mistral\",\n+        \"output_dir\",\n+        help=\"Location to write HF model and tokenizer\",\n     )\n     parser.add_argument(\n-        \"--output_dir\",\n-        help=\"Location to write HF model and tokenizer\",\n+        \"--max_position_embeddings\",\n+        type=int,\n+        default=32768,\n+        help=\"`max_position_embeddings` field in the config. This needs to be manually passed (not present anywhere otherwise).\",\n     )\n-    parser.add_argument(\"--safe_serialization\", type=bool, help=\"Whether or not to save using `safetensors`.\")\n     parser.add_argument(\n-        \"--is_v3\", action=\"store_true\", help=\"Whether the checkpoints correspond to the 3rd version or not.\"\n+        \"--modules_are_split\",\n+        action=\"store_true\",\n+        help=\"If passed, then the weights of a single `nn.Module` are assumed to be split between different files.\",\n     )\n+    parser.add_argument(\n+        \"--tokenizer_only\",\n+        action=\"store_true\",\n+        help=\"If passed, will only convert the tokenizer.\",\n+    )\n+\n     args = parser.parse_args()\n-    spm_path = os.path.join(args.input_dir, \"tokenizer.model\")\n-    if args.model_size != \"tokenizer_only\":\n-        write_model(\n-            model_path=args.output_dir,\n-            input_base_path=args.input_dir,\n-            model_size=args.model_size,\n-            safe_serialization=args.safe_serialization,\n-            tokenizer_path=spm_path,\n-            is_v3=args.is_v3,\n-        )\n-    else:\n-        write_tokenizer(args.output_dir, spm_path)\n+\n+    if not args.tokenizer_only:\n+        convert_and_write_model(args.input_dir, args.output_dir, args.max_position_embeddings, args.modules_are_split)\n+    convert_and_write_tokenizer(args.input_dir, args.output_dir)\n \n \n if __name__ == \"__main__\":"
        }
    ],
    "stats": {
        "total": 402,
        "additions": 194,
        "deletions": 208
    }
}