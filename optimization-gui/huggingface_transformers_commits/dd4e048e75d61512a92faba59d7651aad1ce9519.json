{
    "author": "remi-or",
    "message": "Reduce the number of benchmark in the CI (#42008)\n\nChanged how benchmark cfgs are chosen",
    "sha": "dd4e048e75d61512a92faba59d7651aad1ce9519",
    "files": [
        {
            "sha": "b54ca69737df9b45d35bfdca35535d5d5765255e",
            "filename": ".github/workflows/benchmark.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd4e048e75d61512a92faba59d7651aad1ce9519/.github%2Fworkflows%2Fbenchmark.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd4e048e75d61512a92faba59d7651aad1ce9519/.github%2Fworkflows%2Fbenchmark.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fbenchmark.yml?ref=dd4e048e75d61512a92faba59d7651aad1ce9519",
            "patch": "@@ -52,7 +52,7 @@ jobs:\n             commit_id=$GITHUB_SHA\r\n           fi\r\n           commit_msg=$(git show -s --format=%s | cut -c1-70)\r\n-          python3 benchmark_v2/run_benchmarks.py -b 32 -s 128 -n 256 --cross-generate --branch-name \"$BRANCH_NAME\" --commit-id \"$commit_id\" --commit-message \"$commit_msg\" --model-id \"$MODEL_ID\" --log-level INFO --push-result-to-dataset \"$DATASET_ID\"\r\n+          python3 benchmark_v2/run_benchmarks.py -b 32 -s 128 -n 256 --level 2 --branch-name \"$BRANCH_NAME\" --commit-id \"$commit_id\" --commit-message \"$commit_msg\" --model-id \"$MODEL_ID\" --log-level INFO --push-result-to-dataset \"$DATASET_ID\"\r\n         env:\r\n           HF_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\r\n           PUSH_TO_HUB_TOKEN: ${{ secrets.PUSH_TO_HUB_TOKEN }}\r"
        },
        {
            "sha": "52e6f89956eda58246fac7a9eed74179cb0d1dfa",
            "filename": "benchmark_v2/framework/benchmark_config.py",
            "status": "modified",
            "additions": 63,
            "deletions": 54,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd4e048e75d61512a92faba59d7651aad1ce9519/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd4e048e75d61512a92faba59d7651aad1ce9519/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_config.py?ref=dd4e048e75d61512a92faba59d7651aad1ce9519",
            "patch": "@@ -1,4 +1,5 @@\n import hashlib\n+import itertools\n import json\n import logging\n from typing import Any\n@@ -146,60 +147,68 @@ def from_dict(cls, data: dict[str, Any], skip_validity_check: bool = False) -> \"\n         )\n \n \n-def cross_generate_configs(\n-    attn_impl_and_sdpa_backend: list[tuple[str, str | None]],\n-    compiled_mode: list[str | None],\n-    kernelized: list[bool],\n-    warmup_iterations: int = 5,\n-    measurement_iterations: int = 20,\n-    batch_size: int = 1,\n-    sequence_length: int = 128,\n-    num_tokens_to_generate: int = 128,\n-    gpu_monitoring: bool = True,\n+def adapt_configs(\n+    configs: list[BenchmarkConfig],\n+    warmup_iterations: int | list[int] = 5,\n+    measurement_iterations: int | list[int] = 20,\n+    batch_size: int | list[int] = 1,\n+    sequence_length: int | list[int] = 128,\n+    num_tokens_to_generate: int | list[int] = 128,\n+    gpu_monitoring: bool | list[bool] = True,\n ) -> list[BenchmarkConfig]:\n-    # Create kwargs common to all configs\n-    kwargs = {\n-        \"warmup_iterations\": warmup_iterations,\n-        \"measurement_iterations\": measurement_iterations,\n-        \"batch_size\": batch_size,\n-        \"sequence_length\": sequence_length,\n-        \"num_tokens_to_generate\": num_tokens_to_generate,\n-        \"gpu_monitoring\": gpu_monitoring,\n-    }\n-    # Cross-generate all combinations of attn_implementation, compiled_mode, and kernelized\n+    parameters = (\n+        x if isinstance(x, list) else [x]\n+        for x in [\n+            warmup_iterations,\n+            measurement_iterations,\n+            batch_size,\n+            sequence_length,\n+            num_tokens_to_generate,\n+            gpu_monitoring,\n+        ]\n+    )\n+    iterator = itertools.product(*parameters)\n+\n+    adapted_configs = []\n+    for warmup_iters, measurement_iters, bs, seqlen, ntok, monitor in iterator:\n+        for config in configs:\n+            config = config.to_dict()\n+            config[\"warmup_iterations\"] = warmup_iters\n+            config[\"measurement_iterations\"] = measurement_iters\n+            config[\"batch_size\"] = bs\n+            config[\"sequence_length\"] = seqlen\n+            config[\"num_tokens_to_generate\"] = ntok\n+            config[\"gpu_monitoring\"] = monitor\n+            adapted_configs.append(BenchmarkConfig.from_dict(config))\n+    return adapted_configs\n+\n+\n+def get_config_by_level(level: int) -> list[BenchmarkConfig]:\n     configs = []\n-    for attn_implementation, sdpa_backend in list(dict.fromkeys(attn_impl_and_sdpa_backend)):\n-        for cm in list(dict.fromkeys(compiled_mode)):\n-            for kernelize_on in list(dict.fromkeys(kernelized)):\n-                config = BenchmarkConfig(\n-                    attn_implementation=attn_implementation,\n-                    sdpa_backend=sdpa_backend,\n-                    compile_mode=cm,\n-                    kernelize=kernelize_on,\n-                    **kwargs,\n-                )\n-                configs.append(config)\n+    # Early return if level is greater than 3: we generate all combinations of configs, maybe even w/ all compile modes\n+    if level >= 3:\n+        for attn_implementation, sdpa_backend in BenchmarkConfig.all_attn_implementations:\n+            # Usually there is not much to gain by compiling with other modes, but we allow it for level 4\n+            compile_modes = BenchmarkConfig.all_compiled_modes if level >= 4 else [None, \"default\"]\n+            for cm in compile_modes:\n+                for kernelize_on in [False, KERNELIZATION_AVAILABLE]:\n+                    configs.append(\n+                        BenchmarkConfig(\n+                            attn_implementation=attn_implementation,\n+                            sdpa_backend=sdpa_backend,\n+                            compile_mode=cm,\n+                            kernelize=kernelize_on,\n+                        )\n+                    )\n+        return configs\n+    # Otherwise, we add the configs for the given level\n+    if level >= 0:\n+        configs.append(BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\"))\n+    if level >= 1:\n+        configs.append(BenchmarkConfig(attn_implementation=\"flash_attention_2\"))\n+        configs.append(BenchmarkConfig(attn_implementation=\"eager\", compile_mode=\"default\"))\n+    if level >= 2:\n+        configs.append(BenchmarkConfig(attn_implementation=\"sdpa\", compile_mode=\"default\"))\n+        configs.append(BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", kernelize=True))\n+        configs.append(BenchmarkConfig(attn_implementation=\"flash_attention_2\", kernelize=True))\n     return configs\n-\n-\n-def generate_main_configs(\n-    warmup_iterations: int = 5,\n-    measurement_iterations: int = 20,\n-    batch_size: int = 1,\n-    sequence_length: int = 128,\n-    num_tokens_to_generate: int = 128,\n-) -> list[BenchmarkConfig]:\n-    # Create kwargs common to all configs\n-    kwargs = {\n-        \"warmup_iterations\": warmup_iterations,\n-        \"measurement_iterations\": measurement_iterations,\n-        \"batch_size\": batch_size,\n-        \"sequence_length\": sequence_length,\n-        \"num_tokens_to_generate\": num_tokens_to_generate,\n-    }\n-    return [  # TODO: test max-autotune instead of default\n-        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", gpu_monitoring=False, **kwargs),\n-        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", gpu_monitoring=True, **kwargs),\n-        BenchmarkConfig(attn_implementation=\"eager\", compile_mode=\"default\", gpu_monitoring=True, **kwargs),\n-        BenchmarkConfig(attn_implementation=\"flash_attention_2\", gpu_monitoring=True, **kwargs),\n-    ]"
        },
        {
            "sha": "93a6628085cfbc7c271978383cd60817129679be",
            "filename": "benchmark_v2/run_benchmarks.py",
            "status": "modified",
            "additions": 24,
            "deletions": 65,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd4e048e75d61512a92faba59d7651aad1ce9519/benchmark_v2%2Frun_benchmarks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd4e048e75d61512a92faba59d7651aad1ce9519/benchmark_v2%2Frun_benchmarks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Frun_benchmarks.py?ref=dd4e048e75d61512a92faba59d7651aad1ce9519",
            "patch": "@@ -23,12 +23,7 @@\n import sys\n import uuid\n \n-from framework.benchmark_config import (\n-    KERNELIZATION_AVAILABLE,\n-    BenchmarkConfig,\n-    cross_generate_configs,\n-    generate_main_configs,\n-)\n+from framework.benchmark_config import adapt_configs, get_config_by_level\n from framework.benchmark_runner import BenchmarkRunner\n \n \n@@ -45,7 +40,14 @@\n     parser.add_argument(\"--sequence-length\", \"-s\", type=int, nargs=\"+\", help=\"Sequence length\")\n     parser.add_argument(\"--num-tokens-to-generate\", \"-n\", type=int, nargs=\"+\", help=\"Number of tokens to generate\")\n \n-    parser.add_argument(\"--cross-generate\", action=\"store_true\", help=\"Cross-generate all combinations of configs\")\n+    parser.add_argument(\n+        \"--level\",\n+        type=int,\n+        default=1,\n+        help=\"Level of coverage for the benchmark. 0: only the main config, 1: a few important configs, 2: a config for\"\n+        \" each attn implementation an option, 3: cross-generate all combinations of configs, 4: cross-generate all\"\n+        \" combinations of configs w/ all compile modes\",\n+    )\n     parser.add_argument(\"--num-tokens-to-profile\", \"-p\", type=int, default=0, help=\"Number of tokens to profile\")\n \n     parser.add_argument(\"--branch-name\", type=str, help=\"Git branch name\")\n@@ -84,67 +86,24 @@\n             \"At least one of the arguments --batch-size, --sequence-length, or --num-tokens-to-generate is required\"\n         )\n \n-    # If there is only one (batch_size, sequence_length, num_tokens_to_generate), we benchmark across configs\n-    elif len(args.batch_size) * len(args.sequence_length) * len(args.num_tokens_to_generate) == 1:\n-        if args.cross_generate:\n-            benchmark_configs = cross_generate_configs(\n-                attn_impl_and_sdpa_backend=BenchmarkConfig.all_attn_implementations,\n-                compiled_mode=[None, \"default\"],  # usually there is not much to gain by compiling with other modes\n-                kernelized=[False, KERNELIZATION_AVAILABLE],\n-                warmup_iterations=args.warmup,\n-                measurement_iterations=args.iterations,\n-                batch_size=args.batch_size[0],\n-                sequence_length=args.sequence_length[0],\n-                num_tokens_to_generate=args.num_tokens_to_generate[0],\n-                gpu_monitoring=not args.no_gpu_monitoring,\n-            )\n-        else:\n-            benchmark_configs = generate_main_configs(\n-                warmup_iterations=args.warmup,\n-                measurement_iterations=args.iterations,\n-                batch_size=args.batch_size[0],\n-                sequence_length=args.sequence_length[0],\n-                num_tokens_to_generate=args.num_tokens_to_generate[0],\n-            )\n-\n-    # Otherwise, we benchmark across all combinations of dimensions\n-    else:\n-        main_config = generate_main_configs(\n-            warmup_iterations=args.warmup,\n-            measurement_iterations=args.iterations,\n-            batch_size=args.batch_size[0],\n-            sequence_length=args.sequence_length[0],\n-            num_tokens_to_generate=args.num_tokens_to_generate[0],\n-        )[0]\n-        benchmark_configs = []\n-        for num_tokens_to_generate in args.num_tokens_to_generate:\n-            for sequence_length in args.sequence_length:\n-                for batch_size in args.batch_size:\n-                    cfg_dict = main_config.to_dict()\n-                    cfg_dict[\"batch_size\"] = batch_size\n-                    cfg_dict[\"sequence_length\"] = sequence_length\n-                    cfg_dict[\"num_tokens_to_generate\"] = num_tokens_to_generate\n-                    cfg_dict.pop(\"name\")\n-                    benchmark_configs.append(BenchmarkConfig.from_dict(cfg_dict))\n-\n-    runner = BenchmarkRunner(\n-        logger,\n-        args.output_dir,\n-        args.branch_name,\n-        args.commit_id,\n-        args.commit_message,\n+    # Get the configs for the given coverage level\n+    configs = get_config_by_level(args.level)\n+    # Adapt the configs to the given arguments\n+    configs = adapt_configs(\n+        configs,\n+        args.warmup,\n+        args.iterations,\n+        args.batch_size,\n+        args.sequence_length,\n+        args.num_tokens_to_generate,\n+        not args.no_gpu_monitoring,\n     )\n+\n+    runner = BenchmarkRunner(logger, args.output_dir, args.branch_name, args.commit_id, args.commit_message)\n     timestamp, results = runner.run_benchmarks(\n-        args.model_id,\n-        benchmark_configs,\n-        args.num_tokens_to_profile,\n-        pretty_print_summary=True,\n+        args.model_id, configs, args.num_tokens_to_profile, pretty_print_summary=True\n     )\n \n     dataset_id = args.push_result_to_dataset\n     if dataset_id is not None and len(results) > 0:\n-        runner.push_results_to_hub(\n-            dataset_id,\n-            results,\n-            timestamp,\n-        )\n+        runner.push_results_to_hub(dataset_id, results, timestamp)"
        }
    ],
    "stats": {
        "total": 208,
        "additions": 88,
        "deletions": 120
    }
}