{
    "author": "zucchini-nlp",
    "message": "Generation: fix test (#34369)\n\n* fix test\r\n\r\n* fix copies",
    "sha": "808d6c50f8c6911d972f27bb5155c04e513c99ee",
    "files": [
        {
            "sha": "d552bf73442ce75b5543cc9a72b8a3a1ff1e2d44",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 32,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/808d6c50f8c6911d972f27bb5155c04e513c99ee/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/808d6c50f8c6911d972f27bb5155c04e513c99ee/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=808d6c50f8c6911d972f27bb5155c04e513c99ee",
            "patch": "@@ -671,29 +671,6 @@ def test_beam_sample_generate(self):\n             else:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n \n-            # for VLMs inputs embeds won't match input ids unless images are encoded and merged with ids properly\n-            # no quick fix available, since obtaining image embeddings step is very model-specific\n-            if any(name in model.__class__.__name__.lower() for name in (\"blip\", \"llava\", \"paligemma\")):\n-                prepare_inputs_for_generation_args = set(\n-                    inspect.signature(model.prepare_inputs_for_generation).parameters\n-                )\n-                # `inputs_embeds` input is well supported when `cache_positions` is used, because it means the modeling\n-                # code is up to date with our most recent standards\n-                if (\n-                    \"inputs_embeds\" in prepare_inputs_for_generation_args\n-                    and \"cache_positions\" in prepare_inputs_for_generation_args\n-                ):\n-                    input_embeds = model.get_input_embeddings()(inputs_dict[\"input_ids\"])\n-                    beam_kwargs.update({\"inputs_embeds\": input_embeds})\n-                    output_generate2 = self._beam_sample_generate(\n-                        model=model,\n-                        input_ids=None,\n-                        inputs_dict={},\n-                        beam_kwargs=beam_kwargs,\n-                    )\n-\n-                    torch.testing.assert_close(output_generate[:, input_embeds.shape[1] :], output_generate2)\n-\n     @pytest.mark.generate\n     def test_beam_sample_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n@@ -1570,7 +1547,8 @@ def test_past_key_values_format(self):\n                     )\n \n     @pytest.mark.generate\n-    def test_generate_from_inputs_embeds_decoder_only(self):\n+    @parameterized.expand([(1,), (2,)])\n+    def test_generate_from_inputs_embeds_decoder_only(self, num_beams):\n         # When supported, tests that the decoder model can generate from `inputs_embeds` instead of `input_ids`\n         # if fails, you should probably update the `prepare_inputs_for_generation` function\n         for model_class in self.all_generative_model_classes:\n@@ -1597,11 +1575,15 @@ def test_generate_from_inputs_embeds_decoder_only(self):\n                 continue\n \n             input_ids = inputs_dict.pop(\"input_ids\")\n+            generation_kwargs = {\n+                \"return_dict_in_generate\": True,\n+                \"output_scores\": True,\n+                \"num_beams\": num_beams,\n+                \"do_sample\": False,\n+            }\n \n             # Traditional way of generating text\n-            outputs_from_ids = model.generate(\n-                input_ids, max_new_tokens=5, return_dict_in_generate=True, output_scores=True\n-            )\n+            outputs_from_ids = model.generate(input_ids, max_new_tokens=5, **generation_kwargs)\n             self.assertEqual(outputs_from_ids.sequences.shape, (input_ids.shape[0], input_ids.shape[1] + 5))\n \n             # Same thing, but from input embeddings (`input_ids` is passed so the prompt is present in the output)\n@@ -1610,8 +1592,7 @@ def test_generate_from_inputs_embeds_decoder_only(self):\n                 input_ids,\n                 inputs_embeds=inputs_embeds,\n                 max_new_tokens=5,\n-                return_dict_in_generate=True,\n-                output_scores=True,\n+                **generation_kwargs,\n             )\n             self.assertListEqual(outputs_from_ids.sequences.tolist(), outputs_from_embeds.sequences.tolist())\n \n@@ -1622,15 +1603,14 @@ def test_generate_from_inputs_embeds_decoder_only(self):\n                 input_ids,\n                 inputs_embeds=random_embeds,\n                 max_new_tokens=5,\n-                return_dict_in_generate=True,\n-                output_scores=True,\n+                **generation_kwargs,\n             )\n             for i in range(len(outputs_from_rand_embeds.scores)):\n                 self.assertFalse(torch.allclose(outputs_from_embeds.scores[i], outputs_from_rand_embeds.scores[i]))\n \n             # input_ids is not a required input -- if we don't pass it, the newly generated tokens will be the same\n             outputs_from_embeds_wo_ids = model.generate(\n-                inputs_embeds=inputs_embeds, max_new_tokens=5, return_dict_in_generate=True, output_scores=True\n+                inputs_embeds=inputs_embeds, max_new_tokens=5, **generation_kwargs\n             )\n             self.assertListEqual(\n                 outputs_from_embeds.sequences[:, inputs_embeds.shape[1] :].tolist(),"
        },
        {
            "sha": "c2f0ef8ccd01d3adb36360ae12571d99b2c7214e",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/808d6c50f8c6911d972f27bb5155c04e513c99ee/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/808d6c50f8c6911d972f27bb5155c04e513c99ee/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=808d6c50f8c6911d972f27bb5155c04e513c99ee",
            "patch": "@@ -773,7 +773,8 @@ def test_custom_4d_attention_mask(self):\n     @unittest.skip(\n         reason=\"IDEFICS has specific requirements for working with inputs embeds like passing also the ids and pixels\"\n     )\n-    def test_generate_from_inputs_embeds_decoder_only(self):\n+    @parameterized.expand([(1,), (2,)])\n+    def test_generate_from_inputs_embeds_decoder_only(self, num_beams):\n         pass\n \n     @unittest.skip(reason=\"IDEFICS cannot compile due to dynamic control flow when checking inputs\")"
        },
        {
            "sha": "1a8cf04774531fe9243be02a90fb389e05a157bf",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/808d6c50f8c6911d972f27bb5155c04e513c99ee/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/808d6c50f8c6911d972f27bb5155c04e513c99ee/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=808d6c50f8c6911d972f27bb5155c04e513c99ee",
            "patch": "@@ -204,7 +204,8 @@ def test_generate_without_input_ids(self):\n         pass\n \n     @unittest.skip(reason=\"To fix, Mamba 2 cache slicing test case is an edge case\")\n-    def test_generate_from_inputs_embeds_decoder_only(self):\n+    @parameterized.expand([(1,), (2,)])\n+    def test_generate_from_inputs_embeds_decoder_only(self, num_beams):\n         pass\n \n     @unittest.skip(reason=\"To fix, Mamba 2 cache slicing test case is an edge case\")"
        },
        {
            "sha": "b77a6ff10364ca1b5bd1c0c7e68b93d87a4f929d",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 12,
            "deletions": 10,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/808d6c50f8c6911d972f27bb5155c04e513c99ee/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/808d6c50f8c6911d972f27bb5155c04e513c99ee/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=808d6c50f8c6911d972f27bb5155c04e513c99ee",
            "patch": "@@ -656,16 +656,21 @@ def test_initialization(self):\n                         )\n \n     @pytest.mark.generate\n-    def test_generate_from_inputs_embeds_decoder_only(self):\n+    @parameterized.expand([(1,), (2,)])\n+    def test_generate_from_inputs_embeds_decoder_only(self, num_beams):\n         for model_class in self.all_generative_model_classes:\n             config, input_ids, _, inputs_dict = self._get_input_ids_and_config()\n \n             model = model_class(config).to(torch_device).eval()\n+            generation_kwargs = {\n+                \"return_dict_in_generate\": True,\n+                \"output_scores\": True,\n+                \"num_beams\": num_beams,\n+                \"do_sample\": False,\n+            }\n \n             # Traditional way of generating text\n-            outputs_from_ids = model.generate(\n-                input_ids, max_new_tokens=5, return_dict_in_generate=True, output_scores=True, **inputs_dict\n-            )\n+            outputs_from_ids = model.generate(input_ids, max_new_tokens=5, **generation_kwargs, **inputs_dict)\n             self.assertEqual(outputs_from_ids.sequences.shape, (input_ids.shape[0], input_ids.shape[1] + 5))\n \n             # Same thing, but from input embeddings (`input_ids` is passed so the prompt is present in the output)\n@@ -674,8 +679,7 @@ def test_generate_from_inputs_embeds_decoder_only(self):\n                 input_ids,\n                 inputs_embeds=inputs_embeds,\n                 max_new_tokens=5,\n-                return_dict_in_generate=True,\n-                output_scores=True,\n+                **generation_kwargs,\n                 **inputs_dict,\n             )\n \n@@ -686,8 +690,7 @@ def test_generate_from_inputs_embeds_decoder_only(self):\n                 input_ids,\n                 inputs_embeds=random_embeds,\n                 max_new_tokens=5,\n-                return_dict_in_generate=True,\n-                output_scores=True,\n+                **generation_kwargs,\n                 **inputs_dict,\n             )\n             for i in range(len(outputs_from_rand_embeds.scores)):\n@@ -697,8 +700,7 @@ def test_generate_from_inputs_embeds_decoder_only(self):\n             outputs_from_embeds_wo_ids = model.generate(\n                 inputs_embeds=inputs_embeds,\n                 max_new_tokens=5,\n-                return_dict_in_generate=True,\n-                output_scores=True,\n+                **generation_kwargs,\n                 **inputs_dict,\n             )\n             self.assertListEqual("
        }
    ],
    "stats": {
        "total": 72,
        "additions": 28,
        "deletions": 44
    }
}