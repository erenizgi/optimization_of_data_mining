{
    "author": "LoserCheems",
    "message": "Support constant lr with cooldown (#35453)\n\n* Add support for constant learning rate with cooldown\r\n\r\n* Add support for constant learning rate with cooldown\r\n\r\n* Add support for constant learning rate with cooldown\r\n\r\n* Add support for constant learning rate with cooldown\r\n\r\n* Add support for constant learning rate with cooldown\r\n\r\n* Add support for constant learning rate with cooldown\r\n\r\n* Add support for constant learning rate with cooldown\r\n\r\n* Add more warmup and cooldown methods to 'get_wsc_schedule'\r\n\r\n* Add more warmup and cooldown methods to 'get_wsc_schedule'\r\n\r\n* Add more warmup and cooldown methods to 'get_wsc_schedule'\r\n\r\n* Add more warmup and cooldown methods to 'get_wsc_schedule'\r\n\r\n* Add more warmup and decay methods to 'get_wsd_schedule'\r\n\r\n* support num_training_steps and num_stable_steps for get_wsd_schedule\r\n\r\n* support num_training_steps and num_stable_steps for get_wsd_schedule\r\n\r\n* get wsd scheduler before the `num_training_steps` decision\r\n\r\n* fix code_quality\r\n\r\n* Update stable branch logic\r\n\r\n* fix code_quality\r\n\r\n* Move stable stage decide to `get_wsd_schedule`\r\n\r\n* Update docstring of `get_wsd_schedule`\r\n\r\n* Update `num_train_steps` to optional\r\n\r\n* Update `num_train_steps` to optional\r\n\r\n* Update docstring of `get_wsd_schedule`\r\n\r\n* Update src/transformers/optimization.py\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "48a309d0d21383f72a461d3c5e9b4c639f373bb9",
    "files": [
        {
            "sha": "d00c65925ef20013d95569b5586f3831f3cf1ffc",
            "filename": "src/transformers/optimization.py",
            "status": "modified",
            "additions": 62,
            "deletions": 13,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/48a309d0d21383f72a461d3c5e9b4c639f373bb9/src%2Ftransformers%2Foptimization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48a309d0d21383f72a461d3c5e9b4c639f373bb9/src%2Ftransformers%2Foptimization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Foptimization.py?ref=48a309d0d21383f72a461d3c5e9b4c639f373bb9",
            "patch": "@@ -393,45 +393,71 @@ def _get_wsd_scheduler_lambda(\n     num_warmup_steps: int,\n     num_stable_steps: int,\n     num_decay_steps: int,\n-    num_cycles: float,\n+    warmup_type: str,\n+    decay_type: str,\n     min_lr_ratio: float,\n+    num_cycles: float,\n ):\n     if current_step < num_warmup_steps:\n-        return float(current_step) / float(max(1, num_warmup_steps))\n+        progress = float(current_step) / float(max(1, num_warmup_steps))\n+        if warmup_type == \"linear\":\n+            factor = progress\n+        elif warmup_type == \"cosine\":\n+            factor = 0.5 * (1.0 - math.cos(math.pi * progress))\n+        elif warmup_type == \"1-sqrt\":\n+            factor = 1.0 - math.sqrt(1.0 - progress)\n+        factor = factor * (1.0 - min_lr_ratio) + min_lr_ratio\n+        return max(0.0, factor)\n+\n     if current_step < num_warmup_steps + num_stable_steps:\n         return 1.0\n+\n     if current_step < num_warmup_steps + num_stable_steps + num_decay_steps:\n         progress = float(current_step - num_warmup_steps - num_stable_steps) / float(max(1, num_decay_steps))\n-        value = max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n-        return (1.0 - min_lr_ratio) * value + min_lr_ratio\n+        if decay_type == \"linear\":\n+            factor = 1.0 - progress\n+        elif decay_type == \"cosine\":\n+            factor = 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n+        elif decay_type == \"1-sqrt\":\n+            factor = 1.0 - math.sqrt(progress)\n+        factor = factor * (1.0 - min_lr_ratio) + min_lr_ratio\n+        return max(0.0, factor)\n     return min_lr_ratio\n \n \n def get_wsd_schedule(\n     optimizer: Optimizer,\n     num_warmup_steps: int,\n-    num_stable_steps: int,\n     num_decay_steps: int,\n+    num_training_steps: Optional[int] = None,\n+    num_stable_steps: Optional[int] = None,\n+    warmup_type: str = \"linear\",\n+    decay_type: str = \"cosine\",\n     min_lr_ratio: float = 0,\n     num_cycles: float = 0.5,\n     last_epoch: int = -1,\n ):\n     \"\"\"\n     Create a schedule with a learning rate that has three stages:\n-    1. linear increase from 0 to initial lr.\n-    2. constant lr (equal to initial lr).\n-    3. decrease following the values of the cosine function between the initial lr set in the optimizer to\n-       a fraction of initial lr.\n+    1. warmup: increase from min_lr_ratio times the initial learning rate to the initial learning rate following a warmup_type.\n+    2. stable: constant learning rate.\n+    3. decay: decrease from the initial learning rate to min_lr_ratio times the initial learning rate following a decay_type.\n \n     Args:\n         optimizer ([`~torch.optim.Optimizer`]):\n             The optimizer for which to schedule the learning rate.\n         num_warmup_steps (`int`):\n             The number of steps for the warmup phase.\n-        num_stable_steps (`int`):\n-            The number of steps for the stable phase.\n         num_decay_steps (`int`):\n-            The number of steps for the cosine annealing phase.\n+            The number of steps for the decay phase.\n+        num_training_steps (`int`, *optional*):\n+            The total number of training steps. This is the sum of the warmup, stable and decay steps. If `num_stable_steps` is not provided, the stable phase will be `num_training_steps - num_warmup_steps - num_decay_steps`.\n+        num_stable_steps (`int`, *optional*):\n+            The number of steps for the stable phase. Please ensure that `num_warmup_steps + num_stable_steps + num_decay_steps` equals `num_training_steps`, otherwise the other steps will default to the minimum learning rate.\n+        warmup_type (`str`, *optional*, defaults to \"linear\"):\n+            The type of warmup to use. Can be 'linear', 'cosine' or '1-sqrt'.\n+        decay_type (`str`, *optional*, defaults to \"cosine\"):\n+            The type of decay to use. Can be 'linear', 'cosine' or '1-sqrt'.\n         min_lr_ratio (`float`, *optional*, defaults to 0):\n             The minimum learning rate as a ratio of the initial learning rate.\n         num_cycles (`float`, *optional*, defaults to 0.5):\n@@ -443,11 +469,29 @@ def get_wsd_schedule(\n     Return:\n         `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n     \"\"\"\n+\n+    if num_training_steps is None and num_stable_steps is None:\n+        raise ValueError(\"Either num_training_steps or num_stable_steps must be specified.\")\n+\n+    if num_training_steps is not None and num_stable_steps is not None:\n+        warnings.warn(\"Both num_training_steps and num_stable_steps are specified. num_stable_steps will be used.\")\n+\n+    if warmup_type not in [\"linear\", \"cosine\", \"1-sqrt\"]:\n+        raise ValueError(f\"Unknown warmup type: {warmup_type}, expected 'linear', 'cosine' or '1-sqrt'\")\n+\n+    if decay_type not in [\"linear\", \"cosine\", \"1-sqrt\"]:\n+        raise ValueError(f\"Unknown decay type: {decay_type}, expected 'linear', 'cosine' or '1-sqrt'\")\n+\n+    if num_stable_steps is None:\n+        num_stable_steps = num_training_steps - num_warmup_steps - num_decay_steps\n+\n     lr_lambda = partial(\n         _get_wsd_scheduler_lambda,\n         num_warmup_steps=num_warmup_steps,\n         num_stable_steps=num_stable_steps,\n         num_decay_steps=num_decay_steps,\n+        warmup_type=warmup_type,\n+        decay_type=decay_type,\n         min_lr_ratio=min_lr_ratio,\n         num_cycles=num_cycles,\n     )\n@@ -541,7 +585,12 @@ def scheduler_hook(param):\n         return schedule_func(optimizer, num_warmup_steps=num_warmup_steps)\n \n     if name == SchedulerType.WARMUP_STABLE_DECAY:\n-        return schedule_func(optimizer, num_warmup_steps=num_warmup_steps, **scheduler_specific_kwargs)\n+        return schedule_func(\n+            optimizer,\n+            num_warmup_steps=num_warmup_steps,\n+            num_training_steps=num_training_steps,\n+            **scheduler_specific_kwargs,\n+        )\n \n     # All other schedulers require `num_training_steps`\n     if num_training_steps is None:"
        },
        {
            "sha": "4ab248e75a9ab743e1041dbe7bdd3897b61ec743",
            "filename": "tests/optimization/test_optimization.py",
            "status": "modified",
            "additions": 24,
            "deletions": 4,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/48a309d0d21383f72a461d3c5e9b4c639f373bb9/tests%2Foptimization%2Ftest_optimization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48a309d0d21383f72a461d3c5e9b4c639f373bb9/tests%2Foptimization%2Ftest_optimization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Foptimization%2Ftest_optimization.py?ref=48a309d0d21383f72a461d3c5e9b4c639f373bb9",
            "patch": "@@ -153,8 +153,8 @@ def test_schedulers(self):\n                 [0.0, 5.0, 10.0, 8.165, 7.071, 6.325, 5.774, 5.345, 5.0, 4.714],\n             ),\n             get_wsd_schedule: (\n-                {\"num_warmup_steps\": 2, \"num_stable_steps\": 2, \"num_decay_steps\": 3, \"min_lr_ratio\": 0.1},\n-                [0.0, 5.0, 10.0, 10.0, 10.0, 7.75, 3.25, 1.0, 1.0, 1.0],\n+                {**common_kwargs, \"num_decay_steps\": 2, \"min_lr_ratio\": 0.0},\n+                [0.0, 5.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 5.0],\n             ),\n         }\n \n@@ -183,14 +183,34 @@ def test_get_scheduler(self):\n                 \"name\": \"warmup_stable_decay\",\n                 \"optimizer\": self.optimizer,\n                 \"num_warmup_steps\": 2,\n-                \"scheduler_specific_kwargs\": {\"num_stable_steps\": 1, \"num_decay_steps\": 3},\n+                \"num_training_steps\": 10,\n+                \"scheduler_specific_kwargs\": {\n+                    \"num_decay_steps\": 2,\n+                    \"warmup_type\": \"linear\",\n+                    \"decay_type\": \"linear\",\n+                },\n+            },\n+            {\n+                \"name\": \"warmup_stable_decay\",\n+                \"optimizer\": self.optimizer,\n+                \"num_warmup_steps\": 2,\n+                \"num_training_steps\": 10,\n+                \"scheduler_specific_kwargs\": {\n+                    \"num_decay_steps\": 2,\n+                    \"warmup_type\": \"cosine\",\n+                    \"decay_type\": \"cosine\",\n+                },\n             },\n             {\n                 \"name\": \"warmup_stable_decay\",\n                 \"optimizer\": self.optimizer,\n                 \"num_warmup_steps\": 2,\n                 \"num_training_steps\": 10,\n-                \"scheduler_specific_kwargs\": {\"num_stable_steps\": 1, \"num_decay_steps\": 3},\n+                \"scheduler_specific_kwargs\": {\n+                    \"num_decay_steps\": 2,\n+                    \"warmup_type\": \"1-sqrt\",\n+                    \"decay_type\": \"1-sqrt\",\n+                },\n             },\n             {\"name\": \"cosine\", \"optimizer\": self.optimizer, \"num_warmup_steps\": 2, \"num_training_steps\": 10},\n         ]"
        }
    ],
    "stats": {
        "total": 103,
        "additions": 86,
        "deletions": 17
    }
}