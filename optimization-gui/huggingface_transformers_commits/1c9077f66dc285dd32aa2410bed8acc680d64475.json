{
    "author": "zucchini-nlp",
    "message": "Fix base model prefix in VLMs (#42059)\n\n* fix base model prefix\n\n* it is now defined",
    "sha": "1c9077f66dc285dd32aa2410bed8acc680d64475",
    "files": [
        {
            "sha": "74fed6174ea4e3009c0878225d6f0cd675211db0",
            "filename": "src/transformers/models/cohere2_vision/modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py?ref=1c9077f66dc285dd32aa2410bed8acc680d64475",
            "patch": "@@ -129,7 +129,7 @@ class Cohere2VisionCausalLMOutputWithPast(ModelOutput):\n @auto_docstring\n class Cohere2VisionPreTrainedModel(PreTrainedModel):\n     config: Cohere2VisionConfig\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "28074e4c585ceb997730e05f008f47115dc16d34",
            "filename": "src/transformers/models/cohere2_vision/modular_cohere2_vision.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py?ref=1c9077f66dc285dd32aa2410bed8acc680d64475",
            "patch": "@@ -26,6 +26,7 @@\n     AyaVisionForConditionalGeneration,\n     AyaVisionModel,\n     AyaVisionModelOutputWithPast,\n+    AyaVisionPreTrainedModel,\n )\n from transformers.models.got_ocr2.image_processing_got_ocr2_fast import GotOcr2ImageProcessorFast\n \n@@ -89,6 +90,10 @@ class Cohere2VisionCausalLMOutputWithPast(AyaVisionCausalLMOutputWithPast):\n     pass\n \n \n+class Cohere2VisionPreTrainedModel(AyaVisionPreTrainedModel):\n+    base_model_prefix = \"model\"\n+\n+\n class Cohere2VisionModel(AyaVisionModel):\n     _checkpoint_conversion_mapping = {}\n \n@@ -340,7 +345,7 @@ def preprocess(self, images: ImageInput, **kwargs: Unpack[Cohere2VisionFastImage\n \n __all__ = [\n     \"Cohere2VisionForConditionalGeneration\",\n-    \"Cohere2VisionPreTrainedModel\",  # noqa: F822\n+    \"Cohere2VisionPreTrainedModel\",\n     \"Cohere2VisionModel\",\n     \"Cohere2VisionImageProcessorFast\",\n ]"
        },
        {
            "sha": "bc8ff5c27ba8b839642eee91652a13e374d7b3d2",
            "filename": "src/transformers/models/florence2/modeling_florence2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py?ref=1c9077f66dc285dd32aa2410bed8acc680d64475",
            "patch": "@@ -615,7 +615,7 @@ class Florence2Seq2SeqLMOutput(Seq2SeqLMOutput):\n @auto_docstring\n class Florence2PreTrainedModel(PreTrainedModel):\n     config: Florence2Config\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "2a09edc7ad100ee39eeeedfabc53354f4cc4fc7b",
            "filename": "src/transformers/models/florence2/modular_florence2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py?ref=1c9077f66dc285dd32aa2410bed8acc680d64475",
            "patch": "@@ -1496,6 +1496,7 @@ class Florence2Seq2SeqLMOutput(Seq2SeqLMOutput):\n @auto_docstring\n class Florence2PreTrainedModel(LlavaPreTrainedModel):\n     config_class = Florence2Config\n+    base_model_prefix = \"model\"\n \n     _supports_attention_backend = False\n "
        },
        {
            "sha": "6db8a44fe1df7c590a75a45dce68ad058d15df90",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=1c9077f66dc285dd32aa2410bed8acc680d64475",
            "patch": "@@ -1585,7 +1585,7 @@ def forward(\n @auto_docstring\n class Gemma3nPreTrainedModel(PreTrainedModel):\n     config: Gemma3nConfig\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Gemma3nTextDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]"
        },
        {
            "sha": "7bfbd4d74fb3d8f0784305a1a34ee4f96dd905a1",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=1c9077f66dc285dd32aa2410bed8acc680d64475",
            "patch": "@@ -1872,7 +1872,6 @@ def forward(\n \n class Gemma3nPreTrainedModel(Gemma2PreTrainedModel):\n     config: Gemma3nConfig\n-    base_model_prefix = \"\"\n     input_modalities = [\"image\", \"text\", \"audio\"]\n     _no_split_modules = [\"Gemma3nTextDecoderLayer\"]\n "
        },
        {
            "sha": "d9a00e8e1f92a98e560a3155a32a7849b6952b4e",
            "filename": "src/transformers/models/lfm2_vl/modeling_lfm2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py?ref=1c9077f66dc285dd32aa2410bed8acc680d64475",
            "patch": "@@ -76,7 +76,7 @@ def pixel_unshuffle(self, hidden_states: torch.Tensor):\n @auto_docstring\n class Lfm2VlPreTrainedModel(PreTrainedModel):\n     config: Lfm2VlConfig\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "4115a27cbf9cc7634413cac24faec5766d0c08d3",
            "filename": "src/transformers/models/lfm2_vl/modular_lfm2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodular_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodular_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodular_lfm2_vl.py?ref=1c9077f66dc285dd32aa2410bed8acc680d64475",
            "patch": "@@ -75,6 +75,7 @@ def pixel_unshuffle(self, hidden_states: torch.Tensor):\n \n class Lfm2VlPreTrainedModel(LlavaPreTrainedModel):\n     _can_compile_fullgraph = False\n+    base_model_prefix = \"model\"\n \n \n class Lfm2VlCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):"
        },
        {
            "sha": "81cbf38f354dc40fb4bf0b74d54c3cb66f0460b6",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=1c9077f66dc285dd32aa2410bed8acc680d64475",
            "patch": "@@ -915,7 +915,7 @@ def _deepstack_process(\n \n @auto_docstring\n class Qwen3VLModel(Qwen3VLPreTrainedModel):\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     _checkpoint_conversion_mapping = {}\n     # Reference: fix gemma3 grad acc #37208\n     accepts_loss_kwargs = False"
        },
        {
            "sha": "67a6de3b2723fc28d510880103a19022130c28d1",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=1c9077f66dc285dd32aa2410bed8acc680d64475",
            "patch": "@@ -804,6 +804,7 @@ def forward(\n @auto_docstring\n class Qwen3VLModel(Qwen2_5_VLModel):\n     config: Qwen3VLConfig\n+    base_model_prefix = \"model\"\n     _checkpoint_conversion_mapping = {}\n     _no_split_modules = [\"Qwen3VLTextDecoderLayer\", \"Qwen3VLVisionBlock\"]\n "
        },
        {
            "sha": "23546a67d73b686a4be35bcc6ad61683339aca4c",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c9077f66dc285dd32aa2410bed8acc680d64475/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=1c9077f66dc285dd32aa2410bed8acc680d64475",
            "patch": "@@ -1049,7 +1049,7 @@ class Qwen3VLMoeModelOutputWithPast(ModelOutput):\n \n @auto_docstring\n class Qwen3VLMoeModel(Qwen3VLMoePreTrainedModel):\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     _checkpoint_conversion_mapping = {}\n     # Reference: fix gemma3 grad acc #37208\n     accepts_loss_kwargs = False"
        }
    ],
    "stats": {
        "total": 23,
        "additions": 15,
        "deletions": 8
    }
}