{
    "author": "YushunXiang",
    "message": "fix: Add method to get image features in PaliGemmaForConditionalGeneration (#38730)\n\n* fix: Add method to retrieve image features in PaliGemmaForConditionalGeneration\n\n* feat: Add get_image_features method to multiple models for image feature extraction\n\n* fix: reformat the files with ruff.\n\n* feat: Add methods for packing and retrieving image and video features across multiple models\n\nmodified:\n- modeling_chameleon.py\n- modeling_llava_next.py\n- modular_llava_next_video.py\n- modeling_qwen2_vl.py\n\nand generate the:\n- modeling_llava_next_video.py\n- modeling_llava_onevision.py\n- modeling_qwen2_5_vl.py\n\n* feat: Implement get_image_features method in Aria, Mistral3, and VipLlava models with updated parameters\n\n* fix: reformatted the code with fix-style",
    "sha": "56a7cf5546b2a172ec4d96ad212c188d31492696",
    "files": [
        {
            "sha": "55b0ed79d707d870f9ff35606f6c30a8311ab157",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -1225,6 +1225,18 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        pixel_mask: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: int = -1,\n+    ):\n+        return self.model.get_image_features(\n+            pixel_values=pixel_values,\n+            pixel_mask=pixel_mask,\n+            vision_feature_layer=vision_feature_layer,\n+        )\n+\n     # Make modules available throught conditional class for BC\n     @property\n     def language_model(self):"
        },
        {
            "sha": "b8ae65d1d516118c541b37cdaa6691e22f36713f",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -1497,6 +1497,18 @@ def forward(\n     \"\"\"\n )\n class AriaForConditionalGeneration(LlavaForConditionalGeneration):\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        pixel_mask: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: int = -1,\n+    ):\n+        return self.model.get_image_features(\n+            pixel_values=pixel_values,\n+            pixel_mask=pixel_mask,\n+            vision_feature_layer=vision_feature_layer,\n+        )\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "f3b9d77b2ce51efff4c133ffa0c50cce1e6287f6",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -394,6 +394,20 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        **kwargs,\n+    ):\n+        return self.model.get_image_features(\n+            pixel_values=pixel_values,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+            **kwargs,\n+        )\n+\n     # Make modules available throught conditional class for BC\n     @property\n     def language_model(self):"
        },
        {
            "sha": "63ec521882c87f525536ece501a3334767c02924",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -1229,6 +1229,12 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    def get_image_tokens(self, pixel_values):\n+        return self.model.get_image_tokens(pixel_values)\n+\n+    def get_image_features(self, pixel_values):\n+        return self.model.get_image_features(pixel_values)\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "dd8587e9d1140843fd02b9568c6fcc1194a8adef",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -1019,6 +1019,9 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    def get_image_features(self, pixel_values):\n+        return self.model.get_image_features(pixel_values)\n+\n     # Make modules available throught conditional class for BC\n     @property\n     def language_model(self):"
        },
        {
            "sha": "044013bed45e2f7a4e8ae67291703005eff6a12c",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -762,6 +762,20 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        **kwargs,\n+    ):\n+        return self.model.get_image_features(\n+            pixel_values=pixel_values,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+            **kwargs,\n+        )\n+\n     # Make modules available throught conditional class for BC\n     @property\n     def language_model(self):"
        },
        {
            "sha": "4607051b195a275e1a6d503554e607edc936af9d",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -1186,6 +1186,9 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n+    def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n+        return self.model.get_image_features(pixel_values=pixel_values, pixel_attention_mask=pixel_attention_mask)\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "99ea7fd0d3d093b5ba26bc333dc47222509f07e9",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -909,6 +909,9 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n+    def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n+        return self.model.get_image_features(pixel_values=pixel_values, pixel_attention_mask=pixel_attention_mask)\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "84a2d412ff42cb58fb8b78b2ed029a3134faf733",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -883,6 +883,20 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        **kwargs,\n+    ):\n+        return self.model.get_image_features(\n+            pixel_values=pixel_values,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+            **kwargs,\n+        )\n+\n     # Make modules available throught conditional class for BC\n     @property\n     def language_model(self):"
        },
        {
            "sha": "aa83b57678b631918ec18709d5ca5a5655c9f9fd",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -376,6 +376,20 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        **kwargs,\n+    ):\n+        return self.model.get_image_features(\n+            pixel_values=pixel_values,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+            **kwargs,\n+        )\n+\n     # Make modules available throught conditional class for BC\n     @property\n     def language_model(self):"
        },
        {
            "sha": "010de9060a1bd9a6d49173be9f25bcdfd5b0e7e6",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -574,6 +574,28 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n+        return self.model.pack_image_features(\n+            image_features=image_features,\n+            image_sizes=image_sizes,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+            image_newline=image_newline,\n+        )\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        image_sizes: torch.Tensor,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+    ):\n+        return self.model.get_image_features(\n+            pixel_values=pixel_values,\n+            image_sizes=image_sizes,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+        )\n+\n     # Make modules available throught conditional class for BC\n     @property\n     def language_model(self):"
        },
        {
            "sha": "9b663bd3903217bf03573b558878681b588e18e2",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -706,6 +706,28 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n+        return self.model.pack_image_features(\n+            image_features=image_features,\n+            image_sizes=image_sizes,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+            image_newline=image_newline,\n+        )\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        image_sizes: torch.Tensor,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+    ):\n+        return self.model.get_image_features(\n+            pixel_values=pixel_values,\n+            image_sizes=image_sizes,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+        )\n+\n     # Make modules available throught conditional class for BC\n     @property\n     def language_model(self):\n@@ -952,5 +974,17 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n         return causal_mask\n \n+    def get_video_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+    ):\n+        return self.model.get_video_features(\n+            pixel_values=pixel_values,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+        )\n+\n \n __all__ = [\"LlavaNextVideoForConditionalGeneration\", \"LlavaNextVideoModel\", \"LlavaNextVideoPreTrainedModel\"]"
        },
        {
            "sha": "97ea22ae2e641535d6ad4c6f012302860ab14248",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -496,6 +496,18 @@ def forward(\n \n \n class LlavaNextVideoForConditionalGeneration(LlavaNextForConditionalGeneration):\n+    def get_video_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+    ):\n+        return self.model.get_video_features(\n+            pixel_values=pixel_values,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+        )\n+\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,"
        },
        {
            "sha": "4205abf857153213d53546f9adf7119891141f2f",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -747,6 +747,28 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n+        return self.model.pack_image_features(\n+            image_features=image_features,\n+            image_sizes=image_sizes,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+            image_newline=image_newline,\n+        )\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        image_sizes: torch.Tensor,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+    ):\n+        return self.model.get_image_features(\n+            pixel_values=pixel_values,\n+            image_sizes=image_sizes,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+        )\n+\n     # Make modules available throught conditional class for BC\n     @property\n     def language_model(self):\n@@ -988,5 +1010,17 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n         return causal_mask\n \n+    def get_video_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+    ):\n+        return self.model.get_video_features(\n+            pixel_values=pixel_values,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+        )\n+\n \n __all__ = [\"LlavaOnevisionModel\", \"LlavaOnevisionForConditionalGeneration\", \"LlavaOnevisionPreTrainedModel\"]"
        },
        {
            "sha": "e034392b740a07e2feba5674d85cc78c39132ad9",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -412,6 +412,20 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        image_sizes: torch.Tensor,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        **kwargs,\n+    ):\n+        return self.model.get_image_features(\n+            pixel_values=pixel_values,\n+            image_sizes=image_sizes,\n+            vision_feature_layer=vision_feature_layer,\n+            **kwargs,\n+        )\n+\n     # Make modules available throught conditional class for BC\n     @property\n     def language_model(self):"
        },
        {
            "sha": "611973b64bdd71b7b7fedae41c7962d1a1399e90",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -254,6 +254,20 @@ def forward(\n \n \n class Mistral3ForConditionalGeneration(LlavaForConditionalGeneration):\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        image_sizes: torch.Tensor,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        **kwargs,\n+    ):\n+        return self.model.get_image_features(\n+            pixel_values=pixel_values,\n+            image_sizes=image_sizes,\n+            vision_feature_layer=vision_feature_layer,\n+            **kwargs,\n+        )\n+\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,"
        },
        {
            "sha": "4782da29ce48d69733af549be3fc167002078e47",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -423,6 +423,9 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    def get_image_features(self, pixel_values):\n+        return self.model.get_image_features(pixel_values)\n+\n     # Make modules available throught conditional class for BC\n     @property\n     def language_model(self):"
        },
        {
            "sha": "17861e714a980fafe23f96136b95fb103eb07dd3",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -1503,6 +1503,14 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    def get_video_features(\n+        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n+    ):\n+        return self.model.get_video_features(pixel_values_videos, video_grid_thw)\n+\n+    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n+        return self.model.get_image_features(pixel_values, image_grid_thw)\n+\n     # Make modules available throught conditional class for BC\n     @property\n     def language_model(self):"
        },
        {
            "sha": "67981d78cb9dde84f02e504d637ecd7955e2bd65",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -1387,6 +1387,14 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    def get_video_features(\n+        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n+    ):\n+        return self.model.get_video_features(pixel_values_videos, video_grid_thw)\n+\n+    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n+        return self.model.get_image_features(pixel_values, image_grid_thw)\n+\n     # Make modules available throught conditional class for BC\n     @property\n     def language_model(self):"
        },
        {
            "sha": "5b472cb5c2a38775a1d506f087b8b07e4b165bd8",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -874,6 +874,9 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n+    def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n+        return self.model.get_image_features(pixel_values=pixel_values, pixel_attention_mask=pixel_attention_mask)\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "8994bf805ed49cc765529f6071877bc6218a8f7b",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -449,6 +449,18 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    def get_image_features(\n+        self,\n+        pixel_values_images: torch.FloatTensor,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+    ):\n+        return self.model.get_image_features(\n+            pixel_values_images=pixel_values_images,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+        )\n+\n     # Make modules available throught conditional class for BC\n     @property\n     def language_model(self):"
        },
        {
            "sha": "e451f3d9af8cfdef11b2788e921797e3e829e6e4",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -332,6 +332,11 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    def get_image_features(\n+        self, pixel_values: torch.FloatTensor, vision_feature_layers: Optional[Union[int, List[int]]] = None\n+    ):\n+        return self.model.get_image_features(pixel_values=pixel_values, vision_feature_layers=vision_feature_layers)\n+\n     # Make modules available throught conditional class for BC\n     @property\n     def language_model(self):"
        },
        {
            "sha": "a673bba7a99aca8d4d3ef4efebb36ae78e0f7165",
            "filename": "src/transformers/models/vipllava/modular_vipllava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a7cf5546b2a172ec4d96ad212c188d31492696/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py?ref=56a7cf5546b2a172ec4d96ad212c188d31492696",
            "patch": "@@ -184,6 +184,11 @@ def forward(\n \n \n class VipLlavaForConditionalGeneration(LlavaForConditionalGeneration):\n+    def get_image_features(\n+        self, pixel_values: torch.FloatTensor, vision_feature_layers: Optional[Union[int, List[int]]] = None\n+    ):\n+        return self.model.get_image_features(pixel_values=pixel_values, vision_feature_layers=vision_feature_layers)\n+\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,"
        }
    ],
    "stats": {
        "total": 269,
        "additions": 269,
        "deletions": 0
    }
}