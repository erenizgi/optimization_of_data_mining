{
    "author": "threewebcode",
    "message": "chore: fix typos in the tests directory (#36813)\n\n* chore: fix typos in the tests\n\n* chore: fix typos in the tests\n\n* chore: fix typos in the tests\n\n* chore: fix typos in the tests\n\n* chore: fix typos in the tests\n\n* chore: fix typos in the tests\n\n* chore: fix typos in the tests\n\n* chore: fix typos in the tests\n\n* chore: fix typos in the tests\n\n* chore: fix typos in the tests\n\n* chore: fix typos in the tests\n\n* chore: fix typos in the tests\n\n* chore: fix typos in the tests\n\n* fix: format codes\n\n* chore: fix copy mismatch issue\n\n* fix: format codes\n\n* chore: fix copy mismatch issue\n\n* chore: fix copy mismatch issue\n\n* chore: fix copy mismatch issue\n\n* chore: restore previous words\n\n* chore: revert unexpected changes",
    "sha": "26c83490d2fef4b6622bd69c80beae14f2626cbf",
    "files": [
        {
            "sha": "11bea3c3aa886a25cf8f8e0a6f1c0e6770d22e0f",
            "filename": "tests/deepspeed/test_deepspeed.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_deepspeed.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -628,7 +628,7 @@ def model_init():\n                 with CaptureStd() as cs:\n                     trainer.hyperparameter_search(direction=\"maximize\", n_trials=n_trials)\n             self.assertIn(\"DeepSpeed info\", cl.out, \"expected DeepSpeed logger output but got none\")\n-            self.assertIn(f\"Trial {n_trials-1} finished with value\", cs.err, \"expected hyperparameter_search output\")\n+            self.assertIn(f\"Trial {n_trials - 1} finished with value\", cs.err, \"expected hyperparameter_search output\")\n             self.assertIn(\"Best is trial\", cs.err, \"expected hyperparameter_search output\")\n \n     # --- These tests need to run on both zero stages --- #"
        },
        {
            "sha": "e93ab4479506fc5c867a28eda6cd8826139059d3",
            "filename": "tests/models/bart/test_modeling_tf_bart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fbart%2Ftest_modeling_tf_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fbart%2Ftest_modeling_tf_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_tf_bart.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -87,7 +87,7 @@ def prepare_config_and_inputs_for_common(self):\n             clip_value_min=self.eos_token_id + 1,\n             clip_value_max=self.vocab_size + 1,\n         )\n-        # Explicity add \"end of sequence\" to the inputs\n+        # Explicitly add \"end of sequence\" to the inputs\n         eos_tensor = tf.expand_dims(tf.constant([self.eos_token_id] * self.batch_size), 1)\n         input_ids = tf.concat([input_ids, eos_tensor], axis=1)\n \n@@ -225,7 +225,7 @@ def test_decoder_model_past_large_inputs(self):\n         self.model_tester.check_decoder_model_past_large_inputs(*config_and_inputs)\n \n     # TODO (Joao): fix me\n-    @unittest.skip(\"Onnx compliancy broke with TF 2.10\")\n+    @unittest.skip(\"Onnx compliance broke with TF 2.10\")\n     def test_onnx_compliancy(self):\n         pass\n "
        },
        {
            "sha": "a8f8b73477674d4cf74d99dd0ca5b115c0b8cf91",
            "filename": "tests/models/bert/test_modeling_tf_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fbert%2Ftest_modeling_tf_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fbert%2Ftest_modeling_tf_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_tf_bert.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -735,7 +735,7 @@ def test_custom_load_tf_weights(self):\n             self.assertTrue(layer.split(\"_\")[0] in [\"dropout\", \"classifier\"])\n \n     # TODO (Joao): fix me\n-    @unittest.skip(\"Onnx compliancy broke with TF 2.10\")\n+    @unittest.skip(\"Onnx compliance broke with TF 2.10\")\n     def test_onnx_compliancy(self):\n         pass\n "
        },
        {
            "sha": "3dac349fb4fd055c7c63dac300cdf7098d0f8fef",
            "filename": "tests/models/blip/test_modeling_blip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -474,7 +474,7 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    # override as the `logit_scale` parameter initilization is different for Blip\n+    # override as the `logit_scale` parameter initialization is different for Blip\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -483,7 +483,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    # check if `logit_scale` is initilized as per the original implementation\n+                    # check if `logit_scale` is initialized as per the original implementation\n                     if name == \"logit_scale\":\n                         self.assertAlmostEqual(\n                             param.data.item(),\n@@ -988,7 +988,7 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    # override as the `logit_scale` parameter initilization is different for Blip\n+    # override as the `logit_scale` parameter initialization is different for Blip\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -997,7 +997,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    # check if `logit_scale` is initilized as per the original implementation\n+                    # check if `logit_scale` is initialized as per the original implementation\n                     if name == \"logit_scale\":\n                         self.assertAlmostEqual(\n                             param.data.item(),\n@@ -1206,7 +1206,7 @@ def test_training_gradient_checkpointing(self):\n             loss = model(**inputs).loss\n             loss.backward()\n \n-    # override as the `logit_scale` parameter initilization is different for Blip\n+    # override as the `logit_scale` parameter initialization is different for Blip\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -1215,7 +1215,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    # check if `logit_scale` is initilized as per the original implementation\n+                    # check if `logit_scale` is initialized as per the original implementation\n                     if name == \"logit_scale\":\n                         self.assertAlmostEqual(\n                             param.data.item(),"
        },
        {
            "sha": "b55ec4a23c05d895aca3a55cedc97927726e8988",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -521,7 +521,7 @@ def test_save_load_fast_init_to_base(self):\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n-        This tests only by looking at layer names, as usually SDPA layers are calles \"SDPAAttention\".\n+        This tests only by looking at layer names, as usually SDPA layers are called \"SDPAAttention\".\n         In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n         is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n         See https://github.com/huggingface/transformers/pull/32238 for more info\n@@ -970,7 +970,7 @@ def test_cpu_offload(self):\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n-        This tests only by looking at layer names, as usually SDPA layers are calles \"SDPAAttention\".\n+        This tests only by looking at layer names, as usually SDPA layers are called \"SDPAAttention\".\n         In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n         is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n         See https://github.com/huggingface/transformers/pull/32238 for more info\n@@ -1647,7 +1647,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    # check if `logit_scale` is initilized as per the original implementation\n+                    # check if `logit_scale` is initialized as per the original implementation\n                     if name == \"logit_scale\":\n                         self.assertAlmostEqual(\n                             param.data.item(),"
        },
        {
            "sha": "6c2fffef64f1fc45e1cee77e705ad4d157fdf155",
            "filename": "tests/models/bloom/test_tokenization_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -135,7 +135,7 @@ def test_encodings_from_xnli_dataset(self):\n     @require_jinja\n     def test_tokenization_for_chat(self):\n         tokenizer = self.get_rust_tokenizer()\n-        tokenizer.chat_template = \"{% for message in messages %}\" \"{{ message.content }}{{ eos_token }}\" \"{% endfor %}\"\n+        tokenizer.chat_template = \"{% for message in messages %}{{ message.content }}{{ eos_token }}{% endfor %}\"\n         test_chats = [\n             [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n             ["
        },
        {
            "sha": "13fcbb0499916ac2bb2f23f80e74ef2ecb38f766",
            "filename": "tests/models/canine/test_tokenization_canine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fcanine%2Ftest_tokenization_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fcanine%2Ftest_tokenization_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcanine%2Ftest_tokenization_canine.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -64,7 +64,7 @@ def test_prepare_batch_integration(self):\n     @require_torch\n     def test_encoding_keys(self):\n         tokenizer = self.canine_tokenizer\n-        src_text = [\"Once there was a man.\", \"He wrote a test in HuggingFace Tranformers.\"]\n+        src_text = [\"Once there was a man.\", \"He wrote a test in HuggingFace Transformers.\"]\n         batch = tokenizer(src_text, padding=True, return_tensors=\"pt\")\n         # check if input_ids, attention_mask and token_type_ids are returned\n         self.assertIn(\"input_ids\", batch)"
        },
        {
            "sha": "bc14d80524c4252ad629dfd402a6f860fa6357aa",
            "filename": "tests/models/chinese_clip/test_modeling_chinese_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -596,7 +596,7 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    # override as the `logit_scale` parameter initilization is different for CHINESE_CLIP\n+    # override as the `logit_scale` parameter initialization is different for CHINESE_CLIP\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -608,7 +608,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    # check if `logit_scale` is initilized as per the original implementation\n+                    # check if `logit_scale` is initialized as per the original implementation\n                     if name == \"logit_scale\":\n                         self.assertAlmostEqual(\n                             param.data.item(),"
        },
        {
            "sha": "c8250648c6708bd54bec72e6fb9fdb8227bec251",
            "filename": "tests/models/clap/test_modeling_clap.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -543,7 +543,7 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    # override as the `logit_scale` parameter initilization is different for CLAP\n+    # override as the `logit_scale` parameter initialization is different for CLAP\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -552,7 +552,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    # check if `logit_scale` is initilized as per the original implementation\n+                    # check if `logit_scale` is initialized as per the original implementation\n                     if name == \"logit_scale\":\n                         self.assertAlmostEqual(\n                             param.data.item(),"
        },
        {
            "sha": "66f741e6f4b2a907e0a1e81999cb32b4555adc92",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -761,7 +761,7 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    # override as the `logit_scale` parameter initilization is different for CLIP\n+    # override as the `logit_scale` parameter initialization is different for CLIP\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -770,7 +770,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    # check if `logit_scale` is initilized as per the original implementation\n+                    # check if `logit_scale` is initialized as per the original implementation\n                     if name == \"logit_scale\":\n                         self.assertAlmostEqual(\n                             param.data.item(),"
        },
        {
            "sha": "a116b82f5f4e6cf87e527e5db4bc018e7574f27b",
            "filename": "tests/models/clipseg/test_modeling_clipseg.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -519,7 +519,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    # check if `logit_scale` is initilized as per the original implementation\n+                    # check if `logit_scale` is initialized as per the original implementation\n                     if \"logit_scale\" in name:\n                         self.assertAlmostEqual(\n                             param.data.item(),"
        },
        {
            "sha": "8064d4059ec6d356b291d29156d3371ae243953b",
            "filename": "tests/models/clvp/test_modeling_clvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -500,7 +500,7 @@ def test_inputs_embeds(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    # override as the `logit_scale` parameter initilization is different for Clvp\n+    # override as the `logit_scale` parameter initialization is different for Clvp\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -509,7 +509,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    # check if `logit_scale` is initilized as per the original implementation\n+                    # check if `logit_scale` is initialized as per the original implementation\n                     if name == \"logit_scale\":\n                         expected_value = np.log(1 / 0.07)\n                         returned_value = param.data.item()"
        },
        {
            "sha": "118552fce5fd0f1a8a9b1391b839408c81ed01d7",
            "filename": "tests/models/conditional_detr/test_modeling_conditional_detr.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -465,13 +465,13 @@ def test_different_timm_backbone(self):\n                     self.model_tester.num_labels,\n                 )\n                 self.assertEqual(outputs.logits.shape, expected_shape)\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n             elif model_class.__name__ == \"ConditionalDetrForSegmentation\":\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.conditional_detr.model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n             else:\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n \n             self.assertTrue(outputs)\n@@ -501,13 +501,13 @@ def test_hf_backbone(self):\n                     self.model_tester.num_labels,\n                 )\n                 self.assertEqual(outputs.logits.shape, expected_shape)\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n             elif model_class.__name__ == \"ConditionalDetrForSegmentation\":\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.conditional_detr.model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n             else:\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n \n             self.assertTrue(outputs)"
        },
        {
            "sha": "4a6c28e1eb919cc1c2a18a0dc872470c7393d41c",
            "filename": "tests/models/cpmant/test_modeling_cpmant.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -161,7 +161,7 @@ def test_inputs_embeds(self):\n     def test_retain_grad_hidden_states_attentions(self):\n         unittest.skip(\n             \"CPMAnt doesn't support retain grad in hidden_states or attentions, because prompt management will peel off the output.hidden_states from graph.\\\n-                 So is attentions. We strongly recommand you use loss to tune model.\"\n+                 So is attentions. We strongly recommend you use loss to tune model.\"\n         )(self.test_retain_grad_hidden_states_attentions)\n \n     def test_cpmant_model(self):"
        },
        {
            "sha": "e3441e606d23865bb92a8b85cb51b384b66a7df5",
            "filename": "tests/models/deformable_detr/test_modeling_deformable_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -542,10 +542,10 @@ def test_different_timm_backbone(self):\n                     self.model_tester.num_labels,\n                 )\n                 self.assertEqual(outputs.logits.shape, expected_shape)\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.model.backbone.conv_encoder.intermediate_channel_sizes), 4)\n             else:\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.backbone.conv_encoder.intermediate_channel_sizes), 4)\n \n             self.assertTrue(outputs)\n@@ -574,10 +574,10 @@ def test_hf_backbone(self):\n                     self.model_tester.num_labels,\n                 )\n                 self.assertEqual(outputs.logits.shape, expected_shape)\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.model.backbone.conv_encoder.intermediate_channel_sizes), 4)\n             else:\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.backbone.conv_encoder.intermediate_channel_sizes), 4)\n \n             self.assertTrue(outputs)"
        },
        {
            "sha": "63d57d671706be0b911582b6643823baef71b480",
            "filename": "tests/models/depth_anything/test_modeling_depth_anything.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -214,7 +214,7 @@ def _validate_backbone_init():\n                 model.to(torch_device)\n                 model.eval()\n \n-                # Confirm out_indices propogated to backbone\n+                # Confirm out_indices propagated to backbone\n                 self.assertEqual(len(model.backbone.out_indices), 2)\n \n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "cb38380dc740bff3dcb570b2303f4961a59b263a",
            "filename": "tests/models/depth_pro/test_modeling_depth_pro.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -323,7 +323,7 @@ def test_initialization(self):\n                             msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                         )\n \n-    # this started when switched from normal initialization to kaiming_normal intialization\n+    # this started when switched from normal initialization to kaiming_normal initialization\n     # maybe because the magnitude of offset values from ViT-encoders increases when followed by many convolution layers\n     def test_batching_equivalence(self, atol=1e-4, rtol=1e-4):\n         super().test_batching_equivalence(atol=atol, rtol=rtol)"
        },
        {
            "sha": "003deceab7625beabdd23edf08d24958bee42890",
            "filename": "tests/models/detr/test_modeling_detr.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -465,13 +465,13 @@ def test_different_timm_backbone(self):\n                     self.model_tester.num_labels + 1,\n                 )\n                 self.assertEqual(outputs.logits.shape, expected_shape)\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n             elif model_class.__name__ == \"DetrForSegmentation\":\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.detr.model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n             else:\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n \n             self.assertTrue(outputs)\n@@ -500,13 +500,13 @@ def test_hf_backbone(self):\n                     self.model_tester.num_labels + 1,\n                 )\n                 self.assertEqual(outputs.logits.shape, expected_shape)\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n             elif model_class.__name__ == \"DetrForSegmentation\":\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.detr.model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n             else:\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n \n             self.assertTrue(outputs)"
        },
        {
            "sha": "9864b713a59df01aac7b75cf439f0ef3ef510e21",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -433,7 +433,9 @@ def test_model_rope_scaling(self):\n         long_input_length = int(config.max_position_embeddings * 1.5)\n \n         # Inputs\n-        x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n+        x = torch.randn(\n+            1, dtype=torch.float32, device=torch_device\n+        )  # used exclusively to get the dtype and the device\n         position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n         position_ids_short = position_ids_short.unsqueeze(0)\n         position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)\n@@ -553,7 +555,7 @@ def _reinitialize_config(base_config, new_kwargs):\n     @slow\n     def test_flash_attn_2_generate_padding_right(self):\n         \"\"\"\n-        Overwritting the common test as the test is flaky on tiny models\n+        Overwriting the common test as the test is flaky on tiny models\n         \"\"\"\n         model = DiffLlamaForCausalLM.from_pretrained(\n             \"kajuma/DiffLlama-0.3B-handcut\",\n@@ -617,7 +619,7 @@ def test_use_flash_attention_2_true(self):\n     @slow\n     def test_eager_matches_sdpa_generate(self):\n         \"\"\"\n-        Overwritting the common test as the test is flaky on tiny models\n+        Overwriting the common test as the test is flaky on tiny models\n         \"\"\"\n         max_new_tokens = 30\n "
        },
        {
            "sha": "bdfd480a90ab1d0e2acfec5d2e707384473eb817",
            "filename": "tests/models/dpt/test_modeling_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -290,7 +290,7 @@ def _validate_backbone_init():\n                 model.eval()\n \n                 if model.__class__.__name__ == \"DPTForDepthEstimation\":\n-                    # Confirm out_indices propogated to backbone\n+                    # Confirm out_indices propagated to backbone\n                     self.assertEqual(len(model.backbone.out_indices), 2)\n \n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "209d02ad19c380e5242fb490f1c86044ef86a01e",
            "filename": "tests/models/encodec/test_modeling_encodec.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -492,7 +492,7 @@ def test_integration_24kHz(self):\n \n         for bandwidth, expected_rmse in expected_rmse.items():\n             with torch.no_grad():\n-                # use max bandwith for best possible reconstruction\n+                # use max bandwidth for best possible reconstruction\n                 encoder_outputs = model.encode(inputs[\"input_values\"], bandwidth=float(bandwidth))\n \n                 audio_code_sums = [a[0].sum().cpu().item() for a in encoder_outputs[0]]\n@@ -548,7 +548,7 @@ def test_integration_48kHz(self):\n \n         for bandwidth, expected_rmse in expected_rmse.items():\n             with torch.no_grad():\n-                # use max bandwith for best possible reconstruction\n+                # use max bandwidth for best possible reconstruction\n                 encoder_outputs = model.encode(\n                     inputs[\"input_values\"], inputs[\"padding_mask\"], bandwidth=float(bandwidth), return_dict=False\n                 )\n@@ -608,7 +608,7 @@ def test_batch_48kHz(self):\n         input_values = inputs[\"input_values\"].to(torch_device)\n         for bandwidth, expected_rmse in expected_rmse.items():\n             with torch.no_grad():\n-                # use max bandwith for best possible reconstruction\n+                # use max bandwidth for best possible reconstruction\n                 encoder_outputs = model.encode(input_values, bandwidth=float(bandwidth), return_dict=False)\n                 audio_code_sums_0 = [a[0][0].sum().cpu().item() for a in encoder_outputs[0]]\n                 audio_code_sums_1 = [a[0][1].sum().cpu().item() for a in encoder_outputs[0]]"
        },
        {
            "sha": "4f1cb32348d8d3e384d49c7819102d27b2b28082",
            "filename": "tests/models/encoder_decoder/test_modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -179,7 +179,10 @@ def check_encoder_decoder_model_from_pretrained_using_model_paths(\n         **kwargs,\n     ):\n         encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n-        with tempfile.TemporaryDirectory() as encoder_tmp_dirname, tempfile.TemporaryDirectory() as decoder_tmp_dirname:\n+        with (\n+            tempfile.TemporaryDirectory() as encoder_tmp_dirname,\n+            tempfile.TemporaryDirectory() as decoder_tmp_dirname,\n+        ):\n             encoder_model.save_pretrained(encoder_tmp_dirname)\n             decoder_model.save_pretrained(decoder_tmp_dirname)\n             model_kwargs = {\"encoder_hidden_dropout_prob\": 0.0}\n@@ -306,7 +309,10 @@ def check_save_and_load_encoder_decoder_model(\n             out_2 = outputs[0].cpu().numpy()\n             out_2[np.isnan(out_2)] = 0\n \n-            with tempfile.TemporaryDirectory() as encoder_tmp_dirname, tempfile.TemporaryDirectory() as decoder_tmp_dirname:\n+            with (\n+                tempfile.TemporaryDirectory() as encoder_tmp_dirname,\n+                tempfile.TemporaryDirectory() as decoder_tmp_dirname,\n+            ):\n                 enc_dec_model.encoder.save_pretrained(encoder_tmp_dirname)\n                 enc_dec_model.decoder.save_pretrained(decoder_tmp_dirname)\n                 enc_dec_model = EncoderDecoderModel.from_encoder_decoder_pretrained("
        },
        {
            "sha": "aec59eca3356ebd0f388140e15ed2066649caaa5",
            "filename": "tests/models/falcon/test_modeling_falcon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -460,7 +460,9 @@ def test_model_rope_scaling(self):\n         long_input_length = int(config.max_position_embeddings * 1.5)\n \n         # Inputs\n-        x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n+        x = torch.randn(\n+            1, dtype=torch.float32, device=torch_device\n+        )  # used exclusively to get the dtype and the device\n         position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n         position_ids_short = position_ids_short.unsqueeze(0)\n         position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)"
        },
        {
            "sha": "a75238265cd47d0e2fb51846e0c6ca9c651aba41",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -211,7 +211,7 @@ def create_and_check_state_equivalency(self, config, input_ids, *args):\n         output_two = outputs.last_hidden_state\n \n         self.parent.assertTrue(torch.allclose(torch.cat([output_one, output_two], dim=1), output_whole, atol=1e-5))\n-        # TODO the orignal mamba does not support decoding more than 1 token neither do we\n+        # TODO the original mamba does not support decoding more than 1 token neither do we\n \n     def create_and_check_falcon_mamba_cached_slow_forward_and_backwards(\n         self, config, input_ids, *args, gradient_checkpointing=False"
        },
        {
            "sha": "0480335f05aa50d89dac8724fd8be65732840a5f",
            "filename": "tests/models/fastspeech2_conformer/test_modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -104,7 +104,7 @@ def create_and_check_model(self, config, input_ids, *args):\n         # check batch sizes match\n         for value in result.values():\n             self.parent.assertEqual(value.size(0), self.batch_size)\n-        # check duration, pitch, and energy have the appopriate shapes\n+        # check duration, pitch, and energy have the appropriate shapes\n         # duration: (batch_size, max_text_length), pitch and energy: (batch_size, max_text_length, 1)\n         self.parent.assertEqual(result[\"duration_outputs\"].shape + (1,), result[\"pitch_outputs\"].shape)\n         self.parent.assertEqual(result[\"pitch_outputs\"].shape, result[\"energy_outputs\"].shape)\n@@ -527,7 +527,7 @@ def create_and_check_model(self, config, input_ids, *args):\n         # check batch sizes match\n         for value in result.values():\n             self.parent.assertEqual(value.size(0), self.batch_size)\n-        # check duration, pitch, and energy have the appopriate shapes\n+        # check duration, pitch, and energy have the appropriate shapes\n         # duration: (batch_size, max_text_length), pitch and energy: (batch_size, max_text_length, 1)\n         self.parent.assertEqual(result[\"duration_outputs\"].shape + (1,), result[\"pitch_outputs\"].shape)\n         self.parent.assertEqual(result[\"pitch_outputs\"].shape, result[\"energy_outputs\"].shape)"
        },
        {
            "sha": "2aa35863697bfb39206040338a6aea3ed4cc798a",
            "filename": "tests/models/fnet/test_modeling_fnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -326,7 +326,7 @@ def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n                 )\n         return inputs_dict\n \n-    # Overriden Tests\n+    # Overridden Tests\n     @unittest.skip\n     def test_attention_outputs(self):\n         pass"
        },
        {
            "sha": "a55c142b25d8a682e80ad411aa7af0a26d0aef49",
            "filename": "tests/models/fnet/test_tokenization_fnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ffnet%2Ftest_tokenization_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ffnet%2Ftest_tokenization_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffnet%2Ftest_tokenization_fnet.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -141,7 +141,7 @@ def test_sequence_builders(self):\n             tokenizer.sep_token_id\n         ]\n \n-    # Overriden Tests - loading the fast tokenizer from slow just takes too long\n+    # Overridden Tests - loading the fast tokenizer from slow just takes too long\n     def test_special_tokens_initialization(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n@@ -190,7 +190,7 @@ def test_special_tokens_initialization_from_slow(self):\n                 self.assertTrue(special_token_id in p_output)\n                 self.assertTrue(special_token_id in cr_output)\n \n-    # Overriden Tests\n+    # Overridden Tests\n     def test_padding(self, max_length=50):\n         if not self.test_slow_tokenizer:\n             # as we don't have a slow version, we can't compare the outputs between slow and fast versions"
        },
        {
            "sha": "0b43fec80556a9083a4df17248356be1ed5466c9",
            "filename": "tests/models/gemma/test_tokenization_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fgemma%2Ftest_tokenization_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fgemma%2Ftest_tokenization_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_tokenization_gemma.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -342,8 +342,8 @@ def test_integration_test_xnli(self):\n                 encoded1,\n                 encoded2,\n                 msg=\"Hint: the following tokenization diff were obtained for slow vs fast:\\n \"\n-                f\"elements in slow: {set(pyth_tokenizer.tokenize(string))-set(rust_tokenizer.tokenize(string))} \\nvs\\n \"\n-                f\"elements in fast: {set(rust_tokenizer.tokenize(string))-set(pyth_tokenizer.tokenize(string))} \\n\\n{string}\",\n+                f\"elements in slow: {set(pyth_tokenizer.tokenize(string)) - set(rust_tokenizer.tokenize(string))} \\nvs\\n \"\n+                f\"elements in fast: {set(rust_tokenizer.tokenize(string)) - set(pyth_tokenizer.tokenize(string))} \\n\\n{string}\",\n             )\n \n             decoded1 = pyth_tokenizer.decode(encoded1, skip_special_tokens=True)"
        },
        {
            "sha": "0f66e1681882657ad99d77ee63108ef9f7ca6314",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -866,7 +866,7 @@ def test_contrastive_search_gpt2(self):\n     @slow\n     def test_flash_attn_2_generate_padding_left(self):\n         \"\"\"\n-        Overwritting the common test as the test is flaky on tiny models\n+        Overwriting the common test as the test is flaky on tiny models\n         \"\"\"\n         model = GPT2LMHeadModel.from_pretrained(\"gpt2\", torch_dtype=torch.float16).to(0)\n "
        },
        {
            "sha": "ed4c4a2e89b70ec64af52e9d817750495311df24",
            "filename": "tests/models/gpt2/test_modeling_tf_gpt2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fgpt2%2Ftest_modeling_tf_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fgpt2%2Ftest_modeling_tf_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_tf_gpt2.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -451,7 +451,7 @@ def test_onnx_runtime_optimize(self):\n             onnxruntime.InferenceSession(onnx_model_proto.SerializeToString())\n \n     # TODO (Joao): fix me\n-    @unittest.skip(\"Onnx compliancy broke with TF 2.10\")\n+    @unittest.skip(\"Onnx compliance broke with TF 2.10\")\n     def test_onnx_compliancy(self):\n         pass\n \n@@ -548,7 +548,7 @@ def test_lm_generate_greedy_distilgpt2_beam_search_special(self):\n \n     @slow\n     def test_lm_generate_distilgpt2_left_padding(self):\n-        \"\"\"Tests that the generated text is the same, regarless of left padding\"\"\"\n+        \"\"\"Tests that the generated text is the same, regardless of left padding\"\"\"\n         model = TFGPT2LMHeadModel.from_pretrained(\"distilbert/distilgpt2\")\n         tokenizer = GPT2Tokenizer.from_pretrained(\"distilbert/distilgpt2\")\n "
        },
        {
            "sha": "c0c3639781ca4b54d348addbbaa06c1d0b1e826b",
            "filename": "tests/models/gpt_neo/test_modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -479,7 +479,7 @@ def test_local_attn_probs(self):\n         # the last 2 tokens are masked, and should have 0 attn_probs\n         self.assertTrue(torch.all(attn_probs[:, :, -mask_tokens:, -mask_tokens:] == 0))\n \n-        # in loacal attention each token can only attend to the previous window_size tokens (inlcuding itself)\n+        # in loacal attention each token can only attend to the previous window_size tokens (including itself)\n         # here window_size is 4, so a token at index 5 can only attend to indcies [2, 3, 4, 5]\n         # and the attn_probs should be 0 for token [0, 1]\n         self.assertTrue(torch.all(attn_probs[:, :, 5, 2:6] != 0))"
        },
        {
            "sha": "874c62f0c8abad2938daf3989ceaa7dcfe0e3985",
            "filename": "tests/models/gpt_neox/test_modeling_gpt_neox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -381,7 +381,9 @@ def test_model_rope_scaling(self):\n         long_input_length = int(config.max_position_embeddings * 1.5)\n \n         # Inputs\n-        x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n+        x = torch.randn(\n+            1, dtype=torch.float32, device=torch_device\n+        )  # used exclusively to get the dtype and the device\n         position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n         position_ids_short = position_ids_short.unsqueeze(0)\n         position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)"
        },
        {
            "sha": "826cda3f67c8acb1f0aab0f2d162fbc8d9bc50d5",
            "filename": "tests/models/granite/test_modeling_granite.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -356,7 +356,9 @@ def test_model_rope_scaling(self):\n         long_input_length = int(config.max_position_embeddings * 1.5)\n \n         # Inputs\n-        x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n+        x = torch.randn(\n+            1, dtype=torch.float32, device=torch_device\n+        )  # used exclusively to get the dtype and the device\n         position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n         position_ids_short = position_ids_short.unsqueeze(0)\n         position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)"
        },
        {
            "sha": "cd2470827b90d240b66d3e4986bf83dae55d0634",
            "filename": "tests/models/granitemoe/test_modeling_granitemoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -355,7 +355,9 @@ def test_model_rope_scaling(self):\n         long_input_length = int(config.max_position_embeddings * 1.5)\n \n         # Inputs\n-        x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n+        x = torch.randn(\n+            1, dtype=torch.float32, device=torch_device\n+        )  # used exclusively to get the dtype and the device\n         position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n         position_ids_short = position_ids_short.unsqueeze(0)\n         position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)"
        },
        {
            "sha": "4e836f827e9b31f85d77f18eedc3d44b121804bd",
            "filename": "tests/models/groupvit/test_modeling_groupvit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -583,7 +583,7 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    # override as the `logit_scale` parameter initilization is different for GROUPVIT\n+    # override as the `logit_scale` parameter initialization is different for GROUPVIT\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -592,7 +592,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    # check if `logit_scale` is initilized as per the original implementation\n+                    # check if `logit_scale` is initialized as per the original implementation\n                     if name == \"logit_scale\":\n                         self.assertAlmostEqual(\n                             param.data.item(),"
        },
        {
            "sha": "ee62ed011bcd0d41995f33c2e708ac8a55a0316a",
            "filename": "tests/models/layoutlmv2/test_modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -497,7 +497,7 @@ def recursive_check(batched_object, single_row_object, model_name, key):\n                     single_batch_shape = value.shape[0] // batch_size\n                     single_row_input[key] = value[:single_batch_shape]\n                 elif hasattr(value, \"tensor\"):\n-                    # layoutlmv2uses ImageList intead of pixel values (needs for torchscript)\n+                    # layoutlmv2uses ImageList instead of pixel values (needs for torchscript)\n                     single_row_input[key] = value.tensor[:single_batch_shape]\n \n             with torch.no_grad():"
        },
        {
            "sha": "319187f11346c118dac68ff6f53b822dced1fe54",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -426,7 +426,9 @@ def test_model_rope_scaling(self):\n         long_input_length = int(config.max_position_embeddings * 1.5)\n \n         # Inputs\n-        x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n+        x = torch.randn(\n+            1, dtype=torch.float32, device=torch_device\n+        )  # used exclusively to get the dtype and the device\n         position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n         position_ids_short = position_ids_short.unsqueeze(0)\n         position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)"
        },
        {
            "sha": "8d517c644d1debb578be7883cbae1402fd00227d",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -298,7 +298,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n-            _ = model(**input_dict)  # successfull forward with no modifications\n+            _ = model(**input_dict)  # successful forward with no modifications\n \n             # remove one image but leave the image token in text\n             input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]"
        },
        {
            "sha": "ab3a34ab3b12a24e68833fdf525304fa25e11fcb",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -315,7 +315,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n-            _ = model(**input_dict)  # successfull forward with no modifications\n+            _ = model(**input_dict)  # successful forward with no modifications\n \n             # remove one image but leave the image token in text\n             input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]"
        },
        {
            "sha": "9cfcbfb778f34d60823de27e41145030b95b4637",
            "filename": "tests/models/m2m_100/test_modeling_m2m_100.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -433,7 +433,7 @@ def test_seq_to_seq_generation(self):\n     @slow\n     def test_flash_attn_2_seq_to_seq_generation(self):\n         \"\"\"\n-        Overwritting the common test as the test is flaky on tiny models\n+        Overwriting the common test as the test is flaky on tiny models\n         \"\"\"\n         model = M2M100ForConditionalGeneration.from_pretrained(\n             \"facebook/m2m100_418M\", attn_implementation=\"flash_attention_2\""
        },
        {
            "sha": "849eb90a75a04e3b5891f7f0f5847ab1204160b8",
            "filename": "tests/models/mgp_str/test_modeling_mgp_str.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmgp_str%2Ftest_modeling_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmgp_str%2Ftest_modeling_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmgp_str%2Ftest_modeling_mgp_str.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -202,7 +202,7 @@ def check_hidden_states_output(inputs_dict, config, model_class):\n \n             check_hidden_states_output(inputs_dict, config, model_class)\n \n-    # override as the `logit_scale` parameter initilization is different for MgpstrModel\n+    # override as the `logit_scale` parameter initialization is different for MgpstrModel\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "f42dfb266149b682883f5d4d0bff6ca4f0b9d40c",
            "filename": "tests/models/mgp_str/test_processor_mgp_str.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmgp_str%2Ftest_processor_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmgp_str%2Ftest_processor_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmgp_str%2Ftest_processor_mgp_str.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -70,7 +70,7 @@ def setUp(self):\n         with open(self.image_processor_file, \"w\", encoding=\"utf-8\") as fp:\n             json.dump(image_processor_map, fp)\n \n-    # We copy here rather than use the ProcessorTesterMixin as this processor has a `char_tokenizer` instad of a\n+    # We copy here rather than use the ProcessorTesterMixin as this processor has a `char_tokenizer` instead of a\n     # tokenizer attribute, which means all the tests would need to be overridden.\n     @require_vision\n     def prepare_image_inputs(self):"
        },
        {
            "sha": "f660171774f0dfd98d8e7a03bbe23786ada11493",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -450,7 +450,7 @@ def test_load_balancing_loss(self):\n         padded_result = model(padded_input_ids, attention_mask=padded_attention_mask)\n         torch.testing.assert_close(result.aux_loss.cpu(), padded_result.aux_loss.cpu(), rtol=1e-4, atol=1e-4)\n \n-        # We make sure that the loss of includding padding tokens != the loss without padding tokens\n+        # We make sure that the loss of including padding tokens != the loss without padding tokens\n         # if attention_mask=None --> we don't exclude padding tokens\n         include_padding_result = model(padded_input_ids, attention_mask=None)\n \n@@ -480,7 +480,7 @@ def test_small_model_logits(self):\n             torch_device\n         )\n         # TODO: might need to tweak it in case the logits do not match on our daily runners\n-        # these logits have been obtained with the original megablocks impelmentation.\n+        # these logits have been obtained with the original megablocks implementation.\n         # Key 9 for MI300, Key 8 for A100/A10, and Key 7 for T4.\n         #\n         # Note: Key 9 is currently set for MI300, but may need potential future adjustments for H100s,"
        },
        {
            "sha": "023dd8ea2be3b96f330f5bff0e90e30e9a40062e",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -414,7 +414,7 @@ def test_past_key_values_format(self):\n             embed_dim = getattr(text_config, \"d_model\", text_config.hidden_size)\n             per_head_embed_dim = embed_dim // num_attention_heads\n \n-            # some models have diffent num-head for query vs key/value so we need to assign correct value\n+            # some models have different num-head for query vs key/value so we need to assign correct value\n             # BUT only after `per_head_embed_dim` is set\n             num_attention_heads = (\n                 text_config.num_key_value_heads"
        },
        {
            "sha": "61967ec3414fb3c6637eca2d85b09294d729e5a5",
            "filename": "tests/models/mobilevit/test_modeling_tf_mobilevit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_tf_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_tf_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_tf_mobilevit.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -284,7 +284,7 @@ def test_keras_fit(self):\n                     super().test_keras_fit()\n \n     # The default test_loss_computation() uses -100 as a proxy ignore_index\n-    # to test masked losses. Overridding to avoid -100 since semantic segmentation\n+    # to test masked losses. Overriding to avoid -100 since semantic segmentation\n     #  models use `semantic_loss_ignore_index` from the config.\n     def test_loss_computation(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "4229410ed3834c64c34c611ec86e9cd2c40b0926",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -581,7 +581,7 @@ def prepare_config_and_inputs_for_generate(self, batch_size=2):\n         return config, filtered_inputs_dict\n \n     def _check_generate_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n-        # Overwrite because the generate method actually alway uses `inputs_embeds` so `use_cache` is always `True`\n+        # Overwrite because the generate method actually always uses `inputs_embeds` so `use_cache` is always `True`\n         super()._check_generate_outputs(\n             output, config, use_cache=True, num_return_sequences=num_return_sequences, num_beams=num_beams\n         )\n@@ -618,13 +618,13 @@ def test_contrastive_generate_low_memory(self):\n         pass\n \n     @unittest.skip(\n-        \"Moshi either needs deafult generation config or fix for fullgraph compile because it hardcodes SlidingWindowCache in custom generation loop.\"\n+        \"Moshi either needs default generation config or fix for fullgraph compile because it hardcodes SlidingWindowCache in custom generation loop.\"\n     )\n     def test_greedy_generate_dict_outputs_use_cache(self):\n         pass\n \n     @unittest.skip(\n-        \"Moshi either needs deafult generation config or fix for fullgraph compile because it hardcodes SlidingWindowCache in custom generation loop.\"\n+        \"Moshi either needs default generation config or fix for fullgraph compile because it hardcodes SlidingWindowCache in custom generation loop.\"\n     )\n     def test_beam_search_generate_dict_outputs_use_cache(self):\n         pass\n@@ -849,7 +849,7 @@ def test_generate_from_unconditional(self):\n                 **model.get_unconditional_inputs(num_samples=4), max_new_tokens=5, concat_unconditional_inputs=False\n             )\n \n-            # check same results from uncondtional or no inputs\n+            # check same results from unconditional or no inputs\n             outputs_from_unconditional = model.generate(\n                 **model.get_unconditional_inputs(num_samples=1), max_new_tokens=5, concat_unconditional_inputs=False\n             )"
        },
        {
            "sha": "0aaa6295ea668c6bbbbec676dba946f6089275cc",
            "filename": "tests/models/moshi/test_tokenization_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmoshi%2Ftest_tokenization_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmoshi%2Ftest_tokenization_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_tokenization_moshi.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -289,7 +289,7 @@ def test_training_new_tokenizer_with_special_tokens_change(self):\n                 self.assertTrue(\n                     find,\n                     f\"'{special_token.__repr__()}' should appear as an `AddedToken` in the all_special_tokens_extended = \"\n-                    f\"{[k for k in new_tokenizer.all_special_tokens_extended if str(k)==new_special_token_str]} but it is missing\"\n+                    f\"{[k for k in new_tokenizer.all_special_tokens_extended if str(k) == new_special_token_str]} but it is missing\"\n                     \", this means that the new tokenizers did not keep the `rstrip`, `lstrip`, `normalized` etc attributes.\",\n                 )\n             elif special_token not in special_tokens_map:"
        },
        {
            "sha": "c14292b093f2866dc2a7db7fdd00b10374c85144",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -971,7 +971,7 @@ def test_greedy_generate_stereo_outputs(self):\n         self.model_tester.audio_channels = original_audio_channels\n \n     @unittest.skip(\n-        reason=\"MusicgenModel is actually not the base of MusicgenForCausalLM as the latter is a composit model\"\n+        reason=\"MusicgenModel is actually not the base of MusicgenForCausalLM as the latter is a composite model\"\n     )\n     def test_save_load_fast_init_from_base(self):\n         pass"
        },
        {
            "sha": "0fcfa254afa29872b5646c4c1f5be17d0e60ca38",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -961,7 +961,7 @@ def test_greedy_generate_stereo_outputs(self):\n         self.model_tester.audio_channels = original_audio_channels\n \n     @unittest.skip(\n-        reason=\"MusicgenMelodyModel is actually not the base of MusicgenMelodyForCausalLM as the latter is a composit model\"\n+        reason=\"MusicgenMelodyModel is actually not the base of MusicgenMelodyForCausalLM as the latter is a composite model\"\n     )\n     def test_save_load_fast_init_from_base(self):\n         pass"
        },
        {
            "sha": "c544154b2364af391e8731fc378c2bb137c1d9c6",
            "filename": "tests/models/oneformer/test_modeling_oneformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -173,7 +173,7 @@ def create_and_check_oneformer_model(\n \n             output = model(pixel_values=pixel_values, task_inputs=task_inputs, pixel_mask=pixel_mask)\n             output = model(pixel_values, task_inputs=task_inputs, output_hidden_states=True)\n-        # the correct shape of output.transformer_decoder_hidden_states ensure the correcteness of the\n+        # the correct shape of output.transformer_decoder_hidden_states ensure the correctness of the\n         # encoder and pixel decoder\n         self.parent.assertEqual(\n             output.transformer_decoder_object_queries.shape,"
        },
        {
            "sha": "dff1cbe8c00c7587513af515f336bf8f4f555288",
            "filename": "tests/models/owlv2/test_modeling_owlv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -475,7 +475,7 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    # override as the `logit_scale` parameter initilization is different for OWLV2\n+    # override as the `logit_scale` parameter initialization is different for OWLV2\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -484,7 +484,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    # check if `logit_scale` is initilized as per the original implementation\n+                    # check if `logit_scale` is initialized as per the original implementation\n                     if name == \"logit_scale\":\n                         self.assertAlmostEqual(\n                             param.data.item(),"
        },
        {
            "sha": "1ad85cb379193247cf64d5c63b9b79f4645f149d",
            "filename": "tests/models/owlvit/test_modeling_owlvit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -470,7 +470,7 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    # override as the `logit_scale` parameter initilization is different for OWLVIT\n+    # override as the `logit_scale` parameter initialization is different for OWLVIT\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -479,7 +479,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    # check if `logit_scale` is initilized as per the original implementation\n+                    # check if `logit_scale` is initialized as per the original implementation\n                     if name == \"logit_scale\":\n                         self.assertAlmostEqual(\n                             param.data.item(),"
        },
        {
            "sha": "4867e38acb6816a943c5f7d5041483cc5ed19a84",
            "filename": "tests/models/persimmon/test_modeling_persimmon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -424,7 +424,9 @@ def test_model_rope_scaling(self):\n         long_input_length = int(config.max_position_embeddings * 1.5)\n \n         # Inputs\n-        x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n+        x = torch.randn(\n+            1, dtype=torch.float32, device=torch_device\n+        )  # used exclusively to get the dtype and the device\n         position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n         position_ids_short = position_ids_short.unsqueeze(0)\n         position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)"
        },
        {
            "sha": "5fe681d995931899ca9c188606bc4704ea8b4864",
            "filename": "tests/models/phi/test_modeling_phi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -403,7 +403,9 @@ def test_model_rope_scaling(self):\n         long_input_length = int(config.max_position_embeddings * 1.5)\n \n         # Inputs\n-        x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n+        x = torch.randn(\n+            1, dtype=torch.float32, device=torch_device\n+        )  # used exclusively to get the dtype and the device\n         position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n         position_ids_short = position_ids_short.unsqueeze(0)\n         position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)"
        },
        {
            "sha": "3e4f02626d5fc1ad56cc6a96a56166daeee208cb",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -263,7 +263,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n-            _ = model(**input_dict)  # successfull forward with no modifications\n+            _ = model(**input_dict)  # successful forward with no modifications\n \n             # remove one image but leave the image token in text\n             patch_size = config.vision_config.patch_size"
        },
        {
            "sha": "b4e493dd2c1d1631156579addf7d7f240eb8efab",
            "filename": "tests/models/reformer/test_modeling_reformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -696,7 +696,7 @@ def test_left_padding_compatibility(self):\n         pass\n \n     def prepare_config_and_inputs_for_generate(self, *args, **kwargs):\n-        # override because overwise we hit max possible seq length for model (4*8=32)\n+        # override because otherwise we hit max possible seq length for model (4*8=32)\n         # decreasing the seq_length in tester causes errors for \"training_tests\", those need exactly max seq length\n         # NOTE: seq_length has to be multiple of 4, otherwise it fails for other tests\n         original_sequence_length = self.model_tester.seq_length\n@@ -887,7 +887,7 @@ def test_left_padding_compatibility(self):\n @require_tokenizers\n class ReformerIntegrationTests(unittest.TestCase):\n     \"\"\"\n-    These integration tests test the current layer activations and gradients againts the output of the Hugging Face Reformer model at time of integration: 29/06/2020. During integration, the model was tested against the output of the official Trax ReformerLM model for various cases (\"lsh\" only, \"lsh\" only, masked / non-masked, different chunk length, ....). In order to recover the original trax integration tests, one should use patrickvonplaten's fork of trax and the code that lives on the branch `reformer_trax_tests`.\n+    These integration tests test the current layer activations and gradients against the output of the Hugging Face Reformer model at time of integration: 29/06/2020. During integration, the model was tested against the output of the official Trax ReformerLM model for various cases (\"lsh\" only, \"lsh\" only, masked / non-masked, different chunk length, ....). In order to recover the original trax integration tests, one should use patrickvonplaten's fork of trax and the code that lives on the branch `reformer_trax_tests`.\n     \"\"\"\n \n     def _get_basic_config_and_input(self):\n@@ -1246,7 +1246,7 @@ def test_local_lm_model_grad(self):\n         )\n         loss.backward()\n \n-        # check last grads to cover all proable errors\n+        # check last grads to cover all probable errors\n         grad_slice_word = model.reformer.embeddings.word_embeddings.weight.grad[0, :5]\n         expected_grad_slice_word = torch.tensor(\n             [-0.0005, -0.0001, -0.0002, -0.0006, -0.0006],\n@@ -1287,7 +1287,7 @@ def test_lsh_lm_model_grad(self):\n             loss, torch.tensor(5.7854, dtype=torch.float, device=torch_device), rtol=1e-3, atol=1e-3\n         )\n         loss.backward()\n-        # check last grads to cover all proable errors\n+        # check last grads to cover all probable errors\n         grad_slice_word = model.reformer.embeddings.word_embeddings.weight.grad[0, :5]\n         expected_grad_slice_word = torch.tensor(\n             [0.0004, 0.0003, 0.0006, -0.0004, 0.0002],"
        },
        {
            "sha": "e8af79ca7baf84bc8feb1f67abe88d24591de0b9",
            "filename": "tests/models/rt_detr_v2/test_modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -545,10 +545,10 @@ def test_different_timm_backbone(self):\n                     self.model_tester.num_labels,\n                 )\n                 self.assertEqual(outputs.logits.shape, expected_shape)\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.model.backbone.intermediate_channel_sizes), 3)\n             else:\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.backbone.intermediate_channel_sizes), 3)\n \n             self.assertTrue(outputs)\n@@ -577,10 +577,10 @@ def test_hf_backbone(self):\n                     self.model_tester.num_labels,\n                 )\n                 self.assertEqual(outputs.logits.shape, expected_shape)\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.model.backbone.intermediate_channel_sizes), 3)\n             else:\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.backbone.intermediate_channel_sizes), 3)\n \n             self.assertTrue(outputs)"
        },
        {
            "sha": "c30ebcc87fcdfd0f4c8b5af8ad27cc47e6139021",
            "filename": "tests/models/seamless_m4t/test_modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -1017,7 +1017,7 @@ def test_to_eng_text(self):\n         output = model.generate(**self.input_text, num_beams=1, tgt_lang=\"eng\", return_intermediate_token_ids=True)\n \n         self.assertListEqual(expected_text_tokens, output.sequences.squeeze().tolist())\n-        # FOR NOW, only first units correspondance\n+        # FOR NOW, only first units correspondence\n         self.assertListEqual(expected_unit_tokens[:10], output.unit_sequences.squeeze().tolist()[:10])\n \n         self.assertListAlmostEqual(expected_wav_slice, output.waveform.squeeze().tolist()[50:60])"
        },
        {
            "sha": "399a111530d1dd5bd8723a2e160f7ba8b5d523a1",
            "filename": "tests/models/speech_encoder_decoder/test_modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -266,7 +266,10 @@ def check_save_and_load_encoder_decoder_model(\n             out_2 = outputs[0].cpu().numpy()\n             out_2[np.isnan(out_2)] = 0\n \n-            with tempfile.TemporaryDirectory() as encoder_tmp_dirname, tempfile.TemporaryDirectory() as decoder_tmp_dirname:\n+            with (\n+                tempfile.TemporaryDirectory() as encoder_tmp_dirname,\n+                tempfile.TemporaryDirectory() as decoder_tmp_dirname,\n+            ):\n                 enc_dec_model.encoder.save_pretrained(encoder_tmp_dirname)\n                 enc_dec_model.decoder.save_pretrained(decoder_tmp_dirname)\n                 SpeechEncoderDecoderModel.from_encoder_decoder_pretrained("
        },
        {
            "sha": "14af6d5f7275e2179d5c54a885da798c942aaf0d",
            "filename": "tests/models/stablelm/test_modeling_stablelm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -409,7 +409,9 @@ def test_model_rope_scaling(self):\n         long_input_length = int(config.max_position_embeddings * 1.5)\n \n         # Inputs\n-        x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n+        x = torch.randn(\n+            1, dtype=torch.float32, device=torch_device\n+        )  # used exclusively to get the dtype and the device\n         position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n         position_ids_short = position_ids_short.unsqueeze(0)\n         position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)"
        },
        {
            "sha": "703b9973cb701d6dd91a4d6451f00d3a2f0a5a09",
            "filename": "tests/models/t5/test_modeling_flax_t5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ft5%2Ftest_modeling_flax_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ft5%2Ftest_modeling_flax_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_flax_t5.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -574,7 +574,7 @@ class FlaxT5ModelIntegrationTests(unittest.TestCase):\n     @slow\n     def test_small_integration_test(self):\n         \"\"\"\n-        For comparision run:\n+        For comparison run:\n         >>> import t5  # pip install t5==0.7.1\n         >>> from t5.data.sentencepiece_vocabulary import SentencePieceVocabulary\n \n@@ -604,7 +604,7 @@ def test_small_integration_test(self):\n     @slow\n     def test_small_v1_1_integration_test(self):\n         \"\"\"\n-        For comparision run:\n+        For comparison run:\n         >>> import t5  # pip install t5==0.7.1\n         >>> from t5.data.sentencepiece_vocabulary import SentencePieceVocabulary\n \n@@ -634,7 +634,7 @@ def test_small_v1_1_integration_test(self):\n     @slow\n     def test_small_byt5_integration_test(self):\n         \"\"\"\n-        For comparision run:\n+        For comparison run:\n         >>> import t5  # pip install t5==0.9.1\n \n         >>> path_to_byt5_small_checkpoint = '<fill_in>'"
        },
        {
            "sha": "fb807edc0bc71c60c5fb54e374da9309b69af9f7",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -1208,7 +1208,7 @@ def test_small_generation(self):\n     @slow\n     def test_small_integration_test(self):\n         \"\"\"\n-        For comparision run:\n+        For comparison run:\n         >>> import t5  # pip install t5==0.7.1\n         >>> from t5.data.sentencepiece_vocabulary import SentencePieceVocabulary\n \n@@ -1234,7 +1234,7 @@ def test_small_integration_test(self):\n     @slow\n     def test_small_v1_1_integration_test(self):\n         \"\"\"\n-        For comparision run:\n+        For comparison run:\n         >>> import t5  # pip install t5==0.7.1\n         >>> from t5.data.sentencepiece_vocabulary import SentencePieceVocabulary\n \n@@ -1260,7 +1260,7 @@ def test_small_v1_1_integration_test(self):\n     @slow\n     def test_small_byt5_integration_test(self):\n         \"\"\"\n-        For comparision run:\n+        For comparison run:\n         >>> import t5  # pip install t5==0.9.1\n \n         >>> path_to_byt5_small_checkpoint = '<fill_in>'"
        },
        {
            "sha": "7e6367582ead4b403be9954bb801724f84472e85",
            "filename": "tests/models/t5/test_modeling_tf_t5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ft5%2Ftest_modeling_tf_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ft5%2Ftest_modeling_tf_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_tf_t5.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -618,7 +618,7 @@ def model(self):\n     @slow\n     def test_small_integration_test(self):\n         \"\"\"\n-        For comparision run:\n+        For comparison run:\n         >>> import t5  # pip install t5==0.7.1\n         >>> from t5.data.sentencepiece_vocabulary import SentencePieceVocabulary\n \n@@ -644,7 +644,7 @@ def test_small_integration_test(self):\n     @slow\n     def test_small_v1_1_integration_test(self):\n         \"\"\"\n-        For comparision run:\n+        For comparison run:\n         >>> import t5  # pip install t5==0.7.1\n         >>> from t5.data.sentencepiece_vocabulary import SentencePieceVocabulary\n \n@@ -670,7 +670,7 @@ def test_small_v1_1_integration_test(self):\n     @slow\n     def test_small_byt5_integration_test(self):\n         \"\"\"\n-        For comparision run:\n+        For comparison run:\n         >>> import t5  # pip install t5==0.9.1\n \n         >>> path_to_byt5_small_checkpoint = '<fill_in>'"
        },
        {
            "sha": "a7ad5320af2f252ac0e022bd060e07f92c78417c",
            "filename": "tests/models/t5/test_tokenization_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ft5%2Ftest_tokenization_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ft5%2Ftest_tokenization_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_tokenization_t5.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -498,7 +498,7 @@ def setUpClass(cls):\n         tokenizer.add_special_tokens(\n             {\"additional_special_tokens\": [AddedToken(\"<extra_id_0>\", rstrip=False, lstrip=False)]}\n         )\n-        # TODO ArthurZ the above is necessary as addedTokens / intialization sucks. Trie is not correctly created\n+        # TODO ArthurZ the above is necessary as addedTokens / initialization sucks. Trie is not correctly created\n         # So the extra ids are split....\n         cls.tokenizer = tokenizer\n "
        },
        {
            "sha": "9995aae7d4b4616d51d9136546ed245c98797446",
            "filename": "tests/models/table_transformer/test_modeling_table_transformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -477,10 +477,10 @@ def test_different_timm_backbone(self):\n                     self.model_tester.num_labels + 1,\n                 )\n                 self.assertEqual(outputs.logits.shape, expected_shape)\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n             else:\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n \n             self.assertTrue(outputs)\n@@ -509,10 +509,10 @@ def test_hf_backbone(self):\n                     self.model_tester.num_labels + 1,\n                 )\n                 self.assertEqual(outputs.logits.shape, expected_shape)\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n             else:\n-                # Confirm out_indices was propogated to backbone\n+                # Confirm out_indices was propagated to backbone\n                 self.assertEqual(len(model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n \n             self.assertTrue(outputs)"
        },
        {
            "sha": "3ac01c53d52bf79ab228881ae0d02e67f203d5ab",
            "filename": "tests/models/tvp/test_modeling_tvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ftvp%2Ftest_modeling_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Ftvp%2Ftest_modeling_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftvp%2Ftest_modeling_tvp.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -194,7 +194,7 @@ def test_inputs_embeds(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    # override as the `logit_scale` parameter initilization is different for TVP\n+    # override as the `logit_scale` parameter initialization is different for TVP\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -219,7 +219,7 @@ def _validate_backbone_init():\n                 model.to(torch_device)\n                 model.eval()\n \n-                # Confirm out_indices propogated to backbone\n+                # Confirm out_indices propagated to backbone\n                 if model.__class__.__name__ == \"TvpModel\":\n                     self.assertEqual(len(model.vision_model.backbone.out_indices), 2)\n                 elif model.__class__.__name__ == \"TvpForVideoGrounding\":"
        },
        {
            "sha": "003c63a3e640b7ee2ef3163b25491786fd88161a",
            "filename": "tests/models/univnet/test_modeling_univnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -227,7 +227,7 @@ def get_inputs(self, device, num_samples: int = 3, noise_length: int = 10, seed:\n             noise_sequence_shape = (64, noise_length)\n         else:\n             noise_sequence_shape = (num_samples, 64, noise_length)\n-        # Explicity generate noise_sequence on CPU for consistency.\n+        # Explicitly generate noise_sequence on CPU for consistency.\n         noise_sequence = torch.randn(noise_sequence_shape, generator=generator, dtype=torch.float32, device=\"cpu\")\n         # Put noise_sequence on the desired device.\n         noise_sequence = noise_sequence.to(device)"
        },
        {
            "sha": "317048550e5cdc19f2a8210f6e2336c3101077b6",
            "filename": "tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -216,7 +216,10 @@ def check_save_and_load_encoder_decoder_model(\n             out_2 = outputs[0].cpu().numpy()\n             out_2[np.isnan(out_2)] = 0\n \n-            with tempfile.TemporaryDirectory() as encoder_tmp_dirname, tempfile.TemporaryDirectory() as decoder_tmp_dirname:\n+            with (\n+                tempfile.TemporaryDirectory() as encoder_tmp_dirname,\n+                tempfile.TemporaryDirectory() as decoder_tmp_dirname,\n+            ):\n                 enc_dec_model.encoder.save_pretrained(encoder_tmp_dirname)\n                 enc_dec_model.decoder.save_pretrained(decoder_tmp_dirname)\n                 VisionEncoderDecoderModel.from_encoder_decoder_pretrained("
        },
        {
            "sha": "b75cfc886c0c92b6a2f120d92baa833fce1ed5e9",
            "filename": "tests/models/vitmatte/test_modeling_vitmatte.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -244,7 +244,7 @@ def _validate_backbone_init():\n                 model.eval()\n \n                 if model.__class__.__name__ == \"VitMatteForImageMatting\":\n-                    # Confirm out_indices propogated to backbone\n+                    # Confirm out_indices propagated to backbone\n                     self.assertEqual(len(model.backbone.out_indices), 2)\n \n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "aa55557691b5e76789e9fea22c42359b9be84570",
            "filename": "tests/models/wav2vec2/test_modeling_flax_wav2vec2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_flax_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_flax_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_flax_wav2vec2.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -616,9 +616,10 @@ def test_wav2vec2_with_lm_pool(self):\n         self.assertEqual(transcription[0], \"bien y qu regalo vas a abrir primero\")\n \n         # user-managed pool + num_processes should trigger a warning\n-        with CaptureLogger(processing_wav2vec2_with_lm.logger) as cl, multiprocessing.get_context(\"fork\").Pool(\n-            2\n-        ) as pool:\n+        with (\n+            CaptureLogger(processing_wav2vec2_with_lm.logger) as cl,\n+            multiprocessing.get_context(\"fork\").Pool(2) as pool,\n+        ):\n             transcription = processor.batch_decode(np.array(logits), pool, num_processes=2).text\n \n         self.assertIn(\"num_process\", cl.out)"
        },
        {
            "sha": "593d627ccf97cc5fccbaa260e17a6613c6a69f94",
            "filename": "tests/models/wav2vec2/test_modeling_tf_wav2vec2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_tf_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_tf_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_tf_wav2vec2.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -712,9 +712,10 @@ def test_wav2vec2_with_lm_pool(self):\n         self.assertEqual(transcription[0], \"el libro ha sido escrito por cervantes\")\n \n         # user-managed pool + num_processes should trigger a warning\n-        with CaptureLogger(processing_wav2vec2_with_lm.logger) as cl, multiprocessing.get_context(\"fork\").Pool(\n-            2\n-        ) as pool:\n+        with (\n+            CaptureLogger(processing_wav2vec2_with_lm.logger) as cl,\n+            multiprocessing.get_context(\"fork\").Pool(2) as pool,\n+        ):\n             transcription = processor.batch_decode(logits.numpy(), pool, num_processes=2).text\n \n         self.assertIn(\"num_process\", cl.out)"
        },
        {
            "sha": "df5fd7c452bfd05308ca1928fc3cf25f38748af2",
            "filename": "tests/models/wav2vec2/test_modeling_wav2vec2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -1885,9 +1885,10 @@ def test_wav2vec2_with_lm_pool(self):\n         self.assertEqual(transcription[0], \"habitan aguas poco profundas y rocosas\")\n \n         # user-managed pool + num_processes should trigger a warning\n-        with CaptureLogger(processing_wav2vec2_with_lm.logger) as cl, multiprocessing.get_context(\"fork\").Pool(\n-            2\n-        ) as pool:\n+        with (\n+            CaptureLogger(processing_wav2vec2_with_lm.logger) as cl,\n+            multiprocessing.get_context(\"fork\").Pool(2) as pool,\n+        ):\n             transcription = processor.batch_decode(logits.cpu().numpy(), pool, num_processes=2).text\n \n         self.assertIn(\"num_process\", cl.out)"
        },
        {
            "sha": "ac402d2ff9cac80cbcbfdde2495079b09a7383e6",
            "filename": "tests/models/x_clip/test_modeling_x_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -588,7 +588,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    # check if `logit_scale` is initilized as per the original implementation\n+                    # check if `logit_scale` is initialized as per the original implementation\n                     if name == \"logit_scale\":\n                         self.assertAlmostEqual(\n                             param.data.item(),"
        },
        {
            "sha": "56094003279e8cb98b1cfcf1d9873543d83e29ae",
            "filename": "tests/models/xglm/test_modeling_tf_xglm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fxglm%2Ftest_modeling_tf_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fxglm%2Ftest_modeling_tf_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_tf_xglm.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -208,7 +208,7 @@ def test_batch_generation(self):\n \n         # use different length sentences to test batching\n         sentences = [\n-            \"This is an extremelly long sentence that only exists to test the ability of the model to cope with \"\n+            \"This is an extremely long sentence that only exists to test the ability of the model to cope with \"\n             \"left-padding, such as in batched generation. The output for the sequence below should be the same \"\n             \"regardless of whether left padding is applied or not. When\",\n             \"Hello, my dog is a little\",\n@@ -230,7 +230,7 @@ def test_batch_generation(self):\n         padded_sentence = tokenizer.decode(output_padded[0], skip_special_tokens=True)\n \n         expected_output_sentence = [\n-            \"This is an extremelly long sentence that only exists to test the ability of the model to cope with \"\n+            \"This is an extremely long sentence that only exists to test the ability of the model to cope with \"\n             \"left-padding, such as in batched generation. The output for the sequence below should be the same \"\n             \"regardless of whether left padding is applied or not. When left padding is applied, the sequence will be \"\n             \"a single\","
        },
        {
            "sha": "31c298132e146a5bda125580601080aea43ab766",
            "filename": "tests/models/xglm/test_modeling_xglm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -371,7 +371,7 @@ def test_batch_generation(self):\n \n         # use different length sentences to test batching\n         sentences = [\n-            \"This is an extremelly long sentence that only exists to test the ability of the model to cope with \"\n+            \"This is an extremely long sentence that only exists to test the ability of the model to cope with \"\n             \"left-padding, such as in batched generation. The output for the sequence below should be the same \"\n             \"regardless of whether left padding is applied or not. When\",\n             \"Hello, my dog is a little\",\n@@ -395,7 +395,7 @@ def test_batch_generation(self):\n         padded_sentence = tokenizer.decode(output_padded[0], skip_special_tokens=True)\n \n         expected_output_sentence = [\n-            \"This is an extremelly long sentence that only exists to test the ability of the model to cope with \"\n+            \"This is an extremely long sentence that only exists to test the ability of the model to cope with \"\n             \"left-padding, such as in batched generation. The output for the sequence below should be the same \"\n             \"regardless of whether left padding is applied or not. When left padding is applied, the sequence will be \"\n             \"a single\","
        },
        {
            "sha": "474285841b0d360a2895e6110dd40fc23542f0cf",
            "filename": "tests/sagemaker/scripts/pytorch/run_ddp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_ddp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_ddp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_ddp.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -36,12 +36,12 @@ def main():\n                 --master_addr={hosts[0]}  \\\n                 --master_port={port} \\\n                 ./run_glue.py \\\n-                {\"\".join([f\" --{parameter} {value}\" for parameter,value in args.__dict__.items()])}\"\"\"\n+                {\"\".join([f\" --{parameter} {value}\" for parameter, value in args.__dict__.items()])}\"\"\"\n     else:\n         cmd = f\"\"\"python -m torch.distributed.launch \\\n             --nproc_per_node={num_gpus}  \\\n             ./run_glue.py \\\n-            {\"\".join([f\" --{parameter} {value}\" for parameter,value in args.__dict__.items()])}\"\"\"\n+            {\"\".join([f\" --{parameter} {value}\" for parameter, value in args.__dict__.items()])}\"\"\"\n     try:\n         subprocess.run(cmd, shell=True)\n     except Exception as e:"
        },
        {
            "sha": "eba94a45c9271e865cb90eea42e8630e66c96c24",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -2065,21 +2065,21 @@ def test_encode_decode_fast_slow_all_tokens(self):\n \n                 for chunk in range(0, len(input_full_vocab_string) - 1024, 1024):\n                     string_to_check = input_full_vocab_string[chunk : chunk + 1024]\n-                    with self.subTest(f\"{(chunk/len(input_full_vocab_string))*100}%\"):\n+                    with self.subTest(f\"{(chunk / len(input_full_vocab_string)) * 100}%\"):\n                         slow_encode = slow_tokenizer.encode(string_to_check)\n                         fast_encode = rust_tokenizer.encode(string_to_check)\n                         self.assertEqual(\n                             slow_encode,\n                             fast_encode,\n                             \"Hint: the following tokenization diff were obtained for slow vs fast:\\n \"\n-                            f\"elements in slow: {set(slow_tokenizer.tokenize(string_to_check))-set(rust_tokenizer.tokenize(string_to_check))} \\nvs\\n \"\n-                            f\"elements in fast: {set(rust_tokenizer.tokenize(string_to_check))-set(slow_tokenizer.tokenize(string_to_check))} \\n\"\n+                            f\"elements in slow: {set(slow_tokenizer.tokenize(string_to_check)) - set(rust_tokenizer.tokenize(string_to_check))} \\nvs\\n \"\n+                            f\"elements in fast: {set(rust_tokenizer.tokenize(string_to_check)) - set(slow_tokenizer.tokenize(string_to_check))} \\n\"\n                             f\"string used     : {string_to_check}\",\n                         )\n                 print(f\"Length of the input ids that is tested: {len(input_full_vocab_ids)}\")\n                 for chunk in range(0, len(input_full_vocab_ids) - 100, 100):\n                     ids_to_decode = input_full_vocab_ids[chunk : chunk + 100]\n-                    with self.subTest(f\"{(chunk/len(input_full_vocab_string))*100}%\"):\n+                    with self.subTest(f\"{(chunk / len(input_full_vocab_string)) * 100}%\"):\n                         self.assertEqual(\n                             slow_tokenizer.decode(\n                                 ids_to_decode,\n@@ -4423,7 +4423,7 @@ def test_training_new_tokenizer_with_special_tokens_change(self):\n                 self.assertTrue(\n                     find,\n                     f\"'{special_token.__repr__()}' should appear as an `AddedToken` in the all_special_tokens_extended = \"\n-                    f\"{[k for k in new_tokenizer.all_special_tokens_extended if str(k)==new_special_token_str]} but it is missing\"\n+                    f\"{[k for k in new_tokenizer.all_special_tokens_extended if str(k) == new_special_token_str]} but it is missing\"\n                     \", this means that the new tokenizers did not keep the `rstrip`, `lstrip`, `normalized` etc attributes.\",\n                 )\n             elif special_token not in special_tokens_map:"
        },
        {
            "sha": "6ea5d785231cb822966c4f2763f20b5b3c554859",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26c83490d2fef4b6622bd69c80beae14f2626cbf/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=26c83490d2fef4b6622bd69c80beae14f2626cbf",
            "patch": "@@ -3200,7 +3200,7 @@ def test_can_resume_training_lm(self):\n \n             # Checkpoint at intermediate step\n             enable_full_determinism(0)\n-            checkpoint = os.path.join(tmpdir, f\"checkpoint-{resume_from_step+1}\")\n+            checkpoint = os.path.join(tmpdir, f\"checkpoint-{resume_from_step + 1}\")\n             trainer = get_language_model_trainer(**kwargs)\n             trainer.train(resume_from_checkpoint=checkpoint)\n             model_params = torch.cat([p.cpu().flatten() for p in trainer.model.parameters()])"
        }
    ],
    "stats": {
        "total": 329,
        "additions": 181,
        "deletions": 148
    }
}