{
    "author": "cyyever",
    "message": "Fix white space in documentation (#41157)\n\n* Fix white space\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Revert changes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix autodoc\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "374ded5ea40c9026cf36003c737e07c584dbd63d",
    "files": [
        {
            "sha": "ea62fd545882aac91bb6de4525705707a3c2f8f7",
            "filename": "CONTRIBUTING.md",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/CONTRIBUTING.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/CONTRIBUTING.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/CONTRIBUTING.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -278,13 +278,14 @@ are working on it).<br>\n useful to avoid duplicated work, and to differentiate it from PRs ready to be merged.<br>\n ‚òê Make sure existing tests pass.<br>\n ‚òê If adding a new feature, also add tests for it.<br>\n-   - If you are adding a new model, make sure you use\n+\n+- If you are adding a new model, make sure you use\n      `ModelTester.all_model_classes = (MyModel, MyModelWithLMHead,...)` to trigger the common tests.\n-   - If you are adding new `@slow` tests, make sure they pass using\n+- If you are adding new `@slow` tests, make sure they pass using\n      `RUN_SLOW=1 python -m pytest tests/models/my_new_model/test_my_new_model.py`.\n-   - If you are adding a new tokenizer, write tests and make sure\n+- If you are adding a new tokenizer, write tests and make sure\n      `RUN_SLOW=1 python -m pytest tests/models/{your_model_name}/test_tokenization_{your_model_name}.py` passes.\n-   - CircleCI does not run the slow tests, but GitHub Actions does every night!<br>\n+- CircleCI does not run the slow tests, but GitHub Actions does every night!<br>\n \n ‚òê All public methods must have informative docstrings (see\n [`modeling_bert.py`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py)\n@@ -340,6 +341,7 @@ RUN_SLOW=yes python -m pytest -n auto --dist=loadfile -s -v ./examples/pytorch/t\n ```\n \n Like the slow tests, there are other environment variables available which are not enabled by default during testing:\n+\n - `RUN_CUSTOM_TOKENIZERS`: Enables tests for custom tokenizers.\n \n More environment variables and additional information can be found in the [testing_utils.py](https://github.com/huggingface/transformers/blob/main/src/transformers/testing_utils.py)."
        },
        {
            "sha": "621aa7409da076dba78d01892a02e3f3b21f8208",
            "filename": "docs/source/en/attention_interface.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fattention_interface.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fattention_interface.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fattention_interface.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -193,4 +193,4 @@ def custom_attention_mask(\n \n It mostly works thanks to the `mask_function`, which is a `Callable` in the form of [torch's mask_mod functions](https://pytorch.org/blog/flexattention/), taking 4 indices as input and returning a boolean to indicate if this position should take part in the attention computation.\n \n-If you cannot use the `mask_function` to create your mask for some reason, you can try to work around it by doing something similar to our [torch export workaround](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/executorch.py).\n\\ No newline at end of file\n+If you cannot use the `mask_function` to create your mask for some reason, you can try to work around it by doing something similar to our [torch export workaround](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/executorch.py)."
        },
        {
            "sha": "e6c7534199787c40bd5ad805cf7de680850a8fd0",
            "filename": "docs/source/en/auto_docstring.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fauto_docstring.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fauto_docstring.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fauto_docstring.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -210,9 +210,9 @@ There are some rules for documenting different types of arguments and they're li\n         This can span multiple lines.\n     ```\n \n-    * Include `type` in backticks.\n-    * Add *optional* if the argument is not required or has a default value.\n-    * Add \"defaults to X\" if it has a default value. You don't need to add \"defaults to `None`\" if the default value is `None`.\n+  * Include `type` in backticks.\n+  * Add *optional* if the argument is not required or has a default value.\n+  * Add \"defaults to X\" if it has a default value. You don't need to add \"defaults to `None`\" if the default value is `None`.\n \n     These arguments can also be passed to `@auto_docstring` as a `custom_args` argument. It is used to define the docstring block for new arguments once if they are repeated in multiple places in the modeling file.\n "
        },
        {
            "sha": "6d6718b8cab8240002bb2e1674ede920ddcceee6",
            "filename": "docs/source/en/cache_explanation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcache_explanation.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -162,6 +162,7 @@ generated_ids = model.generate(**inputs, use_cache=True, max_new_tokens=10)\n Before the [`Cache`] class, the cache used to be stored as a tuple of tuples of tensors. This format is dynamic because it grows as text is generated, similar to [`DynamicCache`].\n \n The legacy format is essentially the same data structure but organized differently.\n+\n - It's a tuple of tuples, where each inner tuple contains the key and value tensors for a layer.\n - The tensors have the same shape `[batch_size, num_heads, seq_len, head_dim]`.\n - The format is less flexible and doesn't support features like quantization or offloading."
        },
        {
            "sha": "f52825158272a0e1cd577449afca8bd20a7dc262",
            "filename": "docs/source/en/chat_extras.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fchat_extras.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fchat_extras.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_extras.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -221,4 +221,4 @@ model_input = tokenizer.apply_chat_template(\n     messages,\n     tools = [current_time, multiply]\n )\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "1e83da188a03954ac5707157c27fb178cdfd8830",
            "filename": "docs/source/en/chat_templating.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fchat_templating.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fchat_templating.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -77,9 +77,9 @@ Mistral-7B-Instruct uses `[INST]` and `[/INST]` tokens to indicate the start and\n \n The input to `apply_chat_template` should be structured as a list of dictionaries with `role` and `content` keys. The `role` key specifies the speaker, and the `content` key contains the message. The common roles are:\n \n- - `user` for messages from the user\n- - `assistant` for messages from the model\n- - `system` for directives on how the model should act (usually placed at the beginning of the chat)\n+- `user` for messages from the user\n+- `assistant` for messages from the model\n+- `system` for directives on how the model should act (usually placed at the beginning of the chat)\n \n [`apply_chat_template`] takes this list and returns a formatted sequence. Set `tokenize=True` if you want to tokenize the sequence.\n "
        },
        {
            "sha": "e56155a8e42ce9819a079b036c21dcea72682ee2",
            "filename": "docs/source/en/cursor.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fcursor.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fcursor.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcursor.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -21,6 +21,7 @@ where `port` is the port used by `transformers serve` (`8000` by default). On th\n </h3>\n \n You're now ready to set things up on the app side! In Cursor, while you can't set a new provider, you can change the endpoint for OpenAI requests in the model selection settings. First, navigate to \"Settings\" > \"Cursor Settings\", \"Models\" tab, and expand the \"API Keys\" collapsible. To set your `transformers serve` endpoint, follow this order:\n+\n 1. Unselect ALL models in the list above (e.g. `gpt4`, ...);\n 2. Add and select the model you want to use (e.g. `Qwen/Qwen3-4B`)\n 3. Add some random text to OpenAI API Key. This field won't be used, but it can't be empty;"
        },
        {
            "sha": "d2d49e1f702842aa9627857f223beecc1818758e",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -229,6 +229,7 @@ tokenizer.batch_decode(outputs, skip_special_tokens=True)\n ## Custom generation methods\n \n Custom generation methods enable specialized behavior such as:\n+\n - have the model continue thinking if it is uncertain;\n - roll back generation if the model gets stuck;\n - handle special tokens with custom logic;\n@@ -301,6 +302,7 @@ Updating your Python requirements accordingly will remove this error message.\n ### Creating a custom generation method\n \n To create a new generation method, you need to create a new [**Model**](https://huggingface.co/new) repository and push a few files into it.\n+\n 1. The model you've designed your generation method with.\n 2. `custom_generate/generate.py`, which contains all the logic for your custom generation method.\n 3. `custom_generate/requirements.txt`, used to optionally add new Python requirements and/or lock specific versions to correctly use your method.\n@@ -377,6 +379,7 @@ def generate(model, input_ids, generation_config=None, left_padding=None, **kwar\n ```\n \n Follow the recommended practices below to ensure your custom generation method works as expected.\n+\n - Feel free to reuse the logic for validation and input preparation in the original [`~GenerationMixin.generate`].\n - Pin the `transformers` version in the requirements if you use any private method/attribute in `model`.\n - Consider adding model validation, input validation, or even a separate test file to help users sanity-check your code in their environment.\n@@ -410,6 +413,7 @@ tags:\n ```\n \n Recommended practices:\n+\n - Document input and output differences in [`~GenerationMixin.generate`].\n - Add self-contained examples to enable quick experimentation.\n - Describe soft-requirements such as if the method only works well with a certain family of models.\n@@ -442,6 +446,7 @@ output = model.generate(\n ### Finding custom generation methods\n \n You can find all custom generation methods by [searching for their custom tag.](https://huggingface.co/models?other=custom_generate), `custom_generate`. In addition to the tag, we curate two collections of `custom_generate` methods:\n+\n - [Custom generation methods - Community](https://huggingface.co/collections/transformers-community/custom-generation-methods-community-6888fb1da0efbc592d3a8ab6) -- a collection of powerful methods contributed by the community;\n - [Custom generation methods - Tutorials](https://huggingface.co/collections/transformers-community/custom-generation-methods-tutorials-6823589657a94940ea02cfec) -- a collection of reference implementations for methods that previously were part of `transformers`, as well as tutorials for `custom_generate`.\n "
        },
        {
            "sha": "1c8d8ebc21460fc73e418099d6400ef7dd1e62c2",
            "filename": "docs/source/en/glossary.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fglossary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fglossary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fglossary.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -185,9 +185,9 @@ See the [Fine-tune a pretrained model](https://huggingface.co/docs/transformers/\n \n The model head refers to the last layer of a neural network that accepts the raw hidden states and projects them onto a different dimension. There is a different model head for each task. For example:\n \n-  * [`GPT2ForSequenceClassification`] is a sequence classification head - a linear layer - on top of the base [`GPT2Model`].\n-  * [`ViTForImageClassification`] is an image classification head - a linear layer on top of the final hidden state of the `CLS` token - on top of the base [`ViTModel`].\n-  * [`Wav2Vec2ForCTC`] is a language modeling head with [CTC](#connectionist-temporal-classification-ctc) on top of the base [`Wav2Vec2Model`].\n+* [`GPT2ForSequenceClassification`] is a sequence classification head - a linear layer - on top of the base [`GPT2Model`].\n+* [`ViTForImageClassification`] is an image classification head - a linear layer on top of the final hidden state of the `CLS` token - on top of the base [`ViTModel`].\n+* [`Wav2Vec2ForCTC`] is a language modeling head with [CTC](#connectionist-temporal-classification-ctc) on top of the base [`Wav2Vec2Model`].\n \n ## I\n "
        },
        {
            "sha": "d5ce5bde7901f58807a0c5a2dd4a521b3aac5776",
            "filename": "docs/source/en/how_to_hack_models.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fhow_to_hack_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fhow_to_hack_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fhow_to_hack_models.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -149,4 +149,4 @@ Call [print_trainable_parameters](https://huggingface.co/docs/peft/package_refer\n ```py\n model.print_trainable_parameters()\n \"trainable params: 589,824 || all params: 94,274,096 || trainable%: 0.6256\"\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "b2bded0b895bb02e4af36961bf0a3d2e991e6140",
            "filename": "docs/source/en/internal/model_debugging_utils.md",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -218,9 +218,9 @@ path reference to the associated `.safetensors` file. Each tensor is written to\n the state dictionary. File names are constructed using the `module_path` as a prefix with a few possible postfixes that\n are built recursively.\n \n-*   Module inputs are denoted with the `_inputs` and outputs by `_outputs`.\n-*   `list` and `tuple` instances, such as `args` or function return values, will be postfixed with `_{index}`.\n-*   `dict` instances will be postfixed with `_{key}`.\n+* Module inputs are denoted with the `_inputs` and outputs by `_outputs`.\n+* `list` and `tuple` instances, such as `args` or function return values, will be postfixed with `_{index}`.\n+* `dict` instances will be postfixed with `_{key}`.\n \n ### Comparing between implementations\n \n@@ -255,6 +255,7 @@ how many tests are being skipped and for which models.\n When porting models to transformers, tests fail as they should, and sometimes `test_modeling_common` feels irreconcilable with the peculiarities of our brand new model. But how can we be sure we're not breaking everything by adding a seemingly innocent skip?\n \n This utility:\n+\n - scans all test_modeling_common methods\n - looks for times where a method is skipped\n - returns a summary json you can load as a DataFrame/inspect"
        },
        {
            "sha": "bf7ac5e3145bd09b755a502c346a21b0dac7ea44",
            "filename": "docs/source/en/llm_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -94,6 +94,7 @@ model.generate(**inputs, num_beams=4, do_sample=True)\n ```\n \n [`~GenerationMixin.generate`] can also be extended with external libraries or custom code:\n+\n 1. the `logits_processor` parameter accepts custom [`LogitsProcessor`] instances for manipulating the next token probability distribution;\n 2. the `stopping_criteria` parameters supports custom [`StoppingCriteria`] to stop text generation;\n 3. other custom generation methods can be loaded through the `custom_generate` flag ([docs](generation_strategies.md/#custom-decoding-methods))."
        },
        {
            "sha": "330c68218bf9f08662a0523a91d5dd366b3fa663",
            "filename": "docs/source/en/main_classes/logging.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmain_classes%2Flogging.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmain_classes%2Flogging.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Flogging.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -80,6 +80,7 @@ We use both in the `transformers` library. We leverage and adapt `logging`'s `ca\n management of these warning messages by the verbosity setters above.\n \n What does that mean for developers of the library? We should respect the following heuristics:\n+\n - `warnings` should be favored for developers of the library and libraries dependent on `transformers`\n - `logging` should be used for end-users of the library using it in every-day projects\n "
        },
        {
            "sha": "44a2bceeca68c3aace6d8c542d787024df6dbd07",
            "filename": "docs/source/en/main_classes/processors.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmain_classes%2Fprocessors.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmain_classes%2Fprocessors.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fprocessors.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -17,6 +17,7 @@ rendered properly in your Markdown viewer.\n # Processors\n \n Processors can mean two different things in the Transformers library:\n+\n - the objects that pre-process inputs for multi-modal models such as [Wav2Vec2](../model_doc/wav2vec2) (speech and text)\n   or [CLIP](../model_doc/clip) (text and vision)\n - deprecated objects that were used in older versions of the library to preprocess data for GLUE or SQUAD."
        },
        {
            "sha": "d879669bcab8533cce82e6ce5886918993aa8440",
            "filename": "docs/source/en/main_classes/text_generation.md",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -30,15 +30,15 @@ like token streaming.\n ## GenerationConfig\n \n [[autodoc]] generation.GenerationConfig\n-\t- from_pretrained\n-\t- from_model_config\n-\t- save_pretrained\n-\t- update\n-\t- validate\n-\t- get_generation_mode\n+    - from_pretrained\n+    - from_model_config\n+    - save_pretrained\n+    - update\n+    - validate\n+    - get_generation_mode\n \n ## GenerationMixin\n \n [[autodoc]] GenerationMixin\n-\t- generate\n-\t- compute_transition_scores\n+    - generate\n+    - compute_transition_scores"
        },
        {
            "sha": "275b510ccd5c180f88e2e6dcb26ee56461b285d5",
            "filename": "docs/source/en/model_doc/align.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -148,6 +148,7 @@ for label, score in zip(candidate_labels, probs):\n   ```\n \n ## Resources\n+\n - Refer to the [Kakao Brain‚Äôs Open Source ViT, ALIGN, and the New COYO Text-Image Dataset](https://huggingface.co/blog/vit-align) blog post for more details.\n \n ## AlignConfig"
        },
        {
            "sha": "ebedd73a4a4668445dbe24e89d1d36b12439443c",
            "filename": "docs/source/en/model_doc/arcee.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Farcee.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Farcee.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Farcee.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -102,4 +102,4 @@ print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ## ArceeForTokenClassification\n \n [[autodoc]] ArceeForTokenClassification\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "ee516a935ed45ad9c63a9d088f2f543505c3595d",
            "filename": "docs/source/en/model_doc/beit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -123,6 +123,7 @@ A list of official Hugging Face and community (indicated by üåé) resources to h\n - See also: [Image classification task guide](../tasks/image_classification)\n \n **Semantic segmentation**\n+\n - [Semantic segmentation task guide](../tasks/semantic_segmentation)\n \n If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource."
        },
        {
            "sha": "d57734b069ba63f42e03c86e994040f347f13c11",
            "filename": "docs/source/en/model_doc/bert-generation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -156,4 +156,4 @@ print(tokenizer.decode(outputs[0]))\n ## BertGenerationDecoder\n \n [[autodoc]] BertGenerationDecoder\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "6d34b88a561ece32675eb5ab53c9cabe15eb0720",
            "filename": "docs/source/en/model_doc/bertweet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -88,6 +88,7 @@ echo -e \"Plants create <mask> through a process known as photosynthesis.\" | tran\n </hfoptions>\n \n ## Notes\n+\n - Use the [`AutoTokenizer`] or [`BertweetTokenizer`] because it's preloaded with a custom vocabulary adapted to tweet-specific tokens like hashtags (#), mentions (@), emojis, and common abbreviations. Make sure to also install the [emoji](https://pypi.org/project/emoji/) library.\n - Inputs should be padded on the right (`padding=\"max_length\"`) because BERT uses absolute position embeddings.\n "
        },
        {
            "sha": "c3137725814a79b8a036cd5f096bf3459b3b0f8c",
            "filename": "docs/source/en/model_doc/big_bird.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -87,6 +87,7 @@ print(f\"The predicted token is: {predicted_token}\")\n </hfoptions>\n \n ## Notes\n+\n - Inputs should be padded on the right because BigBird uses absolute position embeddings.\n - BigBird supports `original_full` and `block_sparse` attention. If the input sequence length is less than 1024, it is recommended to use `original_full` since sparse patterns don't offer much benefit for smaller inputs.\n - The current implementation uses window size of 3 blocks and 2 global blocks, only supports the ITC-implementation, and doesn't support `num_random_blocks=0`."
        },
        {
            "sha": "5ed3b8f816ab96707b2f71e667f5550e9d0fb44c",
            "filename": "docs/source/en/model_doc/bit.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbit.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -36,6 +36,7 @@ The original code can be found [here](https://github.com/google-research/big_tra\n ## Usage tips\n \n - BiT models are equivalent to ResNetv2 in terms of architecture, except that: 1) all batch normalization layers are replaced by [group normalization](https://huggingface.co/papers/1803.08494),\n+\n 2) [weight standardization](https://huggingface.co/papers/1903.10520) is used for convolutional layers. The authors show that the combination of both is useful for training with large batch sizes, and has a significant\n impact on transfer learning.\n \n@@ -72,4 +73,4 @@ If you're interested in submitting a resource to be included here, please feel f\n ## BitForImageClassification\n \n [[autodoc]] BitForImageClassification\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "c674f51fc3055ad87308d0691179263d7b23af93",
            "filename": "docs/source/en/model_doc/bitnet.md",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbitnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbitnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbitnet.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -38,22 +38,22 @@ Several versions of the model weights are available on Hugging Face:\n ### Model Details\n \n * **Architecture:** Transformer-based, modified with `BitLinear` layers (BitNet framework).\n-    * Uses Rotary Position Embeddings (RoPE).\n-    * Uses squared ReLU (ReLU¬≤) activation in FFN layers.\n-    * Employs [`subln`](https://proceedings.mlr.press/v202/wang23u.html) normalization.\n-    * No bias terms in linear or normalization layers.\n+  * Uses Rotary Position Embeddings (RoPE).\n+  * Uses squared ReLU (ReLU¬≤) activation in FFN layers.\n+  * Employs [`subln`](https://proceedings.mlr.press/v202/wang23u.html) normalization.\n+  * No bias terms in linear or normalization layers.\n * **Quantization:** Native 1.58-bit weights and 8-bit activations (W1.58A8).\n-    * Weights are quantized to ternary values {-1, 0, +1} using absmean quantization during the forward pass.\n-    * Activations are quantized to 8-bit integers using absmax quantization (per-token).\n-    * **Crucially, the model was *trained from scratch* with this quantization scheme, not post-training quantized.**\n+  * Weights are quantized to ternary values {-1, 0, +1} using absmean quantization during the forward pass.\n+  * Activations are quantized to 8-bit integers using absmax quantization (per-token).\n+  * **Crucially, the model was *trained from scratch* with this quantization scheme, not post-training quantized.**\n * **Parameters:** ~2 Billion\n * **Training Tokens:** 4 Trillion\n-*   **Context Length:** Maximum sequence length of **4096 tokens**.\n-    *   *Recommendation:* For optimal performance on tasks requiring very long contexts (beyond the pre-training length or for specialized long-reasoning tasks), we recommend performing intermediate long-sequence adaptation/training before the final fine-tuning stage.\n+* **Context Length:** Maximum sequence length of **4096 tokens**.\n+  * *Recommendation:* For optimal performance on tasks requiring very long contexts (beyond the pre-training length or for specialized long-reasoning tasks), we recommend performing intermediate long-sequence adaptation/training before the final fine-tuning stage.\n * **Training Stages:**\n-    1.  **Pre-training:** Large-scale training on public text/code and synthetic math data using a two-stage learning rate and weight decay schedule.\n-    2.  **Supervised Fine-tuning (SFT):** Fine-tuned on instruction-following and conversational datasets using sum loss aggregation and specific hyperparameter tuning.\n-    3.  **Direct Preference Optimization (DPO):** Aligned with human preferences using preference pairs.\n+    1. **Pre-training:** Large-scale training on public text/code and synthetic math data using a two-stage learning rate and weight decay schedule.\n+    2. **Supervised Fine-tuning (SFT):** Fine-tuned on instruction-following and conversational datasets using sum loss aggregation and specific hyperparameter tuning.\n+    3. **Direct Preference Optimization (DPO):** Aligned with human preferences using preference pairs.\n * **Tokenizer:** LLaMA 3 Tokenizer (vocab size: 128,256).\n \n ## Usage tips"
        },
        {
            "sha": "5e727050f6eeb4ffd2ac8c74641159ffe7c2327b",
            "filename": "docs/source/en/model_doc/blip.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -128,7 +128,7 @@ Refer to this [notebook](https://github.com/huggingface/notebooks/blob/main/exam\n ## BlipTextLMHeadModel\n \n [[autodoc]] BlipTextLMHeadModel\n-- forward\n+    - forward\n \n ## BlipVisionModel\n "
        },
        {
            "sha": "51e2970c25f6b538c12f3ac15ca560c12bf6d8f2",
            "filename": "docs/source/en/model_doc/bloom.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbloom.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbloom.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbloom.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -43,16 +43,19 @@ A list of official Hugging Face and community (indicated by üåé) resources to h\n - [`BloomForCausalLM`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n \n See also:\n+\n - [Causal language modeling task guide](../tasks/language_modeling)\n - [Text classification task guide](../tasks/sequence_classification)\n - [Token classification task guide](../tasks/token_classification)\n - [Question answering task guide](../tasks/question_answering)\n \n ‚ö°Ô∏è Inference\n+\n - A blog on [Optimization story: Bloom inference](https://huggingface.co/blog/bloom-inference-optimization).\n - A blog on [Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts).\n \n ‚öôÔ∏è Training\n+\n - A blog on [The Technology Behind BLOOM Training](https://huggingface.co/blog/bloom-megatron-deepspeed).\n \n ## BloomConfig"
        },
        {
            "sha": "8affbd73a5702239db1fcb551203f5c50f946d64",
            "filename": "docs/source/en/model_doc/camembert.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -16,10 +16,10 @@ rendered properly in your Markdown viewer.\n *This model was released on 2019-11-10 and added to Hugging Face Transformers on 2020-11-16.*\n \n <div style=\"float: right;\">\n-\t<div class=\"flex flex-wrap space-x-1\">\n-\t\t<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+ <div class=\"flex flex-wrap space-x-1\">\n+  <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n         <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-\t</div>\n+ </div>\n </div>\n \n # CamemBERT"
        },
        {
            "sha": "96b094ccd91b20ef76326718de908bedba1efcd7",
            "filename": "docs/source/en/model_doc/chinese_clip.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -119,4 +119,4 @@ Currently, following scales of pretrained Chinese-CLIP models are available on \n ## ChineseCLIPVisionModel\n \n [[autodoc]] ChineseCLIPVisionModel\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "099fd4fb1bac6aceefe500297d0a52bba83e0947",
            "filename": "docs/source/en/model_doc/clipseg.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -106,4 +106,4 @@ A list of official Hugging Face and community (indicated by üåé) resources to h\n ## CLIPSegForImageSegmentation\n \n [[autodoc]] CLIPSegForImageSegmentation\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "022a178b5cfa557dda28ec965ece0a5ed47e3a51",
            "filename": "docs/source/en/model_doc/cohere.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -122,6 +122,7 @@ visualizer(\"Plants create energy through a process known as\")\n </div>\n \n ## Notes\n+\n - Don't use the dtype parameter in [`~AutoModel.from_pretrained`] if you're using FlashAttention-2 because it only supports fp16 or bf16. You should use [Automatic Mixed Precision](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html), set fp16 or bf16 to True if using [`Trainer`], or use [torch.autocast](https://pytorch.org/docs/stable/amp.html#torch.autocast).\n \n ## CohereConfig"
        },
        {
            "sha": "bb70a369bb7f7d1685c148fd4fe1a53384ba1805",
            "filename": "docs/source/en/model_doc/cpmant.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpmant.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpmant.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpmant.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -49,4 +49,4 @@ This model was contributed by [OpenBMB](https://huggingface.co/openbmb). The ori\n ## CpmAntForCausalLM\n \n [[autodoc]] CpmAntForCausalLM\n-    - all\n\\ No newline at end of file\n+    - all"
        },
        {
            "sha": "66213b42ae7bd86a4006b9c88aea6580dbbad4d8",
            "filename": "docs/source/en/model_doc/data2vec.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -103,6 +103,7 @@ A list of official Hugging Face and community (indicated by üåé) resources to h\n - [`Data2VecVisionForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n \n **Data2VecText documentation resources**\n+\n - [Text classification task guide](../tasks/sequence_classification)\n - [Token classification task guide](../tasks/token_classification)\n - [Question answering task guide](../tasks/question_answering)\n@@ -111,10 +112,12 @@ A list of official Hugging Face and community (indicated by üåé) resources to h\n - [Multiple choice task guide](../tasks/multiple_choice)\n \n **Data2VecAudio documentation resources**\n+\n - [Audio classification task guide](../tasks/audio_classification)\n - [Automatic speech recognition task guide](../tasks/asr)\n \n **Data2VecVision documentation resources**\n+\n - [Image classification](../tasks/image_classification)\n - [Semantic segmentation](../tasks/semantic_segmentation)\n "
        },
        {
            "sha": "08be80c19ff025e579ac1864c505c3c81aa9774f",
            "filename": "docs/source/en/model_doc/deberta.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -92,6 +92,7 @@ echo -e '{\"text\": \"A soccer game with multiple people playing.\", \"text_pair\": \"S\n </hfoptions>\n \n ## Notes\n+\n - DeBERTa uses **relative position embeddings**, so it does not require **right-padding** like BERT.\n - For best results, use DeBERTa on sentence-level or sentence-pair classification tasks like MNLI, RTE, or SST-2.\n - If you're using DeBERTa for token-level tasks like masked language modeling, make sure to load a checkpoint specifically pretrained or fine-tuned for token-level tasks."
        },
        {
            "sha": "fcff8521c07199974c5d8f2fc0cf72d419f3af1d",
            "filename": "docs/source/en/model_doc/deepseek_v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v2.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -47,4 +47,4 @@ The model uses Multi-head Latent Attention (MLA) and DeepSeekMoE architectures f\n ## DeepseekV2ForSequenceClassification\n \n [[autodoc]] DeepseekV2ForSequenceClassification\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "c83dede7808624d6c50634c0d13562f52f6feb24",
            "filename": "docs/source/en/model_doc/deformable_detr.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -16,9 +16,9 @@ rendered properly in your Markdown viewer.\n *This model was released on 2020-10-08 and added to Hugging Face Transformers on 2022-09-14.*\n \n <div style=\"float: right;\">\n-\t<div class=\"flex flex-wrap space-x-1\">\n-\t\t<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-\t</div>\n+ <div class=\"flex flex-wrap space-x-1\">\n+  <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+ </div>\n </div>\n \n # Deformable DETR"
        },
        {
            "sha": "5a7d4d12dcd643eed81841326abf67dc37f63ca7",
            "filename": "docs/source/en/model_doc/deplot.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeplot.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeplot.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeplot.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -68,4 +68,4 @@ scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=1000, nu\n \n DePlot is a model trained using `Pix2Struct` architecture. For API reference, see [`Pix2Struct` documentation](pix2struct).\n \n-</Tip>\n\\ No newline at end of file\n+</Tip>"
        },
        {
            "sha": "44774c961eaab97ce17142b612a4ec86ca3c08d9",
            "filename": "docs/source/en/model_doc/depth_anything.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -86,4 +86,4 @@ Image.fromarray(depth.astype(\"uint8\"))\n ## DepthAnythingForDepthEstimation\n \n [[autodoc]] DepthAnythingForDepthEstimation\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "fbcf2248f6588ca1ac822822b45ea2c48f9a16a9",
            "filename": "docs/source/en/model_doc/depth_anything_v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything_v2.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -110,4 +110,4 @@ If you're interested in submitting a resource to be included here, please feel f\n ## DepthAnythingForDepthEstimation\n \n [[autodoc]] DepthAnythingForDepthEstimation\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "c19703cdccc3979d87650f9043dad486d10accec",
            "filename": "docs/source/en/model_doc/depth_pro.md",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -84,12 +84,13 @@ alt=\"drawing\" width=\"600\"/>\n The `DepthProForDepthEstimation` model uses a `DepthProEncoder`, for encoding the input image and a `FeatureFusionStage` for fusing the output features from encoder.\n \n The `DepthProEncoder` further uses two encoders:\n+\n - `patch_encoder`\n-   - Input image is scaled with multiple ratios, as specified in the `scaled_images_ratios` configuration.\n-   - Each scaled image is split into smaller **patches** of size `patch_size` with overlapping areas determined by `scaled_images_overlap_ratios`.\n-   - These patches are processed by the **`patch_encoder`**\n+  - Input image is scaled with multiple ratios, as specified in the `scaled_images_ratios` configuration.\n+  - Each scaled image is split into smaller **patches** of size `patch_size` with overlapping areas determined by `scaled_images_overlap_ratios`.\n+  - These patches are processed by the **`patch_encoder`**\n - `image_encoder`\n-   - Input image is also rescaled to `patch_size` and processed by the **`image_encoder`**\n+  - Input image is also rescaled to `patch_size` and processed by the **`image_encoder`**\n \n Both these encoders can be configured via `patch_model_config` and `image_model_config` respectively, both of which are separate `Dinov2Model` by default.\n \n@@ -159,8 +160,8 @@ A list of official Hugging Face and community (indicated by üåé) resources to h\n - Official Implementation: [apple/ml-depth-pro](https://github.com/apple/ml-depth-pro)\n - DepthPro Inference Notebook: [DepthPro Inference](https://github.com/qubvel/transformers-notebooks/blob/main/notebooks/DepthPro_inference.ipynb)\n - DepthPro for Super Resolution and Image Segmentation\n-    - Read blog on Medium: [Depth Pro: Beyond Depth](https://medium.com/@raoarmaghanshakir040/depth-pro-beyond-depth-9d822fc557ba)\n-    - Code on Github: [geetu040/depthpro-beyond-depth](https://github.com/geetu040/depthpro-beyond-depth)\n+  - Read blog on Medium: [Depth Pro: Beyond Depth](https://medium.com/@raoarmaghanshakir040/depth-pro-beyond-depth-9d822fc557ba)\n+  - Code on Github: [geetu040/depthpro-beyond-depth](https://github.com/geetu040/depthpro-beyond-depth)\n \n If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n "
        },
        {
            "sha": "46c9d3dadce62a0eb96978f5d2fc4f7cdb16f15a",
            "filename": "docs/source/en/model_doc/detr.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -16,9 +16,9 @@ rendered properly in your Markdown viewer.\n *This model was released on 2020-05-26 and added to Hugging Face Transformers on 2021-06-09.*\n \n <div style=\"float: right;\">\n-\t<div class=\"flex flex-wrap space-x-1\">\n-\t\t<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-\t</div>\n+ <div class=\"flex flex-wrap space-x-1\">\n+  <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+ </div>\n </div>\n \n # DETR"
        },
        {
            "sha": "89f0f5cb65729035ae814096ad7b0870a8ea9fcc",
            "filename": "docs/source/en/model_doc/dinat.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinat.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -65,6 +65,7 @@ DiNAT can be used as a *backbone*. When `output_hidden_states = True`,\n it will output both `hidden_states` and `reshaped_hidden_states`. The `reshaped_hidden_states` have a shape of `(batch, num_channels, height, width)` rather than `(batch_size, height, width, num_channels)`.\n \n Notes:\n+\n - DiNAT depends on [NATTEN](https://github.com/SHI-Labs/NATTEN/)'s implementation of Neighborhood Attention and Dilated Neighborhood Attention.\n You can install it with pre-built wheels for Linux by referring to [shi-labs.com/natten](https://shi-labs.com/natten), or build on your system by running `pip install natten`.\n Note that the latter will likely take time to compile. NATTEN does not support Windows devices yet."
        },
        {
            "sha": "d6b9c08f2f8ff9630466aeee8828839112bc8d1c",
            "filename": "docs/source/en/model_doc/dinov2_with_registers.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -25,6 +25,7 @@ The [Vision Transformer](vit) (ViT) is a transformer encoder model (BERT-like) o\n Next, people figured out ways to make ViT work really well on self-supervised image feature extraction (i.e. learning meaningful features, also called embeddings) on images without requiring any labels. Some example papers here include [DINOv2](dinov2) and [MAE](vit_mae).\n \n The authors of DINOv2 noticed that ViTs have artifacts in attention maps. It's due to the model using some image patches as ‚Äúregisters‚Äù. The authors propose a fix: just add some new tokens (called \"register\" tokens), which you only use during pre-training (and throw away afterwards). This results in:\n+\n - no artifacts\n - interpretable attention maps\n - and improved performances.\n@@ -57,4 +58,4 @@ The original code can be found [here](https://github.com/facebookresearch/dinov2\n ## Dinov2WithRegistersForImageClassification\n \n [[autodoc]] Dinov2WithRegistersForImageClassification\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "b2e44356ddc4e4de3e15562f66692329f61a8a03",
            "filename": "docs/source/en/model_doc/doge.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdoge.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdoge.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdoge.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -101,4 +101,4 @@ outputs = model.generate(\n ## DogeForSequenceClassification\n \n [[autodoc]] DogeForSequenceClassification\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "18b060cb111d2e675862b2266583f0036b6e30dd",
            "filename": "docs/source/en/model_doc/dpr.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpr.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -44,9 +44,9 @@ This model was contributed by [lhoestq](https://huggingface.co/lhoestq). The ori\n \n - DPR consists in three models:\n \n-    * Question encoder: encode questions as vectors\n-    * Context encoder: encode contexts as vectors\n-    * Reader: extract the answer of the questions inside retrieved contexts, along with a relevance score (high if the inferred span actually answers the question).\n+  * Question encoder: encode questions as vectors\n+  * Context encoder: encode contexts as vectors\n+  * Reader: extract the answer of the questions inside retrieved contexts, along with a relevance score (high if the inferred span actually answers the question).\n \n ## DPRConfig\n "
        },
        {
            "sha": "da28a68074a03d1bec9f9a5d62d528b718bb875c",
            "filename": "docs/source/en/model_doc/efficientloftr.md",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -144,27 +144,23 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n ## EfficientLoFTRImageProcessor\n \n [[autodoc]] EfficientLoFTRImageProcessor\n-\n-- preprocess\n-- post_process_keypoint_matching\n-- visualize_keypoint_matching\n+    - preprocess\n+    - post_process_keypoint_matching\n+    - visualize_keypoint_matching\n \n ## EfficientLoFTRImageProcessorFast\n \n [[autodoc]] EfficientLoFTRImageProcessorFast\n-\n-- preprocess\n-- post_process_keypoint_matching\n-- visualize_keypoint_matching\n+    - preprocess\n+    - post_process_keypoint_matching\n+    - visualize_keypoint_matching\n \n ## EfficientLoFTRModel\n \n [[autodoc]] EfficientLoFTRModel\n-\n-- forward\n+    - forward\n \n ## EfficientLoFTRForKeypointMatching\n \n [[autodoc]] EfficientLoFTRForKeypointMatching\n-\n-- forward\n+    - forward"
        },
        {
            "sha": "7ff1419b3814933332d5ab132da658bc8facc7b9",
            "filename": "docs/source/en/model_doc/eomt.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Feomt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Feomt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Feomt.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -207,4 +207,4 @@ plt.show()\n ## EomtForUniversalSegmentation\n \n [[autodoc]] EomtForUniversalSegmentation\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "9482f5be2c06ea3c313e7abb9cb63a4372691eb8",
            "filename": "docs/source/en/model_doc/exaone4.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fexaone4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fexaone4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fexaone4.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -204,4 +204,4 @@ print(tokenizer.decode(output[0]))\n ## Exaone4ForQuestionAnswering\n \n [[autodoc]] Exaone4ForQuestionAnswering\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "3d79a4e225dd9f878cf84cf6b1938387cde14420",
            "filename": "docs/source/en/model_doc/falcon3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon3.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -30,5 +30,6 @@ Depth up-scaling for improved reasoning: Building on recent studies on the effec\n Knowledge distillation for better tiny models: To provide compact and efficient alternatives, we developed Falcon3-1B-Base and Falcon3-3B-Base by leveraging pruning and knowledge distillation techniques, using less than 100GT of curated high-quality data, thereby redefining pre-training efficiency.\n \n ## Resources\n+\n - [Blog post](https://huggingface.co/blog/falcon3)\n - [Models on Huggingface](https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026)"
        },
        {
            "sha": "48a647cd37973e8af2455501bf660f0bbcbb7ede",
            "filename": "docs/source/en/model_doc/falcon_h1.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_h1.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_h1.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_h1.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -60,4 +60,4 @@ print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\n [[autodoc]] FalconH1ForCausalLM\n     - forward\n \n-This HF implementation is contributed by [younesbelkada](https://github.com/younesbelkada) and [DhiaEddineRhaiem](https://github.com/dhiaEddineRhaiem).\n\\ No newline at end of file\n+This HF implementation is contributed by [younesbelkada](https://github.com/younesbelkada) and [DhiaEddineRhaiem](https://github.com/dhiaEddineRhaiem)."
        },
        {
            "sha": "fe5b96d00c5f6d044e6c326839ca4fe1310feae6",
            "filename": "docs/source/en/model_doc/flaubert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fflaubert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fflaubert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflaubert.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -44,6 +44,7 @@ community for further reproducible experiments in French NLP.*\n This model was contributed by [formiel](https://huggingface.co/formiel). The original code can be found [here](https://github.com/getalp/Flaubert).\n \n Tips:\n+\n - Like RoBERTa, without the sentence ordering prediction (so just trained on the MLM objective).\n \n ## Resources"
        },
        {
            "sha": "b7171e1faabdfc28190e8cf7ea1a423f84d01f37",
            "filename": "docs/source/en/model_doc/florence2.md",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fflorence2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fflorence2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflorence2.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -138,21 +138,21 @@ print(parsed_answer)\n ## Notes\n \n - Florence-2 is a prompt-based model. You need to provide a task prompt to tell the model what to do. Supported tasks are:\n-    - `<OCR>`\n-    - `<OCR_WITH_REGION>`\n-    - `<CAPTION>`\n-    - `<DETAILED_CAPTION>`\n-    - `<MORE_DETAILED_CAPTION>`\n-    - `<OD>`\n-    - `<DENSE_REGION_CAPTION>`\n-    - `<CAPTION_TO_PHRASE_GROUNDING>`\n-    - `<REFERRING_EXPRESSION_SEGMENTATION>`\n-    - `<REGION_TO_SEGMENTATION>`\n-    - `<OPEN_VOCABULARY_DETECTION>`\n-    - `<REGION_TO_CATEGORY>`\n-    - `<REGION_TO_DESCRIPTION>`\n-    - `<REGION_TO_OCR>`\n-    - `<REGION_PROPOSAL>`\n+  - `<OCR>`\n+  - `<OCR_WITH_REGION>`\n+  - `<CAPTION>`\n+  - `<DETAILED_CAPTION>`\n+  - `<MORE_DETAILED_CAPTION>`\n+  - `<OD>`\n+  - `<DENSE_REGION_CAPTION>`\n+  - `<CAPTION_TO_PHRASE_GROUNDING>`\n+  - `<REFERRING_EXPRESSION_SEGMENTATION>`\n+  - `<REGION_TO_SEGMENTATION>`\n+  - `<OPEN_VOCABULARY_DETECTION>`\n+  - `<REGION_TO_CATEGORY>`\n+  - `<REGION_TO_DESCRIPTION>`\n+  - `<REGION_TO_OCR>`\n+  - `<REGION_PROPOSAL>`\n - The raw output of the model is a string that needs to be parsed. The [`Florence2Processor`] has a [`~Florence2Processor.post_process_generation`] method that can parse the string into a more usable format, like bounding boxes and labels for object detection.\n \n ## Resources"
        },
        {
            "sha": "8012ed675a2ac4c0979e5bad2f97b5e9c1f2a740",
            "filename": "docs/source/en/model_doc/gemma3n.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -121,9 +121,9 @@ echo -e \"Plants create energy through a process known as\" | transformers run --t\n \n ## Notes\n \n--   Use [`Gemma3nForConditionalGeneration`] for image-audio-and-text, image-and-text, image-and-audio, audio-and-text,\n+- Use [`Gemma3nForConditionalGeneration`] for image-audio-and-text, image-and-text, image-and-audio, audio-and-text,\n     image-only and audio-only inputs.\n--   Gemma 3n supports multiple images per input, but make sure the images are correctly batched before passing them to\n+- Gemma 3n supports multiple images per input, but make sure the images are correctly batched before passing them to\n     the processor. Each batch should be a list of one or more images.\n \n     ```py\n@@ -148,11 +148,11 @@ echo -e \"Plants create energy through a process known as\" | transformers run --t\n     ]\n     ```\n \n--   Text passed to the processor should have a `<image_soft_token>` token wherever an image should be inserted.\n--   Gemma 3n accept at most one target audio clip per input, though multiple audio clips can be provided in few-shot\n+- Text passed to the processor should have a `<image_soft_token>` token wherever an image should be inserted.\n+- Gemma 3n accept at most one target audio clip per input, though multiple audio clips can be provided in few-shot\n     prompts, for example.\n--   Text passed to the processor should have a `<audio_soft_token>` token wherever an audio clip should be inserted.\n--   The processor has its own [`~ProcessorMixin.apply_chat_template`] method to convert chat messages to model inputs.\n+- Text passed to the processor should have a `<audio_soft_token>` token wherever an audio clip should be inserted.\n+- The processor has its own [`~ProcessorMixin.apply_chat_template`] method to convert chat messages to model inputs.\n \n ## Gemma3nAudioFeatureExtractor\n "
        },
        {
            "sha": "06a65a6dd896f8c8bbfa37892035c2309cc6103d",
            "filename": "docs/source/en/model_doc/git.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgit.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -81,4 +81,4 @@ The resource should ideally demonstrate something new instead of duplicating an\n ## GitForCausalLM\n \n [[autodoc]] GitForCausalLM\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "c814fdb5becdbde5508854f7d0ed168202b7b624",
            "filename": "docs/source/en/model_doc/glm4v_moe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v_moe.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -35,6 +35,7 @@ Through our open-source work, we aim to explore the technological frontier toget\n ![bench_45](https://raw.githubusercontent.com/zai-org/GLM-V/refs/heads/main/resources/bench_45v.jpeg)\n \n Beyond benchmark performance, GLM-4.5V focuses on real-world usability. Through efficient hybrid training, it can handle diverse types of visual content, enabling full-spectrum vision reasoning, including:\n+\n - **Image reasoning** (scene understanding, complex multi-image analysis, spatial recognition)\n - **Video understanding** (long video segmentation and event recognition)\n - **GUI tasks** (screen reading, icon recognition, desktop operation assistance)"
        },
        {
            "sha": "9f051c347f9fdbd185f3929c9ef2e821b82f351f",
            "filename": "docs/source/en/model_doc/gpt_bigcode.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -36,6 +36,7 @@ The model is an optimized [GPT2 model](https://huggingface.co/docs/transformers/\n ## Implementation details\n \n The main differences compared to GPT2.\n+\n - Added support for Multi-Query Attention.\n - Use `gelu_pytorch_tanh` instead of classic `gelu`.\n - Avoid unnecessary synchronizations (this has since been added to GPT2 in #20061, but wasn't in the reference codebase)."
        },
        {
            "sha": "7b81ee12d270bf015bf5f5d13faaf18410a19f20",
            "filename": "docs/source/en/model_doc/gptj.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptj.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptj.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptj.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -133,6 +133,7 @@ A list of official Hugging Face and community (indicated by üåé) resources to h\n - [`GPTJForCausalLM`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling), [text generation example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation), and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n \n **Documentation resources**\n+\n - [Text classification task guide](../tasks/sequence_classification)\n - [Question answering task guide](../tasks/question_answering)\n - [Causal language modeling task guide](../tasks/language_modeling)"
        },
        {
            "sha": "1d05ee346b6722e2566b9eb405bbc9b005f65939",
            "filename": "docs/source/en/model_doc/granite_speech.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -37,6 +37,7 @@ Note that most of the aforementioned components are implemented generically to e\n This model was contributed by [Alexander Brooks](https://huggingface.co/abrooks9944), [Avihu Dekel](https://huggingface.co/Avihu), and [George Saon](https://huggingface.co/gsaon).\n \n ## Usage tips\n+\n - This model bundles its own LoRA adapter, which will be automatically loaded and enabled/disabled as needed during inference calls. Be sure to install [PEFT](https://github.com/huggingface/peft) to ensure the LoRA is correctly applied!\n \n <!-- TODO (@alex-jw-brooks) Add an example here once the model compatible with the transformers implementation is released -->"
        },
        {
            "sha": "9db702c9f705aed9b79050ad8469e6fd00a9d053",
            "filename": "docs/source/en/model_doc/granitemoeshared.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoeshared.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoeshared.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoeshared.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -62,4 +62,4 @@ This HF implementation is contributed by [Mayank Mishra](https://huggingface.co/\n ## GraniteMoeSharedForCausalLM\n \n [[autodoc]] GraniteMoeSharedForCausalLM\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "b95982ee81f97274cf59a4c577721f78a3c30bb6",
            "filename": "docs/source/en/model_doc/granitevision.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -22,6 +22,7 @@ rendered properly in your Markdown viewer.\n The [Granite Vision](https://www.ibm.com/new/announcements/ibm-granite-3-1-powerful-performance-long-context-and-more) model is a variant of [LLaVA-NeXT](llava_next), leveraging a [Granite](granite) language model alongside a [SigLIP](SigLIP) visual encoder. It utilizes multiple concatenated vision hidden states as its image features, similar to [VipLlava](vipllava). It also uses a larger set of image grid pinpoints than the original LlaVa-NeXT models to support additional aspect ratios.\n \n Tips:\n+\n - This model is loaded into Transformers as an instance of LlaVA-Next. The usage and tips from [LLaVA-NeXT](llava_next) apply to this model as well.\n \n - You can apply the chat template on the tokenizer / processor in the same way as well. Example chat format:"
        },
        {
            "sha": "8e7791ce71eab93628e2b5bf767ea5e2795a944a",
            "filename": "docs/source/en/model_doc/hgnet_v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -89,4 +89,4 @@ print(f\"The predicted class label is: {predicted_class_label}\")\n ## HGNetV2ForImageClassification\n \n [[autodoc]] HGNetV2ForImageClassification\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "a9cea0f09cab5ef7ff1996b3c7503c93f9568072",
            "filename": "docs/source/en/model_doc/informer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Finformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Finformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finformer.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -52,4 +52,4 @@ A list of official Hugging Face and community (indicated by üåé) resources to h\n ## InformerForPrediction\n \n [[autodoc]] InformerForPrediction\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "ac84a71d887eb338f7109ee35b8cd9328af98839",
            "filename": "docs/source/en/model_doc/instructblip.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -77,4 +77,4 @@ The attributes can be obtained from model config, as `model.config.num_query_tok\n \n [[autodoc]] InstructBlipForConditionalGeneration\n     - forward\n-    - generate\n\\ No newline at end of file\n+    - generate"
        },
        {
            "sha": "f3482c37ae05d27b82c219bb848b6f1ef05e4ad1",
            "filename": "docs/source/en/model_doc/kyutai_speech_to_text.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -19,6 +19,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n [Kyutai STT](https://kyutai.org/next/stt) is a speech-to-text model architecture based on the [Mimi codec](https://huggingface.co/docs/transformers/en/model_doc/mimi), which encodes audio into discrete tokens in a streaming fashion, and a [Moshi-like](https://huggingface.co/docs/transformers/en/model_doc/moshi) autoregressive decoder. Kyutai's lab has released two model checkpoints:\n+\n - [kyutai/stt-1b-en_fr](https://huggingface.co/kyutai/stt-1b-en_fr): a 1B-parameter model capable of transcribing both English and French\n - [kyutai/stt-2.6b-en](https://huggingface.co/kyutai/stt-2.6b-en): a 2.6B-parameter model focused solely on English, optimized for maximum transcription accuracy\n "
        },
        {
            "sha": "b9964fa3f86cbd6080bf938fe141007a1da7b7e5",
            "filename": "docs/source/en/model_doc/layoutlmv3.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv3.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -37,8 +37,8 @@ This model was contributed by [nielsr](https://huggingface.co/nielsr). The origi\n ## Usage tips\n \n - In terms of data processing, LayoutLMv3 is identical to its predecessor [LayoutLMv2](layoutlmv2), except that:\n-    - images need to be resized and normalized with channels in regular RGB format. LayoutLMv2 on the other hand normalizes the images internally and expects the channels in BGR format.\n-    - text is tokenized using byte-pair encoding (BPE), as opposed to WordPiece.\n+  - images need to be resized and normalized with channels in regular RGB format. LayoutLMv2 on the other hand normalizes the images internally and expects the channels in BGR format.\n+  - text is tokenized using byte-pair encoding (BPE), as opposed to WordPiece.\n   Due to these differences in data preprocessing, one can use [`LayoutLMv3Processor`] which internally combines a [`LayoutLMv3ImageProcessor`] (for the image modality) and a [`LayoutLMv3Tokenizer`]/[`LayoutLMv3TokenizerFast`] (for the text modality) to prepare all data for the model.\n - Regarding usage of [`LayoutLMv3Processor`], we refer to the [usage guide](layoutlmv2#usage-layoutlmv2processor) of its predecessor.\n \n@@ -73,6 +73,7 @@ LayoutLMv3 is nearly identical to LayoutLMv2, so we've also included LayoutLMv2\n - [Question answering task guide](../tasks/question_answering)\n \n **Document question answering**\n+\n - [Document question answering task guide](../tasks/document_question_answering)\n \n ## LayoutLMv3Config"
        },
        {
            "sha": "58f1d754588d0cd73fa122d26cc8d5b03e656ded",
            "filename": "docs/source/en/model_doc/lfm2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -82,4 +82,4 @@ print(tokenizer.decode(output[0], skip_special_tokens=False))\n ## Lfm2ForCausalLM\n \n [[autodoc]] Lfm2ForCausalLM\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "fb6b2ad8a4e2689f1f9f62e5b5690a7b04951cb4",
            "filename": "docs/source/en/model_doc/lfm2_vl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_vl.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -28,6 +28,7 @@ rendered properly in your Markdown viewer.\n ## Architecture\n \n LFM2-VL consists of three main components: a language model backbone, a vision encoder, and a multimodal projector. LFM2-VL builds upon the LFM2 backbone, inheriting from either LFM2-1.2B (for LFM2-VL-1.6B) or LFM2-350M (for LFM2-VL-450M). For the vision tower, LFM2-VL uses SigLIP2 NaFlex encoders to convert input images into token sequences. Two variants are implemented:\n+\n * Shape-optimized (400M) for more fine-grained vision capabilities for LFM2-VL-1.6B\n * Base (86M) for fast image processing for LFM2-VL-450M\n "
        },
        {
            "sha": "878bd1982ed440c24f40f3de57765e62a36c2ea0",
            "filename": "docs/source/en/model_doc/lightglue.md",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -143,13 +143,11 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n ## LightGlueImageProcessor\n \n [[autodoc]] LightGlueImageProcessor\n-\n-- preprocess\n-- post_process_keypoint_matching\n-- visualize_keypoint_matching\n+    - preprocess\n+    - post_process_keypoint_matching\n+    - visualize_keypoint_matching\n \n ## LightGlueForKeypointMatching\n \n [[autodoc]] LightGlueForKeypointMatching\n-\n-- forward\n+    - forward"
        },
        {
            "sha": "407e4aad3c40517350ae803567b7eedf4687fa3f",
            "filename": "docs/source/en/model_doc/lilt.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Flilt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Flilt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flilt.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -62,6 +62,7 @@ A list of official Hugging Face and community (indicated by üåé) resources to h\n - Demo notebooks for LiLT can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/LiLT).\n \n **Documentation resources**\n+\n - [Text classification task guide](../tasks/sequence_classification)\n - [Token classification task guide](../tasks/token_classification)\n - [Question answering task guide](../tasks/question_answering)"
        },
        {
            "sha": "ee7f2e2a54f5961ac15caf36ead2db6a9d7768d4",
            "filename": "docs/source/en/model_doc/llama4.md",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -27,9 +27,11 @@ rendered properly in your Markdown viewer.\n \n [Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/), developed by Meta, introduces a new auto-regressive Mixture-of-Experts (MoE) architecture.\n This generation includes two models:\n+\n - The highly capable Llama 4 Maverick with 17B active parameters out of ~400B total, with 128 experts.\n - The efficient Llama 4 Scout also  has 17B active parameters out of ~109B total, using just 16 experts.\n -\n+\n Both models leverage early fusion for native multimodality, enabling them to process text and image inputs.\n Maverick and Scout are both trained on up to 40 trillion tokens on data encompassing 200 languages\n (with specific fine-tuning support for 12 languages including Arabic, Spanish, German, and Hindi).\n@@ -421,24 +423,24 @@ model = Llama4ForConditionalGeneration.from_pretrained(\n ## Llama4ForConditionalGeneration\n \n [[autodoc]] Llama4ForConditionalGeneration\n-- forward\n+    - forward\n \n ## Llama4ForCausalLM\n \n [[autodoc]] Llama4ForCausalLM\n-- forward\n+    - forward\n \n ## Llama4TextModel\n \n [[autodoc]] Llama4TextModel\n-- forward\n+    - forward\n \n ## Llama4ForCausalLM\n \n [[autodoc]] Llama4ForCausalLM\n-- forward\n+    - forward\n \n ## Llama4VisionModel\n \n [[autodoc]] Llama4VisionModel\n-- forward\n+    - forward"
        },
        {
            "sha": "e387fb4b54c77b8f0398325e82c59e0c5e2df331",
            "filename": "docs/source/en/model_doc/llava.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -57,6 +57,7 @@ The attributes can be obtained from model config, as `model.config.vision_config\n Each **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor's `apply_chat_template` method.  \n \n **Important:**  \n+\n - You must construct a conversation history ‚Äî passing a plain string won't work.  \n - Each message should be a dictionary with `\"role\"` and `\"content\"` keys.  \n - The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.  "
        },
        {
            "sha": "61aa7e1ffc51a2aa3b7911707e48ba9a3dada025",
            "filename": "docs/source/en/model_doc/llava_next_video.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -64,6 +64,7 @@ The attributes can be obtained from model config, as `model.config.vision_config\n Each **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor's `apply_chat_template` method.  \n \n **Important:**  \n+\n - You must construct a conversation history ‚Äî passing a plain string won't work.  \n - Each message should be a dictionary with `\"role\"` and `\"content\"` keys.  \n - The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.  "
        },
        {
            "sha": "08bc075495b019773a9cb1062a981e557b61ae2e",
            "filename": "docs/source/en/model_doc/llava_onevision.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -59,6 +59,7 @@ Tips:\n Each **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor‚Äôs `apply_chat_template` method.  \n \n **Important:**  \n+\n - You must construct a conversation history ‚Äî passing a plain string won't work.  \n - Each message should be a dictionary with `\"role\"` and `\"content\"` keys.  \n - The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.  "
        },
        {
            "sha": "504817a14996c21a203824484ecad429a9b838c2",
            "filename": "docs/source/en/model_doc/markuplm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarkuplm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarkuplm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarkuplm.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -30,6 +30,7 @@ performance, similar to [LayoutLM](layoutlm).\n \n The model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\n state-of-the-art results on 2 important benchmarks:\n+\n - [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n - [SWDE](https://www.researchgate.net/publication/221299838_From_one_tree_to_a_forest_a_unified_solution_for_structured_web_data_extraction), a dataset\n for information extraction from web pages (basically named-entity recognition on web pages)"
        },
        {
            "sha": "91a02cf6f7145a22775fddb351fe8d7821f86342",
            "filename": "docs/source/en/model_doc/mask2former.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmask2former.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmask2former.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmask2former.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -86,4 +86,4 @@ The resource should ideally demonstrate something new instead of duplicating an\n     - preprocess\n     - post_process_semantic_segmentation\n     - post_process_instance_segmentation\n-    - post_process_panoptic_segmentation\n\\ No newline at end of file\n+    - post_process_panoptic_segmentation"
        },
        {
            "sha": "aed2dcfa6c400dcb991f7ccf0682d1236720583a",
            "filename": "docs/source/en/model_doc/maskformer.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmaskformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmaskformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmaskformer.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -44,7 +44,7 @@ This model was contributed by [francesco](https://huggingface.co/francesco). The\n \n ## Usage tips\n \n--  MaskFormer's Transformer decoder is identical to the decoder of [DETR](detr). During training, the authors of DETR did find it helpful to use auxiliary losses in the decoder, especially to help the model output the correct number of objects of each class. If you set the parameter `use_auxiliary_loss` of [`MaskFormerConfig`] to `True`, then prediction feedforward neural networks and Hungarian losses are added after each decoder layer (with the FFNs sharing parameters).\n+- MaskFormer's Transformer decoder is identical to the decoder of [DETR](detr). During training, the authors of DETR did find it helpful to use auxiliary losses in the decoder, especially to help the model output the correct number of objects of each class. If you set the parameter `use_auxiliary_loss` of [`MaskFormerConfig`] to `True`, then prediction feedforward neural networks and Hungarian losses are added after each decoder layer (with the FFNs sharing parameters).\n - If you want to train the model in a distributed environment across multiple nodes, then one should update the\n   `get_num_masks` function inside in the `MaskFormerLoss` class of `modeling_maskformer.py`. When training on multiple nodes, this should be\n   set to the average number of target masks across all nodes, as can be seen in the original implementation [here](https://github.com/facebookresearch/MaskFormer/blob/da3e60d85fdeedcb31476b5edd7d328826ce56cc/mask_former/modeling/criterion.py#L169).\n@@ -102,4 +102,4 @@ This model was contributed by [francesco](https://huggingface.co/francesco). The\n ## MaskFormerForInstanceSegmentation\n \n [[autodoc]] MaskFormerForInstanceSegmentation\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "a5b2689dcb5d7cdd63119887feedb3024dc265d6",
            "filename": "docs/source/en/model_doc/matcha.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmatcha.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmatcha.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmatcha.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -79,4 +79,4 @@ scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=1000, nu\n \n MatCha is a model that is trained using `Pix2Struct` architecture. You can find more information about `Pix2Struct` in the [Pix2Struct documentation](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct).\n \n-</Tip>\n\\ No newline at end of file\n+</Tip>"
        },
        {
            "sha": "d1fe109c2431ed368384c3c2d629309288c1bf3a",
            "filename": "docs/source/en/model_doc/minimax.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -35,12 +35,12 @@ The architecture of MiniMax is briefly described as follows:\n - Activated Parameters per Token: 45.9B\n - Number Layers: 80\n - Hybrid Attention: a softmax attention is positioned after every 7 lightning attention.\n-    - Number of attention heads: 64\n-    - Attention head dimension: 128\n+  - Number of attention heads: 64\n+  - Attention head dimension: 128\n - Mixture of Experts:\n-    - Number of experts: 32\n-    - Expert hidden dimension: 9216\n-    - Top-2 routing strategy\n+  - Number of experts: 32\n+  - Expert hidden dimension: 9216\n+  - Top-2 routing strategy\n - Positional Encoding: Rotary Position Embedding (RoPE) applied to half of the attention head dimension with a base frequency of 10,000,000\n - Hidden Size: 6144\n - Vocab Size: 200,064"
        },
        {
            "sha": "117547934f33bca48a80a2b4f43a21763a4343a5",
            "filename": "docs/source/en/model_doc/ministral.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -83,4 +83,4 @@ The example below demonstrates how to use Ministral for text generation:\n ## MinistralForQuestionAnswering\n \n [[autodoc]] MinistralForQuestionAnswering\n-- forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "4c598fc79a71fa717bf2e9f7fc6626c50f6b4313",
            "filename": "docs/source/en/model_doc/mistral.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -163,4 +163,4 @@ Use the [AttentionMaskVisualizer](https://github.com/huggingface/transformers/bl\n ## MistralForQuestionAnswering\n \n [[autodoc]] MistralForQuestionAnswering\n-- forward\n+    - forward"
        },
        {
            "sha": "1e9574145aa14d480efe9040d96943e0c4986770",
            "filename": "docs/source/en/model_doc/mixtral.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -42,6 +42,7 @@ Mixtral-8x7B is a decoder-only Transformer with the following architectural choi\n - Despite the model having 45 billion parameters, the compute required for a single forward pass is the same as that of a 14 billion parameter model. This is because even though each of the experts have to be loaded in RAM (70B like ram requirement) each token from the hidden states are dispatched twice (top 2 routing) and thus the compute (the operation required at each forward computation) is just 2 X sequence_length.\n \n The following implementation details are shared with Mistral AI's first model [Mistral-7B](mistral):\n+\n - Sliding Window Attention - Trained with 8k context length and fixed cache size, with a theoretical attention span of 128K tokens\n - GQA (Grouped Query Attention) - allowing faster inference and lower cache size.\n - Byte-fallback BPE tokenizer - ensures that characters are never mapped to out of vocabulary tokens.\n@@ -55,6 +56,7 @@ For more details refer to the [release blog post](https://mistral.ai/news/mixtra\n ## Usage tips\n \n The Mistral team has released 2 checkpoints:\n+\n - a base model, [Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1), which has been pre-trained to predict the next token on internet-scale data.\n - an instruction tuned model, [Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1), which is the base model optimized for chat purposes using supervised fine-tuning (SFT) and direct preference optimization (DPO).\n "
        },
        {
            "sha": "eea159bdd738aac90391d0a83c2d550e035fb0af",
            "filename": "docs/source/en/model_doc/mobilenet_v1.md",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -85,22 +85,22 @@ print(f\"The predicted class label is: {predicted_class_label}\")\n \n ## Notes\n \n--   Checkpoint names follow the pattern `mobilenet_v1_{depth_multiplier}_{resolution}`, like `mobilenet_v1_1.0_224`. `1.0` is the depth multiplier and `224` is the image resolution.\n--   While trained on images of a specific sizes, the model architecture works with images of different sizes (minimum 32x32). The [`MobileNetV1ImageProcessor`] handles the necessary preprocessing.\n--   MobileNet is pretrained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k), a dataset with 1000 classes. However, the model actually predicts 1001 classes. The additional class is an extra \"background\" class (index 0).\n--   The original TensorFlow checkpoints determines the padding amount at inference because it depends on the input image size. To use the native PyTorch padding behavior, set `tf_padding=False` in [`MobileNetV1Config`].\n+- Checkpoint names follow the pattern `mobilenet_v1_{depth_multiplier}_{resolution}`, like `mobilenet_v1_1.0_224`. `1.0` is the depth multiplier and `224` is the image resolution.\n+- While trained on images of a specific sizes, the model architecture works with images of different sizes (minimum 32x32). The [`MobileNetV1ImageProcessor`] handles the necessary preprocessing.\n+- MobileNet is pretrained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k), a dataset with 1000 classes. However, the model actually predicts 1001 classes. The additional class is an extra \"background\" class (index 0).\n+- The original TensorFlow checkpoints determines the padding amount at inference because it depends on the input image size. To use the native PyTorch padding behavior, set `tf_padding=False` in [`MobileNetV1Config`].\n \n     ```python\n     from transformers import MobileNetV1Config\n \n     config = MobileNetV1Config.from_pretrained(\"google/mobilenet_v1_1.0_224\", tf_padding=True)\n     ```\n \n--   The Transformers implementation does not support the following features.\n-    -   Uses global average pooling instead of the optional 7x7 average pooling with stride 2. For larger inputs, this gives a pooled output that is larger than a 1x1 pixel.\n-    -   Does not support other `output_stride` values (fixed at 32). For smaller `output_strides`, the original implementation uses dilated convolution to prevent spatial resolution from being reduced further. (which would require dilated convolutions).\n-    -   `output_hidden_states=True` returns *all* intermediate hidden states. It is not possible to extract the output from specific layers for other downstream purposes.\n-    - Does not include the quantized models from the original checkpoints because they include \"FakeQuantization\" operations to unquantize the weights.\n+- The Transformers implementation does not support the following features.\n+  - Uses global average pooling instead of the optional 7x7 average pooling with stride 2. For larger inputs, this gives a pooled output that is larger than a 1x1 pixel.\n+  - Does not support other `output_stride` values (fixed at 32). For smaller `output_strides`, the original implementation uses dilated convolution to prevent spatial resolution from being reduced further. (which would require dilated convolutions).\n+  - `output_hidden_states=True` returns *all* intermediate hidden states. It is not possible to extract the output from specific layers for other downstream purposes.\n+  - Does not include the quantized models from the original checkpoints because they include \"FakeQuantization\" operations to unquantize the weights.\n \n ## MobileNetV1Config\n "
        },
        {
            "sha": "bf94454e438de29abb9681a2b10fc4e6d31875fb",
            "filename": "docs/source/en/model_doc/mobilenet_v2.md",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -82,23 +82,23 @@ print(f\"The predicted class label is: {predicted_class_label}\")\n \n ## Notes\n \n--   Classification checkpoint names follow the pattern `mobilenet_v2_{depth_multiplier}_{resolution}`, like `mobilenet_v2_1.4_224`. `1.4` is the depth multiplier and `224` is the image resolution. Segmentation checkpoint names follow the pattern `deeplabv3_mobilenet_v2_{depth_multiplier}_{resolution}`.\n--   While trained on images of a specific sizes, the model architecture works with images of different sizes (minimum 32x32). The [`MobileNetV2ImageProcessor`] handles the necessary preprocessing.\n--   MobileNet is pretrained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k), a dataset with 1000 classes. However, the model actually predicts 1001 classes. The additional class is an extra \"background\" class (index 0).\n--   The segmentation models use a [DeepLabV3+](https://huggingface.co/papers/1802.02611) head which is often pretrained on datasets like [PASCAL VOC](https://huggingface.co/datasets/merve/pascal-voc).\n--   The original TensorFlow checkpoints determines the padding amount at inference because it depends on the input image size. To use the native PyTorch padding behavior, set `tf_padding=False` in [`MobileNetV2Config`].\n+- Classification checkpoint names follow the pattern `mobilenet_v2_{depth_multiplier}_{resolution}`, like `mobilenet_v2_1.4_224`. `1.4` is the depth multiplier and `224` is the image resolution. Segmentation checkpoint names follow the pattern `deeplabv3_mobilenet_v2_{depth_multiplier}_{resolution}`.\n+- While trained on images of a specific sizes, the model architecture works with images of different sizes (minimum 32x32). The [`MobileNetV2ImageProcessor`] handles the necessary preprocessing.\n+- MobileNet is pretrained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k), a dataset with 1000 classes. However, the model actually predicts 1001 classes. The additional class is an extra \"background\" class (index 0).\n+- The segmentation models use a [DeepLabV3+](https://huggingface.co/papers/1802.02611) head which is often pretrained on datasets like [PASCAL VOC](https://huggingface.co/datasets/merve/pascal-voc).\n+- The original TensorFlow checkpoints determines the padding amount at inference because it depends on the input image size. To use the native PyTorch padding behavior, set `tf_padding=False` in [`MobileNetV2Config`].\n \n     ```python\n     from transformers import MobileNetV2Config\n \n     config = MobileNetV2Config.from_pretrained(\"google/mobilenet_v2_1.4_224\", tf_padding=True)\n     ```\n \n--   The Transformers implementation does not support the following features.\n-    -   Uses global average pooling instead of the optional 7x7 average pooling with stride 2. For larger inputs, this gives a pooled output that is larger than a 1x1 pixel.\n-    -   `output_hidden_states=True` returns *all* intermediate hidden states. It is not possible to extract the output from specific layers for other downstream purposes.\n-    - Does not include the quantized models from the original checkpoints because they include \"FakeQuantization\" operations to unquantize the weights.\n-    -   For segmentation models, the final convolution layer of the backbone is computed even though the DeepLabV3+ head doesn't use it.\n+- The Transformers implementation does not support the following features.\n+  - Uses global average pooling instead of the optional 7x7 average pooling with stride 2. For larger inputs, this gives a pooled output that is larger than a 1x1 pixel.\n+  - `output_hidden_states=True` returns *all* intermediate hidden states. It is not possible to extract the output from specific layers for other downstream purposes.\n+  - Does not include the quantized models from the original checkpoints because they include \"FakeQuantization\" operations to unquantize the weights.\n+  - For segmentation models, the final convolution layer of the backbone is computed even though the DeepLabV3+ head doesn't use it.\n \n ## MobileNetV2Config\n "
        },
        {
            "sha": "ca0a35f6ece84cc3a9d630cdb3a5e66353f75b42",
            "filename": "docs/source/en/model_doc/mobilevit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -28,6 +28,7 @@ Unless required by applicable law or agreed to in writing, software distributed\n You can find all the original MobileViT checkpoints under the [Apple](https://huggingface.co/apple/models?search=mobilevit) organization.\n \n > [!TIP]\n+>\n > - This model was contributed by [matthijs](https://huggingface.co/Matthijs).\n >\n > Click on the MobileViT models in the right sidebar for more examples of how to apply MobileViT to different vision tasks."
        },
        {
            "sha": "885623b26e520d46935c708c4a264a446ebbfbe3",
            "filename": "docs/source/en/model_doc/moshi.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoshi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoshi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoshi.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -38,6 +38,7 @@ The abstract from the paper is the following:\n *We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning‚Äî such as emotion or non-speech sounds‚Äî is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this ‚ÄúInner Monologue‚Äù method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at github.com/kyutai-labs/moshi.*\n \n Moshi deals with 3 streams of information:\n+\n 1. The user's audio\n 2. Moshi's audio\n 3. Moshi's textual output\n@@ -70,6 +71,7 @@ The original checkpoints can be converted using the conversion script `src/trans\n ### How to use the model:\n \n This implementation has two main aims:\n+\n 1. quickly test model generation by simplifying the original API\n 2. simplify training. A training guide will come soon, but user contributions are welcomed!\n \n@@ -84,13 +86,15 @@ It is designed for intermediate use. We strongly recommend using the original [i\n Moshi is a streaming auto-regressive model with two streams of audio. To put it differently, one audio stream corresponds to what the model said/will say and the other audio stream corresponds to what the user said/will say.\n \n [`MoshiForConditionalGeneration.generate`] thus needs 3 inputs:\n+\n 1. `input_ids` - corresponding to the text token history\n 2. `moshi_input_values` or `moshi_audio_codes`- corresponding to the model audio history\n 3. `user_input_values` or `user_audio_codes` - corresponding to the user audio history\n \n These three inputs must be synchronized. Meaning that their lengths must correspond to the same number of tokens.\n \n You can dynamically use the 3 inputs depending on what you want to test:\n+\n 1. Simply check the model response to an user prompt - in that case, `input_ids` can be filled with pad tokens and `user_input_values` can be a zero tensor of the same shape than the user prompt.\n 2. Test more complex behaviour - in that case, you must be careful about how the input tokens are synchronized with the audios.\n "
        },
        {
            "sha": "422ed3cec5154256531db95313a941810747906a",
            "filename": "docs/source/en/model_doc/mra.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmra.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmra.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmra.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -64,4 +64,4 @@ The original code can be found [here](https://github.com/mlpen/mra-attention).\n ## MraForQuestionAnswering\n \n [[autodoc]] MraForQuestionAnswering\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "cda41e0df2af13f7831198dd999669e89d373869",
            "filename": "docs/source/en/model_doc/musicgen.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -230,6 +230,7 @@ generation config.\n ## Model Structure\n \n The MusicGen model can be de-composed into three distinct stages:\n+\n 1. Text encoder: maps the text inputs to a sequence of hidden-state representations. The pre-trained MusicGen models use a frozen text encoder from either T5 or Flan-T5\n 2. MusicGen decoder: a language model (LM) that auto-regressively generates audio tokens (or codes) conditional on the encoder hidden-state representations\n 3. Audio encoder/decoder: used to encode an audio prompt to use as prompt tokens, and recover the audio waveform from the audio tokens predicted by the decoder\n@@ -256,6 +257,7 @@ be combined with the frozen text encoder and audio encoder/decoders to recover t\n model.\n \n Tips:\n+\n * MusicGen is trained on the 32kHz checkpoint of Encodec. You should ensure you use a compatible version of the Encodec model.\n * Sampling mode tends to deliver better results than greedy - you can toggle sampling with the variable `do_sample` in the call to [`MusicgenForConditionalGeneration.generate`]\n "
        },
        {
            "sha": "caf8cdd739d3b8dbd5c22e3872e61b6e30d3397c",
            "filename": "docs/source/en/model_doc/musicgen_melody.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -40,6 +40,7 @@ This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The o\n ## Difference with [MusicGen](https://huggingface.co/docs/transformers/main/en/model_doc/musicgen)\n \n There are two key differences with MusicGen:\n+\n 1. The audio prompt is used here as a conditional signal for the generated audio sample, whereas it's used for audio continuation in [MusicGen](https://huggingface.co/docs/transformers/main/en/model_doc/musicgen).\n 2. Conditional text and audio signals are concatenated to the decoder's hidden states instead of being used as a cross-attention signal, as in MusicGen.\n \n@@ -224,6 +225,7 @@ Note that any arguments passed to the generate method will **supersede** those i\n ## Model Structure\n \n The MusicGen model can be de-composed into three distinct stages:\n+\n 1. Text encoder: maps the text inputs to a sequence of hidden-state representations. The pre-trained MusicGen models use a frozen text encoder from either T5 or Flan-T5.\n 2. MusicGen Melody decoder: a language model (LM) that auto-regressively generates audio tokens (or codes) conditional on the encoder hidden-state representations\n 3. Audio decoder: used to recover the audio waveform from the audio tokens predicted by the decoder.\n@@ -253,6 +255,7 @@ python src/transformers/models/musicgen_melody/convert_musicgen_melody_transform\n ```\n \n Tips:\n+\n * MusicGen is trained on the 32kHz checkpoint of Encodec. You should ensure you use a compatible version of the Encodec model.\n * Sampling mode tends to deliver better results than greedy - you can toggle sampling with the variable `do_sample` in the call to [`MusicgenMelodyForConditionalGeneration.generate`]\n "
        },
        {
            "sha": "36662173f2f48f728bc0f1e54481905c2cfdd1c2",
            "filename": "docs/source/en/model_doc/nat.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fnat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fnat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnat.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -68,6 +68,7 @@ The `reshaped_hidden_states` have a shape of `(batch, num_channels, height, widt\n `(batch_size, height, width, num_channels)`.\n \n Notes:\n+\n - NAT depends on [NATTEN](https://github.com/SHI-Labs/NATTEN/)'s implementation of Neighborhood Attention.\n You can install it with pre-built wheels for Linux by referring to [shi-labs.com/natten](https://shi-labs.com/natten),\n or build on your system by running `pip install natten`."
        },
        {
            "sha": "f44c03dcfdd3d78c0c9916f06a14b751f3d8abd4",
            "filename": "docs/source/en/model_doc/nllb.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -128,9 +128,9 @@ visualizer(\"UN Chief says there is no military solution in Syria\")\n    >>> tokenizer = NllbTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\", legacy_behaviour=True)\n    ```\n \n- - For non-English languages, specify the language's [BCP-47](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200) code with the `src_lang` keyword as shown below.\n+- For non-English languages, specify the language's [BCP-47](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200) code with the `src_lang` keyword as shown below.\n \n- - See example below for a translation from Romanian to German.\n+- See example below for a translation from Romanian to German.\n \n     ```python\n     >>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
        },
        {
            "sha": "7f5d32bc55a80b41b72589dbe4d894abc09b91e7",
            "filename": "docs/source/en/model_doc/oneformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Foneformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Foneformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Foneformer.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -39,7 +39,7 @@ This model was contributed by [Jitesh Jain](https://huggingface.co/praeclarumjj3\n \n ## Usage tips\n \n--  OneFormer requires two inputs during inference: *image* and *task token*.\n+- OneFormer requires two inputs during inference: *image* and *task token*.\n - During training, OneFormer only uses panoptic annotations.\n - If you want to train the model in a distributed environment across multiple nodes, then one should update the\n   `get_num_masks` function inside in the `OneFormerLoss` class of `modeling_oneformer.py`. When training on multiple nodes, this should be"
        },
        {
            "sha": "04d37d89cc49ce9c08f8b06017ee560363e742d3",
            "filename": "docs/source/en/model_doc/openai-gpt.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -84,22 +84,22 @@ echo -e \"The future of AI is\" | transformers run --task text-generation --model\n ## OpenAIGPTModel\n \n [[autodoc]] OpenAIGPTModel\n-- forward\n+    - forward\n \n ## OpenAIGPTLMHeadModel\n \n [[autodoc]] OpenAIGPTLMHeadModel\n-- forward\n+    - forward\n \n ## OpenAIGPTDoubleHeadsModel\n \n [[autodoc]] OpenAIGPTDoubleHeadsModel\n-- forward\n+    - forward\n \n ## OpenAIGPTForSequenceClassification\n \n [[autodoc]] OpenAIGPTForSequenceClassification\n-- forward\n+    - forward\n \n ## OpenAIGPTTokenizer\n "
        },
        {
            "sha": "4cb72e7e458529f43167ca1f921da1404ae5934d",
            "filename": "docs/source/en/model_doc/parakeet.md",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fparakeet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fparakeet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fparakeet.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -27,12 +27,13 @@ rendered properly in your Markdown viewer.\n Parakeet models, [introduced by NVIDIA NeMo](https://developer.nvidia.com/blog/pushing-the-boundaries-of-speech-recognition-with-nemo-parakeet-asr-models/), are models that combine a [Fast Conformer](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html#fast-conformer) encoder with connectionist temporal classification (CTC), recurrent neural network transducer (RNNT) or token and duration transducer (TDT) decoder for automatic speech recognition.\n \n **Model Architecture**\n+\n - **Fast Conformer Encoder**: A linearly scalable Conformer architecture that processes mel-spectrogram features and reduces sequence length through subsampling. This is more efficient version of the Conformer Encoder found in [FastSpeech2Conformer](./fastspeech2_conformer.md) (see [`ParakeetEncoder`] for the encoder implementation and details).\n - [**ParakeetForCTC**](#parakeetforctc): a Fast Conformer Encoder + a CTC decoder\n-    - **CTC Decoder**: Simple but effective decoder consisting of:\n-        - 1D convolution projection from encoder hidden size to vocabulary size (for optimal NeMo compatibility).\n-        - CTC loss computation for training.\n-        - Greedy CTC decoding for inference.\n+  - **CTC Decoder**: Simple but effective decoder consisting of:\n+    - 1D convolution projection from encoder hidden size to vocabulary size (for optimal NeMo compatibility).\n+    - CTC loss computation for training.\n+    - Greedy CTC decoding for inference.\n \n The original implementation can be found in [NVIDIA NeMo](https://github.com/NVIDIA/NeMo).\n Model checkpoints are to be found under [the NVIDIA organization](https://huggingface.co/nvidia/models?search=parakeet).\n@@ -189,7 +190,7 @@ outputs.loss.backward()\n \n ## ParakeetTokenizerFast\n \n-[[autodoc]] ParakeetTokenizerFast \n+[[autodoc]] ParakeetTokenizerFast\n \n ## ParakeetFeatureExtractor\n \n@@ -205,11 +206,11 @@ outputs.loss.backward()\n \n ## ParakeetEncoderConfig\n \n-[[autodoc]] ParakeetEncoderConfig \n+[[autodoc]] ParakeetEncoderConfig\n \n ## ParakeetCTCConfig\n \n-[[autodoc]] ParakeetCTCConfig \n+[[autodoc]] ParakeetCTCConfig\n \n ## ParakeetEncoder\n \n@@ -218,4 +219,3 @@ outputs.loss.backward()\n ## ParakeetForCTC\n \n [[autodoc]] ParakeetForCTC\n-"
        },
        {
            "sha": "4a9ddef46416c7baea592d69b33f290ab8503f43",
            "filename": "docs/source/en/model_doc/patchtsmixer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtsmixer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtsmixer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtsmixer.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -89,4 +89,4 @@ The model can also be used for time series classification and time series regres\n ## PatchTSMixerForRegression\n \n [[autodoc]] PatchTSMixerForRegression\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "7394e26b5b9800f59c08409ede935fa7a9f785b9",
            "filename": "docs/source/en/model_doc/phimoe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -45,6 +45,7 @@ The original code for PhiMoE can be found [here](https://huggingface.co/microsof\n <Tip warning={true}>\n \n Phi-3.5-MoE-instruct has been integrated in the development version (4.44.2.dev) of `transformers`. Until the official version is released through `pip`, ensure that you are doing the following:\n+\n * When loading the model, ensure that `trust_remote_code=True` is passed as an argument of the `from_pretrained()` function.\n \n The current `transformers` version can be verified with: `pip list | grep transformers`."
        },
        {
            "sha": "412d2c2fef9558c5817e5543fe082e34f5264d74",
            "filename": "docs/source/en/model_doc/pix2struct.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -79,4 +79,4 @@ The original code can be found [here](https://github.com/google-research/pix2str\n ## Pix2StructForConditionalGeneration\n \n [[autodoc]] Pix2StructForConditionalGeneration\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "b3459299437e4c6e961939104cdef4768b45a6fc",
            "filename": "docs/source/en/model_doc/plbart.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fplbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fplbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fplbart.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -120,4 +120,4 @@ it's passed with the `text_target` keyword argument.\n ## PLBartForCausalLM\n \n [[autodoc]] PLBartForCausalLM\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "c934d8789037f368d991b98bf583f077f1a995da",
            "filename": "docs/source/en/model_doc/pop2piano.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fpop2piano.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fpop2piano.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpop2piano.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -59,6 +59,7 @@ pip install pretty-midi==0.2.9 essentia==2.1b6.dev1034 librosa scipy\n ```\n \n Please note that you may need to restart your runtime after installation.\n+\n * Pop2Piano is an Encoder-Decoder based model like T5.\n * Pop2Piano can be used to generate midi-audio files for a given audio sequence.\n * Choosing different composers in `Pop2PianoForConditionalGeneration.generate()` can lead to variety of different results."
        },
        {
            "sha": "d4b6f4cc259884a6b4bbe882ec7774b1101edfb4",
            "filename": "docs/source/en/model_doc/prompt_depth_anything.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fprompt_depth_anything.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fprompt_depth_anything.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fprompt_depth_anything.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -99,4 +99,4 @@ If you are interested in submitting a resource to be included here, please feel\n \n [[autodoc]] PromptDepthAnythingImageProcessorFast\n     - preprocess\n-    - post_process_depth_estimation\n\\ No newline at end of file\n+    - post_process_depth_estimation"
        },
        {
            "sha": "62b52e3d6d5e044d736f9c5669ae4109b84daf19",
            "filename": "docs/source/en/model_doc/qwen3_next.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_next.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -19,6 +19,7 @@ rendered properly in your Markdown viewer.\n \n The Qwen3-Next series represents our next-generation foundation models, optimized for extreme context length and large-scale parameter efficiency.\n The series introduces a suite of architectural innovations designed to maximize performance while minimizing computational cost:\n+\n - **Hybrid Attention**: Replaces standard attention with the combination of **Gated DeltaNet** and **Gated Attention**, enabling efficient context modeling.\n - **High-Sparsity MoE**: Achieves an extreme low activation ratio as 1:50 in MoE layers ‚Äî drastically reducing FLOPs per token while preserving model capacity.\n - **Multi-Token Prediction(MTP)**: Boosts pretraining model performance, and accelerates inference."
        },
        {
            "sha": "9b5d64fedbb712eb8cad596cbbf0e3cc44b4b7d1",
            "filename": "docs/source/en/model_doc/rwkv.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Frwkv.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Frwkv.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frwkv.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -152,4 +152,4 @@ $$D_{i} = e^{u + K_{i} - q} + e^{M_{i}} \\tilde{D}_{i} \\hbox{  where  } q = \\max(\n \n which finally gives us\n \n-$$O_{i} = \\sigma(R_{i}) \\frac{N_{i}}{D_{i}}$$\n\\ No newline at end of file\n+$$O_{i} = \\sigma(R_{i}) \\frac{N_{i}}{D_{i}}$$"
        },
        {
            "sha": "4a32199243abe744c87f1f13a682f3e628e634d2",
            "filename": "docs/source/en/model_doc/seamless_m4t_v2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -139,13 +139,15 @@ The architecture of this new version differs from the first in a few aspects:\n #### Improvements on the second-pass model\n \n The second seq2seq model, named text-to-unit model, is now non-auto regressive, meaning that it computes units in a **single forward pass**. This achievement is made possible by:\n+\n - the use of **character-level embeddings**, meaning that each character of the predicted translated text has its own embeddings, which are then used to predict the unit tokens.\n - the use of an intermediate duration predictor, that predicts speech duration at the **character-level** on the predicted translated text.\n - the use of a new text-to-unit decoder mixing convolutions and self-attention to handle longer context.\n \n #### Difference in the speech encoder\n \n The speech encoder, which is used during the first-pass generation process to predict the translated text, differs mainly from the previous speech encoder through these mechanisms:\n+\n - the use of chunked attention mask to prevent attention across chunks, ensuring that each position attends only to positions within its own chunk and a fixed number of previous chunks.\n - the use of relative position embeddings which only considers distance between sequence elements rather than absolute positions. Please refer to [Self-Attentionwith Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155) for more details.\n - the use of a causal depth-wise convolution instead of a non-causal one."
        },
        {
            "sha": "356b0f7abcf6f6b70a38cfdc6720a38f0cf7d534",
            "filename": "docs/source/en/model_doc/seggpt.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fseggpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fseggpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseggpt.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -30,6 +30,7 @@ The abstract from the paper is the following:\n *We present SegGPT, a generalist model for segmenting everything in context. We unify various segmentation tasks into a generalist in-context learning framework that accommodates different kinds of segmentation data by transforming them into the same format of images. The training of SegGPT is formulated as an in-context coloring problem with random color mapping for each data sample. The objective is to accomplish diverse tasks according to the context, rather than relying on specific colors. After training, SegGPT can perform arbitrary segmentation tasks in images or videos via in-context inference, such as object instance, stuff, part, contour, and text. SegGPT is evaluated on a broad range of tasks, including few-shot semantic segmentation, video object segmentation, semantic segmentation, and panoptic segmentation. Our results show strong capabilities in segmenting in-domain and out-of*\n \n Tips:\n+\n - One can use [`SegGptImageProcessor`] to prepare image input, prompt and mask to the model.\n - One can either use segmentation maps or RGB images as prompt masks. If using the latter make sure to set `do_convert_rgb=False` in the `preprocess` method.\n - It's highly advisable to pass `num_labels` when using `segmentation_maps` (not considering background) during preprocessing and postprocessing with [`SegGptImageProcessor`] for your use case."
        },
        {
            "sha": "6a67c2d61b5aebec392ea5a584617591829bc0b9",
            "filename": "docs/source/en/model_doc/shieldgemma2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -22,9 +22,9 @@ rendered properly in your Markdown viewer.\n \n The ShieldGemma 2 model was proposed in a [technical report](https://huggingface.co/papers/2504.01081) by Google. ShieldGemma 2, built on [Gemma 3](https://ai.google.dev/gemma/docs/core/model_card_3), is a 4 billion (4B) parameter model that checks the safety of both synthetic and natural images against key categories to help you build robust datasets and models. With this addition to the Gemma family of models, researchers and developers can now easily minimize the risk of harmful content in their models across key areas of harm as defined below:\n \n--   No Sexually Explicit content: The image shall not contain content that depicts explicit or graphic sexual acts (e.g., pornography, erotic nudity, depictions of rape or sexual assault).\n--   No Dangerous Content: The image shall not contain content that facilitates or encourages activities that could cause real-world harm (e.g., building firearms and explosive devices, promotion of terrorism, instructions for suicide).\n--   No Violence/Gore content: The image shall not contain content that depicts shocking, sensational, or gratuitous violence (e.g., excessive blood and gore, gratuitous violence against animals, extreme injury or moment of death).\n+- No Sexually Explicit content: The image shall not contain content that depicts explicit or graphic sexual acts (e.g., pornography, erotic nudity, depictions of rape or sexual assault).\n+- No Dangerous Content: The image shall not contain content that facilitates or encourages activities that could cause real-world harm (e.g., building firearms and explosive devices, promotion of terrorism, instructions for suicide).\n+- No Violence/Gore content: The image shall not contain content that depicts shocking, sensational, or gratuitous violence (e.g., excessive blood and gore, gratuitous violence against animals, extreme injury or moment of death).\n \n We recommend using ShieldGemma 2 as an input filter to vision language models, or as an output filter of image generation systems. To train a robust image safety model, we curated training datasets of natural and synthetic images and instruction-tuned Gemma 3 to demonstrate strong performance.\n "
        },
        {
            "sha": "2b65da80defe25c8b579898714236a8dc878cd97",
            "filename": "docs/source/en/model_doc/superglue.md",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -143,13 +143,11 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n ## SuperGlueImageProcessor\n \n [[autodoc]] SuperGlueImageProcessor\n-\n-- preprocess\n-- post_process_keypoint_matching\n-- visualize_keypoint_matching\n+    - preprocess\n+    - post_process_keypoint_matching\n+    - visualize_keypoint_matching\n \n ## SuperGlueForKeypointMatching\n \n [[autodoc]] SuperGlueForKeypointMatching\n-\n-- forward\n+    - forward"
        },
        {
            "sha": "3efd5ecf90f21cc6a41c961421aea7c9e164ec6b",
            "filename": "docs/source/en/model_doc/superpoint.md",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -129,16 +129,15 @@ processed_outputs = processor.post_process_keypoint_detection(outputs, [image_si\n ## SuperPointImageProcessor\n \n [[autodoc]] SuperPointImageProcessor\n-\n-- preprocess\n+    - preprocess\n \n ## SuperPointImageProcessorFast\n \n [[autodoc]] SuperPointImageProcessorFast\n-- preprocess\n-- post_process_keypoint_detection\n+    - preprocess\n+    - post_process_keypoint_detection\n \n ## SuperPointForKeypointDetection\n \n [[autodoc]] SuperPointForKeypointDetection\n-- forward\n+    - forward"
        },
        {
            "sha": "09c624c7fb7e153fac0cfe5c2044277b35204089",
            "filename": "docs/source/en/model_doc/tapas.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapas.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapas.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapas.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -30,6 +30,7 @@ token types that encode tabular structure. TAPAS is pre-trained on the masked la\n millions of tables from English Wikipedia and corresponding texts.\n \n For question answering, TAPAS has 2 heads on top: a cell selection head and an aggregation head, for (optionally) performing aggregations (such as counting or summing) among selected cells. TAPAS has been fine-tuned on several datasets:\n+\n - [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) (Sequential Question Answering by Microsoft)\n - [WTQ](https://github.com/ppasupat/WikiTableQuestions) (Wiki Table Questions by Stanford University)\n - [WikiSQL](https://github.com/salesforce/WikiSQL) (by Salesforce)."
        },
        {
            "sha": "606d8940c4edbaccfbea9daee5e9b85973875357",
            "filename": "docs/source/en/model_doc/tapex.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapex.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -37,6 +37,7 @@ Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. TAPE\n which it can be fine-tuned to answer natural language questions related to tabular data, as well as performing table fact checking.\n \n TAPEX has been fine-tuned on several datasets:\n+\n - [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) (Sequential Question Answering by Microsoft)\n - [WTQ](https://github.com/ppasupat/WikiTableQuestions) (Wiki Table Questions by Stanford University)\n - [WikiSQL](https://github.com/salesforce/WikiSQL) (by Salesforce)"
        },
        {
            "sha": "36a68af80ca8edc7225872167bcafd517cdb5f99",
            "filename": "docs/source/en/model_doc/time_series_transformer.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Ftime_series_transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Ftime_series_transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftime_series_transformer.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -35,16 +35,16 @@ point forecasting model. This means that the model learns a distribution, from w\n and a decoder, which predicts a `prediction_length` of time series values into the future (called `future_values`). During training, one needs to provide\n pairs of (`past_values` and `future_values`) to the model.\n - In addition to the raw (`past_values` and `future_values`), one typically provides additional features to the model. These can be the following:\n-    - `past_time_features`: temporal features which the model will add to `past_values`. These serve as \"positional encodings\" for the Transformer encoder.\n+  - `past_time_features`: temporal features which the model will add to `past_values`. These serve as \"positional encodings\" for the Transformer encoder.\n     Examples are \"day of the month\", \"month of the year\", etc. as scalar values (and then stacked together as a vector).\n     e.g. if a given time-series value was obtained on the 11th of August, then one could have [11, 8] as time feature vector (11 being \"day of the month\", 8 being \"month of the year\").\n-    - `future_time_features`: temporal features which the model will add to `future_values`. These serve as \"positional encodings\" for the Transformer decoder.\n+  - `future_time_features`: temporal features which the model will add to `future_values`. These serve as \"positional encodings\" for the Transformer decoder.\n     Examples are \"day of the month\", \"month of the year\", etc. as scalar values (and then stacked together as a vector).\n     e.g. if a given time-series value was obtained on the 11th of August, then one could have [11, 8] as time feature vector (11 being \"day of the month\", 8 being \"month of the year\").\n-    - `static_categorical_features`: categorical features which are static over time (i.e., have the same value for all `past_values` and `future_values`).\n+  - `static_categorical_features`: categorical features which are static over time (i.e., have the same value for all `past_values` and `future_values`).\n     An example here is the store ID or region ID that identifies a given time-series.\n     Note that these features need to be known for ALL data points (also those in the future).\n-    - `static_real_features`: real-valued features which are static over time (i.e., have the same value for all `past_values` and `future_values`).\n+  - `static_real_features`: real-valued features which are static over time (i.e., have the same value for all `past_values` and `future_values`).\n     An example here is the image representation of the product for which you have the time-series values (like the [ResNet](resnet) embedding of a \"shoe\" picture,\n     if your time-series is about the sales of shoes).\n     Note that these features need to be known for ALL data points (also those in the future)."
        },
        {
            "sha": "1d87158d72e17be48da004697321f8c24aa8fa23",
            "filename": "docs/source/en/model_doc/timesformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimesformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimesformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimesformer.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -54,4 +54,4 @@ the number of input frames per clip changes based on the model size so you shoul\n ## TimesformerForVideoClassification\n \n [[autodoc]] TimesformerForVideoClassification\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "cc370accf3e3da90eff2a0c5f5ce3a589b7c71ae",
            "filename": "docs/source/en/model_doc/udop.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fudop.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fudop.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fudop.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -115,4 +115,4 @@ to fine-tune UDOP on a custom dataset as well as inference. üåé\n ## UdopEncoderModel\n \n [[autodoc]] UdopEncoderModel\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "4329846ab7f95514e16d00fad01d3638427f3673",
            "filename": "docs/source/en/model_doc/univnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Funivnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Funivnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Funivnet.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -81,4 +81,4 @@ To the best of my knowledge, there is no official code release, but an unofficia\n ## UnivNetModel\n \n [[autodoc]] UnivNetModel\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "900b5635fc162bb983de786523780e9e55c15770",
            "filename": "docs/source/en/model_doc/upernet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fupernet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fupernet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fupernet.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -81,4 +81,4 @@ If you're interested in submitting a resource to be included here, please feel f\n ## UperNetForSemanticSegmentation\n \n [[autodoc]] UperNetForSemanticSegmentation\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "eb02fc48bb40bf2d100a8332591e2e32ca0897dc",
            "filename": "docs/source/en/model_doc/videomae.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -75,6 +75,7 @@ you're interested in submitting a resource to be included here, please feel free\n review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n \n **Video classification**\n+\n - [A notebook](https://github.com/huggingface/notebooks/blob/main/examples/video_classification.ipynb) that shows how\n to fine-tune a VideoMAE model on a custom dataset.\n - [Video classification task guide](../tasks/video_classification)"
        },
        {
            "sha": "0547594ae118d50e1a8a756ddc6a7d1f6a9a070c",
            "filename": "docs/source/en/model_doc/vit_mae.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -66,6 +66,7 @@ reconstruction = outputs.logits\n </hfoptions>\n \n ## Notes\n+\n - ViTMAE is typically used in two stages. Self-supervised pretraining with [`ViTMAEForPreTraining`], and then discarding the decoder and fine-tuning the encoder. After fine-tuning, the weights can be plugged into a model like [`ViTForImageClassification`].\n - Use [`ViTImageProcessor`] for input preparation.\n "
        },
        {
            "sha": "a1250f1bb909e5bc80b38619655306b3a5d586be",
            "filename": "docs/source/en/model_doc/vitdet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitdet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitdet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitdet.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -40,4 +40,4 @@ Tips:\n ## VitDetModel\n \n [[autodoc]] VitDetModel\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "0584df8e67a5d5efb0e067d7617e0b9893cb4f28",
            "filename": "docs/source/en/model_doc/vitmatte.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitmatte.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitmatte.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitmatte.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -62,4 +62,4 @@ The model expects both the image and trimap (concatenated) as input. Use [`ViTMa\n ## VitMatteForImageMatting\n \n [[autodoc]] VitMatteForImageMatting\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "96dc93892470541f985fd7c5f1e78e60e748bddd",
            "filename": "docs/source/en/model_doc/vits.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvits.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvits.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvits.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -149,10 +149,10 @@ Audio(waveform, rate=model.config.sampling_rate)\n ## VitsTokenizer\n \n [[autodoc]] VitsTokenizer\n-- __call__\n-- save_vocabulary\n+    - __call__\n+    - save_vocabulary\n \n ## VitsModel\n \n [[autodoc]] VitsModel\n-- forward\n+    - forward"
        },
        {
            "sha": "3dd2fc9e0d31fb3e7072de36b47751a14c15b099",
            "filename": "docs/source/en/model_doc/voxtral.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -22,6 +22,7 @@ Voxtral is an upgrade of [Ministral 3B and Mistral Small 3B](https://mistral.ai/\n You can read more in Mistral's [realease blog post](https://mistral.ai/news/voxtral).\n \n The model is available in two checkpoints:\n+\n - 3B: [mistralai/Voxtral-Mini-3B-2507](https://huggingface.co/mistralai/Voxtral-Mini-3B-2507)\n - 24B: [mistralai/Voxtral-Small-24B-2507](https://huggingface.co/mistralai/Voxtral-Small-24B-2507)\n "
        },
        {
            "sha": "206ea048c023b9894c1ef21556d57ba9cd178229",
            "filename": "docs/source/en/model_doc/wav2vec2_phoneme.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2_phoneme.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2_phoneme.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2_phoneme.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -63,7 +63,7 @@ except for the tokenizer.\n ## Wav2Vec2PhonemeCTCTokenizer\n \n [[autodoc]] Wav2Vec2PhonemeCTCTokenizer\n-\t- __call__\n-\t- batch_decode\n-\t- decode\n-\t- phonemize\n+    - __call__\n+    - batch_decode\n+    - decode\n+    - phonemize"
        },
        {
            "sha": "957a740934849713b9bfe14eb30b21983a5b316f",
            "filename": "docs/source/en/model_doc/xcodec.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fxcodec.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fxcodec.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxcodec.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -36,6 +36,7 @@ The abstract of the paper states the following:\n *Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation.*\n \n Model cards:\n+\n - [xcodec-hubert-librispeech](https://huggingface.co/hf-audio/xcodec-hubert-librispeech) (for speech)\n - [xcodec-wavlm-mls](https://huggingface.co/hf-audio/xcodec-wavlm-mls) (for speech)\n - [xcodec-wavlm-more-data](https://huggingface.co/hf-audio/xcodec-wavlm-more-data) (for speech)\n@@ -97,4 +98,4 @@ sf.write(\"reconstruction.wav\", reconstruction.T, sampling_rate)\n [[autodoc]] XcodecModel\n     - decode\n     - encode\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "624b7ebb2d235c2bff1d70ea05d71880be82ff3e",
            "filename": "docs/source/en/model_doc/xmod.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fxmod.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fxmod.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxmod.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -36,6 +36,7 @@ The original code can be found [here](https://github.com/facebookresearch/fairse\n ## Usage tips\n \n Tips:\n+\n - X-MOD is similar to [XLM-R](xlm-roberta), but a difference is that the input language needs to be specified so that the correct language adapter can be activated.\n - The main models ‚Äì base and large ‚Äì have adapters for 81 languages.\n \n@@ -44,6 +45,7 @@ Tips:\n ### Input language\n \n There are two ways to specify the input language:\n+\n 1. By setting a default language before using the model:\n \n ```python"
        },
        {
            "sha": "4a75b2ed020ffd9d68735859baad38251e73dbc3",
            "filename": "docs/source/en/model_doc/yolos.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -97,6 +97,7 @@ for score, label, box in zip(filtered_scores, filtered_labels, pixel_boxes):\n </hfoptions>\n \n ## Notes\n+\n - Use [`YolosImageProcessor`] for preparing images (and optional targets) for the model. Contrary to [DETR](./detr), YOLOS doesn't require a `pixel_mask`.\n \n ## Resources"
        },
        {
            "sha": "211b0dcf80915a2554771c1cf56ecf7e744acae4",
            "filename": "docs/source/en/model_doc/yoso.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fyoso.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fyoso.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fyoso.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -99,4 +99,4 @@ alt=\"drawing\" width=\"600\"/>\n ## YosoForQuestionAnswering\n \n [[autodoc]] YosoForQuestionAnswering\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "847f0532e2a769a1845d2d0b956814cd8d43de09",
            "filename": "docs/source/en/model_doc/zamba.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -69,6 +69,7 @@ print(tokenizer.decode(outputs[0]))\n ## Model card\n \n The model cards can be found at:\n+\n * [Zamba-7B](https://huggingface.co/Zyphra/Zamba-7B-v1)\n \n ## Issues"
        },
        {
            "sha": "c9d3d3d1de75d4756dcbad211d86b8b8345ecc83",
            "filename": "docs/source/en/model_doc/zamba2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba2.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -61,6 +61,7 @@ print(tokenizer.decode(outputs[0]))\n ## Model card\n \n The model cards can be found at:\n+\n * [Zamba2-1.2B](https://huggingface.co/Zyphra/Zamba2-1.2B)\n * [Zamba2-2.7B](https://huggingface.co/Zyphra/Zamba2-2.7B)\n * [Zamba2-7B](https://huggingface.co/Zyphra/Zamba2-7B)"
        },
        {
            "sha": "92840a770462667f7d34b3c6b9ac3fbbc3f5dfee",
            "filename": "docs/source/en/model_doc/zoedepth.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -109,6 +109,7 @@ Image.fromarray(depth.astype(\"uint8\"))\n    ```\n \n ## Resources\n+\n - Refer to this [notebook](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ZoeDepth) for an inference example.\n \n ## ZoeDepthConfig"
        },
        {
            "sha": "2946fc95f1453b0c22cd4e85564278431f159ede",
            "filename": "docs/source/en/open_webui.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fopen_webui.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fopen_webui.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fopen_webui.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -9,6 +9,7 @@ transformers serve --enable-cors\n ```\n \n Before you can speak into Open WebUI, you need to update its settings to use your server for speech to text (STT) tasks. Launch Open WebUI, and navigate to the audio tab inside the admin settings. If you're using Open WebUI with the default ports, [this link (default)](http://localhost:3000/admin/settings/audio) or [this link (python deployment)](http://localhost:8080/admin/settings/audio) will take you there. Do the following changes there:\n+\n 1. Change the type of \"Speech-to-Text Engine\" to \"OpenAI\";\n 2. Update the address to your server's address -- `http://localhost:8000/v1` by default;\n 3. Type your model of choice into the \"STT Model\" field, e.g. `openai/whisper-large-v3` ([available models](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=trending))."
        },
        {
            "sha": "45b2509e86deb2db07cd0b535736804fe50b6c98",
            "filename": "docs/source/en/pad_truncation.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fpad_truncation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fpad_truncation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fpad_truncation.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -22,25 +22,25 @@ In most cases, padding your batch to the length of the longest sequence and trun\n \n The `padding` argument controls padding. It can be a boolean or a string:\n \n-  - `True` or `'longest'`: pad to the longest sequence in the batch (no padding is applied if you only provide\n+- `True` or `'longest'`: pad to the longest sequence in the batch (no padding is applied if you only provide\n     a single sequence).\n-  - `'max_length'`: pad to a length specified by the `max_length` argument or the maximum length accepted\n+- `'max_length'`: pad to a length specified by the `max_length` argument or the maximum length accepted\n     by the model if no `max_length` is provided (`max_length=None`). Padding will still be applied if you only provide a single sequence.\n-  - `False` or `'do_not_pad'`: no padding is applied. This is the default behavior.\n+- `False` or `'do_not_pad'`: no padding is applied. This is the default behavior.\n \n The `truncation` argument controls truncation. It can be a boolean or a string:\n \n-  - `True` or `'longest_first'`: truncate to a maximum length specified by the `max_length` argument or\n+- `True` or `'longest_first'`: truncate to a maximum length specified by the `max_length` argument or\n     the maximum length accepted by the model if no `max_length` is provided (`max_length=None`). This will\n     truncate token by token, removing a token from the longest sequence in the pair until the proper length is\n     reached.\n-  - `'only_second'`: truncate to a maximum length specified by the `max_length` argument or the maximum\n+- `'only_second'`: truncate to a maximum length specified by the `max_length` argument or the maximum\n     length accepted by the model if no `max_length` is provided (`max_length=None`). This will only truncate\n     the second sentence of a pair if a pair of sequences (or a batch of pairs of sequences) is provided.\n-  - `'only_first'`: truncate to a maximum length specified by the `max_length` argument or the maximum\n+- `'only_first'`: truncate to a maximum length specified by the `max_length` argument or the maximum\n     length accepted by the model if no `max_length` is provided (`max_length=None`). This will only truncate\n     the first sentence of a pair if a pair of sequences (or a batch of pairs of sequences) is provided.\n-  - `False` or `'do_not_truncate'`: no truncation is applied. This is the default behavior.\n+- `False` or `'do_not_truncate'`: no truncation is applied. This is the default behavior.\n \n The `max_length` argument controls the length of the padding and truncation. It can be an integer or `None`, in which case it will default to the maximum length the model can accept. If the model has no specific maximum input length, truncation or padding to `max_length` is deactivated.\n "
        },
        {
            "sha": "e98b1fa57bd955edaef5d5b2d138736d84fc83c8",
            "filename": "docs/source/en/philosophy.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fphilosophy.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fphilosophy.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fphilosophy.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -26,24 +26,24 @@ The library was designed with two strong goals in mind:\n \n 1. Be as easy and fast to use as possible:\n \n-  - We strongly limited the number of user-facing abstractions to learn, in fact, there are almost no abstractions,\n+- We strongly limited the number of user-facing abstractions to learn, in fact, there are almost no abstractions,\n     just three standard classes required to use each model: [configuration](main_classes/configuration),\n     [models](main_classes/model), and a preprocessing class ([tokenizer](main_classes/tokenizer) for NLP, [image processor](main_classes/image_processor) for vision, [feature extractor](main_classes/feature_extractor) for audio, and [processor](main_classes/processors) for multimodal inputs).\n-  - All of these classes can be initialized in a simple and unified way from pretrained instances by using a common\n+- All of these classes can be initialized in a simple and unified way from pretrained instances by using a common\n     `from_pretrained()` method which downloads (if needed), caches and\n     loads the related class instance and associated data (configurations' hyperparameters, tokenizers' vocabulary,\n     and models' weights) from a pretrained checkpoint provided on [Hugging Face Hub](https://huggingface.co/models) or your own saved checkpoint.\n-  - On top of those three base classes, the library provides two APIs: [`pipeline`] for quickly\n+- On top of those three base classes, the library provides two APIs: [`pipeline`] for quickly\n     using a model for inference on a given task and [`Trainer`] to quickly train or fine-tune a PyTorch model.\n-  - As a consequence, this library is NOT a modular toolbox of building blocks for neural nets. If you want to\n+- As a consequence, this library is NOT a modular toolbox of building blocks for neural nets. If you want to\n     extend or build upon the library, just use regular Python or PyTorch and inherit from the base\n     classes of the library to reuse functionalities like model loading and saving. If you'd like to learn more about our coding philosophy for models, check out our [Repeat Yourself](https://huggingface.co/blog/transformers-design-philosophy) blog post.\n \n 2. Provide state-of-the-art models with performances as close as possible to the original models:\n \n-  - We provide at least one example for each architecture which reproduces a result provided by the official authors\n+- We provide at least one example for each architecture which reproduces a result provided by the official authors\n     of said architecture.\n-  - The code is usually as close to the original code base as possible which means some PyTorch code may be not as\n+- The code is usually as close to the original code base as possible which means some PyTorch code may be not as\n     *pytorchic* as it could be as a result of being converted from other Deep Learning frameworks.\n \n A few other goals:"
        },
        {
            "sha": "b53bcc8bd1840f67f230db667398b30286cd4d8f",
            "filename": "docs/source/en/pipeline_gradio.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fpipeline_gradio.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fpipeline_gradio.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fpipeline_gradio.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -45,8 +45,8 @@ gr.Interface.from_pipeline(pipeline).launch(share=True)\n The Space below is created with the code above and hosted on Spaces.\n \n <iframe\n-\tsrc=\"https://stevhliu-gradio-pipeline-demo.hf.space\"\n-\tframeborder=\"0\"\n-\twidth=\"850\"\n-\theight=\"850\"\n+ src=\"https://stevhliu-gradio-pipeline-demo.hf.space\"\n+ frameborder=\"0\"\n+ width=\"850\"\n+ height=\"850\"\n ></iframe>"
        },
        {
            "sha": "5fdbbbab05bc2fc0536f8a0690ef33566a997b71",
            "filename": "docs/source/en/pr_checks.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fpr_checks.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fpr_checks.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fpr_checks.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -21,6 +21,7 @@ rendered properly in your Markdown viewer.\n # Checks on a Pull Request\n \n When you open a pull request on ü§ó Transformers, a fair number of checks will be run to make sure the patch you are adding is not breaking anything existing. Those checks are of four types:\n+\n - regular tests\n - documentation build\n - code and documentation style\n@@ -194,6 +195,7 @@ Another way when the patterns are just different casings of the same replacement\n ```\n \n In this case, the code is copied from `BertForSequenceClassification` by replacing:\n+\n - `Bert` by `MobileBert` (for instance when using `MobileBertModel` in the init)\n - `bert` by `mobilebert` (for instance when defining `self.mobilebert`)\n - `BERT` by `MOBILEBERT` (in the constant `MOBILEBERT_INPUTS_DOCSTRING`)"
        },
        {
            "sha": "df3a2bdc6f2aa693c76aae7f7bd2b8867604b860",
            "filename": "docs/source/en/quantization/concept_guide.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fquantization%2Fconcept_guide.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fquantization%2Fconcept_guide.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fconcept_guide.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -20,9 +20,9 @@ Quantization reduces the memory footprint and computational cost of large machin\n \n Reducing a model's precision offers several significant benefits:\n \n--  Smaller model size: Lower-precision data types require less storage space. An int8 model, for example, is roughly 4 times smaller than its float32 counterpart.\n--  Faster inference: Operations on lower-precision data types, especially integers, can be significantly faster on compatible hardware (CPUs and GPUs often have specialized instructions for int8 operations). This leads to lower latency.\n--  Reduced energy consumption: Faster computations and smaller memory transfers often translate to lower power usage.\n+- Smaller model size: Lower-precision data types require less storage space. An int8 model, for example, is roughly 4 times smaller than its float32 counterpart.\n+- Faster inference: Operations on lower-precision data types, especially integers, can be significantly faster on compatible hardware (CPUs and GPUs often have specialized instructions for int8 operations). This leads to lower latency.\n+- Reduced energy consumption: Faster computations and smaller memory transfers often translate to lower power usage.\n \n The primary trade-off in quantization is *efficiency* vs. *accuracy*. Reducing precision saves resources but inevitably introduces small errors (quantization noise). The goal is to minimize this error using appropriate schemes (affine/symmetric), granularity (per-tensor/channel), and techniques (PTQ/QAT) so that the model's performance on its target task degrades as little as possible.\n \n@@ -171,4 +171,4 @@ To explore quantization and related performance optimization concepts more deepl\n - [Introduction to Quantization cooked in ü§ó with üíóüßë‚Äçüç≥](https://huggingface.co/blog/merve/quantization)\n - [EfficientML.ai Lecture 5 - Quantization Part I](https://www.youtube.com/watch?v=RP23-dRVDWM)\n - [Making Deep Learning Go Brrrr From First Principles](https://horace.io/brrr_intro.html)\n-- [Accelerating Generative AI with PyTorch Part 2: LLM Optimizations](https://pytorch.org/blog/accelerating-generative-ai-2/)\n\\ No newline at end of file\n+- [Accelerating Generative AI with PyTorch Part 2: LLM Optimizations](https://pytorch.org/blog/accelerating-generative-ai-2/)"
        },
        {
            "sha": "1afd1505029b7174ba3a09bc90f8cfb6695a9b08",
            "filename": "docs/source/en/quantization/finegrained_fp8.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fquantization%2Ffinegrained_fp8.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fquantization%2Ffinegrained_fp8.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ffinegrained_fp8.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -59,4 +59,4 @@ Use [`~PreTrainedModel.save_pretrained`] to save the quantized model and reload\n quant_path = \"/path/to/save/quantized/model\"\n model.save_pretrained(quant_path)\n model = AutoModelForCausalLM.from_pretrained(quant_path, device_map=\"auto\")\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "f58f93025f45ff86d2bd3fc963ea8be35e9ade55",
            "filename": "docs/source/en/quantization/quanto.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fquantization%2Fquanto.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fquantization%2Fquanto.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fquanto.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -66,4 +66,4 @@ model = torch.compile(model)\n \n Read the [Quanto: a PyTorch quantization backend for Optimum](https://huggingface.co/blog/quanto-introduction) blog post to learn more about the library design and benchmarks.\n \n-For more hands-on examples, take a look at the Quanto [notebook](https://colab.research.google.com/drive/16CXfVmtdQvciSh9BopZUDYcmXCDpvgrT?usp=sharing).\n\\ No newline at end of file\n+For more hands-on examples, take a look at the Quanto [notebook](https://colab.research.google.com/drive/16CXfVmtdQvciSh9BopZUDYcmXCDpvgrT?usp=sharing)."
        },
        {
            "sha": "49502c15b6c61c49f0219adf32adc0eaba4130c7",
            "filename": "docs/source/en/quantization/selecting.md",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fquantization%2Fselecting.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fquantization%2Fselecting.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fselecting.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -112,9 +112,9 @@ Consider the quantization method below during fine-tuning to save memory.\n \n ### bitsandbytes[[training]]\n \n-*   **Description:** The standard method for QLoRA fine-tuning via PEFT.\n-*   **Pros:** Enables fine-tuning large models on consumer GPUs; widely supported and documented for PEFT.\n-*   **Cons:** Primarily for NVIDIA GPUs.\n+* **Description:** The standard method for QLoRA fine-tuning via PEFT.\n+* **Pros:** Enables fine-tuning large models on consumer GPUs; widely supported and documented for PEFT.\n+* **Cons:** Primarily for NVIDIA GPUs.\n \n Other methods offer PEFT compatibility, though bitsandbytes is the most established and straightforward path for QLoRA.\n \n@@ -124,10 +124,10 @@ See the [bitsandbytes documentation](./bitsandbytes#qlora) and [PEFT Docs](https\n \n Methods like [AQLM](./aqlm), [SpQR](./spqr), [VPTQ](./vptq), [HIGGS](./higgs), etc., push the boundaries of compression (< 2-bit) or explore novel techniques.\n \n-*   Consider these if:\n-    *   You need extreme compression (sub-4-bit).\n-    *   You are conducting research or require state-of-the-art results from their respective papers.\n-    *   You have significant compute resources available for potentially complex quantization procedures.\n+* Consider these if:\n+  * You need extreme compression (sub-4-bit).\n+  * You are conducting research or require state-of-the-art results from their respective papers.\n+  * You have significant compute resources available for potentially complex quantization procedures.\n We recommend consulting each methods documentation and associated papers carefully before choosing one for use in production.\n \n ## Benchmark Comparison\n@@ -154,4 +154,4 @@ The key takeaways are:\n | **Sub-4-bit** (VPTQ, AQLM, 2-bit GPTQ) | Extreme (>4x)            | Noticeable drop, especially at 2-bit | Quantization times can be very long (AQLM, VPTQ). Performance varies. |\n \n > [!TIP]\n-> Always benchmark the performance (accuracy and speed) of the quantized model on your specific task and hardware to ensure it meets your requirements. Refer to the individual documentation pages linked above for detailed usage instructions.\n\\ No newline at end of file\n+> Always benchmark the performance (accuracy and speed) of the quantized model on your specific task and hardware to ensure it meets your requirements. Refer to the individual documentation pages linked above for detailed usage instructions."
        },
        {
            "sha": "4287c5d2d5ec6b88e694b18a9d3508a3906348b2",
            "filename": "docs/source/en/serving.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fserving.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Fserving.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserving.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -37,6 +37,7 @@ In this document, we dive into the different supported endpoints and modalities;\n You can serve models of diverse modalities supported by `transformers` with the `transformers serve` CLI. It spawns a local server that offers compatibility with the OpenAI SDK, which is the _de facto_ standard for LLM conversations and other related tasks. This way, you can use the server from many third party applications, or test it using the `transformers chat` CLI ([docs](conversations#chat-cli)).\n \n The server supports the following REST APIs:\n+\n - `/v1/chat/completions`\n - `/v1/responses`\n - `/v1/audio/transcriptions`"
        },
        {
            "sha": "2c729f76adcb5d20e65f01ee878859679bc2b640",
            "filename": "docs/source/en/tasks/document_question_answering.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fdocument_question_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fdocument_question_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fdocument_question_answering.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -104,6 +104,7 @@ yourself with the features.\n ```\n \n Here's what the individual fields represent:\n+\n * `id`: the example's id\n * `image`: a PIL.Image.Image object containing the document image\n * `query`: the question string - natural language asked question, in several languages\n@@ -257,6 +258,7 @@ Once examples are encoded, however, they will look like this:\n ```\n \n We'll need to find the position of the answer in the encoded input.\n+\n * `token_type_ids` tells us which tokens are part of the question, and which ones are part of the document's words.\n * `tokenizer.cls_token_id` will help find the special token at the beginning of the input.\n * `word_ids` will help match the answer found in the original `words` to the same answer in the full encoded input and determine\n@@ -365,6 +367,7 @@ of the Hugging Face course for inspiration.\n \n Congratulations! You've successfully navigated the toughest part of this guide and now you are ready to train your own model.\n Training involves the following steps:\n+\n * Load the model with [`AutoModelForDocumentQuestionAnswering`] using the same checkpoint as in the preprocessing.\n * Define your training hyperparameters in [`TrainingArguments`].\n * Define a function to batch examples together, here the [`DefaultDataCollator`] will do just fine\n@@ -465,6 +468,7 @@ document question answering with your model, and pass the image + question combi\n ```\n \n You can also manually replicate the results of the pipeline if you'd like:\n+\n 1. Take an image and a question, prepare them for the model using the processor from your model.\n 2. Forward the result or preprocessing through the model.\n 3. The model returns `start_logits` and `end_logits`, which indicate which token is at the start of the answer and"
        },
        {
            "sha": "b03c7bccd9c29d7363225356b34041efb1258079",
            "filename": "docs/source/en/tasks/idefics.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fidefics.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fidefics.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fidefics.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -36,6 +36,7 @@ being a large model means it requires significant computational resources and in\n this approach suits your use case better than fine-tuning specialized models for each individual task.\n \n In this guide, you'll learn how to:\n+\n - [Load IDEFICS](#loading-the-model) and [load the quantized version of the model](#quantized-model)\n - Use IDEFICS for:\n   - [Image captioning](#image-captioning)"
        },
        {
            "sha": "8820a534030ccc9894ad972bf8e1a9c92e3d633d",
            "filename": "docs/source/en/tasks/image_text_to_text.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -23,6 +23,7 @@ Image-text-to-text models, also known as vision language models (VLMs), are lang\n In this guide, we provide a brief overview of VLMs and show how to use them with Transformers for inference.\n \n To begin with, there are multiple types of VLMs:\n+\n - base models used for fine-tuning\n - chat fine-tuned models for conversation\n - instruction fine-tuned models"
        },
        {
            "sha": "55380e9b0d1ecaf099fa73d9a64bc40eee622a8f",
            "filename": "docs/source/en/tasks/image_to_image.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fimage_to_image.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fimage_to_image.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_to_image.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -21,6 +21,7 @@ rendered properly in your Markdown viewer.\n Image-to-Image task is the task where an application receives an image and outputs another image. This has various subtasks, including image enhancement (super resolution, low light enhancement, deraining and so on), image inpainting, and more.\n \n This guide will show you how to:\n+\n - Use an image-to-image pipeline for super resolution task,\n - Run image-to-image models for same task without a pipeline.\n "
        },
        {
            "sha": "817cb9819e7d41356ebd039fbe3385adb15c511c",
            "filename": "docs/source/en/tasks/mask_generation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -20,6 +20,7 @@ Mask generation is the task of generating semantically meaningful masks for an i\n This task is very similar to [image segmentation](semantic_segmentation), but many differences exist. Image segmentation models are trained on labeled datasets and are limited to the classes they have seen during training; they return a set of masks and corresponding classes, given an image.\n \n Mask generation models are trained on large amounts of data and operate in two modes.\n+\n - Prompting mode: In this mode, the model takes in an image and a prompt, where a prompt can be a 2D point location (XY coordinates) in the image within an object or a bounding box surrounding an object. In prompting mode, the model only returns the mask over the object\n that the prompt is pointing out.\n - Segment Everything mode: In segment everything, given an image, the model generates every mask in the image. To do so, a grid of points is generated and overlaid on the image for inference.\n@@ -34,6 +35,7 @@ SAM serves as a powerful foundation model for segmentation as it has large data\n [SA-1B](https://ai.meta.com/datasets/segment-anything/), a dataset with 1 million images and 1.1 billion masks.\n \n In this guide, you will learn how to:\n+\n - Infer in segment everything mode with batching,\n - Infer in point prompting mode,\n - Infer in box prompting mode."
        },
        {
            "sha": "619374f91daee6afbd88e387aadd4784c4430179",
            "filename": "docs/source/en/tasks/masked_language_modeling.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fmasked_language_modeling.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fmasked_language_modeling.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmasked_language_modeling.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -150,6 +150,7 @@ To apply this preprocessing function over the entire dataset, use the ü§ó Datas\n This dataset contains the token sequences, but some of these are longer than the maximum input length for the model.\n \n You can now use a second preprocessing function to\n+\n - concatenate all the sequences\n - split the concatenated sequences into shorter chunks defined by `block_size`, which should be both shorter than the maximum input length and short enough for your GPU RAM.\n "
        },
        {
            "sha": "ef2a86190bbcdbd522d1a2d9925e315f76b353ad",
            "filename": "docs/source/en/tasks/object_detection.md",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -121,6 +121,7 @@ To get familiar with the data, explore what the examples look like.\n ```\n \n The examples in the dataset have the following fields:\n+\n - `image_id`: the example image id\n - `image`: a `PIL.Image.Image` object containing the image\n - `width`: width of the image\n@@ -216,6 +217,7 @@ Instantiate the image processor from the same checkpoint as the model you want t\n ```\n \n Before passing the images to the `image_processor`, apply two preprocessing transformations to the dataset:\n+\n - Augmenting images\n - Reformatting annotations to meet DETR expectations\n \n@@ -505,6 +507,7 @@ The images in this dataset are still quite large, even after resizing. This mean\n require at least one GPU.\n \n Training involves the following steps:\n+\n 1. Load the model with [`AutoModelForObjectDetection`] using the same checkpoint as in the preprocessing.\n 2. Define your training hyperparameters in [`TrainingArguments`].\n 3. Pass the training arguments to [`Trainer`] along with the model, dataset, image processor, and data collator.\n@@ -527,9 +530,10 @@ and `id2label` maps that you created earlier from the dataset's metadata. Additi\n In the [`TrainingArguments`] use `output_dir` to specify where to save your model, then configure hyperparameters as you see fit. For `num_train_epochs=30` training will take about 35 minutes in Google Colab T4 GPU, increase the number of epoch to get better results.\n \n Important notes:\n- - Do not remove unused columns because this will drop the image column. Without the image column, you\n+\n+- Do not remove unused columns because this will drop the image column. Without the image column, you\n can't create `pixel_values`. For this reason, set `remove_unused_columns` to `False`.\n- - Set `eval_do_concat_batches=False` to get proper evaluation results. Images have different number of target boxes, if batches are concatenated we will not be able to determine which boxes belongs to particular image.\n+- Set `eval_do_concat_batches=False` to get proper evaluation results. Images have different number of target boxes, if batches are concatenated we will not be able to determine which boxes belongs to particular image.\n \n If you wish to share your model by pushing to the Hub, set `push_to_hub` to `True` (you must be signed in to Hugging\n Face to upload your model)."
        },
        {
            "sha": "de88a0af686622b2466c9eb17530c9cd725a4736",
            "filename": "docs/source/en/tasks/semantic_segmentation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -23,6 +23,7 @@ rendered properly in your Markdown viewer.\n Image segmentation models separate areas corresponding to different areas of interest in an image. These models work by assigning a label to each pixel. There are several types of segmentation: semantic segmentation, instance segmentation, and panoptic segmentation.\n \n In this guide, we will:\n+\n 1. [Take a look at different types of segmentation](#types-of-segmentation).\n 2. [Have an end-to-end fine-tuning example for semantic segmentation](#fine-tuning-a-model-for-segmentation).\n "
        },
        {
            "sha": "58ca97e9a56ca340f2d904b3164fb42a65b02101",
            "filename": "docs/source/en/tasks/video_text_to_text.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -25,6 +25,7 @@ These models have nearly the same architecture as [image-text-to-text](../image_\n In this guide, we provide a brief overview of video LMs and show how to use them with Transformers for inference.\n \n To begin with, there are multiple types of video LMs:\n+\n - base models used for fine-tuning\n - chat fine-tuned models for conversation\n - instruction fine-tuned models"
        },
        {
            "sha": "e0f7873760e8cc43141458bc39c42c7b404a593c",
            "filename": "docs/source/en/tasks/visual_question_answering.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -23,6 +23,7 @@ The input to models supporting this task is typically a combination of an image\n answer expressed in natural language.\n \n Some noteworthy use case examples for VQA include:\n+\n * Accessibility applications for visually impaired individuals.\n * Education: posing questions about visual materials presented in lectures or textbooks. VQA can also be utilized in interactive museum exhibits or historical sites.\n * Customer service and e-commerce: VQA can enhance user experience by letting users ask questions about products.\n@@ -105,6 +106,7 @@ Let's take a look at an example to understand the dataset's features:\n ```\n \n The features relevant to the task include:\n+\n * `question`: the question to be answered from the image\n * `image_id`: the path to the image the question refers to\n * `label`: the annotations\n@@ -325,6 +327,7 @@ learned something from the data and take the first example from the dataset to i\n Even though not very confident, the model indeed has learned something. With more examples and longer training, you'll get far better results!\n \n You can also manually replicate the results of the pipeline if you'd like:\n+\n 1. Take an image and a question, prepare them for the model using the processor from your model.\n 2. Forward the result or preprocessing through the model.\n 3. From the logits, get the most likely answer's id, and find the actual answer in the `id2label`."
        },
        {
            "sha": "b4ea0529b21536bf1d4f1fbc3205b29736404bf9",
            "filename": "docs/source/en/tasks/zero_shot_image_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fzero_shot_image_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fzero_shot_image_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fzero_shot_image_classification.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -146,4 +146,4 @@ Pass the inputs through the model, and post-process the results:\n  {'score': 0.0010570387, 'label': 'bike'},\n  {'score': 0.0003393686, 'label': 'tree'},\n  {'score': 3.1572064e-05, 'label': 'cat'}]\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "434eca36e33d185a0d1e629e1f9a7cbc95f25738",
            "filename": "docs/source/en/tasks/zero_shot_object_detection.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fzero_shot_object_detection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftasks%2Fzero_shot_object_detection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fzero_shot_object_detection.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -29,6 +29,7 @@ as a list of candidate classes, and output the bounding boxes and labels where t\n > Hugging Face houses many such [open vocabulary zero shot object detectors](https://huggingface.co/models?pipeline_tag=zero-shot-object-detection).\n \n In this guide, you will learn how to use such models:\n+\n - to detect objects based on text prompts\n - for batch object detection\n - for image-guided object detection"
        },
        {
            "sha": "01658aa2beb74b1bae9c162ab95b9321bd436451",
            "filename": "docs/source/en/testing.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftesting.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/374ded5ea40c9026cf36003c737e07c584dbd63d/docs%2Fsource%2Fen%2Ftesting.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftesting.md?ref=374ded5ea40c9026cf36003c737e07c584dbd63d",
            "patch": "@@ -845,11 +845,11 @@ commit it to the main repository we need make sure it's skipped during `make tes\n \n Methods:\n \n--  A **skip** means that you expect your test to pass only if some conditions are met, otherwise pytest should skip\n+- A **skip** means that you expect your test to pass only if some conditions are met, otherwise pytest should skip\n   running the test altogether. Common examples are skipping windows-only tests on non-windows platforms, or skipping\n   tests that depend on an external resource which is not available at the moment (for example a database).\n \n--  A **xfail** means that you expect a test to fail for some reason. A common example is a test for a feature not yet\n+- A **xfail** means that you expect a test to fail for some reason. A common example is a test for a feature not yet\n   implemented, or a bug not yet fixed. When a test passes despite being expected to fail (marked with\n   pytest.mark.xfail), it's an xpass and will be reported in the test summary.\n \n@@ -908,7 +908,7 @@ def test_feature_x():\n docutils = pytest.importorskip(\"docutils\", minversion=\"0.3\")\n ```\n \n--  Skip a test based on a condition:\n+- Skip a test based on a condition:\n \n ```python no-style\n @pytest.mark.skipif(sys.version_info < (3,6), reason=\"requires python3.6 or higher\")"
        }
    ],
    "stats": {
        "total": 584,
        "additions": 338,
        "deletions": 246
    }
}