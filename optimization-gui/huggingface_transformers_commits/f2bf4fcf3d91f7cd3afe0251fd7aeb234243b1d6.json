{
    "author": "ariepratama",
    "message": "Add `SplinterTokenizer` unit test (#32652)\n\n* add unit tests for splinter_tokenizer\r\n\r\n* add unit test for splinter tokenizer, pass in the question_token to be saved on save_pretrained called\r\n\r\n* remove unused import\r\n\r\n* remove vocab_splinter.txt, add Copied from, use fmt:on and fmt:off to prevent autoformatting on long lines\r\n\r\n* remove all the spaces\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "f2bf4fcf3d91f7cd3afe0251fd7aeb234243b1d6",
    "files": [
        {
            "sha": "ffa135556aa47d03cb61d9685a6460018693b533",
            "filename": "src/transformers/models/splinter/tokenization_splinter.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2bf4fcf3d91f7cd3afe0251fd7aeb234243b1d6/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2bf4fcf3d91f7cd3afe0251fd7aeb234243b1d6/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter.py?ref=f2bf4fcf3d91f7cd3afe0251fd7aeb234243b1d6",
            "patch": "@@ -137,6 +137,7 @@ def __init__(\n             pad_token=pad_token,\n             cls_token=cls_token,\n             mask_token=mask_token,\n+            question_token=question_token,\n             tokenize_chinese_chars=tokenize_chinese_chars,\n             strip_accents=strip_accents,\n             **kwargs,"
        },
        {
            "sha": "4c6d295e8a82813536f2511ea6b7007e9a5951a7",
            "filename": "tests/models/splinter/test_tokenization_splinter.py",
            "status": "added",
            "additions": 174,
            "deletions": 0,
            "changes": 174,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2bf4fcf3d91f7cd3afe0251fd7aeb234243b1d6/tests%2Fmodels%2Fsplinter%2Ftest_tokenization_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2bf4fcf3d91f7cd3afe0251fd7aeb234243b1d6/tests%2Fmodels%2Fsplinter%2Ftest_tokenization_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsplinter%2Ftest_tokenization_splinter.py?ref=f2bf4fcf3d91f7cd3afe0251fd7aeb234243b1d6",
            "patch": "@@ -0,0 +1,174 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import unittest\n+\n+from tests.test_tokenization_common import TokenizerTesterMixin\n+from transformers import SplinterTokenizerFast, is_tf_available, is_torch_available\n+from transformers.models.splinter import SplinterTokenizer\n+from transformers.testing_utils import get_tests_dir, slow\n+\n+\n+SAMPLE_VOCAB = get_tests_dir(\"fixtures/vocab.txt\")\n+\n+\n+if is_torch_available():\n+    FRAMEWORK = \"pt\"\n+elif is_tf_available():\n+    FRAMEWORK = \"tf\"\n+else:\n+    FRAMEWORK = \"jax\"\n+\n+\n+class SplinterTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n+    tokenizer_class = SplinterTokenizer\n+    rust_tokenizer_class = SplinterTokenizerFast\n+    space_between_special_tokens = False\n+    test_rust_tokenizer = False\n+    test_sentencepiece_ignore_case = False\n+    pre_trained_model_path = \"tau/splinter-base\"\n+\n+    # Copied from transformers.models.siglip.SiglipTokenizationTest.setUp\n+    def setUp(self):\n+        super().setUp()\n+        tokenizer = SplinterTokenizer(SAMPLE_VOCAB)\n+        tokenizer.vocab[\"[UNK]\"] = len(tokenizer.vocab)\n+        tokenizer.vocab[\"[QUESTION]\"] = len(tokenizer.vocab)\n+        tokenizer.vocab[\".\"] = len(tokenizer.vocab)\n+        tokenizer.add_tokens(\"this is a test thou shall not determine rigor truly\".split())\n+        tokenizer.save_pretrained(self.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs) -> SplinterTokenizer:\n+        return self.tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+\n+    def get_rust_tokenizer(self, **kwargs) -> SplinterTokenizerFast:\n+        return self.rust_tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+\n+    # Copied from transformers.models.siglip.SiglipTokenizationTest.test_get_vocab\n+    def test_get_vocab(self):\n+        vocab_keys = list(self.get_tokenizer().get_vocab().keys())\n+        self.assertEqual(vocab_keys[0], \"[PAD]\")\n+        self.assertEqual(vocab_keys[1], \"[SEP]\")\n+        self.assertEqual(vocab_keys[2], \"[MASK]\")\n+\n+    # Copied from transformers.models.siglip.SiglipTokenizationTest.test_convert_token_and_id\n+    def test_convert_token_and_id(self):\n+        token = \"[PAD]\"\n+        token_id = 0\n+\n+        self.assertEqual(self.get_tokenizer()._convert_token_to_id(token), token_id)\n+        self.assertEqual(self.get_tokenizer()._convert_id_to_token(token_id), token)\n+\n+    def test_question_token_id(self):\n+        tokenizer = self.get_tokenizer()\n+        self.assertEqual(tokenizer.question_token_id, tokenizer.convert_tokens_to_ids(tokenizer.question_token))\n+\n+    # Copied from transformers.models.siglip.SiglipTokenizationTest.test_full_tokenizer\n+    def test_full_tokenizer(self):\n+        tokenizer = self.get_tokenizer()\n+        test_str = \"This is a test\"\n+\n+        unk_token = tokenizer.unk_token\n+        unk_token_id = tokenizer._convert_token_to_id_with_added_voc(unk_token)\n+\n+        expected_tokens = test_str.lower().split()\n+        tokenizer.add_tokens(expected_tokens)\n+        tokens = tokenizer.tokenize(test_str)\n+        self.assertListEqual(tokens, expected_tokens)\n+\n+        # test with out of vocabulary string\n+        tokens = tokenizer.tokenize(test_str + \" oov\")\n+        self.assertListEqual(tokens, expected_tokens + [unk_token])\n+\n+        expected_token_ids = [13, 14, 15, 16, unk_token_id]\n+        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n+        self.assertListEqual(token_ids, expected_token_ids)\n+\n+        tokenizer = self.get_tokenizer(basic_tokenize=False)\n+        expected_token_ids = [13, 14, 15, 16, unk_token_id]\n+        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n+        self.assertListEqual(token_ids, expected_token_ids)\n+\n+    # Copied from transformers.models.siglip.SiglipTokenizationTest.test_rust_and_python_full_tokenizers\n+    def test_rust_and_python_full_tokenizers(self):\n+        tokenizer = self.get_tokenizer()\n+        rust_tokenizer = self.get_rust_tokenizer()\n+\n+        sequence = \"I need to test this rigor\"\n+        tokens = tokenizer.tokenize(sequence, add_special_tokens=False)\n+        rust_tokens = rust_tokenizer.tokenize(sequence, add_special_tokens=False)\n+        self.assertListEqual(tokens, rust_tokens)\n+\n+        ids = tokenizer.encode(sequence)\n+        rust_ids = rust_tokenizer.encode(sequence)\n+        self.assertListEqual(ids, rust_ids)\n+\n+    # Copied from transformers.models.siglip.SiglipTokenizationTest.test_max_length\n+    def test_max_length(self):\n+        max_length = 20\n+        tokenizer = self.get_tokenizer()\n+        texts = [\"this is a test\", \"I have pizza for lunch\"]\n+        tokenized = tokenizer(\n+            text_target=texts,\n+            max_length=max_length,\n+            padding=\"max_length\",\n+            truncation=True,\n+            return_tensors=FRAMEWORK,\n+        )\n+        self.assertEqual(len(tokenized[\"input_ids\"]), len(texts))\n+        self.assertEqual(len(tokenized[\"input_ids\"][0]), max_length)\n+        self.assertEqual(len(tokenized[\"input_ids\"][1]), max_length)\n+        self.assertEqual(len(tokenized[\"attention_mask\"][0]), max_length)\n+        self.assertEqual(len(tokenized[\"attention_mask\"][1]), max_length)\n+        self.assertEqual(len(tokenized[\"token_type_ids\"][0]), max_length)\n+        self.assertEqual(len(tokenized[\"token_type_ids\"][1]), max_length)\n+\n+    # Copied from transformers.models.siglip.SiglipTokenizationTest.test_tokenizer_integration\n+    # fmt:skip\n+    @slow\n+    def test_tokenizer_integration(self):\n+        tokenizer = SplinterTokenizer.from_pretrained(\"tau/splinter-base\", max_length=10)\n+        texts = [\n+            \"The cat sat on the windowsill, watching birds in the garden.\",\n+            \"She baked a delicious cake for her sister's birthday party.\",\n+            \"The sun set over the horizon, painting the sky with vibrant colors.\",\n+        ]\n+        # fmt:off\n+        expected_token_id_list = [\n+            [101, 1109, 5855, 2068, 1113, 1103, 3751, 7956, 117, 2903, 4939, 1107, 1103, 4605, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1153, 19983, 170, 13108, 10851, 1111, 1123, 2104, 112, 188, 5913, 1710, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1109, 3336, 1383, 1166, 1103, 11385, 117, 3504, 1103, 3901, 1114, 18652, 5769, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n+        ]\n+        # fmt:on\n+        for text, expected_token_ids in zip(texts, expected_token_id_list):\n+            input_ids = tokenizer(text, padding=\"max_length\").input_ids\n+            self.assertListEqual(input_ids, expected_token_ids)\n+\n+    def test_special_tokens_mask_input_pairs(self):\n+        tokenizers = self.get_tokenizers(do_lower_case=False)\n+        for tokenizer in tokenizers:\n+            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n+                sequence_0 = \"Encode this.\"\n+                sequence_1 = \"This one too please.\"\n+                encoded_sequence = tokenizer.encode(sequence_0, add_special_tokens=False)\n+                encoded_sequence += tokenizer.encode(sequence_1, add_special_tokens=False)\n+                encoded_sequence_dict = tokenizer.encode_plus(\n+                    sequence_0,\n+                    sequence_1,\n+                    add_special_tokens=True,\n+                    return_special_tokens_mask=True,\n+                )\n+                encoded_sequence_w_special = encoded_sequence_dict[\"input_ids\"]\n+                special_tokens_mask = encoded_sequence_dict[\"special_tokens_mask\"]\n+                # splinter tokenizer always add cls, question_suffix, and 2 separators\n+                # while in special_token_mask it does not seems to do that\n+                self.assertEqual(len(special_tokens_mask), len(encoded_sequence_w_special) - 2)"
        }
    ],
    "stats": {
        "total": 175,
        "additions": 175,
        "deletions": 0
    }
}