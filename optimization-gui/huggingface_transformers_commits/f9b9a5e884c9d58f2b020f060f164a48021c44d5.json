{
    "author": "jiqing-feng",
    "message": "Update quantization overview for XPU (#40331)\n\n* update xpu quantization overview\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix aqlm tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix format\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* update gguf support\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix gguf tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix xpu gguf precision error\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* replace deprecated models\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix import org\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* update xpu ggml tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* revert wrong change\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix xpu tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* xpu optimum-quanto goes green\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix format\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "f9b9a5e884c9d58f2b020f060f164a48021c44d5",
    "files": [
        {
            "sha": "0d3343ef22e5849bd97a60d5b0d26392ca115a50",
            "filename": "docs/source/en/gguf.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f9b9a5e884c9d58f2b020f060f164a48021c44d5/docs%2Fsource%2Fen%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f9b9a5e884c9d58f2b020f060f164a48021c44d5/docs%2Fsource%2Fen%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgguf.md?ref=f9b9a5e884c9d58f2b020f060f164a48021c44d5",
            "patch": "@@ -33,6 +33,7 @@ Add the `gguf_file` parameter to [`~PreTrainedModel.from_pretrained`] to specify\n \n ```py\n # pip install gguf\n+import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM\n \n model_id = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\""
        },
        {
            "sha": "ceab195b2b590698747f150270fc0f5972e97d22",
            "filename": "docs/source/en/quantization/overview.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f9b9a5e884c9d58f2b020f060f164a48021c44d5/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f9b9a5e884c9d58f2b020f060f164a48021c44d5/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Foverview.md?ref=f9b9a5e884c9d58f2b020f060f164a48021c44d5",
            "patch": "@@ -24,23 +24,23 @@ Use the Space below to help you pick a quantization method depending on your har\n \n | Quantization Method                       | On the fly quantization | CPU             | CUDA GPU | ROCm GPU  | Metal (Apple Silicon)              | Intel GPU       | Torch compile() | Bits         | PEFT Fine Tuning | Serializable with 游뱅Transformers | 游뱅Transformers Support  | Link to library                             |\n |-------------------------------------------|----------------------|-----------------|----------|-----------|------------------------------------|-----------------|-----------------|--------------|------------------|-----------------------------|-------------------------|---------------------------------------------|\n-| [AQLM](./aqlm)                            | 游댮                   | 游릭              |     游릭     | 游댮        | 游댮                                 | 游댮              | 游릭              | 1/2          | 游릭               | 游릭                          | 游릭                      | https://github.com/Vahe1994/AQLM            |\n+| [AQLM](./aqlm)                            | 游댮                   | 游릭              |     游릭     | 游댮        | 游댮                                 | 游릭              | 游릭              | 1/2          | 游릭               | 游릭                          | 游릭                      | https://github.com/Vahe1994/AQLM            |\n | [AutoRound](./auto_round)                 | 游댮                   | 游릭               | 游릭          |   游댮        |   游댮                                |   游릭              |   游댮               | 2/3/4/8      |    游댮              |       游릭                      |    游릭                       |      https://github.com/intel/auto-round                                       |\n | [AWQ](./awq)                              | 游댮                   | 游릭              | 游릭        | 游릭        | 游댮                                 | 游릭              | ?               | 4            | 游릭               | 游릭                          | 游릭                      | https://github.com/casper-hansen/AutoAWQ    |\n | [bitsandbytes](./bitsandbytes)            | 游릭                   | 游리 |     游릭     | 游리 | 游댮                    | 游리 | 游릭 | 4/8          | 游릭               | 游릭                          | 游릭                      | https://github.com/bitsandbytes-foundation/bitsandbytes |\n | [compressed-tensors](./compressed_tensors) | 游댮                   | 游릭              |     游릭     | 游릭        | 游댮                                 | 游댮              | 游댮              | 1/8          | 游릭               | 游릭                          | 游릭                      | https://github.com/neuralmagic/compressed-tensors |\n | [EETQ](./eetq)                            | 游릭                   | 游댮              | 游릭        | 游댮        | 游댮                                 | 游댮              | ?               | 8            | 游릭               | 游릭                          | 游릭                      | https://github.com/NetEase-FuXi/EETQ        |\n | [FP-Quant](./fp_quant)                          | 游릭                   | 游댮              | 游릭        | 游댮        | 游댮                                 | 游댮              | 游릭              | 4           | 游댮               | 游릭                          | 游릭                      | https://github.com/IST-DASLab/FP-Quant      |\n-| [GGUF / GGML (llama.cpp)](../gguf)        | 游릭                   | 游릭              | 游릭        | 游댮        | 游릭                                 | 游댮              | 游댮              | 1/8          | 游댮               | [See Notes](../gguf)     | [See Notes](../gguf) | https://github.com/ggerganov/llama.cpp      |\n+| [GGUF / GGML (llama.cpp)](../gguf)        | 游릭                   | 游릭              | 游릭        | 游댮        | 游릭                                 | 游릭              | 游댮              | 1/8          | 游댮               | [See Notes](../gguf)     | [See Notes](../gguf) | https://github.com/ggerganov/llama.cpp      |\n | [GPTQModel](./gptq)                       | 游댮                   | 游릭 | 游릭        | 游릭        | 游릭                                 | 游릭 | 游댮              | 2/3/4/8      | 游릭               | 游릭                          | 游릭                      | https://github.com/ModelCloud/GPTQModel        |\n | [AutoGPTQ](./gptq)                        | 游댮                   | 游댮              | 游릭        | 游릭        | 游댮                                 | 游댮              | 游댮              | 2/3/4/8      | 游릭               | 游릭                          | 游릭                      | https://github.com/AutoGPTQ/AutoGPTQ        |\n | [HIGGS](./higgs)                          | 游릭                   | 游댮              | 游릭        | 游댮        | 游댮                                 | 游댮              | 游릭              | 2/4          | 游댮               | 游릭                          | 游릭                      | https://github.com/HanGuo97/flute           |       \n-| [HQQ](./hqq)                              | 游릭                   | 游릭              | 游릭        | 游댮        | 游댮                                 | 游댮              | 游릭              | 1/8          | 游릭               | 游댮                          | 游릭                      | https://github.com/mobiusml/hqq/            |\n-| [optimum-quanto](./quanto)                | 游릭                   | 游릭              | 游릭        | 游댮        | 游릭                                 | 游댮              | 游릭              | 2/4/8        | 游댮               | 游댮                          | 游릭                      | https://github.com/huggingface/optimum-quanto       |\n+| [HQQ](./hqq)                              | 游릭                   | 游릭              | 游릭        | 游댮        | 游댮                                 | 游릭              | 游릭              | 1/8          | 游릭               | 游댮                          | 游릭                      | https://github.com/mobiusml/hqq/            |\n+| [optimum-quanto](./quanto)                | 游릭                   | 游릭              | 游릭        | 游댮        | 游릭                                 | 游릭              | 游릭              | 2/4/8        | 游댮               | 游댮                          | 游릭                      | https://github.com/huggingface/optimum-quanto       |\n | [FBGEMM_FP8](./fbgemm_fp8)                | 游릭                   | 游댮              | 游릭        | 游댮        | 游댮                                 | 游댮              | 游댮              | 8            | 游댮               | 游릭                          | 游릭                      | https://github.com/pytorch/FBGEMM       |\n-| [torchao](./torchao)                      | 游릭                   | 游릭               | 游릭        | 游댮        | 游리 | 游댮              |                 | 4/8          |                  | 游릭游댮                        | 游릭                      | https://github.com/pytorch/ao       |\n+| [torchao](./torchao)                      | 游릭                   | 游릭               | 游릭        | 游댮        | 游리 | 游릭              |                 | 4/8          |                  | 游릭游댮                        | 游릭                      | https://github.com/pytorch/ao       |\n | [VPTQ](./vptq)                            | 游댮                   | 游댮              |     游릭     | 游리        | 游댮                                 | 游댮              | 游릭              | 1/8          | 游댮               | 游릭                          | 游릭                      | https://github.com/microsoft/VPTQ            |\n-| [FINEGRAINED_FP8](./finegrained_fp8)      | 游릭                   | 游댮              | 游릭        | 游댮        | 游댮                                 | 游댮              | 游댮              | 8            | 游댮               | 游릭                          | 游릭                      |        |\n+| [FINEGRAINED_FP8](./finegrained_fp8)      | 游릭                   | 游댮              | 游릭        | 游댮        | 游댮                                 | 游릭              | 游댮              | 8            | 游댮               | 游릭                          | 游릭                      |        |\n | [SpQR](./spqr)                            | 游댮                     |  游댮   | 游릭        | 游댮              |    游댮    | 游댮         |         游릭              | 3            |              游댮                     | 游릭           | 游릭                      | https://github.com/Vahe1994/SpQR/       |\n | [Quark](./quark)                          | 游댮                     | 游릭 | 游릭      | 游릭      | 游릭                   | 游릭       | ?               | 2/4/6/8/9/16 | 游댮                | 游댮                               | 游릭                       | https://quark.docs.amd.com/latest/                      |\n "
        },
        {
            "sha": "49d0dfb85b015f89d72bce7857204fdf2c57fe3e",
            "filename": "tests/quantization/aqlm_integration/test_aqlm.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f9b9a5e884c9d58f2b020f060f164a48021c44d5/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f9b9a5e884c9d58f2b020f060f164a48021c44d5/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py?ref=f9b9a5e884c9d58f2b020f060f164a48021c44d5",
            "patch": "@@ -26,8 +26,8 @@\n     backend_empty_cache,\n     require_accelerate,\n     require_aqlm,\n-    require_torch_gpu,\n-    require_torch_multi_gpu,\n+    require_torch_accelerator,\n+    require_torch_multi_accelerator,\n     slow,\n     torch_device,\n )\n@@ -41,7 +41,7 @@\n     from accelerate import init_empty_weights\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n class AqlmConfigTest(unittest.TestCase):\n     def test_to_dict(self):\n         \"\"\"\n@@ -72,7 +72,7 @@ def test_from_dict(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n @require_aqlm\n @require_accelerate\n class AqlmTest(unittest.TestCase):\n@@ -180,7 +180,7 @@ def test_save_pretrained(self):\n     @skip(\n         \"inference doesn't work with quantized aqlm models using torch.Any type with recent torch versions. Waiting for the fix from AQLM side\"\n     )\n-    @require_torch_multi_gpu\n+    @require_torch_multi_accelerator\n     def test_quantized_model_multi_gpu(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly with multiple GPUs\n@@ -225,7 +225,9 @@ def decode_one_tokens(model, cur_token, input_pos, cache_position, past_key_valu\n \n         # Setup static KV cache for generation\n         past_key_values = StaticCache(\n-            config=self.quantized_model.config, max_cache_len=seq_length + self.max_new_tokens + 1\n+            config=self.quantized_model.config,\n+            batch_size=input_ids.shape[0],\n+            max_cache_len=seq_length + self.max_new_tokens + 1,\n         )\n \n         # Allocate token ids to be generated and copy prefix ids"
        },
        {
            "sha": "9b640510f7ce61a21f6b2e3b63e0ee8521cf9caa",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f9b9a5e884c9d58f2b020f060f164a48021c44d5/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f9b9a5e884c9d58f2b020f060f164a48021c44d5/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=f9b9a5e884c9d58f2b020f060f164a48021c44d5",
            "patch": "@@ -279,7 +279,7 @@ class GgufModelTests(unittest.TestCase):\n     falcon7b_model_id_fp16 = \"medmekk/falcon-7b-gguf\"\n     falcon40b_model_id = \"maddes8cht/tiiuae-falcon-40b-gguf\"\n     original_flacon7b_model_id = \"tiiuae/falcon-7b\"\n-    t5_model_id = \"repetitio/flan-t5-small\"\n+    t5_model_id = \"Felladrin/gguf-flan-t5-small\"\n     original_t5_model_id = \"google/flan-t5-small\"\n     stablelm_model_id = \"afrideva/stablelm-3b-4e1t-GGUF\"\n     stablelm2_model_id = \"afrideva/stablelm-2-1_6b-GGUF\"\n@@ -317,8 +317,8 @@ class GgufModelTests(unittest.TestCase):\n     q2_k_falcon7b_model_id = \"falcon-7b-q2_k.gguf\"\n     fp16_falcon7b_model_id = \"falcon-7b-fp16.gguf\"\n     q2_k_falcon40b_model_id = \"tiiuae-falcon-40b-Q2_K.gguf\"\n-    fp16_t5_model_id = \"flan-t5-small-f16.gguf\"\n-    q8_0_t5_model_id = \"flan-t5-small-q8_0.gguf\"\n+    fp16_t5_model_id = \"flan-t5-small.F16.gguf\"\n+    q8_0_t5_model_id = \"flan-t5-small.Q8_0.gguf\"\n     fp16_qwen2moe_model_id = \"Qwen1.5-MoE-A2.7B.gguf\"\n     fp16_gpt2_model_id = \"gpt2.f16.gguf\"\n     q8_gpt2_model_id = \"gpt2.Q8_0.gguf\"\n@@ -952,7 +952,7 @@ def test_gemma3_vision_weights_conversion_bf16(self):\n             self.gemma3_vision_model_id,\n             gguf_file=self.bf16_gemma3_vision_model_id,\n             dtype=torch.float16,\n-        )\n+        ).model\n \n         converted_state_dict = converted_model.state_dict()\n         original_state_dict = original_model.state_dict()"
        },
        {
            "sha": "06e9060df6c53bdd31cff49608e9b96d2879372a",
            "filename": "tests/quantization/quanto_integration/test_quanto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f9b9a5e884c9d58f2b020f060f164a48021c44d5/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f9b9a5e884c9d58f2b020f060f164a48021c44d5/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py?ref=f9b9a5e884c9d58f2b020f060f164a48021c44d5",
            "patch": "@@ -422,7 +422,10 @@ class QuantoQuantizationQBitsTensorTest(QuantoQuantizationTest):\n \n \n class QuantoQuantizationQBitsTensorOffloadTest(QuantoQuantizationOffloadTest):\n-    EXPECTED_OUTPUTS = \"Hello my name is John, I am a professional photographer, I\"\n+    EXPECTED_OUTPUTS = [\n+        \"Hello my name is John, I am a professional photographer, I\",  # CUDA output\n+        \"Hello my name is Nils, I am a student of the University\",  # XPU output\n+    ]\n     weights = \"int4\"\n \n "
        }
    ],
    "stats": {
        "total": 40,
        "additions": 23,
        "deletions": 17
    }
}