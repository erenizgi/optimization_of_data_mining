{
    "author": "SunMarc",
    "message": "fix awq  (#42776)\n\n* fix\n\n* fix\n\n* style\n\n* style\n\n* fix\n\n* style\n\n* let's try\n\n* maybe\n\n* fix\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "f8e8ddb0871a46d4baf80f9a3830221e4d1125b9",
    "files": [
        {
            "sha": "a9a9541d5d4a4828e5dc62c50de0e9fe0012444e",
            "filename": "src/transformers/integrations/awq.py",
            "status": "modified",
            "additions": 50,
            "deletions": 106,
            "changes": 156,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8e8ddb0871a46d4baf80f9a3830221e4d1125b9/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8e8ddb0871a46d4baf80f9a3830221e4d1125b9/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fawq.py?ref=f8e8ddb0871a46d4baf80f9a3830221e4d1125b9",
            "patch": "@@ -15,12 +15,13 @@\n \n from typing import Optional, Union\n \n-from ..utils import is_gptqmodel_available, is_llm_awq_available, is_torch_available, logging\n-from ..utils.quantization_config import (\n-    AwqBackend,\n-)\n+from ..quantizers.quantizers_utils import should_convert_module\n+from ..utils import is_accelerate_available, is_torch_available, logging\n \n \n+if is_accelerate_available():\n+    from accelerate import init_empty_weights\n+\n if is_torch_available():\n     import torch\n     import torch.nn as nn\n@@ -61,120 +62,63 @@ def replace_with_awq_linear(\n     model,\n     modules_to_not_convert=None,\n     quantization_config=None,\n-    current_key_name=None,\n-    has_been_replaced=False,\n     device_map: Optional[Union[str, dict]] = None,\n ) -> bool:\n     \"\"\"\n-    Public method that recursively replaces the Linear layers of the given model with AWQ quantized layers.\n-    `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\n-    conversion has been successful or not.\n-\n-    During the module replacement, we also infer the backend to use through the `quantization_config` object.\n+    Public method that replaces the linear layers of the given model with awq quantized layers.\n \n     Args:\n         model (`torch.nn.Module`):\n             The model to convert, can be any `torch.nn.Module` instance.\n         quantization_config (`AwqConfig`):\n             The quantization config object that contains the quantization parameters.\n-        modules_to_not_convert (`list`, *optional*):\n-            A list of modules to not convert. If a module name is in the list (e.g. `lm_head`), it will not be\n+        modules_to_not_convert (`list[str]`, *optional*, defaults to `None`):\n+            A list of nn.Linear weights to not convert. If a parameter path is in the list (e.g. `lm_head.weight`), the corresponding module will not be\n             converted.\n-        current_key_name (`list`, *optional*):\n-            A list that contains the current key name. This is used for recursion and should not be passed by the user.\n-        has_been_replaced (`bool`, *optional*):\n-            A boolean that indicates if the conversion has been successful or not. This is used for recursion and\n-            should not be passed by the user.\n+        device_map (`Union[str, dict]`, *optional*, defaults to `None`):\n+            The device map that maps the parameters to the device\n     \"\"\"\n-    if modules_to_not_convert is None:\n-        modules_to_not_convert = []\n-\n-    backend = quantization_config.backend\n-\n-    if not is_gptqmodel_available() and not is_llm_awq_available():\n-        raise ValueError(\n-            \"AWQ (either `llmawq`) is not available. Please install it with `pip install gptqmodel` or check out the installation guide in https://github.com/mit-han-lab/llm-awq\"\n-        )\n-\n-    if backend != AwqBackend.LLMAWQ:\n-        from gptqmodel.quantization import METHOD\n-        from gptqmodel.utils.importer import hf_select_quant_linear_v2\n-\n-        target_cls = hf_select_quant_linear_v2(\n-            bits=quantization_config.bits,\n-            group_size=quantization_config.group_size,\n-            desc_act=False,\n-            sym=False,\n-            format=quantization_config.format,\n-            backend=quantization_config.backend,\n-            device_map=device_map,\n-            quant_method=METHOD.AWQ,\n-            zero_point=quantization_config.zero_point,\n-            pack=False,\n-        )\n-    else:\n-        from awq.quantize.qmodule import WQLinear\n-\n-        target_cls = WQLinear\n-\n-    for name, module in model.named_children():\n-        if current_key_name is None:\n-            current_key_name = []\n-        current_key_name.append(name)\n-\n-        if isinstance(module, nn.Linear) and name not in modules_to_not_convert:\n-            # Check if the current key is not in the `modules_to_not_convert`\n-            if not any(key in \".\".join(current_key_name) for key in modules_to_not_convert):\n-                in_features = module.in_features\n-                out_features = module.out_features\n-\n-                if backend != AwqBackend.LLMAWQ:\n-                    model._modules[name] = target_cls(\n-                        bits=quantization_config.bits,\n-                        sym=quantization_config.sym,\n-                        desc_act=quantization_config.desc_act,\n-                        group_size=quantization_config.group_size,\n-                        in_features=in_features,\n-                        out_features=out_features,\n-                        bias=module.bias is not None,\n-                        dev=module.weight.device,\n-                        register_buffers=True,\n-                    )\n-                else:\n-                    model._modules[name] = target_cls(\n-                        w_bit=quantization_config.bits,\n-                        group_size=quantization_config.group_size,\n-                        in_features=in_features,\n-                        out_features=out_features,\n-                        bias=module.bias is not None,\n-                        dev=module.weight.device,\n-                    )\n+    from gptqmodel.quantization import METHOD\n+    from gptqmodel.utils.importer import hf_select_quant_linear_v2\n+\n+    target_cls = hf_select_quant_linear_v2(\n+        bits=quantization_config.bits,\n+        group_size=quantization_config.group_size,\n+        desc_act=False,\n+        sym=False,\n+        format=quantization_config.format,\n+        backend=quantization_config.backend,\n+        device_map=device_map,\n+        quant_method=METHOD.AWQ,\n+        zero_point=quantization_config.zero_point,\n+        pack=False,\n+    )\n+\n+    for module_name, module in model.named_modules():\n+        if not should_convert_module(module_name, modules_to_not_convert):\n+            continue\n+        with init_empty_weights():\n+            if isinstance(module, nn.Linear):\n+                new_module = target_cls(\n+                    bits=quantization_config.bits,\n+                    sym=quantization_config.sym,\n+                    desc_act=quantization_config.desc_act,\n+                    group_size=quantization_config.group_size,\n+                    in_features=module.in_features,\n+                    out_features=module.out_features,\n+                    bias=module.bias is not None,\n+                    dev=module.weight.device,\n+                    register_buffers=True,\n+                )\n+                new_module.requires_grad_(False)\n+                model.set_submodule(module_name, new_module)\n                 has_been_replaced = True\n \n-                # Force requires grad to False to avoid unexpected errors\n-                model._modules[name].requires_grad_(False)\n-        if len(list(module.children())) > 0:\n-            _, has_been_replaced = replace_with_awq_linear(\n-                module,\n-                modules_to_not_convert=modules_to_not_convert,\n-                current_key_name=current_key_name,\n-                quantization_config=quantization_config,\n-                has_been_replaced=has_been_replaced,\n-                device_map=device_map,\n-            )\n-        # Remove the last key for recursion\n-        current_key_name.pop(-1)\n-    return model, has_been_replaced\n-\n-\n-def post_init_awq_ipex_modules(model):\n-    \"\"\"\n-    Runs post init for IPEX layers which performs:\n-        - Weights packing, reordering and repacking\n-    \"\"\"\n-\n-    from gptqmodel.quantization.awq.modules.linear.gemm_ipex import ipex_post_init\n-\n-    model = ipex_post_init(model)\n+    if not has_been_replaced:\n+        logger.warning(\n+            \"You are loading your model using eetq but no linear modules were found in your model.\"\n+            \" Please double check your model architecture, or submit an issue on github if you think this is\"\n+            \" a bug.\"\n+        )\n \n     return model"
        },
        {
            "sha": "d7583721d6aab029644034d4c35447a6b4ee4929",
            "filename": "src/transformers/quantizers/quantizer_awq.py",
            "status": "modified",
            "additions": 6,
            "deletions": 10,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8e8ddb0871a46d4baf80f9a3830221e4d1125b9/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8e8ddb0871a46d4baf80f9a3830221e4d1125b9/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py?ref=f8e8ddb0871a46d4baf80f9a3830221e4d1125b9",
            "patch": "@@ -43,9 +43,11 @@ class AwqQuantizer(HfQuantizer):\n     def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n \n-    def validate_environment(self, device_map, **kwargs):\n+    def validate_environment(self, **kwargs):\n         if not is_gptqmodel_available():\n-            raise ImportError(\"Loading an AWQ quantized model requires gptqmodel library (`pip install gptqmodel`)\")\n+            raise ImportError(\n+                \"Loading an AWQ quantized model requires gptqmodel. Please install it with `pip install gptqmodel`\"\n+            )\n \n         if not is_accelerate_available():\n             raise ImportError(\"Loading an AWQ quantized model requires accelerate (`pip install accelerate`)\")\n@@ -72,7 +74,7 @@ def _process_model_before_weight_loading(\n             model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules, add_default_skips=True\n         )\n \n-        model, has_been_replaced = replace_with_awq_linear(\n+        model = replace_with_awq_linear(\n             model,\n             quantization_config=self.quantization_config,\n             modules_to_not_convert=self.modules_to_not_convert,\n@@ -81,18 +83,12 @@ def _process_model_before_weight_loading(\n \n         model = replace_quantization_scales(model, model.config.model_type)\n \n-        if not has_been_replaced:\n-            logger.warning(\n-                \"You are loading an AWQ model but no linear modules were found in your model.\"\n-                \" Please double check your model architecture, or submit an issue on github if you think this is a bug.\"\n-            )\n-\n     def _process_model_after_weight_loading(self, model, **kwargs):\n         from gptqmodel.utils.model import hf_gptqmodel_post_init\n \n         hf_gptqmodel_post_init(model, use_act_order=self.quantization_config.desc_act)\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self):\n         if self.quantization_config.backend in [AwqBackend.EXLLAMA_V1, AwqBackend.EXLLAMA_V2]:\n             logger.warning(\"You cannot save an AWQ model that uses Exllama backend!\")\n             return False"
        },
        {
            "sha": "21435860329baa629e0202aa350b3431357de316",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8e8ddb0871a46d4baf80f9a3830221e4d1125b9/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8e8ddb0871a46d4baf80f9a3830221e4d1125b9/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=f8e8ddb0871a46d4baf80f9a3830221e4d1125b9",
            "patch": "@@ -85,7 +85,6 @@ class AwqBackend(str, Enum):\n     GEMV_FAST = \"gemv_fast\"\n     TORCH_AWQ = \"torch_awq\"\n     TORCH_FUSED_AWQ = \"torch_fused_awq\"\n-    LLMAWQ = \"llm-awq\"\n \n \n @dataclass\n@@ -806,8 +805,7 @@ class AwqConfig(GPTQConfig):\n         zero_point (`bool`, *optional*, defaults to `True`):\n             Whether to use zero point quantization.\n         backend (`AwqBackend`, *optional*, defaults to `AwqBackend.AUTO`):\n-            The quantization backend. Some models might be quantized using `llm-awq` backend. This is useful for users\n-            that quantize their own models using `llm-awq` library.\n+            The quantization backend.\n         modules_to_not_convert (`list`, *optional*, default to `None`):\n             The list of modules to not quantize, useful for quantizing models that explicitly require to have\n             some modules left in their original precision (e.g. Whisper encoder, Llava encoder, Mixtral gate layers).\n@@ -852,16 +850,6 @@ def post_init(self):\n         if self.backend not in AwqBackend.__members__.values():\n             raise ValueError(f\"Invalid backend '{self.backend}'. Must be one of: {[b.value for b in AwqBackend]}\")\n \n-        if self.backend == AwqBackend.LLMAWQ:\n-            # Only cuda device can run this function\n-            if not (torch.cuda.is_available() or torch.xpu.is_available()):\n-                raise ValueError(\"LLM-AWQ backend is only supported on CUDA and XPU\")\n-            if torch.cuda.is_available():\n-                compute_capability = torch.cuda.get_device_capability()\n-                major, minor = compute_capability\n-                if major < 8:\n-                    raise ValueError(\"LLM-AWQ backend is only supported on CUDA GPUs with compute capability >= 8.0\")\n-\n     def to_dict(self) -> dict[str, Any]:\n         config_dict = super().to_dict()\n         config_dict.pop(\"checkpoint_format\")"
        },
        {
            "sha": "04f4b933592ec3b22d6682c217747e3246ae25fb",
            "filename": "tests/quantization/autoawq/test_awq.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8e8ddb0871a46d4baf80f9a3830221e4d1125b9/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8e8ddb0871a46d4baf80f9a3830221e4d1125b9/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoawq%2Ftest_awq.py?ref=f8e8ddb0871a46d4baf80f9a3830221e4d1125b9",
            "patch": "@@ -19,7 +19,6 @@\n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AwqConfig, OPTForCausalLM\n from transformers.testing_utils import (\n     backend_empty_cache,\n-    get_device_properties,\n     require_accelerate,\n     require_gptqmodel,\n     require_torch_accelerator,\n@@ -58,22 +57,6 @@ def test_wrong_backend(self):\n         with self.assertRaises(ValueError):\n             AwqConfig(bits=4, backend=\"unexisting-backend\")\n \n-        # Only cuda and xpu devices can run this function\n-        support_llm_awq = False\n-        device_type, major, _ = get_device_properties()\n-        if device_type == \"cuda\" and major >= 8:\n-            support_llm_awq = True\n-        elif device_type == \"xpu\":\n-            support_llm_awq = True\n-\n-        if support_llm_awq:\n-            # LLMAWQ should work on an A100\n-            AwqConfig(bits=4, backend=\"llm-awq\")\n-        else:\n-            # LLMAWQ does not work on a T4\n-            with self.assertRaises(ValueError):\n-                AwqConfig(bits=4, backend=\"llm-awq\")\n-\n     def test_to_dict(self):\n         \"\"\"\n         Simple test that checks if one uses a config and converts it to a dict, the dict is the same as the config object"
        },
        {
            "sha": "666e054f61a7ed2273e2758bf4e8376122d0e25b",
            "filename": "tests/quantization/gptq/test_gptq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8e8ddb0871a46d4baf80f9a3830221e4d1125b9/tests%2Fquantization%2Fgptq%2Ftest_gptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8e8ddb0871a46d4baf80f9a3830221e4d1125b9/tests%2Fquantization%2Fgptq%2Ftest_gptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fgptq%2Ftest_gptq.py?ref=f8e8ddb0871a46d4baf80f9a3830221e4d1125b9",
            "patch": "@@ -113,8 +113,6 @@ class GPTQTest(unittest.TestCase):\n     group_size = 128\n     desc_act = False\n     act_group_aware = True\n-    quant_backend = BACKEND.AUTO\n-    load_backend = BACKEND.AUTO\n     dataset = [\n         \"gptqmodel is an easy-to-use model quantization library with user-friendly APIs, based on the GPTQ algorithm.\"\n     ]\n@@ -143,7 +141,7 @@ def setUpClass(cls):\n             desc_act=cls.desc_act,\n             act_group_aware=cls.act_group_aware,\n             sym=cls.sym,\n-            backend=cls.quant_backend,\n+            backend=BACKEND.AUTO,\n         )\n \n         cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n@@ -309,7 +307,6 @@ class GPTQTestActOrderExllamaV2(unittest.TestCase):\n     # `act_group_aware` == `True` requires `desc_act` == `False` when both are explicitly set\n     desc_act = True\n     act_group_aware = False\n-    load_backend = BACKEND.EXLLAMA_V2\n \n     EXPECTED_OUTPUTS = set()\n     # flaky test: gptqmodel kernels are not always bitwise deterministic even between transformer/torch versions\n@@ -328,7 +325,7 @@ def setUpClass(cls):\n             max_input_length=4028,\n             desc_act=cls.desc_act,\n             act_group_aware=cls.act_group_aware,\n-            backend=cls.load_backend,\n+            backend=BACKEND.EXLLAMA_V2,\n         )\n         cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n             cls.model_name,\n@@ -376,7 +373,6 @@ class GPTQTestExllamaV2(unittest.TestCase):\n     https://huggingface.co/docs/transformers/main_classes/quantization#transformers.GPTQConfig\n     \"\"\"\n \n-    load_backend = BACKEND.EXLLAMA_V2\n     EXPECTED_OUTPUTS = set()\n     # flaky test: gptqmodel kernels are not always bitwise deterministic even between transformer/torch versions\n     EXPECTED_OUTPUTS.add(\"Hello, how are you ? I'm doing good, thanks for asking.\")\n@@ -389,7 +385,7 @@ def setUpClass(cls):\n         \"\"\"\n         Setup quantized model\n         \"\"\"\n-        cls.quantization_config = GPTQConfig(bits=4, backend=cls.load_backend)\n+        cls.quantization_config = GPTQConfig(bits=4, backend=BACKEND.EXLLAMA_V2)\n         cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n             cls.model_name,\n             dtype=torch.float16,"
        }
    ],
    "stats": {
        "total": 213,
        "additions": 60,
        "deletions": 153
    }
}