{
    "author": "vasqu",
    "message": "[`Attn Masks`] Non-vmap default for attention masks (#41852)\n\n* atmpt 1\n\n* fixup masking to work correctly with old torch\n\n* few changes to make things a bit more cleaner\n\n* oopsie\n\n* fix integer overflow on bidirectional masks via indexing fn\n\n* rm executorch workarounds --> still need to handle on sliding etc fns properly\n\n* typo\n\n* docs, fix older torch inplace issue, proper kwarg handling\n\n* chunked works with non vmap and older torch, add warning on non guaranteed masks\n\n* lift unnecessary restriction on older torch\n\n* simplify a few things, restrict torch < 2.6 to non-vmap (for now)\n\n* try fix\n\n* remove unnecessary slicing logic\n\n* remove legacy func\n\n* harmonize slightly\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "03538a80be2b96d11590c2aee4d0f7f70abcc017",
    "files": [
        {
            "sha": "635ab7abe744221d059e72154d583b95caade4ee",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 111,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/03538a80be2b96d11590c2aee4d0f7f70abcc017/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/03538a80be2b96d11590c2aee4d0f7f70abcc017/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=03538a80be2b96d11590c2aee4d0f7f70abcc017",
            "patch": "@@ -11,7 +11,6 @@\n # specific language governing permissions and limitations under the License.\n \n import logging\n-from collections.abc import Callable\n from typing import Optional\n \n import torch\n@@ -24,13 +23,7 @@\n     StaticCache,\n )\n from ..generation.configuration_utils import GenerationConfig\n-from ..masking_utils import (\n-    ALL_MASK_ATTENTION_FUNCTIONS,\n-    _ignore_causal_mask_sdpa,\n-    _is_torch_greater_or_equal_than_2_5,\n-    prepare_padding_mask,\n-)\n-from ..modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ..modeling_utils import PreTrainedModel\n from ..pytorch_utils import (\n     is_torch_greater_or_equal,\n     is_torch_greater_or_equal_than_2_3,\n@@ -229,10 +222,6 @@ def __init__(\n                 \"Using `StaticCache` for export as `layer_types` is not specified or `sliding_window` is `null` in the config.\"\n             )\n             self.model = TorchExportableModuleWithStaticCache(model, batch_size, max_cache_len, device)\n-        # This is the same as sdpa, but mask creation does not use `vmap` which is not exportable\n-        ALL_MASK_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", sdpa_mask_without_vmap)\n-        ALL_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", ALL_ATTENTION_FUNCTIONS[\"sdpa\"])\n-        self.model.model.config._attn_implementation = \"sdpa_without_vmap\"\n \n     def forward(\n         self,\n@@ -768,11 +757,6 @@ def convert_and_export_with_cache(\n \n     import torch.export._trace\n \n-    # This is the same as sdpa, but mask creation does not use `vmap` which is not exportable\n-    ALL_MASK_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", sdpa_mask_without_vmap)\n-    ALL_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", ALL_ATTENTION_FUNCTIONS[\"sdpa\"])\n-    model.config._attn_implementation = \"sdpa_without_vmap\"\n-\n     with torch.no_grad():\n         # TODO: The default inputs only work for text models. We need to add support for vision/audio models.\n         example_input_ids = (\n@@ -1036,11 +1020,6 @@ def export_with_dynamic_cache(\n     if not is_torch_greater_or_equal_than_2_3:\n         raise ImportError(\"torch >= 2.3 is required.\")\n \n-    # This is the same as sdpa, but mask creation does not use `vmap` which is not exportable\n-    ALL_MASK_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", sdpa_mask_without_vmap)\n-    ALL_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", ALL_ATTENTION_FUNCTIONS[\"sdpa\"])\n-    model.config._attn_implementation = \"sdpa_without_vmap\"\n-\n     register_dynamic_cache_export_support()\n \n     with torch.no_grad():\n@@ -1109,92 +1088,3 @@ def _unflatten_dynamic_cache(values, context: torch.utils._pytree.Context):\n         value = value_list[idx] if idx < len(value_list) else None\n         cache.update(key, value, idx)\n     return cache\n-\n-\n-def sdpa_mask_without_vmap(\n-    batch_size: int,\n-    cache_position: torch.Tensor,\n-    kv_length: int,\n-    kv_offset: int = 0,\n-    mask_function: Optional[Callable] = None,\n-    attention_mask: Optional[torch.Tensor] = None,\n-    local_size: Optional[int] = None,\n-    allow_is_causal_skip: bool = True,\n-    allow_torch_fix: bool = True,\n-    **kwargs,\n-) -> Optional[torch.Tensor]:\n-    \"\"\"\n-    Create a 4D boolean mask of shape `(batch_size, 1, query_length, kv_length)` where a value of True indicates that\n-    the element should take part in the attention computation, and False that it should not.\n-\n-    This is similar to `masking_utils.sdpa_mask` but does not use `vmap` which is incompatible with export.\n-\n-    Args:\n-        batch_size (`int`):\n-            The batch size of the input sequence.\n-        cache_position (`torch.Tensor`):\n-            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n-        kv_length (`int`):\n-            The size that the key and value states will have during the attention computation.\n-        kv_offset (`int`, optional):\n-            An optional offset to indicate at which first position the key and values states will refer to.\n-        mask_function (`Callable`):\n-            The mask factory function describing the mask pattern.\n-        attention_mask (`torch.Tensor`, optional):\n-            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length)\n-        local_size (`int`, optional):\n-            The size of the local attention, if we do not use full attention. This is used only if `allow_is_causal_skip=True`\n-            to try to skip mask creation if possible.\n-        allow_is_causal_skip (`bool`, optional):\n-            Whether to allow to return `None` for the mask under conditions where we can use the `is_causal` argument in\n-            `torch.sdpa` instead. Default to `True`.\n-        allow_torch_fix (`bool`, optional):\n-            Whether to update the mask in case a query is not attending to any tokens, to solve a bug in torch's older\n-            versions. We need an arg to skip it when using eager. By default `True`.\n-\n-    \"\"\"\n-\n-    q_length = cache_position.shape[0]\n-    # Potentially pad the 2D mask, and slice it correctly\n-    padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset)\n-\n-    #  Under specific conditions, we can avoid materializing the mask, instead relying on the `is_causal` argument\n-    if allow_is_causal_skip and _ignore_causal_mask_sdpa(padding_mask, q_length, kv_length, local_size):\n-        return None\n-\n-    # Similar to `kv_arange = torch.arange(start=kv_offset, end=kv_offset + kv_length, device=cache_position.device)`\n-    # but without data-dependent slicing (i.e. torch.compile friendly)\n-    kv_arange = torch.arange(kv_length, device=cache_position.device)\n-    kv_arange += kv_offset\n-    reshaped_cache_position = cache_position.view(-1, 1)\n-\n-    # This is a bit hacky to know what pattern we are using, but all mask creation function actually forward\n-    # the config through kwargs anyway, so it allows to rely on it\n-    # Usually, the `mask_function` is the only entry-point to define the pattern - we could do for loops over it,\n-    # but this is more efficient\n-    sliding_window = getattr(kwargs[\"config\"], \"sliding_window\", None)\n-    chunk_size = getattr(kwargs[\"config\"], \"attention_chunk_size\", None)\n-\n-    if sliding_window is not None and chunk_size is not None:\n-        raise ValueError(\"Cannot use both `sliding_window` and `attention_chunk_size`\")\n-\n-    # Simplest and most efficient way to obtain a causal mask\n-    causal_mask = kv_arange <= reshaped_cache_position\n-    # If using sliding window, add the sliding mask\n-    if sliding_window is not None:\n-        sliding_mask_overlay = kv_arange > reshaped_cache_position - sliding_window\n-        causal_mask *= sliding_mask_overlay\n-    # If using chunk attention, add the chunked mask\n-    elif chunk_size is not None:\n-        chunked_mask_overlay = kv_arange // chunk_size == reshaped_cache_position // chunk_size\n-        causal_mask *= chunked_mask_overlay\n-\n-    causal_mask = causal_mask[None, None, :, :].expand(batch_size, -1, -1, -1)\n-    if padding_mask is not None:\n-        causal_mask = causal_mask * padding_mask[:, None, None, :]\n-\n-    # Due to a bug in some older torch version, we need to update the mask in case a query is not attending to any\n-    # tokens (due to padding). See details in https://github.com/pytorch/pytorch/issues/110213\n-    if not _is_torch_greater_or_equal_than_2_5 and allow_torch_fix:\n-        causal_mask |= torch.all(~causal_mask, dim=-1, keepdim=True)\n-    return causal_mask"
        },
        {
            "sha": "45ba50baf44dff95b838585ad1131af1d41da1f3",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 114,
            "deletions": 179,
            "changes": 293,
            "blob_url": "https://github.com/huggingface/transformers/blob/03538a80be2b96d11590c2aee4d0f7f70abcc017/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/03538a80be2b96d11590c2aee4d0f7f70abcc017/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=03538a80be2b96d11590c2aee4d0f7f70abcc017",
            "patch": "@@ -82,8 +82,10 @@ def causal_mask_function(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int)\n def bidirectional_mask_function(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n     \"\"\"\n     This creates a full bidirectional mask.\n+\n+    NOTE: It is important to keep an index-based version for non-vmap expansion.\n     \"\"\"\n-    return q_idx.new_ones((), dtype=torch.bool)\n+    return q_idx >= 0\n \n \n def sliding_window_overlay(sliding_window: int) -> Callable:\n@@ -110,18 +112,6 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n     return inner_mask\n \n \n-def _legacy_chunked_overlay(chunk_size: int) -> Callable:\n-    \"\"\"\n-    Same as the above function, but do not correctly account for left padding tokens.\n-    Only kept for compatibility with older torch versions (< 2.6).\n-    \"\"\"\n-\n-    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n-        return kv_idx // chunk_size == q_idx // chunk_size\n-\n-    return inner_mask\n-\n-\n def sliding_window_causal_mask_function(sliding_window: int) -> Callable:\n     \"\"\"\n     This return the mask_function function to create a sliding window mask.\n@@ -133,8 +123,6 @@ def chunked_causal_mask_function(chunk_size: int, left_padding: torch.Tensor) ->\n     \"\"\"\n     This return the mask_function function to create a chunked attention mask.\n     \"\"\"\n-    if not _is_torch_greater_or_equal_than_2_6:\n-        return and_masks(_legacy_chunked_overlay(chunk_size), causal_mask_function)\n     return and_masks(chunked_overlay(chunk_size, left_padding), causal_mask_function)\n \n \n@@ -175,52 +163,17 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n     return inner_mask\n \n \n-def _vmap_for_bhqkv(mask_function: Callable, bh_indices: bool = True) -> Callable:\n-    \"\"\"\n-    Used to vmap our mask_functions over the q_idx and kv_idx dimensions of the inputs. Optionally, vmap over\n-    the batch and head indices as well if `bh_indices=True`.\n-    Using vmap here allows us to keep the performance of vectorized ops, while having a single set of primitive\n-    functions between attention interfaces (i.e. between flex and sdpa/eager, FA2 being a bit different).\n-\n-    Args:\n-        mask_function (`Callable`):\n-            The mask_function to vmap.\n-        bh_indices (`bool`, optional):\n-            Whether to vmap over the batch and head indices as well, or only q and kv indices.\n-\n-    Returns:\n-        Callable: The vmapped function.\n-    \"\"\"\n-    # We vmap the function 2 times, broadcasting the [q_idx, kv_idx] dimensions\n-    dimensions = [(None, None, None, 0), (None, None, 0, None)]\n-    if bh_indices:\n-        # We extend broadcasting over the [batch_idx, head_idx] dimensions\n-        dimensions.extend([(None, 0, None, None), (0, None, None, None)])\n-\n-    for dims in dimensions:\n-        mask_function = torch.vmap(mask_function, in_dims=dims, out_dims=0)\n-    return mask_function\n-\n-\n def prepare_padding_mask(\n-    attention_mask: Optional[torch.Tensor], kv_length: int, kv_offset: int, _slice: bool = True\n+    attention_mask: Optional[torch.Tensor], kv_length: int, kv_offset: int\n ) -> Optional[torch.Tensor]:\n     \"\"\"\n-    From the 2D attention mask, prepare the correct padding mask to use by potentially padding it, and slicing\n-    according to the `kv_offset` if `_slice` is `True`.\n+    From the 2D attention mask, prepare the correct padding mask to use by potentially padding it.\n     \"\"\"\n     local_padding_mask = attention_mask\n     if attention_mask is not None:\n         # Pad it if necessary\n         if (padding_length := kv_length + kv_offset - attention_mask.shape[-1]) > 0:\n             local_padding_mask = torch.nn.functional.pad(attention_mask, (0, padding_length))\n-        # For flex, we should not slice them, only use an offset\n-        if _slice:\n-            # Equivalent to: `local_padding_mask = attention_mask[:, kv_offset : kv_offset + kv_length]`,\n-            # but without data-dependent slicing (i.e. torch.compile friendly)\n-            mask_indices = torch.arange(kv_length, device=local_padding_mask.device)\n-            mask_indices += kv_offset\n-            local_padding_mask = local_padding_mask[:, mask_indices]\n     return local_padding_mask\n \n \n@@ -282,7 +235,39 @@ def _ignore_bidirectional_mask_sdpa(padding_mask: Optional[torch.Tensor]) -> boo\n     return False\n \n \n-def sdpa_mask_recent_torch(\n+def _vmap_expansion_sdpa(mask_function: Callable) -> Callable:\n+    \"\"\"\n+    Used to vmap our mask_functions over the all 4 dimensions (b_idx, h_idx, q_idx, kv_idx) of the inputs.\n+    Using vmap here allows us to keep the performance of vectorized ops, while having a single set of primitive\n+    functions between attention interfaces (i.e. between flex and sdpa/eager, FA2 being a bit different).\n+    \"\"\"\n+    # We vmap the function over all 4 dimensions, broadcasting [b_idx, h_idx, q_idx, kv_idx]\n+    dimensions = [(None, None, None, 0), (None, None, 0, None), (None, 0, None, None), (0, None, None, None)]\n+    for dims in dimensions:\n+        mask_function = torch.vmap(mask_function, in_dims=dims, out_dims=0)\n+    return mask_function\n+\n+\n+def _non_vmap_expansion_sdpa(\n+    batch_indices: torch.Tensor, head_indices: torch.Tensor, q_indices: torch.Tensor, kv_indices: torch.Tensor\n+):\n+    \"\"\"\n+    Used to broadcast our mask_functions over the all 4 dimensions (b_idx, h_idx, q_idx, kv_idx) of the inputs.\n+    Allows the usage of any index-based mask function without relying on vmap.\n+\n+    NOTE: This is limited to index based functions only and is not guaranteed to work otherwise.\n+\n+    Reference:\n+        - https://github.com/huggingface/optimum-onnx/blob/c123e8f4fab61b54a8e0e31ce74462bcacca576e/optimum/exporters/onnx/model_patcher.py#L362-L365\n+    \"\"\"\n+    batch_indices = batch_indices[:, None, None, None]\n+    head_indices = head_indices[None, :, None, None]\n+    q_indices = q_indices[None, None, :, None]\n+    kv_indices = kv_indices[None, None, None, :]\n+    return batch_indices, head_indices, q_indices, kv_indices\n+\n+\n+def sdpa_mask(\n     batch_size: int,\n     cache_position: torch.Tensor,\n     kv_length: int,\n@@ -292,6 +277,8 @@ def sdpa_mask_recent_torch(\n     local_size: Optional[int] = None,\n     allow_is_causal_skip: bool = True,\n     allow_is_bidirectional_skip: bool = False,\n+    allow_torch_fix: bool = True,\n+    use_vmap: bool = False,\n     **kwargs,\n ) -> Optional[torch.Tensor]:\n     \"\"\"\n@@ -324,6 +311,12 @@ def sdpa_mask_recent_torch(\n         allow_is_bidirectional_skip (`bool`, optional):\n             Whether to allow to return `None` for the mask under conditions where we do not have to add any bias,\n             i.e. full attention without any padding. Default to `False`.\n+        allow_torch_fix (`bool`, optional):\n+            Whether to update the mask in case a query is not attending to any tokens, to solve a bug in torch's older\n+            versions. We need an arg to skip it when using eager. By default `True`.\n+        use_vmap (`bool`, optional):\n+            Whether to use `vmap` during the mask construction or not. Allows powerful custom patterns that may not be\n+            index-based (for the cost of speed performance). By default `False`.\n \n \n     ## Creating a simple causal mask:\n@@ -391,8 +384,9 @@ def sdpa_mask_recent_torch(\n \n     \"\"\"\n     q_length = cache_position.shape[0]\n-    # Potentially pad the 2D mask, and slice it correctly\n-    padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset, _slice=False)\n+\n+    # Potentially pad the 2D mask\n+    padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset)\n \n     # Under specific conditions, we can avoid materializing the mask\n     #   1. Causal masks can rely on the `is_causal` argument\n@@ -402,128 +396,45 @@ def sdpa_mask_recent_torch(\n     if allow_is_bidirectional_skip and _ignore_bidirectional_mask_sdpa(padding_mask):\n         return None\n \n-    # vmap can incur performance issues as reported in #41566 for bidirectional mask as we only need to expand the\n-    # padding mask. Thus, we allow early exit here if we do not detect any modification to the base mask function\n-    if mask_function is bidirectional_mask_function:\n-        if padding_mask is not None:\n-            # used for slicing without data-dependent slicing\n-            mask_indices = torch.arange(kv_length, device=cache_position.device) + kv_offset\n-            return padding_mask[:, None, None, mask_indices].expand(-1, -1, q_length, -1)\n-        else:\n-            return torch.ones(batch_size, 1, q_length, kv_length, dtype=torch.bool, device=cache_position.device)\n-\n-    # Similar to `kv_arange = torch.arange(start=kv_offset, end=kv_offset + kv_length, device=cache_position.device)`\n-    # but without data-dependent slicing (i.e. torch.compile friendly)\n-    kv_arange = torch.arange(kv_length, device=cache_position.device)\n-    kv_arange += kv_offset\n-\n     # Potentially add the padding 2D mask\n     if padding_mask is not None:\n         mask_function = and_masks(mask_function, padding_mask_function(padding_mask))\n \n     batch_arange = torch.arange(batch_size, device=cache_position.device)\n     head_arange = torch.arange(1, device=cache_position.device)\n-    # This creates the 4D mask easily. Note that we need this context manager as vmap cannot handle slicing a tensor from\n-    # scalar tensor (it internally calls `.item()` which vmap does not allow, but this context works around it\n-    # We don't need to add an offset to the mask_function either, as we vmap directly the correct indices for k and kv indices\n-    with TransformGetItemToIndex():\n-        causal_mask = _vmap_for_bhqkv(mask_function)(batch_arange, head_arange, cache_position, kv_arange)\n-\n-    return causal_mask\n-\n-\n-def sdpa_mask_older_torch(\n-    batch_size: int,\n-    cache_position: torch.Tensor,\n-    kv_length: int,\n-    kv_offset: int = 0,\n-    mask_function: Callable = causal_mask_function,\n-    attention_mask: Optional[torch.Tensor] = None,\n-    local_size: Optional[int] = None,\n-    allow_is_causal_skip: bool = True,\n-    allow_torch_fix: bool = True,\n-    allow_is_bidirectional_skip: bool = False,\n-    **kwargs,\n-) -> Optional[torch.Tensor]:\n-    \"\"\"\n-    NOTE: This function is only used when torch version is torch<2.5 - see `sdpa_mask_recent_torch` otherwise.\n-\n-    Create a 4D boolean mask of shape `(batch_size, 1, query_length, kv_length)` where a value of True indicates that\n-    the element should take part in the attention computation, and False that it should not.\n-    If `allow_torch_fix=True` (the default), rows corresponding to query tokens that do not attend\n-    to any other tokens (due to padding) will be fully attended to instead, in order to avoid `nan` propagation (this does\n-    not change the final result).\n-\n-    Args:\n-        batch_size (`int`):\n-            The batch size of the input sequence.\n-        cache_position (`torch.Tensor`):\n-            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n-        kv_length (`int`):\n-            The size that the key and value states will have during the attention computation.\n-        kv_offset (`int`, optional):\n-            An optional offset to indicate at which first position the key and values states will refer to.\n-        mask_function (`Callable`):\n-            The mask factory function describing the mask pattern.\n-        attention_mask (`torch.Tensor`, optional):\n-            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length)\n-        local_size (`int`, optional):\n-            The size of the local attention, if we do not use full attention. This is used only if `allow_is_causal_skip=True`\n-            to try to skip mask creation if possible.\n-        allow_is_causal_skip (`bool`, optional):\n-            Whether to allow to return `None` for the mask under conditions where we can use the `is_causal` argument in\n-            `torch.sdpa` instead. Default to `True`.\n-        allow_torch_fix (`bool`, optional):\n-            Whether to update the mask in case a query is not attending to any tokens, to solve a bug in torch's older\n-            versions. We need an arg to skip it when using eager. By default `True`.\n-        allow_is_bidirectional_skip (`bool`, optional):\n-            Whether to allow to return `None` for the mask under conditions where we do not have to add any bias,\n-            i.e. full attention without any padding. Default to `False`.\n-    \"\"\"\n-    q_length = cache_position.shape[0]\n-    # Potentially pad the 2D mask, and slice it correctly\n-    padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset)\n-\n-    # Under specific conditions, we can avoid materializing the mask\n-    #   1. Causal masks can rely on the `is_causal` argument\n-    #   2. Bidirectional do not need any further processing (no bias)\n-    if allow_is_causal_skip and _ignore_causal_mask_sdpa(padding_mask, q_length, kv_length, kv_offset, local_size):\n-        return None\n-    if allow_is_bidirectional_skip and _ignore_bidirectional_mask_sdpa(padding_mask):\n-        return None\n-\n-    # vmap can incur performance issues as reported in #41566 for bidirectional mask as we only need to expand the\n-    # padding mask. Thus, we allow early exit here if we do not detect any modification to the base mask function\n-    if mask_function is bidirectional_mask_function:\n-        if padding_mask is not None:\n-            return padding_mask[:, None, None, :].expand(-1, -1, q_length, -1)\n-        else:\n-            return torch.ones(batch_size, 1, q_length, kv_length, dtype=torch.bool, device=cache_position.device)\n-\n     # Similar to `kv_arange = torch.arange(start=kv_offset, end=kv_offset + kv_length, device=cache_position.device)`\n     # but without data-dependent slicing (i.e. torch.compile friendly)\n-    kv_arange = torch.arange(kv_length, device=cache_position.device)\n-    kv_arange += kv_offset\n-\n-    # This creates the 4D mask easily. Note that we do not include vmap over the batch_idx dimension as well,\n-    # as vmap cannot handle slicing a tensor from scalar tensor (it internally calls `.item()` which vmap does not allow\n-    # However, in more recent version of Pytorch, a trick was introduced to handle it - which is the reason we have\n-    # `sdpa_mask_recent_torch`, as it allows more general `mask_function`\n-    causal_mask = _vmap_for_bhqkv(mask_function, bh_indices=False)(None, None, cache_position, kv_arange)\n-    causal_mask = causal_mask[None, None, :, :].expand(batch_size, -1, -1, -1)\n-    if padding_mask is not None:\n-        causal_mask = causal_mask * padding_mask[:, None, None, :]\n+    kv_arange = torch.arange(kv_length, device=cache_position.device) + kv_offset\n+\n+    # Actual mask creation\n+    # Option 1: Fast non-vmap mask creation (default)\n+    if not use_vmap:\n+        # Apply mask function element-wise through broadcasting\n+        attention_mask = mask_function(*_non_vmap_expansion_sdpa(batch_arange, head_arange, cache_position, kv_arange))\n+        # Expand the mask to match batch size and query length if they weren't used in the mask function\n+        attention_mask = attention_mask.expand(batch_size, -1, q_length, kv_length)\n+\n+    # Option 2: Vmap mask creation (torch>=2.6 and custom patterns)\n+    elif _is_torch_greater_or_equal_than_2_6:\n+        # This creates the 4D mask easily. Note that we need this context manager as vmap cannot handle slicing a tensor from\n+        # scalar tensor (it internally calls `.item()` which vmap does not allow, but this context works around it\n+        # We don't need to add an offset to the mask_function either, as we vmap directly the correct indices for k and kv indices\n+        with TransformGetItemToIndex():\n+            attention_mask = _vmap_expansion_sdpa(mask_function)(batch_arange, head_arange, cache_position, kv_arange)\n+\n+    # Option 3: Error out since it indicates that the user did something custom, which they shouldn't have (torch<2.6)\n+    else:\n+        raise ValueError(\n+            \"The vmap functionality for mask creation is only supported from torch>=2.6. \"\n+            \"Please update your torch version or use `use_vmap=False` with index-based masks.\"\n+        )\n \n     # Due to a bug in versions of torch<2.5, we need to update the mask in case a query is not attending to any\n     # tokens (due to padding). See details in https://github.com/pytorch/pytorch/issues/110213\n     if not _is_torch_greater_or_equal_than_2_5 and allow_torch_fix:\n-        causal_mask |= torch.all(~causal_mask, dim=-1, keepdim=True)\n-    return causal_mask\n-\n+        attention_mask = attention_mask | torch.all(~attention_mask, dim=-1, keepdim=True)\n \n-# We use the version with newer torch whenever possible, as it is more general and can handle arbitrary mask functions\n-# (especially mask_function indexing a tensor, such as the padding mask function)\n-sdpa_mask = sdpa_mask_recent_torch if _is_torch_greater_or_equal_than_2_6 else sdpa_mask_older_torch\n+    return attention_mask\n \n \n def eager_mask(\n@@ -534,6 +445,7 @@ def eager_mask(\n     mask_function: Callable = causal_mask_function,\n     attention_mask: Optional[torch.Tensor] = None,\n     dtype: torch.dtype = torch.float32,\n+    use_vmap: bool = False,\n     **kwargs,\n ) -> torch.Tensor:\n     \"\"\"\n@@ -556,10 +468,14 @@ def eager_mask(\n             The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length)\n         dtype (`torch.dtype`, optional):\n             The dtype to use for the mask. By default, `torch.float32`.\n+        use_vmap (`bool`, optional):\n+            Whether to use `vmap` during the mask construction or not. Allows powerful custom patterns that may not be\n+            index-based (for the cost of speed performance). By default `False`.\n     \"\"\"\n     # The masks for eager attention are simply boolean mask from sdpa, casted to 0 and -inf\n     _ = kwargs.pop(\"allow_is_causal_skip\", None)\n     _ = kwargs.pop(\"allow_is_bidirectional_skip\", None)\n+    _ = kwargs.pop(\"allow_torch_fix\", None)\n     mask = sdpa_mask(\n         batch_size=batch_size,\n         cache_position=cache_position,\n@@ -570,6 +486,7 @@ def eager_mask(\n         allow_is_causal_skip=False,\n         allow_is_bidirectional_skip=False,\n         allow_torch_fix=False,\n+        use_vmap=use_vmap,\n         **kwargs,\n     )\n     min_dtype = torch.finfo(dtype).min\n@@ -655,7 +572,7 @@ def flex_attention_mask(\n         if not _is_torch_greater_or_equal_than_2_6 and pad_len > 0:\n             attention_mask = torch.nn.functional.pad(attention_mask, value=0, pad=(0, pad_len))\n \n-        padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset, _slice=False)\n+        padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset)\n         mask_function = and_masks(mask_function, padding_mask_function(padding_mask))\n \n     # Add the offsets on top (because flex interface only allows length, not start and end indices)\n@@ -851,6 +768,11 @@ def create_causal_mask(\n     mask_factory_function = causal_mask_function\n     mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]\n \n+    # Defaulting to using non-vmap based mask creations except when detecting\n+    # users passing custom mask functions (as we cannot guarantee that they\n+    # are properly index-based as required by our implementation).\n+    use_vmap = False\n+\n     # Do not allow skip if we are compiling (this is to match BC)\n     # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n     if _is_torch_xpu_available:\n@@ -867,14 +789,16 @@ def create_causal_mask(\n             raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n         mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n         allow_is_causal_skip = False\n+        use_vmap = True\n     if and_mask_function is not None:\n         if not _is_torch_greater_or_equal_than_2_6:\n             raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n         mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n         allow_is_causal_skip = False\n+        use_vmap = True\n \n     # If we detected packing format\n-    if packed_sequence_mask is not None and _is_torch_greater_or_equal_than_2_6:\n+    if packed_sequence_mask is not None:\n         mask_factory_function = and_masks(mask_factory_function, packed_sequence_mask_function(packed_sequence_mask))\n         allow_is_causal_skip = False\n \n@@ -889,6 +813,7 @@ def create_causal_mask(\n         allow_is_causal_skip=allow_is_causal_skip,  # additional kwarg for sdpa\n         dtype=dtype,  # Additional kwarg for eager\n         config=config,  # Pass the config as well, in case someone wants to easily have their own mask_interface\n+        use_vmap=use_vmap,  # Short-circuit to non-vmap expansions for the mask\n     )\n     return causal_mask\n \n@@ -942,6 +867,10 @@ def create_bidirectional_mask(\n \n     # Allow skipping the mask creation except we have additional masking operators (and/or masks)\n     allow_is_bidirectional_skip = True\n+    # Defaulting to using non-vmap based mask creations except when detecting\n+    # users passing custom mask functions (as we cannot guarantee that they\n+    # are properly index-based as required by our implementation).\n+    use_vmap = False\n \n     # Allow slight deviations from the base mask\n     # Note that it is very important to apply this before any other deviations of the mask (such as packed sequence mask,\n@@ -951,11 +880,13 @@ def create_bidirectional_mask(\n             raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n         mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n         allow_is_bidirectional_skip = False\n+        use_vmap = True\n     if and_mask_function is not None:\n         if not _is_torch_greater_or_equal_than_2_6:\n             raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n         mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n         allow_is_bidirectional_skip = False\n+        use_vmap = True\n \n     # We now create the mask\n     attention_mask = mask_interface(\n@@ -970,6 +901,7 @@ def create_bidirectional_mask(\n         allow_is_bidirectional_skip=allow_is_bidirectional_skip,\n         dtype=dtype,  # Additional kwarg for eager\n         config=config,  # Pass the config as well, in case someone wants to easily have their own mask_interface\n+        use_vmap=use_vmap,  # Short-circuit to non-vmap expansions for the mask\n     )\n     return attention_mask\n \n@@ -1032,6 +964,10 @@ def create_sliding_window_causal_mask(\n     mask_factory_function = sliding_window_causal_mask_function(sliding_window)\n     mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]\n \n+    # Defaulting to using non-vmap based mask creations except when detecting\n+    # users passing custom mask functions (as we cannot guarantee that they\n+    # are properly index-based as required by our implementation).\n+    use_vmap = False\n     # Do not allow skip if we are compiling (this is to match BC)\n     # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n     allow_is_causal_skip = not getattr(past_key_values, \"is_compileable\", False)\n@@ -1044,14 +980,16 @@ def create_sliding_window_causal_mask(\n             raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n         mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n         allow_is_causal_skip = False\n+        use_vmap = True\n     if and_mask_function is not None:\n         if not _is_torch_greater_or_equal_than_2_6:\n             raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n         mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n         allow_is_causal_skip = False\n+        use_vmap = True\n \n     # If we detected packing format\n-    if packed_sequence_mask is not None and _is_torch_greater_or_equal_than_2_6:\n+    if packed_sequence_mask is not None:\n         mask_factory_function = and_masks(mask_factory_function, packed_sequence_mask_function(packed_sequence_mask))\n         allow_is_causal_skip = False\n \n@@ -1067,6 +1005,7 @@ def create_sliding_window_causal_mask(\n         local_size=sliding_window,  # Additional kwarg for sdpa\n         dtype=dtype,  # Additional kwarg for eager\n         config=config,  # Pass the config as well, in case someone wants to easily have their own mask_interface\n+        use_vmap=use_vmap,  # Short-circuit to non-vmap expansions for the mask\n     )\n     return causal_mask\n \n@@ -1140,20 +1079,13 @@ def create_chunked_causal_mask(\n         left_padding_tokens = (attention_mask.cumsum(dim=-1) == torch.zeros_like(attention_mask)).sum(dim=-1)\n     else:\n         left_padding_tokens = torch.zeros(batch_size, device=cache_position.device, dtype=int)\n-    # Raise a warning for older versions if the problematic left-padding situation arises\n-    if (\n-        not _is_torch_greater_or_equal_than_2_6\n-        and kv_length + kv_offset > chunk_size\n-        and (left_padding_tokens > 0).any()\n-    ):\n-        logger.warning_once(\n-            \"Due to limitations of your current torch version, we cannot correctly account for the left-padding \"\n-            \"when computing the chunked attention pattern. This will lead to a wrong attention mask for the padded \"\n-            \"sequences. Behavior will be undefined. Please upgrade to `torch>=2.6` to solve this issue.\"\n-        )\n     mask_factory_function = chunked_causal_mask_function(chunk_size, left_padding_tokens)\n     mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]\n \n+    # Defaulting to using non-vmap based mask creations except when detecting\n+    # users passing custom mask functions (as we cannot guarantee that they\n+    # are properly index-based as required by our implementation).\n+    use_vmap = False\n     # Do not allow skip if we are compiling (this is to match BC)\n     # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n     allow_is_causal_skip = not getattr(past_key_values, \"is_compileable\", False)\n@@ -1166,14 +1098,16 @@ def create_chunked_causal_mask(\n             raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n         mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n         allow_is_causal_skip = False\n+        use_vmap = True\n     if and_mask_function is not None:\n         if not _is_torch_greater_or_equal_than_2_6:\n             raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n         mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n         allow_is_causal_skip = False\n+        use_vmap = True\n \n     # If we detected packing format\n-    if packed_sequence_mask is not None and _is_torch_greater_or_equal_than_2_6:\n+    if packed_sequence_mask is not None:\n         mask_factory_function = and_masks(mask_factory_function, packed_sequence_mask_function(packed_sequence_mask))\n         allow_is_causal_skip = False\n \n@@ -1189,6 +1123,7 @@ def create_chunked_causal_mask(\n         local_size=chunk_size,  # Additional kwarg for sdpa\n         dtype=dtype,  # Additional kwarg for eager\n         config=config,  # Pass the config as well, in case someone wants to easily have their own mask_interface\n+        use_vmap=use_vmap,  # Short-circuit to non-vmap expansions for the mask\n     )\n     return causal_mask\n "
        }
    ],
    "stats": {
        "total": 405,
        "additions": 115,
        "deletions": 290
    }
}