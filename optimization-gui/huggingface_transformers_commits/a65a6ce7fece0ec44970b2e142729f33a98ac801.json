{
    "author": "akakakakakaa",
    "message": "fix error in _get_eval_sampler when group_by_length enabled (#34237)\n\n* remove self in _get_eval_sampler\r\n\r\n* remove self in front of _get_eval_sampler",
    "sha": "a65a6ce7fece0ec44970b2e142729f33a98ac801",
    "files": [
        {
            "sha": "7890e084871a1ec2892b0a90227cacb7d2756014",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a65a6ce7fece0ec44970b2e142729f33a98ac801/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a65a6ce7fece0ec44970b2e142729f33a98ac801/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=a65a6ce7fece0ec44970b2e142729f33a98ac801",
            "patch": "@@ -965,7 +965,7 @@ def get_train_dataloader(self) -> DataLoader:\n         return self.accelerator.prepare(DataLoader(train_dataset, **dataloader_params))\n \n     def _get_eval_sampler(self, eval_dataset: Dataset) -> Optional[torch.utils.data.Sampler]:\n-        if self.eval_dataset is None or not has_length(self.eval_dataset):\n+        if eval_dataset is None or not has_length(eval_dataset):\n             return None\n         # Build the sampler.\n \n@@ -986,18 +986,18 @@ def _get_eval_sampler(self, eval_dataset: Dataset) -> Optional[torch.utils.data.\n                 return SequentialSampler(eval_dataset)\n \n         if self.args.group_by_length:\n-            if is_datasets_available() and isinstance(self.eval_dataset, datasets.Dataset):\n+            if is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):\n                 lengths = (\n-                    self.eval_dataset[self.args.length_column_name]\n-                    if self.args.length_column_name in self.eval_dataset.column_names\n+                    eval_dataset[self.args.length_column_name]\n+                    if self.args.length_column_name in eval_dataset.column_names\n                     else None\n                 )\n             else:\n                 lengths = None\n             model_input_name = self.tokenizer.model_input_names[0] if self.tokenizer is not None else None\n             return LengthGroupedSampler(\n                 self.args.eval_batch_size,\n-                dataset=self.eval_dataset,\n+                dataset=eval_dataset,\n                 lengths=lengths,\n                 model_input_name=model_input_name,\n             )"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 5,
        "deletions": 5
    }
}