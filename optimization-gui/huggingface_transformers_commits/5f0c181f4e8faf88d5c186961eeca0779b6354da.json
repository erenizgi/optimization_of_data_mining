{
    "author": "yonigozlan",
    "message": "Uniformize kwargs for image-text-to-text processors (#32544)\n\n* uniformize FUYU processor kwargs\r\n\r\n* Uniformize instructblip processor kwargs\r\n\r\n* Fix processor kwargs and tests Fuyu, InstructBlip, Kosmos2\r\n\r\n* Uniformize llava_next processor\r\n\r\n* Fix save_load test for processor with chat_template only as extra init args\r\n\r\n* Fix import Unpack\r\n\r\n* Fix Fuyu Processor import\r\n\r\n* Fix FuyuProcessor import\r\n\r\n* Fix FuyuProcessor\r\n\r\n* Add defaults for specific kwargs kosmos2\r\n\r\n* Fix Udop to return BatchFeature instead of BatchEncoding and uniformize kwargs\r\n\r\n* Add tests processor Udop\r\n\r\n* remove Copied from in processing Udop as change of input orders caused by BatchEncoding -> BatchFeature\r\n\r\n* Fix overwrite tests kwargs processors\r\n\r\n* Add warnings and BC for changes in processor inputs order, change docs, add BC for text_pair as arg for Udop\r\n\r\n* Fix processing test fuyu\r\n\r\n* remove unnecessary pad_token check in instructblip ProcessorTest\r\n\r\n* Fix BC tests and cleanup\r\n\r\n* FIx imports fuyu\r\n\r\n* Uniformize Pix2Struct\r\n\r\n* Fix wrong name for FuyuProcessorKwargs\r\n\r\n* Fix slow tests reversed inputs align fuyu llava-next, change udop warning\r\n\r\n* Fix wrong logging import udop\r\n\r\n* Add check images text input order\r\n\r\n* Fix copies\r\n\r\n* change text pair handling when positional arg\r\n\r\n* rebase on main, fix imports in test_processing_common\r\n\r\n* remove optional args and udop uniformization from this PR\r\n\r\n* fix failing tests\r\n\r\n* remove unnecessary test, fix processing utils and test processing common\r\n\r\n* cleanup Unpack\r\n\r\n* cleanup\r\n\r\n* fix conflict grounding dino",
    "sha": "5f0c181f4e8faf88d5c186961eeca0779b6354da",
    "files": [
        {
            "sha": "0d34d95a79810962aad64fc87d0bc9b8227edee6",
            "filename": "docs/source/en/model_doc/align.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -46,7 +46,7 @@ url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n image = Image.open(requests.get(url, stream=True).raw)\n candidate_labels = [\"an image of a cat\", \"an image of a dog\"]\n \n-inputs = processor(text=candidate_labels, images=image, return_tensors=\"pt\")\n+inputs = processor(images=image ,text=candidate_labels, return_tensors=\"pt\")\n \n with torch.no_grad():\n     outputs = model(**inputs)"
        },
        {
            "sha": "bd55737da58ff8df3753cff4b9e27dfdcf7bf2a9",
            "filename": "docs/source/en/model_doc/fuyu.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -18,16 +18,16 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Fuyu model was created by [ADEPT](https://www.adept.ai/blog/fuyu-8b), and authored by Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, Sağnak Taşırlar. \n+The Fuyu model was created by [ADEPT](https://www.adept.ai/blog/fuyu-8b), and authored by Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, Sağnak Taşırlar.\n \n-The authors introduced Fuyu-8B, a decoder-only multimodal model based on the classic transformers architecture, with query and key normalization. A linear encoder is added to create multimodal embeddings from image inputs. \n+The authors introduced Fuyu-8B, a decoder-only multimodal model based on the classic transformers architecture, with query and key normalization. A linear encoder is added to create multimodal embeddings from image inputs.\n \n By treating image tokens like text tokens and using a special image-newline character, the model knows when an image line ends. Image positional embeddings are removed. This avoids the need for different training phases for various image resolutions. With 8 billion parameters and licensed under CC-BY-NC, Fuyu-8B is notable for its ability to handle both text and images, its impressive context size of 16K, and its overall performance.\n \n <Tip warning={true}>\n \n The `Fuyu` models were trained using `bfloat16`, but the original inference uses `float16` The checkpoints uploaded on the hub use `torch_dtype = 'float16'` which will be\n-used by the `AutoModel` API to cast the checkpoints from `torch.float32` to `torch.float16`. \n+used by the `AutoModel` API to cast the checkpoints from `torch.float32` to `torch.float16`.\n \n The `dtype` of the online weights is mostly irrelevant, unless you are using `torch_dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online) then it will be cast to the default `dtype` of `torch` (becomes `torch.float32`). Users should specify the `torch_dtype` they want, and if they don't it will be `torch.float32`.\n \n@@ -56,7 +56,7 @@ tar -xvf 8b_base_model_release.tar\n ```\n Then, model can be loaded via:\n \n-```py \n+```py\n from transformers import FuyuConfig, FuyuForCausalLM\n model_config = FuyuConfig()\n model = FuyuForCausalLM(model_config).from_pretrained('/output/path')\n@@ -81,7 +81,7 @@ text_prompt = \"Generate a coco-style caption.\\\\n\"\n \n bus_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bus.png\"\n bus_image_pil = Image.open(io.BytesIO(requests.get(bus_image_url).content))\n-inputs_to_model = processor(text=text_prompt, images=bus_image_pil)\n+inputs_to_model = processor(images=bus_image_pil, text=text_prompt)\n \n \n ```\n@@ -90,7 +90,7 @@ This model was contributed by [Molbap](https://huggingface.co/Molbap).\n The original code can be found [here](https://github.com/persimmon-ai-labs/adept-inference).\n \n - Fuyu uses a `sentencepiece` based tokenizer, with a `Unigram` model. It supports bytefallback, which is only available in `tokenizers==0.14.0` for the fast tokenizer.\n-The `LlamaTokenizer` is used as it is a standard wrapper around sentencepiece. \n+The `LlamaTokenizer` is used as it is a standard wrapper around sentencepiece.\n \n - The authors suggest to use the following prompt for image captioning: `f\"Generate a coco-style caption.\\\\n\"`\n "
        },
        {
            "sha": "4c041f4e8963aba46ec31f756650bb8dc848a357",
            "filename": "docs/source/en/model_doc/llava_next.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -133,7 +133,7 @@ import requests\n \n processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n \n-model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True) \n+model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n model.to(\"cuda:0\")\n \n # prepare image and text prompt, using the appropriate prompt template\n@@ -150,7 +150,7 @@ conversation = [\n     },\n ]\n prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n-inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda:0\")\n+inputs = processor(image, prompt, return_tensors=\"pt\").to(\"cuda:0\")\n \n # autoregressively complete prompt\n output = model.generate(**inputs, max_new_tokens=100)\n@@ -222,7 +222,7 @@ prompts = [prompt_1, prompt_2]\n \n # We can simply feed images in the order they have to be used in the text prompt\n # Each \"<image>\" token uses one image leaving the next for the subsequent \"<image>\" tokens\n-inputs = processor(text=prompts, images=[image_stop, image_cats, image_snowman], padding=True, return_tensors=\"pt\").to(model.device)\n+inputs = processor(images=[image_stop, image_cats, image_snowman], text=prompts, padding=True, return_tensors=\"pt\").to(model.device)\n \n # Generate\n generate_ids = model.generate(**inputs, max_new_tokens=30)\n@@ -266,8 +266,8 @@ First make sure to install flash-attn. Refer to the [original repository of Flas\n from transformers import LlavaNextForConditionalGeneration\n \n model = LlavaNextForConditionalGeneration.from_pretrained(\n-    model_id, \n-    torch_dtype=torch.float16, \n+    model_id,\n+    torch_dtype=torch.float16,\n     low_cpu_mem_usage=True,\n     use_flash_attention_2=True\n ).to(0)"
        },
        {
            "sha": "f834aecf6932a3d555a1a06018c13639458b29e4",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -1575,7 +1575,7 @@ def forward(\n         >>> image = Image.open(requests.get(url, stream=True).raw)\n \n         >>> inputs = processor(\n-        ...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n+        ...     images=image, text=[\"a photo of a cat\", \"a photo of a dog\"], return_tensors=\"pt\", padding=True\n         ... )\n \n         >>> outputs = model(**inputs)"
        },
        {
            "sha": "792f614b10bea0340add68fe1e655285ace087a6",
            "filename": "src/transformers/models/align/processing_align.py",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -19,11 +19,7 @@\n from typing import List, Union\n \n from ...image_utils import ImageInput\n-from ...processing_utils import (\n-    ProcessingKwargs,\n-    ProcessorMixin,\n-    Unpack,\n-)\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n \n \n@@ -76,8 +72,8 @@ def __init__(self, image_processor, tokenizer):\n \n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n         images: ImageInput = None,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[AlignProcessorKwargs],\n@@ -90,13 +86,13 @@ def __call__(\n         to the doctsring of the above two methods for more information.\n \n         Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n             text (`str`, `List[str]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors of a particular framework. Acceptable values are:\n                     - `'tf'`: Return TensorFlow `tf.constant` objects.\n@@ -114,6 +110,9 @@ def __call__(\n         \"\"\"\n         if text is None and images is None:\n             raise ValueError(\"You must specify either text or images.\")\n+        # check if images and text inputs are reversed for BC\n+        images, text = _validate_images_text_input_order(images, text)\n+\n         output_kwargs = self._merge_kwargs(\n             AlignProcessorKwargs,\n             tokenizer_init_kwargs=self.tokenizer.init_kwargs,"
        },
        {
            "sha": "d2fd197073c032e7647a3e25f9f5488bceeac70a",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -265,7 +265,7 @@ def forward(\n         >>> image = Image.open(requests.get(url, stream=True).raw)\n         >>> prompt = \"Generate a coco-style caption.\\n\"\n \n-        >>> inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n+        >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n         >>> outputs = model(**inputs)\n \n         >>> generated_ids = model.generate(**inputs, max_new_tokens=7)"
        },
        {
            "sha": "ff7d2c547dc44c4bb93ea9cd1607ac6061110232",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 46,
            "deletions": 45,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -21,9 +21,10 @@\n \n import numpy as np\n \n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import PaddingStrategy, TruncationStrategy\n-from ...utils import TensorType, is_torch_available, logging, requires_backends\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import is_torch_available, logging, requires_backends\n \n \n if is_torch_available():\n@@ -49,6 +50,24 @@\n BEGINNING_OF_ANSWER_STRING = \"<0x04>\"  # <boa>\n \n \n+class FuyuProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"add_special_tokens\": True,\n+            \"padding\": False,\n+            \"stride\": 0,\n+            \"return_attention_mask\": True,\n+            \"return_overflowing_tokens\": False,\n+            \"return_special_tokens_mask\": False,\n+            \"return_offsets_mapping\": False,\n+            \"return_token_type_ids\": False,\n+            \"return_length\": False,\n+            \"verbose\": True,\n+        },\n+        \"images_kwargs\": {},\n+    }\n+\n+\n def full_unpacked_stream_to_tensor(\n     all_bi_tokens_to_place: List[int],\n     full_unpacked_stream: List[\"torch.Tensor\"],\n@@ -452,23 +471,11 @@ def get_sample_encoding(\n \n     def __call__(\n         self,\n-        text=None,\n-        images=None,\n-        add_special_tokens: bool = True,\n-        return_attention_mask: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n-        stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_overflowing_tokens: bool = False,\n-        return_special_tokens_mask: bool = False,\n-        return_offsets_mapping: bool = False,\n-        return_token_type_ids: bool = False,\n-        return_length: bool = False,\n-        verbose: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        **kwargs,\n+        images: ImageInput = None,\n+        text: Optional[Union[str, List[str], TextInput, PreTokenizedInput]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[FuyuProcessorKwargs],\n     ) -> \"FuyuBatchFeature\":\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n@@ -478,13 +485,13 @@ def __call__(\n         of the above two methods for more information.\n \n         Args:\n+            images (`PIL.Image.Image`, `List[PIL.Image.Image]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n             text (`str`, `List[str]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `List[PIL.Image.Image]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n \n         Returns:\n             [`FuyuBatchEncoding`]: A [`FuyuBatchEncoding`] with the following fields:\n@@ -498,31 +505,24 @@ def __call__(\n         requires_backends(self, [\"torch\"])\n \n         # --- Check input validity ---\n-        if not return_attention_mask:\n-            raise ValueError(\"`return_attention_mask=False` is not supported for this model.\")\n         if text is None and images is None:\n             raise ValueError(\"You have to specify either text or images. Both cannot be None.\")\n+        # check if images and text inputs are reversed for BC\n+        images, text = _validate_images_text_input_order(images, text)\n+\n+        output_kwargs = self._merge_kwargs(\n+            FuyuProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        if not output_kwargs[\"text_kwargs\"].setdefault(\"return_attention_mask\", True):\n+            raise ValueError(\"`return_attention_mask=False` is not supported for this model.\")\n+\n         if text is not None and images is None:\n             logger.warning(\"You are processing a text with no associated image. Make sure it is intended.\")\n             self.current_processor = self.tokenizer\n-            text_encoding = self.tokenizer(\n-                text=text,\n-                add_special_tokens=add_special_tokens,\n-                padding=padding,\n-                truncation=truncation,\n-                max_length=max_length,\n-                stride=stride,\n-                pad_to_multiple_of=pad_to_multiple_of,\n-                return_attention_mask=return_attention_mask,\n-                return_overflowing_tokens=return_overflowing_tokens,\n-                return_special_tokens_mask=return_special_tokens_mask,\n-                return_offsets_mapping=return_offsets_mapping,\n-                return_token_type_ids=return_token_type_ids,\n-                return_length=return_length,\n-                verbose=verbose,\n-                return_tensors=return_tensors,\n-                **kwargs,\n-            )\n+            text_encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n             return text_encoding\n \n         if text is None and images is not None:\n@@ -537,7 +537,8 @@ def __call__(\n         # --- Preprocess images using self.image_processor ---\n \n         # FIXME - We hard code \"pt\" here because the rest of the processing assumes torch tensors\n-        image_encoding = self.image_processor.preprocess(images, return_tensors=\"pt\")\n+        output_kwargs[\"images_kwargs\"][\"return_tensors\"] = \"pt\"\n+        image_encoding = self.image_processor.preprocess(images, **output_kwargs[\"images_kwargs\"])\n         batch_images = image_encoding[\"images\"]\n         image_unpadded_heights = image_encoding[\"image_unpadded_heights\"]\n         image_unpadded_widths = image_encoding[\"image_unpadded_widths\"]\n@@ -568,7 +569,7 @@ def __call__(\n             )\n             all_encodings.append(sample_encoding)\n         batch_encoding = self._left_pad_inputs_with_attention_mask(\n-            model_inputs=all_encodings, return_attention_mask=return_attention_mask\n+            model_inputs=all_encodings, return_attention_mask=True\n         )\n         return FuyuBatchFeature(data=batch_encoding)\n "
        },
        {
            "sha": "f6d35c1e6f72597a46d78996d06d4d284b1609d5",
            "filename": "src/transformers/models/instructblip/processing_instructblip.py",
            "status": "modified",
            "additions": 44,
            "deletions": 58,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -17,26 +17,41 @@\n \"\"\"\n \n import os\n-from typing import List, Optional, Union\n+from typing import List, Union\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ProcessorMixin\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import (\n     AddedToken,\n     BatchEncoding,\n-    PaddingStrategy,\n     PreTokenizedInput,\n     TextInput,\n-    TruncationStrategy,\n )\n-from ...utils import TensorType, logging\n+from ...utils import logging\n from ..auto import AutoTokenizer\n \n \n logger = logging.get_logger(__name__)\n \n \n+class InstructBlipProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"add_special_tokens\": True,\n+            \"padding\": False,\n+            \"stride\": 0,\n+            \"return_overflowing_tokens\": False,\n+            \"return_special_tokens_mask\": False,\n+            \"return_offsets_mapping\": False,\n+            \"return_token_type_ids\": False,\n+            \"return_length\": False,\n+            \"verbose\": True,\n+        },\n+        \"images_kwargs\": {},\n+    }\n+\n+\n class InstructBlipProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs an InstructBLIP processor which wraps a BLIP image processor and a LLaMa/T5 tokenizer into a single\n@@ -72,31 +87,33 @@ def __call__(\n         self,\n         images: ImageInput = None,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n-        add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n-        stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_attention_mask: Optional[bool] = None,\n-        return_overflowing_tokens: bool = False,\n-        return_special_tokens_mask: bool = False,\n-        return_offsets_mapping: bool = False,\n-        return_token_type_ids: bool = False,\n-        return_length: bool = False,\n-        verbose: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        **kwargs,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[InstructBlipProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         This method uses [`BlipImageProcessor.__call__`] method to prepare image(s) for the model, and\n         [`BertTokenizerFast.__call__`] to prepare text for the model.\n \n         Please refer to the docstring of the above two methods for more information.\n+        Args:\n+            images (`ImageInput`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`TextInput`, `PreTokenizedInput`, `List[TextInput]`, `List[PreTokenizedInput]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n         \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You have to specify at least images or text.\")\n \n+        output_kwargs = self._merge_kwargs(\n+            InstructBlipProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n         encoding = BatchFeature()\n \n         if text is not None:\n@@ -105,24 +122,7 @@ def __call__(\n             elif not isinstance(text, list) and not isinstance(text[0], str):\n                 raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n-            _text_encoding = self.tokenizer(\n-                text=text,\n-                add_special_tokens=add_special_tokens,\n-                padding=padding,\n-                truncation=truncation,\n-                max_length=max_length,\n-                stride=stride,\n-                pad_to_multiple_of=pad_to_multiple_of,\n-                return_attention_mask=return_attention_mask,\n-                return_overflowing_tokens=return_overflowing_tokens,\n-                return_special_tokens_mask=return_special_tokens_mask,\n-                return_offsets_mapping=return_offsets_mapping,\n-                return_token_type_ids=return_token_type_ids,\n-                return_length=return_length,\n-                verbose=verbose,\n-                return_tensors=None,  # needed to concatenate below\n-                **kwargs,\n-            )\n+            _text_encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n \n             # if we know how many query tokens, expand text inside processor. We need this hacky manipulation\n             # because BLIP expects image tokens to be at the beginning even before BOS token\n@@ -145,31 +145,17 @@ def __call__(\n                     )\n \n             # cast to desired return tensors type after concatenating\n-            text_encoding = BatchEncoding(text_encoding, tensor_type=return_tensors)\n-            encoding.update(text_encoding)\n-            qformer_text_encoding = self.qformer_tokenizer(\n-                text=text,\n-                add_special_tokens=add_special_tokens,\n-                padding=padding,\n-                truncation=truncation,\n-                max_length=max_length,\n-                stride=stride,\n-                pad_to_multiple_of=pad_to_multiple_of,\n-                return_attention_mask=return_attention_mask,\n-                return_overflowing_tokens=return_overflowing_tokens,\n-                return_special_tokens_mask=return_special_tokens_mask,\n-                return_offsets_mapping=return_offsets_mapping,\n-                return_token_type_ids=return_token_type_ids,\n-                return_length=return_length,\n-                verbose=verbose,\n-                return_tensors=return_tensors,\n-                **kwargs,\n+            text_encoding = BatchEncoding(\n+                text_encoding, tensor_type=output_kwargs[\"common_kwargs\"].get(\"return_tensors\")\n             )\n+\n+            encoding.update(text_encoding)\n+            qformer_text_encoding = self.qformer_tokenizer(text, **output_kwargs[\"text_kwargs\"])\n             encoding[\"qformer_input_ids\"] = qformer_text_encoding.pop(\"input_ids\")\n             encoding[\"qformer_attention_mask\"] = qformer_text_encoding.pop(\"attention_mask\")\n \n         if images is not None:\n-            image_encoding = self.image_processor(images, return_tensors=return_tensors)\n+            image_encoding = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n             encoding.update(image_encoding)\n \n         return encoding"
        },
        {
            "sha": "76108789718b41efa498e023d69f3b1d80d9cb1f",
            "filename": "src/transformers/models/kosmos2/processing_kosmos2.py",
            "status": "modified",
            "additions": 67,
            "deletions": 42,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -21,10 +21,9 @@\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput, is_batched\n-from ...processing_utils import ProcessorMixin\n+from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n from ...tokenization_utils import AddedToken\n-from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, TextInput, TruncationStrategy\n-from ...utils import TensorType\n+from ...tokenization_utils_base import BatchEncoding, TextInput\n \n \n BboxInput = Union[\n@@ -35,6 +34,37 @@\n ]\n \n \n+class Kosmos2ImagesKwargs(ImagesKwargs, total=False):\n+    bboxes: Optional[List[float]]\n+    num_image_tokens: Optional[int]\n+    first_image_token_id: Optional[int]\n+\n+\n+class Kosmos2TextKwargs(TextKwargs, total=False):\n+    add_eos_token: Optional[bool]\n+\n+\n+class Kosmos2ProcessorKwargs(ProcessingKwargs, total=False):\n+    text_kwargs: Kosmos2TextKwargs\n+    images_kwargs: Kosmos2ImagesKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"add_special_tokens\": True,\n+            \"padding\": False,\n+            \"stride\": 0,\n+            \"return_overflowing_tokens\": False,\n+            \"return_special_tokens_mask\": False,\n+            \"return_offsets_mapping\": False,\n+            \"return_token_type_ids\": False,\n+            \"verbose\": True,\n+            \"add_eos_token\": False,\n+        },\n+        \"images_kwargs\": {\n+            \"num_image_tokens\": 64,\n+        },\n+    }\n+\n+\n class Kosmos2Processor(ProcessorMixin):\n     r\"\"\"\n     Constructs an KOSMOS-2 processor which wraps a KOSMOS-2 image processor and a KOSMOS-2 tokenizer into a single\n@@ -56,7 +86,7 @@ class Kosmos2Processor(ProcessorMixin):\n     attributes = [\"image_processor\", \"tokenizer\"]\n     valid_kwargs = [\"num_patch_index_tokens\"]\n     image_processor_class = \"CLIPImageProcessor\"\n-    tokenizer_class = (\"XLMRobertaTokenizer\", \"XLMRobertaTokenizerFast\")\n+    tokenizer_class = \"AutoTokenizer\"\n \n     def __init__(self, image_processor, tokenizer, num_patch_index_tokens=1024, *kwargs):\n         tokenizer.return_token_type_ids = False\n@@ -107,20 +137,9 @@ def __call__(\n         self,\n         images: ImageInput = None,\n         text: Union[TextInput, List[TextInput]] = None,\n-        bboxes: BboxInput = None,\n-        num_image_tokens: Optional[int] = 64,\n-        first_image_token_id: Optional[int] = None,\n-        add_special_tokens: bool = True,\n-        add_eos_token: bool = False,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_attention_mask: Optional[bool] = None,\n-        return_length: bool = False,\n-        verbose: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        **kwargs,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[Kosmos2ProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         This method uses [`CLIPImageProcessor.__call__`] method to prepare image(s) for the model, and\n@@ -145,10 +164,25 @@ def __call__(\n         if images is None and text is None:\n             raise ValueError(\"You have to specify either images or text.\")\n \n+        output_kwargs = self._merge_kwargs(\n+            Kosmos2ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        bboxes = output_kwargs[\"images_kwargs\"].pop(\"bboxes\", None)\n+        num_image_tokens = output_kwargs[\"images_kwargs\"].pop(\"num_image_tokens\", 64)\n+        first_image_token_id = output_kwargs[\"images_kwargs\"].pop(\"first_image_token_id\", None)\n+        add_eos_token = output_kwargs[\"text_kwargs\"].pop(\"add_eos_token\", False)\n+\n+        add_special_tokens = output_kwargs[\"text_kwargs\"][\"add_special_tokens\"]\n+        padding = output_kwargs[\"text_kwargs\"][\"padding\"]\n+        return_tensors = output_kwargs[\"text_kwargs\"].setdefault(\"return_tensors\", None)\n+\n         encoding = BatchFeature()\n \n         if images is not None:\n-            image_encoding = self.image_processor(images, return_tensors=return_tensors)\n+            image_encoding = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n             encoding.update(image_encoding)\n \n         if text is not None:\n@@ -159,21 +193,18 @@ def __call__(\n                     text = f\"{self.tokenizer.bos_token}{text}\"\n                 elif isinstance(text, list):\n                     text = [f\"{self.tokenizer.bos_token}{s}\" for s in text]\n-\n-            text_encoding = self.tokenizer(\n-                text=text,\n-                add_special_tokens=(add_special_tokens and add_eos_token),\n-                padding=padding and images is None,\n-                truncation=truncation,\n-                max_length=max_length,\n-                pad_to_multiple_of=pad_to_multiple_of if images is None else pad_to_multiple_of,\n-                return_attention_mask=return_attention_mask,\n-                verbose=verbose,\n-                return_tensors=return_tensors if images is None else None,\n-                **kwargs,\n+            output_kwargs[\"text_kwargs\"][\"add_special_tokens\"] = (\n+                output_kwargs[\"text_kwargs\"][\"add_special_tokens\"] and add_eos_token\n             )\n+            output_kwargs[\"text_kwargs\"][\"padding\"] = padding if images is None else False\n+            output_kwargs[\"text_kwargs\"][\"return_tensors\"] = return_tensors if images is None else None\n+            text_encoding = self.tokenizer(text=text, **output_kwargs[\"text_kwargs\"])\n             encoding.update(text_encoding)\n \n+        output_kwargs[\"text_kwargs\"][\"add_special_tokens\"] = add_special_tokens\n+        output_kwargs[\"text_kwargs\"][\"padding\"] = padding\n+        output_kwargs[\"text_kwargs\"][\"return_tensors\"] = return_tensors\n+\n         if text is not None and images is not None:\n             # Use the id of the first token after <unk>\n             if first_image_token_id is None:\n@@ -218,18 +249,12 @@ def __call__(\n                 )\n                 _, min_len_not_padded = sorted_length[0]\n                 idx, _ = sorted_length[-1]\n-\n-                text_encoding = self.tokenizer(\n-                    text=[text[idx]],\n-                    add_special_tokens=(add_special_tokens and add_eos_token),\n-                    padding=padding,\n-                    truncation=truncation,\n-                    max_length=max_length,\n-                    pad_to_multiple_of=pad_to_multiple_of,\n-                    verbose=verbose,\n-                    return_tensors=None,\n-                    **kwargs,\n+                output_kwargs[\"text_kwargs\"][\"add_special_tokens\"] = (\n+                    output_kwargs[\"text_kwargs\"][\"add_special_tokens\"] and add_eos_token\n                 )\n+                output_kwargs[\"text_kwargs\"][\"return_tensors\"] = None\n+\n+                text_encoding = self.tokenizer(text=[text[idx]], **output_kwargs[\"text_kwargs\"])\n                 max_len_padded = len(text_encoding.input_ids[0])\n \n                 if min_len_not_padded != max_len_padded:"
        },
        {
            "sha": "41118599ec93b7126f9595bec5646d94818342d3",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -715,7 +715,9 @@ def preprocess(\n             image_patches = self.get_image_patches(\n                 image,\n                 image_grid_pinpoints,\n-                size=(size[\"shortest_edge\"], size[\"shortest_edge\"]),\n+                size=(size[\"shortest_edge\"], size[\"shortest_edge\"])\n+                if \"shortest_edge\" in size\n+                else (min(size[\"height\"], size[\"width\"]), min(size[\"height\"], size[\"width\"])),\n                 patch_size=crop_size[\"height\"],\n                 resample=resample,\n                 data_format=input_data_format,"
        },
        {
            "sha": "bb07ddfdaccd96737e8503639d20a0c1b92c36f0",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -763,7 +763,7 @@ def forward(\n         >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n         >>> image = Image.open(requests.get(url, stream=True).raw)\n \n-        >>> inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n+        >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n \n         >>> # Generate\n         >>> generate_ids = model.generate(**inputs, max_length=30)"
        },
        {
            "sha": "ce11be6d6309a85437168f387098f00ab2915c3c",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 34,
            "deletions": 44,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -16,19 +16,30 @@\n Processor class for LLaVa-NeXT.\n \"\"\"\n \n-from typing import List, Optional, Union\n+from typing import List, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_processing_utils import select_best_resolution\n from ...image_utils import ImageInput, get_image_size, to_numpy_array\n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType, logging\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n+class LlavaNextProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+        },\n+        \"images_kwargs\": {\n+            \"do_pad\": True,\n+        },\n+    }\n+\n+\n class LlavaNextProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a LLaVa-NeXT processor which wraps a LLaVa-NeXT image processor and a LLaMa tokenizer into a single processor.\n@@ -74,13 +85,11 @@ def __init__(\n \n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n         images: ImageInput = None,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n-        do_pad: Optional[bool] = True,\n-        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[LlavaNextProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n@@ -90,36 +99,13 @@ def __call__(\n         of the above two methods for more information.\n \n         Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n             text (`str`, `List[str]`, `List[List[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n-                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n-                index) among:\n-                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n-                  sequence if provided).\n-                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n-                  acceptable input length for the model if that argument is not provided.\n-                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n-                  lengths).\n-            max_length (`int`, *optional*):\n-                Maximum length of the returned list and optionally padding length (see above).\n-            do_pad (`bool`, *optional*, defaults to self.do_pad):\n-                Whether to pad the image. If `True` will pad the images in the batch to the largest image in the batch\n-                and create a pixel mask. Padding will be applied to the bottom and right of the image with zeros.\n-            truncation (`bool`, *optional*):\n-                Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'tf'`: Return TensorFlow `tf.constant` objects.\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-                - `'jax'`: Return JAX `jnp.ndarray` objects.\n \n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n@@ -130,8 +116,18 @@ def __call__(\n               `None`).\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n         \"\"\"\n+        if images is None and text is None:\n+            raise ValueError(\"You have to specify at least images or text.\")\n+        # check if images and text inputs are reversed for BC\n+        images, text = _validate_images_text_input_order(images, text)\n+\n+        output_kwargs = self._merge_kwargs(\n+            LlavaNextProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n         if images is not None:\n-            image_inputs = self.image_processor(images, do_pad=do_pad, return_tensors=return_tensors)\n+            image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n         else:\n             image_inputs = {}\n \n@@ -164,13 +160,7 @@ def __call__(\n                     prompt_strings.append(sample)\n                 prompt_strings = [sample.replace(\"<placeholder>\", self.image_token) for sample in prompt_strings]\n \n-        text_inputs = self.tokenizer(\n-            prompt_strings,\n-            return_tensors=return_tensors,\n-            padding=padding,\n-            truncation=truncation,\n-            max_length=max_length,\n-        )\n+        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n \n         return BatchFeature(data={**text_inputs, **image_inputs})\n "
        },
        {
            "sha": "de8c594f94c9f2099c6e5198ebe858ca2b1c931c",
            "filename": "src/transformers/models/pix2struct/processing_pix2struct.py",
            "status": "modified",
            "additions": 42,
            "deletions": 62,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -18,9 +18,34 @@\n \n from typing import List, Optional, Union\n \n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType\n+from ...feature_extraction_utils import BatchFeature\n+from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n+\n+\n+class Pix2StructImagesKwargs(ImagesKwargs, total=False):\n+    max_patches: Optional[int]\n+    header_text: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]]\n+\n+\n+class Pix2StructProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: Pix2StructImagesKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"add_special_tokens\": True,\n+            \"padding\": False,\n+            \"stride\": 0,\n+            \"return_overflowing_tokens\": False,\n+            \"return_special_tokens_mask\": False,\n+            \"return_offsets_mapping\": False,\n+            \"return_token_type_ids\": False,\n+            \"return_length\": False,\n+            \"verbose\": True,\n+        },\n+        \"images_kwargs\": {\n+            \"max_patches\": 2048,\n+        },\n+    }\n \n \n class Pix2StructProcessor(ProcessorMixin):\n@@ -50,23 +75,10 @@ def __call__(\n         self,\n         images=None,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n-        add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n-        max_patches: Optional[int] = 2048,\n-        stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_attention_mask: Optional[bool] = None,\n-        return_overflowing_tokens: bool = False,\n-        return_special_tokens_mask: bool = False,\n-        return_offsets_mapping: bool = False,\n-        return_token_type_ids: bool = False,\n-        return_length: bool = False,\n-        verbose: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        **kwargs,\n-    ) -> BatchEncoding:\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[Pix2StructProcessorKwargs],\n+    ) -> Union[BatchEncoding, BatchFeature]:\n         \"\"\"\n         This method uses [`Pix2StructImageProcessor.preprocess`] method to prepare image(s) for the model, and\n         [`T5TokenizerFast.__call__`] to prepare text for the model.\n@@ -76,59 +88,27 @@ def __call__(\n         if images is None and text is None:\n             raise ValueError(\"You have to specify either images or text.\")\n \n+        output_kwargs = self._merge_kwargs(\n+            Pix2StructProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n         # Get only text\n         if images is None and not self.image_processor.is_vqa:\n             self.current_processor = self.tokenizer\n-            text_encoding = self.tokenizer(\n-                text=text,\n-                add_special_tokens=add_special_tokens,\n-                padding=padding,\n-                truncation=truncation,\n-                max_length=max_length,\n-                stride=stride,\n-                pad_to_multiple_of=pad_to_multiple_of,\n-                return_attention_mask=return_attention_mask,\n-                return_overflowing_tokens=return_overflowing_tokens,\n-                return_special_tokens_mask=return_special_tokens_mask,\n-                return_offsets_mapping=return_offsets_mapping,\n-                return_token_type_ids=return_token_type_ids,\n-                return_length=return_length,\n-                verbose=verbose,\n-                return_tensors=return_tensors,\n-                **kwargs,\n-            )\n+            text_encoding = self.tokenizer(text=text, **output_kwargs[\"text_kwargs\"])\n             return text_encoding\n \n         if not self.image_processor.is_vqa:\n             # add pixel_values\n-            encoding_image_processor = self.image_processor(\n-                images, return_tensors=return_tensors, max_patches=max_patches, **kwargs\n-            )\n+            encoding_image_processor = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n         else:\n             # add pixel_values and bbox\n-            encoding_image_processor = self.image_processor(\n-                images, return_tensors=return_tensors, max_patches=max_patches, header_text=text, **kwargs\n-            )\n+            output_kwargs[\"images_kwargs\"].setdefault(\"header_text\", text)\n+            encoding_image_processor = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n \n         if text is not None and not self.image_processor.is_vqa:\n-            text_encoding = self.tokenizer(\n-                text=text,\n-                add_special_tokens=add_special_tokens,\n-                padding=padding,\n-                truncation=truncation,\n-                max_length=max_length,\n-                stride=stride,\n-                pad_to_multiple_of=pad_to_multiple_of,\n-                return_attention_mask=return_attention_mask,\n-                return_overflowing_tokens=return_overflowing_tokens,\n-                return_special_tokens_mask=return_special_tokens_mask,\n-                return_offsets_mapping=return_offsets_mapping,\n-                return_token_type_ids=return_token_type_ids,\n-                return_length=return_length,\n-                verbose=verbose,\n-                return_tensors=return_tensors,\n-                **kwargs,\n-            )\n+            text_encoding = self.tokenizer(text=text, **output_kwargs[\"text_kwargs\"])\n \n             if \"attention_mask\" in text_encoding:\n                 text_encoding[\"decoder_attention_mask\"] = text_encoding.pop(\"attention_mask\")"
        },
        {
            "sha": "ddeb585a757d5d5d98dcdecea05875ec69f6c8d7",
            "filename": "tests/models/align/test_modeling_align.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falign%2Ftest_modeling_align.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -626,7 +626,7 @@ def test_inference(self):\n \n         image = prepare_img()\n         texts = [\"a photo of a cat\", \"a photo of a dog\"]\n-        inputs = processor(text=texts, images=image, return_tensors=\"pt\").to(torch_device)\n+        inputs = processor(images=image, text=texts, return_tensors=\"pt\").to(torch_device)\n \n         # forward pass\n         with torch.no_grad():"
        },
        {
            "sha": "9425bddb6f703ca70f0e0de16eeb70593eff927f",
            "filename": "tests/models/fuyu/test_modeling_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -330,7 +330,7 @@ def test_greedy_generation(self):\n \n         text_prompt_coco_captioning = \"Generate a coco-style caption.\\n\"\n \n-        inputs = processor(text=text_prompt_coco_captioning, images=image, return_tensors=\"pt\")\n+        inputs = processor(images=image, text=text_prompt_coco_captioning, return_tensors=\"pt\")\n         generated_ids = model.generate(**inputs, max_new_tokens=10)\n \n         # take the last 8 tokens (in order to skip special \\n\\x04 characters) and decode them"
        },
        {
            "sha": "69a1d53e86f766f797354ef26ff10e4c0211902d",
            "filename": "tests/models/fuyu/test_processing_fuyu.py",
            "status": "modified",
            "additions": 192,
            "deletions": 20,
            "changes": 212,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -1,17 +1,25 @@\n import io\n+import tempfile\n import unittest\n \n import requests\n \n-from transformers import AutoTokenizer, is_torch_available, is_vision_available\n-from transformers.testing_utils import require_torch, require_torch_gpu, slow\n+from transformers import (\n+    AutoProcessor,\n+    AutoTokenizer,\n+    FuyuImageProcessor,\n+    FuyuProcessor,\n+    is_torch_available,\n+    is_vision_available,\n+)\n+from transformers.testing_utils import require_torch, require_vision\n+\n+from ...test_processing_common import ProcessorTesterMixin\n \n \n if is_vision_available():\n     from PIL import Image\n \n-if is_vision_available() and is_torch_available():\n-    from transformers import FuyuImageProcessor, FuyuProcessor\n \n if is_torch_available():\n     import torch\n@@ -20,21 +28,36 @@\n \n \n @require_torch\n-@require_torch_gpu\n-@slow\n-class FuyuProcessingTest(unittest.TestCase):  # TODO Which mixins do we add here?\n-    \"\"\" \"\"\"\n+@require_vision\n+class FuyuProcessingTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = FuyuProcessor\n \n     def setUp(self):\n-        pretrained_model_name = \"adept/fuyu-8b\"\n-        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n-        self.image_processor = FuyuImageProcessor()\n+        self.tmpdirname = tempfile.mkdtemp()\n+\n+        image_processor = FuyuImageProcessor()\n+        tokenizer = AutoTokenizer.from_pretrained(\"adept/fuyu-8b\")\n+\n+        processor = FuyuProcessor(image_processor=image_processor, tokenizer=tokenizer)\n+        processor.save_pretrained(self.tmpdirname)\n \n-        self.processor = FuyuProcessor(image_processor=self.image_processor, tokenizer=self.tokenizer)\n         self.text_prompt = \"Generate a coco-style caption.\\\\n\"\n         bus_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bus.png\"\n         self.bus_image_pil = Image.open(io.BytesIO(requests.get(bus_image_url).content))\n \n+    def get_processor(self):\n+        image_processor = FuyuImageProcessor()\n+        tokenizer = AutoTokenizer.from_pretrained(\"adept/fuyu-8b\")\n+        processor = FuyuProcessor(image_processor, tokenizer, **self.prepare_processor_dict())\n+\n+        return processor\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n     def test_fuyu_processing(self):\n         \"\"\"\n         Test to ensure that the standard processing on a gold example matches adept's code.\n@@ -43,7 +66,7 @@ def test_fuyu_processing(self):\n         EXPECTED_IMAGE_PATCH_INPUTS = torch.Tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, -1, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, -1, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, -1, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, -1, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, -1, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, -1, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, -1, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, -1, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, -1, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, -1, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, -1, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, -1, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, -1, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,]]).to(torch.int64)\n         EXPECTED_PADDED_UNPACKED_TOKEN_INPUTS = torch.Tensor([[71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71019, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71019, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71019, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71019, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71019, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71019, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71019, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71019, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71019, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71019, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71019, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71019, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71019, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71011, 71019, 1, 128340, 71374, 71389, 120412, 71377, 71835, 71374, 73615, 71375, 71399, 71435, 71122,]]).to(torch.int64)\n \n-        one_image_bus_model_inputs = self.processor(text=self.text_prompt, images=self.bus_image_pil)\n+        one_image_bus_model_inputs = self.get_processor()(text=self.text_prompt, images=self.bus_image_pil)\n \n         # fmt: on\n         torch.testing.assert_close(one_image_bus_model_inputs[\"image_patches_indices\"], EXPECTED_IMAGE_PATCH_INPUTS)\n@@ -53,8 +76,8 @@ def test_fuyu_processing_no_image(self):\n         \"\"\"\n         Test to check processor works with just text input\n         \"\"\"\n-        processor_outputs = self.processor(text=self.text_prompt)\n-        tokenizer_outputs = self.tokenizer(self.text_prompt)\n+        processor_outputs = self.get_processor()(text=self.text_prompt)\n+        tokenizer_outputs = self.get_tokenizer()(self.text_prompt)\n         self.assertEqual(processor_outputs[\"input_ids\"], tokenizer_outputs[\"input_ids\"])\n \n     def test_fuyu_processing_no_text(self):\n@@ -90,7 +113,7 @@ def test_fuyu_processing_no_text(self):\n         ]).to(torch.int64)\n         # fmt: on\n \n-        processor_outputs = self.processor(images=self.bus_image_pil)\n+        processor_outputs = self.get_processor()(images=self.bus_image_pil)\n         self.assertTrue((processor_outputs[\"image_patches_indices\"] == EXPECTED_IMAGE_PATCH_INPUTS).all())\n \n     def test_fuyu_processing_multiple_image_sample(self):\n@@ -107,7 +130,7 @@ def test_fuyu_processing_multiple_image_sample(self):\n \n         # Batch of two images - equally sized\n         images = [self.bus_image_pil, self.bus_image_pil]\n-        processor_outputs = self.processor(text=[self.text_prompt, self.text_prompt], images=images)\n+        processor_outputs = self.get_processor()(text=[self.text_prompt, self.text_prompt], images=images)\n \n         self.assertTrue(\n             (\n@@ -124,18 +147,18 @@ def test_fuyu_processing_multiple_image_sample(self):\n \n         # Processes single images with different sizes as expected\n         images = [self.bus_image_pil]\n-        processor_outputs = self.processor(text=self.text_prompt, images=images)\n+        processor_outputs = self.get_processor()(text=self.text_prompt, images=images)\n         self.assertTrue((processor_outputs[\"image_patches_indices\"] == SINGLE_IMAGE_PATCH_INPUTS).all())\n         self.assertTrue((processor_outputs[\"input_ids\"] == SINGLE_PADDED_UNPACKED_TOKEN_INPUTS).all())\n \n         images = [self.bus_image_pil.resize((64, 300))]\n-        processor_outputs = self.processor(text=self.text_prompt, images=images)\n+        processor_outputs = self.get_processor()(text=self.text_prompt, images=images)\n         self.assertTrue((processor_outputs[\"image_patches_indices\"] == SINGLE_RESIZED_IMAGE_PATCH_INPUTS).all())\n         self.assertTrue((processor_outputs[\"input_ids\"] == SINGLE_RESIZED_PADDED_UNPACKED_TOKEN_INPUTS).all())\n \n         # Batch of two images - different sizes. Left-pads the smaller image inputs\n         images = [self.bus_image_pil, self.bus_image_pil.resize((64, 300))]\n-        processor_outputs = self.processor(text=[self.text_prompt, self.text_prompt], images=images)\n+        processor_outputs = self.get_processor()(text=[self.text_prompt, self.text_prompt], images=images)\n \n         padding_len_patch = SINGLE_IMAGE_PATCH_INPUTS.shape[1] - SINGLE_RESIZED_IMAGE_PATCH_INPUTS.shape[1]\n         padded_single_resized_image_patch = torch.cat(\n@@ -156,6 +179,155 @@ def test_fuyu_processing_multiple_image_sample(self):\n         self.assertTrue((processor_outputs[\"image_patches_indices\"] == expected_image_patch_inputs).all())\n         self.assertTrue((processor_outputs[\"input_ids\"] == expected_padded_unpacked_token_inputs).all())\n \n+    # Rewrite as Fuyu supports tokenizer kwargs only when image is None.\n+    @require_vision\n+    @require_torch\n+    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        # Fuyu uses tokenizer kwargs only when image is None.\n+        image_input = None\n+\n+        inputs = processor(\n+            text=input_str, images=image_input, return_tensors=\"pt\", max_length=112, padding=\"max_length\"\n+        )\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 112)\n+\n+    @unittest.skip(\"Fuyu processor does not support image_processor kwargs\")\n+    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n+        pass\n+\n+    @unittest.skip(\"Fuyu processor does not support image_processor kwargs\")\n+    def test_kwargs_overrides_default_image_processor_kwargs(self):\n+        pass\n+\n+    # Rewrite as Fuyu supports tokenizer kwargs only when image is None.\n+    @require_vision\n+    @require_torch\n+    def test_tokenizer_defaults_preserved_by_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        # Fuyu uses tokenizer kwargs only when image is None.\n+        image_input = None\n+\n+        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 117)\n+\n+    # Rewrite as Fuyu image processor does not return pixel values\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        # Fuyu uses tokenizer kwargs only when image is None.\n+        image_input = None\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    # Rewrite as Fuyu image processor does not return pixel values\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested_from_dict(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        # Fuyu uses tokenizer kwargs only when image is None.\n+        image_input = None\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    # Rewrite as Fuyu supports tokenizer kwargs only when image is None.\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        # Fuyu uses tokenizer kwargs only when image is None.\n+        image_input = None\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            padding=\"max_length\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    # Rewrite as Fuyu supports tokenizer kwargs only when image is None.\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\", \"upper older longer string\"]\n+        # Fuyu uses tokenizer kwargs only when image is None.\n+        image_input = None\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 6)\n+\n \n @require_torch\n class TestImageTextProcessingUtils(unittest.TestCase):"
        },
        {
            "sha": "ffec4b01112c2fb304e8bb9442ea796f1d8fc5b2",
            "filename": "tests/models/instructblip/test_processor_instructblip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 259,
            "changes": 260,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Finstructblip%2Ftest_processor_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Finstructblip%2Ftest_processor_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_processor_instructblip.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -17,7 +17,7 @@\n \n import pytest\n \n-from transformers.testing_utils import require_torch, require_vision\n+from transformers.testing_utils import require_vision\n from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -179,261 +179,3 @@ def test_model_input_names(self):\n             list(inputs.keys()),\n             [\"input_ids\", \"attention_mask\", \"qformer_input_ids\", \"qformer_attention_mask\", \"pixel_values\"],\n         )\n-\n-    # Override as InstructBlipProcessor has qformer_tokenizer\n-    @require_vision\n-    @require_torch\n-    def test_tokenizer_defaults_preserved_by_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\", max_length=117, padding=\"max_length\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 117)\n-\n-    # Override as InstructBlipProcessor has qformer_tokenizer\n-    @require_torch\n-    @require_vision\n-    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\", size=(234, 234))\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\", max_length=117, padding=\"max_length\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 234)\n-\n-    # Override as InstructBlipProcessor has qformer_tokenizer\n-    @require_vision\n-    @require_torch\n-    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", padding=\"longest\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\", padding=\"longest\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(\n-            text=input_str, images=image_input, return_tensors=\"pt\", max_length=112, padding=\"max_length\"\n-        )\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 112)\n-\n-    # Override as InstructBlipProcessor has qformer_tokenizer\n-    @require_torch\n-    @require_vision\n-    def test_kwargs_overrides_default_image_processor_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\", size=(234, 234))\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\", max_length=117, padding=\"max_length\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, size=[224, 224])\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 224)\n-\n-    # Override as InstructBlipProcessor has qformer_tokenizer\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            size={\"height\": 214, \"width\": 214},\n-            padding=\"max_length\",\n-            max_length=76,\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    # Override as InstructBlipProcessor has qformer_tokenizer\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = [\"lower newer\", \"upper older longer string\"]\n-        image_input = self.prepare_image_inputs() * 2\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            size={\"height\": 214, \"width\": 214},\n-            padding=\"longest\",\n-            max_length=76,\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 6)\n-\n-    # Override as InstructBlipProcessor has qformer_tokenizer\n-    @require_torch\n-    @require_vision\n-    def test_doubly_passed_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = [\"lower newer\"]\n-        image_input = self.prepare_image_inputs()\n-        with self.assertRaises(ValueError):\n-            _ = processor(\n-                text=input_str,\n-                images=image_input,\n-                images_kwargs={\"size\": {\"height\": 222, \"width\": 222}},\n-                size={\"height\": 214, \"width\": 214},\n-            )\n-\n-    # Override as InstructBlipProcessor has qformer_tokenizer\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    # Override as InstructBlipProcessor has qformer_tokenizer\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested_from_dict(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    def test_overlapping_text_kwargs_handling(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        processor_kwargs = {}\n-        processor_kwargs[\"image_processor\"] = self.get_component(\"image_processor\")\n-        processor_kwargs[\"tokenizer\"] = tokenizer = self.get_component(\"tokenizer\")\n-        if not tokenizer.pad_token:\n-            tokenizer.pad_token = \"[TEST_PAD]\"\n-        if \"video_processor\" in self.processor_class.attributes:\n-            processor_kwargs[\"video_processor\"] = self.get_component(\"video_processor\")\n-\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n-\n-        processor = self.processor_class(**processor_kwargs, qformer_tokenizer=qformer_tokenizer)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        with self.assertRaises(ValueError):\n-            _ = processor(\n-                text=input_str,\n-                images=image_input,\n-                return_tensors=\"pt\",\n-                padding=\"max_length\",\n-                text_kwargs={\"padding\": \"do_not_pad\"},\n-            )"
        },
        {
            "sha": "d613d878223213bd01837c2493c091b04de83766",
            "filename": "tests/models/instructblipvideo/test_processor_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 272,
            "changes": 273,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Finstructblipvideo%2Ftest_processor_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Finstructblipvideo%2Ftest_processor_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_processor_instructblipvideo.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -15,18 +15,15 @@\n import tempfile\n import unittest\n \n-import numpy as np\n import pytest\n \n-from transformers.testing_utils import require_torch, require_vision\n+from transformers.testing_utils import require_vision\n from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n \n if is_vision_available():\n-    from PIL import Image\n-\n     from transformers import (\n         AutoProcessor,\n         BertTokenizerFast,\n@@ -65,16 +62,6 @@ def get_qformer_tokenizer(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    # Ignore copy\n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of list of PIL images\"\"\"\n-\n-        video_inputs = [\n-            [Image.fromarray(np.random.randint(255, size=(30, 400, 3), dtype=np.uint8)) for _ in range(5)]\n-            for _ in range(2)\n-        ]\n-        return video_inputs\n-\n     def test_save_load_pretrained_additional_features(self):\n         processor = InstructBlipVideoProcessor(\n             tokenizer=self.get_tokenizer(),\n@@ -193,261 +180,3 @@ def test_model_input_names(self):\n             list(inputs.keys()),\n             [\"input_ids\", \"attention_mask\", \"qformer_input_ids\", \"qformer_attention_mask\", \"pixel_values\"],\n         )\n-\n-    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n-    @require_vision\n-    @require_torch\n-    def test_tokenizer_defaults_preserved_by_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\", max_length=117, padding=\"max_length\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 117)\n-\n-    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n-    @require_torch\n-    @require_vision\n-    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\", size=(234, 234))\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\", max_length=117, padding=\"max_length\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 234)\n-\n-    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n-    @require_vision\n-    @require_torch\n-    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", padding=\"longest\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\", padding=\"longest\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(\n-            text=input_str, images=image_input, return_tensors=\"pt\", max_length=112, padding=\"max_length\"\n-        )\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 112)\n-\n-    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n-    @require_torch\n-    @require_vision\n-    def test_kwargs_overrides_default_image_processor_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\", size=(234, 234))\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\", max_length=117, padding=\"max_length\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, size=[224, 224])\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 224)\n-\n-    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            size={\"height\": 214, \"width\": 214},\n-            padding=\"max_length\",\n-            max_length=76,\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = [\"lower newer\", \"upper older longer string\"]\n-        image_input = self.prepare_image_inputs() * 2\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            size={\"height\": 214, \"width\": 214},\n-            padding=\"longest\",\n-            max_length=76,\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 6)\n-\n-    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n-    @require_torch\n-    @require_vision\n-    def test_doubly_passed_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = [\"lower newer\"]\n-        image_input = self.prepare_image_inputs()\n-        with self.assertRaises(ValueError):\n-            _ = processor(\n-                text=input_str,\n-                images=image_input,\n-                images_kwargs={\"size\": {\"height\": 222, \"width\": 222}},\n-                size={\"height\": 214, \"width\": 214},\n-            )\n-\n-    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    # Override as InstructBlipVideoProcessor has qformer_tokenizer\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested_from_dict(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    def test_overlapping_text_kwargs_handling(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        processor_kwargs = {}\n-        processor_kwargs[\"image_processor\"] = self.get_component(\"image_processor\")\n-        processor_kwargs[\"tokenizer\"] = tokenizer = self.get_component(\"tokenizer\")\n-        if not tokenizer.pad_token:\n-            tokenizer.pad_token = \"[TEST_PAD]\"\n-        if \"video_processor\" in self.processor_class.attributes:\n-            processor_kwargs[\"video_processor\"] = self.get_component(\"video_processor\")\n-\n-        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n-\n-        processor = self.processor_class(**processor_kwargs, qformer_tokenizer=qformer_tokenizer)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        with self.assertRaises(ValueError):\n-            _ = processor(\n-                text=input_str,\n-                images=image_input,\n-                return_tensors=\"pt\",\n-                padding=\"max_length\",\n-                text_kwargs={\"padding\": \"do_not_pad\"},\n-            )"
        },
        {
            "sha": "8de398ade70c7149c34efa76a8760c5a0730f2a6",
            "filename": "tests/models/kosmos2/test_processor_kosmos2.py",
            "status": "modified",
            "additions": 145,
            "deletions": 1,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Fkosmos2%2Ftest_processor_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Fkosmos2%2Ftest_processor_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_processor_kosmos2.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -61,7 +61,7 @@ class Kosmos2ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n \n-        image_processor = CLIPImageProcessor()\n+        image_processor = CLIPImageProcessor(do_center_crop=False)\n \n         # We have a SentencePiece fixture for testing\n         slow_tokenizer = XLMRobertaTokenizer(SAMPLE_VOCAB)\n@@ -487,3 +487,147 @@ def check(texts, bboxes, expected_input_ids):\n         self.assertListEqual(outputs.input_ids.numpy().tolist()[-1], EXPECTED_IDS_BATCH[-1])\n         self.assertListEqual(outputs.attention_mask.numpy().tolist()[-1], EXPECTED_MASK_BATCH[-1])\n         self.assertListEqual(outputs.image_embeds_position_mask.numpy().tolist()[-1], EXPECTED_IMG_POS_MASK_BATCH[-1])\n+\n+    # Rewrite as Kosmos-2 supports custom padding only when image is None.\n+    @require_vision\n+    @require_torch\n+    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        # set image input to None\n+        image_input = None\n+\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            max_length=112,\n+            padding=\"max_length\",\n+        )\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 112)\n+\n+    # Rewrite to test only image_processor kwargs\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+    # Rewrite to test only image_processor kwargs\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested_from_dict(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+    # Rewrite as Kosmos-2 supports custom padding only when image is None.\n+    @require_vision\n+    @require_torch\n+    def test_tokenizer_defaults_preserved_by_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        # set image input to None\n+        image_input = None\n+\n+        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 117)\n+\n+    # Rewrite as Kosmos-2 supports custom padding only when image is None.\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        # set image input to None\n+        image_input = None\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            padding=\"max_length\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    # Rewrite as Kosmos-2 supports custom padding only when image is None.\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\", \"upper older longer string\"]\n+        # set image input to None\n+        image_input = None\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            size={\"height\": 214, \"width\": 214},\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 10)"
        },
        {
            "sha": "a54aeab8a28252e420b90fccb01eff4de4f2488b",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -338,7 +338,7 @@ def test_small_model_integration_test(self):\n             load_in_4bit=True,\n         )\n \n-        inputs = self.processor(self.prompt, self.image, return_tensors=\"pt\")\n+        inputs = self.processor(images=self.image, text=self.prompt, return_tensors=\"pt\")\n \n         # verify inputs against original implementation\n         filepath = hf_hub_download(\n@@ -390,8 +390,8 @@ def test_small_model_integration_test_batch(self):\n         cats_image = Image.open(requests.get(url, stream=True).raw)\n \n         inputs = self.processor(\n-            [self.prompt, self.prompt],\n             images=[self.image, cats_image],\n+            text=[self.prompt, self.prompt],\n             return_tensors=\"pt\",\n             padding=True,\n         ).to(torch_device)\n@@ -415,7 +415,7 @@ def test_small_model_integration_test_unk_token(self):\n         )\n \n         prompt_with_unk = \"[INST] <image>\\nWhat is shown in this <unk> image? [/INST]\"\n-        inputs = self.processor(prompt_with_unk, self.image, return_tensors=\"pt\")\n+        inputs = self.processor(images=self.image, text=prompt_with_unk, return_tensors=\"pt\")\n \n         # verify single forward pass\n         inputs = inputs.to(torch_device)\n@@ -445,7 +445,7 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n         lowres_img = Image.open(requests.get(lowres_url, stream=True).raw)\n \n         inputs = self.processor(\n-            [self.prompt, self.prompt], images=[lowres_img, cats_image], return_tensors=\"pt\", padding=True\n+            images=[lowres_img, cats_image], text=[self.prompt, self.prompt], return_tensors=\"pt\", padding=True\n         ).to(torch_device)\n         pixel_values = inputs[\"pixel_values\"]\n \n@@ -498,10 +498,10 @@ def test_small_model_integration_test_batch_matches_single(self):\n         lowres_img = Image.open(requests.get(lowres_url, stream=True).raw)\n \n         inputs_batched = self.processor(\n-            [self.prompt, self.prompt], images=[lowres_img, cats_image], return_tensors=\"pt\", padding=True\n+            images=[lowres_img, cats_image], text=[self.prompt, self.prompt], return_tensors=\"pt\", padding=True\n         ).to(torch_device)\n \n-        inputs_single = self.processor(self.prompt, images=lowres_img, return_tensors=\"pt\", padding=True).to(\n+        inputs_single = self.processor(images=lowres_img, text=self.prompt, return_tensors=\"pt\", padding=True).to(\n             torch_device\n         )\n \n@@ -527,7 +527,7 @@ def test_padding_side_when_merging_inputs(self):\n         lowres_img = Image.open(requests.get(lowres_url, stream=True).raw)\n \n         inputs_batched = self.processor(\n-            [self.prompt, self.prompt], images=[lowres_img, cats_image], return_tensors=\"pt\", padding=True\n+            images=[lowres_img, cats_image], text=[self.prompt, self.prompt], return_tensors=\"pt\", padding=True\n         ).to(torch_device)\n \n         # model is in eval mode by default so we should get pad on the left side\n@@ -607,13 +607,13 @@ def test_expansion_in_processing(self):\n         # check processing with expansion of inputs\n         processor.vision_feature_select_strategy = \"default\"\n         processor.patch_size = 14\n-        inputs_expanded = processor(prompt, raw_image, return_tensors=\"pt\").to(torch_device, torch.float16)\n+        inputs_expanded = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n         self.assertTrue(inputs_expanded.input_ids.shape[-1] == 2356)\n \n         # check processing without expansion of inputs (legacy behavior)\n         processor.vision_feature_select_strategy = None\n         processor.patch_size = None\n-        inputs = processor(prompt, raw_image, return_tensors=\"pt\").to(torch_device, torch.float16)\n+        inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n         self.assertTrue(inputs.input_ids.shape[-1] == 17)\n \n         # generate exactly 20 tokens"
        },
        {
            "sha": "45faa24526305cf7579dcd70f551cfcaf7b92ea8",
            "filename": "tests/models/llava_next/test_processor_llava_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -18,7 +18,9 @@\n import torch\n \n from transformers import AutoProcessor, LlamaTokenizerFast, LlavaNextProcessor\n-from transformers.testing_utils import require_vision\n+from transformers.testing_utils import (\n+    require_vision,\n+)\n from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin"
        },
        {
            "sha": "ac8d4822f1c09f856f11ef48cd2b9d52547f8943",
            "filename": "tests/models/pix2struct/test_processor_pix2struct.py",
            "status": "modified",
            "additions": 147,
            "deletions": 0,
            "changes": 147,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Fpix2struct%2Ftest_processor_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Fpix2struct%2Ftest_processor_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_processor_pix2struct.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -37,6 +37,8 @@\n @require_torch\n class Pix2StructProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     processor_class = Pix2StructProcessor\n+    text_input_name = \"decoder_input_ids\"\n+    images_input_name = \"flattened_patches\"\n \n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n@@ -180,3 +182,148 @@ def test_model_input_names(self):\n \n         # For now the processor supports only [\"flattened_patches\", \"input_ids\", \"attention_mask\", \"decoder_attention_mask\"]\n         self.assertListEqual(list(inputs.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+    @require_torch\n+    @require_vision\n+    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n+        # Rewrite as pix2struct processor return \"flattened_patches\" and not \"pixel_values\"\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\", max_patches=1024, patch_size={\"height\": 8, \"width\": 8})\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input)\n+        self.assertEqual(len(inputs[\"flattened_patches\"][0][0]), 194)\n+\n+    @require_torch\n+    @require_vision\n+    def test_kwargs_overrides_default_image_processor_kwargs(self):\n+        # Rewrite as pix2struct processor return \"flattened_patches\" and not \"pixel_values\"\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\", max_patches=4096)\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input, max_patches=1024)\n+        self.assertEqual(len(inputs[\"flattened_patches\"][0]), 1024)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs(self):\n+        # Rewrite as pix2struct processor return \"decoder_input_ids\" and not \"input_ids\"\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            max_patches=1024,\n+            padding=\"max_length\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"flattened_patches\"].shape[1], 1024)\n+        self.assertEqual(len(inputs[\"decoder_input_ids\"][0]), 76)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched(self):\n+        # Rewrite as pix2struct processor return \"decoder_input_ids\" and not \"input_ids\"\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\", \"upper older longer string\"]\n+        image_input = self.prepare_image_inputs() * 2\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            max_patches=1024,\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"flattened_patches\"].shape[1], 1024)\n+\n+        self.assertEqual(len(inputs[\"decoder_input_ids\"][0]), 5)\n+\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested(self):\n+        # Rewrite as pix2struct processor return \"decoder_input_ids\" and not \"input_ids\"\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"max_patches\": 1024},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        self.assertEqual(inputs[\"flattened_patches\"].shape[1], 1024)\n+\n+        self.assertEqual(len(inputs[\"decoder_input_ids\"][0]), 76)\n+\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested_from_dict(self):\n+        # Rewrite as pix2struct processor return \"decoder_input_ids\" and not \"input_ids\"\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"max_patches\": 1024},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.assertEqual(inputs[\"flattened_patches\"].shape[1], 1024)\n+\n+        self.assertEqual(len(inputs[\"decoder_input_ids\"][0]), 76)"
        },
        {
            "sha": "59c19eabcaf53b61293e1744d24a7b392d36e8d4",
            "filename": "tests/models/pixtral/test_processor_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -18,9 +18,7 @@\n import requests\n import torch\n \n-from transformers.testing_utils import (\n-    require_vision,\n-)\n+from transformers.testing_utils import require_vision\n from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin"
        },
        {
            "sha": "8cc71147c22013bd17aec9a26007e5f495b21886",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0c181f4e8faf88d5c186961eeca0779b6354da/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=5f0c181f4e8faf88d5c186961eeca0779b6354da",
            "patch": "@@ -66,7 +66,7 @@ def get_component(self, attribute, **kwargs):\n \n         component_class = processor_class_from_name(component_class_name)\n         component = component_class.from_pretrained(self.tmpdirname, **kwargs)  # noqa\n-        if attribute == \"tokenizer\" and not component.pad_token:\n+        if \"tokenizer\" in attribute and not component.pad_token:\n             component.pad_token = \"[TEST_PAD]\"\n             if component.pad_token_id is None:\n                 component.pad_token_id = 0\n@@ -322,14 +322,8 @@ def test_structured_kwargs_nested_from_dict(self):\n     def test_overlapping_text_kwargs_handling(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        processor_kwargs = {}\n-        processor_kwargs[\"image_processor\"] = self.get_component(\"image_processor\")\n-        processor_kwargs[\"tokenizer\"] = tokenizer = self.get_component(\"tokenizer\")\n-        if not tokenizer.pad_token:\n-            tokenizer.pad_token = \"[TEST_PAD]\"\n-        if \"video_processor\" in self.processor_class.attributes:\n-            processor_kwargs[\"video_processor\"] = self.get_component(\"video_processor\")\n-        processor = self.processor_class(**processor_kwargs)\n+        processor_components = self.prepare_components()\n+        processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = \"lower newer\""
        }
    ],
    "stats": {
        "total": 1615,
        "additions": 763,
        "deletions": 852
    }
}