{
    "author": "ivarflakstad",
    "message": "Mamba2 remove unecessary test parameterization (#38227)",
    "sha": "3f0b7d0fac38bf326db672a9f246eaa247ff266d",
    "files": [
        {
            "sha": "5777053923a336980e73ead49c6bed2c8eb8317d",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f0b7d0fac38bf326db672a9f246eaa247ff266d/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f0b7d0fac38bf326db672a9f246eaa247ff266d/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=3f0b7d0fac38bf326db672a9f246eaa247ff266d",
            "patch": "@@ -15,8 +15,6 @@\n \n import unittest\n \n-from parameterized import parameterized\n-\n from transformers import AutoTokenizer, Mamba2Config, is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n@@ -362,14 +360,9 @@ def setUp(self):\n         self.prompt = (\"[INST]Write a hello world program in C++.\",)\n \n     @require_read_token\n-    @parameterized.expand(\n-        [\n-            (torch_device,),\n-        ]\n-    )\n     @slow\n     @require_torch\n-    def test_simple_generate(self, device):\n+    def test_simple_generate(self):\n         \"\"\"\n         Simple generate test to avoid regressions.\n         Note: state-spaces (cuda) implementation and pure torch implementation\n@@ -380,9 +373,9 @@ def test_simple_generate(self, device):\n         tokenizer.pad_token_id = tokenizer.eos_token_id\n \n         model = Mamba2ForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.bfloat16)\n-        model.to(device)\n+        model.to(torch_device)\n         input_ids = tokenizer(\"[INST]Write a hello world program in C++.[/INST]\", return_tensors=\"pt\")[\"input_ids\"].to(\n-            device\n+            torch_device\n         )\n \n         out = model.generate(input_ids, do_sample=False, use_cache=True, max_new_tokens=30)"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 3,
        "deletions": 10
    }
}