{
    "author": "Cyrilvallez",
    "message": "Enforce call to `post_init` and fix all of them (#42873)\n\n* fix all\n\n* style\n\n* simplify\n\n* improve msg\n\n* fix\n\n* oupsi\n\n* add colors and sort",
    "sha": "c7aec088a63183292518359880271e2e93a288cc",
    "files": [
        {
            "sha": "59a217ba3a81bd579fc18fe3e61349c87886cbe3",
            "filename": "Makefile",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/Makefile",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/Makefile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/Makefile?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -45,7 +45,7 @@ repo-consistency:\n \tpython utils/check_modular_conversion.py\n \tpython utils/check_dummies.py\n \tpython utils/check_repo.py\n-\tpython utils/check_init_weights_data.py\n+\tpython utils/check_modeling_structure.py\n \tpython utils/check_inits.py\n \tpython utils/check_pipeline_typing.py\n \tpython utils/check_config_docstrings.py"
        },
        {
            "sha": "d33ce67ca8d21cff2fcf20da92ae2db14f3a3f94",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -1463,6 +1463,7 @@ class BartDecoderWrapper(BartPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.decoder = BartDecoder(config)\n+        self.post_init()\n \n     def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)"
        },
        {
            "sha": "91dbb7d3db7de7a022d8679a1862b32f8d4f5509",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -2582,6 +2582,7 @@ class BigBirdPegasusDecoderWrapper(BigBirdPegasusPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.decoder = BigBirdPegasusDecoder(config)\n+        self.post_init()\n \n     def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)"
        },
        {
            "sha": "88d577e57ff63f8cbd267d6b9ea7c684524ca86d",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -1156,6 +1156,7 @@ class BlenderbotDecoderWrapper(BlenderbotPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.decoder = BlenderbotDecoder(config)\n+        self.post_init()\n \n     def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)"
        },
        {
            "sha": "74bde662265df423b735289eb194b4c0cbcb754e",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -1116,6 +1116,7 @@ class BlenderbotSmallDecoderWrapper(BlenderbotSmallPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.decoder = BlenderbotSmallDecoder(config)\n+        self.post_init()\n \n     def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)"
        },
        {
            "sha": "0d1a65acc97bde4a86a90d92bf48fa63b81a6b82",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -740,6 +740,8 @@ def __init__(self, config):\n         self.cls = BlipTextOnlyMLMHead(config)\n         self.label_smoothing = config.label_smoothing\n \n+        self.post_init()\n+\n     def get_input_embeddings(self):\n         return self.bert.get_input_embeddings()\n "
        },
        {
            "sha": "441d5356c1954c212839a39eb74d99143192fa89",
            "filename": "src/transformers/models/blt/modeling_blt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -753,6 +753,8 @@ def __init__(self, config: BltPatcherConfig):\n             bias=False,\n         )\n \n+        self.post_init()\n+\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "352017b6b53642e8888cc8212e811f405f16ad9a",
            "filename": "src/transformers/models/blt/modular_blt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -634,6 +634,8 @@ def __init__(self, config: BltPatcherConfig):\n             bias=False,\n         )\n \n+        self.post_init()\n+\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "bb93533676af394a6de0ff6f5882d946ab1651d8",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -955,6 +955,7 @@ class BridgeTowerVisionModel(BridgeTowerPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.visual = BridgeTowerVisionTransformer(config)\n+        self.post_init()\n \n     @property\n     def dtype(self):"
        },
        {
            "sha": "25307fc90fc6c7093ced98a34c727c0c38e4cb6a",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -809,6 +809,7 @@ def __init__(self, config: ChameleonVQVAEConfig):\n         self.quant_conv = torch.nn.Conv2d(config.latent_channels, config.embed_dim, 1)\n         self.post_quant_conv = torch.nn.Conv2d(config.embed_dim, config.latent_channels, 1)\n         self.eval()  # Chameleon's VQ model is frozen\n+        self.post_init()\n \n     def encode(self, pixel_values: torch.LongTensor):\n         hidden_states = self.encoder(pixel_values)"
        },
        {
            "sha": "88a881db3db2f073a26ceb041486ba2c3925de44",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -1121,6 +1121,8 @@ def __init__(self, config: CLIPSegConfig):\n         decoder_config.hidden_act = \"relu\"\n         self.layers = nn.ModuleList([CLIPSegDecoderLayer(decoder_config) for _ in range(len(config.extract_layers))])\n \n+        self.post_init()\n+\n     def forward(\n         self,\n         hidden_states: tuple[torch.Tensor],"
        },
        {
            "sha": "d00c281ab1dbf2f614528055dd528f11192fd171",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -367,12 +367,8 @@ class DecisionTransformerGPT2PreTrainedModel(PreTrainedModel):\n     config: DecisionTransformerConfig\n     base_model_prefix = \"transformer\"\n     supports_gradient_checkpointing = True\n-\n     _can_compile_fullgraph = False\n \n-    def __init__(self, *inputs, **kwargs):\n-        super().__init__(*inputs, **kwargs)\n-\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\""
        },
        {
            "sha": "2ce385242c745dca3b077d17ddd25d70ba1d895e",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -452,6 +452,8 @@ def __init__(self, config: DiaEncoderConfig):\n         self.norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n         self.rotary_emb = DiaRotaryEmbedding(config=config)\n \n+        self.post_init()\n+\n     @auto_docstring\n     @can_return_tuple\n     def forward(\n@@ -578,6 +580,8 @@ def __init__(self, config: DiaDecoderConfig):\n         self.norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n         self.rotary_emb = DiaRotaryEmbedding(config=config)\n \n+        self.post_init()\n+\n     @auto_docstring\n     @can_return_tuple\n     def forward("
        },
        {
            "sha": "ae4a4e3f6e33ffafa00af2030d060dc4ebf9d9fd",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -241,6 +241,8 @@ def __init__(self, config: DiaEncoderConfig):\n         self.norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n         self.rotary_emb = DiaRotaryEmbedding(config=config)\n \n+        self.post_init()\n+\n     @auto_docstring\n     @can_return_tuple\n     def forward(\n@@ -367,6 +369,8 @@ def __init__(self, config: DiaDecoderConfig):\n         self.norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n         self.rotary_emb = DiaRotaryEmbedding(config=config)\n \n+        self.post_init()\n+\n     @auto_docstring\n     @can_return_tuple\n     def forward("
        },
        {
            "sha": "cb06976c5145dfc220b6dc629f0fbe54415b12b8",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -524,6 +524,7 @@ def __init__(self, config: SaProtConfig):\n         super().__init__(config)\n         self.embeddings = EvollaSaProtEmbeddings(config)\n         self.encoder = EvollaSaProtEncoder(config)\n+        self.post_init()\n \n     def get_input_embeddings(self):\n         return self.embeddings.word_embeddings"
        },
        {
            "sha": "ef27e7b1570b7a70d1380facf6389f37ce30ff41",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -209,6 +209,7 @@ def __init__(self, config: SaProtConfig):\n         super().__init__(config)\n         self.embeddings = EvollaSaProtEmbeddings(config)\n         self.encoder = EvollaSaProtEncoder(config)\n+        self.post_init()\n \n     def get_input_embeddings(self):\n         return self.embeddings.word_embeddings"
        },
        {
            "sha": "5b6c7e961c91bedda002f3fc0b7d4ab51b7e9a47",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -660,9 +660,6 @@ class FlaubertPreTrainedModel(PreTrainedModel):\n     config: FlaubertConfig\n     base_model_prefix = \"transformer\"\n \n-    def __init__(self, *inputs, **kwargs):\n-        super().__init__(*inputs, **kwargs)\n-\n     @property\n     def dummy_inputs(self):\n         inputs_list = torch.tensor([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])"
        },
        {
            "sha": "2a15321b9808bbfb7b90136e1593c4564631aa3f",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -919,6 +919,7 @@ def __init__(self, config: Gemma3nAudioConfig):\n         self.conformer = nn.ModuleList(\n             [Gemma3nAudioConformerBlock(config) for _ in range(config.conf_num_hidden_layers)]\n         )\n+        self.post_init()\n \n     def forward(\n         self, audio_mel: torch.Tensor, audio_mel_mask: torch.BoolTensor, **kwargs"
        },
        {
            "sha": "ece8df69a931fb765b2fb7f2d771f8b20201c6c4",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -1472,6 +1472,7 @@ def __init__(self, config: Gemma3nAudioConfig):\n         self.conformer = nn.ModuleList(\n             [Gemma3nAudioConformerBlock(config) for _ in range(config.conf_num_hidden_layers)]\n         )\n+        self.post_init()\n \n     def forward(\n         self, audio_mel: torch.Tensor, audio_mel_mask: torch.BoolTensor, **kwargs"
        },
        {
            "sha": "bbb70becc9b6bf6970403bac995672c09513e235",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -433,6 +433,7 @@ def __init__(self, config: GotOcr2VisionConfig):\n         self.neck = GotOcr2VisionNeck(config)\n \n         self.gradient_checkpointing = False\n+        self.post_init()\n \n     def get_input_embeddings(self):\n         return self.patch_embed"
        },
        {
            "sha": "c2f61664d51d8a0437f830a483a43bee0f60ff5f",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -476,12 +476,8 @@ class GPT2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_attention_backend = True\n-\n     _can_compile_fullgraph = True\n \n-    def __init__(self, *inputs, **kwargs):\n-        super().__init__(*inputs, **kwargs)\n-\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\""
        },
        {
            "sha": "39f0a3e486d1d71e298f99036bd6df73af46e9df",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -26,7 +26,6 @@\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n-from ...modeling_flash_attention_utils import is_flash_attn_available\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -43,10 +42,6 @@\n from .configuration_gpt_bigcode import GPTBigCodeConfig\n \n \n-if is_flash_attn_available():\n-    pass\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -360,9 +355,6 @@ class GPTBigCodePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    def __init__(self, *inputs, **kwargs):\n-        super().__init__(*inputs, **kwargs)\n-\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\""
        },
        {
            "sha": "349324ff6743ba5d93422199953c4978306b2395",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -452,6 +452,8 @@ def __init__(self, config: Idefics2VisionConfig):\n         self.encoder = Idefics2Encoder(config)\n         self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n+        self.post_init()\n+\n     def get_input_embeddings(self):\n         return self.embeddings\n \n@@ -711,6 +713,8 @@ def __init__(self, config) -> None:\n         self.layers = nn.ModuleList([Idefics2PerceiverLayer(config, idx) for idx in range(self.depth)])\n         self.norm = Idefics2RMSNorm(self.hidden_size, eps=self.rms_norm_eps)\n \n+        self.post_init()\n+\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "2a394b4e6fd7a6c3c76d7caae69d30c1845acaa7",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -458,6 +458,8 @@ def __init__(self, config: Idefics3VisionConfig):\n         self.patch_size = config.patch_size\n         self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n+        self.post_init()\n+\n     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2VisionTransformer.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.embeddings"
        },
        {
            "sha": "65d7ce4e63c48853c4e44667367da5a63da7767f",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -973,8 +973,6 @@ def __init__(self, config: JanusVQVAEConfig):\n         self.eval()  # Janus's VQ model is frozen\n         self.decoder = JanusVQVAEDecoder(config)\n         self.gradient_checkpointing = False\n-\n-        # Initialize the VQVAE model.\n         self.post_init()\n \n     def encode(self, pixel_values: torch.LongTensor):"
        },
        {
            "sha": "24bc14ad23dbb64ab7cae2706973036a7e4a687a",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -1248,6 +1248,7 @@ class MarianDecoderWrapper(MarianPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.decoder = MarianDecoder(config)\n+        self.post_init()\n \n     def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)"
        },
        {
            "sha": "9eed3ff768a2edbf0dfa31456a6e118d6e68f4d3",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -1442,6 +1442,7 @@ class MBartDecoderWrapper(MBartPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.decoder = MBartDecoder(config)\n+        self.post_init()\n \n     def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)"
        },
        {
            "sha": "4f1e02e97edd045727dd8160fbd179fcbe928369",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -869,6 +869,8 @@ def __init__(self, config: MoshiDepthConfig):\n         self.gradient_checkpointing = False\n         self.config = config\n \n+        self.post_init()\n+\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "966c7bc8cf8f5821d3d081b980406b8a80f2ba45",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -1509,6 +1509,7 @@ class MvpDecoderWrapper(MvpPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.decoder = MvpDecoder(config)\n+        self.post_init()\n \n     def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)"
        },
        {
            "sha": "8e77e36f863359e29ffbf2df409a00eb6cfdf55e",
            "filename": "src/transformers/models/ovis2/modeling_ovis2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -457,6 +457,8 @@ def __init__(self, config: Ovis2VisionConfig):\n         )\n         self.head_norm = nn.LayerNorm(self.vocab_size - self.num_visual_indicator_tokens)\n \n+        self.post_init()\n+\n     def forward(self, pixel_values: torch.FloatTensor, **kwargs) -> tuple[torch.Tensor, torch.Tensor]:\n         outputs = self.transformer(pixel_values, **kwargs)\n         last_hidden_state = outputs[0]"
        },
        {
            "sha": "d051ee3b1dd074bbd9a08fb36611ee2954514cd5",
            "filename": "src/transformers/models/ovis2/modular_ovis2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -176,6 +176,8 @@ def __init__(self, config: Ovis2VisionConfig):\n         )\n         self.head_norm = nn.LayerNorm(self.vocab_size - self.num_visual_indicator_tokens)\n \n+        self.post_init()\n+\n     def forward(self, pixel_values: torch.FloatTensor, **kwargs) -> tuple[torch.Tensor, torch.Tensor]:\n         outputs = self.transformer(pixel_values, **kwargs)\n         last_hidden_state = outputs[0]"
        },
        {
            "sha": "dbc729375ce205dd730eab19f9e610c6b4fca13a",
            "filename": "src/transformers/models/paddleocr_vl/modeling_paddleocr_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fmodeling_paddleocr_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fmodeling_paddleocr_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fmodeling_paddleocr_vl.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -919,6 +919,8 @@ def __init__(self, config: PaddleOCRVisionConfig):\n         self.encoder = PaddleOCRVisionEncoder(config)\n         self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n+        self.post_init()\n+\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,"
        },
        {
            "sha": "63e00172346db2fe3fa5813edd1860581431d02a",
            "filename": "src/transformers/models/paddleocr_vl/modular_paddleocr_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fmodular_paddleocr_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fmodular_paddleocr_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fmodular_paddleocr_vl.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -1037,6 +1037,8 @@ def __init__(self, config: PaddleOCRVisionConfig):\n         self.encoder = PaddleOCRVisionEncoder(config)\n         self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n+        self.post_init()\n+\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,"
        },
        {
            "sha": "afaa69a806e5a3cbb1d1266d8420c66d32a50c32",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -1220,6 +1220,7 @@ class PegasusDecoderWrapper(PegasusPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.decoder = PegasusDecoder(config)\n+        self.post_init()\n \n     def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)"
        },
        {
            "sha": "ada8433ffed954b1e94846fa28cbe19232810316",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -1476,6 +1476,7 @@ class PegasusXDecoderWrapper(PegasusXPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.decoder = PegasusXDecoder(config)\n+        self.post_init()\n \n     def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)"
        },
        {
            "sha": "e252146706604534c0400e1d0615a5ddae818933",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -1273,6 +1273,7 @@ class PLBartDecoderWrapper(PLBartPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.decoder = PLBartDecoder(config)\n+        self.post_init()\n \n     def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)"
        },
        {
            "sha": "35e5d5f8c173d3cce8ec452854f9aadbeac89c66",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -1105,6 +1105,8 @@ def __init__(self, config: Qwen2_5OmniVisionEncoderConfig, *inputs, **kwargs) ->\n         )\n         self.gradient_checkpointing = False\n \n+        self.post_init()\n+\n     def rot_pos_emb(self, grid_thw):\n         pos_ids = []\n         for t, h, w in grid_thw:\n@@ -3441,6 +3443,8 @@ def __init__(self, config: Qwen2_5OmniBigVGANConfig):\n             config.upsample_initial_channel // (2**self.num_upsample_layers), 1, 7, 1, padding=3, bias=False\n         )\n \n+        self.post_init()\n+\n     def normalize_spectrogram(self, spectrogram, max_value, min_db):\n         return torch.clamp((2 * max_value) * ((spectrogram - min_db) / (-min_db)) - max_value, -max_value, max_value)\n \n@@ -3568,6 +3572,8 @@ def __init__(self, config: Qwen2_5OmniDiTConfig):\n         self.norm_out = Qwen2_5_OmniAdaLayerNormZero_Final(config.hidden_size)  # final modulation\n         self.proj_out = nn.Linear(config.hidden_size, config.mel_dim)\n \n+        self.post_init()\n+\n     def _create_block_diff(self, hidden_states):\n         batch, seq_len = hidden_states.shape[0], hidden_states.shape[1]\n         block_indices = torch.arange(seq_len, device=hidden_states.device) // self.block_size  # [seq_length]\n@@ -3720,6 +3726,8 @@ def __init__(self, config: Qwen2_5OmniToken2WavConfig):\n             config.bigvgan_config, attn_implementation=attn_impl\n         )\n \n+        self.post_init()\n+\n     def forward(\n         self,\n         code,"
        },
        {
            "sha": "5af62ceddba811c634fdc78319cbf24d63dc9ef0",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -3600,6 +3600,8 @@ def __init__(self, config: Qwen2_5OmniBigVGANConfig):\n             config.upsample_initial_channel // (2**self.num_upsample_layers), 1, 7, 1, padding=3, bias=False\n         )\n \n+        self.post_init()\n+\n     def normalize_spectrogram(self, spectrogram, max_value, min_db):\n         return torch.clamp((2 * max_value) * ((spectrogram - min_db) / (-min_db)) - max_value, -max_value, max_value)\n \n@@ -3727,6 +3729,8 @@ def __init__(self, config: Qwen2_5OmniDiTConfig):\n         self.norm_out = Qwen2_5_OmniAdaLayerNormZero_Final(config.hidden_size)  # final modulation\n         self.proj_out = nn.Linear(config.hidden_size, config.mel_dim)\n \n+        self.post_init()\n+\n     def _create_block_diff(self, hidden_states):\n         batch, seq_len = hidden_states.shape[0], hidden_states.shape[1]\n         block_indices = torch.arange(seq_len, device=hidden_states.device) // self.block_size  # [seq_length]\n@@ -3879,6 +3883,8 @@ def __init__(self, config: Qwen2_5OmniToken2WavConfig):\n             config.bigvgan_config, attn_implementation=attn_impl\n         )\n \n+        self.post_init()\n+\n     def forward(\n         self,\n         code,"
        },
        {
            "sha": "8031afd5aa53abb2e300feef729b27ea912aea9a",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -336,6 +336,8 @@ def __init__(self, config, *inputs, **kwargs) -> None:\n         )\n         self.gradient_checkpointing = False\n \n+        self.post_init()\n+\n     def rot_pos_emb(self, grid_thw):\n         pos_ids = []\n         for t, h, w in grid_thw:"
        },
        {
            "sha": "8db13ed33de0075b589b9328bf54a546bf8645ab",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -207,6 +207,8 @@ def __init__(self, config, *inputs, **kwargs) -> None:\n         )\n         self.gradient_checkpointing = False\n \n+        self.post_init()\n+\n     def rot_pos_emb(self, grid_thw):\n         pos_ids = []\n         for t, h, w in grid_thw:"
        },
        {
            "sha": "82e8bb38d5e0244efb3b78c73fe46e5c292aac46",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -693,6 +693,8 @@ def __init__(self, config) -> None:\n         )\n         self.gradient_checkpointing = False\n \n+        self.post_init()\n+\n     def get_dtype(self) -> torch.dtype:\n         return self.blocks[0].mlp.fc2.weight.dtype\n "
        },
        {
            "sha": "11cd873592b810b2e7de0fcdf1911311024aaa85",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -1073,6 +1073,8 @@ def __init__(self, config, *inputs, **kwargs) -> None:\n \n         self.gradient_checkpointing = False\n \n+        self.post_init()\n+\n     def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n         merge_size = self.spatial_merge_size\n \n@@ -3716,6 +3718,8 @@ def __init__(self, config: Qwen3OmniMoeCode2WavConfig, layer_idx):\n \n         self.block = nn.ModuleList(block)\n \n+        self.post_init()\n+\n     def forward(self, hidden, **kwargs):\n         for block in self.block:\n             hidden = block(hidden)"
        },
        {
            "sha": "8c07d2bcacb59f930bf8c6a78bfcfba05df0d1b2",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -2337,6 +2337,8 @@ def __init__(self, config: Qwen3OmniMoeCode2WavConfig, layer_idx):\n \n         self.block = nn.ModuleList(block)\n \n+        self.post_init()\n+\n     def forward(self, hidden, **kwargs):\n         for block in self.block:\n             hidden = block(hidden)"
        },
        {
            "sha": "60de64f063ab76b01d7c31ab0f3fd2a8b403ee0c",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -632,6 +632,8 @@ def __init__(self, config, *inputs, **kwargs) -> None:\n \n         self.gradient_checkpointing = False\n \n+        self.post_init()\n+\n     def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n         merge_size = self.spatial_merge_size\n "
        },
        {
            "sha": "ed83adad79e46b2cc1b90d540b323522f577dc3a",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -528,6 +528,8 @@ def __init__(self, config, *inputs, **kwargs) -> None:\n \n         self.gradient_checkpointing = False\n \n+        self.post_init()\n+\n     def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n         merge_size = self.spatial_merge_size\n "
        },
        {
            "sha": "cd59bd25eabc2059785554fe68e5406168b96274",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -646,6 +646,8 @@ def __init__(self, config, *inputs, **kwargs) -> None:\n \n         self.gradient_checkpointing = False\n \n+        self.post_init()\n+\n     def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n         merge_size = self.spatial_merge_size\n "
        },
        {
            "sha": "1d4ced6f00d5d88a71031f035bdcd8e0c65f97fe",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -422,6 +422,8 @@ def __init__(\n         self.ctx_encoder = None\n         self.context_encoder_training = False\n \n+        self.post_init()\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -690,6 +692,8 @@ def __init__(\n         # instantiate model\n         self.rag = RagModel(config=config, question_encoder=question_encoder, generator=generator, retriever=retriever)\n \n+        self.post_init()\n+\n     def set_retriever(self, retriever: RagRetriever):\n         self.rag.retriever = retriever\n \n@@ -1126,6 +1130,8 @@ def __init__(\n         # instantiate model\n         self.rag = RagModel(config=config, question_encoder=question_encoder, generator=generator, retriever=retriever)\n \n+        self.post_init()\n+\n     def set_retriever(self, retriever: RagRetriever):\n         self.rag.retriever = retriever\n "
        },
        {
            "sha": "5a6e673ea382a922c19cf948e85ba2118c5b48cc",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -1048,6 +1048,7 @@ def __init__(self, config: SamVisionConfig):\n         self.neck = SamVisionNeck(config)\n \n         self.gradient_checkpointing = False\n+        self.post_init()\n \n     def get_input_embeddings(self):\n         return self.patch_embed"
        },
        {
            "sha": "dbaf6bab13912014b7f12a8f1ffe25c6057ac142",
            "filename": "src/transformers/models/sam2/modeling_sam2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -600,6 +600,8 @@ def __init__(self, config: Sam2HieraDetConfig):\n                 self.blocks.append(block)\n                 total_block_idx += 1\n \n+        self.post_init()\n+\n     def get_input_embeddings(self):\n         return self.patch_embed\n "
        },
        {
            "sha": "481a592454c5c0deb1e99315ba02521b11443198",
            "filename": "src/transformers/models/sam2/modular_sam2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -716,6 +716,8 @@ def __init__(self, config: Sam2HieraDetConfig):\n                 self.blocks.append(block)\n                 total_block_idx += 1\n \n+        self.post_init()\n+\n     def get_input_embeddings(self):\n         return self.patch_embed\n "
        },
        {
            "sha": "1b121043c9628faa7aecf2704edf30628cf57324",
            "filename": "src/transformers/models/sam3/modeling_sam3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -1338,6 +1338,8 @@ def __init__(self, config: Sam3DETREncoderConfig):\n \n         self.layers = nn.ModuleList([Sam3DetrEncoderLayer(config) for _ in range(config.num_layers)])\n \n+        self.post_init()\n+\n     def _prepare_multilevel_features(\n         self,\n         vision_features: list[torch.Tensor],\n@@ -1617,6 +1619,8 @@ def __init__(\n \n         self.position_encoding = Sam3SinePositionEmbedding(num_pos_feats=config.hidden_size // 2, normalize=False)\n \n+        self.post_init()\n+\n     @compile_compatible_method_lru_cache(maxsize=1)\n     def _get_coords(\n         self, height: torch.Tensor, width: torch.Tensor, dtype: torch.dtype, device: torch.device\n@@ -1987,6 +1991,8 @@ def __init__(self, config: Sam3MaskDecoderConfig):\n         self.prompt_cross_attn_norm = nn.LayerNorm(hidden_size)\n         self.prompt_cross_attn_dropout = nn.Dropout(config.dropout)\n \n+        self.post_init()\n+\n     @check_model_inputs\n     def forward(\n         self,"
        },
        {
            "sha": "dc97a45c624e960d36a1dedb916413614aff772a",
            "filename": "src/transformers/models/sam3_video/modeling_sam3_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -540,6 +540,8 @@ def __init__(self, config: Sam3VideoConfig):\n \n         self.tracker_neck = Sam3VisionNeck(config.detector_config.vision_config)\n \n+        self.post_init()\n+\n     def get_vision_features_for_tracker(self, vision_embeds: torch.Tensor):\n         hidden_states = vision_embeds.last_hidden_state\n         batch_size = hidden_states.shape[0]"
        },
        {
            "sha": "1c0061c473346029add947cfb80d110b0c38f114",
            "filename": "src/transformers/models/sam_hq/modeling_sam_hq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -525,6 +525,7 @@ def __init__(self, config: SamHQVisionConfig):\n         self.neck = SamHQVisionNeck(config)\n \n         self.gradient_checkpointing = False\n+        self.post_init()\n \n     def get_input_embeddings(self):\n         return self.patch_embed"
        },
        {
            "sha": "e400cd740efd2a549bba97d7c5be27fde0f17324",
            "filename": "src/transformers/models/segformer/modeling_segformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -549,9 +549,9 @@ def forward(self, hidden_states: torch.Tensor):\n         return hidden_states\n \n \n-class SegformerDecodeHead(SegformerPreTrainedModel):\n+class SegformerDecodeHead(nn.Module):\n     def __init__(self, config):\n-        super().__init__(config)\n+        super().__init__()\n         # linear layers which will unify the channel dimension of each of the encoder blocks to the same config.decoder_hidden_size\n         mlps = []\n         for i in range(config.num_encoder_blocks):"
        },
        {
            "sha": "723ddbd38f2d6a0033bc6ebe9b872176e133b300",
            "filename": "src/transformers/models/shieldgemma2/modeling_shieldgemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -57,6 +57,7 @@ def __init__(self, config: ShieldGemma2Config):\n         self.yes_token_index = getattr(config, \"yes_token_index\", 10_784)\n         self.no_token_index = getattr(config, \"no_token_index\", 3771)\n         self.model = AutoModelForImageTextToText.from_config(config=config)\n+        self.post_init()\n \n     def get_input_embeddings(self):\n         return self.model.language_model.get_input_embeddings()"
        },
        {
            "sha": "39d2765c645c8cf9383886bada75a76e8a0189fc",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -631,6 +631,8 @@ def __init__(self, config: SiglipVisionConfig):\n         if self.use_head:\n             self.head = SiglipMultiheadAttentionPoolingHead(config)\n \n+        self.post_init()\n+\n     @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "477ee3348599b3af1af32e926f3bb92e32035543",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -501,6 +501,8 @@ def __init__(self, config: Siglip2VisionConfig):\n         if self.use_head:\n             self.head = Siglip2MultiheadAttentionPoolingHead(config)\n \n+        self.post_init()\n+\n     @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "5785107107492f92bdc738d2c9ba3f2c5469d987",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -330,6 +330,8 @@ def __init__(self, config: SmolVLMVisionConfig):\n         self.patch_size = config.patch_size\n         self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n+        self.post_init()\n+\n     def get_input_embeddings(self):\n         return self.embeddings\n "
        },
        {
            "sha": "685434c4983a733668286964fa9a5331af1c8bf8",
            "filename": "src/transformers/models/timm_backbone/modeling_timm_backbone.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -84,6 +84,8 @@ def __init__(self, config, **kwargs):\n         self._all_layers = {layer[\"module\"]: str(i) for i, layer in enumerate(self._backbone.feature_info.info)}\n         super()._init_backbone(config)\n \n+        self.post_init()\n+\n     @classmethod\n     def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n         requires_backends(cls, [\"vision\", \"timm\"])"
        },
        {
            "sha": "df16e6c448fc4de4f0036b7dfef869be73dcc08b",
            "filename": "src/transformers/models/timm_wrapper/modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -90,10 +90,6 @@ class TimmWrapperPreTrainedModel(PreTrainedModel):\n     # used in Trainer to avoid passing `loss_kwargs` to model forward\n     accepts_loss_kwargs = False\n \n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\", \"timm\"])\n-        super().__init__(*args, **kwargs)\n-\n     def post_init(self):\n         self.supports_gradient_checkpointing = self._timm_model_supports_gradient_checkpointing()\n         super().post_init()\n@@ -143,6 +139,7 @@ class TimmWrapperModel(TimmWrapperPreTrainedModel):\n     \"\"\"\n \n     def __init__(self, config: TimmWrapperConfig):\n+        requires_backends(self, [\"vision\", \"timm\"])\n         super().__init__(config)\n         # using num_classes=0 to avoid creating classification head\n         extra_init_kwargs = config.model_args or {}\n@@ -265,6 +262,7 @@ class TimmWrapperForImageClassification(TimmWrapperPreTrainedModel):\n     \"\"\"\n \n     def __init__(self, config: TimmWrapperConfig):\n+        requires_backends(self, [\"vision\", \"timm\"])\n         super().__init__(config)\n \n         if config.num_labels == 0:"
        },
        {
            "sha": "0726ebf077329b57d6706206becf1b388f892cfc",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -636,6 +636,7 @@ class TrOCRDecoderWrapper(TrOCRPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.decoder = TrOCRDecoder(config)\n+        self.post_init()\n \n     def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)"
        },
        {
            "sha": "2f35208771d931506bd28754cbeb81d022109825",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -1247,6 +1247,7 @@ def __init__(self, config):\n         super().__init__(config)\n         config.is_encoder_decoder = False\n         self.decoder = WhisperDecoder(config)\n+        self.post_init()\n \n     def get_input_embeddings(self):\n         return self.decoder.embed_tokens"
        },
        {
            "sha": "9907259c7f56c0270033a2c878b64e33d69fb649",
            "filename": "src/transformers/models/xlm/modeling_xlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -603,9 +603,6 @@ class XLMPreTrainedModel(PreTrainedModel):\n     config: XLMConfig\n     base_model_prefix = \"transformer\"\n \n-    def __init__(self, *inputs, **kwargs):\n-        super().__init__(*inputs, **kwargs)\n-\n     @property\n     def dummy_inputs(self):\n         inputs_list = torch.tensor([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])"
        },
        {
            "sha": "cf549102c5ac0a7eb75aa6148e9d4af88f2a5d72",
            "filename": "utils/check_init_weights_data.py",
            "status": "removed",
            "additions": 0,
            "deletions": 101,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/f3d5f2558b75369416ffe255a87a34358d08e429/utils%2Fcheck_init_weights_data.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f3d5f2558b75369416ffe255a87a34358d08e429/utils%2Fcheck_init_weights_data.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_init_weights_data.py?ref=f3d5f2558b75369416ffe255a87a34358d08e429",
            "patch": "@@ -1,101 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-# Copyright 2024 The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Utility that ensures `_init_weights(self, module)` implementations do not use `.data`.\n-\n-Direct `.data` access breaks the lazy-initialization safeguards handled by `HFParameter`, so the library forbids it.\n-\"\"\"\n-\n-import ast\n-import sys\n-from pathlib import Path\n-\n-\n-MODELING_ROOT = Path(\"src/transformers/models\")\n-MODELING_PATTERNS = (\"modeling_*.py\", \"modular_*.py\")\n-\n-\n-def iter_modeling_files():\n-    for pattern in MODELING_PATTERNS:\n-        yield from MODELING_ROOT.rglob(pattern)\n-\n-\n-def full_name(node):\n-    \"\"\"\n-    Return full dotted name from an Attribute or Name node.\n-    \"\"\"\n-    if isinstance(node, ast.Name):\n-        return node.id\n-    elif isinstance(node, ast.Attribute):\n-        return full_name(node.value) + \".\" + node.attr\n-    else:\n-        raise ValueError(\"Not a Name or Attribute node\")\n-\n-\n-def function_has_forbidden_usage(fn: ast.FunctionDef) -> int | None:\n-    \"\"\"\n-    Returns the first offending line number if we detect an in-place operation on a module's weight, otherwise `None`.\n-    \"\"\"\n-\n-    args = fn.args.args\n-    if len(args) < 2 or getattr(args[0], \"arg\", None) != \"self\" or getattr(args[1], \"arg\", None) != \"module\":\n-        return None\n-\n-    for node in ast.walk(fn):\n-        if isinstance(node, ast.Call) and isinstance(node.func, ast.Attribute):\n-            is_inplace_ops = node.func.attr.endswith(\"_\")\n-            # We allow in-place ops on tensors that are not part of the module itself (see e.g. modeling_qwen3_next.py L997)\n-            is_on_module_weight = isinstance(node.func.value, (ast.Name, ast.Attribute)) and \"module.\" in full_name(\n-                node.func.value\n-            )\n-            if is_inplace_ops and is_on_module_weight:\n-                return node.lineno\n-\n-    return None\n-\n-\n-def main() -> int:\n-    violations: list[str] = []\n-\n-    for file_path in iter_modeling_files():\n-        try:\n-            text = file_path.read_text(encoding=\"utf-8\")\n-            tree = ast.parse(text, filename=str(file_path))\n-        except Exception as exc:\n-            violations.append(f\"{file_path}: failed to parse ({exc}).\")\n-            continue\n-\n-        for node in ast.walk(tree):\n-            if isinstance(node, ast.FunctionDef) and node.name == \"_init_weights\":\n-                offending_line = function_has_forbidden_usage(node)\n-                if offending_line is not None:\n-                    violations.append(\n-                        f\"{file_path}:{offending_line}: `_init_weights(self, module)` uses an in-place operation on a \"\n-                        \"module's weight. Please use the `init` functions primitives instead, usually imported as \"\n-                        \"`from ... import initialization as init`.\"\n-                    )\n-                    break\n-\n-    if violations:\n-        print(\"Found forbidden usage inside `_init_weights(self, module)`:\\n\", file=sys.stderr)\n-        print(\"\\n\".join(violations), file=sys.stderr)\n-        return 1\n-\n-    return 0\n-\n-\n-if __name__ == \"__main__\":\n-    sys.exit(main())"
        },
        {
            "sha": "a214a8bc4c93916876fa92b2bf4b0cc74b86d2bc",
            "filename": "utils/check_modeling_structure.py",
            "status": "added",
            "additions": 150,
            "deletions": 0,
            "changes": 150,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7aec088a63183292518359880271e2e93a288cc/utils%2Fcheck_modeling_structure.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7aec088a63183292518359880271e2e93a288cc/utils%2Fcheck_modeling_structure.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_modeling_structure.py?ref=c7aec088a63183292518359880271e2e93a288cc",
            "patch": "@@ -0,0 +1,150 @@\n+#!/usr/bin/env python\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"\n+Utility that ensures that modeling (and modular) files respect some important conventions we have in Transformers.\n+\"\"\"\n+\n+import ast\n+import sys\n+from pathlib import Path\n+\n+from rich import print\n+\n+\n+MODELS_ROOT = Path(\"src/transformers/models\")\n+MODELING_PATTERNS = (\"modeling_*.py\", \"modular_*.py\")\n+\n+\n+def iter_modeling_files():\n+    for pattern in MODELING_PATTERNS:\n+        yield from MODELS_ROOT.rglob(pattern)\n+\n+\n+def colored_error_message(file_path: str, line_number: int, message: str) -> str:\n+    return f\"[bold red]{file_path}[/bold red]:[bold yellow]L{line_number}[/bold yellow]: {message}\"\n+\n+\n+def full_name(node: ast.AST):\n+    \"\"\"\n+    Return full dotted name from an Attribute or Name node.\n+    \"\"\"\n+    if isinstance(node, ast.Name):\n+        return node.id\n+    elif isinstance(node, ast.Attribute):\n+        return full_name(node.value) + \".\" + node.attr\n+    else:\n+        raise ValueError(\"Not a Name or Attribute node\")\n+\n+\n+def check_init_weights(node: ast.AST, violations: list[str], file_path: str) -> list[str]:\n+    \"\"\"\n+    Check that `_init_weights` correctly use `init.(...)` patterns to init the weights in-place. This is very important,\n+    as we rely on the internal flag set on the parameters themselves to check if they need to be re-init or not.\n+    \"\"\"\n+    if isinstance(node, ast.FunctionDef) and node.name == \"_init_weights\":\n+        args = node.args.args\n+        if len(args) < 2 or getattr(args[0], \"arg\", None) != \"self\" or getattr(args[1], \"arg\", None) != \"module\":\n+            return violations\n+\n+        for sub_node in ast.walk(node):\n+            if isinstance(sub_node, ast.Call) and isinstance(sub_node.func, ast.Attribute):\n+                is_inplace_ops = sub_node.func.attr.endswith(\"_\")\n+                # We allow in-place ops on tensors that are not part of the module itself (see e.g. modeling_qwen3_next.py L997)\n+                is_on_module_weight = isinstance(\n+                    sub_node.func.value, (ast.Name, ast.Attribute)\n+                ) and \"module.\" in full_name(sub_node.func.value)\n+                if is_inplace_ops and is_on_module_weight:\n+                    error_msg = (\n+                        \"`_init_weights(self, module)` uses an in-place operation on a module's weight. Please use the \"\n+                        \"`init` functions primitives instead, usually imported as `from ... import initialization as init`\"\n+                    )\n+                    violations.append(colored_error_message(file_path, sub_node.lineno, error_msg))\n+\n+    return violations\n+\n+\n+def is_self_method_call(node: ast.AST, method: str) -> bool:\n+    \"\"\"Check if `node` is a method call on `self`, such as `self.method(...)`\"\"\"\n+    return (\n+        isinstance(node, ast.Call)\n+        and isinstance(node.func, ast.Attribute)\n+        and isinstance(node.func.value, ast.Name)\n+        and node.func.value.id == \"self\"\n+        and node.func.attr == method\n+    )\n+\n+\n+def is_super_method_call(node: ast.AST, method: str) -> bool:\n+    \"\"\"Check if `node` is a call to `super().method(...)`\"\"\"\n+    return (\n+        isinstance(node, ast.Call)\n+        and isinstance(node.func, ast.Attribute)\n+        and isinstance(node.func.value, ast.Call)\n+        and isinstance(node.func.value.func, ast.Name)\n+        and node.func.value.func.id == \"super\"\n+        and node.func.attr == method\n+    )\n+\n+\n+def check_post_init(node: ast.AST, violations: list[str], file_path: str) -> list[str]:\n+    \"\"\"\n+    Check that `self.post_init()` is correctly called at the end of `__init__` for all `PreTrainedModel`s. This is\n+    very important as we need to do some processing there.\n+    \"\"\"\n+    # Check if it's a PreTrainedModel class definition\n+    if isinstance(node, ast.ClassDef) and any(full_name(parent).endswith(\"PreTrainedModel\") for parent in node.bases):\n+        for sub_node in node.body:\n+            # Check that we are in __init__\n+            if isinstance(sub_node, ast.FunctionDef) and sub_node.name == \"__init__\":\n+                for statement in ast.walk(sub_node):\n+                    # This means it's correctly called verbatim\n+                    if is_self_method_call(statement, method=\"post_init\"):\n+                        break\n+                    # This means `super().__init__` is called in a modular, so it is already called in the parent\n+                    elif \"modular_\" in str(file_path) and is_super_method_call(statement, method=\"__init__\"):\n+                        break\n+                # If we did not break, `post_init` was never called\n+                else:\n+                    error_msg = f\"`__init__` of {node.name} does not call `self.post_init`\"\n+                    violations.append(colored_error_message(file_path, sub_node.lineno, error_msg))\n+                break\n+\n+    return violations\n+\n+\n+def main():\n+    violations: list[str] = []\n+\n+    for file_path in iter_modeling_files():\n+        try:\n+            text = file_path.read_text(encoding=\"utf-8\")\n+            tree = ast.parse(text, filename=str(file_path))\n+        except Exception as exc:\n+            violations.append(f\"{file_path}: failed to parse ({exc}).\")\n+            continue\n+\n+        for node in ast.walk(tree):\n+            violations = check_init_weights(node, violations, file_path)\n+            violations = check_post_init(node, violations, file_path)\n+\n+    if len(violations) > 0:\n+        violations = sorted(violations)\n+        print(\"\\n\".join(violations), file=sys.stderr)\n+        raise ValueError(\"Some errors in modelings. Check the above message\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        }
    ],
    "stats": {
        "total": 399,
        "additions": 267,
        "deletions": 132
    }
}