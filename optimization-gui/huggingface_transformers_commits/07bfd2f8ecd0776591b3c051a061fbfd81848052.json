{
    "author": "YangKai0616",
    "message": "[XPU] Add flash_attn2 support for XPU (#41956)\n\n* Add flash_attention_2 and kernels-community/flash-attn support for XPU\n\n* Add flash-attn-2 support for XPU\n\n* Delete deterministic algorithm for xpu\n\n* Fix code style\n\n* Modify repo_id to match the latest kernels-community/flash-attn2\n\n* Fix code style\n\n* Update\n\n* Make quality\n\n* Use kernels loading\n\n* Update\n\n* Delete invalid import\n\n* Update comment\n\n---------\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "07bfd2f8ecd0776591b3c051a061fbfd81848052",
    "files": [
        {
            "sha": "5e31bdae01ebee72efbb61b8636b9d6e212dbd25",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -24,6 +24,7 @@\n     is_flash_attn_3_available,\n     is_flash_attn_greater_or_equal_2_10,\n     is_torch_npu_available,\n+    is_torch_xpu_available,\n     logging,\n )\n \n@@ -45,7 +46,12 @@ def flash_attn_supports_top_left_mask():\n \n # TODO Deprecate when all models have the attention interface\n def is_flash_attn_available():\n-    return is_flash_attn_3_available() or is_flash_attn_2_available() or is_torch_npu_available()\n+    return (\n+        is_flash_attn_3_available()\n+        or is_flash_attn_2_available()\n+        or is_torch_npu_available()\n+        or is_torch_xpu_available()\n+    )\n \n \n # `globals()` is not compatible with dynamo, hence we have do define them in global scope ourselves\n@@ -97,7 +103,7 @@ def _lazy_imports(implementation: Optional[str]):\n             if flash_attn_varlen_func is None or flash_attn_func is None:\n                 raise ValueError(\n                     f\"Could not find the currently requested flash attention implementation at `{implementation}`.\"\n-                    f\"Make sure that you request a valid kernel from the hub, e.g. `kernels-community/flash-attn`.\"\n+                    f\"Make sure that you request a valid kernel from the hub, e.g. `kernels-community/flash-attn2`.\"\n                 )\n \n     return flash_attn_func, flash_attn_varlen_func, pad_input, unpad_input"
        },
        {
            "sha": "ae96c6d89a3da182ba63faf3957277fa0a0eb866",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -116,6 +116,7 @@\n     is_torch_greater_or_equal,\n     is_torch_mlu_available,\n     is_torch_npu_available,\n+    is_torch_xpu_available,\n     logging,\n )\n from .utils.generic import _CAN_RECORD_REGISTRY, GeneralInterface, OutputRecorder\n@@ -1575,6 +1576,10 @@ def _flash_attn_2_can_dispatch(self, is_init_check: bool = False) -> bool:\n                 logger.info(\"Detect using FlashAttention2 on Ascend NPU.\")\n                 return True\n \n+            if is_torch_xpu_available():\n+                logger.info(\"Detect using FlashAttention2 (via kernel `kernels-community/flash-attn2`) on XPU.\")\n+                return True\n+\n             if importlib.util.find_spec(\"flash_attn\") is None:\n                 raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n             else:\n@@ -1800,7 +1805,10 @@ def _check_and_adjust_attn_implementation(\n             and not is_torch_npu_available()\n         ):\n             if attn_implementation.endswith(\"2\"):\n-                applicable_attn_implementation = \"kernels-community/flash-attn\"\n+                applicable_attn_implementation = \"kernels-community/flash-attn2\"\n+                if is_torch_xpu_available():\n+                    # On XPU, kernels library is the native implementation. Rename variable to avoid \"fallback\" warning and irrelevant checks.\n+                    attn_implementation = \"kernels-community/flash-attn2\"\n             else:\n                 applicable_attn_implementation = \"kernels-community/vllm-flash-attn3\"\n "
        },
        {
            "sha": "14ed19f48e9deb8c66469b45176c37c78390d559",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -593,7 +593,7 @@ def require_flash_attn(test_case):\n     try:\n         from kernels import get_kernel\n \n-        get_kernel(\"kernels-community/flash-attn\")\n+        get_kernel(\"kernels-community/flash-attn2\")\n     except Exception as _:\n         kernels_available = False\n "
        },
        {
            "sha": "6dbacb3995765b86319e69211e40b993936477d0",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -24,7 +24,7 @@\n     _COMMON_MODEL_NAMES_MAP,\n     is_flaky,\n     require_flash_attn,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n )\n \n@@ -550,7 +550,7 @@ def test_model_rope_scaling_frequencies(self):\n             torch.testing.assert_close(yarn_sin_long, original_sin_long)\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     @is_flaky()\n     @slow"
        },
        {
            "sha": "d1f14ad0c9c8ba9ebe78e94e97dd89a2ee07b931",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -1848,7 +1848,7 @@ def test_eager_matches_sdpa_generate(self):\n \n     @pytest.mark.flash_attn_test\n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_eager_matches_fa2_generate(self):\n         \"\"\"Tests that generate has equivalent outputs with FA2 and eager attention implementations.\"\"\"\n@@ -1863,7 +1863,7 @@ def test_eager_matches_fa3_generate(self):\n         self._test_attention_implementation(\"flash_attention_3\")\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     def test_flash_attention_2_continue_generate_with_position_ids(self):\n         \"\"\"\n@@ -2065,14 +2065,14 @@ def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n         self.attention_mask_padding_matches_padding_free_with_position_ids(attn_implementation=\"sdpa\")\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     @slow\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         self.attention_mask_padding_matches_padding_free_with_position_ids(attn_implementation=\"flash_attention_2\")\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     @slow\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):"
        },
        {
            "sha": "df11a02605b0ddd16214bb0ee2dcf4b45c7c1f1a",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -34,7 +34,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -444,7 +443,7 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa\n         pass\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     @slow\n     @unittest.skip("
        },
        {
            "sha": "bb2fb03b56155e2ac6aa6bd26b9fb5e8c3151699",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -29,7 +29,6 @@\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -324,7 +323,7 @@ def _reinitialize_config(base_config, new_kwargs):\n             )  # missing \"factor\"\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_bitsandbytes\n     @pytest.mark.flash_attn_test\n     @require_read_token\n@@ -364,7 +363,7 @@ def test_flash_attn_2_generate_padding_right(self):\n         self.assertListEqual(output_native, output_fa_2)\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     @pytest.mark.flash_attn_test\n     def test_use_flash_attention_2_true(self):\n@@ -379,7 +378,7 @@ def test_use_flash_attention_2_true(self):\n \n                 new_model = DiffLlamaForCausalLM.from_pretrained(\n                     tmp_dir, attn_implementation=\"flash_attention_2\", dtype=torch.float16\n-                ).to(\"cuda\")\n+                ).to(torch_device)\n \n                 self.assertTrue(new_model.config._attn_implementation == \"flash_attention_2\")\n "
        },
        {
            "sha": "c1d96ecaf80ff0c0b15182be78422f36a0457c0a",
            "filename": "tests/models/ernie4_5_moe/test_modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -25,7 +25,7 @@\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     require_torch_large_accelerator,\n     require_torch_multi_accelerator,\n     slow,\n@@ -56,7 +56,7 @@ class Ernie4_5_MoeModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = Ernie4_5_MoeModelTester\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     @is_flaky()\n     @slow"
        },
        {
            "sha": "5a530743e403d12da11f13600c88e2caac1734c2",
            "filename": "tests/models/esm/test_modeling_esm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -25,7 +25,7 @@\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -306,7 +306,7 @@ def test_resize_tokens_embeddings(self):\n         pass\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     @is_flaky()\n     @slow"
        },
        {
            "sha": "13fa78da4dff18c084fd63e350b606bc1d5f6e4a",
            "filename": "tests/models/glm4/test_modeling_glm4.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -25,7 +25,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_large_accelerator,\n-    require_torch_large_gpu,\n     slow,\n     torch_device,\n )\n@@ -177,7 +176,7 @@ def test_model_9b_sdpa(self):\n         self.assertEqual(output_text, EXPECTED_TEXT)\n \n     @require_flash_attn\n-    @require_torch_large_gpu\n+    @require_torch_large_accelerator\n     @pytest.mark.flash_attn_test\n     def test_model_9b_flash_attn(self):\n         EXPECTED_TEXTS = Expectations(\n@@ -187,6 +186,10 @@ def test_model_9b_flash_attn(self):\n                     \"Hello I am doing a project on the history of the internet and I need to know what the first website was and what\",\n                     \"Hi today I am going to tell you about the most common disease in the world. This disease is called diabetes\",\n                 ],\n+                (\"xpu\", None): [\n+                    \"Hello I am doing a project on the history of the internet and I need to know what the first website was and what\",\n+                    \"Hi today I am going to tell you about the most common disease in the world. This disease is called diabetes\",\n+                ],\n             }\n         )\n         EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()"
        },
        {
            "sha": "d9ce30a21cc0fb22e8e603b127b8cbd05b6491ee",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -22,7 +22,7 @@\n     cleanup,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -410,7 +410,7 @@ def test_contrastive_search_gpt2(self):\n         )\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     @slow\n     def test_flash_attn_2_generate_padding_left(self):"
        },
        {
            "sha": "d46f3c212d2bcaa9cf6601088405d6f8ffea9b0e",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -28,7 +28,7 @@\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -513,7 +513,7 @@ def test_attention_outputs(self):\n             )\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_bitsandbytes\n     @pytest.mark.flash_attn_test\n     @slow"
        },
        {
            "sha": "87d4798c52fd549210ed5f4713cd8c167a7ca720",
            "filename": "tests/models/jetmoe/test_modeling_jetmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -22,7 +22,7 @@\n     cleanup,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -107,7 +107,7 @@ class JetMoeModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = JetMoeModelTester\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     @slow\n     def test_flash_attn_2_inference_equivalence_right_padding(self):"
        },
        {
            "sha": "105fba5e596b6f67569a74753ddace725a9906db",
            "filename": "tests/models/kosmos2_5/test_modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -32,6 +32,7 @@\n from transformers.testing_utils import (\n     require_flash_attn,\n     require_torch,\n+    require_torch_accelerator,\n     require_torch_gpu,\n     require_vision,\n     slow,\n@@ -627,7 +628,7 @@ def test_sdpa(self):\n         self.assertListEqual(generated_text, EXPECTED_TEXT[self.cuda_compute_capability_major_version])\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     @slow\n     def test_FA2(self):"
        },
        {
            "sha": "8a4dbdc36f27acb4b6aeb6897752667e911190e2",
            "filename": "tests/models/m2m_100/test_modeling_m2m_100.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -26,8 +26,8 @@\n     require_sentencepiece,\n     require_tokenizers,\n     require_torch,\n+    require_torch_accelerator,\n     require_torch_fp16,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -418,7 +418,7 @@ def test_seq_to_seq_generation(self):\n         assert generated == expected_en\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     @slow\n     def test_flash_attn_2_seq_to_seq_generation(self):"
        },
        {
            "sha": "fe2c4755901b7c109f78e1d9edde882e1b0a500b",
            "filename": "tests/models/ministral/test_modeling_ministral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -27,7 +27,7 @@\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -69,7 +69,7 @@ def is_pipeline_test_to_skip(\n         return True\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     @slow\n     def test_flash_attn_2_inference_equivalence_right_padding(self):"
        },
        {
            "sha": "36c61a21fbcecbc654229898a6789fd9456c13ea",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -23,7 +23,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -63,7 +62,7 @@ def is_pipeline_test_to_skip(\n         return True\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     @slow\n     def test_flash_attn_2_inference_equivalence_right_padding(self):"
        },
        {
            "sha": "5f74e15cd3a6734062dc3ea39cded9a77c5d045a",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -26,6 +26,7 @@\n     CaptureLogger,\n     require_flash_attn,\n     require_torch,\n+    require_torch_accelerator,\n     require_torch_gpu,\n     slow,\n     torch_device,\n@@ -366,7 +367,7 @@ def test_saved_config_excludes_reference_compile(self):\n             self.assertNotIn(\"reference_compile\", config_dict)\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     def test_flash_attention_dispatches_by_default(self):\n         \"ModernBert should dispatch to FA2 by default, not SDPA\""
        },
        {
            "sha": "1884e3c03b16b27eaa6fe363bd6a136791da7adb",
            "filename": "tests/models/seed_oss/test_modeling_seed_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fseed_oss%2Ftest_modeling_seed_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fseed_oss%2Ftest_modeling_seed_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseed_oss%2Ftest_modeling_seed_oss.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -23,7 +23,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_large_accelerator,\n-    require_torch_large_gpu,\n     slow,\n     torch_device,\n )\n@@ -106,7 +105,7 @@ def test_model_36b_sdpa(self):\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     @require_flash_attn\n-    @require_torch_large_gpu\n+    @require_torch_large_accelerator\n     @pytest.mark.flash_attn_test\n     def test_model_36b_flash_attn(self):\n         EXPECTED_TEXTS = ["
        },
        {
            "sha": "4575894dcb3bc88aa5df2ef34ae6a92665751a33",
            "filename": "tests/models/zamba/test_modeling_zamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -24,7 +24,7 @@\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -443,7 +443,7 @@ def _get_input_ids_and_config(self):\n         return config, input_ids, input_mask\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_bitsandbytes\n     @pytest.mark.flash_attn_test\n     @slow"
        },
        {
            "sha": "5c8e42257cca9225f0a5fee41d3a373e940d2613",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -26,7 +26,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -459,7 +458,7 @@ def _get_input_ids_and_config(self):\n         return config, input_ids, input_mask\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_bitsandbytes\n     @pytest.mark.flash_attn_test\n     @slow"
        },
        {
            "sha": "297c8cfc219290bf91a10ec2a7cdf1c6753e1d09",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 11,
            "deletions": 5,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -2737,6 +2737,12 @@ def flash_attn_inference_equivalence(\n             if getattr(config, \"sliding_window\", None):\n                 config.sliding_window = 2\n \n+                if torch_device == \"xpu\" and (\n+                    attn_implementation == \"kernels-community/flash-attn2\"\n+                    or attn_implementation == \"flash_attention_2\"\n+                ):\n+                    self.skipTest(\"XPU does not support sliding window attention with Flash-Attention-2 currently.\")\n+\n             model = model_class(config)\n             if not all(\n                 submodel._supports_flash_attn for submodel in model.modules() if isinstance(submodel, PreTrainedModel)\n@@ -2878,15 +2884,15 @@ def test_flash_attn_kernels_mps_inference_equivalence(self):\n         )\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     @slow\n     @is_flaky()\n     def test_flash_attn_2_inference_equivalence(self):\n         self.flash_attn_inference_equivalence(attn_implementation=\"flash_attention_2\", padding_side=\"left\")\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     @slow\n     @is_flaky()\n@@ -3278,7 +3284,7 @@ def flash_attn_can_dispatch_composite_models(self, attn_implementation: str):\n                         raise ValueError(f\"The {attn_implementation} model should have {attn_implementation} layers\")\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     def test_flash_attn_2_can_dispatch_composite_models(self):\n         self.flash_attn_can_dispatch_composite_models(attn_implementation=\"flash_attention_2\")\n@@ -3290,7 +3296,7 @@ def test_flash_attn_3_can_dispatch_composite_models(self):\n         self.flash_attn_can_dispatch_composite_models(attn_implementation=\"flash_attention_3\")\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_bitsandbytes\n     @mark.flash_attn_test\n     @slow\n@@ -3433,7 +3439,7 @@ def flash_attn_from_config(self, attn_implementation: str, test_fwd_in_train: bo\n                 self.assertTrue(model_from_pretrained.config._attn_implementation != attn_implementation)\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     @slow\n     def test_flash_attn_2_from_config(self):"
        },
        {
            "sha": "79749d35983c845b06f549ac9ad2d34ea969d827",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07bfd2f8ecd0776591b3c051a061fbfd81848052/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=07bfd2f8ecd0776591b3c051a061fbfd81848052",
            "patch": "@@ -2885,7 +2885,7 @@ def test_kernels_fallback(self):\n                 )\n \n         self.assertTrue(\n-            \"You do not have `flash_attn` installed, using `kernels-community/flash-attn` from the `kernels` library instead!\"\n+            \"You do not have `flash_attn` installed, using `kernels-community/flash-attn2` from the `kernels` library instead!\"\n             in cl.out\n         )\n \n@@ -2897,7 +2897,7 @@ def test_not_available_kernels(self):\n \n         with self.assertRaises(ImportError) as cm:\n             _ = AutoModel.from_pretrained(\n-                \"hf-tiny-model-private/tiny-random-MCTCTModel\", attn_implementation=\"kernels-community/flash-attn\"\n+                \"hf-tiny-model-private/tiny-random-MCTCTModel\", attn_implementation=\"kernels-community/flash-attn2\"\n             )\n \n         self.assertTrue(\"`kernels` is either not installed or uses an incompatible version.\" in str(cm.exception))"
        }
    ],
    "stats": {
        "total": 118,
        "additions": 69,
        "deletions": 49
    }
}