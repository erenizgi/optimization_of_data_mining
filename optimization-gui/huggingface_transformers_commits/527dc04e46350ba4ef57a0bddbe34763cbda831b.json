{
    "author": "faaany",
    "message": "[docs] add the missing import for Image and bug fix (#34776)\n\n* add the missing import for Image lib\r\n\r\n* add more devices in comment\r\n\r\n* bug fix",
    "sha": "527dc04e46350ba4ef57a0bddbe34763cbda831b",
    "files": [
        {
            "sha": "3929f7994bdafb320fbafa465084984abc63409f",
            "filename": "docs/source/en/tasks/video_text_to_text.md",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/527dc04e46350ba4ef57a0bddbe34763cbda831b/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/527dc04e46350ba4ef57a0bddbe34763cbda831b/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md?ref=527dc04e46350ba4ef57a0bddbe34763cbda831b",
            "patch": "@@ -47,7 +47,7 @@ model_id = \"llava-hf/llava-interleave-qwen-0.5b-hf\"\n processor = LlavaProcessor.from_pretrained(model_id)\n \n model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16)\n-model.to(\"cuda\")\n+model.to(\"cuda\") # can also be xpu, mps, npu etc. depending on your hardware accelerator\n ```\n \n Some models directly consume the `<video>` token, and others accept `<image>` tokens equal to the number of sampled frames. This model handles videos in the latter fashion. We will write a simple utility to handle image tokens, and another utility to get a video from a url and sample frames from it. \n@@ -56,6 +56,7 @@ Some models directly consume the `<video>` token, and others accept `<image>` to\n import uuid\n import requests\n import cv2\n+from PIL import Image\n \n def replace_video_with_images(text, frames):\n   return text.replace(\"<video>\", \"<image>\" * frames)\n@@ -82,7 +83,7 @@ def sample_frames(url, num_frames):\n         if i % interval == 0:\n             frames.append(pil_img)\n     video.release()\n-    return frames\n+    return frames[:num_frames]\n ```\n \n Let's get our inputs. We will sample frames and concatenate them.\n@@ -127,7 +128,7 @@ This model has a prompt template that looks like following. First, we'll put all\n user_prompt = \"Are these two cats in these two videos doing the same thing?\"\n toks = \"<image>\" * 12\n prompt = \"<|im_start|>user\"+ toks + f\"\\n{user_prompt}<|im_end|><|im_start|>assistant\"\n-inputs = processor(prompt, images=videos).to(model.device, model.dtype)\n+inputs = processor(text=prompt, images=videos, return_tensors=\"pt\").to(model.device, model.dtype)\n ```\n \n We can now call [`~GenerationMixin.generate`] for inference. The model outputs the question in our input and answer, so we only take the text after the prompt and `assistant` part from the model output. "
        }
    ],
    "stats": {
        "total": 7,
        "additions": 4,
        "deletions": 3
    }
}