{
    "author": "vasqu",
    "message": "üî¥üî¥üî¥ [`Attention`] Refactor Attention Interface for Bart-based Models (#38108)\n\n* starting attn refactor for encoder decoder models via bart (eager + sdpa)\n\n* flash attention works, remove unnecessary code\n\n* flex attention support for bart!, gotta check if the renaming is not too aggressive\n\n* some comments\n\n* skip flex grad test for standalone as done with the other test\n\n* revert flex attn rename (for now), sdpa simplify, and todos\n\n* more todos\n\n* refactor mask creation for reuse\n\n* modular attempt at biogpt\n\n* first batch of other models\n\n* fix attn dropout\n\n* fix autoformer copies\n\n* hubert\n\n* another batch of models\n\n* copies/style + last round of bart models --> whisper next?\n\n* remove unnecessary _reshape function and remove copy to whisper\n\n* add skip for decoder-only models out of enc-dec (same as in bart)\n\n* bring back licences\n\n* remove comment, added to pr read instead\n\n* mostly docs\n\n* disable sew flex attn as it's unclear attn mask for now\n\n* oops\n\n* test fixes for enc-dec\n\n* torch fx fixes + try at flex attn\n\n* skip on mbart\n\n* some more fixes\n\n* musicgen skip / delete old attn class logic + sdpa compose compile skip\n\n* disable flex attn for musicgen, not worth the effort\n\n* more fixes and style\n\n* flex attention test for dropout and encoder decoder that dont have main input names\n\n* informer fixes\n\n* the weirdest thing I've encountered yet...\n\n* style\n\n* remove empty tensor attempt, found core root in previous commits\n\n* disable time series due to tests being very text centric on inputs\n\n* add speech to text to be ignoring the other attns, also due to tests\n\n* update docs\n\n* remaining issues resolved ?\n\n* update docs for current state --> nllb moe and pegasus x sdpa is questionable :D\n\n* some models have not set the is_causal flag...\n\n* change dtype in softmax tol old behaviour + some modular fixes\n\n* I hate it but it is what it is\n\n* fixes from main for bart\n\n* forgot this one\n\n* some model fixes\n\n* style\n\n* current status\n\n* marian works now\n\n* fixing some copies\n\n* some copy fixes + time series x informer\n\n* last models possibly and fixes on style/copies\n\n* some post merge fixes\n\n* more fixes\n\n* make attention interface callable and move warnings there\n\n* style lol\n\n* add comment to \"unsupported\"\n\n* remove callable interface and change interface warnings + some copies\n\n* fix\n\n* ternary is ugly af, make it simpler\n\n* how did that happen\n\n* fix flex attn test\n\n* failing the test\n\n* no more fallback! fixing copies next\n\n* style + attn fixed\n\n* fixing copies and mask creation\n\n* wrong copy\n\n* fixup tests and disable flex attn for now\n\n* fixup last tests?",
    "sha": "d95c864a254ae93c6b34d18b3a7e9ec36322549f",
    "files": [
        {
            "sha": "11ab89c9f2d90710aed25dd05e029f385aa8cabd",
            "filename": "docs/source/en/model_doc/biogpt.md",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -18,6 +18,7 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n@@ -40,13 +41,13 @@ This model was contributed by [kamalkraj](https://huggingface.co/kamalkraj). The\n \n ### Using Scaled Dot Product Attention (SDPA)\n \n-PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n-encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n-[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n page for more information.\n \n-SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set \n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n \n ```\n@@ -109,7 +110,7 @@ we saw the following speedups during inference.\n [[autodoc]] BioGptForCausalLM\n     - forward\n \n-    \n+\n ## BioGptForTokenClassification\n \n [[autodoc]] BioGptForTokenClassification"
        },
        {
            "sha": "341e43c03040c5e48c7ee56938c92c3d219dd209",
            "filename": "docs/source/en/model_doc/blenderbot-small.md",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot-small.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot-small.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot-small.md?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -21,6 +21,8 @@ rendered properly in your Markdown viewer.\n <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n <img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n \">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n Note that [`BlenderbotSmallModel`] and\n@@ -52,7 +54,7 @@ found [here](https://github.com/facebookresearch/ParlAI).\n \n ## Usage tips\n \n-Blenderbot Small is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than \n+Blenderbot Small is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n the left.\n \n "
        },
        {
            "sha": "adfa6841e10a472bb00b5620b5af5447cf64be1c",
            "filename": "docs/source/en/model_doc/blenderbot.md",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot.md?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -21,6 +21,8 @@ rendered properly in your Markdown viewer.\n <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n <img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n \">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n ## Overview\n@@ -45,7 +47,7 @@ This model was contributed by [sshleifer](https://huggingface.co/sshleifer). The\n \n ## Usage tips and example\n \n-Blenderbot is a model with absolute position embeddings so it's usually advised to pad the inputs on the right \n+Blenderbot is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\n rather than the left.\n \n An example:\n@@ -71,7 +73,7 @@ An example:\n   `facebook/blenderbot_small_90M`, have a different architecture and consequently should be used with\n   [BlenderbotSmall](blenderbot-small).\n \n-  \n+\n ## Resources\n \n - [Causal language modeling task guide](../tasks/language_modeling)"
        },
        {
            "sha": "4fcd6363559c8bbd1a98962f521567f66febb66c",
            "filename": "docs/source/en/model_doc/marian.md",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarian.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarian.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarian.md?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -21,6 +21,8 @@ rendered properly in your Markdown viewer.\n <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n <img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n \">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n ## Overview\n@@ -155,7 +157,7 @@ Example of translating english to many romance languages, using old-style 2 char\n >>> model = MarianMTModel.from_pretrained(model_name)\n >>> translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n >>> tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n-[\"c'est une phrase en anglais que nous voulons traduire en fran√ßais\", \n+[\"c'est une phrase en anglais que nous voulons traduire en fran√ßais\",\n  'Isto deve ir para o portugu√™s.',\n  'Y esto al espa√±ol']\n ```"
        },
        {
            "sha": "fc8c8c92115d9aca9b6e0b7aa4686230a25c6aff",
            "filename": "docs/source/en/model_doc/nllb-moe.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb-moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb-moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb-moe.md?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -51,10 +51,10 @@ The original code can be found [here](https://github.com/facebookresearch/fairse\n \n ## Implementation differences with SwitchTransformers\n \n-The biggest difference is the way the tokens are routed. NLLB-MoE uses a `top-2-gate` which means that for each input, only the top two experts are selected based on the \n-highest predicted probabilities from the gating network, and the remaining experts are ignored. In `SwitchTransformers`, only the top-1 probabilities are computed, \n-which means that tokens have less probability of being forwarded. Moreover, if a token is not routed to any expert, `SwitchTransformers` still adds its unmodified hidden \n-states (kind of like a residual connection) while they are masked in `NLLB`'s top-2 routing mechanism. \n+The biggest difference is the way the tokens are routed. NLLB-MoE uses a `top-2-gate` which means that for each input, only the top two experts are selected based on the\n+highest predicted probabilities from the gating network, and the remaining experts are ignored. In `SwitchTransformers`, only the top-1 probabilities are computed,\n+which means that tokens have less probability of being forwarded. Moreover, if a token is not routed to any expert, `SwitchTransformers` still adds its unmodified hidden\n+states (kind of like a residual connection) while they are masked in `NLLB`'s top-2 routing mechanism.\n \n ## Generating with NLLB-MoE\n "
        },
        {
            "sha": "5681ac9b58a07551d2bdfd50884bfc3cb8f26171",
            "filename": "docs/source/en/model_doc/pegasus.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus.md?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -21,6 +21,8 @@ rendered properly in your Markdown viewer.\n <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n <img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAMAAAANxBKoAAAC7lBMVEUAAADg5vYHPVgAoJH+/v76+v39/f9JbLP///9+AIgAnY3///+mcqzt8fXy9fgkXa3Ax9709fr+///9/f8qXq49qp5AaLGMwrv8/P0eW60VWawxYq8yqJzG2dytt9Wyu9elzci519Lf3O3S2efY3OrY0+Xp7PT///////+dqNCexMc6Z7AGpJeGvbenstPZ5ejQ1OfJzOLa7ejh4+/r8fT29vpccbklWK8PVa0AS6ghW63O498vYa+lsdKz1NDRt9Kw1c672tbD3tnAxt7R6OHp5vDe7OrDyuDn6vLl6/EAQKak0MgATakkppo3ZK/Bz9y8w9yzu9jey97axdvHzeG21NHH4trTwthKZrVGZLSUSpuPQJiGAI+GAI8SWKydycLL4d7f2OTi1+S9xNzL0ePT6OLGzeEAo5U0qJw/aLEAo5JFa7JBabEAp5Y4qZ2QxLyKmsm3kL2xoMOehrRNb7RIbbOZgrGre68AUqwAqZqNN5aKJ5N/lMq+qsd8kMa4pcWzh7muhLMEV69juq2kbKqgUaOTR5uMMZWLLZSGAI5VAIdEAH+ovNDHuNCnxcy3qcaYx8K8msGplrx+wLahjbYdXrV6vbMvYK9DrZ8QrZ8tqJuFms+Sos6sw8ecy8RffsNVeMCvmb43aLltv7Q4Y7EZWK4QWa1gt6meZKUdr6GOAZVeA4xPAISyveLUwtivxtKTpNJ2jcqfvcltiMiwwcfAoMVxhL+Kx7xjdrqTe60tsaNQs6KaRKACrJ6UTZwkqpqTL5pkHY4AloSgsd2ptNXPvNOOncuxxsqFl8lmg8apt8FJcr9EbryGxLqlkrkrY7dRa7ZGZLQ5t6iXUZ6PPpgVpZeJCJFKAIGareTa0+KJod3H0deY2M+esM25usmYu8d2zsJOdcBVvrCLbqcAOaaHaKQAMaScWqKBXqCXMJ2RHpiLF5NmJZAdAHN2kta11dKu1M+DkcZLdb+Mcql3TppyRJdzQ5ZtNZNlIY+DF4+voCOQAAAAZ3RSTlMABAT+MEEJ/RH+/TP+Zlv+pUo6Ifz8+fco/fz6+evr39S9nJmOilQaF/7+/f38+smmoYp6b1T+/v7++vj189zU0tDJxsGzsrKSfv34+Pf27dDOysG9t6+n/vv6+vr59uzr1tG+tZ6Qg9Ym3QAABR5JREFUSMeNlVVUG1EQhpcuxEspXqS0SKEtxQp1d3d332STTRpIQhIISQgJhODu7lAoDoUCpe7u7u7+1puGpqnCPOyZvffbOXPm/PsP9JfQgyCC+tmTABTOcbxDz/heENS7/1F+9nhvkHePG0wNDLbGWwdXL+rbLWvpmZHXD8+gMfBjTh+aSe6Gnn7lwQIOTR0c8wfX3PWgv7avbdKwf/ZoBp1Gp/PvuvXW3vw5ib7emnTW4OR+3D4jB9vjNJ/7gNvfWWeH/TO/JyYrsiKCRjVEZA3UB+96kON+DxOQ/NLE8PE5iUYgIXjFnCOlxEQMaSGVxjg4gxOnEycGz8bptuNjVx08LscIgrzH3umcn+KKtiBIyvzOO2O99aAdR8cF19oZalnCtvREUw79tCd5sow1g1UKM6kXqUx4T8wsi3sTjJ3yzDmmhenLXLpo8u45eG5y4Vvbk6kkC4LLtJMowkSQxmk4ggVJEG+7c6QpHT8vvW9X7/o7+3ELmiJi2mEzZJiz8cT6TBlanBk70cB5GGIGC1gRDdZ00yADLW1FL6gqhtvNXNG5S9gdSrk4M1qu7JAsmYshzDS4peoMrU/gT7qQdqYGZaYhxZmVbGJAm/CS/HloWyhRUlknQ9KYcExTwS80d3VNOxUZJpITYyspl0LbhArhpZCD9cRWEQuhYkNGMHToQ/2Cs6swJlb39CsllxdXX6IUKh/H5jbnSsPKjgmoaFQ1f8wRLR0UnGE/RcDEjj2jXG1WVTwUs8+zxfcrVO+vSsuOpVKxCfYZiQ0/aPKuxQbQ8lIz+DClxC8u+snlcJ7Yr1z1JPqUH0V+GDXbOwAib931Y4Imaq0NTIXPXY+N5L18GJ37SVWu+hwXff8l72Ds9XuwYIBaXPq6Shm4l+Vl/5QiOlV+uTk6YR9PxKsI9xNJny31ygK1e+nIRC1N97EGkFPI+jCpiHe5PCEy7oWqWSwRrpOvhFzcbTWMbm3ZJAOn1rUKpYIt/lDhW/5RHHteeWFN60qo98YJuoq1nK3uW5AabyspC1BcIEpOhft+SZAShYoLSvnmSfnYADUERP5jJn2h5XtsgCRuhYQqAvwTwn33+YWEKUI72HX5AtfSAZDe8F2DtPPm77afhl0EkthzuCQU0BWApgQIH9+KB0JhopMM7bJrdTRoleM2JAVNMyPF+wdoaz+XJpGoVAQ7WXUkcV7gT3oUZyi/ISIJAVKhgNp+4b4veCFhYVJw4locdSjZCp9cPUhLF9EZ3KKzURepMEtCDPP3VcWFx4UIiZIklIpFNfHpdEafIF2aRmOcrUmjohbT2WUllbmRvgfbythbQO3222fpDJoufaQPncYYuqoGtUEsCJZL6/3PR5b4syeSjZMQG/T2maGANlXT2v8S4AULWaUkCxfLyW8iW4kdka+nEMjxpL2NCwsYNBp+Q61PF43zyDg9Bm9+3NNySn78jMZUUkumqE4Gp7JmFOdP1vc8PpRrzj9+wPinCy8K1PiJ4aYbnTYpCCbDkBSbzhu2QJ1Gd82t8jI8TH51+OzvXoWbnXUOBkNW+0mWFwGcGOUVpU81/n3TOHb5oMt2FgYGjzau0Nif0Ss7Q3XB33hjjQHjHA5E5aOyIQc8CBrLdQSs3j92VG+3nNEjbkbdbBr9zm04ruvw37vh0QKOdeGIkckc80fX3KH/h7PT4BOjgCty8VZ5ux1MoO5Cf5naca2LAsEgehI+drX8o/0Nu+W0m6K/I9gGPd/dfx/EN/wN62AhsBWuAAAAAElFTkSuQmCC\n \">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "97e50601b725769ef3a53c089410b2d76cc7dcaa",
            "filename": "docs/source/en/model_doc/pegasus_x.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -18,6 +18,7 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "d57ee8ed99e85933578dd9941d5e4230149291f6",
            "filename": "docs/source/en/model_doc/plbart.md",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/docs%2Fsource%2Fen%2Fmodel_doc%2Fplbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/docs%2Fsource%2Fen%2Fmodel_doc%2Fplbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fplbart.md?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -18,6 +18,8 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n ## Overview\n@@ -29,7 +31,7 @@ on Java, Python and English.\n According to the abstract\n \n *Code summarization and generation empower conversion between programming language (PL) and natural language (NL),\n-while code translation avails the migration of legacy code from one PL to another. This paper introduces PLBART, \n+while code translation avails the migration of legacy code from one PL to another. This paper introduces PLBART,\n a sequence-to-sequence model capable of performing a broad spectrum of program and language understanding and generation tasks.\n PLBART is pre-trained on an extensive collection of Java and Python functions and associated NL text via denoising autoencoding.\n Experiments on code summarization in the English language, code generation, and code translation in seven programming languages\n@@ -50,7 +52,7 @@ target text format is `[tgt_lang_code] X [eos]`. `bos` is never used.\n \n However, for fine-tuning, in some cases no language token is provided in cases where a single language is used. Please refer to [the paper](https://arxiv.org/abs/2103.06333) to learn more about this.\n \n-In cases where the language code is needed, the regular [`~PLBartTokenizer.__call__`] will encode source text format \n+In cases where the language code is needed, the regular [`~PLBartTokenizer.__call__`] will encode source text format\n when you pass texts as the first argument or with the keyword argument `text`, and will encode target text format if\n it's passed with the `text_target` keyword argument.\n "
        },
        {
            "sha": "4f76e65a847b82e90710dd8308515ae761b84b19",
            "filename": "src/transformers/integrations/flash_attention.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_attention.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -3,8 +3,11 @@\n import torch\n \n from ..modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n+from ..utils import logging\n \n \n+logger = logging.get_logger(__name__)\n+\n _use_top_left_mask = flash_attn_supports_top_left_mask()\n \n \n@@ -20,6 +23,12 @@ def flash_attention_forward(\n     softcap: Optional[float] = None,\n     **kwargs,\n ) -> Tuple[torch.Tensor, None]:\n+    if kwargs.get(\"output_attentions\", False) or kwargs.get(\"head_mask\", None) is not None:\n+        logger.warning_once(\n+            \"`flash_attention_2` does not support `output_attentions=True` or `head_mask`.\"\n+            \" Please set your attention to `eager` if you want any of these features.\"\n+        )\n+\n     # This is before the transpose\n     seq_len = query.shape[2]\n "
        },
        {
            "sha": "cc9787657bc7399ded3fd3089b3bb60941670a7a",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 45,
            "deletions": 14,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -31,13 +31,15 @@\n import torch\n from packaging import version\n \n-from ..utils import is_torch_flex_attn_available\n+from ..utils import is_torch_flex_attn_available, logging\n from ..utils.import_utils import _torch_version, is_torchdynamo_compiling\n \n \n if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask, flex_attention\n-    from torch.nn.attention.flex_attention import create_block_mask as create_block_causal_mask_flex\n+    from torch.nn.attention.flex_attention import BlockMask, create_block_mask, flex_attention\n+\n+\n+logger = logging.get_logger(__name__)\n \n \n class WrappedFlexAttention:\n@@ -98,21 +100,23 @@ def compile_friendly_flex_attention(\n Offset = Union[torch.Tensor, int]\n \n \n+# TODO: deprecate / rename to make_flex_block_mask for clarity as it's not only causal anymore\n def make_flex_block_causal_mask(\n     attention_mask_2d: torch.Tensor,\n     attention_chunk_size: Optional[int] = None,\n     query_length=None,\n     key_length=None,\n     offsets: Optional[Tuple[Offset, Offset]] = None,\n+    is_causal: Optional[bool] = True,\n ) -> \"BlockMask\":\n     \"\"\"\n     IMPORTANT NOTICE: This function is deprecated in favor of using the mask primitives in `masking_utils.py`,\n     and will be removed in a future version without warnings. New code should not use it. It is only kept here\n     for BC for now, while models using it are being patched accordingly.\n \n-    Create a block causal document mask for a batch of sequences, both packed and unpacked.\n-    Create Block causal logic and passing it into :func:`torch.nn.attention.flex_attention.create_block_mask`.\n-    The resultant BlockMask is a compressed representation of the full block causal\n+    Create a block (causal) document mask for a batch of sequences, both packed and unpacked.\n+    Create Block (causal) logic and passing it into :func:`torch.nn.attention.flex_attention.create_block_mask`.\n+    The resultant BlockMask is a compressed representation of the full (causal) block\n     mask. BlockMask is essential for performant computation of flex attention.\n     See: https://pytorch.org/blog/flexattention/\n \n@@ -170,7 +174,21 @@ def chunk_causal_mask_mod(batch_idx, head_idx, q_idx, kv_idx):\n         causal_doc_mask = causal_mask_mod(batch_idx, head_idx, q_idx, kv_idx)\n         return chunk_mask & causal_doc_mask\n \n-    mask_mod_maybe_combined = causal_mask_mod if attention_chunk_size is None else chunk_causal_mask_mod\n+    def default_mask_mod(batch_idx, head_idx, q_idx, kv_idx):\n+        \"\"\"\n+        Utilizes default attention mask to enable encoder and encoder-decoder\n+        attention masks.\n+        \"\"\"\n+        document_mask = document_ids[batch_idx, q_idx] == document_ids[batch_idx, kv_idx]\n+        # kv indexing is crucial in order to work correctly\n+        padding_mask = attention_mask_2d[batch_idx, kv_idx] > 0\n+        final_mask = padding_mask & document_mask\n+        return final_mask\n+\n+    if not is_causal:\n+        mask_mod_maybe_combined = default_mask_mod\n+    else:\n+        mask_mod_maybe_combined = causal_mask_mod if attention_chunk_size is None else chunk_causal_mask_mod\n \n     if offsets is not None:\n         q_offset = offsets[0]\n@@ -182,7 +200,8 @@ def mask_mod(batch_idx, head_idx, q_idx, kv_idx):\n             return mask_mod_maybe_combined(batch_idx, head_idx, offset_q, offset_kv)\n     else:\n         mask_mod = mask_mod_maybe_combined\n-    return create_block_causal_mask_flex(\n+\n+    return create_block_mask(\n         mask_mod=mask_mod,\n         B=batch_size,\n         H=None,  # attention head\n@@ -216,21 +235,33 @@ def flex_attention_forward(\n     head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n+    if kwargs.get(\"output_attentions\", False) or head_mask is not None:\n+        logger.warning_once(\n+            \"`flex_attention` does not support `output_attentions=True` or `head_mask`.\"\n+            \" Please set your attention to `eager` if you want any of these features.\"\n+        )\n+\n+    if kwargs.get(\"dropout\", 0.0) > 0:\n+        raise ValueError(\n+            \"`flex_attention` does not support `dropout`. Please use it with inference\"\n+            \" only (`model.eval()`) or turn off the attention dropout in the respective config.\"\n+        )\n+\n     block_mask = None\n-    causal_mask = None\n+    score_mask = None\n     if isinstance(attention_mask, BlockMask):\n         block_mask = attention_mask\n     else:\n-        causal_mask = attention_mask\n+        score_mask = attention_mask\n \n-    if causal_mask is not None:\n-        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n+    if score_mask is not None:\n+        score_mask = score_mask[:, :, :, : key.shape[-2]]\n \n     def score_mod(score, batch_idx, head_idx, q_idx, kv_idx):\n         if softcap is not None:\n             score = softcap * torch.tanh(score / softcap)\n-        if causal_mask is not None:\n-            score = score + causal_mask[batch_idx][0][q_idx][kv_idx]\n+        if score_mask is not None:\n+            score = score + score_mask[batch_idx][0][q_idx][kv_idx]\n         if head_mask is not None:\n             score = score + head_mask[batch_idx][head_idx][0][0]\n         return score"
        },
        {
            "sha": "247cd2821679b720d7b97cca75574a61149bc47a",
            "filename": "src/transformers/integrations/sdpa_attention.py",
            "status": "modified",
            "additions": 17,
            "deletions": 5,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -2,6 +2,11 @@\n \n import torch\n \n+from ..utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n \n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n@@ -26,13 +31,18 @@ def sdpa_attention_forward(\n     is_causal: Optional[bool] = None,\n     **kwargs,\n ) -> Tuple[torch.Tensor, None]:\n+    if kwargs.get(\"output_attentions\", False) or kwargs.get(\"head_mask\", None) is not None:\n+        logger.warning_once(\n+            \"`sdpa` attention does not support `output_attentions=True` or `head_mask`.\"\n+            \" Please set your attention to `eager` if you want any of these features.\"\n+        )\n+\n     if hasattr(module, \"num_key_value_groups\"):\n         key = repeat_kv(key, module.num_key_value_groups)\n         value = repeat_kv(value, module.num_key_value_groups)\n \n-    causal_mask = attention_mask\n-    if attention_mask is not None and causal_mask.ndim == 4:\n-        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n \n     # SDPA with memory-efficient backend is bugged with non-contiguous inputs and custom attn_mask for some torch versions\n     # Reference: https://github.com/pytorch/pytorch/issues/112577.\n@@ -44,7 +54,9 @@ def sdpa_attention_forward(\n     # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n     # Note that it is important to check first for the shape, otherwise compile will fail with `argument 'is_causal' must be bool, not SymBool`\n     if is_causal is None:\n-        is_causal = query.shape[2] > 1 and causal_mask is None\n+        # The last condition is for encoder (decoder) models which specify this by passing their own `is_causal` flag\n+        # This is mainly due to those models having mixed implementations for encoder, decoder, and encoder-decoder attns\n+        is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, \"is_causal\", True)\n \n     # Shapes (e.g. query.shape[2]) are tensors during jit tracing, resulting in `is_causal` being a tensor.\n     # We convert it to a bool for the SDPA kernel that only accepts bools.\n@@ -55,7 +67,7 @@ def sdpa_attention_forward(\n         query,\n         key,\n         value,\n-        attn_mask=causal_mask,\n+        attn_mask=attention_mask,\n         dropout_p=dropout,\n         scale=scaling,\n         is_causal=is_causal,"
        },
        {
            "sha": "0ad3947815b2b2bc6382b38c82aa1dad91698cce",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -165,6 +165,7 @@\n if is_kernels_available():\n     from kernels import get_kernel\n \n+\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "0a41692f69cd1b656721dbc08c4abda91bc046cf",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 36,
            "deletions": 6,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -26,14 +26,21 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n+)\n from ...modeling_outputs import BaseModelOutput, ModelOutput, SampleTSPredictionOutput, Seq2SeqTSPredictionOutput\n from ...modeling_utils import PreTrainedModel\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n-from ...utils import auto_docstring, logging\n+from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n from .configuration_autoformer import AutoformerConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -904,6 +911,29 @@ def _init_weights(self, module):\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n \n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n \n # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesTransformerEncoder with TimeSeriesTransformer->Autoformer,TimeSeries->Autoformer\n class AutoformerEncoder(AutoformerPreTrainedModel):\n@@ -983,10 +1013,10 @@ def forward(\n         hidden_states = self.layernorm_embedding(hidden_states + embed_pos)\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        # expand attention_mask\n-        if attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            inputs_embeds,\n+        )\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None"
        },
        {
            "sha": "60d9cdba2aee420a2bee4aaf3a18a3fbdc793059",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 164,
            "deletions": 353,
            "changes": 517,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -17,7 +17,7 @@\n import copy\n import math\n import warnings\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -32,7 +32,7 @@\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n )\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -42,7 +42,8 @@\n     Seq2SeqQuestionAnsweringModelOutput,\n     Seq2SeqSequenceClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n     auto_docstring,\n     is_torch_flex_attn_available,\n@@ -53,13 +54,7 @@\n \n \n if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n+    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n \n \n logger = logging.get_logger(__name__)\n@@ -119,6 +114,36 @@ def forward(self, input_ids: torch.Tensor):\n         return super().forward(input_ids) * self.embed_scale\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class BartAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -170,151 +195,25 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        query_states = query_states * self.scaling\n-\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_value\n-\n-        current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n-        else:\n-            key_states = self.k_proj(current_states)\n-            value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-            if past_key_value is not None:\n-                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n-                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = query_states.reshape(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n \n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n \n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped, past_key_value\n-\n-\n-class BartFlashAttention2(BartAttention):\n-    \"\"\"\n-    Bart flash attention module. This module inherits from `BartAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # BartFlashAttention2 attention does not support output_attentions\n-        if output_attentions:\n-            raise ValueError(\n-                \"BartSdpaAttention2 attention does not support `output_attentions`. \"\n-                \"Use the argument `attn_implementation='eager'` when loading the model.\"\n-            )\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-        bsz, q_len, _ = hidden_states.size()\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim)\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n@@ -335,8 +234,8 @@ def forward(\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n+            value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n             if past_key_value is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n@@ -348,171 +247,35 @@ def forward(\n                 if is_cross_attention:\n                     past_key_value.is_updated[self.layer_idx] = True\n \n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (LlamaRMSNorm handles it correctly)\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            dropout=self.dropout if self.training else 0.0,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-class BartSdpaAttention(BartAttention):\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"BartModel is using BartSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n-                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                key_value_states=key_value_states,\n-                past_key_value=past_key_value,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n-            )\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_value\n-\n-        current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n-        else:\n-            key_states = self.k_proj(current_states)\n-            value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-            if past_key_value is not None:\n-                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n-                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n-\n-        causal_mask = None\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and causal_mask is None and tgt_len > 1 else False\n-\n-        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n-        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.view(bsz, tgt_len, self.embed_dim)\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n-\n-\n-BART_ATTENTION_CLASSES = {\n-    \"eager\": BartAttention,\n-    \"sdpa\": BartSdpaAttention,\n-    \"flash_attention_2\": BartFlashAttention2,\n-}\n+        return attn_output, attn_weights, past_key_value\n \n \n class BartEncoderLayer(nn.Module):\n     def __init__(self, config: BartConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = BART_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = BartAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -583,7 +346,7 @@ def __init__(self, config: BartConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = BART_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = BartAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -597,7 +360,7 @@ def __init__(self, config: BartConfig, layer_idx: Optional[int] = None):\n         self.activation_dropout = config.activation_dropout\n \n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n-        self.encoder_attn = BART_ATTENTION_CLASSES[config._attn_implementation](\n+        self.encoder_attn = BartAttention(\n             self.embed_dim,\n             config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -671,6 +434,7 @@ def forward(\n                 layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n@@ -730,6 +494,8 @@ class BartPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n     _supports_cache_class = True\n     _supports_static_cache = True\n \n@@ -757,32 +523,62 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n     def _update_causal_mask(\n         self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool = False,\n     ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n+                        device=attention_mask.device,\n+                    )\n+                )\n             return attention_mask\n \n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+\n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -816,7 +612,6 @@ def _update_causal_mask(\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n             and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n@@ -882,6 +677,41 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n         return causal_mask\n \n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n class PretrainedBartModel(BartPreTrainedModel):\n     def __init_subclass__(self):\n@@ -932,8 +762,6 @@ def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = No\n             embed_dim,\n         )\n         self.layers = nn.ModuleList([BartEncoderLayer(config, layer_idx=i) for i in range(config.encoder_layers)])\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n-        self._use_sdpa = config._attn_implementation == \"sdpa\"\n         self.layernorm_embedding = nn.LayerNorm(embed_dim)\n \n         self.gradient_checkpointing = False\n@@ -1019,18 +847,10 @@ def forward(\n         hidden_states = self.layernorm_embedding(hidden_states)\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        # expand attention_mask\n-        if attention_mask is not None:\n-            if self._use_flash_attention_2:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self._use_sdpa and head_mask is None and not output_attentions:\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            inputs_embeds,\n+        )\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -1116,8 +936,6 @@ def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = No\n             config.d_model,\n         )\n         self.layers = nn.ModuleList([BartDecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)])\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n-        self._use_sdpa = config._attn_implementation == \"sdpa\"\n \n         self.layernorm_embedding = nn.LayerNorm(config.d_model)\n \n@@ -1232,12 +1050,18 @@ def forward(\n         # retrieve input_ids and inputs_embeds\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-\n-        if input_ids is not None:\n-            input_ids = input_ids.view(-1, input_ids.shape[-1])\n+        elif input_ids is not None:\n+            input = input_ids\n+            input_shape = input.shape\n+            input_ids = input_ids.view(-1, input_shape[-1])\n+        elif inputs_embeds is not None:\n+            input_shape = inputs_embeds.size()[:-1]\n+            input = inputs_embeds[:, :, -1]\n+        else:\n+            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n+            inputs_embeds = self.embed_tokens(input)\n \n         # initialize `past_key_values`\n         return_legacy_cache = False\n@@ -1267,38 +1091,25 @@ def forward(\n             if isinstance(past_key_values, EncoderDecoderCache)\n             else past_key_values\n         )\n-        causal_mask = self._update_causal_mask(\n+\n+        attention_mask = self._update_causal_mask(\n             attention_mask,\n             inputs_embeds,\n             cache_position,\n             self_attn_cache,\n-            output_attentions,\n         )\n-\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self._use_flash_attention_2:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self._use_sdpa and cross_attn_head_mask is None and not output_attentions:\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=seq_length,\n-                )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_length\n-                )\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            input_shape,\n+            inputs_embeds,\n+        )\n \n         # embed positions\n-        position_ids = self.embed_positions(input, past_key_values_length, position_ids=cache_position)\n-        position_ids = position_ids.to(inputs_embeds.device)\n+        positions = self.embed_positions(input, past_key_values_length, position_ids=cache_position)\n+        positions = positions.to(inputs_embeds.device)\n \n-        hidden_states = inputs_embeds + position_ids\n+        hidden_states = inputs_embeds + positions\n         hidden_states = self.layernorm_embedding(hidden_states)\n \n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -1331,7 +1142,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    causal_mask,\n+                    attention_mask,\n                     encoder_hidden_states,\n                     encoder_attention_mask,\n                     head_mask[idx] if head_mask is not None else None,\n@@ -1344,7 +1155,7 @@ def forward(\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=causal_mask,\n+                    attention_mask=attention_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),"
        },
        {
            "sha": "d49d4e65bd70b70e65411c548c6df20112656e86",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 146,
            "deletions": 103,
            "changes": 249,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -16,7 +16,7 @@\n \n import copy\n import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -29,7 +29,9 @@\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n     _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n )\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -39,20 +41,14 @@\n     Seq2SeqQuestionAnsweringModelOutput,\n     Seq2SeqSequenceClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    auto_docstring,\n-    is_torch_flex_attn_available,\n-    is_torchdynamo_compiling,\n-    logging,\n-)\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n from .configuration_bigbird_pegasus import BigBirdPegasusConfig\n \n \n if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n+    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n \n \n logger = logging.get_logger(__name__)\n@@ -1179,6 +1175,37 @@ def forward(\n         return outputs\n \n \n+# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.bart.modeling_bart.BartAttention with BartConfig->BigBirdPegasusConfig, Bart->BigBirdPegasusDecoder\n class BigBirdPegasusDecoderAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n@@ -1231,17 +1258,25 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-        bsz, tgt_len, _ = hidden_states.size()\n+\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n+\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        query_states = query_states * self.scaling\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n@@ -1262,8 +1297,8 @@ def forward(\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n+            value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n             if past_key_value is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n@@ -1275,66 +1310,27 @@ def forward(\n                 if is_cross_attention:\n                     past_key_value.is_updated[self.layer_idx] = True\n \n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = query_states.reshape(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n+        )\n \n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights, past_key_value\n \n \n class BigBirdPegasusEncoderLayer(nn.Module):\n@@ -1434,6 +1430,7 @@ def __init__(self, config: BigBirdPegasusConfig, layer_idx: Optional[int] = None\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             bias=config.use_bias,\n+            config=config,\n             layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n@@ -1447,6 +1444,7 @@ def __init__(self, config: BigBirdPegasusConfig, layer_idx: Optional[int] = None\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             bias=config.use_bias,\n+            config=config,\n             layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n@@ -1510,7 +1508,6 @@ def forward(\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n             hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n@@ -1602,32 +1599,41 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool = False,\n     ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n+                        device=attention_mask.device,\n+                    )\n+                )\n             return attention_mask\n \n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+\n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1661,7 +1667,6 @@ def _update_causal_mask(\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n             and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n@@ -1727,6 +1732,42 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n         return causal_mask\n \n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n class BigBirdPegasusEncoder(BigBirdPegasusPreTrainedModel):\n     \"\"\"\n@@ -2172,9 +2213,13 @@ def forward(\n         # retrieve input_ids and inputs_embeds\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-\n-        if input_ids is not None:\n-            input_ids = input_ids.view(-1, input_ids.shape[-1])\n+        elif input_ids is not None:\n+            input_shape = input_ids.size()\n+            input_ids = input_ids.view(-1, input_shape[-1])\n+        elif inputs_embeds is not None:\n+            input_shape = inputs_embeds.size()[:-1]\n+        else:\n+            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n@@ -2207,28 +2252,26 @@ def forward(\n             if isinstance(past_key_values, EncoderDecoderCache)\n             else past_key_values\n         )\n-        causal_mask = self._update_causal_mask(\n+\n+        attention_mask = self._update_causal_mask(\n             attention_mask,\n             inputs_embeds,\n             cache_position,\n             self_attn_cache,\n-            output_attentions,\n         )\n-\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            encoder_attention_mask = _prepare_4d_attention_mask(\n-                encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_length\n-            )\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            input_shape,\n+            inputs_embeds,\n+        )\n \n         # embed positions\n-        position_ids = cache_position.unsqueeze(0)\n-        position_ids = self.embed_positions(\n-            (batch_size, seq_length), past_key_values_length, position_ids=position_ids\n-        )\n-        position_ids = position_ids.to(inputs_embeds.device)\n-        hidden_states = inputs_embeds + position_ids\n+        positions = self.embed_positions(input, past_key_values_length, position_ids=cache_position)\n+        positions = positions.to(inputs_embeds.device)\n+\n+        hidden_states = inputs_embeds + positions\n+\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n         # decoder layers\n@@ -2258,7 +2301,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    causal_mask,\n+                    attention_mask,\n                     encoder_hidden_states,\n                     encoder_attention_mask,\n                     head_mask[idx] if head_mask is not None else None,\n@@ -2271,7 +2314,7 @@ def forward(\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=causal_mask,\n+                    attention_mask=attention_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n@@ -2979,7 +3022,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):"
        },
        {
            "sha": "0b2a0dc2749fc2c11261a7c4b3996946a1cc0083",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 150,
            "deletions": 225,
            "changes": 375,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -1,3 +1,9 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/biogpt/modular_biogpt.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_biogpt.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n # coding=utf-8\n # Copyright 2022 The HuggingFace Team and Microsoft Research AI4Science All rights reserved.\n #\n@@ -12,56 +18,46 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch BioGPT model.\"\"\"\n \n import math\n-from typing import Optional, Tuple, Union\n+from functools import partial\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n-import torch.utils.checkpoint\n-from torch import nn\n+import torch.nn as nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import (\n-    AttentionMaskConverter,\n-)\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    auto_docstring,\n-    is_torch_flex_attn_available,\n-    is_torchdynamo_compiling,\n-    logging,\n-)\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import LossKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from .configuration_biogpt import BioGptConfig\n \n \n if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n+    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n \n \n logger = logging.get_logger(__name__)\n \n \n-# copied from transformers.models.opt.modeling_opt.OPTLearnedPositionalEmbedding with OPT->BioGpt\n-# TODO @ArthurZucker bring copied from back\n class BioGptLearnedPositionalEmbedding(nn.Embedding):\n     \"\"\"\n     This module learns positional embeddings up to a fixed maximum size.\n     \"\"\"\n \n     def __init__(self, num_embeddings: int, embedding_dim: int):\n-        # BioGpt is set up so that if padding_idx is specified then offset the embedding ids by 2\n+        # BIOGPT is set up so that if padding_idx is specified then offset the embedding ids by 2\n         # and adjust num_embeddings appropriately. Other models don't have this hack\n         self.offset = 2\n         super().__init__(num_embeddings + self.offset, embedding_dim)\n@@ -70,22 +66,19 @@ def forward(\n         self,\n         attention_mask: torch.LongTensor,\n         past_key_values_length: int = 0,\n-        position_ids: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n     ):\n         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n-        if position_ids is None:\n-            attention_mask = attention_mask.long()\n-\n-            # create positions depending on attention_mask\n-            positions = (torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask).long() - 1\n \n+        if position_ids is None:\n+            position_ids = torch.cumsum(attention_mask, dim=1)\n+            position_ids = (position_ids * attention_mask - 1).long()\n             # cut positions if `past_key_values_length` is > 0\n-            position_ids = positions[:, past_key_values_length:]\n+            position_ids = position_ids[:, past_key_values_length:]\n \n         return super().forward(position_ids + self.offset)\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartScaledWordEmbedding with Bart->BioGpt\n class BioGptScaledWordEmbedding(nn.Embedding):\n     \"\"\"\n     This module overrides nn.Embeddings' forward by multiplying with embeddings scale.\n@@ -99,7 +92,36 @@ def forward(self, input_ids: torch.Tensor):\n         return super().forward(input_ids) * self.embed_scale\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->BioGpt\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class BioGptAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -151,148 +173,25 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        query_states = query_states * self.scaling\n-\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_value\n-\n-        current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n-        else:\n-            key_states = self.k_proj(current_states)\n-            value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-            if past_key_value is not None:\n-                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n-                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = query_states.reshape(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped, past_key_value\n \n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n \n-# Copied from transformers.models.bart.modeling_bart.BartSdpaAttention with Bart->BioGpt\n-class BioGptSdpaAttention(BioGptAttention):\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"BioGptModel is using BioGptSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n-                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                key_value_states=key_value_states,\n-                past_key_value=past_key_value,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n-            )\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n-        bsz, tgt_len, _ = hidden_states.size()\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n@@ -313,8 +212,8 @@ def forward(\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n+            value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n             if past_key_value is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n@@ -326,60 +225,41 @@ def forward(\n                 if is_cross_attention:\n                     past_key_value.is_updated[self.layer_idx] = True\n \n-        causal_mask = None\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and causal_mask is None and tgt_len > 1 else False\n-\n-        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n-        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.view(bsz, tgt_len, self.embed_dim)\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n-\n-\n-BIOGPT_ATTENTION_CLASSES = {\n-    \"eager\": BioGptAttention,\n-    \"sdpa\": BioGptSdpaAttention,\n-}\n+        return attn_output, attn_weights, past_key_value\n \n \n class BioGptDecoderLayer(nn.Module):\n     def __init__(self, config: BioGptConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n \n-        self.self_attn = BIOGPT_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = BioGptAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.num_attention_heads,\n             dropout=config.attention_probs_dropout_prob,\n             is_decoder=True,\n             is_causal=True,\n+            config=config,\n             layer_idx=layer_idx,\n         )\n         self.dropout = config.hidden_dropout_prob\n@@ -400,7 +280,9 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        position_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -431,7 +313,9 @@ def forward(\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            position_ids=position_ids,\n             cache_position=cache_position,\n+            **flash_attn_kwargs,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n@@ -462,7 +346,10 @@ class BioGptPreTrainedModel(PreTrainedModel):\n     config_class = BioGptConfig\n     base_model_prefix = \"biogpt\"\n     supports_gradient_checkpointing = True\n+    _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n     _supports_cache_class = True\n     _supports_static_cache = True\n \n@@ -482,32 +369,41 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool = False,\n     ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n+                        device=attention_mask.device,\n+                    )\n+                )\n             return attention_mask\n \n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+\n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -541,7 +437,6 @@ def _update_causal_mask(\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n             and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n@@ -628,7 +523,6 @@ def __init__(self, config: BioGptConfig):\n         self.layer_norm = nn.LayerNorm(self.embed_dim)\n \n         self.gradient_checkpointing = False\n-        self._use_sdpa = config._attn_implementation == \"sdpa\"\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -652,7 +546,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n-        **kwargs,  # NOOP kwargs, for now\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -663,18 +557,24 @@ def forward(\n \n         # retrieve input_ids and inputs_embeds\n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-\n-        if input_ids is not None:\n-            input_ids = input_ids.view(-1, input_ids.shape[-1])\n+            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n+        elif input_ids is not None:\n+            input = input_ids\n+            input_shape = input.shape\n+            input_ids = input_ids.view(-1, input_shape[-1])\n+        elif inputs_embeds is not None:\n+            input_shape = inputs_embeds.size()[:-1]\n+            input = inputs_embeds[:, :, -1]\n+        else:\n+            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n+            inputs_embeds = self.embed_tokens(input)\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:\n                 logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                    \"`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...\"\n                 )\n                 use_cache = False\n \n@@ -696,7 +596,7 @@ def forward(\n                 past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n             )\n \n-        if attention_mask is None and not is_torchdynamo_compiling():\n+        if attention_mask is None:\n             # required mask seq length can be calculated via length of past cache\n             mask_seq_length = past_key_values_length + seq_length\n             attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n@@ -706,27 +606,37 @@ def forward(\n             if isinstance(past_key_values, EncoderDecoderCache)\n             else past_key_values\n         )\n+\n         causal_mask = self._update_causal_mask(\n             attention_mask,\n             inputs_embeds,\n             cache_position,\n             self_attn_cache,\n-            output_attentions,\n         )\n \n         # embed positions\n         if position_ids is None:\n-            position_ids = cache_position.unsqueeze(0)\n-\n-        position_ids = self.embed_positions(attention_mask, past_key_values_length, position_ids=position_ids)\n-\n-        hidden_states = inputs_embeds + position_ids\n+            # position_ids = cache_position.unsqueeze(0)\n+            position_ids = torch.cumsum(attention_mask, dim=1)\n+            position_ids = (position_ids * attention_mask - 1).long()\n+            # cut positions if `past_seen_tokens` is > 0\n+            position_ids = position_ids[:, past_key_values_length:]\n+\n+        positions = self.embed_positions(attention_mask, past_key_values_length, position_ids=position_ids)\n+        hidden_states = inputs_embeds + positions\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = None\n-        next_decoder_cache = None\n+        next_decoder_cache = () if use_cache else None\n \n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n@@ -739,13 +649,14 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     head_mask[idx] if head_mask is not None else None,\n                     None,\n                     output_attentions,\n                     use_cache,\n+                    position_ids,\n                     cache_position,\n                 )\n             else:\n@@ -756,7 +667,9 @@ def forward(\n                     past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n+                    position_ids=position_ids,\n                     cache_position=cache_position,\n+                    **flash_attn_kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n@@ -792,6 +705,9 @@ def forward(\n         )\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n @auto_docstring(\n     custom_intro=\"\"\"\n     BioGPT Model with a `language modeling` head on top for CLM fine-tuning.\n@@ -830,7 +746,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n-        **kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -852,6 +768,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -916,9 +833,11 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -935,9 +854,11 @@ def forward(\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n+            position_ids=position_ids,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         hidden_states = transformer_outputs[0]\n@@ -1004,9 +925,11 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1023,9 +946,11 @@ def forward(\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n+            position_ids=position_ids,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         hidden_states = transformer_outputs[0]\n         logits = self.score(hidden_states)"
        },
        {
            "sha": "4bd675be92166dfd3fabdf5df0dc5b633ff5da56",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "added",
            "additions": 851,
            "deletions": 0,
            "changes": 851,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -0,0 +1,851 @@\n+# coding=utf-8\n+# Copyright 2022 The HuggingFace Team and Microsoft Research AI4Science All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch BioGPT model.\"\"\"\n+\n+import math\n+from functools import partial\n+from typing import Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.utils.checkpoint\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n+from ...generation import GenerationMixin\n+from ...modeling_attn_mask_utils import (\n+    AttentionMaskConverter,\n+)\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPastAndCrossAttentions,\n+    CausalLMOutputWithCrossAttentions,\n+    SequenceClassifierOutputWithPast,\n+    TokenClassifierOutput,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    LossKwargs,\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+    logger,\n+)\n+from ..bart.modeling_bart import (\n+    BartAttention,\n+    BartDecoderLayer,\n+    BartScaledWordEmbedding,\n+)\n+from ..opt.modeling_opt import OPTLearnedPositionalEmbedding\n+from .configuration_biogpt import BioGptConfig\n+\n+\n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n+\n+\n+class BioGptLearnedPositionalEmbedding(OPTLearnedPositionalEmbedding):\n+    def forward(\n+        self,\n+        attention_mask: torch.LongTensor,\n+        past_key_values_length: int = 0,\n+        position_ids: Optional[torch.LongTensor] = None,\n+    ):\n+        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n+        super().forward(attention_mask, past_key_values_length, position_ids)\n+\n+\n+class BioGptScaledWordEmbedding(BartScaledWordEmbedding):\n+    pass\n+\n+\n+class BioGptAttention(BartAttention):\n+    pass\n+\n+\n+class BioGptDecoderLayer(BartDecoderLayer):\n+    def __init__(self, config: BioGptConfig, layer_idx: Optional[int] = None):\n+        super().__init__(config)\n+        self.embed_dim = config.hidden_size\n+\n+        self.self_attn = BioGptAttention(\n+            embed_dim=self.embed_dim,\n+            num_heads=config.num_attention_heads,\n+            dropout=config.attention_probs_dropout_prob,\n+            is_decoder=True,\n+            is_causal=True,\n+            config=config,\n+            layer_idx=layer_idx,\n+        )\n+        self.dropout = config.hidden_dropout_prob\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+\n+        self.fc1 = nn.Linear(self.embed_dim, config.intermediate_size)\n+        self.fc2 = nn.Linear(config.intermediate_size, self.embed_dim)\n+\n+        del self.encoder_attn\n+        del self.encoder_attn_layer_norm\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        layer_head_mask: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = True,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`): attention mask of size\n+                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n+            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n+                `(encoder_attention_heads,)`.\n+            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.self_attn_layer_norm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n+            hidden_states=hidden_states,\n+            past_key_value=past_key_value,\n+            attention_mask=attention_mask,\n+            layer_head_mask=layer_head_mask,\n+            output_attentions=output_attentions,\n+            position_ids=position_ids,\n+            cache_position=cache_position,\n+            **flash_attn_kwargs,\n+        )\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.final_layer_norm(hidden_states)\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n+        hidden_states = self.fc2(hidden_states)\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        if use_cache:\n+            outputs += (past_key_value,)\n+\n+        return outputs\n+\n+\n+@auto_docstring\n+class BioGptPreTrainedModel(PreTrainedModel):\n+    config_class = BioGptConfig\n+    base_model_prefix = \"biogpt\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            # Slightly different from the TF version which uses truncated_normal for initialization\n+            # cf https://github.com/pytorch/pytorch/pull/5617\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+    ):\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n+                        device=attention_mask.device,\n+                    )\n+                )\n+            return attention_mask\n+\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype = input_tensor.dtype\n+        sequence_length = input_tensor.shape[1]\n+        if using_compilable_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n+\n+@auto_docstring\n+class BioGptModel(BioGptPreTrainedModel):\n+    def __init__(self, config: BioGptConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.layerdrop = config.layerdrop\n+        self.dropout = config.hidden_dropout_prob\n+        self.embed_dim = config.hidden_size\n+        self.padding_idx = config.pad_token_id\n+        embed_scale = math.sqrt(config.hidden_size) if config.scale_embedding else 1.0\n+\n+        self.embed_tokens = BioGptScaledWordEmbedding(\n+            config.vocab_size, self.embed_dim, self.padding_idx, embed_scale=embed_scale\n+        )\n+        self.embed_positions = BioGptLearnedPositionalEmbedding(config.max_position_embeddings, self.embed_dim)\n+\n+        self.layers = nn.ModuleList([BioGptDecoderLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n+        self.layer_norm = nn.LayerNorm(self.embed_dim)\n+\n+        self.gradient_checkpointing = False\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        use_cache: Optional[bool] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # retrieve input_ids and inputs_embeds\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n+        elif input_ids is not None:\n+            input = input_ids\n+            input_shape = input.shape\n+            input_ids = input_ids.view(-1, input_shape[-1])\n+        elif inputs_embeds is not None:\n+            input_shape = inputs_embeds.size()[:-1]\n+            input = inputs_embeds[:, :, -1]\n+        else:\n+            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input)\n+\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        # initialize past_key_values\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        batch_size, seq_length = inputs_embeds.size()[:-1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n+            )\n+\n+        if attention_mask is None:\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n+\n+        self_attn_cache = (\n+            past_key_values.self_attention_cache\n+            if isinstance(past_key_values, EncoderDecoderCache)\n+            else past_key_values\n+        )\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask,\n+            inputs_embeds,\n+            cache_position,\n+            self_attn_cache,\n+        )\n+\n+        # embed positions\n+        if position_ids is None:\n+            # position_ids = cache_position.unsqueeze(0)\n+            position_ids = torch.cumsum(attention_mask, dim=1)\n+            position_ids = (position_ids * attention_mask - 1).long()\n+            # cut positions if `past_seen_tokens` is > 0\n+            position_ids = position_ids[:, past_key_values_length:]\n+\n+        positions = self.embed_positions(attention_mask, past_key_values_length, position_ids=position_ids)\n+        hidden_states = inputs_embeds + positions\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        all_cross_attentions = None\n+        next_decoder_cache = () if use_cache else None\n+\n+        for idx, decoder_layer in enumerate(self.layers):\n+            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+            if self.training:\n+                dropout_probability = torch.rand([])\n+                if dropout_probability < self.layerdrop:\n+                    continue\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n+                    hidden_states,\n+                    causal_mask,\n+                    head_mask[idx] if head_mask is not None else None,\n+                    None,\n+                    output_attentions,\n+                    use_cache,\n+                    position_ids,\n+                    cache_position,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask=causal_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    position_ids=position_ids,\n+                    cache_position=cache_position,\n+                    **flash_attn_kwargs,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if use_cache:\n+                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        hidden_states = self.layer_norm(hidden_states)\n+\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n+        if not return_dict:\n+            return tuple(\n+                v\n+                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                if v is not None\n+            )\n+        return BaseModelOutputWithPastAndCrossAttentions(\n+            last_hidden_state=hidden_states,\n+            past_key_values=next_cache,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+            cross_attentions=all_cross_attentions,\n+        )\n+\n+\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    BioGPT Model with a `language modeling` head on top for CLM fine-tuning.\n+    \"\"\"\n+)\n+class BioGptForCausalLM(BioGptPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"output_projection.weight\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+        self.biogpt = BioGptModel(config)\n+        self.output_projection = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_output_embeddings(self):\n+        return self.output_projection\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.output_projection = new_embeddings\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n+    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n+            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n+            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.biogpt(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            position_ids=position_ids,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        sequence_output = outputs[0]\n+        prediction_scores = self.output_projection(sequence_output)\n+\n+        lm_loss = None\n+        if labels is not None:\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n+\n+        if not return_dict:\n+            output = (prediction_scores,) + outputs[1:]\n+            return ((lm_loss,) + output) if lm_loss is not None else output\n+\n+        return CausalLMOutputWithCrossAttentions(\n+            loss=lm_loss,\n+            logits=prediction_scores,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            cross_attentions=outputs.cross_attentions,\n+        )\n+\n+    @staticmethod\n+    def _reorder_cache(past_key_values, beam_idx):\n+        reordered_past = ()\n+        for layer_past in past_key_values:\n+            reordered_past += (\n+                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n+            )\n+        return reordered_past\n+\n+\n+@auto_docstring\n+class BioGptForTokenClassification(BioGptPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+\n+        self.biogpt = BioGptModel(config)\n+        if hasattr(config, \"classifier_dropout\") and config.classifier_dropout is not None:\n+            classifier_dropout = config.classifier_dropout\n+        else:\n+            classifier_dropout = config.hidden_dropout_prob\n+        self.dropout = nn.Dropout(classifier_dropout)\n+        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n+\n+        self.post_init()\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+    ) -> Union[Tuple, TokenClassifierOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        transformer_outputs = self.biogpt(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            position_ids=position_ids,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+        )\n+\n+        hidden_states = transformer_outputs[0]\n+        hidden_states = self.dropout(hidden_states)\n+        logits = self.classifier(hidden_states)\n+\n+        loss = None\n+        if labels is not None:\n+            loss_fct = CrossEntropyLoss()\n+            # Only keep active parts of the loss\n+            if attention_mask is not None:\n+                active_loss = attention_mask.view(-1) == 1\n+                active_logits = logits.view(-1, self.num_labels)\n+                active_labels = torch.where(\n+                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n+                )\n+                loss = loss_fct(active_logits, active_labels)\n+            else:\n+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+\n+        if not return_dict:\n+            output = (logits,) + transformer_outputs[2:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return TokenClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=transformer_outputs.hidden_states,\n+            attentions=transformer_outputs.attentions,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The BioGpt Model transformer with a sequence classification head on top (linear layer).\n+\n+    [`BioGptForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n+    (e.g. GPT-2) do.\n+\n+    Since it does classification on the last token, it is required to know the position of the last token. If a\n+    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n+    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n+    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n+    each row of the batch).\n+    \"\"\"\n+)\n+class BioGptForSequenceClassification(BioGptPreTrainedModel):\n+    def __init__(self, config: BioGptConfig):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        self.biogpt = BioGptModel(config)\n+        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        transformer_outputs = self.biogpt(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            position_ids=position_ids,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+        )\n+        hidden_states = transformer_outputs[0]\n+        logits = self.score(hidden_states)\n+\n+        if input_ids is not None:\n+            batch_size, sequence_length = input_ids.shape[:2]\n+        else:\n+            batch_size, sequence_length = inputs_embeds.shape[:2]\n+\n+        if self.config.pad_token_id is None:\n+            sequence_length = -1\n+        else:\n+            if input_ids is not None:\n+                sequence_length = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\n+            else:\n+                sequence_length = -1\n+                logger.warning_once(\n+                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                    \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+                )\n+\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_length]\n+\n+        loss = None\n+        if labels is not None:\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(pooled_logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(pooled_logits, labels)\n+        if not return_dict:\n+            output = (pooled_logits,) + transformer_outputs[1:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return SequenceClassifierOutputWithPast(\n+            loss=loss,\n+            logits=pooled_logits,\n+            past_key_values=transformer_outputs.past_key_values,\n+            hidden_states=transformer_outputs.hidden_states,\n+            attentions=transformer_outputs.attentions,\n+        )\n+\n+    def get_input_embeddings(self):\n+        return self.biogpt.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.biogpt.embed_tokens = value\n+\n+\n+__all__ = [\n+    \"BioGptForCausalLM\",\n+    \"BioGptForTokenClassification\",\n+    \"BioGptForSequenceClassification\",\n+    \"BioGptModel\",\n+    \"BioGptPreTrainedModel\",\n+]"
        },
        {
            "sha": "da7282d388fb3b60f7bc07610a935721a2071de9",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 179,
            "deletions": 106,
            "changes": 285,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -18,7 +18,7 @@\n import math\n import os\n import warnings\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -31,15 +31,18 @@\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n     _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n )\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n     Seq2SeqLMOutput,\n     Seq2SeqModelOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n     auto_docstring,\n     is_torch_flex_attn_available,\n@@ -51,9 +54,7 @@\n \n \n if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n+    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n \n \n logger = logging.get_logger(__name__)\n@@ -110,6 +111,37 @@ def forward(self, input_ids: torch.Tensor):\n         return super().forward(input_ids) * self.embed_scale\n \n \n+# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->Blenderbot\n class BlenderbotAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n@@ -162,17 +194,25 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-        bsz, tgt_len, _ = hidden_states.size()\n+\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n+\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        query_states = query_states * self.scaling\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n@@ -193,8 +233,8 @@ def forward(\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n+            value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n             if past_key_value is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n@@ -206,69 +246,27 @@ def forward(\n                 if is_cross_attention:\n                     past_key_value.is_updated[self.layer_idx] = True\n \n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = query_states.reshape(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n+        )\n \n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n-\n-\n-BLENDERBOT_ATTENTION_CLASSES = {\"eager\": BlenderbotAttention}\n+        return attn_output, attn_weights, past_key_value\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->Blenderbot, MBART->BLENDERBOT\n@@ -277,7 +275,7 @@ def __init__(self, config: BlenderbotConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = BLENDERBOT_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = BlenderbotAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -346,7 +344,7 @@ def __init__(self, config: BlenderbotConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = BLENDERBOT_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = BlenderbotAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -360,7 +358,7 @@ def __init__(self, config: BlenderbotConfig, layer_idx: Optional[int] = None):\n         self.activation_dropout = config.activation_dropout\n \n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n-        self.encoder_attn = BLENDERBOT_ATTENTION_CLASSES[config._attn_implementation](\n+        self.encoder_attn = BlenderbotAttention(\n             self.embed_dim,\n             config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -428,7 +426,6 @@ def forward(\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n             hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n@@ -465,6 +462,10 @@ class BlenderbotPreTrainedModel(PreTrainedModel):\n     config_class = BlenderbotConfig\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n     _supports_cache_class = True\n     _supports_static_cache = True\n \n@@ -493,32 +494,64 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool = False,\n     ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n+                        device=attention_mask.device,\n+                    )\n+                )\n             return attention_mask\n \n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+\n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -552,7 +585,6 @@ def _update_causal_mask(\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n             and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n@@ -618,6 +650,42 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n         return causal_mask\n \n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n class BlenderbotEncoder(BlenderbotPreTrainedModel):\n     \"\"\"\n@@ -730,10 +798,10 @@ def forward(\n         hidden_states = inputs_embeds + embed_pos\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        # expand attention_mask\n-        if attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            inputs_embeds,\n+        )\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -927,22 +995,28 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        ## retrieve input_ids and inputs_embeds\n+        # retrieve input_ids and inputs_embeds\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-\n-        if input_ids is not None:\n-            input_ids = input_ids.view(-1, input_ids.shape[-1])\n+        elif input_ids is not None:\n+            input = input_ids\n+            input_shape = input.shape\n+            input_ids = input_ids.view(-1, input_shape[-1])\n+        elif inputs_embeds is not None:\n+            input_shape = inputs_embeds.size()[:-1]\n+            input = inputs_embeds[:, :, -1]\n+        else:\n+            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n+            inputs_embeds = self.embed_tokens(input)\n+\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n \n         # initialize `past_key_values`\n         return_legacy_cache = False\n@@ -972,20 +1046,19 @@ def forward(\n             if isinstance(past_key_values, EncoderDecoderCache)\n             else past_key_values\n         )\n+\n         causal_mask = self._update_causal_mask(\n             attention_mask,\n             inputs_embeds,\n             cache_position,\n             self_attn_cache,\n-            output_attentions,\n         )\n-\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            encoder_attention_mask = _prepare_4d_attention_mask(\n-                encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_length\n-            )\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            input_shape,\n+            inputs_embeds,\n+        )\n \n         # embed positions\n         position_ids = self.embed_positions("
        },
        {
            "sha": "2237907aa0e8a8344e28483aeb0dad3138ea5cd7",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 178,
            "deletions": 108,
            "changes": 286,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -16,7 +16,7 @@\n \n import copy\n import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -29,15 +29,18 @@\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n     _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n )\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n     Seq2SeqLMOutput,\n     Seq2SeqModelOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n     auto_docstring,\n     is_torch_flex_attn_available,\n@@ -48,9 +51,7 @@\n \n \n if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n+    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n \n \n logger = logging.get_logger(__name__)\n@@ -94,6 +95,37 @@ def forward(\n         return super().forward(position_ids)\n \n \n+# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->BlenderbotSmall\n class BlenderbotSmallAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n@@ -146,17 +178,25 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-        bsz, tgt_len, _ = hidden_states.size()\n+\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n+\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        query_states = query_states * self.scaling\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n@@ -177,8 +217,8 @@ def forward(\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n+            value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n             if past_key_value is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n@@ -190,66 +230,27 @@ def forward(\n                 if is_cross_attention:\n                     past_key_value.is_updated[self.layer_idx] = True\n \n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = query_states.reshape(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n+        )\n \n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights, past_key_value\n \n \n # Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->BlenderbotSmall, BART->BLENDERBOT_SMALL\n@@ -258,7 +259,7 @@ def __init__(self, config: BlenderbotSmallConfig, layer_idx: Optional[int] = Non\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = BLENDERBOT_SMALL_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = BlenderbotSmallAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -324,19 +325,13 @@ def forward(\n         return outputs\n \n \n-# TODO: Implement attention with SDPA for TimeSeriesTransformer.\n-BLENDERBOT_SMALL_ATTENTION_CLASSES = {\n-    \"eager\": BlenderbotSmallAttention,\n-}\n-\n-\n # Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->BlenderbotSmall, BART->BLENDERBOT_SMALL\n class BlenderbotSmallDecoderLayer(nn.Module):\n     def __init__(self, config: BlenderbotSmallConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = BLENDERBOT_SMALL_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = BlenderbotSmallAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -350,7 +345,7 @@ def __init__(self, config: BlenderbotSmallConfig, layer_idx: Optional[int] = Non\n         self.activation_dropout = config.activation_dropout\n \n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n-        self.encoder_attn = BLENDERBOT_SMALL_ATTENTION_CLASSES[config._attn_implementation](\n+        self.encoder_attn = BlenderbotSmallAttention(\n             self.embed_dim,\n             config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -424,6 +419,7 @@ def forward(\n                 layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n@@ -454,6 +450,10 @@ class BlenderbotSmallPreTrainedModel(PreTrainedModel):\n     config_class = BlenderbotSmallConfig\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n     _supports_cache_class = True\n     _supports_static_cache = True\n \n@@ -482,32 +482,64 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool = False,\n     ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n+                        device=attention_mask.device,\n+                    )\n+                )\n             return attention_mask\n \n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+\n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -541,7 +573,6 @@ def _update_causal_mask(\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n             and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n@@ -607,6 +638,42 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n         return causal_mask\n \n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n class BlenderbotSmallEncoder(BlenderbotSmallPreTrainedModel):\n     \"\"\"\n@@ -718,10 +785,10 @@ def forward(\n         hidden_states = self.layernorm_embedding(hidden_states)\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        # expand attention_mask\n-        if attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            inputs_embeds,\n+        )\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -909,24 +976,28 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # retrieve input_ids and inputs_embeds\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-\n-        if input_ids is not None:\n-            input_ids = input_ids.view(-1, input_ids.shape[-1])\n+        elif input_ids is not None:\n+            input = input_ids\n+            input_shape = input.shape\n+            input_ids = input_ids.view(-1, input_shape[-1])\n+        elif inputs_embeds is not None:\n+            input_shape = inputs_embeds.size()[:-1]\n+            input = inputs_embeds[:, :, -1]\n+        else:\n+            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n+            inputs_embeds = self.embed_tokens(input)\n \n-        inputs_embeds = inputs_embeds * self.embed_scale\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n \n         # initialize `past_key_values`\n         return_legacy_cache = False\n@@ -956,20 +1027,19 @@ def forward(\n             if isinstance(past_key_values, EncoderDecoderCache)\n             else past_key_values\n         )\n+\n         causal_mask = self._update_causal_mask(\n             attention_mask,\n             inputs_embeds,\n             cache_position,\n             self_attn_cache,\n-            output_attentions,\n         )\n-\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            encoder_attention_mask = _prepare_4d_attention_mask(\n-                encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_length\n-            )\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            input_shape,\n+            inputs_embeds,\n+        )\n \n         # embed positions\n         position_ids = self.embed_positions("
        },
        {
            "sha": "d9046ea6e8c9ab5ba970d4c5d6a7da7227358473",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 112,
            "deletions": 319,
            "changes": 431,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -4,9 +4,24 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_data2vec_audio.py file directly. One of our CI enforces this.\n #                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2022 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n import math\n import warnings\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -16,7 +31,8 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -25,16 +41,14 @@\n     Wav2Vec2BaseModelOutput,\n     XVectorOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, is_peft_available, logging\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring, is_peft_available, is_torch_flex_attn_available\n from .configuration_data2vec_audio import Data2VecAudioConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n-logger = logging.get_logger(__name__)\n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n class Data2VecAudioConvLayer(nn.Module):\n@@ -167,6 +181,36 @@ def forward(self, hidden_states):\n         return hidden_states, norm_hidden_states\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class Data2VecAudioAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -201,9 +245,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -212,274 +253,26 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n \n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-        else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped, past_key_value\n-\n-\n-class Data2VecAudioFlashAttention2(Data2VecAudioAttention):\n-    \"\"\"\n-    Data2VecAudio flash attention module. This module inherits from `Data2VecAudioAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n \n-        bsz, q_len, _ = hidden_states.size()\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self._reshape(self.q_proj(hidden_states), -1, bsz)\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0].transpose(1, 2)\n-            value_states = past_key_value[1].transpose(1, 2)\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._reshape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0].transpose(1, 2), key_states], dim=1)\n-            value_states = torch.cat([past_key_value[1].transpose(1, 2), value_states], dim=1)\n-        else:\n-            # self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states.transpose(1, 2), value_states.transpose(1, 2))\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value[0].shape[-2]\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (LlamaRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask,\n-            q_len,\n-            dropout=self.dropout if self.training else 0.0,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-        attn_output = self.out_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class Data2VecAudioSdpaAttention(Data2VecAudioAttention):\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Data2VecAudioModel is using Data2VecAudioSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n-                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                key_value_states=key_value_states,\n-                past_key_value=past_key_value,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states)\n         # get key, value proj\n         # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n         # is checking that the `sequence_length` of the `past_key_value` is the same as\n@@ -494,18 +287,18 @@ def forward(\n             value_states = past_key_value[1]\n         elif is_cross_attention:\n             # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n+            key_states = self.k_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n         elif past_key_value is not None:\n             # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n             key_states = torch.cat([past_key_value[0], key_states], dim=2)\n             value_states = torch.cat([past_key_value[1], value_states], dim=2)\n         else:\n             # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n         if self.is_decoder:\n             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n@@ -517,39 +310,27 @@ def forward(\n             # if encoder bi-directional self-attention `past_key_value` is always `None`\n             past_key_value = (key_states, value_states)\n \n-        query_states = self._shape(query_states, tgt_len, bsz)\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and attention_mask is None and tgt_len > 1 else False\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n-        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n         )\n \n-        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+        return attn_output, attn_weights, past_key_value\n \n \n class Data2VecAudioFeedForward(nn.Module):\n@@ -576,21 +357,15 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-DATA2VEC_AUDIO_ATTENTION_CLASSES = {\n-    \"eager\": Data2VecAudioAttention,\n-    \"sdpa\": Data2VecAudioSdpaAttention,\n-    \"flash_attention_2\": Data2VecAudioFlashAttention2,\n-}\n-\n-\n class Data2VecAudioEncoderLayer(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.attention = DATA2VEC_AUDIO_ATTENTION_CLASSES[config._attn_implementation](\n+        self.attention = Data2VecAudioAttention(\n             embed_dim=config.hidden_size,\n             num_heads=config.num_attention_heads,\n             dropout=config.attention_dropout,\n             is_decoder=False,\n+            config=config,\n         )\n \n         self.dropout = nn.Dropout(config.hidden_dropout)\n@@ -627,7 +402,6 @@ def __init__(self, config):\n         self.dropout = nn.Dropout(config.hidden_dropout)\n         self.layers = nn.ModuleList([Data2VecAudioEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     def forward(\n         self,\n@@ -644,16 +418,11 @@ def forward(\n             # make sure padded tokens output 0\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n             hidden_states[~expand_attention_mask] = 0\n-            if self._use_flash_attention_2:\n-                # 2d mask is passed through the layers\n-                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-            else:\n-                # extend attention_mask\n-                attention_mask = 1.0 - attention_mask[:, None, None, :].to(dtype=hidden_states.dtype)\n-                attention_mask = attention_mask * torch.finfo(hidden_states.dtype).min\n-                attention_mask = attention_mask.expand(\n-                    attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1]\n-                )\n+\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            hidden_states,\n+        )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n         hidden_states = hidden_states + position_embeddings\n@@ -702,6 +471,28 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n \n class Data2VecAudioAdapterLayer(nn.Module):\n     def __init__(self, config):\n@@ -760,6 +551,8 @@ class Data2VecAudioPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "73a42937bd8e194857ed6fda84e3349cec6a6b02",
            "filename": "src/transformers/models/data2vec/modular_data2vec_audio.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -1,3 +1,19 @@\n+# coding=utf-8\n+# Copyright 2022 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Data2VecText model.\"\"\"\n+\n import math\n \n import torch\n@@ -124,6 +140,8 @@ class Data2VecAudioPreTrainedModel(PreTrainedModel, Wav2Vec2PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "eb366963a674e53b1631b82a08ceda9e44c84aa4",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 143,
            "deletions": 330,
            "changes": 473,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -4,8 +4,23 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_hubert.py file directly. One of our CI enforces this.\n #                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2021 The Fairseq Authors and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n import warnings\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -15,15 +30,17 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n from .configuration_hubert import HubertConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n logger = logging.get_logger(__name__)\n@@ -224,6 +241,36 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class HubertAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -258,9 +305,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -269,274 +313,26 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n \n-        bsz, tgt_len, _ = hidden_states.size()\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n \n-        # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-        else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped, past_key_value\n-\n-\n-class HubertFlashAttention2(HubertAttention):\n-    \"\"\"\n-    Hubert flash attention module. This module inherits from `HubertAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n-        bsz, q_len, _ = hidden_states.size()\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self._reshape(self.q_proj(hidden_states), -1, bsz)\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0].transpose(1, 2)\n-            value_states = past_key_value[1].transpose(1, 2)\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._reshape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0].transpose(1, 2), key_states], dim=1)\n-            value_states = torch.cat([past_key_value[1].transpose(1, 2), value_states], dim=1)\n-        else:\n-            # self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states.transpose(1, 2), value_states.transpose(1, 2))\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value[0].shape[-2]\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (LlamaRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask,\n-            q_len,\n-            dropout=self.dropout if self.training else 0.0,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-        attn_output = self.out_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class HubertSdpaAttention(HubertAttention):\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"HubertModel is using HubertSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n-                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                key_value_states=key_value_states,\n-                past_key_value=past_key_value,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states)\n         # get key, value proj\n         # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n         # is checking that the `sequence_length` of the `past_key_value` is the same as\n@@ -551,18 +347,18 @@ def forward(\n             value_states = past_key_value[1]\n         elif is_cross_attention:\n             # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n+            key_states = self.k_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n         elif past_key_value is not None:\n             # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n             key_states = torch.cat([past_key_value[0], key_states], dim=2)\n             value_states = torch.cat([past_key_value[1], value_states], dim=2)\n         else:\n             # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n         if self.is_decoder:\n             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n@@ -574,39 +370,27 @@ def forward(\n             # if encoder bi-directional self-attention `past_key_value` is always `None`\n             past_key_value = (key_states, value_states)\n \n-        query_states = self._shape(query_states, tgt_len, bsz)\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and attention_mask is None and tgt_len > 1 else False\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n-        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n         )\n \n-        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+        return attn_output, attn_weights, past_key_value\n \n \n class HubertFeedForward(nn.Module):\n@@ -633,21 +417,15 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-HUBERT_ATTENTION_CLASSES = {\n-    \"eager\": HubertAttention,\n-    \"sdpa\": HubertSdpaAttention,\n-    \"flash_attention_2\": HubertFlashAttention2,\n-}\n-\n-\n class HubertEncoderLayer(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.attention = HUBERT_ATTENTION_CLASSES[config._attn_implementation](\n+        self.attention = HubertAttention(\n             embed_dim=config.hidden_size,\n             num_heads=config.num_attention_heads,\n             dropout=config.attention_dropout,\n             is_decoder=False,\n+            config=config,\n         )\n \n         self.dropout = nn.Dropout(config.hidden_dropout)\n@@ -684,7 +462,6 @@ def __init__(self, config):\n         self.dropout = nn.Dropout(config.hidden_dropout)\n         self.layers = nn.ModuleList([HubertEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     def forward(\n         self,\n@@ -701,16 +478,11 @@ def forward(\n             # make sure padded tokens output 0\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n             hidden_states[~expand_attention_mask] = 0\n-            if self._use_flash_attention_2:\n-                # 2d mask is passed through the layers\n-                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-            else:\n-                # extend attention_mask\n-                attention_mask = 1.0 - attention_mask[:, None, None, :].to(dtype=hidden_states.dtype)\n-                attention_mask = attention_mask * torch.finfo(hidden_states.dtype).min\n-                attention_mask = attention_mask.expand(\n-                    attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1]\n-                )\n+\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            hidden_states,\n+        )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n         hidden_states = hidden_states + position_embeddings\n@@ -759,6 +531,28 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n \n class HubertAttnAdapterLayer(nn.Module):\n     def __init__(self, config):\n@@ -788,11 +582,12 @@ def forward(self, hidden_states: torch.FloatTensor):\n class HubertEncoderLayerStableLayerNorm(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.attention = HUBERT_ATTENTION_CLASSES[config._attn_implementation](\n+        self.attention = HubertAttention(\n             embed_dim=config.hidden_size,\n             num_heads=config.num_attention_heads,\n             dropout=config.attention_dropout,\n             is_decoder=False,\n+            config=config,\n         )\n         self.dropout = nn.Dropout(config.hidden_dropout)\n         self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -841,7 +636,6 @@ def __init__(self, config):\n             [HubertEncoderLayerStableLayerNorm(config) for _ in range(config.num_hidden_layers)]\n         )\n         self.gradient_checkpointing = False\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     def forward(\n         self,\n@@ -855,19 +649,14 @@ def forward(\n         all_self_attentions = () if output_attentions else None\n \n         if attention_mask is not None:\n-            # make sure padded tokens are not attended to\n+            # make sure padded tokens output 0\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n-            hidden_states = hidden_states * expand_attention_mask.to(dtype=hidden_states.dtype)\n-            if self._use_flash_attention_2:\n-                # 2d mask is passed through the layers\n-                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-            else:\n-                # extend attention_mask\n-                attention_mask = 1.0 - attention_mask[:, None, None, :].to(dtype=hidden_states.dtype)\n-                attention_mask = attention_mask * torch.finfo(hidden_states.dtype).min\n-                attention_mask = attention_mask.expand(\n-                    attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1]\n-                )\n+            hidden_states[~expand_attention_mask] = 0\n+\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            hidden_states,\n+        )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n         hidden_states = hidden_states + position_embeddings\n@@ -918,6 +707,28 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n \n @auto_docstring\n class HubertPreTrainedModel(PreTrainedModel):\n@@ -927,6 +738,8 @@ class HubertPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "75000c95cb3822cadba1b9f7845388e83d0eaf68",
            "filename": "src/transformers/models/hubert/modular_hubert.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -1,3 +1,19 @@\n+# coding=utf-8\n+# Copyright 2021 The Fairseq Authors and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Hubert model.\"\"\"\n+\n from typing import Optional, Tuple, Union\n \n import torch\n@@ -115,6 +131,8 @@ class HubertPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "330bc620bc0304a4ebd454b95856e44dd9477c2d",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 402,
            "deletions": 267,
            "changes": 669,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -1,3 +1,9 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/informer/modular_informer.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_informer.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n # coding=utf-8\n # Copyright 2023 Amazon and The HuggingFace Inc. team. All rights reserved.\n #\n@@ -12,39 +18,43 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch Informer model.\"\"\"\n \n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import numpy as np\n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n     _prepare_4d_causal_attention_mask,\n+    _prepare_4d_causal_attention_mask_for_sdpa,\n )\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n     SampleTSPredictionOutput,\n     Seq2SeqTSModelOutput,\n     Seq2SeqTSPredictionOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n-from ...utils import (\n-    auto_docstring,\n-    logging,\n-)\n+from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n from .configuration_informer import InformerConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n-# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesFeatureEmbedder with TimeSeries->Informer\n class InformerFeatureEmbedder(nn.Module):\n     \"\"\"\n     Embed a sequence of categorical features.\n@@ -79,7 +89,6 @@ def forward(self, features: torch.Tensor) -> torch.Tensor:\n         )\n \n \n-# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesStdScaler with TimeSeriesTransformer->Informer,TimeSeries->Informer\n class InformerStdScaler(nn.Module):\n     \"\"\"\n     Standardize features by calculating the mean and scaling along the first dimension, and then normalizes it by\n@@ -115,7 +124,6 @@ def forward(\n         return (data - loc) / scale, loc, scale\n \n \n-# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesMeanScaler with TimeSeriesTransformer->Informer,TimeSeries->Informer\n class InformerMeanScaler(nn.Module):\n     \"\"\"\n     Computes a scaling factor as the weighted average absolute value along the first dimension, and scales the data\n@@ -170,7 +178,6 @@ def forward(\n         return scaled_data, torch.zeros_like(scale), scale\n \n \n-# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesNOPScaler with TimeSeriesTransformer->Informer,TimeSeries->Informer\n class InformerNOPScaler(nn.Module):\n     \"\"\"\n     Assigns a scaling factor equal to 1 along the first dimension, and therefore applies no scaling to the input data.\n@@ -198,40 +205,6 @@ def forward(\n         return data, loc, scale\n \n \n-# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.weighted_average\n-def weighted_average(input_tensor: torch.Tensor, weights: Optional[torch.Tensor] = None, dim=None) -> torch.Tensor:\n-    \"\"\"\n-    Computes the weighted average of a given tensor across a given `dim`, masking values associated with weight zero,\n-    meaning instead of `nan * 0 = nan` you will get `0 * 0 = 0`.\n-\n-    Args:\n-        input_tensor (`torch.FloatTensor`):\n-            Input tensor, of which the average must be computed.\n-        weights (`torch.FloatTensor`, *optional*):\n-            Weights tensor, of the same shape as `input_tensor`.\n-        dim (`int`, *optional*):\n-            The dim along which to average `input_tensor`.\n-\n-    Returns:\n-        `torch.FloatTensor`: The tensor with values averaged along the specified `dim`.\n-    \"\"\"\n-    if weights is not None:\n-        weighted_tensor = torch.where(weights != 0, input_tensor * weights, torch.zeros_like(input_tensor))\n-        sum_weights = torch.clamp(weights.sum(dim=dim) if dim else weights.sum(), min=1.0)\n-        return (weighted_tensor.sum(dim=dim) if dim else weighted_tensor.sum()) / sum_weights\n-    else:\n-        return input_tensor.mean(dim=dim)\n-\n-\n-# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.nll\n-def nll(input: torch.distributions.Distribution, target: torch.Tensor) -> torch.Tensor:\n-    \"\"\"\n-    Computes the negative log likelihood loss from input distribution with respect to target.\n-    \"\"\"\n-    return -input.log_prob(target)\n-\n-\n-# Copied from transformers.models.marian.modeling_marian.MarianSinusoidalPositionalEmbedding with Marian->Informer\n class InformerSinusoidalPositionalEmbedding(nn.Embedding):\n     \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n \n@@ -266,7 +239,6 @@ def forward(\n         return super().forward(position_ids)\n \n \n-# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesValueEmbedding with TimeSeries->Info\n class InformerValueEmbedding(nn.Module):\n     def __init__(self, feature_size, d_model):\n         super().__init__()\n@@ -276,7 +248,156 @@ def forward(self, x):\n         return self.value_projection(x)\n \n \n-# Copied from transformers.models.hubert.modeling_hubert.HubertAttention with Hubert->Informer\n+@auto_docstring\n+class InformerPreTrainedModel(PreTrainedModel):\n+    config_class = InformerConfig\n+    base_model_prefix = \"model\"\n+    main_input_name = \"past_values\"\n+    supports_gradient_checkpointing = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.init_std\n+        if isinstance(module, (nn.Linear, nn.Conv1d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, InformerSinusoidalPositionalEmbedding):\n+            module._init_weight()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+        past_key_values_length: int,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            # 2d mask is passed through the layers\n+            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n+        elif self.config._attn_implementation == \"sdpa\":\n+            # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+            # the manual implementation that requires a 4D causal mask in all cases.\n+            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n+                attention_mask,\n+                input_shape,\n+                inputs_embeds,\n+                past_key_values_length,\n+            )\n+        elif self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_shape),\n+                        device=inputs_embeds.device,\n+                    )\n+                )\n+        else:\n+            # 4d mask is passed through the layers\n+            attention_mask = _prepare_4d_causal_attention_mask(\n+                attention_mask, input_shape, inputs_embeds, past_key_values_length\n+            )\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class InformerAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -289,6 +410,7 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[InformerConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -305,134 +427,101 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n \n-        bsz, tgt_len, _ = hidden_states.size()\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n+\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n+            value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n+        )\n \n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights, past_key_value\n \n \n class InformerProbSparseAttention(nn.Module):\n@@ -448,6 +537,7 @@ def __init__(\n         is_decoder: bool = False,\n         sampling_factor: int = 5,\n         bias: bool = True,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.factor = sampling_factor\n@@ -463,6 +553,7 @@ def __init__(\n             )\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n+        self.layer_idx = layer_idx\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n@@ -480,6 +571,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -488,45 +580,43 @@ def forward(\n         is_cross_attention = key_value_states is not None\n \n         bsz, tgt_len, _ = hidden_states.size()\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n+            value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n         query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n@@ -681,6 +771,14 @@ class InformerEncoderLayer(nn.Module):\n     def __init__(self, config: InformerConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n+        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n+        self.dropout = config.dropout\n+        self.activation_fn = ACT2FN[config.activation_function]\n+        self.activation_dropout = config.activation_dropout\n+        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n+        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n+        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n+\n         if config.attention_type == \"prob\":\n             self.self_attn = InformerProbSparseAttention(\n                 embed_dim=self.embed_dim,\n@@ -693,14 +791,8 @@ def __init__(self, config: InformerConfig):\n                 embed_dim=self.embed_dim,\n                 num_heads=config.encoder_attention_heads,\n                 dropout=config.attention_dropout,\n+                config=config,\n             )\n-        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n-        self.dropout = config.dropout\n-        self.activation_fn = ACT2FN[config.activation_function]\n-        self.activation_dropout = config.activation_dropout\n-        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n-        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n-        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n     def forward(\n         self,\n@@ -754,9 +846,26 @@ def forward(\n \n \n class InformerDecoderLayer(nn.Module):\n-    def __init__(self, config: InformerConfig):\n+    def __init__(self, config: InformerConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n+        self.dropout = config.dropout\n+        self.activation_fn = ACT2FN[config.activation_function]\n+        self.activation_dropout = config.activation_dropout\n+\n+        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n+        self.encoder_attn = InformerAttention(\n+            self.embed_dim,\n+            config.decoder_attention_heads,\n+            dropout=config.attention_dropout,\n+            is_decoder=True,\n+            config=config,\n+            layer_idx=layer_idx,\n+        )\n+        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n+        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n+        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n+        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n         if config.attention_type == \"prob\":\n             self.self_attn = InformerProbSparseAttention(\n@@ -765,29 +874,17 @@ def __init__(self, config: InformerConfig):\n                 dropout=config.attention_dropout,\n                 sampling_factor=config.sampling_factor,\n                 is_decoder=True,\n+                layer_idx=layer_idx,\n             )\n         else:\n             self.self_attn = InformerAttention(\n                 embed_dim=self.embed_dim,\n                 num_heads=config.decoder_attention_heads,\n                 dropout=config.attention_dropout,\n                 is_decoder=True,\n+                config=config,\n+                layer_idx=layer_idx,\n             )\n-        self.dropout = config.dropout\n-        self.activation_fn = ACT2FN[config.activation_function]\n-        self.activation_dropout = config.activation_dropout\n-\n-        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n-        self.encoder_attn = InformerAttention(\n-            self.embed_dim,\n-            config.decoder_attention_heads,\n-            dropout=config.attention_dropout,\n-            is_decoder=True,\n-        )\n-        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n-        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n-        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n-        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n     def forward(\n         self,\n@@ -797,9 +894,10 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -818,47 +916,43 @@ def forward(\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         residual = hidden_states\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.activation_fn(self.fc1(hidden_states))\n@@ -874,36 +968,15 @@ def forward(\n             outputs += (self_attn_weights, cross_attn_weights)\n \n         if use_cache:\n-            outputs += (present_key_value,)\n+            outputs += (past_key_value,)\n \n         return outputs\n \n \n-@auto_docstring\n-class InformerPreTrainedModel(PreTrainedModel):\n-    config_class = InformerConfig\n-    base_model_prefix = \"model\"\n-    main_input_name = \"past_values\"\n-    supports_gradient_checkpointing = True\n-\n-    def _init_weights(self, module):\n-        std = self.config.init_std\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, InformerSinusoidalPositionalEmbedding):\n-            module._init_weight()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-\n-\n class InformerEncoder(InformerPreTrainedModel):\n     \"\"\"\n-    Informer encoder consisting of *config.encoder_layers* self attention layers with distillation layers. Each\n-    attention layer is an [`InformerEncoderLayer`].\n+    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n+    [`InformerEncoderLayer`].\n \n     Args:\n         config: InformerConfig\n@@ -914,7 +987,6 @@ def __init__(self, config: InformerConfig):\n \n         self.dropout = config.dropout\n         self.layerdrop = config.encoder_layerdrop\n-        self.gradient_checkpointing = False\n         if config.prediction_length is None:\n             raise ValueError(\"The `prediction_length` config needs to be specified.\")\n \n@@ -924,6 +996,7 @@ def __init__(self, config: InformerConfig):\n         )\n         self.layers = nn.ModuleList([InformerEncoderLayer(config) for _ in range(config.encoder_layers)])\n         self.layernorm_embedding = nn.LayerNorm(config.d_model)\n+        self.gradient_checkpointing = False\n \n         if config.distil:\n             self.conv_layers = nn.ModuleList(\n@@ -932,7 +1005,6 @@ def __init__(self, config: InformerConfig):\n             self.conv_layers.append(None)\n         else:\n             self.conv_layers = [None] * config.encoder_layers\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -1053,7 +1125,7 @@ def forward(\n \n class InformerDecoder(InformerPreTrainedModel):\n     \"\"\"\n-    Informer decoder consisting of *config.decoder_layers* layers. Each layer is a\n+    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a\n     [`InformerDecoderLayer`]\n \n     Args:\n@@ -1071,7 +1143,7 @@ def __init__(self, config: InformerConfig):\n         self.embed_positions = InformerSinusoidalPositionalEmbedding(\n             config.context_length + config.prediction_length, config.d_model\n         )\n-        self.layers = nn.ModuleList([InformerDecoderLayer(config) for _ in range(config.decoder_layers)])\n+        self.layers = nn.ModuleList([InformerDecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)])\n         self.layernorm_embedding = nn.LayerNorm(config.d_model)\n \n         self.gradient_checkpointing = False\n@@ -1091,6 +1163,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         Args:\n@@ -1148,6 +1221,9 @@ def forward(\n                 for more detail.\n             return_dict (`bool`, *optional*):\n                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1157,20 +1233,35 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         input_shape = inputs_embeds.size()[:-1]\n+        # initialize `past_key_values`\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + input_shape[1], device=inputs_embeds.device\n+            )\n \n-        attention_mask = _prepare_4d_causal_attention_mask(\n-            attention_mask, input_shape, inputs_embeds, past_key_values_length\n+        attention_mask = self._update_causal_mask(\n+            attention_mask,\n+            input_shape,\n+            inputs_embeds,\n+            past_key_values_length,\n+        )\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            input_shape,\n+            inputs_embeds,\n         )\n-\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            encoder_attention_mask = _prepare_4d_attention_mask(\n-                encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-            )\n \n         hidden_states = self.value_embedding(inputs_embeds)\n         embed_pos = self.embed_positions(inputs_embeds.size(), past_key_values_length=self.config.context_length)\n@@ -1188,7 +1279,7 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = () if use_cache else None\n+        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -1208,8 +1299,6 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n@@ -1222,6 +1311,7 @@ def forward(\n                     None,\n                     output_attentions,\n                     use_cache,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -1233,14 +1323,15 @@ def forward(\n                     cross_attn_layer_head_mask=(\n                         cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n                     ),\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n+                    cache_position=cache_position,\n                 )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:\n-                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n+                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n \n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n@@ -1253,6 +1344,9 @@ def forward(\n             all_hidden_states += (hidden_states,)\n \n         next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n@@ -1269,7 +1363,6 @@ def forward(\n \n \n @auto_docstring\n-# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesTransformerModel with TimeSeriesTransformer->Informer,TIME_SERIES_TRANSFORMER->INFORMER,time-series-transformer->informer,TimeSeries->Informer\n class InformerModel(InformerPreTrainedModel):\n     def __init__(self, config: InformerConfig):\n         super().__init__(config)\n@@ -1408,7 +1501,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.decoder\n \n-    # Ignore copy\n     @auto_docstring\n     def forward(\n         self,\n@@ -1429,6 +1521,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         use_cache: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Seq2SeqTSModelOutput, Tuple]:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`):\n@@ -1586,7 +1679,16 @@ def forward(\n                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n             )\n \n-        dec_input = transformer_inputs[:, self.config.context_length :, ...]\n+        # Avoid empty tensors and instead create a zeroes tensor which\n+        # will be treated the same in torch, i.e. matmul with empty == all 0s\n+        if self.config.context_length >= transformer_inputs.shape[1]:\n+            bsz, _, dim = transformer_inputs.shape\n+            dec_input = torch.zeros(\n+                size=(bsz, 1, dim), device=transformer_inputs.device, dtype=transformer_inputs.dtype\n+            )\n+        else:\n+            dec_input = transformer_inputs[:, self.config.context_length :, ...]\n+\n         decoder_outputs = self.decoder(\n             inputs_embeds=dec_input,\n             attention_mask=decoder_attention_mask,\n@@ -1598,6 +1700,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1618,11 +1721,42 @@ def forward(\n         )\n \n \n+def weighted_average(input_tensor: torch.Tensor, weights: Optional[torch.Tensor] = None, dim=None) -> torch.Tensor:\n+    \"\"\"\n+    Computes the weighted average of a given tensor across a given `dim`, masking values associated with weight zero,\n+    meaning instead of `nan * 0 = nan` you will get `0 * 0 = 0`.\n+\n+    Args:\n+        input_tensor (`torch.FloatTensor`):\n+            Input tensor, of which the average must be computed.\n+        weights (`torch.FloatTensor`, *optional*):\n+            Weights tensor, of the same shape as `input_tensor`.\n+        dim (`int`, *optional*):\n+            The dim along which to average `input_tensor`.\n+\n+    Returns:\n+        `torch.FloatTensor`: The tensor with values averaged along the specified `dim`.\n+    \"\"\"\n+    if weights is not None:\n+        weighted_tensor = torch.where(weights != 0, input_tensor * weights, torch.zeros_like(input_tensor))\n+        sum_weights = torch.clamp(weights.sum(dim=dim) if dim else weights.sum(), min=1.0)\n+        return (weighted_tensor.sum(dim=dim) if dim else weighted_tensor.sum()) / sum_weights\n+    else:\n+        return input_tensor.mean(dim=dim)\n+\n+\n+def nll(input: torch.distributions.Distribution, target: torch.Tensor) -> torch.Tensor:\n+    \"\"\"\n+    Computes the negative log likelihood loss from input distribution with respect to target.\n+    \"\"\"\n+    return -input.log_prob(target)\n+\n+\n @auto_docstring\n-# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesTransformerForPrediction with TimeSeriesTransformer->Informer,TIME_SERIES_TRANSFORMER->INFORMER,time-series-transformer->informer\n class InformerForPrediction(InformerPreTrainedModel):\n     def __init__(self, config: InformerConfig):\n         super().__init__(config)\n+\n         self.model = InformerModel(config)\n         if config.distribution_output == \"student_t\":\n             self.distribution_output = StudentTOutput(dim=config.input_size)\n@@ -1660,7 +1794,6 @@ def output_distribution(self, params, loc=None, scale=None, trailing_n=None) ->\n             sliced_params = [p[:, -trailing_n:] for p in params]\n         return self.distribution_output.distribution(sliced_params, loc=loc, scale=scale)\n \n-    # Ignore copy\n     @auto_docstring\n     def forward(\n         self,\n@@ -1682,6 +1815,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         use_cache: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Seq2SeqTSModelOutput, Tuple]:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`):\n@@ -1853,6 +1987,7 @@ def forward(\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         prediction_loss = None"
        },
        {
            "sha": "15bcb8d38a83b9a1483134a045f42fa044e8153d",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "added",
            "additions": 997,
            "deletions": 0,
            "changes": 997,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -0,0 +1,997 @@\n+# coding=utf-8\n+# Copyright 2023 Amazon and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Informer model.\"\"\"\n+\n+from typing import Optional, Tuple, Union\n+\n+import numpy as np\n+import torch\n+from torch import nn\n+\n+from ...cache_utils import EncoderDecoderCache\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n+    _prepare_4d_causal_attention_mask,\n+    _prepare_4d_causal_attention_mask_for_sdpa,\n+)\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n+from ...utils import (\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+)\n+from ..bart.modeling_bart import BartAttention\n+from ..time_series_transformer.modeling_time_series_transformer import (\n+    TimeSeriesFeatureEmbedder,\n+    TimeSeriesMeanScaler,\n+    TimeSeriesNOPScaler,\n+    TimeSeriesSinusoidalPositionalEmbedding,\n+    TimeSeriesStdScaler,\n+    TimeSeriesTransformerDecoder,\n+    TimeSeriesTransformerDecoderLayer,\n+    TimeSeriesTransformerEncoder,\n+    TimeSeriesTransformerEncoderLayer,\n+    TimeSeriesTransformerForPrediction,\n+    TimeSeriesTransformerModel,\n+    TimeSeriesValueEmbedding,\n+)\n+from .configuration_informer import InformerConfig\n+\n+\n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n+def nll(input: torch.distributions.Distribution, target: torch.Tensor) -> torch.Tensor:\n+    \"\"\"\n+    Computes the negative log likelihood loss from input distribution with respect to target.\n+    \"\"\"\n+    return -input.log_prob(target)\n+\n+\n+class InformerFeatureEmbedder(TimeSeriesFeatureEmbedder):\n+    pass\n+\n+\n+class InformerStdScaler(TimeSeriesStdScaler):\n+    pass\n+\n+\n+class InformerMeanScaler(TimeSeriesMeanScaler):\n+    pass\n+\n+\n+class InformerNOPScaler(TimeSeriesNOPScaler):\n+    pass\n+\n+\n+class InformerSinusoidalPositionalEmbedding(TimeSeriesSinusoidalPositionalEmbedding):\n+    pass\n+\n+\n+class InformerValueEmbedding(TimeSeriesValueEmbedding):\n+    pass\n+\n+\n+@auto_docstring\n+class InformerPreTrainedModel(PreTrainedModel):\n+    config_class = InformerConfig\n+    base_model_prefix = \"model\"\n+    main_input_name = \"past_values\"\n+    supports_gradient_checkpointing = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.init_std\n+        if isinstance(module, (nn.Linear, nn.Conv1d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, InformerSinusoidalPositionalEmbedding):\n+            module._init_weight()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+        past_key_values_length: int,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            # 2d mask is passed through the layers\n+            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n+        elif self.config._attn_implementation == \"sdpa\":\n+            # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+            # the manual implementation that requires a 4D causal mask in all cases.\n+            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n+                attention_mask,\n+                input_shape,\n+                inputs_embeds,\n+                past_key_values_length,\n+            )\n+        elif self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_shape),\n+                        device=inputs_embeds.device,\n+                    )\n+                )\n+        else:\n+            # 4d mask is passed through the layers\n+            attention_mask = _prepare_4d_causal_attention_mask(\n+                attention_mask, input_shape, inputs_embeds, past_key_values_length\n+            )\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n+\n+class InformerAttention(BartAttention):\n+    pass\n+\n+\n+class InformerProbSparseAttention(nn.Module):\n+    \"\"\"Probabilistic Attention mechanism to select the \"active\"\n+    queries rather than the \"lazy\" queries and provides a sparse Transformer thus mitigating the quadratic compute and\n+    memory requirements of vanilla attention\"\"\"\n+\n+    def __init__(\n+        self,\n+        embed_dim: int,\n+        num_heads: int,\n+        dropout: float = 0.0,\n+        is_decoder: bool = False,\n+        sampling_factor: int = 5,\n+        bias: bool = True,\n+        layer_idx: Optional[int] = None,\n+    ):\n+        super().__init__()\n+        self.factor = sampling_factor\n+        self.embed_dim = embed_dim\n+        self.num_heads = num_heads\n+        self.dropout = dropout\n+        self.head_dim = embed_dim // num_heads\n+\n+        if (self.head_dim * num_heads) != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n+                f\" and `num_heads`: {num_heads}).\"\n+            )\n+        self.scaling = self.head_dim**-0.5\n+        self.is_decoder = is_decoder\n+        self.layer_idx = layer_idx\n+\n+        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n+        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n+        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n+        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n+\n+    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n+        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        key_value_states: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        layer_head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        # if key_value_states are provided this layer is used as a cross-attention layer\n+        # for the decoder\n+        is_cross_attention = key_value_states is not None\n+\n+        bsz, tgt_len, _ = hidden_states.size()\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n+\n+        # get query proj\n+        query_states = self.q_proj(hidden_states) * self.scaling\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+        else:\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n+            value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n+\n+        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n+        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n+        key_states = key_states.reshape(*proj_shape)\n+        value_states = value_states.reshape(*proj_shape)\n+\n+        key_states_time_length = key_states.size(1)  # L_K\n+        log_key_states_time_length = np.ceil(np.log1p(key_states_time_length)).astype(\"int\").item()  # log_L_K\n+\n+        query_states_time_length = query_states.size(1)  # L_Q\n+        log_query_states_time_length = np.ceil(np.log1p(query_states_time_length)).astype(\"int\").item()  # log_L_Q\n+\n+        u_part = min(self.factor * query_states_time_length * log_key_states_time_length, key_states_time_length)\n+        u = min(self.factor * log_query_states_time_length, query_states_time_length)\n+\n+        if key_states_time_length > 0:\n+            index_sample = torch.randint(0, key_states_time_length, (u_part,))\n+            k_sample = key_states[:, index_sample, :]\n+        else:\n+            k_sample = key_states\n+\n+        queries_keys_sample = torch.bmm(query_states, k_sample.transpose(1, 2))  # Q_K_sampled\n+\n+        # find the Top_k query with sparsity measurement\n+        if u > 0:\n+            sparsity_measurement = queries_keys_sample.max(dim=-1)[0] - torch.div(\n+                queries_keys_sample.sum(dim=-1), key_states_time_length\n+            )  # M\n+            top_u_sparsity_measurement = sparsity_measurement.topk(u, sorted=False)[1]  # M_top\n+\n+            # calculate q_reduce: query_states[:, top_u_sparsity_measurement]\n+            dim_for_slice = torch.arange(query_states.size(0)).unsqueeze(-1)\n+            q_reduce = query_states[dim_for_slice, top_u_sparsity_measurement]\n+        else:\n+            q_reduce = query_states\n+            top_u_sparsity_measurement = None\n+\n+        # Use q_reduce to calculate attention weights\n+        attn_weights = torch.bmm(q_reduce, key_states.transpose(1, 2))\n+\n+        src_len = key_states.size(1)\n+        if attn_weights.size() != (bsz * self.num_heads, u, src_len):\n+            raise ValueError(\n+                f\"Attention weights should be of size {(bsz * self.num_heads, u, src_len)}, but is\"\n+                f\" {attn_weights.size()}\"\n+            )\n+\n+        if attention_mask is not None:\n+            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n+                raise ValueError(\n+                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n+                )\n+            prob_mask = attention_mask.expand(bsz, self.num_heads, tgt_len, src_len).reshape(\n+                bsz * self.num_heads, tgt_len, src_len\n+            )\n+\n+            if top_u_sparsity_measurement is not None:\n+                dim_for_slice = torch.arange(prob_mask.size(0)).unsqueeze(-1)\n+                prob_mask = prob_mask[dim_for_slice, top_u_sparsity_measurement, :]\n+\n+            attn_weights = attn_weights.view(bsz, self.num_heads, u, src_len) + prob_mask.view(\n+                bsz, self.num_heads, u, src_len\n+            )\n+            attn_weights = attn_weights.view(bsz * self.num_heads, u, src_len)\n+\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+        if layer_head_mask is not None:\n+            if layer_head_mask.size() != (self.num_heads,):\n+                raise ValueError(\n+                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n+                    f\" {layer_head_mask.size()}\"\n+                )\n+            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, u, src_len)\n+            attn_weights = attn_weights.view(bsz * self.num_heads, u, src_len)\n+\n+        if output_attentions:\n+            # this operation is a bit awkward, but it's required to\n+            # make sure that attn_weights keeps its gradient.\n+            # In order to do so, attn_weights have to be reshaped\n+            # twice and have to be reused in the following\n+            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, u, src_len)\n+            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, u, src_len)\n+        else:\n+            attn_weights_reshaped = None\n+\n+        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n+        attn_output = torch.bmm(attn_probs, value_states)\n+\n+        # calculate context for updating the attn_output, based on:\n+        # https://github.com/zhouhaoyi/Informer2020/blob/ac59c7447135473fb2aafeafe94395f884d5c7a5/models/attn.py#L74\n+        if self.is_decoder:\n+            # cast to float32 before operation to avoid overflow\n+            context = value_states.cumsum(dim=-2, dtype=torch.float32).to(value_states.dtype)\n+        else:\n+            v_mean_dim_time = value_states.mean(dim=-2)\n+            context = (\n+                v_mean_dim_time.unsqueeze(dim=1)\n+                .expand(bsz * self.num_heads, query_states_time_length, v_mean_dim_time.size(-1))\n+                .clone()\n+            )\n+\n+        if top_u_sparsity_measurement is not None:\n+            # update context: copy the attention output to the context at top_u_sparsity_measurement index\n+            dim_for_slice = torch.arange(context.size(0)).unsqueeze(-1)\n+            context[dim_for_slice, top_u_sparsity_measurement, :] = attn_output\n+            attn_output = context\n+\n+        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n+        attn_output = attn_output.transpose(1, 2)\n+\n+        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n+        # partitioned across GPUs when using tensor-parallelism.\n+        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n+\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, attn_weights_reshaped, past_key_value\n+\n+\n+# source: https://github.com/zhouhaoyi/Informer2020/blob/main/models/encoder.py\n+class InformerConvLayer(nn.Module):\n+    def __init__(self, c_in):\n+        super().__init__()\n+        self.downConv = nn.Conv1d(\n+            in_channels=c_in,\n+            out_channels=c_in,\n+            kernel_size=3,\n+            padding=1,\n+            padding_mode=\"circular\",\n+        )\n+        self.norm = nn.BatchNorm1d(c_in)\n+        self.activation = nn.ELU()\n+        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n+\n+    def forward(self, x):\n+        x = self.downConv(x.permute(0, 2, 1))\n+        x = self.norm(x)\n+        x = self.activation(x)\n+        x = self.maxPool(x)\n+        x = x.transpose(1, 2)\n+        return x\n+\n+\n+class InformerEncoderLayer(TimeSeriesTransformerEncoderLayer):\n+    def __init__(self, config: InformerConfig):\n+        super().__init__(config)\n+\n+        del self.self_attn\n+\n+        if config.attention_type == \"prob\":\n+            self.self_attn = InformerProbSparseAttention(\n+                embed_dim=self.embed_dim,\n+                num_heads=config.encoder_attention_heads,\n+                dropout=config.attention_dropout,\n+                sampling_factor=config.sampling_factor,\n+            )\n+        else:\n+            self.self_attn = InformerAttention(\n+                embed_dim=self.embed_dim,\n+                num_heads=config.encoder_attention_heads,\n+                dropout=config.attention_dropout,\n+                config=config,\n+            )\n+\n+\n+class InformerDecoderLayer(TimeSeriesTransformerDecoderLayer):\n+    def __init__(self, config: InformerConfig, layer_idx: Optional[int] = None):\n+        super().__init__(config)\n+\n+        del self.self_attn\n+\n+        if config.attention_type == \"prob\":\n+            self.self_attn = InformerProbSparseAttention(\n+                embed_dim=self.embed_dim,\n+                num_heads=config.decoder_attention_heads,\n+                dropout=config.attention_dropout,\n+                sampling_factor=config.sampling_factor,\n+                is_decoder=True,\n+                layer_idx=layer_idx,\n+            )\n+        else:\n+            self.self_attn = InformerAttention(\n+                embed_dim=self.embed_dim,\n+                num_heads=config.decoder_attention_heads,\n+                dropout=config.attention_dropout,\n+                is_decoder=True,\n+                config=config,\n+                layer_idx=layer_idx,\n+            )\n+\n+\n+class InformerEncoder(TimeSeriesTransformerEncoder):\n+    def __init__(self, config: InformerConfig):\n+        super().__init__(config)\n+\n+        self.dropout = config.dropout\n+        self.layerdrop = config.encoder_layerdrop\n+        self.gradient_checkpointing = False\n+        if config.prediction_length is None:\n+            raise ValueError(\"The `prediction_length` config needs to be specified.\")\n+\n+        self.value_embedding = InformerValueEmbedding(feature_size=config.feature_size, d_model=config.d_model)\n+        self.embed_positions = InformerSinusoidalPositionalEmbedding(\n+            config.context_length + config.prediction_length, config.d_model\n+        )\n+        self.layers = nn.ModuleList([InformerEncoderLayer(config) for _ in range(config.encoder_layers)])\n+        self.layernorm_embedding = nn.LayerNorm(config.d_model)\n+\n+        if config.distil:\n+            self.conv_layers = nn.ModuleList(\n+                [InformerConvLayer(config.d_model) for _ in range(config.encoder_layers - 1)]\n+            )\n+            self.conv_layers.append(None)\n+        else:\n+            self.conv_layers = [None] * config.encoder_layers\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def forward(\n+        self,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutput]:\n+        r\"\"\"\n+        Args:\n+            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+\n+                [What are attention masks?](../glossary#attention-mask)\n+            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n+                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n+\n+                - 1 indicates the head is **not masked**,\n+                - 0 indicates the head is **masked**.\n+\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n+                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n+                than the model's internal embedding lookup matrix.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        hidden_states = self.value_embedding(inputs_embeds)\n+        embed_pos = self.embed_positions(inputs_embeds.size())\n+\n+        hidden_states = self.layernorm_embedding(hidden_states + embed_pos)\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+\n+        # expand attention_mask\n+        if attention_mask is not None:\n+            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+            attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        encoder_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        # check if head_mask has a correct number of layers specified if desired\n+        if head_mask is not None:\n+            if head_mask.size()[0] != (len(self.layers)):\n+                raise ValueError(\n+                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n+                    f\" {head_mask.size()[0]}.\"\n+                )\n+\n+        for idx, (encoder_layer, conv_layer) in enumerate(zip(self.layers, self.conv_layers)):\n+            if output_hidden_states:\n+                encoder_states = encoder_states + (hidden_states,)\n+            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n+            to_drop = False\n+            if self.training:\n+                dropout_probability = torch.rand([])\n+                if dropout_probability < self.layerdrop:  # skip the layer\n+                    to_drop = True\n+\n+            if to_drop:\n+                layer_outputs = (None, None)\n+            else:\n+                if self.gradient_checkpointing and self.training:\n+                    layer_outputs = self._gradient_checkpointing_func(\n+                        encoder_layer.__call__,\n+                        hidden_states,\n+                        attention_mask,\n+                        (head_mask[idx] if head_mask is not None else None),\n+                        output_attentions,\n+                    )\n+                    if conv_layer is not None:\n+                        output = self._gradient_checkpointing_func(conv_layer, layer_outputs[0])\n+                        layer_outputs = (output,) + layer_outputs[1:]\n+                else:\n+                    layer_outputs = encoder_layer(\n+                        hidden_states,\n+                        attention_mask,\n+                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                        output_attentions=output_attentions,\n+                    )\n+                    if conv_layer is not None:\n+                        output = conv_layer(layer_outputs[0])\n+                        layer_outputs = (output,) + layer_outputs[1:]\n+\n+                hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_attentions = all_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            encoder_states = encoder_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n+        )\n+\n+\n+class InformerDecoder(TimeSeriesTransformerDecoder):\n+    def __init__(self, config: InformerConfig):\n+        super().__init__(config)\n+        self.dropout = config.dropout\n+        self.layerdrop = config.decoder_layerdrop\n+        if config.prediction_length is None:\n+            raise ValueError(\"The `prediction_length` config needs to be specified.\")\n+\n+        self.value_embedding = InformerValueEmbedding(feature_size=config.feature_size, d_model=config.d_model)\n+        self.embed_positions = InformerSinusoidalPositionalEmbedding(\n+            config.context_length + config.prediction_length, config.d_model\n+        )\n+        self.layers = nn.ModuleList([InformerDecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)])\n+        self.layernorm_embedding = nn.LayerNorm(config.d_model)\n+\n+        self.gradient_checkpointing = False\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+\n+class InformerModel(TimeSeriesTransformerModel, nn.Module):\n+    def __init__(self, config: InformerConfig):\n+        nn.Module().__init__(config)\n+\n+        if config.scaling == \"mean\" or config.scaling is True:\n+            self.scaler = InformerMeanScaler(config)\n+        elif config.scaling == \"std\":\n+            self.scaler = InformerStdScaler(config)\n+        else:\n+            self.scaler = InformerNOPScaler(config)\n+\n+        if config.num_static_categorical_features > 0:\n+            self.embedder = InformerFeatureEmbedder(\n+                cardinalities=config.cardinality,\n+                embedding_dims=config.embedding_dimension,\n+            )\n+\n+        # transformer encoder-decoder and mask initializer\n+        self.encoder = InformerEncoder(config)\n+        self.decoder = InformerDecoder(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def forward(self, **super_kwargs):\n+        r\"\"\"\n+        past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`):\n+            Past values of the time series, that serve as context in order to predict the future. The sequence size of\n+            this tensor must be larger than the `context_length` of the model, since the model will use the larger size\n+            to construct lag features, i.e. additional values from the past which are added in order to serve as \"extra\n+            context\".\n+\n+            The `sequence_length` here is equal to `config.context_length` + `max(config.lags_sequence)`, which if no\n+            `lags_sequence` is configured, is equal to `config.context_length` + 7 (as by default, the largest\n+            look-back index in `config.lags_sequence` is 7). The property `_past_length` returns the actual length of\n+            the past.\n+\n+            The `past_values` is what the Transformer encoder gets as input (with optional additional features, such as\n+            `static_categorical_features`, `static_real_features`, `past_time_features` and lags).\n+\n+            Optionally, missing values need to be replaced with zeros and indicated via the `past_observed_mask`.\n+\n+            For multivariate time series, the `input_size` > 1 dimension is required and corresponds to the number of\n+            variates in the time series per time step.\n+        past_time_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_features)`):\n+            Required time features, which the model internally will add to `past_values`. These could be things like\n+            \"month of year\", \"day of the month\", etc. encoded as vectors (for instance as Fourier features). These\n+            could also be so-called \"age\" features, which basically help the model know \"at which point in life\" a\n+            time-series is. Age features have small values for distant past time steps and increase monotonically the\n+            more we approach the current time step. Holiday features are also a good example of time features.\n+\n+            These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT, where\n+            the position encodings are learned from scratch internally as parameters of the model, the Time Series\n+            Transformer requires to provide additional time features. The Time Series Transformer only learns\n+            additional embeddings for `static_categorical_features`.\n+\n+            Additional dynamic real covariates can be concatenated to this tensor, with the caveat that these features\n+            must but known at prediction time.\n+\n+            The `num_features` here is equal to `config.`num_time_features` + `config.num_dynamic_real_features`.\n+        past_observed_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`, *optional*):\n+            Boolean mask to indicate which `past_values` were observed and which were missing. Mask values selected in\n+            `[0, 1]`:\n+\n+            - 1 for values that are **observed**,\n+            - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n+        static_categorical_features (`torch.LongTensor` of shape `(batch_size, number of static categorical features)`, *optional*):\n+            Optional static categorical features for which the model will learn an embedding, which it will add to the\n+            values of the time series.\n+\n+            Static categorical features are features which have the same value for all time steps (static over time).\n+\n+            A typical example of a static categorical feature is a time series ID.\n+        static_real_features (`torch.FloatTensor` of shape `(batch_size, number of static real features)`, *optional*):\n+            Optional static real features which the model will add to the values of the time series.\n+\n+            Static real features are features which have the same value for all time steps (static over time).\n+\n+            A typical example of a static real feature is promotion information.\n+        future_values (`torch.FloatTensor` of shape `(batch_size, prediction_length)` or `(batch_size, prediction_length, input_size)`, *optional*):\n+            Future values of the time series, that serve as labels for the model. The `future_values` is what the\n+            Transformer needs during training to learn to output, given the `past_values`.\n+\n+            The sequence length here is equal to `prediction_length`.\n+\n+            See the demo notebook and code snippets for details.\n+\n+            Optionally, during training any missing values need to be replaced with zeros and indicated via the\n+            `future_observed_mask`.\n+\n+            For multivariate time series, the `input_size` > 1 dimension is required and corresponds to the number of\n+            variates in the time series per time step.\n+        future_time_features (`torch.FloatTensor` of shape `(batch_size, prediction_length, num_features)`):\n+            Required time features for the prediction window, which the model internally will add to `future_values`.\n+            These could be things like \"month of year\", \"day of the month\", etc. encoded as vectors (for instance as\n+            Fourier features). These could also be so-called \"age\" features, which basically help the model know \"at\n+            which point in life\" a time-series is. Age features have small values for distant past time steps and\n+            increase monotonically the more we approach the current time step. Holiday features are also a good example\n+            of time features.\n+\n+            These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT, where\n+            the position encodings are learned from scratch internally as parameters of the model, the Time Series\n+            Transformer requires to provide additional time features. The Time Series Transformer only learns\n+            additional embeddings for `static_categorical_features`.\n+\n+            Additional dynamic real covariates can be concatenated to this tensor, with the caveat that these features\n+            must but known at prediction time.\n+\n+            The `num_features` here is equal to `config.`num_time_features` + `config.num_dynamic_real_features`.\n+        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n+            Tuple consists of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)\n+            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*) is a sequence of\n+            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n+\n+        Examples:\n+\n+        ```python\n+        >>> from huggingface_hub import hf_hub_download\n+        >>> import torch\n+        >>> from transformers import InformerModel\n+\n+        >>> file = hf_hub_download(\n+        ...     repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=\"train-batch.pt\", repo_type=\"dataset\"\n+        ... )\n+        >>> batch = torch.load(file)\n+\n+        >>> model = InformerModel.from_pretrained(\"huggingface/informer-tourism-monthly\")\n+\n+        >>> # during training, one provides both past and future values\n+        >>> # as well as possible additional features\n+        >>> outputs = model(\n+        ...     past_values=batch[\"past_values\"],\n+        ...     past_time_features=batch[\"past_time_features\"],\n+        ...     past_observed_mask=batch[\"past_observed_mask\"],\n+        ...     static_categorical_features=batch[\"static_categorical_features\"],\n+        ...     static_real_features=batch[\"static_real_features\"],\n+        ...     future_values=batch[\"future_values\"],\n+        ...     future_time_features=batch[\"future_time_features\"],\n+        ... )\n+\n+        >>> last_hidden_state = outputs.last_hidden_state\n+        ```\"\"\"\n+        super().forward(**super_kwargs)\n+\n+\n+class InformerForPrediction(TimeSeriesTransformerForPrediction, nn.Module):\n+    def __init__(self, config: InformerConfig):\n+        nn.Module().__init__(config)\n+\n+        self.model = InformerModel(config)\n+        if config.distribution_output == \"student_t\":\n+            self.distribution_output = StudentTOutput(dim=config.input_size)\n+        elif config.distribution_output == \"normal\":\n+            self.distribution_output = NormalOutput(dim=config.input_size)\n+        elif config.distribution_output == \"negative_binomial\":\n+            self.distribution_output = NegativeBinomialOutput(dim=config.input_size)\n+        else:\n+            raise ValueError(f\"Unknown distribution output {config.distribution_output}\")\n+\n+        self.parameter_projection = self.distribution_output.get_parameter_projection(self.model.config.d_model)\n+        self.target_shape = self.distribution_output.event_shape\n+\n+        if config.loss == \"nll\":\n+            self.loss = nll\n+        else:\n+            raise ValueError(f\"Unknown loss function {config.loss}\")\n+\n+        # Initialize weights of distribution_output and apply final processing\n+        self.post_init()\n+\n+    @auto_docstring\n+    def forward(self, **super_kwargs):\n+        r\"\"\"\n+        past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`):\n+            Past values of the time series, that serve as context in order to predict the future. The sequence size of\n+            this tensor must be larger than the `context_length` of the model, since the model will use the larger size\n+            to construct lag features, i.e. additional values from the past which are added in order to serve as \"extra\n+            context\".\n+\n+            The `sequence_length` here is equal to `config.context_length` + `max(config.lags_sequence)`, which if no\n+            `lags_sequence` is configured, is equal to `config.context_length` + 7 (as by default, the largest\n+            look-back index in `config.lags_sequence` is 7). The property `_past_length` returns the actual length of\n+            the past.\n+\n+            The `past_values` is what the Transformer encoder gets as input (with optional additional features, such as\n+            `static_categorical_features`, `static_real_features`, `past_time_features` and lags).\n+\n+            Optionally, missing values need to be replaced with zeros and indicated via the `past_observed_mask`.\n+\n+            For multivariate time series, the `input_size` > 1 dimension is required and corresponds to the number of\n+            variates in the time series per time step.\n+        past_time_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_features)`):\n+            Required time features, which the model internally will add to `past_values`. These could be things like\n+            \"month of year\", \"day of the month\", etc. encoded as vectors (for instance as Fourier features). These\n+            could also be so-called \"age\" features, which basically help the model know \"at which point in life\" a\n+            time-series is. Age features have small values for distant past time steps and increase monotonically the\n+            more we approach the current time step. Holiday features are also a good example of time features.\n+\n+            These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT, where\n+            the position encodings are learned from scratch internally as parameters of the model, the Time Series\n+            Transformer requires to provide additional time features. The Time Series Transformer only learns\n+            additional embeddings for `static_categorical_features`.\n+\n+            Additional dynamic real covariates can be concatenated to this tensor, with the caveat that these features\n+            must but known at prediction time.\n+\n+            The `num_features` here is equal to `config.`num_time_features` + `config.num_dynamic_real_features`.\n+        past_observed_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`, *optional*):\n+            Boolean mask to indicate which `past_values` were observed and which were missing. Mask values selected in\n+            `[0, 1]`:\n+\n+            - 1 for values that are **observed**,\n+            - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n+        static_categorical_features (`torch.LongTensor` of shape `(batch_size, number of static categorical features)`, *optional*):\n+            Optional static categorical features for which the model will learn an embedding, which it will add to the\n+            values of the time series.\n+\n+            Static categorical features are features which have the same value for all time steps (static over time).\n+\n+            A typical example of a static categorical feature is a time series ID.\n+        static_real_features (`torch.FloatTensor` of shape `(batch_size, number of static real features)`, *optional*):\n+            Optional static real features which the model will add to the values of the time series.\n+\n+            Static real features are features which have the same value for all time steps (static over time).\n+\n+            A typical example of a static real feature is promotion information.\n+        future_values (`torch.FloatTensor` of shape `(batch_size, prediction_length)` or `(batch_size, prediction_length, input_size)`, *optional*):\n+            Future values of the time series, that serve as labels for the model. The `future_values` is what the\n+            Transformer needs during training to learn to output, given the `past_values`.\n+\n+            The sequence length here is equal to `prediction_length`.\n+\n+            See the demo notebook and code snippets for details.\n+\n+            Optionally, during training any missing values need to be replaced with zeros and indicated via the\n+            `future_observed_mask`.\n+\n+            For multivariate time series, the `input_size` > 1 dimension is required and corresponds to the number of\n+            variates in the time series per time step.\n+        future_time_features (`torch.FloatTensor` of shape `(batch_size, prediction_length, num_features)`):\n+            Required time features for the prediction window, which the model internally will add to `future_values`.\n+            These could be things like \"month of year\", \"day of the month\", etc. encoded as vectors (for instance as\n+            Fourier features). These could also be so-called \"age\" features, which basically help the model know \"at\n+            which point in life\" a time-series is. Age features have small values for distant past time steps and\n+            increase monotonically the more we approach the current time step. Holiday features are also a good example\n+            of time features.\n+\n+            These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT, where\n+            the position encodings are learned from scratch internally as parameters of the model, the Time Series\n+            Transformer requires to provide additional time features. The Time Series Transformer only learns\n+            additional embeddings for `static_categorical_features`.\n+\n+            Additional dynamic real covariates can be concatenated to this tensor, with the caveat that these features\n+            must but known at prediction time.\n+\n+            The `num_features` here is equal to `config.`num_time_features` + `config.num_dynamic_real_features`.\n+        future_observed_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`, *optional*):\n+            Boolean mask to indicate which `future_values` were observed and which were missing. Mask values selected\n+            in `[0, 1]`:\n+\n+            - 1 for values that are **observed**,\n+            - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n+\n+            This mask is used to filter out missing values for the final loss calculation.\n+        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n+            Tuple consists of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)\n+            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*) is a sequence of\n+            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n+\n+        Examples:\n+\n+        ```python\n+        >>> from huggingface_hub import hf_hub_download\n+        >>> import torch\n+        >>> from transformers import InformerForPrediction\n+\n+        >>> file = hf_hub_download(\n+        ...     repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=\"train-batch.pt\", repo_type=\"dataset\"\n+        ... )\n+        >>> batch = torch.load(file)\n+\n+        >>> model = InformerForPrediction.from_pretrained(\n+        ...     \"huggingface/informer-tourism-monthly\"\n+        ... )\n+\n+        >>> # during training, one provides both past and future values\n+        >>> # as well as possible additional features\n+        >>> outputs = model(\n+        ...     past_values=batch[\"past_values\"],\n+        ...     past_time_features=batch[\"past_time_features\"],\n+        ...     past_observed_mask=batch[\"past_observed_mask\"],\n+        ...     static_categorical_features=batch[\"static_categorical_features\"],\n+        ...     static_real_features=batch[\"static_real_features\"],\n+        ...     future_values=batch[\"future_values\"],\n+        ...     future_time_features=batch[\"future_time_features\"],\n+        ... )\n+\n+        >>> loss = outputs.loss\n+        >>> loss.backward()\n+\n+        >>> # during inference, one only provides past values\n+        >>> # as well as possible additional features\n+        >>> # the model autoregressively generates future values\n+        >>> outputs = model.generate(\n+        ...     past_values=batch[\"past_values\"],\n+        ...     past_time_features=batch[\"past_time_features\"],\n+        ...     past_observed_mask=batch[\"past_observed_mask\"],\n+        ...     static_categorical_features=batch[\"static_categorical_features\"],\n+        ...     static_real_features=batch[\"static_real_features\"],\n+        ...     future_time_features=batch[\"future_time_features\"],\n+        ... )\n+\n+        >>> mean_prediction = outputs.sequences.mean(dim=1)\n+        ```\"\"\"\n+        super().forward(**super_kwargs)\n+\n+\n+__all__ = [\"InformerForPrediction\", \"InformerModel\", \"InformerPreTrainedModel\"]"
        },
        {
            "sha": "55ecad415233f3c17fdc9b44e2cb6a4e95dc3ad2",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 171,
            "deletions": 375,
            "changes": 546,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"PyTorch M2M100 model.\"\"\"\n \n import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n from torch import nn\n@@ -31,31 +31,23 @@\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n )\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_flash_attention_utils import (\n+    FlashAttentionKwargs,\n+)\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n     Seq2SeqLMOutput,\n     Seq2SeqModelOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    auto_docstring,\n-    is_torch_flex_attn_available,\n-    is_torchdynamo_compiling,\n-    logging,\n-)\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n from .configuration_m2m_100 import M2M100Config\n \n \n if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n+    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n \n \n logger = logging.get_logger(__name__)\n@@ -184,6 +176,37 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds, past_key_values_\n         return position_ids.unsqueeze(0).expand(input_shape).contiguous() + past_key_values_length\n \n \n+# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->M2M100\n class M2M100Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n@@ -236,152 +259,25 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        query_states = query_states * self.scaling\n-\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_value\n-\n-        current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n-        else:\n-            key_states = self.k_proj(current_states)\n-            value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n-            if past_key_value is not None:\n-                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n-                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = query_states.reshape(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n \n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped, past_key_value\n-\n-\n-# Copied from transformers.models.bart.modeling_bart.BartFlashAttention2 with Bart->M2M100\n-class M2M100FlashAttention2(M2M100Attention):\n-    \"\"\"\n-    M2M100 flash attention module. This module inherits from `M2M100Attention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # M2M100FlashAttention2 attention does not support output_attentions\n-        if output_attentions:\n-            raise ValueError(\n-                \"M2M100SdpaAttention2 attention does not support `output_attentions`. \"\n-                \"Use the argument `attn_implementation='eager'` when loading the model.\"\n-            )\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-        bsz, q_len, _ = hidden_states.size()\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim)\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n@@ -402,8 +298,8 @@ def forward(\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n+            value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n             if past_key_value is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n@@ -415,157 +311,27 @@ def forward(\n                 if is_cross_attention:\n                     past_key_value.is_updated[self.layer_idx] = True\n \n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (LlamaRMSNorm handles it correctly)\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            dropout=self.dropout if self.training else 0.0,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-# Copied from transformers.models.bart.modeling_bart.BartSdpaAttention with Bart->M2M100\n-class M2M100SdpaAttention(M2M100Attention):\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"M2M100Model is using M2M100SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n-                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                key_value_states=key_value_states,\n-                past_key_value=past_key_value,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n-            )\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_value\n-\n-        current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n-        else:\n-            key_states = self.k_proj(current_states)\n-            value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-            if past_key_value is not None:\n-                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n-                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n-\n-        causal_mask = None\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and causal_mask is None and tgt_len > 1 else False\n-\n-        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n-        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.view(bsz, tgt_len, self.embed_dim)\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+        return attn_output, attn_weights, past_key_value\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->M2M100, MBART->M2M100\n@@ -574,7 +340,7 @@ def __init__(self, config: M2M100Config):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = M2M100_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = M2M100Attention(\n             embed_dim=self.embed_dim,\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -637,20 +403,13 @@ def forward(\n         return outputs\n \n \n-M2M100_ATTENTION_CLASSES = {\n-    \"eager\": M2M100Attention,\n-    \"flash_attention_2\": M2M100FlashAttention2,\n-    \"sdpa\": M2M100SdpaAttention,\n-}\n-\n-\n # Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->M2M100, MBART->M2M100\n class M2M100DecoderLayer(nn.Module):\n     def __init__(self, config: M2M100Config, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = M2M100_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = M2M100Attention(\n             embed_dim=self.embed_dim,\n             num_heads=config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -664,7 +423,7 @@ def __init__(self, config: M2M100Config, layer_idx: Optional[int] = None):\n         self.activation_dropout = config.activation_dropout\n \n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n-        self.encoder_attn = M2M100_ATTENTION_CLASSES[config._attn_implementation](\n+        self.encoder_attn = M2M100Attention(\n             self.embed_dim,\n             config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -732,7 +491,6 @@ def forward(\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n             hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n@@ -772,6 +530,8 @@ class M2M100PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"M2M100EncoderLayer\", \"M2M100DecoderLayer\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n     _supports_cache_class = True\n     # Doesn't support `compile` (dynamic control flow). Can be fixed but low usage model\n     _supports_static_cache = False\n@@ -790,32 +550,64 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n             module.bias.data.zero_()\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool = False,\n     ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n+                        device=attention_mask.device,\n+                    )\n+                )\n             return attention_mask\n \n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+\n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -849,7 +641,6 @@ def _update_causal_mask(\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n             and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n@@ -915,6 +706,42 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n         return causal_mask\n \n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n class M2M100Encoder(M2M100PreTrainedModel):\n     \"\"\"\n@@ -951,8 +778,6 @@ def __init__(self, config: M2M100Config, embed_tokens: Optional[nn.Embedding] =\n         )\n         self.layers = nn.ModuleList([M2M100EncoderLayer(config) for _ in range(config.encoder_layers)])\n         self.layer_norm = nn.LayerNorm(config.d_model)\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n-        self._use_sdpa = config._attn_implementation == \"sdpa\"\n \n         self.gradient_checkpointing = False\n         # Initialize weights and apply final processing\n@@ -1031,18 +856,10 @@ def forward(\n         hidden_states = inputs_embeds + embed_pos\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        # expand attention_mask\n-        if attention_mask is not None:\n-            if self._use_flash_attention_2:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self._use_sdpa and head_mask is None and not output_attentions:\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            inputs_embeds,\n+        )\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -1133,8 +950,6 @@ def __init__(self, config: M2M100Config, embed_tokens: Optional[nn.Embedding] =\n             self.padding_idx,\n         )\n         self.layers = nn.ModuleList([M2M100DecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)])\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n-        self._use_sdpa = config._attn_implementation == \"sdpa\"\n         self.layer_norm = nn.LayerNorm(config.d_model)\n \n         self.gradient_checkpointing = False\n@@ -1232,23 +1047,27 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # retrieve input_ids and inputs_embeds\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-\n-        if input_ids is not None:\n-            input_ids = input_ids.view(-1, input_ids.shape[-1])\n+        elif input_ids is not None:\n+            input_shape = input_ids.size()\n+            input_ids = input_ids.view(-1, input_shape[-1])\n+        elif inputs_embeds is not None:\n+            input_shape = inputs_embeds.size()[:-1]\n+        else:\n+            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n         # initialize `past_key_values`\n         return_legacy_cache = False\n         if use_cache and not isinstance(past_key_values, Cache):\n@@ -1277,49 +1096,26 @@ def forward(\n             if isinstance(past_key_values, EncoderDecoderCache)\n             else past_key_values\n         )\n-        causal_mask = self._update_causal_mask(\n+\n+        attention_mask = self._update_causal_mask(\n             attention_mask,\n             inputs_embeds,\n             cache_position,\n             self_attn_cache,\n-            output_attentions,\n         )\n-\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self._use_flash_attention_2:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self._use_sdpa and cross_attn_head_mask is None and not output_attentions:\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=seq_length,\n-                )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=seq_length,\n-                )\n-\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            input_shape,\n+            inputs_embeds,\n+        )\n         # embed positions\n         positions = self.embed_positions(input_ids, inputs_embeds, past_key_values_length)\n         positions = positions.to(inputs_embeds.device)\n \n         hidden_states = inputs_embeds + positions\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n@@ -1351,7 +1147,7 @@ def forward(\n                     layer_outputs = self._gradient_checkpointing_func(\n                         decoder_layer.__call__,\n                         hidden_states,\n-                        causal_mask,\n+                        attention_mask,\n                         encoder_hidden_states,\n                         encoder_attention_mask,\n                         head_mask[idx] if head_mask is not None else None,\n@@ -1364,7 +1160,7 @@ def forward(\n                 else:\n                     layer_outputs = decoder_layer(\n                         hidden_states,\n-                        attention_mask=causal_mask,\n+                        attention_mask=attention_mask,\n                         encoder_hidden_states=encoder_hidden_states,\n                         encoder_attention_mask=encoder_attention_mask,\n                         layer_head_mask=(head_mask[idx] if head_mask is not None else None),"
        },
        {
            "sha": "016cb865f83dfbae398f5e61d07ef5bf1c24f8d4",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 181,
            "deletions": 106,
            "changes": 287,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -16,7 +16,7 @@\n \n import copy\n import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -30,15 +30,18 @@\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n     _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n )\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n     Seq2SeqLMOutput,\n     Seq2SeqModelOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n     auto_docstring,\n     is_torch_flex_attn_available,\n@@ -108,6 +111,37 @@ def forward(\n         return super().forward(position_ids)\n \n \n+# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->Marian\n class MarianAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n@@ -160,17 +194,25 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-        bsz, tgt_len, _ = hidden_states.size()\n+\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n+\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        query_states = query_states * self.scaling\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n@@ -191,8 +233,8 @@ def forward(\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n+            value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n             if past_key_value is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n@@ -204,66 +246,27 @@ def forward(\n                 if is_cross_attention:\n                     past_key_value.is_updated[self.layer_idx] = True\n \n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = query_states.reshape(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        if attention_mask is not None:\n-            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n+        )\n \n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights, past_key_value\n \n \n # Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->Marian, BART->MARIAN\n@@ -272,7 +275,7 @@ def __init__(self, config: MarianConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = MARIAN_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = MarianAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -338,16 +341,13 @@ def forward(\n         return outputs\n \n \n-MARIAN_ATTENTION_CLASSES = {\"eager\": MarianAttention}\n-\n-\n # Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->Marian, BART->MARIAN\n class MarianDecoderLayer(nn.Module):\n     def __init__(self, config: MarianConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = MARIAN_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = MarianAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -361,7 +361,7 @@ def __init__(self, config: MarianConfig, layer_idx: Optional[int] = None):\n         self.activation_dropout = config.activation_dropout\n \n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n-        self.encoder_attn = MARIAN_ATTENTION_CLASSES[config._attn_implementation](\n+        self.encoder_attn = MarianAttention(\n             self.embed_dim,\n             config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -435,6 +435,7 @@ def forward(\n                 layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n@@ -465,6 +466,10 @@ class MarianPreTrainedModel(PreTrainedModel):\n     config_class = MarianConfig\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n     _supports_cache_class = True\n     _supports_static_cache = True\n \n@@ -484,32 +489,75 @@ def _init_weights(self, module: Union[nn.Linear, nn.Embedding, MarianSinusoidalP\n             module.weight.data.fill_(1.0)\n             module.bias.data.zero_()\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n+    @property\n+    def dummy_inputs(self):\n+        pad_token = self.config.pad_token_id\n+        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n+        dummy_inputs = {\n+            \"attention_mask\": input_ids.ne(pad_token),\n+            \"input_ids\": input_ids,\n+            \"decoder_input_ids\": input_ids,\n+        }\n+        return dummy_inputs\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool = False,\n     ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n+                        device=attention_mask.device,\n+                    )\n+                )\n             return attention_mask\n \n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+\n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -543,7 +591,6 @@ def _update_causal_mask(\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n             and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n@@ -609,16 +656,41 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n         return causal_mask\n \n-    @property\n-    def dummy_inputs(self):\n-        pad_token = self.config.pad_token_id\n-        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n-        dummy_inputs = {\n-            \"attention_mask\": input_ids.ne(pad_token),\n-            \"input_ids\": input_ids,\n-            \"decoder_input_ids\": input_ids,\n-        }\n-        return dummy_inputs\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n \n \n class MarianEncoder(MarianPreTrainedModel):\n@@ -734,10 +806,10 @@ def forward(\n         hidden_states = inputs_embeds + embed_pos\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        # expand attention_mask\n-        if attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            inputs_embeds,\n+        )\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -929,12 +1001,18 @@ def forward(\n         # retrieve input_ids and inputs_embeds\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-\n-        if input_ids is not None:\n-            input_ids = input_ids.view(-1, input_ids.shape[-1])\n+        elif input_ids is not None:\n+            input = input_ids\n+            input_shape = input.shape\n+            input_ids = input_ids.view(-1, input_shape[-1])\n+        elif inputs_embeds is not None:\n+            input_shape = inputs_embeds.size()[:-1]\n+            input = inputs_embeds[:, :, -1]\n+        else:\n+            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n+            inputs_embeds = self.embed_tokens(input)\n \n         # Important to apply outside of the above `if`, in case user passes `embeds`\n         inputs_embeds = inputs_embeds * self.embed_scale\n@@ -967,22 +1045,19 @@ def forward(\n             if isinstance(past_key_values, EncoderDecoderCache)\n             else past_key_values\n         )\n+\n         causal_mask = self._update_causal_mask(\n             attention_mask,\n             inputs_embeds,\n             cache_position,\n             self_attn_cache,\n-            output_attentions,\n         )\n-\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            encoder_attention_mask = _prepare_4d_attention_mask(\n-                encoder_attention_mask,\n-                inputs_embeds.dtype,\n-                tgt_len=seq_length,\n-            )\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            input_shape,\n+            inputs_embeds,\n+        )\n \n         # embed positions\n         position_ids = self.embed_positions("
        },
        {
            "sha": "bdf352a1f646f640eb78e12f2f7a5b785a793ff0",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 163,
            "deletions": 346,
            "changes": 509,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -16,7 +16,7 @@\n \n import copy\n import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -31,7 +31,9 @@\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n )\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_flash_attention_utils import (\n+    FlashAttentionKwargs,\n+)\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -41,7 +43,8 @@\n     Seq2SeqQuestionAnsweringModelOutput,\n     Seq2SeqSequenceClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n     auto_docstring,\n     is_torch_flex_attn_available,\n@@ -52,13 +55,7 @@\n \n \n if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n+    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n \n \n logger = logging.get_logger(__name__)\n@@ -124,6 +121,37 @@ def forward(self, input_ids: torch.Tensor):\n         return super().forward(input_ids) * self.embed_scale\n \n \n+# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->MBart\n class MBartAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n@@ -176,152 +204,25 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        query_states = query_states * self.scaling\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_value\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n \n-        current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n-        else:\n-            key_states = self.k_proj(current_states)\n-            value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-            if past_key_value is not None:\n-                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n-                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = query_states.reshape(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped, past_key_value\n-\n-\n-# Copied from transformers.models.bart.modeling_bart.BartFlashAttention2 with Bart->MBart\n-class MBartFlashAttention2(MBartAttention):\n-    \"\"\"\n-    MBart flash attention module. This module inherits from `MBartAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # MBartFlashAttention2 attention does not support output_attentions\n-        if output_attentions:\n-            raise ValueError(\n-                \"MBartSdpaAttention2 attention does not support `output_attentions`. \"\n-                \"Use the argument `attn_implementation='eager'` when loading the model.\"\n-            )\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-        bsz, q_len, _ = hidden_states.size()\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim)\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n@@ -342,8 +243,8 @@ def forward(\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n+            value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n             if past_key_value is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n@@ -355,172 +256,35 @@ def forward(\n                 if is_cross_attention:\n                     past_key_value.is_updated[self.layer_idx] = True\n \n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (LlamaRMSNorm handles it correctly)\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            dropout=self.dropout if self.training else 0.0,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-# Copied from transformers.models.bart.modeling_bart.BartSdpaAttention with Bart->MBart\n-class MBartSdpaAttention(MBartAttention):\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"MBartModel is using MBartSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n-                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                key_value_states=key_value_states,\n-                past_key_value=past_key_value,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n-            )\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_value\n-\n-        current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n-        else:\n-            key_states = self.k_proj(current_states)\n-            value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-            if past_key_value is not None:\n-                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = curr_past_key_value.update(\n-                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n-\n-        causal_mask = None\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and causal_mask is None and tgt_len > 1 else False\n-\n-        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n-        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.view(bsz, tgt_len, self.embed_dim)\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n-\n-\n-MBART_ATTENTION_CLASSES = {\n-    \"eager\": MBartAttention,\n-    \"sdpa\": MBartSdpaAttention,\n-    \"flash_attention_2\": MBartFlashAttention2,\n-}\n+        return attn_output, attn_weights, past_key_value\n \n \n class MBartEncoderLayer(nn.Module):\n     def __init__(self, config: MBartConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = MBART_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = MBartAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -588,7 +352,7 @@ def __init__(self, config: MBartConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = MBART_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = MBartAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -602,7 +366,7 @@ def __init__(self, config: MBartConfig, layer_idx: Optional[int] = None):\n         self.activation_dropout = config.activation_dropout\n \n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n-        self.encoder_attn = MBART_ATTENTION_CLASSES[config._attn_implementation](\n+        self.encoder_attn = MBartAttention(\n             self.embed_dim,\n             config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -670,7 +434,6 @@ def forward(\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n             hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n@@ -735,6 +498,8 @@ class MBartPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"MBartDecoderLayer\", \"MBartEncoderLayer\", \"MBartAttention\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n     _supports_cache_class = True\n     _supports_static_cache = True\n \n@@ -762,32 +527,64 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool = False,\n     ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n+                        device=attention_mask.device,\n+                    )\n+                )\n             return attention_mask\n \n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+\n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -821,7 +618,6 @@ def _update_causal_mask(\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n             and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n@@ -887,6 +683,42 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n         return causal_mask\n \n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n class MBartEncoder(MBartPreTrainedModel):\n     \"\"\"\n@@ -1007,18 +839,10 @@ def forward(\n         hidden_states = self.layernorm_embedding(hidden_states)\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        # expand attention_mask\n-        if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\" and head_mask is None and not output_attentions:\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            inputs_embeds,\n+        )\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -1214,12 +1038,18 @@ def forward(\n         # retrieve input_ids and inputs_embeds\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-\n-        if input_ids is not None:\n-            input_ids = input_ids.view(-1, input_ids.shape[-1])\n+        elif input_ids is not None:\n+            input = input_ids\n+            input_shape = input.shape\n+            input_ids = input_ids.view(-1, input_shape[-1])\n+        elif inputs_embeds is not None:\n+            input_shape = inputs_embeds.size()[:-1]\n+            input = inputs_embeds[:, :, -1]\n+        else:\n+            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n+            inputs_embeds = self.embed_tokens(input)\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:\n@@ -1256,32 +1086,19 @@ def forward(\n             if isinstance(past_key_values, EncoderDecoderCache)\n             else past_key_values\n         )\n+\n         causal_mask = self._update_causal_mask(\n             attention_mask,\n             inputs_embeds,\n             cache_position,\n             self_attn_cache,\n-            output_attentions,\n         )\n-\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\" and cross_attn_head_mask is None and not output_attentions:\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=seq_length,\n-                )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_length\n-                )\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            input_shape,\n+            inputs_embeds,\n+        )\n \n         # embed positions\n         position_ids = self.embed_positions(input, past_key_values_length, position_ids=cache_position)"
        },
        {
            "sha": "a0e21f586cfc99588f20297c3945b24e86091862",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 163,
            "deletions": 359,
            "changes": 522,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -19,7 +19,7 @@\n import math\n import random\n from dataclasses import dataclass\n-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -40,23 +40,27 @@\n     _prepare_4d_causal_attention_mask,\n     _prepare_4d_causal_attention_mask_for_sdpa,\n )\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_flash_attention_utils import (\n+    FlashAttentionKwargs,\n+)\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n     ModelOutput,\n     Seq2SeqLMOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n from ..auto.configuration_auto import AutoConfig\n from ..auto.modeling_auto import AutoModel\n from .configuration_musicgen import MusicgenConfig, MusicgenDecoderConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n \n if TYPE_CHECKING:\n     from ...generation.streamers import BaseStreamer\n@@ -146,7 +150,38 @@ def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):\n         return self.weights.index_select(0, position_ids.view(-1)).detach()\n \n \n-# Copied from transformers.models.hubert.modeling_hubert.HubertAttention with Hubert->Musicgen\n+# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Attention with Wav2Vec2->Musicgen\n class MusicgenAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -181,9 +216,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -192,292 +224,26 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n \n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-        else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped, past_key_value\n-\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n \n-# Copied from transformers.models.hubert.modeling_hubert.HubertFlashAttention2 with Hubert->Musicgen\n-class MusicgenFlashAttention2(MusicgenAttention):\n-    \"\"\"\n-    Musicgen flash attention module. This module inherits from `MusicgenAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n-        bsz, q_len, _ = hidden_states.size()\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self._reshape(self.q_proj(hidden_states), -1, bsz)\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0].transpose(1, 2)\n-            value_states = past_key_value[1].transpose(1, 2)\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._reshape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0].transpose(1, 2), key_states], dim=1)\n-            value_states = torch.cat([past_key_value[1].transpose(1, 2), value_states], dim=1)\n-        else:\n-            # self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states.transpose(1, 2), value_states.transpose(1, 2))\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value[0].shape[-2]\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (LlamaRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask,\n-            q_len,\n-            dropout=self.dropout if self.training else 0.0,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-        attn_output = self.out_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class MusicgenSdpaAttention(MusicgenAttention):\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"MusicgenModel is using MusicgenSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention\"\n-                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                key_value_states=key_value_states,\n-                past_key_value=past_key_value,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        if (\n-            attention_mask is not None\n-            and (attention_mask.mean(dim=[1, 2, 3]) <= torch.finfo(attention_mask.dtype).min).any()\n-        ):\n-            logger.warning_once(\n-                '`torch.nn.functional.scaled_dot_product_attention` does not support having an empty attention mask. Falling back to the manual attention implementation. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                \"Note that this probably happens because `guidance_scale>1` or because you used `get_unconditional_inputs`. See https://github.com/huggingface/transformers/issues/31189 for more information.\"\n-            )\n-            return super().forward(\n-                hidden_states,\n-                key_value_states=key_value_states,\n-                past_key_value=past_key_value,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states)\n         # get key, value proj\n         # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n         # is checking that the `sequence_length` of the `past_key_value` is the same as\n@@ -492,18 +258,18 @@ def forward(\n             value_states = past_key_value[1]\n         elif is_cross_attention:\n             # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n+            key_states = self.k_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n         elif past_key_value is not None:\n             # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n             key_states = torch.cat([past_key_value[0], key_states], dim=2)\n             value_states = torch.cat([past_key_value[1], value_states], dim=2)\n         else:\n             # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n         if self.is_decoder:\n             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n@@ -515,54 +281,35 @@ def forward(\n             # if encoder bi-directional self-attention `past_key_value` is always `None`\n             past_key_value = (key_states, value_states)\n \n-        query_states = self._shape(query_states, tgt_len, bsz)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and attention_mask is None and tgt_len > 1 else False\n-\n-        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n-        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n         )\n \n-        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n-\n-\n-MUSICGEN_ATTENTION_CLASSES = {\n-    \"eager\": MusicgenAttention,\n-    \"sdpa\": MusicgenSdpaAttention,\n-    \"flash_attention_2\": MusicgenFlashAttention2,\n-}\n+        return attn_output, attn_weights, past_key_value\n \n \n class MusicgenDecoderLayer(nn.Module):\n     def __init__(self, config: MusicgenDecoderConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n \n-        self.self_attn = MUSICGEN_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = MusicgenAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.num_attention_heads,\n             dropout=config.attention_dropout,\n@@ -576,7 +323,7 @@ def __init__(self, config: MusicgenDecoderConfig):\n         self.activation_dropout = config.activation_dropout\n \n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n-        self.encoder_attn = MUSICGEN_ATTENTION_CLASSES[config._attn_implementation](\n+        self.encoder_attn = MusicgenAttention(\n             self.embed_dim,\n             config.num_attention_heads,\n             dropout=config.attention_dropout,\n@@ -590,6 +337,7 @@ def __init__(self, config: MusicgenDecoderConfig):\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n     # copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer.forward\n+    # TODO: change to new cache class\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -688,6 +436,8 @@ class MusicgenPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"MusicgenDecoderLayer\", \"MusicgenAttention\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    # compilation errors occurr atm\n+    _supports_flex_attn = False\n \n     def _init_weights(self, module):\n         std = self.config.initializer_factor\n@@ -819,40 +569,18 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = sum([self.embed_tokens[codebook](input[:, codebook]) for codebook in range(num_codebooks)])\n \n-        if self.attn_implementation == \"flash_attention_2\":\n-            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-        elif self.attn_implementation == \"sdpa\" and head_mask is None and not output_attentions:\n-            # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-            # the manual implementation that requires a 4D causal mask in all cases.\n-            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                attention_mask,\n-                input_shape,\n-                inputs_embeds,\n-                past_key_values_length,\n-            )\n-        else:\n-            attention_mask = _prepare_4d_causal_attention_mask(\n-                attention_mask, input_shape, inputs_embeds, past_key_values_length\n-            )\n-\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.attn_implementation == \"flash_attention_2\":\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.attn_implementation == \"sdpa\" and cross_attn_head_mask is None and not output_attentions:\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n+        attention_mask = self._update_causal_mask(\n+            attention_mask,\n+            input_shape,\n+            inputs_embeds,\n+            past_key_values_length,\n+        )\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            input_shape,\n+            inputs_embeds,\n+        )\n \n         # embed positions\n         positions = self.embed_positions(input, past_key_values_length)\n@@ -951,6 +679,80 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+        past_key_values_length: int,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            # 2d mask is passed through the layers\n+            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n+        elif self.config._attn_implementation == \"sdpa\":\n+            # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+            # the manual implementation that requires a 4D causal mask in all cases.\n+            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n+                attention_mask,\n+                input_shape,\n+                inputs_embeds,\n+                past_key_values_length,\n+            )\n+        elif self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_shape),\n+                        device=inputs_embeds.device,\n+                    )\n+                )\n+        else:\n+            # 4d mask is passed through the layers\n+            attention_mask = _prepare_4d_causal_attention_mask(\n+                attention_mask, input_shape, inputs_embeds, past_key_values_length\n+            )\n+\n+        return attention_mask\n+\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n @auto_docstring\n class MusicgenModel(MusicgenPreTrainedModel):\n@@ -1559,6 +1361,8 @@ class MusicgenForConditionalGeneration(PreTrainedModel, GenerationMixin):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    # compilation errors occurr atm\n+    _supports_flex_attn = False\n \n     def __init__(\n         self,"
        },
        {
            "sha": "3312ad33cdb1d776bb7a204f021263654d8e69d4",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 136,
            "deletions": 324,
            "changes": 460,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -19,7 +19,7 @@\n import math\n import random\n from dataclasses import dataclass\n-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -34,18 +34,25 @@\n     LogitsProcessorList,\n     StoppingCriteriaList,\n )\n-from ...modeling_attn_mask_utils import _prepare_4d_causal_attention_mask, _prepare_4d_causal_attention_mask_for_sdpa\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_causal_attention_mask,\n+    _prepare_4d_causal_attention_mask_for_sdpa,\n+)\n+from ...modeling_flash_attention_utils import (\n+    FlashAttentionKwargs,\n+)\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n from ..auto.configuration_auto import AutoConfig\n from ..auto.modeling_auto import AutoModel, AutoModelForTextEncoding\n from .configuration_musicgen_melody import MusicgenMelodyConfig, MusicgenMelodyDecoderConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n \n if TYPE_CHECKING:\n     from ...generation.streamers import BaseStreamer\n@@ -159,7 +166,38 @@ def forward(self, inputs_embeds: torch.Tensor, past_key_values_length: int = 0):\n         return self.weights.index_select(0, position_ids.view(-1)).detach()\n \n \n-# Copied from transformers.models.hubert.modeling_hubert.HubertAttention with Hubert->MusicgenMelody\n+# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Attention with Wav2Vec2->MusicgenMelody\n class MusicgenMelodyAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -194,9 +232,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -205,276 +240,26 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n \n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-        else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped, past_key_value\n-\n-\n-# Copied from transformers.models.hubert.modeling_hubert.HubertFlashAttention2 with Hubert->MusicgenMelody\n-class MusicgenMelodyFlashAttention2(MusicgenMelodyAttention):\n-    \"\"\"\n-    MusicgenMelody flash attention module. This module inherits from `MusicgenMelodyAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n \n-    def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n-        bsz, q_len, _ = hidden_states.size()\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self._reshape(self.q_proj(hidden_states), -1, bsz)\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0].transpose(1, 2)\n-            value_states = past_key_value[1].transpose(1, 2)\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._reshape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0].transpose(1, 2), key_states], dim=1)\n-            value_states = torch.cat([past_key_value[1].transpose(1, 2), value_states], dim=1)\n-        else:\n-            # self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states.transpose(1, 2), value_states.transpose(1, 2))\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value[0].shape[-2]\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (LlamaRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask,\n-            q_len,\n-            dropout=self.dropout if self.training else 0.0,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-        attn_output = self.out_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-# Copied from transformers.models.hubert.modeling_hubert.HubertSdpaAttention with Hubert->MusicgenMelody\n-class MusicgenMelodySdpaAttention(MusicgenMelodyAttention):\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"MusicgenMelodyModel is using MusicgenMelodySdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n-                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                key_value_states=key_value_states,\n-                past_key_value=past_key_value,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states)\n         # get key, value proj\n         # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n         # is checking that the `sequence_length` of the `past_key_value` is the same as\n@@ -489,18 +274,18 @@ def forward(\n             value_states = past_key_value[1]\n         elif is_cross_attention:\n             # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n+            key_states = self.k_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n         elif past_key_value is not None:\n             # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n             key_states = torch.cat([past_key_value[0], key_states], dim=2)\n             value_states = torch.cat([past_key_value[1], value_states], dim=2)\n         else:\n             # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n         if self.is_decoder:\n             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n@@ -512,54 +297,35 @@ def forward(\n             # if encoder bi-directional self-attention `past_key_value` is always `None`\n             past_key_value = (key_states, value_states)\n \n-        query_states = self._shape(query_states, tgt_len, bsz)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and attention_mask is None and tgt_len > 1 else False\n-\n-        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n-        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n         )\n \n-        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n-\n-\n-MUSICGEN_MELODY_ATTENTION_CLASSES = {\n-    \"eager\": MusicgenMelodyAttention,\n-    \"sdpa\": MusicgenMelodySdpaAttention,\n-    \"flash_attention_2\": MusicgenMelodyFlashAttention2,\n-}\n+        return attn_output, attn_weights, past_key_value\n \n \n class MusicgenMelodyDecoderLayer(nn.Module):\n     def __init__(self, config: MusicgenMelodyDecoderConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n \n-        self.self_attn = MUSICGEN_MELODY_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = MusicgenMelodyAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.num_attention_heads,\n             dropout=config.attention_dropout,\n@@ -644,6 +410,8 @@ class MusicgenMelodyPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"MusicgenMelodyDecoderLayer\", \"MusicgenMelodyAttention\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    # compilation errors occurr atm\n+    _supports_flex_attn = False\n \n     def _init_weights(self, module):\n         std = self.config.initializer_factor\n@@ -785,21 +553,12 @@ def forward(\n \n         input_shape = inputs_embeds.size()[:-1]\n \n-        if self.attn_implementation == \"flash_attention_2\":\n-            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-        elif self.attn_implementation == \"sdpa\" and not output_attentions:\n-            # output_attentions=True can not be supported when using SDPA, and we fall back on\n-            # the manual implementation that requires a 4D causal mask in all cases.\n-            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                attention_mask,\n-                input_shape,\n-                inputs_embeds,\n-                past_key_values_length,\n-            )\n-        else:\n-            attention_mask = _prepare_4d_causal_attention_mask(\n-                attention_mask, input_shape, inputs_embeds, past_key_values_length\n-            )\n+        attention_mask = self._update_causal_mask(\n+            attention_mask,\n+            input_shape,\n+            inputs_embeds,\n+            past_key_values_length,\n+        )\n \n         # embed positions\n         positions = self.embed_positions(inputs_embeds, past_key_values_length)\n@@ -881,6 +640,57 @@ def forward(\n             attentions=all_attentions,\n         )\n \n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+        past_key_values_length: int,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            # 2d mask is passed through the layers\n+            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n+        elif self.config._attn_implementation == \"sdpa\":\n+            # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+            # the manual implementation that requires a 4D causal mask in all cases.\n+            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n+                attention_mask,\n+                input_shape,\n+                inputs_embeds,\n+                past_key_values_length,\n+            )\n+        elif self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_shape),\n+                        device=inputs_embeds.device,\n+                    )\n+                )\n+        else:\n+            # 4d mask is passed through the layers\n+            attention_mask = _prepare_4d_causal_attention_mask(\n+                attention_mask, input_shape, inputs_embeds, past_key_values_length\n+            )\n+\n+        return attention_mask\n+\n+    # Ignore copy\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # MusicgenMelody doesn't apply cross attention, hence it's ignored here\n+        # and only exists to not confuse any copy checks\n+        pass\n+\n \n @auto_docstring\n # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenModel with MUSICGEN->MUSICGEN_MELODY,Musicgen->MusicgenMelody\n@@ -1482,6 +1292,8 @@ class MusicgenMelodyForConditionalGeneration(PreTrainedModel, GenerationMixin):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    # compilation errors occurr atm\n+    _supports_flex_attn = False\n \n     def __init__(\n         self,"
        },
        {
            "sha": "6d7bd6c985d1bf96645caeeab95477a3262c8ed6",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 211,
            "deletions": 94,
            "changes": 305,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"PyTorch NLLB-MoE model.\"\"\"\n \n import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -25,18 +25,29 @@\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n+    _prepare_4d_causal_attention_mask,\n+    _prepare_4d_causal_attention_mask_for_sdpa,\n+)\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     MoEModelOutput,\n     MoEModelOutputWithPastAndCrossAttentions,\n     Seq2SeqMoEModelOutput,\n     Seq2SeqMoEOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n from .configuration_nllb_moe import NllbMoeConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -460,7 +471,38 @@ def forward(self, hidden_states: torch.Tensor, padding_mask: Optional[torch.Tens\n         return hidden_states, (router_probs, top_1_expert_index)\n \n \n-# Copied from transformers.models.hubert.modeling_hubert.HubertAttention with Hubert->NllbMoe,key_value_states->encoder_hidden_states\n+# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Attention with Wav2Vec2->NllbMoe,key_value_states->encoder_hidden_states\n class NllbMoeAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -495,9 +537,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -506,17 +545,26 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if encoder_hidden_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = encoder_hidden_states is not None\n \n-        bsz, tgt_len, _ = hidden_states.size()\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = encoder_hidden_states.shape[1] if is_cross_attention else tgt_len\n+\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n+\n         # get key, value proj\n         # `past_key_value[0].shape[2] == encoder_hidden_states.shape[1]`\n         # is checking that the `sequence_length` of the `past_key_value` is the same as\n@@ -531,18 +579,18 @@ def forward(\n             value_states = past_key_value[1]\n         elif is_cross_attention:\n             # cross_attentions\n-            key_states = self._shape(self.k_proj(encoder_hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(encoder_hidden_states), -1, bsz)\n+            key_states = self.k_proj(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n         elif past_key_value is not None:\n             # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n             key_states = torch.cat([past_key_value[0], key_states], dim=2)\n             value_states = torch.cat([past_key_value[1], value_states], dim=2)\n         else:\n             # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n         if self.is_decoder:\n             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n@@ -554,69 +602,27 @@ def forward(\n             # if encoder bi-directional self-attention `past_key_value` is always `None`\n             past_key_value = (key_states, value_states)\n \n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n+        )\n \n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights, past_key_value\n \n \n class NllbMoeEncoderLayer(nn.Module):\n@@ -628,6 +634,7 @@ def __init__(self, config: NllbMoeConfig, is_sparse: bool = False):\n             embed_dim=self.embed_dim,\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n+            config=config,\n         )\n         self.attn_dropout = nn.Dropout(config.dropout)\n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n@@ -710,14 +717,19 @@ def __init__(self, config: NllbMoeConfig, is_sparse: bool = False):\n             num_heads=config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n             is_decoder=True,\n+            config=config,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n         self.attn_dropout = nn.Dropout(config.dropout)\n \n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.cross_attention = NllbMoeAttention(\n-            self.embed_dim, config.decoder_attention_heads, config.attention_dropout, is_decoder=True\n+            self.embed_dim,\n+            config.decoder_attention_heads,\n+            config.attention_dropout,\n+            is_decoder=True,\n+            config=config,\n         )\n         self.cross_attention_layer_norm = nn.LayerNorm(self.embed_dim)\n         if not self.is_sparse:\n@@ -837,6 +849,12 @@ class NllbMoePreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"NllbMoeEncoderLayer\", \"NllbMoeDecoderLayer\"]\n+    # TODO: If anyone is up to it to make sure tests pass etc\n+    # Flash attention has problems due to not preparing masks the same way as eager/sdpa\n+    # SDPA has more flaky logits which requires more time to look into tests\n+    _supports_flash_attn_2 = False\n+    _supports_sdpa = False\n+    _supports_flex_attn = False\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -975,10 +993,10 @@ def forward(\n         hidden_states = inputs_embeds + embed_pos\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        # expand attention_mask\n-        if attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            inputs_embeds,\n+        )\n \n         encoder_states = () if output_hidden_states else None\n         all_router_probs = () if output_router_logits else None\n@@ -1042,6 +1060,29 @@ def forward(\n             router_probs=all_router_probs,\n         )\n \n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n \n class NllbMoeDecoder(NllbMoePreTrainedModel):\n     \"\"\"\n@@ -1195,18 +1236,18 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # create causal mask\n-        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-        combined_attention_mask = _prepare_4d_causal_attention_mask(\n-            attention_mask, input_shape, inputs_embeds, past_key_values_length\n+        attention_mask = self._update_causal_mask(\n+            attention_mask,\n+            input_shape,\n+            inputs_embeds,\n+            past_key_values_length,\n+        )\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            input_shape,\n+            inputs_embeds,\n         )\n-\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            encoder_attention_mask = _prepare_4d_attention_mask(\n-                encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-            )\n \n         # embed positions\n         positions = self.embed_positions(input_ids, inputs_embeds, past_key_values_length)\n@@ -1264,7 +1305,7 @@ def forward(\n                     layer_outputs = self._gradient_checkpointing_func(\n                         decoder_layer.forward,\n                         hidden_states,\n-                        combined_attention_mask,\n+                        attention_mask,\n                         encoder_hidden_states,\n                         encoder_attention_mask,\n                         layer_head_mask,\n@@ -1276,7 +1317,7 @@ def forward(\n                 else:\n                     layer_outputs = decoder_layer(\n                         hidden_states,\n-                        attention_mask=combined_attention_mask,\n+                        attention_mask=attention_mask,\n                         encoder_hidden_states=encoder_hidden_states,\n                         encoder_attention_mask=encoder_attention_mask,\n                         layer_head_mask=layer_head_mask,\n@@ -1330,6 +1371,82 @@ def forward(\n             router_probs=all_router_probs,\n         )\n \n+    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+        past_key_values_length: int,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            # 2d mask is passed through the layers\n+            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n+        elif self.config._attn_implementation == \"sdpa\":\n+            # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+            # the manual implementation that requires a 4D causal mask in all cases.\n+            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n+                attention_mask,\n+                input_shape,\n+                inputs_embeds,\n+                past_key_values_length,\n+            )\n+        elif self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_shape),\n+                        device=inputs_embeds.device,\n+                    )\n+                )\n+        else:\n+            # 4d mask is passed through the layers\n+            attention_mask = _prepare_4d_causal_attention_mask(\n+                attention_mask, input_shape, inputs_embeds, past_key_values_length\n+            )\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n @auto_docstring\n class NllbMoeModel(NllbMoePreTrainedModel):"
        },
        {
            "sha": "8f00e8900928fe34f7b345f78a7da7f5ee404d68",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 105,
            "deletions": 78,
            "changes": 183,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -16,17 +16,19 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n \n from transformers.modeling_utils import PreTrainedModel\n from transformers.utils import ModelOutput\n \n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_patchtsmixer import PatchTSMixerConfig\n \n \n@@ -235,7 +237,38 @@ def forward(self, inputs: torch.Tensor):\n         return out\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->PatchTSMixer\n+# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Attention with Wav2Vec2->PatchTSMixer\n class PatchTSMixerAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -248,7 +281,6 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[PatchTSMixerConfig] = None,\n-        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -265,23 +297,12 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n-        self.layer_idx = layer_idx\n-        if layer_idx is None and self.is_decoder:\n-            logger.warning_once(\n-                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n-                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    # Ignore copy\n-    @deprecate_kwarg(\"key_value_states\", version=\"4.55\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55\")\n-    @deprecate_kwarg(\"cache_position\", version=\"4.55\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -290,79 +311,84 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        query_states = query_states * self.scaling\n+        # if key_value_states are provided this layer is used as a cross-attention layer\n+        # for the decoder\n+        is_cross_attention = key_value_states is not None\n \n-        key_states = self.k_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = self.v_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n \n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = query_states.reshape(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n+        # get query proj\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n+\n+        # get key, value proj\n+        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n+        # is checking that the `sequence_length` of the `past_key_value` is the same as\n+        # the provided `key_value_states` to support prefix tuning\n+        if (\n+            is_cross_attention\n+            and past_key_value is not None\n+            and past_key_value[0].shape[2] == key_value_states.shape[1]\n+        ):\n+            # reuse k,v, cross_attentions\n+            key_states = past_key_value[0]\n+            value_states = past_key_value[1]\n+        elif is_cross_attention:\n+            # cross_attentions\n+            key_states = self.k_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n+        elif past_key_value is not None:\n+            # reuse k, v, self_attention\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n+            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n         else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n+            # self_attention\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+\n+        if self.is_decoder:\n+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n+            # Further calls to cross_attention layer can then reuse all cross-attention\n+            # key/value_states (first \"if\" case)\n+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n+            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n+            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n+            # if encoder bi-directional self-attention `past_key_value` is always `None`\n+            past_key_value = (key_states, value_states)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n+        )\n \n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, None\n+        return attn_output, attn_weights, past_key_value\n \n \n class PatchMixerBlock(nn.Module):\n@@ -395,6 +421,7 @@ def __init__(self, config: PatchTSMixerConfig):\n                 embed_dim=config.d_model,\n                 num_heads=config.self_attn_heads,\n                 dropout=config.dropout,\n+                config=config,\n             )\n             self.norm_attn = PatchTSMixerNormLayer(config)\n "
        },
        {
            "sha": "b85e8a66b2541fed8557b59804afdc142b0c0700",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 72,
            "deletions": 74,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -16,14 +16,16 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n from torch import nn\n \n from ...activations import ACT2CLS\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n from ...utils import ModelOutput, auto_docstring, logging\n from .configuration_patchtst import PatchTSTConfig\n@@ -32,7 +34,38 @@\n logger = logging.get_logger(__name__)\n \n \n-# Copied from transformers.models.hubert.modeling_hubert.HubertAttention with Hubert->PatchTST\n+# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Attention with Wav2Vec2->PatchTST\n class PatchTSTAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -67,9 +100,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -78,17 +108,26 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n \n-        bsz, tgt_len, _ = hidden_states.size()\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n+\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n+\n         # get key, value proj\n         # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n         # is checking that the `sequence_length` of the `past_key_value` is the same as\n@@ -103,18 +142,18 @@ def forward(\n             value_states = past_key_value[1]\n         elif is_cross_attention:\n             # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n+            key_states = self.k_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n         elif past_key_value is not None:\n             # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n             key_states = torch.cat([past_key_value[0], key_states], dim=2)\n             value_states = torch.cat([past_key_value[1], value_states], dim=2)\n         else:\n             # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n         if self.is_decoder:\n             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n@@ -126,69 +165,27 @@ def forward(\n             # if encoder bi-directional self-attention `past_key_value` is always `None`\n             past_key_value = (key_states, value_states)\n \n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n+        )\n \n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights, past_key_value\n \n \n class PatchTSTBatchNorm(nn.Module):\n@@ -461,6 +458,7 @@ def __init__(self, config: PatchTSTConfig):\n             embed_dim=config.d_model,\n             num_heads=config.num_attention_heads,\n             dropout=config.attention_dropout,\n+            config=config,\n         )\n \n         # Add & Norm of the sublayer 1"
        },
        {
            "sha": "19166bd6091da740e77d33db254f3d96eabbc329",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 180,
            "deletions": 105,
            "changes": 285,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -16,7 +16,7 @@\n \n import copy\n import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -30,15 +30,18 @@\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n     _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n )\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n     Seq2SeqLMOutput,\n     Seq2SeqModelOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n     auto_docstring,\n     is_torch_flex_attn_available,\n@@ -49,9 +52,7 @@\n \n \n if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n+    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n \n \n logger = logging.get_logger(__name__)\n@@ -109,6 +110,37 @@ def forward(\n         return super().forward(position_ids)\n \n \n+# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->Pegasus\n class PegasusAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n@@ -161,17 +193,25 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-        bsz, tgt_len, _ = hidden_states.size()\n+\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n+\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        query_states = query_states * self.scaling\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n@@ -192,8 +232,8 @@ def forward(\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n+            value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n             if past_key_value is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n@@ -205,69 +245,27 @@ def forward(\n                 if is_cross_attention:\n                     past_key_value.is_updated[self.layer_idx] = True\n \n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = query_states.reshape(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n+        )\n \n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n-\n-\n-PEGASUS_ATTENTION_CLASSES = {\"eager\": PegasusAttention}\n+        return attn_output, attn_weights, past_key_value\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->Pegasus, MBART->PEGASUS\n@@ -276,7 +274,7 @@ def __init__(self, config: PegasusConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = PEGASUS_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = PegasusAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -345,7 +343,7 @@ def __init__(self, config: PegasusConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = PEGASUS_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = PegasusAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -359,7 +357,7 @@ def __init__(self, config: PegasusConfig, layer_idx: Optional[int] = None):\n         self.activation_dropout = config.activation_dropout\n \n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n-        self.encoder_attn = PEGASUS_ATTENTION_CLASSES[config._attn_implementation](\n+        self.encoder_attn = PegasusAttention(\n             self.embed_dim,\n             config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -427,7 +425,6 @@ def forward(\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n             hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n@@ -464,6 +461,12 @@ class PegasusPreTrainedModel(PreTrainedModel):\n     config_class = PegasusConfig\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std\n@@ -481,32 +484,64 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n             module.bias.data.zero_()\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool = False,\n     ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n+                        device=attention_mask.device,\n+                    )\n+                )\n             return attention_mask\n \n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+\n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -540,7 +575,6 @@ def _update_causal_mask(\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n             and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n@@ -606,6 +640,42 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n         return causal_mask\n \n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n class PegasusEncoder(PegasusPreTrainedModel):\n     \"\"\"\n@@ -748,10 +818,10 @@ def forward(\n \n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        # expand attention_mask\n-        if attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            inputs_embeds,\n+        )\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -970,26 +1040,32 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # retrieve input_ids and inputs_embeds\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-\n-        if input_ids is not None:\n-            input_ids = input_ids.view(-1, input_ids.shape[-1])\n+        elif input_ids is not None:\n+            input = input_ids\n+            input_shape = input.shape\n+            input_ids = input_ids.view(-1, input_shape[-1])\n+        elif inputs_embeds is not None:\n+            input_shape = inputs_embeds.size()[:-1]\n+            input = inputs_embeds[:, :, -1]\n+        else:\n+            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n+            inputs_embeds = self.embed_tokens(input)\n \n         # important to apply scale outside of `if` in case users pass `embeds`\n         inputs_embeds = inputs_embeds * self.embed_scale\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n         # initialize `past_key_values`\n         return_legacy_cache = False\n         if use_cache and not isinstance(past_key_values, Cache):\n@@ -1018,20 +1094,19 @@ def forward(\n             if isinstance(past_key_values, EncoderDecoderCache)\n             else past_key_values\n         )\n+\n         causal_mask = self._update_causal_mask(\n             attention_mask,\n             inputs_embeds,\n             cache_position,\n             self_attn_cache,\n-            output_attentions,\n         )\n-\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            encoder_attention_mask = _prepare_4d_attention_mask(\n-                encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_length\n-            )\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            input_shape,\n+            inputs_embeds,\n+        )\n \n         # embed positions\n         positions = self.embed_positions((batch_size, seq_length), past_key_values_length, position_ids=cache_position)"
        },
        {
            "sha": "a2fcf5edd1b673455391f3db224079bbd1d162f6",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 174,
            "deletions": 94,
            "changes": 268,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -16,7 +16,7 @@\n \n import dataclasses\n import math\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -30,14 +30,17 @@\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n     _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n )\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n     Seq2SeqLMOutput,\n     Seq2SeqModelOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n     auto_docstring,\n     is_torch_flex_attn_available,\n@@ -48,9 +51,7 @@\n \n \n if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n+    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n \n \n logger = logging.get_logger(__name__)\n@@ -135,6 +136,37 @@ def forward(\n         return pe[None].expand(batch_size, -1, -1)\n \n \n+# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->PegasusX\n class PegasusXAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n@@ -187,17 +219,25 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-        bsz, tgt_len, _ = hidden_states.size()\n+\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n+\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        query_states = query_states * self.scaling\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n@@ -218,8 +258,8 @@ def forward(\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n+            value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n             if past_key_value is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n@@ -231,66 +271,27 @@ def forward(\n                 if is_cross_attention:\n                     past_key_value.is_updated[self.layer_idx] = True\n \n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = query_states.reshape(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n+        )\n \n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights, past_key_value\n \n \n class PegasusXGlobalLocalAttention(nn.Module):\n@@ -653,6 +654,7 @@ def __init__(self, config: PegasusXConfig, layer_idx: Optional[int] = None):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             bias=False,\n+            config=config,\n             layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n@@ -666,6 +668,7 @@ def __init__(self, config: PegasusXConfig, layer_idx: Optional[int] = None):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             bias=False,\n+            config=config,\n             layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n@@ -758,6 +761,11 @@ class PegasusXPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [r\"PegasusXEncoderLayer\", r\"PegasusXDecoderLayer\"]\n+    _supports_flash_attn_2 = True\n+    # Flaky logits\n+    _supports_sdpa = False\n+    # Compile issues\n+    _supports_flex_attn = False\n     _supports_cache_class = True\n     _supports_static_cache = True\n \n@@ -773,32 +781,64 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n             module.bias.data.zero_()\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool = False,\n     ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n+                        device=attention_mask.device,\n+                    )\n+                )\n             return attention_mask\n \n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+\n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -832,7 +872,6 @@ def _update_causal_mask(\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n             and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n@@ -898,6 +937,42 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n         return causal_mask\n \n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n class PegasusXEncoder(PegasusXPreTrainedModel):\n     \"\"\"\n@@ -1227,22 +1302,28 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # retrieve input_ids and inputs_embeds\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-\n-        if input_ids is not None:\n-            input_ids = input_ids.view(-1, input_ids.shape[-1])\n+        elif input_ids is not None:\n+            input = input_ids\n+            input_shape = input.shape\n+            input_ids = input_ids.view(-1, input_shape[-1])\n+        elif inputs_embeds is not None:\n+            input_shape = inputs_embeds.size()[:-1]\n+            input = inputs_embeds[:, :, -1]\n+        else:\n+            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n+            inputs_embeds = self.embed_tokens(input)\n+\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n \n         # initialize `past_key_values`\n         return_legacy_cache = False\n@@ -1272,20 +1353,19 @@ def forward(\n             if isinstance(past_key_values, EncoderDecoderCache)\n             else past_key_values\n         )\n+\n         causal_mask = self._update_causal_mask(\n             attention_mask,\n             inputs_embeds,\n             cache_position,\n             self_attn_cache,\n-            output_attentions,\n         )\n-\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            encoder_attention_mask = _prepare_4d_attention_mask(\n-                encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_length\n-            )\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            input_shape,\n+            inputs_embeds,\n+        )\n \n         # embed positions\n         position_ids = cache_position.unsqueeze(1)"
        },
        {
            "sha": "85813b0242ca477d57ff38dc22c66796b875cf04",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 501,
            "deletions": 466,
            "changes": 967,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -1,3 +1,9 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/plbart/modular_plbart.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_plbart.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n # coding=utf-8\n # Copyright 2022, UCLA NLP, The Facebook AI Research Team and The HuggingFace Inc. team. All rights reserved.\n #\n@@ -12,14 +18,12 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch PLBART model.\"\"\"\n \n import copy\n import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n-import torch.utils.checkpoint\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n@@ -31,6 +35,7 @@\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n )\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -39,47 +44,247 @@\n     Seq2SeqModelOutput,\n     Seq2SeqSequenceClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    auto_docstring,\n-    is_torch_flex_attn_available,\n-    is_torchdynamo_compiling,\n-    logging,\n-)\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n from .configuration_plbart import PLBartConfig\n \n \n if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n+    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n \n \n logger = logging.get_logger(__name__)\n \n \n-# Copied from transformers.models.mbart.modeling_mbart.shift_tokens_right\n-def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int):\n+class PLBartScaledWordEmbedding(nn.Embedding):\n     \"\"\"\n-    Shift input ids one token to the right, and wrap the last non pad token (the <LID> token) Note that MBart does not\n-    have a single `decoder_start_token_id` in contrast to other Bart-like models.\n+    This module overrides nn.Embeddings' forward by multiplying with embeddings scale.\n     \"\"\"\n-    prev_output_tokens = input_ids.clone()\n \n-    if pad_token_id is None:\n-        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n-    # replace possible -100 values in labels by `pad_token_id`\n-    prev_output_tokens.masked_fill_(prev_output_tokens == -100, pad_token_id)\n+    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: Optional[float] = 1.0):\n+        super().__init__(num_embeddings, embedding_dim, padding_idx)\n+        self.embed_scale = embed_scale\n \n-    index_of_eos = (prev_output_tokens.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n-    decoder_start_tokens = prev_output_tokens.gather(1, index_of_eos).squeeze()\n-    prev_output_tokens[:, 1:] = prev_output_tokens[:, :-1].clone()\n-    prev_output_tokens[:, 0] = decoder_start_tokens\n+    def forward(self, input_ids: torch.Tensor):\n+        return super().forward(input_ids) * self.embed_scale\n \n-    return prev_output_tokens\n+\n+@auto_docstring\n+class PLBartPreTrainedModel(PreTrainedModel):\n+    config_class = PLBartConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"PLBartDecoderLayer\", \"PLBartEncoderLayer\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n+\n+    def _init_weights(self, module):\n+        std = self.config.init_std\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+    ):\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n+                        device=attention_mask.device,\n+                    )\n+                )\n+            return attention_mask\n+\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype = input_tensor.dtype\n+        sequence_length = input_tensor.shape[1]\n+        if using_compilable_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartLearnedPositionalEmbedding with Bart->PLBart\n class PLBartLearnedPositionalEmbedding(nn.Embedding):\n     \"\"\"\n     This module learns positional embeddings up to a fixed maximum size.\n@@ -105,21 +310,36 @@ def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0, posi\n         return super().forward(position_ids + self.offset)\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartScaledWordEmbedding with Bart->PLBart\n-class PLBartScaledWordEmbedding(nn.Embedding):\n-    \"\"\"\n-    This module overrides nn.Embeddings' forward by multiplying with embeddings scale.\n-    \"\"\"\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n \n-    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: Optional[float] = 1.0):\n-        super().__init__(num_embeddings, embedding_dim, padding_idx)\n-        self.embed_scale = embed_scale\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n \n-    def forward(self, input_ids: torch.Tensor):\n-        return super().forward(input_ids) * self.embed_scale\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->PLBart\n class PLBartAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -171,17 +391,25 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-        bsz, tgt_len, _ = hidden_states.size()\n+\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n+\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        query_states = query_states * self.scaling\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n@@ -202,8 +430,8 @@ def forward(\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n+            value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n             if past_key_value is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n@@ -215,75 +443,35 @@ def forward(\n                 if is_cross_attention:\n                     past_key_value.is_updated[self.layer_idx] = True\n \n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = query_states.reshape(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n+        )\n \n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights, past_key_value\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->PLBart, BART->PLBART\n class PLBartEncoderLayer(nn.Module):\n     def __init__(self, config: PLBartConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = PLBART_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = PLBartAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -333,320 +521,22 @@ def forward(\n         hidden_states = self.fc2(hidden_states)\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n-        hidden_states = self.final_layer_norm(hidden_states)\n-\n-        if hidden_states.dtype == torch.float16 and (\n-            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n-        ):\n-            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n-            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n-\n-\n-# TODO: Implement attention with SDPA for PLBart.\n-PLBART_ATTENTION_CLASSES = {\"eager\": PLBartAttention}\n-\n-\n-# Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->PLBart, BART->PLBART\n-class PLBartDecoderLayer(nn.Module):\n-    def __init__(self, config: PLBartConfig, layer_idx: Optional[int] = None):\n-        super().__init__()\n-        self.embed_dim = config.d_model\n-\n-        self.self_attn = PLBART_ATTENTION_CLASSES[config._attn_implementation](\n-            embed_dim=self.embed_dim,\n-            num_heads=config.decoder_attention_heads,\n-            dropout=config.attention_dropout,\n-            is_decoder=True,\n-            is_causal=True,\n-            config=config,\n-            layer_idx=layer_idx,\n-        )\n-        self.dropout = config.dropout\n-        self.activation_fn = ACT2FN[config.activation_function]\n-        self.activation_dropout = config.activation_dropout\n-\n-        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n-        self.encoder_attn = PLBART_ATTENTION_CLASSES[config._attn_implementation](\n-            self.embed_dim,\n-            config.decoder_attention_heads,\n-            dropout=config.attention_dropout,\n-            is_decoder=True,\n-            config=config,\n-            layer_idx=layer_idx,\n-        )\n-        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n-        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n-        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n-        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = True,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`): attention mask of size\n-                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            encoder_hidden_states (`torch.FloatTensor`):\n-                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n-            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n-                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n-                `(encoder_attention_heads,)`.\n-            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n-                size `(decoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n-                cache in the correct position and to infer the complete sequence length.\n-        \"\"\"\n-        residual = hidden_states\n-\n-        # Self Attention\n-        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n-            hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n-            attention_mask=attention_mask,\n-            layer_head_mask=layer_head_mask,\n-            output_attentions=output_attentions,\n-            cache_position=cache_position,\n-        )\n-        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n-        hidden_states = residual + hidden_states\n-        hidden_states = self.self_attn_layer_norm(hidden_states)\n-\n-        # Cross-Attention Block\n-        cross_attn_weights = None\n-        if encoder_hidden_states is not None:\n-            residual = hidden_states\n-\n-            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n-                hidden_states=hidden_states,\n-                key_value_states=encoder_hidden_states,\n-                attention_mask=encoder_attention_mask,\n-                layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-            )\n-            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n-            hidden_states = residual + hidden_states\n-            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n-\n-        # Fully Connected\n-        residual = hidden_states\n-        hidden_states = self.activation_fn(self.fc1(hidden_states))\n-        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n-        hidden_states = self.fc2(hidden_states)\n-        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n-        hidden_states = residual + hidden_states\n-        hidden_states = self.final_layer_norm(hidden_states)\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights, cross_attn_weights)\n-\n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n-        return outputs\n-\n-\n-# Copied from transformers.models.bart.modeling_bart.BartClassificationHead with Bart->PLBart\n-class PLBartClassificationHead(nn.Module):\n-    \"\"\"Head for sentence-level classification tasks.\"\"\"\n-\n-    def __init__(\n-        self,\n-        input_dim: int,\n-        inner_dim: int,\n-        num_classes: int,\n-        pooler_dropout: float,\n-    ):\n-        super().__init__()\n-        self.dense = nn.Linear(input_dim, inner_dim)\n-        self.dropout = nn.Dropout(p=pooler_dropout)\n-        self.out_proj = nn.Linear(inner_dim, num_classes)\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        hidden_states = self.dropout(hidden_states)\n-        hidden_states = self.dense(hidden_states)\n-        hidden_states = torch.tanh(hidden_states)\n-        hidden_states = self.dropout(hidden_states)\n-        hidden_states = self.out_proj(hidden_states)\n-        return hidden_states\n-\n-\n-@auto_docstring\n-class PLBartPreTrainedModel(PreTrainedModel):\n-    config_class = PLBartConfig\n-    base_model_prefix = \"model\"\n-    supports_gradient_checkpointing = True\n-    _no_split_modules = [\"PLBartDecoderLayer\", \"PLBartEncoderLayer\"]\n-    _supports_cache_class = True\n-    _supports_static_cache = True\n-\n-    def _init_weights(self, module):\n-        std = self.config.init_std\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+        hidden_states = self.final_layer_norm(hidden_states)\n \n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n+        if hidden_states.dtype == torch.float16 and (\n+            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n+        ):\n+            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n+            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-        return causal_mask\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartEncoder with Bart->PLBart\n class PLBartEncoder(PLBartPreTrainedModel):\n     \"\"\"\n     Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n@@ -680,8 +570,6 @@ def __init__(self, config: PLBartConfig, embed_tokens: Optional[nn.Embedding] =\n             embed_dim,\n         )\n         self.layers = nn.ModuleList([PLBartEncoderLayer(config, layer_idx=i) for i in range(config.encoder_layers)])\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n-        self._use_sdpa = config._attn_implementation == \"sdpa\"\n         self.layernorm_embedding = nn.LayerNorm(embed_dim)\n \n         self.gradient_checkpointing = False\n@@ -767,18 +655,10 @@ def forward(\n         hidden_states = self.layernorm_embedding(hidden_states)\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        # expand attention_mask\n-        if attention_mask is not None:\n-            if self._use_flash_attention_2:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self._use_sdpa and head_mask is None and not output_attentions:\n-                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-                # the manual implementation that requires a 4D causal mask in all cases.\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            inputs_embeds,\n+        )\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -835,7 +715,125 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartDecoder with Bart->PLBart\n+class PLBartDecoderLayer(nn.Module):\n+    def __init__(self, config: PLBartConfig, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.embed_dim = config.d_model\n+\n+        self.self_attn = PLBartAttention(\n+            embed_dim=self.embed_dim,\n+            num_heads=config.decoder_attention_heads,\n+            dropout=config.attention_dropout,\n+            is_decoder=True,\n+            is_causal=True,\n+            config=config,\n+            layer_idx=layer_idx,\n+        )\n+        self.dropout = config.dropout\n+        self.activation_fn = ACT2FN[config.activation_function]\n+        self.activation_dropout = config.activation_dropout\n+\n+        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n+        self.encoder_attn = PLBartAttention(\n+            self.embed_dim,\n+            config.decoder_attention_heads,\n+            dropout=config.attention_dropout,\n+            is_decoder=True,\n+            config=config,\n+            layer_idx=layer_idx,\n+        )\n+        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n+        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n+        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n+        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        layer_head_mask: Optional[torch.Tensor] = None,\n+        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`): attention mask of size\n+                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n+            encoder_hidden_states (`torch.FloatTensor`):\n+                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n+            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n+                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n+            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n+                `(encoder_attention_heads,)`.\n+            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n+                size `(decoder_attention_heads,)`.\n+            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n+        \"\"\"\n+        residual = hidden_states\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n+            hidden_states=hidden_states,\n+            past_key_value=past_key_value,\n+            attention_mask=attention_mask,\n+            layer_head_mask=layer_head_mask,\n+            output_attentions=output_attentions,\n+            cache_position=cache_position,\n+        )\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = residual + hidden_states\n+        hidden_states = self.self_attn_layer_norm(hidden_states)\n+\n+        # Cross-Attention Block\n+        cross_attn_weights = None\n+        if encoder_hidden_states is not None:\n+            residual = hidden_states\n+\n+            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n+                hidden_states=hidden_states,\n+                key_value_states=encoder_hidden_states,\n+                attention_mask=encoder_attention_mask,\n+                layer_head_mask=cross_attn_layer_head_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n+            )\n+            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+            hidden_states = residual + hidden_states\n+            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.activation_fn(self.fc1(hidden_states))\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n+        hidden_states = self.fc2(hidden_states)\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = residual + hidden_states\n+        hidden_states = self.final_layer_norm(hidden_states)\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights, cross_attn_weights)\n+\n+        if use_cache:\n+            outputs += (past_key_value,)\n+\n+        return outputs\n+\n+\n class PLBartDecoder(PLBartPreTrainedModel):\n     \"\"\"\n     Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`PLBartDecoderLayer`]\n@@ -865,8 +863,6 @@ def __init__(self, config: PLBartConfig, embed_tokens: Optional[nn.Embedding] =\n             config.d_model,\n         )\n         self.layers = nn.ModuleList([PLBartDecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)])\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n-        self._use_sdpa = config._attn_implementation == \"sdpa\"\n \n         self.layernorm_embedding = nn.LayerNorm(config.d_model)\n \n@@ -981,12 +977,18 @@ def forward(\n         # retrieve input_ids and inputs_embeds\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-\n-        if input_ids is not None:\n-            input_ids = input_ids.view(-1, input_ids.shape[-1])\n+        elif input_ids is not None:\n+            input = input_ids\n+            input_shape = input.shape\n+            input_ids = input_ids.view(-1, input_shape[-1])\n+        elif inputs_embeds is not None:\n+            input_shape = inputs_embeds.size()[:-1]\n+            input = inputs_embeds[:, :, -1]\n+        else:\n+            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n+            inputs_embeds = self.embed_tokens(input)\n \n         # initialize `past_key_values`\n         return_legacy_cache = False\n@@ -1016,38 +1018,25 @@ def forward(\n             if isinstance(past_key_values, EncoderDecoderCache)\n             else past_key_values\n         )\n-        causal_mask = self._update_causal_mask(\n+\n+        attention_mask = self._update_causal_mask(\n             attention_mask,\n             inputs_embeds,\n             cache_position,\n             self_attn_cache,\n-            output_attentions,\n         )\n-\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self._use_flash_attention_2:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self._use_sdpa and cross_attn_head_mask is None and not output_attentions:\n-                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=seq_length,\n-                )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_length\n-                )\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            input_shape,\n+            inputs_embeds,\n+        )\n \n         # embed positions\n-        position_ids = self.embed_positions(input, past_key_values_length, position_ids=cache_position)\n-        position_ids = position_ids.to(inputs_embeds.device)\n+        positions = self.embed_positions(input, past_key_values_length, position_ids=cache_position)\n+        positions = positions.to(inputs_embeds.device)\n \n-        hidden_states = inputs_embeds + position_ids\n+        hidden_states = inputs_embeds + positions\n         hidden_states = self.layernorm_embedding(hidden_states)\n \n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -1080,7 +1069,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    causal_mask,\n+                    attention_mask,\n                     encoder_hidden_states,\n                     encoder_attention_mask,\n                     head_mask[idx] if head_mask is not None else None,\n@@ -1093,7 +1082,7 @@ def forward(\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=causal_mask,\n+                    attention_mask=attention_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n@@ -1139,6 +1128,26 @@ def forward(\n         )\n \n \n+def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int):\n+    \"\"\"\n+    Shift input ids one token to the right, and wrap the last non pad token (the <LID> token) Note that PLBart does not\n+    have a single `decoder_start_token_id` in contrast to other Bart-like models.\n+    \"\"\"\n+    prev_output_tokens = input_ids.clone()\n+\n+    if pad_token_id is None:\n+        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n+    # replace possible -100 values in labels by `pad_token_id`\n+    prev_output_tokens.masked_fill_(prev_output_tokens == -100, pad_token_id)\n+\n+    index_of_eos = (prev_output_tokens.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n+    decoder_start_tokens = prev_output_tokens.gather(1, index_of_eos).squeeze()\n+    prev_output_tokens[:, 1:] = prev_output_tokens[:, :-1].clone()\n+    prev_output_tokens[:, 0] = decoder_start_tokens\n+\n+    return prev_output_tokens\n+\n+\n @auto_docstring\n class PLBartModel(PLBartPreTrainedModel):\n     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n@@ -1192,7 +1201,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple[torch.Tensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1350,7 +1359,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple[torch.Tensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1469,10 +1478,34 @@ def _reorder_cache(past_key_values, beam_idx):\n         return reordered_past\n \n \n+class PLBartClassificationHead(nn.Module):\n+    \"\"\"Head for sentence-level classification tasks.\"\"\"\n+\n+    def __init__(\n+        self,\n+        input_dim: int,\n+        inner_dim: int,\n+        num_classes: int,\n+        pooler_dropout: float,\n+    ):\n+        super().__init__()\n+        self.dense = nn.Linear(input_dim, inner_dim)\n+        self.dropout = nn.Dropout(p=pooler_dropout)\n+        self.out_proj = nn.Linear(inner_dim, num_classes)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = torch.tanh(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = self.out_proj(hidden_states)\n+        return hidden_states\n+\n+\n @auto_docstring(\n     custom_intro=\"\"\"\n-    PLBart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for code\n-    classification.\n+    PLBart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g.\n+    for GLUE tasks.\n     \"\"\"\n )\n class PLBartForSequenceClassification(PLBartPreTrainedModel):\n@@ -1492,7 +1525,6 @@ def __init__(self, config: PLBartConfig, **kwargs):\n         self.post_init()\n \n     @auto_docstring\n-    # Ignore copy\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1621,7 +1653,6 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartDecoderWrapper with Bart->PLBart\n class PLBartDecoderWrapper(PLBartPreTrainedModel):\n     \"\"\"\n     This wrapper class is a helper class to correctly load pretrained checkpoints when the causal language model is\n@@ -1636,7 +1667,11 @@ def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartForCausalLM with Bart->PLBart, facebook/bart-base->uclanlp/plbart-base\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    PLBART decoder with a language modeling head on top (linear layer with weights tied to the input embeddings).\n+    \"\"\"\n+)\n class PLBartForCausalLM(PLBartPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n "
        },
        {
            "sha": "8a9755b11b179a1b3144207ae3805ccd752c8eea",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "added",
            "additions": 692,
            "deletions": 0,
            "changes": 692,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -0,0 +1,692 @@\n+# coding=utf-8\n+# Copyright 2022, UCLA NLP, The Facebook AI Research Team and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch PLBART model.\"\"\"\n+\n+import math\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+from torch.nn import CrossEntropyLoss\n+\n+from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n+from ...modeling_attn_mask_utils import (\n+    AttentionMaskConverter,\n+    _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n+)\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+    Seq2SeqLMOutput,\n+    Seq2SeqModelOutput,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import auto_docstring, is_torch_flex_attn_available\n+from ..bart.modeling_bart import (\n+    BartClassificationHead,\n+    BartDecoder,\n+    BartEncoder,\n+    BartForCausalLM,\n+    BartScaledWordEmbedding,\n+)\n+from ..bigbird_pegasus.modeling_bigbird_pegasus import BigBirdPegasusForSequenceClassification\n+from ..mbart.modeling_mbart import shift_tokens_right\n+from .configuration_plbart import PLBartConfig\n+\n+\n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n+\n+\n+class PLBartScaledWordEmbedding(BartScaledWordEmbedding):\n+    pass\n+\n+\n+@auto_docstring\n+class PLBartPreTrainedModel(PreTrainedModel):\n+    config_class = PLBartConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"PLBartDecoderLayer\", \"PLBartEncoderLayer\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n+\n+    def _init_weights(self, module):\n+        std = self.config.init_std\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+    ):\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n+                        device=attention_mask.device,\n+                    )\n+                )\n+            return attention_mask\n+\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype = input_tensor.dtype\n+        sequence_length = input_tensor.shape[1]\n+        if using_compilable_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n+\n+class PLBartEncoder(BartEncoder):\n+    pass\n+\n+\n+class PLBartDecoder(BartDecoder):\n+    pass\n+\n+\n+@auto_docstring\n+class PLBartModel(PLBartPreTrainedModel):\n+    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n+\n+    def __init__(self, config: PLBartConfig):\n+        super().__init__(config)\n+\n+        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n+        embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n+        self.shared = PLBartScaledWordEmbedding(vocab_size, config.d_model, padding_idx, embed_scale=embed_scale)\n+\n+        self.encoder = PLBartEncoder(config, self.shared)\n+        self.decoder = PLBartDecoder(config, self.shared)\n+\n+        self.init_weights()\n+\n+    def get_input_embeddings(self):\n+        return self.shared\n+\n+    def set_input_embeddings(self, value):\n+        self.shared = value\n+        self.encoder.embed_tokens = self.shared\n+        self.decoder.embed_tokens = self.shared\n+\n+    def _tie_weights(self):\n+        if self.config.tie_word_embeddings:\n+            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+\n+    def get_encoder(self):\n+        return self.encoder\n+\n+    def get_decoder(self):\n+        return self.decoder\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        decoder_head_mask: Optional[torch.LongTensor] = None,\n+        cross_attn_head_mask: Optional[torch.Tensor] = None,\n+        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple[torch.Tensor], Seq2SeqModelOutput]:\n+        r\"\"\"\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Indices of decoder input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`] or [`PLBartMultiTokenizer`] depending on the checkpoint.\n+            See [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are decoder input IDs?](../glossary#decoder-input-ids)\n+\n+            PLBart uses a specific language id token as the starting token for `decoder_input_ids` generation that\n+            varies according to source and target language, *e.g.* 50003 for *en_XX*, and 50001 for *java*. If\n+            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            For translation and summarization training, `decoder_input_ids` should be provided. If no\n+            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n+            for denoising pre-training following the paper.\n+        decoder_attention_mask (:\n+            obj:*torch.LongTensor* of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior:\n+            generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also be used by default.\n+        cross_attn_head_mask (:\n+            obj:*torch.Tensor* of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify\n+            selected heads of the cross-attention modules in the decoder. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # different to other models, PLBart automatically creates decoder_input_ids from\n+        # input_ids if no decoder_input_ids are provided\n+        if decoder_input_ids is None and decoder_inputs_embeds is None:\n+            decoder_input_ids = shift_tokens_right(input_ids, self.config.pad_token_id)\n+\n+        if encoder_outputs is None:\n+            encoder_outputs = self.encoder(\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                head_mask=head_mask,\n+                inputs_embeds=inputs_embeds,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                return_dict=return_dict,\n+            )\n+        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n+        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n+            encoder_outputs = BaseModelOutput(\n+                last_hidden_state=encoder_outputs[0],\n+                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n+                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n+            )\n+\n+        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        decoder_outputs = self.decoder(\n+            input_ids=decoder_input_ids,\n+            attention_mask=decoder_attention_mask,\n+            encoder_hidden_states=encoder_outputs[0],\n+            encoder_attention_mask=attention_mask,\n+            head_mask=decoder_head_mask,\n+            cross_attn_head_mask=cross_attn_head_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=decoder_inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+        )\n+\n+        if not return_dict:\n+            return decoder_outputs + encoder_outputs\n+\n+        return Seq2SeqModelOutput(\n+            last_hidden_state=decoder_outputs.last_hidden_state,\n+            past_key_values=decoder_outputs.past_key_values,\n+            decoder_hidden_states=decoder_outputs.hidden_states,\n+            decoder_attentions=decoder_outputs.attentions,\n+            cross_attentions=decoder_outputs.cross_attentions,\n+            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n+            encoder_hidden_states=encoder_outputs.hidden_states,\n+            encoder_attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The PLBART Model with a language modeling head. Can be used for code-to-text, text-to-code and code-to-code.\n+    \"\"\"\n+)\n+class PLBartForConditionalGeneration(PLBartPreTrainedModel, GenerationMixin):\n+    base_model_prefix = \"model\"\n+    _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n+    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n+\n+    def __init__(self, config: PLBartConfig):\n+        super().__init__(config)\n+        self.model = PLBartModel(config)\n+        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n+        self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n+\n+        self.init_weights()\n+\n+    def get_encoder(self):\n+        return self.model.get_encoder()\n+\n+    def get_decoder(self):\n+        return self.model.get_decoder()\n+\n+    def resize_token_embeddings(\n+        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n+    ) -> nn.Embedding:\n+        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n+        self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n+        return new_embeddings\n+\n+    def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n+        old_num_tokens = self.final_logits_bias.shape[-1]\n+        if new_num_tokens <= old_num_tokens:\n+            new_bias = self.final_logits_bias[:, :new_num_tokens]\n+        else:\n+            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n+            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n+        self.register_buffer(\"final_logits_bias\", new_bias)\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        decoder_head_mask: Optional[torch.LongTensor] = None,\n+        cross_attn_head_mask: Optional[torch.Tensor] = None,\n+        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple[torch.Tensor], Seq2SeqLMOutput]:\n+        r\"\"\"\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Indices of decoder input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`] or [`PLBartMultiTokenizer`] depending on the checkpoint.\n+            See [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are decoder input IDs?](../glossary#decoder-input-ids)\n+\n+            PLBart uses a specific language id token as the starting token for `decoder_input_ids` generation that\n+            varies according to source and target language, *e.g.* 50003 for *en_XX*, and 50001 for *java*. If\n+            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            For translation and summarization training, `decoder_input_ids` should be provided. If no\n+            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n+            for denoising pre-training following the paper.\n+        decoder_attention_mask (:\n+            obj:*torch.LongTensor* of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior:\n+            generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also be used by default.\n+        cross_attn_head_mask (:\n+            obj:*torch.Tensor* of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify\n+            selected heads of the cross-attention modules in the decoder. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example Mask-filling:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, PLBartForConditionalGeneration\n+\n+        >>> model = PLBartForConditionalGeneration.from_pretrained(\"uclanlp/plbart-base\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n+\n+        >>> # en_XX is the language symbol id <LID> for English\n+        >>> TXT = \"<s> Is 0 the <mask> Fibonacci number ? </s> en_XX\"\n+        >>> input_ids = tokenizer([TXT], add_special_tokens=False, return_tensors=\"pt\").input_ids\n+\n+        >>> logits = model(input_ids).logits\n+        >>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n+        >>> probs = logits[0, masked_index].softmax(dim=0)\n+        >>> values, predictions = probs.topk(5)\n+\n+        >>> tokenizer.decode(predictions).split()\n+        ['first', 'same', 'highest', 'result', 'number']\n+        ```\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if labels is not None:\n+            if decoder_input_ids is None and decoder_inputs_embeds is None:\n+                decoder_input_ids = shift_tokens_right(labels, self.config.pad_token_id)\n+\n+        outputs = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            decoder_input_ids=decoder_input_ids,\n+            encoder_outputs=encoder_outputs,\n+            decoder_attention_mask=decoder_attention_mask,\n+            head_mask=head_mask,\n+            decoder_head_mask=decoder_head_mask,\n+            cross_attn_head_mask=cross_attn_head_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            decoder_inputs_embeds=decoder_inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+        )\n+        lm_logits = self.lm_head(outputs[0])\n+        lm_logits = lm_logits + self.final_logits_bias.to(lm_logits.device)\n+\n+        masked_lm_loss = None\n+        if labels is not None:\n+            loss_fct = CrossEntropyLoss()\n+            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n+\n+        if not return_dict:\n+            output = (lm_logits,) + outputs[1:]\n+            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n+\n+        return Seq2SeqLMOutput(\n+            loss=masked_lm_loss,\n+            logits=lm_logits,\n+            past_key_values=outputs.past_key_values,\n+            decoder_hidden_states=outputs.decoder_hidden_states,\n+            decoder_attentions=outputs.decoder_attentions,\n+            cross_attentions=outputs.cross_attentions,\n+            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n+            encoder_hidden_states=outputs.encoder_hidden_states,\n+            encoder_attentions=outputs.encoder_attentions,\n+        )\n+\n+    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n+        return shift_tokens_right(labels, self.config.pad_token_id)\n+\n+    @staticmethod\n+    def _reorder_cache(past_key_values, beam_idx):\n+        reordered_past = ()\n+        for layer_past in past_key_values:\n+            # cached cross_attention states don't have to be reordered -> they are always the same\n+            reordered_past += (\n+                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n+                + layer_past[2:],\n+            )\n+        return reordered_past\n+\n+\n+class PLBartClassificationHead(BartClassificationHead):\n+    pass\n+\n+\n+class PLBartForSequenceClassification(BigBirdPegasusForSequenceClassification):\n+    def forward(**super_kwargs):\n+        r\"\"\"\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Indices of decoder input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`] or [`PLBartMultiTokenizer`] depending on the checkpoint.\n+            See [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are decoder input IDs?](../glossary#decoder-input-ids)\n+\n+            PLBart uses a specific language id token as the starting token for `decoder_input_ids` generation that\n+            varies according to source and target language, *e.g.* 50003 for *en_XX*, and 50001 for *java*. If\n+            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            For translation and summarization training, `decoder_input_ids` should be provided. If no\n+            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n+            for denoising pre-training following the paper.\n+        decoder_attention_mask (:\n+            obj:*torch.LongTensor* of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior:\n+            generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also be used by default.\n+        cross_attn_head_mask (:\n+            obj:*torch.Tensor* of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify\n+            selected heads of the cross-attention modules in the decoder. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        super().forward(**super_kwargs)\n+\n+\n+class PLBartForCausalLM(BartForCausalLM):\n+    @auto_docstring\n+    def forward(**super_kwargs):\n+        r\"\"\"\n+        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, PLBartForCausalLM\n+\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n+        >>> model = PLBartForCausalLM.from_pretrained(\"uclanlp/plbart-base\", add_cross_attention=False)\n+        >>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n+        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n+        >>> outputs = model(**inputs)\n+\n+        >>> logits = outputs.logits\n+        >>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n+        >>> list(logits.shape) == expected_shape\n+        True\n+        ```\"\"\"\n+        super().forward(**super_kwargs)\n+\n+\n+__all__ = [\n+    \"PLBartForCausalLM\",\n+    \"PLBartForConditionalGeneration\",\n+    \"PLBartForSequenceClassification\",\n+    \"PLBartModel\",\n+    \"PLBartPreTrainedModel\",\n+]"
        },
        {
            "sha": "c58812b58d6bc1ddde5c4049b2f03c1fdbf76223",
            "filename": "src/transformers/models/sew/feature_extractor_sew.py",
            "status": "added",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fsew%2Ffeature_extractor_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fsew%2Ffeature_extractor_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Ffeature_extractor_sew.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -0,0 +1,34 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/sew/modular_sew.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_sew.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2021 ASAPP Inc. and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import warnings\n+\n+from .modeling_sew import SEWFeatureEncoder\n+\n+\n+class SEWFeatureExtractor(SEWFeatureEncoder):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        warnings.warn(\n+            f\"The class `{self.__class__.__name__}` has been depreciated \"\n+            \"and will be removed in Transformers v5. \"\n+            f\"Use `{self.__class__.__bases__[0].__name__}` instead.\",\n+            FutureWarning,\n+        )"
        },
        {
            "sha": "330cd99a7b4e4a2c10bfe0b9324c80449b4bedb7",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 229,
            "deletions": 398,
            "changes": 627,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -1,3 +1,9 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/sew/modular_sew.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_sew.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n # coding=utf-8\n # Copyright 2021 ASAPP Inc. and the HuggingFace Inc. team. All rights reserved.\n #\n@@ -12,160 +18,30 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch SEW model.\"\"\"\n \n import math\n import warnings\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import numpy as np\n import torch\n-import torch.utils.checkpoint\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_sew import SEWConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n \n \n-_HIDDEN_STATES_START_POSITION = 1\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2._compute_mask_indices\n-def _compute_mask_indices(\n-    shape: Tuple[int, int],\n-    mask_prob: float,\n-    mask_length: int,\n-    attention_mask: Optional[torch.LongTensor] = None,\n-    min_masks: int = 0,\n-) -> np.ndarray:\n-    \"\"\"\n-    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n-    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n-    CPU as part of the preprocessing during training.\n-\n-    Args:\n-        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n-               the first element is the batch size and the second element is the length of the axis to span.\n-        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n-                    independently generated mask spans of length `mask_length` is computed by\n-                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n-                    actual percentage will be smaller.\n-        mask_length: size of the mask\n-        min_masks: minimum number of masked spans\n-        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n-                        each batch dimension.\n-    \"\"\"\n-    batch_size, sequence_length = shape\n-\n-    if mask_length < 1:\n-        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n-\n-    if mask_length > sequence_length:\n-        raise ValueError(\n-            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n-            f\" and `sequence_length`: {sequence_length}`\"\n-        )\n-\n-    # epsilon is used for probabilistic rounding\n-    epsilon = np.random.rand(1).item()\n-\n-    def compute_num_masked_span(input_length):\n-        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n-        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n-        num_masked_span = max(num_masked_span, min_masks)\n-\n-        # make sure num masked span <= sequence_length\n-        if num_masked_span * mask_length > sequence_length:\n-            num_masked_span = sequence_length // mask_length\n-\n-        # make sure num_masked span is also <= input_length - (mask_length - 1)\n-        if input_length - (mask_length - 1) < num_masked_span:\n-            num_masked_span = max(input_length - (mask_length - 1), 0)\n-\n-        return num_masked_span\n-\n-    # compute number of masked spans in batch\n-    input_lengths = (\n-        attention_mask.detach().sum(-1).tolist()\n-        if attention_mask is not None\n-        else [sequence_length for _ in range(batch_size)]\n-    )\n-\n-    # SpecAugment mask to fill\n-    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n-    spec_aug_mask_idxs = []\n-\n-    max_num_masked_span = compute_num_masked_span(sequence_length)\n-\n-    if max_num_masked_span == 0:\n-        return spec_aug_mask\n-\n-    for input_length in input_lengths:\n-        # compute num of masked spans for this input\n-        num_masked_span = compute_num_masked_span(input_length)\n-\n-        # get random indices to mask\n-        spec_aug_mask_idx = np.random.choice(\n-            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n-        )\n-\n-        # pick first sampled index that will serve as a dummy index to pad vector\n-        # to ensure same dimension for all batches due to probabilistic rounding\n-        # Picking first sample just pads those vectors twice.\n-        if len(spec_aug_mask_idx) == 0:\n-            # this case can only happen if `input_length` is strictly smaller then\n-            # `sequence_length` in which case the last token has to be a padding\n-            # token which we can use as a dummy mask id\n-            dummy_mask_idx = sequence_length - 1\n-        else:\n-            dummy_mask_idx = spec_aug_mask_idx[0]\n-\n-        spec_aug_mask_idx = np.concatenate(\n-            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n-        )\n-        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n-\n-    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n-\n-    # expand masked indices to masked spans\n-    spec_aug_mask_idxs = np.broadcast_to(\n-        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n-    )\n-    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n-\n-    # add offset to the starting indexes so that indexes now create a span\n-    offsets = np.arange(mask_length)[None, None, :]\n-    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n-        batch_size, max_num_masked_span * mask_length\n-    )\n-    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n-\n-    # ensure that we cannot have indices larger than sequence_length\n-    if spec_aug_mask_idxs.max() > sequence_length - 1:\n-        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n-\n-    # scatter indices to mask\n-    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n-\n-    return spec_aug_mask\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2NoLayerNormConvLayer with Wav2Vec2->SEW\n class SEWNoLayerNormConvLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -187,7 +63,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2LayerNormConvLayer with Wav2Vec2->SEW\n class SEWLayerNormConvLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -215,7 +90,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GroupNormConvLayer with Wav2Vec2->SEW\n class SEWGroupNormConvLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -283,7 +157,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SamePadLayer with Wav2Vec2->SEW\n class SEWSamePadLayer(nn.Module):\n     def __init__(self, num_conv_pos_embeddings):\n         super().__init__()\n@@ -317,7 +190,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureEncoder with Wav2Vec2->SEW\n class SEWFeatureEncoder(nn.Module):\n     \"\"\"Construct the features from raw audio waveform\"\"\"\n \n@@ -362,18 +234,36 @@ def forward(self, input_values):\n         return hidden_states\n \n \n-class SEWFeatureExtractor(SEWFeatureEncoder):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        warnings.warn(\n-            f\"The class `{self.__class__.__name__}` has been depreciated \"\n-            \"and will be removed in Transformers v5. \"\n-            f\"Use `{self.__class__.__bases__[0].__name__}` instead.\",\n-            FutureWarning,\n-        )\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->SEW\n class SEWAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -386,7 +276,6 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[SEWConfig] = None,\n-        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -403,23 +292,12 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n-        self.layer_idx = layer_idx\n-        if layer_idx is None and self.is_decoder:\n-            logger.warning_once(\n-                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n-                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    # Ignore copy\n-    @deprecate_kwarg(\"key_value_states\", version=\"4.55\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55\")\n-    @deprecate_kwarg(\"cache_position\", version=\"4.55\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -428,253 +306,86 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        query_states = query_states * self.scaling\n-\n-        key_states = self.k_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = self.v_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = query_states.reshape(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n+        # if key_value_states are provided this layer is used as a cross-attention layer\n+        # for the decoder\n+        is_cross_attention = key_value_states is not None\n \n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n \n-        return attn_output, attn_weights_reshaped, None\n-\n-\n-# Copied from transformers.models.bart.modeling_bart.BartFlashAttention2 with Bart->SEW\n-class SEWFlashAttention2(SEWAttention):\n-    \"\"\"\n-    SEW flash attention module. This module inherits from `SEWAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    # Ignore copy\n-    @deprecate_kwarg(\"key_value_states\", version=\"4.55\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55\")\n-    @deprecate_kwarg(\"cache_position\", version=\"4.55\")\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # SEWFlashAttention2 attention does not support output_attentions\n-        if output_attentions:\n-            raise ValueError(\n-                \"SEWSdpaAttention2 attention does not support `output_attentions`. \"\n-                \"Use the argument `attn_implementation='eager'` when loading the model.\"\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim)\n-\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-        key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim)\n-        value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim)\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (LlamaRMSNorm handles it correctly)\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n+\n+        # get key, value proj\n+        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n+        # is checking that the `sequence_length` of the `past_key_value` is the same as\n+        # the provided `key_value_states` to support prefix tuning\n+        if (\n+            is_cross_attention\n+            and past_key_value is not None\n+            and past_key_value[0].shape[2] == key_value_states.shape[1]\n+        ):\n+            # reuse k,v, cross_attentions\n+            key_states = past_key_value[0]\n+            value_states = past_key_value[1]\n+        elif is_cross_attention:\n+            # cross_attentions\n+            key_states = self.k_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n+        elif past_key_value is not None:\n+            # reuse k, v, self_attention\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n+            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+        else:\n+            # self_attention\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+\n+        if self.is_decoder:\n+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n+            # Further calls to cross_attention layer can then reuse all cross-attention\n+            # key/value_states (first \"if\" case)\n+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n+            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n+            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n+            # if encoder bi-directional self-attention `past_key_value` is always `None`\n+            past_key_value = (key_states, value_states)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            dropout=self.dropout if self.training else 0.0,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, None, None\n-\n-\n-class SEWSdpaAttention(SEWAttention):\n-    @deprecate_kwarg(\"key_value_states\", version=\"4.55\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55\")\n-    @deprecate_kwarg(\"cache_position\", version=\"4.55\")\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"SEWModel is using SEWSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n-                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                key_value_states=key_value_states,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-        key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        causal_mask = None\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and causal_mask is None and tgt_len > 1 else False\n-\n-        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n-        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.view(bsz, tgt_len, self.embed_dim)\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None, None\n-\n+        return attn_output, attn_weights, past_key_value\n \n-SEW_ATTENTION_CLASSES = {\n-    \"eager\": SEWAttention,\n-    \"sdpa\": SEWSdpaAttention,\n-    \"flash_attention_2\": SEWFlashAttention2,\n-}\n \n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeedForward with Wav2Vec2->SEW\n class SEWFeedForward(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -699,15 +410,15 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderLayer with Wav2Vec2->SEW, WAV2VEC2->SEW\n class SEWEncoderLayer(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.attention = SEW_ATTENTION_CLASSES[config._attn_implementation](\n+        self.attention = SEWAttention(\n             embed_dim=config.hidden_size,\n             num_heads=config.num_attention_heads,\n             dropout=config.attention_dropout,\n             is_decoder=False,\n+            config=config,\n         )\n \n         self.dropout = nn.Dropout(config.hidden_dropout)\n@@ -746,7 +457,6 @@ def __init__(self, config):\n         self.layers = nn.ModuleList([SEWEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n         self.upsample = SEWUpsampling(config)\n         self.gradient_checkpointing = False\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     def forward(\n         self,\n@@ -761,7 +471,7 @@ def forward(\n \n         if attention_mask is not None:\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n-            if self._use_flash_attention_2:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n                 # make sure padded tokens output 0\n                 hidden_states[~expand_attention_mask] = 0.0\n                 # 2d mask is passed through the layers\n@@ -854,6 +564,7 @@ class SEWPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = False  # needs a proper look into the mask creation\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -915,6 +626,125 @@ def _get_feature_vector_attention_mask(self, feature_vector_length: int, attenti\n         return attention_mask\n \n \n+def _compute_mask_indices(\n+    shape: Tuple[int, int],\n+    mask_prob: float,\n+    mask_length: int,\n+    attention_mask: Optional[torch.LongTensor] = None,\n+    min_masks: int = 0,\n+) -> np.ndarray:\n+    \"\"\"\n+    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n+    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n+    CPU as part of the preprocessing during training.\n+\n+    Args:\n+        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n+               the first element is the batch size and the second element is the length of the axis to span.\n+        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n+                    independently generated mask spans of length `mask_length` is computed by\n+                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n+                    actual percentage will be smaller.\n+        mask_length: size of the mask\n+        min_masks: minimum number of masked spans\n+        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n+                        each batch dimension.\n+    \"\"\"\n+    batch_size, sequence_length = shape\n+\n+    if mask_length < 1:\n+        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n+\n+    if mask_length > sequence_length:\n+        raise ValueError(\n+            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n+            f\" and `sequence_length`: {sequence_length}`\"\n+        )\n+\n+    # epsilon is used for probabilistic rounding\n+    epsilon = np.random.rand(1).item()\n+\n+    def compute_num_masked_span(input_length):\n+        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n+        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n+        num_masked_span = max(num_masked_span, min_masks)\n+\n+        # make sure num masked span <= sequence_length\n+        if num_masked_span * mask_length > sequence_length:\n+            num_masked_span = sequence_length // mask_length\n+\n+        # make sure num_masked span is also <= input_length - (mask_length - 1)\n+        if input_length - (mask_length - 1) < num_masked_span:\n+            num_masked_span = max(input_length - (mask_length - 1), 0)\n+\n+        return num_masked_span\n+\n+    # compute number of masked spans in batch\n+    input_lengths = (\n+        attention_mask.detach().sum(-1).tolist()\n+        if attention_mask is not None\n+        else [sequence_length for _ in range(batch_size)]\n+    )\n+\n+    # SpecAugment mask to fill\n+    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n+    spec_aug_mask_idxs = []\n+\n+    max_num_masked_span = compute_num_masked_span(sequence_length)\n+\n+    if max_num_masked_span == 0:\n+        return spec_aug_mask\n+\n+    for input_length in input_lengths:\n+        # compute num of masked spans for this input\n+        num_masked_span = compute_num_masked_span(input_length)\n+\n+        # get random indices to mask\n+        spec_aug_mask_idx = np.random.choice(\n+            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n+        )\n+\n+        # pick first sampled index that will serve as a dummy index to pad vector\n+        # to ensure same dimension for all batches due to probabilistic rounding\n+        # Picking first sample just pads those vectors twice.\n+        if len(spec_aug_mask_idx) == 0:\n+            # this case can only happen if `input_length` is strictly smaller then\n+            # `sequence_length` in which case the last token has to be a padding\n+            # token which we can use as a dummy mask id\n+            dummy_mask_idx = sequence_length - 1\n+        else:\n+            dummy_mask_idx = spec_aug_mask_idx[0]\n+\n+        spec_aug_mask_idx = np.concatenate(\n+            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n+        )\n+        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n+\n+    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n+\n+    # expand masked indices to masked spans\n+    spec_aug_mask_idxs = np.broadcast_to(\n+        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n+\n+    # add offset to the starting indexes so that indexes now create a span\n+    offsets = np.arange(mask_length)[None, None, :]\n+    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n+        batch_size, max_num_masked_span * mask_length\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n+\n+    # ensure that we cannot have indices larger than sequence_length\n+    if spec_aug_mask_idxs.max() > sequence_length - 1:\n+        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n+\n+    # scatter indices to mask\n+    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n+\n+    return spec_aug_mask\n+\n+\n @auto_docstring\n class SEWModel(SEWPreTrainedModel):\n     def __init__(self, config: SEWConfig):\n@@ -1038,12 +868,14 @@ def forward(\n         )\n \n \n+_HIDDEN_STATES_START_POSITION = 1\n+\n+\n @auto_docstring(\n     custom_intro=\"\"\"\n     SEW Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\n     \"\"\"\n )\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC with Wav2Vec2->SEW, wav2vec2->sew, WAV2VEC2->SEW\n class SEWForCTC(SEWPreTrainedModel):\n     def __init__(self, config, target_lang: Optional[str] = None):\n         r\"\"\"\n@@ -1196,11 +1028,10 @@ def forward(\n \n @auto_docstring(\n     custom_intro=\"\"\"\n-    SEW Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like SUPERB\n-    Keyword Spotting.\n+    SEW Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like\n+    SUPERB Keyword Spotting.\n     \"\"\"\n )\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification with Wav2Vec2->SEW, wav2vec2->sew, WAV2VEC2->SEW\n class SEWForSequenceClassification(SEWPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "b3aa3e01b6cd638f239705b2b40760a0896e3d7a",
            "filename": "src/transformers/models/sew/modular_sew.py",
            "status": "added",
            "additions": 469,
            "deletions": 0,
            "changes": 469,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -0,0 +1,469 @@\n+# coding=utf-8\n+# Copyright 2021 ASAPP Inc. and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch SEW model.\"\"\"\n+\n+import math\n+import warnings\n+from typing import Optional, Tuple, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n+from ...modeling_outputs import BaseModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import auto_docstring\n+from ..wav2vec2.modeling_wav2vec2 import (\n+    Wav2Vec2Attention,\n+    Wav2Vec2EncoderLayer,\n+    Wav2Vec2FeatureEncoder,\n+    Wav2Vec2FeedForward,\n+    Wav2Vec2ForCTC,\n+    Wav2Vec2ForSequenceClassification,\n+    Wav2Vec2GroupNormConvLayer,\n+    Wav2Vec2LayerNormConvLayer,\n+    Wav2Vec2NoLayerNormConvLayer,\n+    Wav2Vec2SamePadLayer,\n+    _compute_mask_indices,\n+)\n+from .configuration_sew import SEWConfig\n+\n+\n+_HIDDEN_STATES_START_POSITION = 1\n+\n+\n+class SEWNoLayerNormConvLayer(Wav2Vec2NoLayerNormConvLayer):\n+    pass\n+\n+\n+class SEWLayerNormConvLayer(Wav2Vec2LayerNormConvLayer):\n+    pass\n+\n+\n+class SEWGroupNormConvLayer(Wav2Vec2GroupNormConvLayer):\n+    pass\n+\n+\n+class SEWPositionalConvEmbedding(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.conv = nn.Conv1d(\n+            config.hidden_size,\n+            config.hidden_size,\n+            kernel_size=config.num_conv_pos_embeddings,\n+            padding=config.num_conv_pos_embeddings // 2,\n+            groups=config.num_conv_pos_embedding_groups,\n+            stride=config.squeeze_factor,\n+        )\n+\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n+        if is_deepspeed_zero3_enabled():\n+            import deepspeed\n+\n+            with deepspeed.zero.GatheredParameters(self.conv.weight, modifier_rank=0):\n+                self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n+            if hasattr(self.conv, \"parametrizations\"):\n+                weight_g = self.conv.parametrizations.weight.original0\n+                weight_v = self.conv.parametrizations.weight.original1\n+            else:\n+                weight_g = self.conv.weight_g\n+                weight_v = self.conv.weight_v\n+            deepspeed.zero.register_external_parameter(self, weight_v)\n+            deepspeed.zero.register_external_parameter(self, weight_g)\n+        else:\n+            self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n+\n+        self.padding = SEWSamePadLayer(config.num_conv_pos_embeddings)\n+        self.activation = ACT2FN[config.feat_extract_activation]\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.conv(hidden_states)\n+        hidden_states = self.padding(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+\n+        return hidden_states\n+\n+\n+class SEWSamePadLayer(Wav2Vec2SamePadLayer):\n+    pass\n+\n+\n+class SEWUpsampling(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.projection = nn.Linear(config.hidden_size, config.hidden_size * config.squeeze_factor)\n+        self.activation = ACT2FN[config.feat_extract_activation]\n+        self.squeeze_factor = config.squeeze_factor\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.projection(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+\n+        if self.squeeze_factor > 1:\n+            # transform embedding channels to sequence length\n+            bsz, src_len, src_embed_dim = hidden_states.size()\n+            tgt_len = src_len * self.squeeze_factor\n+            tgt_embed_dim = src_embed_dim // self.squeeze_factor\n+            hidden_states = hidden_states.reshape(bsz, src_len, self.squeeze_factor, tgt_embed_dim)\n+            hidden_states = hidden_states.reshape(bsz, tgt_len, tgt_embed_dim)\n+\n+        return hidden_states\n+\n+\n+class SEWFeatureEncoder(Wav2Vec2FeatureEncoder):\n+    pass\n+\n+\n+class SEWFeatureExtractor(SEWFeatureEncoder):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        warnings.warn(\n+            f\"The class `{self.__class__.__name__}` has been depreciated \"\n+            \"and will be removed in Transformers v5. \"\n+            f\"Use `{self.__class__.__bases__[0].__name__}` instead.\",\n+            FutureWarning,\n+        )\n+\n+\n+class SEWAttention(Wav2Vec2Attention):\n+    pass\n+\n+\n+class SEWFeedForward(Wav2Vec2FeedForward):\n+    pass\n+\n+\n+class SEWEncoderLayer(Wav2Vec2EncoderLayer):\n+    pass\n+\n+\n+class SEWEncoder(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.pos_conv_embed = SEWPositionalConvEmbedding(config)\n+        self.pool = nn.AvgPool1d(config.squeeze_factor, config.squeeze_factor)\n+        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.dropout = nn.Dropout(config.hidden_dropout)\n+        self.layers = nn.ModuleList([SEWEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.upsample = SEWUpsampling(config)\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        hidden_states,\n+        attention_mask=None,\n+        output_attentions=False,\n+        output_hidden_states=False,\n+        return_dict=True,\n+    ):\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+\n+        if attention_mask is not None:\n+            expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                # make sure padded tokens output 0\n+                hidden_states[~expand_attention_mask] = 0.0\n+                # 2d mask is passed through the layers\n+                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n+            else:\n+                # make sure padded tokens output 0\n+                hidden_states[~expand_attention_mask] = 0.0\n+                input_lengths = (attention_mask.long()).sum(-1)\n+                # apply pooling formula to get real output_lengths\n+                output_lengths = input_lengths // self.config.squeeze_factor\n+                max_encoder_length = hidden_states.shape[1] // self.config.squeeze_factor\n+                attention_ids = (\n+                    torch.arange(0, max_encoder_length, device=output_lengths.device)\n+                    .view(1, -1)\n+                    .expand(output_lengths.shape[0], -1)\n+                )\n+                attention_mask = (attention_ids < output_lengths.view(-1, 1)).long()\n+\n+                # extend attention_mask\n+                attention_mask = 1.0 - attention_mask[:, None, None, :].to(dtype=hidden_states.dtype)\n+                attention_mask = attention_mask * torch.finfo(hidden_states.dtype).min\n+                attention_mask = attention_mask.expand(\n+                    attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1]\n+                )\n+\n+        n_input_timesteps = hidden_states.shape[1]\n+\n+        hidden_states = hidden_states.transpose(1, 2)\n+        position_embeddings = self.pos_conv_embed(hidden_states)\n+        pooled_hidden_states = self.pool(hidden_states)\n+        min_length = min(position_embeddings.size(-1), pooled_hidden_states.size(-1))\n+        hidden_states = pooled_hidden_states[..., :min_length] + position_embeddings[..., :min_length]\n+        hidden_states = hidden_states.transpose(1, 2)\n+\n+        hidden_states = self.layer_norm(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n+\n+        for layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n+            dropout_probability = torch.rand([])\n+\n+            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n+                if self.gradient_checkpointing and self.training:\n+                    layer_outputs = self._gradient_checkpointing_func(\n+                        layer.__call__,\n+                        hidden_states,\n+                        attention_mask,\n+                        output_attentions,\n+                    )\n+                else:\n+                    layer_outputs = layer(\n+                        hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n+                    )\n+                hidden_states = layer_outputs[0]\n+\n+            if skip_the_layer:\n+                layer_outputs = (None, None)\n+\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        hidden_states = self.upsample(hidden_states)\n+        if hidden_states.shape[1] < n_input_timesteps:\n+            hidden_states = nn.functional.pad(hidden_states, (0, 0, 0, n_input_timesteps - hidden_states.shape[1]))\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+        )\n+\n+\n+@auto_docstring\n+class SEWPreTrainedModel(PreTrainedModel):\n+    config_class = SEWConfig\n+    base_model_prefix = \"sew\"\n+    main_input_name = \"input_values\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = False  # needs a proper look into the mask creation\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, SEWPositionalConvEmbedding):\n+            nn.init.normal_(\n+                module.conv.weight,\n+                mean=0,\n+                std=2 * math.sqrt(1 / (module.conv.kernel_size[0] * module.conv.in_channels)),\n+            )\n+            nn.init.constant_(module.conv.bias, 0)\n+        elif isinstance(module, nn.Linear):\n+            # Slightly different from the TF version which uses truncated_normal for initialization\n+            # cf https://github.com/pytorch/pytorch/pull/5617\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Conv1d):\n+            if is_deepspeed_zero3_enabled():\n+                import deepspeed\n+\n+                if hasattr(module, \"weight_v\") and hasattr(module, \"weight_g\"):\n+                    with deepspeed.zero.GatheredParameters([module.weight_v, module.weight_g], modifier_rank=0):\n+                        nn.init.kaiming_normal_(module.weight.data)\n+                else:\n+                    with deepspeed.zero.GatheredParameters(module.weight, modifier_rank=0):\n+                        nn.init.kaiming_normal_(module.weight.data)\n+            else:\n+                nn.init.kaiming_normal_(module.weight.data)\n+\n+        if isinstance(module, (nn.Linear, nn.Conv1d)) and module.bias is not None:\n+            module.bias.data.zero_()\n+\n+    def _get_feat_extract_output_lengths(self, input_lengths: Union[torch.LongTensor, int]):\n+        \"\"\"\n+        Computes the output length of the convolutional layers\n+        \"\"\"\n+\n+        def _conv_out_length(input_length, kernel_size, stride):\n+            # 1D convolutional layer output length formula taken\n+            # from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n+            return torch.div(input_length - kernel_size, stride, rounding_mode=\"floor\") + 1\n+\n+        for kernel_size, stride in zip(self.config.conv_kernel, self.config.conv_stride):\n+            input_lengths = _conv_out_length(input_lengths, kernel_size, stride)\n+\n+        return input_lengths\n+\n+    def _get_feature_vector_attention_mask(self, feature_vector_length: int, attention_mask: torch.LongTensor):\n+        output_lengths = self._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)\n+        batch_size = attention_mask.shape[0]\n+\n+        attention_mask = torch.zeros(\n+            (batch_size, feature_vector_length), dtype=attention_mask.dtype, device=attention_mask.device\n+        )\n+        # these two operations makes sure that all values before the output lengths idxs are attended to\n+        attention_mask[(torch.arange(attention_mask.shape[0], device=attention_mask.device), output_lengths - 1)] = 1\n+        attention_mask = attention_mask.flip([-1]).cumsum(-1).flip([-1]).bool()\n+        return attention_mask\n+\n+\n+@auto_docstring\n+class SEWModel(SEWPreTrainedModel):\n+    def __init__(self, config: SEWConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.feature_extractor = SEWFeatureEncoder(config)\n+        self.layer_norm = nn.LayerNorm(config.conv_dim[-1], eps=config.layer_norm_eps)\n+\n+        self.project_features = config.conv_dim[-1] != config.hidden_size\n+        if self.project_features:\n+            self.feature_projection = nn.Linear(config.conv_dim[-1], config.hidden_size)\n+        self.feature_dropout = nn.Dropout(config.feat_proj_dropout)\n+\n+        if config.mask_time_prob > 0.0 or config.mask_feature_prob > 0.0:\n+            self.masked_spec_embed = nn.Parameter(torch.Tensor(config.hidden_size).uniform_())\n+\n+        self.encoder = SEWEncoder(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model._mask_hidden_states\n+    def _mask_hidden_states(\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        mask_time_indices: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+    ):\n+        \"\"\"\n+        Masks extracted features along time axis and/or along feature axis according to\n+        [SpecAugment](https://arxiv.org/abs/1904.08779).\n+        \"\"\"\n+\n+        # `config.apply_spec_augment` can set masking to False\n+        if not getattr(self.config, \"apply_spec_augment\", True):\n+            return hidden_states\n+\n+        # generate indices & apply SpecAugment along time axis\n+        batch_size, sequence_length, hidden_size = hidden_states.size()\n+\n+        if mask_time_indices is not None:\n+            # apply SpecAugment along time axis with given mask_time_indices\n+            hidden_states[mask_time_indices] = self.masked_spec_embed.to(hidden_states.dtype)\n+        elif self.config.mask_time_prob > 0 and self.training:\n+            mask_time_indices = _compute_mask_indices(\n+                (batch_size, sequence_length),\n+                mask_prob=self.config.mask_time_prob,\n+                mask_length=self.config.mask_time_length,\n+                attention_mask=attention_mask,\n+                min_masks=self.config.mask_time_min_masks,\n+            )\n+            mask_time_indices = torch.tensor(mask_time_indices, device=hidden_states.device, dtype=torch.bool)\n+            hidden_states[mask_time_indices] = self.masked_spec_embed.to(hidden_states.dtype)\n+\n+        if self.config.mask_feature_prob > 0 and self.training:\n+            # generate indices & apply SpecAugment along feature axis\n+            mask_feature_indices = _compute_mask_indices(\n+                (batch_size, hidden_size),\n+                mask_prob=self.config.mask_feature_prob,\n+                mask_length=self.config.mask_feature_length,\n+                min_masks=self.config.mask_feature_min_masks,\n+            )\n+            mask_feature_indices = torch.tensor(mask_feature_indices, device=hidden_states.device, dtype=torch.bool)\n+            mask_feature_indices = mask_feature_indices[:, None].expand(-1, sequence_length, -1)\n+            hidden_states[mask_feature_indices] = 0\n+\n+        return hidden_states\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_values: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        mask_time_indices: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutput]:\n+        r\"\"\"\n+        mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices to mask extracted features for contrastive loss. When in training mode, model learns to predict\n+            masked extracted features in *config.proj_codevector_dim* space.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        extract_features = self.feature_extractor(input_values)\n+        extract_features = extract_features.transpose(1, 2)\n+        extract_features = self.layer_norm(extract_features)\n+\n+        if self.project_features:\n+            extract_features = self.feature_projection(extract_features)\n+        hidden_states = self.feature_dropout(extract_features)\n+\n+        if attention_mask is not None:\n+            # compute reduced attention_mask corresponding to feature vectors\n+            attention_mask = self._get_feature_vector_attention_mask(hidden_states.shape[1], attention_mask)\n+\n+        hidden_states = self._mask_hidden_states(hidden_states, mask_time_indices=mask_time_indices)\n+\n+        encoder_outputs = self.encoder(\n+            hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        hidden_states = encoder_outputs[0]\n+\n+        if not return_dict:\n+            return (hidden_states,) + encoder_outputs[1:]\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+class SEWForCTC(Wav2Vec2ForCTC):\n+    pass\n+\n+\n+class SEWForSequenceClassification(Wav2Vec2ForSequenceClassification):\n+    pass\n+\n+\n+__all__ = [\"SEWForCTC\", \"SEWForSequenceClassification\", \"SEWModel\", \"SEWPreTrainedModel\"]"
        },
        {
            "sha": "4acee66424bb6d2eebf6ef696d29d84e7e14c452",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 207,
            "deletions": 94,
            "changes": 301,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -15,29 +15,41 @@\n \"\"\"PyTorch Speech2Text model.\"\"\"\n \n import math\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n+    _prepare_4d_causal_attention_mask,\n+    _prepare_4d_causal_attention_mask_for_sdpa,\n+)\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n     Seq2SeqLMOutput,\n     Seq2SeqModelOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n     auto_docstring,\n+    is_torch_flex_attn_available,\n     logging,\n )\n from .configuration_speech_to_text import Speech2TextConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -161,7 +173,38 @@ def create_position_ids_from_input_ids(\n         return incremental_indices.long() + padding_idx\n \n \n-# Copied from transformers.models.hubert.modeling_hubert.HubertAttention with Hubert->Speech2Text\n+# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Attention with Wav2Vec2->Speech2Text\n class Speech2TextAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -196,9 +239,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -207,17 +247,26 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n \n-        bsz, tgt_len, _ = hidden_states.size()\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n+\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n+\n         # get key, value proj\n         # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n         # is checking that the `sequence_length` of the `past_key_value` is the same as\n@@ -232,18 +281,18 @@ def forward(\n             value_states = past_key_value[1]\n         elif is_cross_attention:\n             # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n+            key_states = self.k_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n         elif past_key_value is not None:\n             # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n             key_states = torch.cat([past_key_value[0], key_states], dim=2)\n             value_states = torch.cat([past_key_value[1], value_states], dim=2)\n         else:\n             # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n         if self.is_decoder:\n             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n@@ -255,72 +304,27 @@ def forward(\n             # if encoder bi-directional self-attention `past_key_value` is always `None`\n             past_key_value = (key_states, value_states)\n \n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n+        )\n \n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n-\n-\n-SPEECH_TO_TEXT_ATTENTION_CLASSES = {\"eager\": Speech2TextAttention}\n+        return attn_output, attn_weights, past_key_value\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->Speech2Text, MBART->SPEECH_TO_TEXT\n@@ -329,7 +333,7 @@ def __init__(self, config: Speech2TextConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = SPEECH_TO_TEXT_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = Speech2TextAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -393,12 +397,13 @@ def forward(\n \n \n # copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->Speech2Text, MBART->SPEECH_TO_TEXT\n+# TODO: change copy when applying cache class\n class Speech2TextDecoderLayer(nn.Module):\n     def __init__(self, config: Speech2TextConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = SPEECH_TO_TEXT_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = Speech2TextAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -411,7 +416,7 @@ def __init__(self, config: Speech2TextConfig):\n         self.activation_dropout = config.activation_dropout\n \n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n-        self.encoder_attn = SPEECH_TO_TEXT_ATTENTION_CLASSES[config._attn_implementation](\n+        self.encoder_attn = Speech2TextAttention(\n             self.embed_dim,\n             config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -423,6 +428,7 @@ def __init__(self, config: Speech2TextConfig):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoderLayer.forward\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -519,6 +525,12 @@ class Speech2TextPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     main_input_name = \"input_features\"\n     supports_gradient_checkpointing = True\n+    # TODO: tests would need a rewrite to check for correct implementation\n+    # Current tests always assume certain inputs to be passed\n+    _supports_flash_attn_2 = False\n+    _supports_sdpa = False\n+    # Compile issues\n+    _supports_flex_attn = False\n \n     def _init_weights(self, module):\n         std = self.config.init_std\n@@ -655,10 +667,10 @@ def forward(\n         hidden_states = inputs_embeds + embed_pos\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        # expand attention_mask\n-        if attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            inputs_embeds,\n+        )\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -713,6 +725,29 @@ def forward(\n             last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n         )\n \n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n \n class Speech2TextDecoder(Speech2TextPreTrainedModel):\n     \"\"\"\n@@ -857,16 +892,18 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n \n-        attention_mask = _prepare_4d_causal_attention_mask(\n-            attention_mask, input_shape, inputs_embeds, past_key_values_length\n+        attention_mask = self._update_causal_mask(\n+            attention_mask,\n+            input_shape,\n+            inputs_embeds,\n+            past_key_values_length,\n+        )\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            input_shape,\n+            inputs_embeds,\n         )\n-\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            encoder_attention_mask = _prepare_4d_attention_mask(\n-                encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-            )\n \n         # embed positions\n         positions = self.embed_positions(input_ids, past_key_values_length=past_key_values_length)\n@@ -963,6 +1000,82 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n+    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+        past_key_values_length: int,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            # 2d mask is passed through the layers\n+            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n+        elif self.config._attn_implementation == \"sdpa\":\n+            # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+            # the manual implementation that requires a 4D causal mask in all cases.\n+            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n+                attention_mask,\n+                input_shape,\n+                inputs_embeds,\n+                past_key_values_length,\n+            )\n+        elif self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_shape),\n+                        device=inputs_embeds.device,\n+                    )\n+                )\n+        else:\n+            # 4d mask is passed through the layers\n+            attention_mask = _prepare_4d_causal_attention_mask(\n+                attention_mask, input_shape, inputs_embeds, past_key_values_length\n+            )\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n @auto_docstring\n class Speech2TextModel(Speech2TextPreTrainedModel):"
        },
        {
            "sha": "3bc19a75b3c380c0eec240dcfa3dff2e05f851c5",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 212,
            "deletions": 89,
            "changes": 301,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -15,28 +15,39 @@\n # limitations under the License.\n \"\"\"PyTorch Time Series Transformer model.\"\"\"\n \n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import numpy as np\n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, EncoderDecoderCache\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n+    _prepare_4d_causal_attention_mask,\n+    _prepare_4d_causal_attention_mask_for_sdpa,\n+)\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n     SampleTSPredictionOutput,\n     Seq2SeqTSModelOutput,\n     Seq2SeqTSPredictionOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n-from ...utils import auto_docstring, logging\n+from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n from .configuration_time_series_transformer import TimeSeriesTransformerConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -265,6 +276,37 @@ def forward(self, x):\n         return self.value_projection(x)\n \n \n+# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->TimeSeriesTransformer\n class TimeSeriesTransformerAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n@@ -317,17 +359,25 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-        bsz, tgt_len, _ = hidden_states.size()\n+\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n+\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        query_states = query_states * self.scaling\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n@@ -348,8 +398,8 @@ def forward(\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n-            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n+            value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n             if past_key_value is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n@@ -361,66 +411,27 @@ def forward(\n                 if is_cross_attention:\n                     past_key_value.is_updated[self.layer_idx] = True\n \n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = query_states.reshape(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n+        )\n \n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights, past_key_value\n \n \n # Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->TimeSeriesTransformer, BART->TIME_SERIES_TRANSFORMER\n@@ -429,7 +440,7 @@ def __init__(self, config: TimeSeriesTransformerConfig, layer_idx: Optional[int]\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = TIME_SERIES_TRANSFORMER_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = TimeSeriesTransformerAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -495,19 +506,13 @@ def forward(\n         return outputs\n \n \n-# TODO: Implement attention with SDPA for TimeSeriesTransformer.\n-TIME_SERIES_TRANSFORMER_ATTENTION_CLASSES = {\n-    \"eager\": TimeSeriesTransformerAttention,\n-}\n-\n-\n # Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->TimeSeriesTransformer, with BART->TIME_SERIES_TRANSFORMER\n class TimeSeriesTransformerDecoderLayer(nn.Module):\n     def __init__(self, config: TimeSeriesTransformerConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = TIME_SERIES_TRANSFORMER_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = TimeSeriesTransformerAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -521,7 +526,7 @@ def __init__(self, config: TimeSeriesTransformerConfig, layer_idx: Optional[int]\n         self.activation_dropout = config.activation_dropout\n \n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n-        self.encoder_attn = TIME_SERIES_TRANSFORMER_ATTENTION_CLASSES[config._attn_implementation](\n+        self.encoder_attn = TimeSeriesTransformerAttention(\n             self.embed_dim,\n             config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -595,6 +600,7 @@ def forward(\n                 layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n@@ -626,6 +632,12 @@ class TimeSeriesTransformerPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     main_input_name = \"past_values\"\n     supports_gradient_checkpointing = True\n+    # TODO: tests would need a rewrite to check for correct implementation\n+    # Current tests always assume certain inputs to be passed\n+    _supports_flash_attn_2 = False\n+    _supports_sdpa = False\n+    # Compile issues\n+    _supports_flex_attn = False\n \n     def _init_weights(self, module):\n         std = self.config.init_std\n@@ -640,6 +652,105 @@ def _init_weights(self, module):\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n \n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+        past_key_values_length: int,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            # 2d mask is passed through the layers\n+            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n+        elif self.config._attn_implementation == \"sdpa\":\n+            # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+            # the manual implementation that requires a 4D causal mask in all cases.\n+            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n+                attention_mask,\n+                input_shape,\n+                inputs_embeds,\n+                past_key_values_length,\n+            )\n+        elif self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            # Other attention flavors support in-built causal (when `mask is None`)\n+            # while we need to create our specific block mask regardless\n+            elif attention_mask is None:\n+                attention_mask = make_flex_block_causal_mask(\n+                    torch.ones(\n+                        size=(input_shape),\n+                        device=inputs_embeds.device,\n+                    )\n+                )\n+        else:\n+            # 4d mask is passed through the layers\n+            attention_mask = _prepare_4d_causal_attention_mask(\n+                attention_mask, input_shape, inputs_embeds, past_key_values_length\n+            )\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n class TimeSeriesTransformerEncoder(TimeSeriesTransformerPreTrainedModel):\n     \"\"\"\n@@ -718,10 +829,10 @@ def forward(\n         hidden_states = self.layernorm_embedding(hidden_states + embed_pos)\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        # expand attention_mask\n-        if attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            inputs_embeds,\n+        )\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -907,16 +1018,18 @@ def forward(\n                 past_key_values_length, past_key_values_length + input_shape[1], device=inputs_embeds.device\n             )\n \n-        attention_mask = _prepare_4d_causal_attention_mask(\n-            attention_mask, input_shape, inputs_embeds, past_key_values_length\n+        attention_mask = self._update_causal_mask(\n+            attention_mask,\n+            input_shape,\n+            inputs_embeds,\n+            past_key_values_length,\n+        )\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            input_shape,\n+            inputs_embeds,\n         )\n-\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            encoder_attention_mask = _prepare_4d_attention_mask(\n-                encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-            )\n \n         hidden_states = self.value_embedding(inputs_embeds)\n         embed_pos = self.embed_positions(inputs_embeds.size(), past_key_values_length=self.config.context_length)\n@@ -966,6 +1079,7 @@ def forward(\n                     None,\n                     output_attentions,\n                     use_cache,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -1333,7 +1447,16 @@ def forward(\n                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n             )\n \n-        dec_input = transformer_inputs[:, self.config.context_length :, ...]\n+        # Avoid empty tensors and instead create a zeroes tensor which\n+        # will be treated the same in torch, i.e. matmul with empty == all 0s\n+        if self.config.context_length >= transformer_inputs.shape[1]:\n+            bsz, _, dim = transformer_inputs.shape\n+            dec_input = torch.zeros(\n+                size=(bsz, 1, dim), device=transformer_inputs.device, dtype=transformer_inputs.dtype\n+            )\n+        else:\n+            dec_input = transformer_inputs[:, self.config.context_length :, ...]\n+\n         decoder_outputs = self.decoder(\n             inputs_embeds=dec_input,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "07ee6608b7eec9a862cebc53736a58f47560700e",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 143,
            "deletions": 330,
            "changes": 473,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -4,10 +4,25 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_unispeech.py file directly. One of our CI enforces this.\n #                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2021 The Fairseq Authors and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n import math\n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -17,21 +32,23 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n     ModelOutput,\n     SequenceClassifierOutput,\n     Wav2Vec2BaseModelOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n from .configuration_unispeech import UniSpeechConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n logger = logging.get_logger(__name__)\n@@ -263,6 +280,36 @@ def forward(self, hidden_states):\n         return hidden_states, norm_hidden_states\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class UniSpeechAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -297,9 +344,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -308,274 +352,26 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n \n-        bsz, tgt_len, _ = hidden_states.size()\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n \n-        # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-        else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped, past_key_value\n-\n-\n-class UniSpeechFlashAttention2(UniSpeechAttention):\n-    \"\"\"\n-    UniSpeech flash attention module. This module inherits from `UniSpeechAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n-        bsz, q_len, _ = hidden_states.size()\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self._reshape(self.q_proj(hidden_states), -1, bsz)\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0].transpose(1, 2)\n-            value_states = past_key_value[1].transpose(1, 2)\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._reshape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0].transpose(1, 2), key_states], dim=1)\n-            value_states = torch.cat([past_key_value[1].transpose(1, 2), value_states], dim=1)\n-        else:\n-            # self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states.transpose(1, 2), value_states.transpose(1, 2))\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value[0].shape[-2]\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (LlamaRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask,\n-            q_len,\n-            dropout=self.dropout if self.training else 0.0,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-        attn_output = self.out_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class UniSpeechSdpaAttention(UniSpeechAttention):\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"UniSpeechModel is using UniSpeechSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n-                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                key_value_states=key_value_states,\n-                past_key_value=past_key_value,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states)\n         # get key, value proj\n         # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n         # is checking that the `sequence_length` of the `past_key_value` is the same as\n@@ -590,18 +386,18 @@ def forward(\n             value_states = past_key_value[1]\n         elif is_cross_attention:\n             # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n+            key_states = self.k_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n         elif past_key_value is not None:\n             # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n             key_states = torch.cat([past_key_value[0], key_states], dim=2)\n             value_states = torch.cat([past_key_value[1], value_states], dim=2)\n         else:\n             # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n         if self.is_decoder:\n             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n@@ -613,39 +409,27 @@ def forward(\n             # if encoder bi-directional self-attention `past_key_value` is always `None`\n             past_key_value = (key_states, value_states)\n \n-        query_states = self._shape(query_states, tgt_len, bsz)\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and attention_mask is None and tgt_len > 1 else False\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n-        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n         )\n \n-        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+        return attn_output, attn_weights, past_key_value\n \n \n class UniSpeechFeedForward(nn.Module):\n@@ -672,21 +456,15 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-UNISPEECH_ATTENTION_CLASSES = {\n-    \"eager\": UniSpeechAttention,\n-    \"sdpa\": UniSpeechSdpaAttention,\n-    \"flash_attention_2\": UniSpeechFlashAttention2,\n-}\n-\n-\n class UniSpeechEncoderLayer(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.attention = UNISPEECH_ATTENTION_CLASSES[config._attn_implementation](\n+        self.attention = UniSpeechAttention(\n             embed_dim=config.hidden_size,\n             num_heads=config.num_attention_heads,\n             dropout=config.attention_dropout,\n             is_decoder=False,\n+            config=config,\n         )\n \n         self.dropout = nn.Dropout(config.hidden_dropout)\n@@ -723,7 +501,6 @@ def __init__(self, config):\n         self.dropout = nn.Dropout(config.hidden_dropout)\n         self.layers = nn.ModuleList([UniSpeechEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     def forward(\n         self,\n@@ -740,16 +517,11 @@ def forward(\n             # make sure padded tokens output 0\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n             hidden_states[~expand_attention_mask] = 0\n-            if self._use_flash_attention_2:\n-                # 2d mask is passed through the layers\n-                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-            else:\n-                # extend attention_mask\n-                attention_mask = 1.0 - attention_mask[:, None, None, :].to(dtype=hidden_states.dtype)\n-                attention_mask = attention_mask * torch.finfo(hidden_states.dtype).min\n-                attention_mask = attention_mask.expand(\n-                    attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1]\n-                )\n+\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            hidden_states,\n+        )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n         hidden_states = hidden_states + position_embeddings\n@@ -798,6 +570,28 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n \n class UniSpeechAttnAdapterLayer(nn.Module):\n     def __init__(self, config):\n@@ -827,11 +621,12 @@ def forward(self, hidden_states: torch.FloatTensor):\n class UniSpeechEncoderLayerStableLayerNorm(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.attention = UNISPEECH_ATTENTION_CLASSES[config._attn_implementation](\n+        self.attention = UniSpeechAttention(\n             embed_dim=config.hidden_size,\n             num_heads=config.num_attention_heads,\n             dropout=config.attention_dropout,\n             is_decoder=False,\n+            config=config,\n         )\n         self.dropout = nn.Dropout(config.hidden_dropout)\n         self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -880,7 +675,6 @@ def __init__(self, config):\n             [UniSpeechEncoderLayerStableLayerNorm(config) for _ in range(config.num_hidden_layers)]\n         )\n         self.gradient_checkpointing = False\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     def forward(\n         self,\n@@ -894,19 +688,14 @@ def forward(\n         all_self_attentions = () if output_attentions else None\n \n         if attention_mask is not None:\n-            # make sure padded tokens are not attended to\n+            # make sure padded tokens output 0\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n-            hidden_states = hidden_states * expand_attention_mask.to(dtype=hidden_states.dtype)\n-            if self._use_flash_attention_2:\n-                # 2d mask is passed through the layers\n-                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-            else:\n-                # extend attention_mask\n-                attention_mask = 1.0 - attention_mask[:, None, None, :].to(dtype=hidden_states.dtype)\n-                attention_mask = attention_mask * torch.finfo(hidden_states.dtype).min\n-                attention_mask = attention_mask.expand(\n-                    attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1]\n-                )\n+            hidden_states[~expand_attention_mask] = 0\n+\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            hidden_states,\n+        )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n         hidden_states = hidden_states + position_embeddings\n@@ -957,6 +746,28 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n \n class UniSpeechGumbelVectorQuantizer(nn.Module):\n     \"\"\"\n@@ -1036,6 +847,8 @@ class UniSpeechPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "795ab859673019f3866c26bfa3245f3e059e69b5",
            "filename": "src/transformers/models/unispeech/modular_unispeech.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -1,3 +1,19 @@\n+# coding=utf-8\n+# Copyright 2021 The Fairseq Authors and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch UniSpeech model.\"\"\"\n+\n import math\n import warnings\n from dataclasses import dataclass\n@@ -135,6 +151,8 @@ class UniSpeechPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "8d9ac9c33fcc4cf5278ea21e4f8888e80d267bb0",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 143,
            "deletions": 330,
            "changes": 473,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -4,10 +4,25 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_unispeech_sat.py file directly. One of our CI enforces this.\n #                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2021 The Fairseq Authors and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n import math\n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -17,7 +32,8 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -27,13 +43,14 @@\n     Wav2Vec2BaseModelOutput,\n     XVectorOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, is_peft_available, logging\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring, is_peft_available, is_torch_flex_attn_available, logging\n from .configuration_unispeech_sat import UniSpeechSatConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n logger = logging.get_logger(__name__)\n@@ -266,6 +283,36 @@ def forward(self, hidden_states):\n         return hidden_states, norm_hidden_states\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class UniSpeechSatAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -300,9 +347,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -311,274 +355,26 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n \n-        bsz, tgt_len, _ = hidden_states.size()\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n \n-        # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-        else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped, past_key_value\n-\n-\n-class UniSpeechSatFlashAttention2(UniSpeechSatAttention):\n-    \"\"\"\n-    UniSpeechSat flash attention module. This module inherits from `UniSpeechSatAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n-        bsz, q_len, _ = hidden_states.size()\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self._reshape(self.q_proj(hidden_states), -1, bsz)\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0].transpose(1, 2)\n-            value_states = past_key_value[1].transpose(1, 2)\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._reshape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0].transpose(1, 2), key_states], dim=1)\n-            value_states = torch.cat([past_key_value[1].transpose(1, 2), value_states], dim=1)\n-        else:\n-            # self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states.transpose(1, 2), value_states.transpose(1, 2))\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value[0].shape[-2]\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (LlamaRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask,\n-            q_len,\n-            dropout=self.dropout if self.training else 0.0,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-        attn_output = self.out_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class UniSpeechSatSdpaAttention(UniSpeechSatAttention):\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"UniSpeechSatModel is using UniSpeechSatSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n-                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                key_value_states=key_value_states,\n-                past_key_value=past_key_value,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states)\n         # get key, value proj\n         # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n         # is checking that the `sequence_length` of the `past_key_value` is the same as\n@@ -593,18 +389,18 @@ def forward(\n             value_states = past_key_value[1]\n         elif is_cross_attention:\n             # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n+            key_states = self.k_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n         elif past_key_value is not None:\n             # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n             key_states = torch.cat([past_key_value[0], key_states], dim=2)\n             value_states = torch.cat([past_key_value[1], value_states], dim=2)\n         else:\n             # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n         if self.is_decoder:\n             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n@@ -616,39 +412,27 @@ def forward(\n             # if encoder bi-directional self-attention `past_key_value` is always `None`\n             past_key_value = (key_states, value_states)\n \n-        query_states = self._shape(query_states, tgt_len, bsz)\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and attention_mask is None and tgt_len > 1 else False\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n-        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n         )\n \n-        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+        return attn_output, attn_weights, past_key_value\n \n \n class UniSpeechSatFeedForward(nn.Module):\n@@ -675,21 +459,15 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-UNISPEECH_SAT_ATTENTION_CLASSES = {\n-    \"eager\": UniSpeechSatAttention,\n-    \"sdpa\": UniSpeechSatSdpaAttention,\n-    \"flash_attention_2\": UniSpeechSatFlashAttention2,\n-}\n-\n-\n class UniSpeechSatEncoderLayer(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.attention = UNISPEECH_SAT_ATTENTION_CLASSES[config._attn_implementation](\n+        self.attention = UniSpeechSatAttention(\n             embed_dim=config.hidden_size,\n             num_heads=config.num_attention_heads,\n             dropout=config.attention_dropout,\n             is_decoder=False,\n+            config=config,\n         )\n \n         self.dropout = nn.Dropout(config.hidden_dropout)\n@@ -726,7 +504,6 @@ def __init__(self, config):\n         self.dropout = nn.Dropout(config.hidden_dropout)\n         self.layers = nn.ModuleList([UniSpeechSatEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     def forward(\n         self,\n@@ -743,16 +520,11 @@ def forward(\n             # make sure padded tokens output 0\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n             hidden_states[~expand_attention_mask] = 0\n-            if self._use_flash_attention_2:\n-                # 2d mask is passed through the layers\n-                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-            else:\n-                # extend attention_mask\n-                attention_mask = 1.0 - attention_mask[:, None, None, :].to(dtype=hidden_states.dtype)\n-                attention_mask = attention_mask * torch.finfo(hidden_states.dtype).min\n-                attention_mask = attention_mask.expand(\n-                    attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1]\n-                )\n+\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            hidden_states,\n+        )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n         hidden_states = hidden_states + position_embeddings\n@@ -801,6 +573,28 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n \n class UniSpeechSatAttnAdapterLayer(nn.Module):\n     def __init__(self, config):\n@@ -830,11 +624,12 @@ def forward(self, hidden_states: torch.FloatTensor):\n class UniSpeechSatEncoderLayerStableLayerNorm(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.attention = UNISPEECH_SAT_ATTENTION_CLASSES[config._attn_implementation](\n+        self.attention = UniSpeechSatAttention(\n             embed_dim=config.hidden_size,\n             num_heads=config.num_attention_heads,\n             dropout=config.attention_dropout,\n             is_decoder=False,\n+            config=config,\n         )\n         self.dropout = nn.Dropout(config.hidden_dropout)\n         self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -883,7 +678,6 @@ def __init__(self, config):\n             [UniSpeechSatEncoderLayerStableLayerNorm(config) for _ in range(config.num_hidden_layers)]\n         )\n         self.gradient_checkpointing = False\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     def forward(\n         self,\n@@ -897,19 +691,14 @@ def forward(\n         all_self_attentions = () if output_attentions else None\n \n         if attention_mask is not None:\n-            # make sure padded tokens are not attended to\n+            # make sure padded tokens output 0\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n-            hidden_states = hidden_states * expand_attention_mask.to(dtype=hidden_states.dtype)\n-            if self._use_flash_attention_2:\n-                # 2d mask is passed through the layers\n-                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-            else:\n-                # extend attention_mask\n-                attention_mask = 1.0 - attention_mask[:, None, None, :].to(dtype=hidden_states.dtype)\n-                attention_mask = attention_mask * torch.finfo(hidden_states.dtype).min\n-                attention_mask = attention_mask.expand(\n-                    attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1]\n-                )\n+            hidden_states[~expand_attention_mask] = 0\n+\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            hidden_states,\n+        )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n         hidden_states = hidden_states + position_embeddings\n@@ -960,6 +749,28 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n \n class UniSpeechSatGumbelVectorQuantizer(nn.Module):\n     \"\"\"\n@@ -1039,6 +850,8 @@ class UniSpeechSatPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "9f9e7d4f3c523502c0c21714cacd851f6137d173",
            "filename": "src/transformers/models/unispeech_sat/modular_unispeech_sat.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -1,3 +1,19 @@\n+# coding=utf-8\n+# Copyright 2021 The Fairseq Authors and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch UniSpeechSat model.\"\"\"\n+\n import math\n import warnings\n from dataclasses import dataclass\n@@ -145,6 +161,8 @@ class UniSpeechSatPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "fb01234e3fed636a69656fae62059085c8fc1c3d",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 134,
            "deletions": 332,
            "changes": 466,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -17,7 +17,7 @@\n import math\n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -28,7 +28,11 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n+)\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -38,14 +42,16 @@\n     Wav2Vec2BaseModelOutput,\n     XVectorOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n     ModelOutput,\n     auto_docstring,\n     cached_file,\n     check_torch_load_is_safe,\n     is_peft_available,\n     is_safetensors_available,\n+    is_torch_flex_attn_available,\n     logging,\n )\n from .configuration_wav2vec2 import Wav2Vec2Config\n@@ -58,8 +64,8 @@\n     from safetensors.torch import load_file as safe_load_file\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n logger = logging.get_logger(__name__)\n@@ -465,7 +471,37 @@ def forward(self, hidden_states):\n         return hidden_states, norm_hidden_states\n \n \n-# Copied from transformers.models.hubert.modeling_hubert.HubertAttention with Hubert->Wav2Vec2\n+# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class Wav2Vec2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -500,9 +536,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -511,276 +544,26 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n \n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-        else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n \n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped, past_key_value\n-\n-\n-# Copied from transformers.models.hubert.modeling_hubert.HubertFlashAttention2 with Hubert->Wav2Vec2\n-class Wav2Vec2FlashAttention2(Wav2Vec2Attention):\n-    \"\"\"\n-    Wav2Vec2 flash attention module. This module inherits from `Wav2Vec2Attention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n-        bsz, q_len, _ = hidden_states.size()\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+        kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n         # get query proj\n-        query_states = self._reshape(self.q_proj(hidden_states), -1, bsz)\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0].transpose(1, 2)\n-            value_states = past_key_value[1].transpose(1, 2)\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._reshape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0].transpose(1, 2), key_states], dim=1)\n-            value_states = torch.cat([past_key_value[1].transpose(1, 2), value_states], dim=1)\n-        else:\n-            # self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states.transpose(1, 2), value_states.transpose(1, 2))\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value[0].shape[-2]\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (LlamaRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask,\n-            q_len,\n-            dropout=self.dropout if self.training else 0.0,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-        attn_output = self.out_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class Wav2Vec2SdpaAttention(Wav2Vec2Attention):\n-    # Copied from transformers.models.hubert.modeling_hubert.HubertSdpaAttention.forward with Hubert->Wav2Vec2\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Wav2Vec2Model is using Wav2Vec2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n-                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                key_value_states=key_value_states,\n-                past_key_value=past_key_value,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n+        query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states)\n         # get key, value proj\n         # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n         # is checking that the `sequence_length` of the `past_key_value` is the same as\n@@ -795,18 +578,18 @@ def forward(\n             value_states = past_key_value[1]\n         elif is_cross_attention:\n             # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n+            key_states = self.k_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n         elif past_key_value is not None:\n             # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n             key_states = torch.cat([past_key_value[0], key_states], dim=2)\n             value_states = torch.cat([past_key_value[1], value_states], dim=2)\n         else:\n             # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n         if self.is_decoder:\n             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n@@ -818,46 +601,27 @@ def forward(\n             # if encoder bi-directional self-attention `past_key_value` is always `None`\n             past_key_value = (key_states, value_states)\n \n-        query_states = self._shape(query_states, tgt_len, bsz)\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and attention_mask is None and tgt_len > 1 else False\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n-        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n         )\n \n-        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n-\n-\n-WAV2VEC2_ATTENTION_CLASSES = {\n-    \"eager\": Wav2Vec2Attention,\n-    \"sdpa\": Wav2Vec2SdpaAttention,\n-    \"flash_attention_2\": Wav2Vec2FlashAttention2,\n-}\n+        return attn_output, attn_weights, past_key_value\n \n \n class Wav2Vec2FeedForward(nn.Module):\n@@ -887,11 +651,12 @@ def forward(self, hidden_states):\n class Wav2Vec2EncoderLayer(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.attention = WAV2VEC2_ATTENTION_CLASSES[config._attn_implementation](\n+        self.attention = Wav2Vec2Attention(\n             embed_dim=config.hidden_size,\n             num_heads=config.num_attention_heads,\n             dropout=config.attention_dropout,\n             is_decoder=False,\n+            config=config,\n         )\n \n         self.dropout = nn.Dropout(config.hidden_dropout)\n@@ -922,11 +687,12 @@ def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n class Wav2Vec2EncoderLayerStableLayerNorm(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.attention = WAV2VEC2_ATTENTION_CLASSES[config._attn_implementation](\n+        self.attention = Wav2Vec2Attention(\n             embed_dim=config.hidden_size,\n             num_heads=config.num_attention_heads,\n             dropout=config.attention_dropout,\n             is_decoder=False,\n+            config=config,\n         )\n         self.dropout = nn.Dropout(config.hidden_dropout)\n         self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -973,7 +739,6 @@ def __init__(self, config):\n         self.dropout = nn.Dropout(config.hidden_dropout)\n         self.layers = nn.ModuleList([Wav2Vec2EncoderLayer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     def forward(\n         self,\n@@ -990,16 +755,11 @@ def forward(\n             # make sure padded tokens output 0\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n             hidden_states[~expand_attention_mask] = 0\n-            if self._use_flash_attention_2:\n-                # 2d mask is passed through the layers\n-                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-            else:\n-                # extend attention_mask\n-                attention_mask = 1.0 - attention_mask[:, None, None, :].to(dtype=hidden_states.dtype)\n-                attention_mask = attention_mask * torch.finfo(hidden_states.dtype).min\n-                attention_mask = attention_mask.expand(\n-                    attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1]\n-                )\n+\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            hidden_states,\n+        )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n         hidden_states = hidden_states + position_embeddings\n@@ -1048,6 +808,29 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n \n class Wav2Vec2EncoderStableLayerNorm(nn.Module):\n     def __init__(self, config):\n@@ -1060,7 +843,6 @@ def __init__(self, config):\n             [Wav2Vec2EncoderLayerStableLayerNorm(config) for _ in range(config.num_hidden_layers)]\n         )\n         self.gradient_checkpointing = False\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     def forward(\n         self,\n@@ -1074,19 +856,14 @@ def forward(\n         all_self_attentions = () if output_attentions else None\n \n         if attention_mask is not None:\n-            # make sure padded tokens are not attended to\n+            # make sure padded tokens output 0\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n-            hidden_states = hidden_states * expand_attention_mask.to(dtype=hidden_states.dtype)\n-            if self._use_flash_attention_2:\n-                # 2d mask is passed through the layers\n-                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-            else:\n-                # extend attention_mask\n-                attention_mask = 1.0 - attention_mask[:, None, None, :].to(dtype=hidden_states.dtype)\n-                attention_mask = attention_mask * torch.finfo(hidden_states.dtype).min\n-                attention_mask = attention_mask.expand(\n-                    attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1]\n-                )\n+            hidden_states[~expand_attention_mask] = 0\n+\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            hidden_states,\n+        )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n         hidden_states = hidden_states + position_embeddings\n@@ -1137,6 +914,29 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n \n class Wav2Vec2GumbelVectorQuantizer(nn.Module):\n     \"\"\"\n@@ -1296,6 +1096,8 @@ class Wav2Vec2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    # Compile issues\n+    _supports_flex_attn = False\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "af02fa91ee1dd3a685a7d38e6c1587c8724f7623",
            "filename": "src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -26,7 +26,11 @@\n     XVectorOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, is_peft_available\n+from ...utils import (\n+    ModelOutput,\n+    auto_docstring,\n+    is_peft_available,\n+)\n from .configuration_wav2vec2_conformer import Wav2Vec2ConformerConfig\n \n "
        },
        {
            "sha": "11670ea7d21bb6c11472c4437e75dc251a2b038f",
            "filename": "src/transformers/models/wavlm/modeling_wavlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -617,6 +617,7 @@ class WavLMPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = False\n     _supports_sdpa = False\n+    _supports_flex_attn = False\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "1ff9d5052c0a1e70250bfdcab863b0bd1a1bbc71",
            "filename": "src/transformers/models/wavlm/modular_wavlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -527,6 +527,7 @@ class WavLMPreTrainedModel(PreTrainedModel, Wav2Vec2PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = False\n     _supports_sdpa = False\n+    _supports_flex_attn = False\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "ef847d60595f89cbfc1c998bc193f82e88e03b63",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -27,7 +27,10 @@\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_flash_attention_utils import (\n+    flash_attn_supports_top_left_mask,\n+    is_flash_attn_available,\n+)\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -259,6 +262,9 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n+        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n+\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -572,7 +578,8 @@ def forward(\n }\n \n \n-# Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->Whisper, MBART->WHISPER\n+# (BC Dep) Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->Whisper, MBART->WHISPER\n+# TODO(vasqu): fix copies when enabling whisper attn interface\n class WhisperEncoderLayer(nn.Module):\n     def __init__(self, config: WhisperConfig):\n         super().__init__()"
        },
        {
            "sha": "e9f3f63c965a2d1249f85eb4108536cc56aad534",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -282,7 +282,7 @@ def __init__(self, config: XGLMConfig):\n         self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    # copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer.forward\n+    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoderLayer.forward\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "57037ee435c7fc1764d3b43e0a01db0270c8d12d",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -1148,6 +1148,10 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n             # enable cache\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n \n+            # force eager attention to support output attentions\n+            if self.has_attentions:\n+                config._attn_implementation = \"eager\"\n+\n             # NOTE: assisted generation only works with cache on at the moment.\n             if not hasattr(config.get_text_config(), \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n@@ -1228,6 +1232,10 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n             # enable cache\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n \n+            # force eager attention to support output attentions\n+            if self.has_attentions:\n+                config._attn_implementation = \"eager\"\n+\n             # NOTE: assisted generation only works with cache on at the moment.\n             if not hasattr(config.get_text_config(), \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n@@ -1282,6 +1290,10 @@ def test_dola_decoding_sample(self):\n             # enable cache if the model is not openai-gpt, xlnet, cpm, or xlm\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n \n+            # force eager attention to support output attentions\n+            if self.has_attentions:\n+                config._attn_implementation = \"eager\"\n+\n             # Encoder-decoder models are not supported\n             if config.is_encoder_decoder:\n                 self.skipTest(\"DoLa is not supported for encoder-decoder models\")\n@@ -1346,6 +1358,10 @@ def test_assisted_decoding_sample(self):\n             # enable cache\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n \n+            # force eager attention to support output attentions\n+            if self.has_attentions:\n+                config._attn_implementation = \"eager\"\n+\n             # NOTE: assisted generation only works with cache on at the moment.\n             if not hasattr(config.get_text_config(), \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")"
        },
        {
            "sha": "ded8d5f0a8e82aeb0bd14a678a4965bb64db9fc1",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -1521,3 +1521,7 @@ def test_decoder_model_attn_mask_past(self):\n     @unittest.skip(reason=\"Decoder cannot keep gradients\")\n     def test_retain_grad_hidden_states_attentions(self):\n         return\n+\n+    @unittest.skip(reason=\"Decoder cannot keep gradients\")\n+    def test_flex_attention_with_grads():\n+        return"
        },
        {
            "sha": "bec16cf5dc13984153351d9ff220c8e840a5f0e6",
            "filename": "tests/models/blenderbot/test_modeling_blenderbot.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -554,3 +554,7 @@ def test_decoder_model_attn_mask_past(self):\n     @unittest.skip(reason=\"decoder cannot keep gradients\")\n     def test_retain_grad_hidden_states_attentions(self):\n         return\n+\n+    @unittest.skip(reason=\"Decoder cannot keep gradients\")\n+    def test_flex_attention_with_grads():\n+        return"
        },
        {
            "sha": "8d75649d8cc152f24284426e00b17e92f105c8f3",
            "filename": "tests/models/blenderbot_small/test_modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -563,3 +563,7 @@ def test_decoder_model_attn_mask_past(self):\n     @unittest.skip(reason=\"decoder cannot keep gradients\")\n     def test_retain_grad_hidden_states_attentions(self):\n         return\n+\n+    @unittest.skip(reason=\"Decoder cannot keep gradients\")\n+    def test_flex_attention_with_grads():\n+        return"
        },
        {
            "sha": "5a8f410a70d7ee61d6538b5e904d4cf8cf26a8de",
            "filename": "tests/models/data2vec/test_modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -420,6 +420,9 @@ def test_retain_grad_hidden_states_attentions(self):\n         config.output_hidden_states = True\n         config.output_attentions = True\n \n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n         # no need to test all models as different heads yield the same functionality\n         model_class = self.all_model_classes[0]\n         model = model_class(config)"
        },
        {
            "sha": "9aceea0359b0563ab38a01dabbbb0dd1d42fda77",
            "filename": "tests/models/encoder_decoder/test_modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -412,6 +412,10 @@ def check_encoder_decoder_model_output_attentions(\n         labels,\n         **kwargs,\n     ):\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+        decoder_config._attn_implementation = \"eager\"\n+\n         # make the decoder inputs a different shape from the encoder inputs to harden the test\n         decoder_input_ids = decoder_input_ids[:, :-1]\n         decoder_attention_mask = decoder_attention_mask[:, :-1]\n@@ -445,6 +449,10 @@ def check_encoder_decoder_model_output_attentions_from_config(\n         # config file. Contrarily to most models, changing the model's config won't work -- the defaults are loaded\n         # from the inner models' configurations.\n \n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+        decoder_config._attn_implementation = \"eager\"\n+\n         decoder_input_ids = decoder_input_ids[:, :-1]\n         decoder_attention_mask = decoder_attention_mask[:, :-1]\n         encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)"
        },
        {
            "sha": "de26f4c7a4e367d8e75a8766d3267c35c8236e1f",
            "filename": "tests/models/hubert/test_modeling_hubert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -370,6 +370,9 @@ def test_retain_grad_hidden_states_attentions(self):\n         config.output_hidden_states = True\n         config.output_attentions = True\n \n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n         # no need to test all models as different heads yield the same functionality\n         model_class = self.all_model_classes[0]\n         model = model_class(config)\n@@ -632,6 +635,9 @@ def test_retain_grad_hidden_states_attentions(self):\n         config.output_hidden_states = True\n         config.output_attentions = True\n \n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n         # no need to test all models as different heads yield the same functionality\n         model_class = self.all_model_classes[0]\n         model = model_class(config)"
        },
        {
            "sha": "ba6e8f9c25eb1bb33d3c4d4a5dda8d00a3d0ff96",
            "filename": "tests/models/m2m_100/test_modeling_m2m_100.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -357,7 +357,8 @@ def test_inference_no_head(self):\n         self.assertEqual(output.shape, expected_shape)\n         # change to expected output here\n         expected_slice = torch.tensor(\n-            [[-0.7780, -0.1676, 0.1038], [-6.7556, -1.3992, 0.0567], [-7.5383, -0.5920, -0.2779]], device=torch_device\n+            [[[-0.7780, -0.1676, 0.1038], [-6.7556, -1.3992, 0.0567], [-7.5383, -0.5920, -0.2779]]],\n+            device=torch_device,\n         )\n         torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n@@ -374,7 +375,8 @@ def test_inference_head(self):\n         self.assertEqual(output.shape, expected_shape)\n         # change to expected output here\n         expected_slice = torch.tensor(\n-            [[-1.0448, -1.0411, 3.7992], [-3.2191, -3.2386, -1.3451], [-3.6210, -3.5993, 0.4925]], device=torch_device\n+            [[[-1.0448, -1.0411, 3.7992], [-3.2191, -3.2386, -1.3451], [-3.6210, -3.5993, 0.4925]]],\n+            device=torch_device,\n         )\n         torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n@@ -426,7 +428,7 @@ def test_flash_attn_2_seq_to_seq_generation(self):\n         Overwriting the common test as the test is flaky on tiny models\n         \"\"\"\n         model = M2M100ForConditionalGeneration.from_pretrained(\n-            \"facebook/m2m100_418M\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\"\n+            \"facebook/m2m100_418M\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16\n         ).to(torch_device)\n \n         tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"fr\", tgt_lang=\"en\")"
        },
        {
            "sha": "ed42b1b29f0058bb1b8256bbabfac6b8ddb1a2dd",
            "filename": "tests/models/marian/test_modeling_marian.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -850,3 +850,7 @@ def test_decoder_model_attn_mask_past(self):\n     @unittest.skip(reason=\"Decoder cannot keep gradients\")\n     def test_retain_grad_hidden_states_attentions(self):\n         return\n+\n+    @unittest.skip(reason=\"Decoder cannot keep gradients\")\n+    def test_flex_attention_with_grads():\n+        return"
        },
        {
            "sha": "4ef22c3c30e0a221677388b90a7af7456119b546",
            "filename": "tests/models/mbart/test_modeling_mbart.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -735,3 +735,7 @@ def test_decoder_model_attn_mask_past(self):\n     @unittest.skip(reason=\"Decoder cannot retain gradients\")\n     def test_retain_grad_hidden_states_attentions(self):\n         return\n+\n+    @unittest.skip(reason=\"Decoder cannot retain gradients\")\n+    def test_flex_attention_with_grads(self):\n+        return"
        },
        {
            "sha": "1a27192506f375eb586591fd203fe2fff7ac84b7",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 11,
            "deletions": 36,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -728,6 +728,9 @@ def check_musicgen_model_output_attentions_from_config(\n     def test_attention_outputs(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n         for model_class in self.all_model_classes:\n             self.check_musicgen_model_output_attentions(model_class, config, **inputs_dict)\n             self.check_musicgen_model_output_attentions_from_config(model_class, config, **inputs_dict)\n@@ -805,6 +808,9 @@ def test_retain_grad_hidden_states_attentions(self):\n         config.text_encoder.output_attentions = True\n         config.decoder.output_attentions = True\n \n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n         # no need to test all models as different heads yield the same functionality\n         model_class = self.all_model_classes[0]\n         model = model_class(config)\n@@ -1036,30 +1042,7 @@ def test_flash_attn_2_inference_equivalence(self):\n     @mark.flash_attn_test\n     @slow\n     def test_flash_attn_2_conversion(self):\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n-                ).to(torch_device)\n-\n-                for _, module in model.named_modules():\n-                    if \"FlashAttention\" in module.__class__.__name__:\n-                        return\n-\n-                self.assertTrue(False, \"FlashAttention2 modules not found in model\")\n+        self.skipTest(reason=\"Musicgen doesn't use the MusicgenFlashAttention2 class method.\")\n \n     @require_torch_sdpa\n     @require_torch_gpu\n@@ -1234,18 +1217,6 @@ def test_sdpa_can_dispatch_composite_models(self):\n                 self.assertTrue(model_eager.decoder.config._attn_implementation == \"eager\")\n                 self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n \n-                for name, submodule in model_eager.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n     def test_requires_grad_with_frozen_encoders(self):\n         config = self.model_tester.get_config()\n         for model_class in self.all_model_classes:\n@@ -1276,6 +1247,10 @@ def test_requires_grad_with_frozen_encoders(self):\n     def test_generation_tester_mixin_inheritance(self):\n         pass\n \n+    @unittest.skip(reason=(\"MusicGen has a set of composite models which might not have SDPA themselves, e.g. T5.\"))\n+    def test_sdpa_can_compile_dynamic(self):\n+        pass\n+\n \n def get_bip_bip(bip_duration=0.125, duration=0.5, sample_rate=32000):\n     \"\"\"Produces a series of 'bip bip' sounds at a given frequency.\"\"\""
        },
        {
            "sha": "abf2edd1ce3e3ce1fd1f05e57fff97fa8c23e675",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 11,
            "deletions": 36,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -731,6 +731,9 @@ def check_musicgen_melody_model_output_attentions_from_config(\n     def test_attention_outputs(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n         for model_class in self.all_model_classes:\n             self.check_musicgen_melody_model_output_attentions(model_class, config, **inputs_dict)\n             self.check_musicgen_melody_model_output_attentions_from_config(model_class, config, **inputs_dict)\n@@ -807,6 +810,9 @@ def test_retain_grad_hidden_states_attentions(self):\n         config.text_encoder.output_attentions = True\n         config.decoder.output_attentions = True\n \n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n         # no need to test all models as different heads yield the same functionality\n         model_class = self.all_model_classes[0]\n         model = model_class(config)\n@@ -1036,30 +1042,7 @@ def test_flash_attn_2_inference_equivalence(self):\n     @mark.flash_attn_test\n     @slow\n     def test_flash_attn_2_conversion(self):\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n-                ).to(torch_device)\n-\n-                for _, module in model.named_modules():\n-                    if \"FlashAttention\" in module.__class__.__name__:\n-                        return\n-\n-                self.assertTrue(False, \"FlashAttention2 modules not found in model\")\n+        self.skipTest(reason=\"MusicgenMelody doesn't use the MusicgenMelodyFlashAttention2 class method.\")\n \n     @require_torch_sdpa\n     @require_torch_gpu\n@@ -1234,18 +1217,6 @@ def test_sdpa_can_dispatch_composite_models(self):\n                 self.assertTrue(model_eager.decoder.config._attn_implementation == \"eager\")\n                 self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n \n-                for name, submodule in model_eager.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n     def test_requires_grad_with_frozen_encoders(self):\n         config = self.model_tester.get_config()\n         for model_class in self.all_model_classes:\n@@ -1276,6 +1247,10 @@ def test_requires_grad_with_frozen_encoders(self):\n     def test_generation_tester_mixin_inheritance(self):\n         pass\n \n+    @unittest.skip(reason=(\"MusicGen has a set of composite models which might not have SDPA themselves, e.g. T5.\"))\n+    def test_sdpa_can_compile_dynamic(self):\n+        pass\n+\n \n # Copied from tests.models.musicgen.test_modeling_musicgen.get_bip_bip\n def get_bip_bip(bip_duration=0.125, duration=0.5, sample_rate=32000):"
        },
        {
            "sha": "9b48a2e30740da54d1df5c0bc6f0786beb7319ce",
            "filename": "tests/models/patchtsmixer/test_modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -480,7 +480,7 @@ def test_pretrain_head(self):\n         )\n         self.assertEqual(output.shape, expected_shape)\n \n-        expected_slice = torch.tensor([[[[-0.9106]],[[1.5326]],[[-0.8245]],[[0.7439]],[[-0.7830]],[[2.6256]],[[-0.6485]],]],device=torch_device)  # fmt: skip\n+        expected_slice = torch.tensor([[[-0.9106]],[[1.5326]],[[-0.8245]],[[0.7439]],[[-0.7830]],[[2.6256]],[[-0.6485]],],device=torch_device)  # fmt: skip\n         torch.testing.assert_close(output[0, :7, :1, :1], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_forecasting_head(self):"
        },
        {
            "sha": "af119c41d335f24cd1099efaed48258bddbc7968",
            "filename": "tests/models/pegasus/test_modeling_pegasus.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -597,3 +597,7 @@ def test_decoder_model_attn_mask_past(self):\n     @unittest.skip(reason=\"Decoder cannot keep gradients\")\n     def test_retain_grad_hidden_states_attentions(self):\n         return\n+\n+    @unittest.skip(reason=\"Decoder cannot keep gradients\")\n+    def test_flex_attention_with_grads():\n+        return"
        },
        {
            "sha": "a6bf913e4c2e5743d152db069b9996acb83a6668",
            "filename": "tests/models/pegasus_x/test_modeling_pegasus_x.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -590,7 +590,7 @@ def test_inference_no_head(self):\n         self.assertEqual(output.shape, expected_shape)\n         # change to expected output here\n         expected_slice = torch.tensor(\n-            [[0.0702, -0.1552, 0.1192], [0.0836, -0.1848, 0.1304], [0.0673, -0.1686, 0.1045]], device=torch_device\n+            [[[0.0702, -0.1552, 0.1192], [0.0836, -0.1848, 0.1304], [0.0673, -0.1686, 0.1045]]], device=torch_device\n         )\n \n         torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n@@ -608,7 +608,8 @@ def test_inference_head(self):\n         self.assertEqual(output.shape, expected_shape)\n         # change to expected output here\n         expected_slice = torch.tensor(\n-            [[0.0, 9.5705185, 1.5897303], [0.0, 9.833374, 1.5828674], [0.0, 10.429961, 1.5643371]], device=torch_device\n+            [[[0.0, 9.5705185, 1.5897303], [0.0, 9.833374, 1.5828674], [0.0, 10.429961, 1.5643371]]],\n+            device=torch_device,\n         )\n         torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n@@ -635,8 +636,7 @@ def test_seq_to_seq_generation(self):\n             batch_input,\n             max_length=512,\n             padding=\"max_length\",\n-            truncation_strategy=\"only_first\",\n-            truncation=True,\n+            truncation=\"only_first\",\n             return_tensors=\"pt\",\n         )\n \n@@ -872,3 +872,7 @@ def test_decoder_model_attn_mask_past(self):\n     @unittest.skip(reason=\"Decoder cannot keep gradients\")\n     def test_retain_grad_hidden_states_attentions(self):\n         return\n+\n+    @unittest.skip(reason=\"Decoder cannot keep gradients\")\n+    def test_flex_attention_with_grads():\n+        return"
        },
        {
            "sha": "17975058283263abe118755d33951706c0c301ae",
            "filename": "tests/models/plbart/test_modeling_plbart.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -677,3 +677,7 @@ def test_decoder_model_attn_mask_past(self):\n     @unittest.skip(reason=\"Decoder cannot keep gradients\")\n     def test_retain_grad_hidden_states_attentions(self):\n         return\n+\n+    @unittest.skip(reason=\"Decoder cannot keep gradients\")\n+    def test_flex_attention_with_grads():\n+        return"
        },
        {
            "sha": "2cab21cf5c92938574a6b3e4ceab042a52266f4c",
            "filename": "tests/models/sew/test_modeling_sew.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -342,6 +342,9 @@ def test_retain_grad_hidden_states_attentions(self):\n         config.output_hidden_states = True\n         config.output_attentions = True\n \n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n         # no need to test all models as different heads yield the same functionality\n         model_class = self.all_model_classes[0]\n         model = model_class(config)"
        },
        {
            "sha": "28cdaf34473ee0e1a77e954727c0ee75220a8d1c",
            "filename": "tests/models/speech_encoder_decoder/test_modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -300,6 +300,10 @@ def check_encoder_decoder_model_output_attentions(\n         input_features=None,\n         **kwargs,\n     ):\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+        decoder_config._attn_implementation = \"eager\"\n+\n         # make the decoder inputs a different shape from the encoder inputs to harden the test\n         decoder_input_ids = decoder_input_ids[:, :-1]\n         decoder_attention_mask = decoder_attention_mask[:, :-1]"
        },
        {
            "sha": "ebc537a4788644d5f910d6dbdf33ed5eb2f5c3d4",
            "filename": "tests/models/unispeech/test_modeling_unispeech.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Funispeech%2Ftest_modeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Funispeech%2Ftest_modeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funispeech%2Ftest_modeling_unispeech.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -383,6 +383,9 @@ def test_retain_grad_hidden_states_attentions(self):\n         config.output_hidden_states = True\n         config.output_attentions = True\n \n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n         # no need to test all models as different heads yield the same functionality\n         model_class = self.all_model_classes[0]\n         model = model_class(config)"
        },
        {
            "sha": "ec438dea96b4ccb0f468f31e3b805ebea12f2663",
            "filename": "tests/models/unispeech_sat/test_modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Funispeech_sat%2Ftest_modeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Funispeech_sat%2Ftest_modeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funispeech_sat%2Ftest_modeling_unispeech_sat.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -423,6 +423,9 @@ def test_retain_grad_hidden_states_attentions(self):\n         config.output_hidden_states = True\n         config.output_attentions = True\n \n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n         # no need to test all models as different heads yield the same functionality\n         model_class = self.all_model_classes[0]\n         model = model_class(config)\n@@ -632,6 +635,9 @@ def test_retain_grad_hidden_states_attentions(self):\n         config.output_hidden_states = True\n         config.output_attentions = True\n \n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n         # no need to test all models as different heads yield the same functionality\n         model_class = self.all_model_classes[0]\n         model = model_class(config)"
        },
        {
            "sha": "ffd08297f147d1cd2c0c87b380d9326266600ea0",
            "filename": "tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -246,6 +246,10 @@ def check_encoder_decoder_model_output_attentions(\n         pixel_values=None,\n         **kwargs,\n     ):\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+        decoder_config._attn_implementation = \"eager\"\n+\n         # make the decoder inputs a different shape from the encoder inputs to harden the test\n         decoder_input_ids = decoder_input_ids[:, :-1]\n         decoder_attention_mask = decoder_attention_mask[:, :-1]\n@@ -480,6 +484,10 @@ def check_encoder_decoder_model_output_attentions(\n         pixel_values=None,\n         **kwargs,\n     ):\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+        decoder_config._attn_implementation = \"eager\"\n+\n         # make the decoder inputs a different shape from the encoder inputs to harden the test\n         decoder_input_ids = decoder_input_ids[:, :-1]\n         decoder_attention_mask = decoder_attention_mask[:, :-1]\n@@ -670,6 +678,10 @@ def check_encoder_decoder_model_output_attentions(\n         pixel_values=None,\n         **kwargs,\n     ):\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+        decoder_config._attn_implementation = \"eager\"\n+\n         # make the decoder inputs a different shape from the encoder inputs to harden the test\n         decoder_input_ids = decoder_input_ids[:, :-1]\n         decoder_attention_mask = decoder_attention_mask[:, :-1]\n@@ -807,6 +819,10 @@ def check_encoder_decoder_model_output_attentions(\n         labels=None,\n         **kwargs,\n     ):\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+        decoder_config._attn_implementation = \"eager\"\n+\n         # make the decoder inputs a different shape from the encoder inputs to harden the test\n         decoder_input_ids = decoder_input_ids[:, :-1]\n         decoder_attention_mask = decoder_attention_mask[:, :-1]\n@@ -929,6 +945,10 @@ def check_encoder_decoder_model_output_attentions(\n         labels=None,\n         **kwargs,\n     ):\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+        decoder_config._attn_implementation = \"eager\"\n+\n         # make the decoder inputs a different shape from the encoder inputs to harden the test\n         decoder_input_ids = decoder_input_ids[:, :-1]\n         decoder_attention_mask = decoder_attention_mask[:, :-1]\n@@ -1047,6 +1067,10 @@ def check_encoder_decoder_model_output_attentions(\n         labels=None,\n         **kwargs,\n     ):\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+        decoder_config._attn_implementation = \"eager\"\n+\n         # make the decoder inputs a different shape from the encoder inputs to harden the test\n         decoder_input_ids = decoder_input_ids[:, :-1]\n         decoder_attention_mask = decoder_attention_mask[:, :-1]"
        },
        {
            "sha": "9597d2e6ef25de9b08af3a52b3153744cf09e457",
            "filename": "tests/models/wav2vec2/test_modeling_wav2vec2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -570,6 +570,9 @@ def test_retain_grad_hidden_states_attentions(self):\n         config.output_hidden_states = True\n         config.output_attentions = True\n \n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n         # no need to test all models as different heads yield the same functionality\n         model_class = self.all_model_classes[0]\n         model = model_class(config)\n@@ -917,6 +920,9 @@ def test_retain_grad_hidden_states_attentions(self):\n         config.output_hidden_states = True\n         config.output_attentions = True\n \n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n         # no need to test all models as different heads yield the same functionality\n         model_class = self.all_model_classes[0]\n         model = model_class(config)"
        },
        {
            "sha": "a85c9e7e6256287b11e66c2b7ad48ca9df83a38f",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 26,
            "deletions": 3,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d95c864a254ae93c6b34d18b3a7e9ec36322549f/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=d95c864a254ae93c6b34d18b3a7e9ec36322549f",
            "patch": "@@ -958,6 +958,8 @@ def test_attention_outputs(self):\n \n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.return_dict = True\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n \n         seq_len = getattr(self.model_tester, \"seq_length\", None)\n         decoder_seq_length = getattr(self.model_tester, \"decoder_seq_length\", seq_len)\n@@ -1106,7 +1108,11 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n         configs_no_init.torchscript = True\n         for model_class in self.all_model_classes:\n             for attn_implementation in [\"eager\", \"sdpa\"]:\n-                if attn_implementation == \"sdpa\" and (not model_class._supports_sdpa or not is_torch_sdpa_available()):\n+                if (\n+                    attn_implementation == \"sdpa\"\n+                    and (not model_class._supports_sdpa or not is_torch_sdpa_available())\n+                    or config.output_attentions\n+                ):\n                     continue\n \n                 configs_no_init._attn_implementation = attn_implementation\n@@ -1708,6 +1714,10 @@ def test_retain_grad_hidden_states_attentions(self):\n         config.output_hidden_states = True\n         config.output_attentions = self.has_attentions\n \n+        # force eager attention to support output attentions\n+        if self.has_attentions:\n+            config._attn_implementation = \"eager\"\n+\n         # no need to test all models as different heads yield the same functionality\n         model_class = self.all_model_classes[0]\n         model = model_class(config)\n@@ -4555,13 +4565,26 @@ def test_flex_attention_with_grads(self):\n             # TODO: raushan, fix for composite models after making VLMs support new attn API\n             if not model_class._supports_flex_attn or self._is_composite:\n                 self.skipTest(reason=\"This model does not support flex attention\")\n+\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             config._attn_implementation = \"flex_attention\"\n-            model = model_class(config).to(device=torch_device, dtype=torch.float16)\n+            # Flex Attention can not use dropout\n+            if hasattr(config, \"attention_dropout\"):\n+                config.attention_dropout = 0\n+            if hasattr(config, \"attention_probs_dropout_prob\"):\n+                config.attention_probs_dropout_prob = 0\n+\n+            model = model_class(config).to(device=torch_device)\n             self.assertTrue(model.config._attn_implementation == \"flex_attention\")\n \n+            # Elaborate workaround for encoder-decoder models as some do not specify their main input\n+            dummy_inputs = {model.main_input_name: inputs_dict[model.main_input_name].to(torch_device)}\n+            if config.is_encoder_decoder:\n+                dummy_inputs[\"decoder_input_ids\"] = inputs_dict[\"decoder_input_ids\"]\n+                dummy_inputs[\"decoder_attention_mask\"] = inputs_dict[\"decoder_attention_mask\"]\n+\n             # If this does not raise an error, the test passes (see https://github.com/huggingface/transformers/pull/35605)\n-            _ = model(inputs_dict[\"input_ids\"].to(torch_device))\n+            _ = model(**dummy_inputs)\n \n     def test_generation_tester_mixin_inheritance(self):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 13967,
        "additions": 8035,
        "deletions": 5932
    }
}