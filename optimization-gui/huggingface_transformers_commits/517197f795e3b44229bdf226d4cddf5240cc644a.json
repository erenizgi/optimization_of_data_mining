{
    "author": "faizan842",
    "message": "Update type hints in tokenization_utils.py to use | syntax (#41713)\n\n* Update type hints to use | syntax for Union types\n\n- Replace Union[str, os.PathLike] with str | os.PathLike\n- Replace Optional[Union[str, dict]] with str | dict | None\n- Keep Union for forward references like 'torch.dtype'\n- Update imports to remove unused Union import where possible\n\nThis modernizes the type hints to use Python 3.10+ syntax while maintaining\ncompatibility with forward references.\n\n* Update type hints in tokenization_utils.py to use | syntax\n\n- Replace Union[AddedToken, str] with AddedToken | str\n- Replace Union[list[str], list[AddedToken]] with list[str] | list[AddedToken]\n- Replace Union[str, list[str]] with str | list[str]\n- Replace Union[int, list[int]] with int | list[int]\n- Update error messages to use | syntax\n- Maintain backward compatibility\n\nThis modernizes the type hints to use Python 3.10+ syntax.\n\n* Fix error message formatting in tokenization_utils.py\n\n- Fix error message to use Union syntax instead of | syntax in string\n- This prevents potential issues with error message formatting\n- Maintains type hint modernization while fixing error messages",
    "sha": "517197f795e3b44229bdf226d4cddf5240cc644a",
    "files": [
        {
            "sha": "a04b6f2f4332d2af0420caec952ba8d50693a05e",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/517197f795e3b44229bdf226d4cddf5240cc644a/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/517197f795e3b44229bdf226d4cddf5240cc644a/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=517197f795e3b44229bdf226d4cddf5240cc644a",
            "patch": "@@ -390,7 +390,7 @@ def _attn_implementation(self):\n         return self._attn_implementation_internal\n \n     @_attn_implementation.setter\n-    def _attn_implementation(self, value: Optional[Union[str, dict]]):\n+    def _attn_implementation(self, value: str | dict | None):\n         \"\"\"We set it recursively on the sub-configs as well\"\"\"\n         # Set if for current config\n         current_attn = getattr(self, \"_attn_implementation\", None)\n@@ -425,7 +425,7 @@ def rope_scaling(self):\n     def rope_scaling(self, value):\n         self.rope_parameters = value\n \n-    def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs):\n+    def save_pretrained(self, save_directory: str | os.PathLike, push_to_hub: bool = False, **kwargs):\n         \"\"\"\n         Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the\n         [`~PreTrainedConfig.from_pretrained`] class method.\n@@ -490,11 +490,11 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n     @classmethod\n     def from_pretrained(\n         cls: type[SpecificPreTrainedConfigType],\n-        pretrained_model_name_or_path: Union[str, os.PathLike],\n-        cache_dir: Optional[Union[str, os.PathLike]] = None,\n+        pretrained_model_name_or_path: str | os.PathLike,\n+        cache_dir: str | os.PathLike | None = None,\n         force_download: bool = False,\n         local_files_only: bool = False,\n-        token: Optional[Union[str, bool]] = None,\n+        token: str | bool | None = None,\n         revision: str = \"main\",\n         **kwargs,\n     ) -> SpecificPreTrainedConfigType:\n@@ -597,7 +597,7 @@ def from_pretrained(\n \n     @classmethod\n     def get_config_dict(\n-        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n+        cls, pretrained_model_name_or_path: str | os.PathLike, **kwargs\n     ) -> tuple[dict[str, Any], dict[str, Any]]:\n         \"\"\"\n         From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a\n@@ -630,7 +630,7 @@ def get_config_dict(\n \n     @classmethod\n     def _get_config_dict(\n-        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n+        cls, pretrained_model_name_or_path: str | os.PathLike, **kwargs\n     ) -> tuple[dict[str, Any], dict[str, Any]]:\n         cache_dir = kwargs.pop(\"cache_dir\", None)\n         force_download = kwargs.pop(\"force_download\", False)\n@@ -793,7 +793,7 @@ def from_dict(\n \n     @classmethod\n     def from_json_file(\n-        cls: type[SpecificPreTrainedConfigType], json_file: Union[str, os.PathLike]\n+        cls: type[SpecificPreTrainedConfigType], json_file: str | os.PathLike\n     ) -> SpecificPreTrainedConfigType:\n         \"\"\"\n         Instantiates a [`PreTrainedConfig`] from the path to a JSON file of parameters.\n@@ -810,7 +810,7 @@ def from_json_file(\n         return cls(**config_dict)\n \n     @classmethod\n-    def _dict_from_json_file(cls, json_file: Union[str, os.PathLike]):\n+    def _dict_from_json_file(cls, json_file: str | os.PathLike):\n         with open(json_file, encoding=\"utf-8\") as reader:\n             text = reader.read()\n         return json.loads(text)\n@@ -935,7 +935,7 @@ def to_json_string(self, use_diff: bool = True) -> str:\n             config_dict = self.to_dict()\n         return json.dumps(config_dict, indent=2, sort_keys=True) + \"\\n\"\n \n-    def to_json_file(self, json_file_path: Union[str, os.PathLike], use_diff: bool = True):\n+    def to_json_file(self, json_file_path: str | os.PathLike, use_diff: bool = True):\n         \"\"\"\n         Save this instance to a JSON file.\n "
        },
        {
            "sha": "c6a66ba1c4b30ab86651bb66d68e2c9181937383",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/517197f795e3b44229bdf226d4cddf5240cc644a/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/517197f795e3b44229bdf226d4cddf5240cc644a/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=517197f795e3b44229bdf226d4cddf5240cc644a",
            "patch": "@@ -14,7 +14,7 @@\n \n import math\n from functools import wraps\n-from typing import Optional, TypedDict, Union\n+from typing import Optional, TypedDict\n \n from .configuration_utils import PreTrainedConfig\n from .utils import is_torch_available, logging\n@@ -27,7 +27,7 @@\n     import torch\n \n \n-def standardize_rope_params(config, rope_theta: Optional[Union[float, dict[str, float]]] = None):\n+def standardize_rope_params(config, rope_theta: float | dict[str, float] | None = None):\n     \"\"\"\n     Helper to standardize the config's rope params field by ensuring the params are defined for each\n     later type. For old model the fn will duplicate a single rope param in each layer type (backward compatibility)"
        },
        {
            "sha": "2abfb9594678606a56e6c0dc16dfe0b1936dc559",
            "filename": "src/transformers/tokenization_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/517197f795e3b44229bdf226d4cddf5240cc644a/src%2Ftransformers%2Ftokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/517197f795e3b44229bdf226d4cddf5240cc644a/src%2Ftransformers%2Ftokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils.py?ref=517197f795e3b44229bdf226d4cddf5240cc644a",
            "patch": "@@ -472,7 +472,7 @@ def added_tokens_decoder(self) -> dict[int, AddedToken]:\n         return dict(sorted(self._added_tokens_decoder.items(), key=lambda item: item[0]))\n \n     @added_tokens_decoder.setter\n-    def added_tokens_decoder(self, value: dict[int, Union[AddedToken, str]]) -> dict[int, AddedToken]:\n+    def added_tokens_decoder(self, value: dict[int, AddedToken | str]) -> dict[int, AddedToken]:\n         # Always raise an error if string because users should define the behavior\n         for index, token in value.items():\n             if not isinstance(token, (str, AddedToken)) or not isinstance(index, int):\n@@ -509,7 +509,7 @@ def _update_total_vocab_size(self):\n         \"\"\"\n         self.total_vocab_size = len(self.get_vocab())\n \n-    def _add_tokens(self, new_tokens: Union[list[str], list[AddedToken]], special_tokens: bool = False) -> int:\n+    def _add_tokens(self, new_tokens: list[str] | list[AddedToken], special_tokens: bool = False) -> int:\n         \"\"\"\n         Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n         it with indices starting from length of the current vocabulary. Special tokens are sometimes already in the\n@@ -707,7 +707,7 @@ def _tokenize(self, text, **kwargs):\n         \"\"\"\n         raise NotImplementedError\n \n-    def convert_tokens_to_ids(self, tokens: Union[str, list[str]]) -> Union[int, list[int]]:\n+    def convert_tokens_to_ids(self, tokens: str | list[str]) -> int | list[int]:\n         \"\"\"\n         Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the\n         vocabulary."
        }
    ],
    "stats": {
        "total": 30,
        "additions": 15,
        "deletions": 15
    }
}