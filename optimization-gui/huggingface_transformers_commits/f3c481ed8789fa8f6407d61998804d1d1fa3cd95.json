{
    "author": "cyyever",
    "message": "Use torch.autocast (#40975)\n\n* Use torch.autocast\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Format code\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "f3c481ed8789fa8f6407d61998804d1d1fa3cd95",
    "files": [
        {
            "sha": "7bc1f0dbdc701991a4109ddbc617eb1b3769c6a1",
            "filename": "src/transformers/models/esm/modeling_esmfold.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f3c481ed8789fa8f6407d61998804d1d1fa3cd95/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f3c481ed8789fa8f6407d61998804d1d1fa3cd95/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py?ref=f3c481ed8789fa8f6407d61998804d1d1fa3cd95",
            "patch": "@@ -293,7 +293,7 @@ def __init__(self, c_in, eps=1e-5):\n     def forward(self, x):\n         d = x.dtype\n         if d is torch.bfloat16 and not is_deepspeed_initialized():\n-            with torch.cuda.amp.autocast(enabled=False):\n+            with torch.autocast(device_type=\"cuda\", enabled=False):\n                 out = nn.functional.layer_norm(x, self.c_in, self.weight.to(dtype=d), self.bias.to(dtype=d), self.eps)\n         else:\n             out = nn.functional.layer_norm(x, self.c_in, self.weight, self.bias, self.eps)\n@@ -308,7 +308,7 @@ def softmax_no_cast(t: torch.Tensor, dim: int = -1) -> torch.Tensor:\n     \"\"\"\n     d = t.dtype\n     if d is torch.bfloat16 and not is_deepspeed_initialized():\n-        with torch.cuda.amp.autocast(enabled=False):\n+        with torch.autocast(device_type=\"cuda\", enabled=False):\n             s = torch.nn.functional.softmax(t, dim=dim)\n     else:\n         s = torch.nn.functional.softmax(t, dim=dim)"
        },
        {
            "sha": "60f1e74eff49dd5c7f02acf62cc80c0094f34d35",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f3c481ed8789fa8f6407d61998804d1d1fa3cd95/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f3c481ed8789fa8f6407d61998804d1d1fa3cd95/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=f3c481ed8789fa8f6407d61998804d1d1fa3cd95",
            "patch": "@@ -23,7 +23,6 @@\n import numpy as np\n import torch\n from torch import Tensor, nn\n-from torch.cuda.amp import autocast\n \n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -322,7 +321,7 @@ def forward(self, masks_queries_logits, class_queries_logits, mask_labels, class\n                 align_corners=False,\n             ).squeeze(1)\n \n-            with autocast(enabled=False):\n+            with torch.autocast(device_type=\"cuda\", enabled=False):\n                 pred_mask = pred_mask.float()\n                 target_mask = target_mask.float()\n "
        },
        {
            "sha": "0cd8fcf8cd141ed9e9944acddc8283bbd7aef866",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f3c481ed8789fa8f6407d61998804d1d1fa3cd95/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f3c481ed8789fa8f6407d61998804d1d1fa3cd95/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=f3c481ed8789fa8f6407d61998804d1d1fa3cd95",
            "patch": "@@ -3971,10 +3971,7 @@ def autocast_smart_context_manager(self, cache_enabled: Optional[bool] = True):\n         arguments, depending on the situation.\n         \"\"\"\n         if self.use_cpu_amp:\n-            # TODO Matt: This syntax is deprecated and the preferred version is\n-            #      torch.amp.autocast(\"cpu\", cache_enabled=cache_enabled, dtype=self.amp_dtype)\n-            #      but this is unavailable on Torch 2.1 or earlier. We can change this when we stop supporting 2.1.\n-            ctx_manager = torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n+            ctx_manager = torch.autocast(device_type=\"cpu\", cache_enabled=cache_enabled, dtype=self.amp_dtype)\n         else:\n             ctx_manager = contextlib.nullcontext()\n "
        }
    ],
    "stats": {
        "total": 12,
        "additions": 4,
        "deletions": 8
    }
}