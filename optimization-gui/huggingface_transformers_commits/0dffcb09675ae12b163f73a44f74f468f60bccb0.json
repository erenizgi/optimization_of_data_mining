{
    "author": "linkedlist771",
    "message": "Fix: reassign in qwen3 moe model (#37848)\n\n* Fix: reassign in qwen3 moe model\n\nFix: reassign in qwen3 moe model\n\n* Remove redundant assignment to self.mlp\n\n* make fix-copies\n\n* Revert unwanted style change\n\n* Revert unwanted style change\n\n---------\n\nCo-authored-by: li.ding <int.li.ding@enflame-tech.com>\nCo-authored-by: Matt <rocketknight1@gmail.com>",
    "sha": "0dffcb09675ae12b163f73a44f74f468f60bccb0",
    "files": [
        {
            "sha": "8c5c8f905bb7d0a9a89d8958bcb2c24288d914e9",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0dffcb09675ae12b163f73a44f74f468f60bccb0/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0dffcb09675ae12b163f73a44f74f468f60bccb0/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=0dffcb09675ae12b163f73a44f74f468f60bccb0",
            "patch": "@@ -322,9 +322,6 @@ def __init__(self, config: Qwen3MoeConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n \n-        self.self_attn = Qwen3MoeAttention(config, layer_idx)\n-        self.mlp = Qwen3MoeMLP(config)\n-\n         self.self_attn = Qwen3MoeAttention(config, layer_idx)\n \n         if (layer_idx not in config.mlp_only_layers) and ("
        },
        {
            "sha": "92bea6ff60df25d778520ee43394a22c7ee84270",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0dffcb09675ae12b163f73a44f74f468f60bccb0/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0dffcb09675ae12b163f73a44f74f468f60bccb0/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=0dffcb09675ae12b163f73a44f74f468f60bccb0",
            "patch": "@@ -132,9 +132,6 @@ def __init__(self, config: Qwen3MoeConfig, layer_idx: int):\n         nn.Module().__init__()\n         self.hidden_size = config.hidden_size\n \n-        self.self_attn = Qwen3MoeAttention(config, layer_idx)\n-        self.mlp = Qwen3MoeMLP(config)\n-\n         self.self_attn = Qwen3MoeAttention(config, layer_idx)\n \n         if (layer_idx not in config.mlp_only_layers) and ("
        }
    ],
    "stats": {
        "total": 6,
        "additions": 0,
        "deletions": 6
    }
}