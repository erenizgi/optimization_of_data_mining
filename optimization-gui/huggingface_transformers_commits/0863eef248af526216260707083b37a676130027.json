{
    "author": "gante",
    "message": "[tests] remove `pt_tf` equivalence tests (#36253)",
    "sha": "0863eef248af526216260707083b37a676130027",
    "files": [
        {
            "sha": "00c213c86ebe7955e5b31db1a793b5f12198c5ae",
            "filename": ".circleci/create_circleci_config.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/.circleci%2Fcreate_circleci_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/.circleci%2Fcreate_circleci_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.circleci%2Fcreate_circleci_config.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -28,7 +28,6 @@\n     \"TRANSFORMERS_IS_CI\": True,\n     \"PYTEST_TIMEOUT\": 120,\n     \"RUN_PIPELINE_TESTS\": False,\n-    \"RUN_PT_TF_CROSS_TESTS\": False,\n     \"RUN_PT_FLAX_CROSS_TESTS\": False,\n }\n # Disable the use of {\"s\": None} as the output is way too long, causing the navigation on CircleCI impractical\n@@ -177,15 +176,6 @@ def job_name(self):\n \n \n # JOBS\n-torch_and_tf_job = CircleCIJob(\n-    \"torch_and_tf\",\n-    docker_image=[{\"image\":\"huggingface/transformers-torch-tf-light\"}],\n-    additional_env={\"RUN_PT_TF_CROSS_TESTS\": True},\n-    marker=\"is_pt_tf_cross_test\",\n-    pytest_options={\"rA\": None, \"durations\": 0},\n-)\n-\n-\n torch_and_flax_job = CircleCIJob(\n     \"torch_and_flax\",\n     additional_env={\"RUN_PT_FLAX_CROSS_TESTS\": True},\n@@ -353,7 +343,7 @@ def job_name(self):\n     pytest_num_workers=1,\n )\n \n-REGULAR_TESTS = [torch_and_tf_job, torch_and_flax_job, torch_job, tf_job, flax_job, hub_job, onnx_job, tokenization_job, processor_job, generate_job, non_model_job] # fmt: skip\n+REGULAR_TESTS = [torch_and_flax_job, torch_job, tf_job, flax_job, hub_job, onnx_job, tokenization_job, processor_job, generate_job, non_model_job] # fmt: skip\n EXAMPLES_TESTS = [examples_torch_job, examples_tensorflow_job]\n PIPELINE_TESTS = [pipelines_torch_job, pipelines_tf_job]\n REPO_UTIL_TESTS = [repo_utils_job]"
        },
        {
            "sha": "5963523fd76c9d5476d034ffe698c87491dc71e1",
            "filename": ".github/workflows/check_failed_model_tests.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fcheck_failed_model_tests.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fcheck_failed_model_tests.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fcheck_failed_model_tests.yml?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -22,7 +22,6 @@ env:\n   HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n   SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n   TF_FORCE_GPU_ALLOW_GROWTH: true\n-  RUN_PT_TF_CROSS_TESTS: 1\n   CUDA_VISIBLE_DEVICES: 0,1\n \n "
        },
        {
            "sha": "0997a1112ad7da0b66e1b37314deee4465686a5a",
            "filename": ".github/workflows/model_jobs.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fmodel_jobs.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fmodel_jobs.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fmodel_jobs.yml?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -30,7 +30,6 @@ env:\n   HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n   SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n   TF_FORCE_GPU_ALLOW_GROWTH: true\n-  RUN_PT_TF_CROSS_TESTS: 1\n   CUDA_VISIBLE_DEVICES: 0,1\n \n jobs:"
        },
        {
            "sha": "c90181ec6f1b6df86a90f9215ff13e64e7a2f781",
            "filename": ".github/workflows/model_jobs_amd.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fmodel_jobs_amd.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fmodel_jobs_amd.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fmodel_jobs_amd.yml?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -30,7 +30,6 @@ env:\n   HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n   SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n   TF_FORCE_GPU_ALLOW_GROWTH: true\n-  RUN_PT_TF_CROSS_TESTS: 1\n   CUDA_VISIBLE_DEVICES: 0,1\n \n jobs:"
        },
        {
            "sha": "6a5f5acbce0d2aeb42ed8a304dddf8914e65d55e",
            "filename": ".github/workflows/push-important-models.yml",
            "status": "modified",
            "additions": 21,
            "deletions": 22,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fpush-important-models.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fpush-important-models.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fpush-important-models.yml?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -7,14 +7,13 @@ on:\n env:\n   OUTPUT_SLACK_CHANNEL_ID: \"C06L2SGMEEA\"\n   HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n-  HF_HOME: /mnt/cache \n-  TRANSFORMERS_IS_CI: yes \n-  OMP_NUM_THREADS: 8 \n-  MKL_NUM_THREADS: 8 \n-  RUN_SLOW: yes # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access. # This token is created under the bot `hf-transformers-bot`. \n-  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }} \n-  TF_FORCE_GPU_ALLOW_GROWTH: true \n-  RUN_PT_TF_CROSS_TESTS: 1\n+  HF_HOME: /mnt/cache\n+  TRANSFORMERS_IS_CI: yes\n+  OMP_NUM_THREADS: 8\n+  MKL_NUM_THREADS: 8\n+  RUN_SLOW: yes # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access. # This token is created under the bot `hf-transformers-bot`.\n+  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n+  TF_FORCE_GPU_ALLOW_GROWTH: true\n \n jobs:\n   get_modified_models:\n@@ -25,13 +24,13 @@ jobs:\n     steps:\n       - name: Check out code\n         uses: actions/checkout@v4\n-      \n+\n       - name: Get changed files\n         id: changed-files\n         uses: tj-actions/changed-files@3f54ebb830831fc121d3263c1857cfbdc310cdb9 #v42\n         with:\n           files: src/transformers/models/**\n-      \n+\n       - name: Run step if only the files listed above change\n         if: steps.changed-files.outputs.any_changed == 'true'\n         id: set-matrix\n@@ -60,41 +59,41 @@ jobs:\n     if: ${{ needs.get_modified_models.outputs.matrix != '[]' && needs.get_modified_models.outputs.matrix != '' && fromJson(needs.get_modified_models.outputs.matrix)[0] != null }}\n     strategy:\n       fail-fast: false\n-      matrix: \n+      matrix:\n         model-name: ${{ fromJson(needs.get_modified_models.outputs.matrix) }}\n \n     steps:\n       - name: Check out code\n         uses: actions/checkout@v4\n-      \n+\n       - name: Install locally transformers & other libs\n         run: |\n           apt install sudo\n           sudo -H pip install --upgrade pip\n-          sudo -H pip uninstall -y transformers \n-          sudo -H pip install -U -e \".[testing]\" \n+          sudo -H pip uninstall -y transformers\n+          sudo -H pip install -U -e \".[testing]\"\n           MAX_JOBS=4 pip install flash-attn --no-build-isolation\n           pip install bitsandbytes\n-      \n+\n       - name: NVIDIA-SMI\n         run: |\n           nvidia-smi\n-      \n+\n       - name: Show installed libraries and their versions\n         run: pip freeze\n-      \n+\n       - name: Run FA2 tests\n         id: run_fa2_tests\n         run:\n           pytest -rsfE -m \"flash_attn_test\" --make-reports=${{ matrix.model-name }}_fa2_tests/ tests/${{ matrix.model-name }}/test_modeling_*\n-      \n+\n       - name: \"Test suite reports artifacts: ${{ matrix.model-name }}_fa2_tests\"\n         if: ${{ always() }}\n         uses: actions/upload-artifact@v4\n         with:\n           name: ${{ matrix.model-name }}_fa2_tests\n           path: /transformers/reports/${{ matrix.model-name }}_fa2_tests\n-      \n+\n       - name: Post to Slack\n         if: always()\n         uses: huggingface/hf-workflows/.github/actions/post-slack@main\n@@ -103,13 +102,13 @@ jobs:\n           title: ğŸ¤— Results of the FA2 tests - ${{ matrix.model-name }}\n           status: ${{ steps.run_fa2_tests.conclusion}}\n           slack_token: ${{ secrets.CI_SLACK_BOT_TOKEN }}\n-      \n+\n       - name: Run integration tests\n         id: run_integration_tests\n         if: always()\n         run:\n           pytest -rsfE -k \"IntegrationTest\"  --make-reports=tests_integration_${{ matrix.model-name }} tests/${{ matrix.model-name }}/test_modeling_*\n-      \n+\n       - name: \"Test suite reports artifacts: tests_integration_${{ matrix.model-name }}\"\n         if: ${{ always() }}\n         uses: actions/upload-artifact@v4\n@@ -119,7 +118,7 @@ jobs:\n \n       - name: Post to Slack\n         if: always()\n-        uses: huggingface/hf-workflows/.github/actions/post-slack@main \n+        uses: huggingface/hf-workflows/.github/actions/post-slack@main\n         with:\n           slack_channel: ${{ env.OUTPUT_SLACK_CHANNEL_ID }}\n           title: ğŸ¤— Results of the Integration tests - ${{ matrix.model-name }}"
        },
        {
            "sha": "a3df2ea97c24a9b5c58073e1d8d6ee854dd7fa97",
            "filename": ".github/workflows/self-comment-ci.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fself-comment-ci.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fself-comment-ci.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-comment-ci.yml?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -22,7 +22,6 @@ env:\n   HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n   SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n   TF_FORCE_GPU_ALLOW_GROWTH: true\n-  RUN_PT_TF_CROSS_TESTS: 1\n   CUDA_VISIBLE_DEVICES: 0,1\n \n jobs:"
        },
        {
            "sha": "621061988949d3f57aa47b2a3740603c86800733",
            "filename": ".github/workflows/self-push-amd.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fself-push-amd.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fself-push-amd.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-push-amd.yml?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -14,7 +14,6 @@ env:\n   MKL_NUM_THREADS: 8\n   PYTEST_TIMEOUT: 60\n   TF_FORCE_GPU_ALLOW_GROWTH: true\n-  RUN_PT_TF_CROSS_TESTS: 1\n   HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n \n jobs:"
        },
        {
            "sha": "3b3be41e3e9b561b0b038ae4ac74c74d9ca5d4ce",
            "filename": ".github/workflows/self-push.yml",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fself-push.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fself-push.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-push.yml?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -24,7 +24,6 @@ env:\n   MKL_NUM_THREADS: 8\n   PYTEST_TIMEOUT: 60\n   TF_FORCE_GPU_ALLOW_GROWTH: true\n-  RUN_PT_TF_CROSS_TESTS: 1\n   CUDA_VISIBLE_DEVICES: 0,1\n \n jobs:\n@@ -293,7 +292,7 @@ jobs:\n \n           echo \"$machine_type\"\n           echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-          \n+\n       - name: Update clone using environment variables\n         working-directory: /transformers\n         run: |\n@@ -406,7 +405,7 @@ jobs:\n \n           echo \"$machine_type\"\n           echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-          \n+\n       - name: Update clone using environment variables\n         working-directory: /workspace/transformers\n         run: |\n@@ -516,7 +515,7 @@ jobs:\n \n           echo \"$machine_type\"\n           echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-          \n+\n       - name: Update clone using environment variables\n         working-directory: /workspace/transformers\n         run: |\n@@ -648,6 +647,6 @@ jobs:\n         # `models/bert` to `models_bert` is required, as the artifact names use `_` instead of `/`.\n         run: |\n           pip install huggingface_hub\n-          pip install slack_sdk \n+          pip install slack_sdk\n           pip show slack_sdk\n           python utils/notification_service.py \"${{ needs.setup.outputs.matrix }}\""
        },
        {
            "sha": "78971820d146de19a345362e8b05c377206a724d",
            "filename": ".github/workflows/self-scheduled.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fself-scheduled.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fself-scheduled.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled.yml?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -40,7 +40,6 @@ env:\n   HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n   SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n   TF_FORCE_GPU_ALLOW_GROWTH: true\n-  RUN_PT_TF_CROSS_TESTS: 1\n   CUDA_VISIBLE_DEVICES: 0,1\n   NUM_SLICES: 2\n \n@@ -571,4 +570,4 @@ jobs:\n     with:\n       docker: ${{ inputs.docker }}\n       start_sha: ${{ github.sha }}\n-    secrets: inherit\n\\ No newline at end of file\n+    secrets: inherit"
        },
        {
            "sha": "e648883f191e0da240c5e72fc09496564b0a2891",
            "filename": ".github/workflows/ssh-runner.yml",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fssh-runner.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/.github%2Fworkflows%2Fssh-runner.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fssh-runner.yml?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -5,7 +5,7 @@ on:\n     inputs:\n       runner_type:\n         description: 'Type of runner to test (a10 or t4)'\n-        required: true \n+        required: true\n       docker_image:\n         description: 'Name of the Docker image'\n         required: true\n@@ -15,15 +15,14 @@ on:\n \n env:\n   HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n-  HF_HOME: /mnt/cache \n-  TRANSFORMERS_IS_CI: yes \n-  OMP_NUM_THREADS: 8 \n-  MKL_NUM_THREADS: 8 \n-  RUN_SLOW: yes # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access. # This token is created under the bot `hf-transformers-bot`. \n-  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }} \n-  TF_FORCE_GPU_ALLOW_GROWTH: true \n+  HF_HOME: /mnt/cache\n+  TRANSFORMERS_IS_CI: yes\n+  OMP_NUM_THREADS: 8\n+  MKL_NUM_THREADS: 8\n+  RUN_SLOW: yes # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access. # This token is created under the bot `hf-transformers-bot`.\n+  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n+  TF_FORCE_GPU_ALLOW_GROWTH: true\n   CUDA_VISIBLE_DEVICES: 0,1\n-  RUN_PT_TF_CROSS_TESTS: 1\n \n jobs:\n   get_runner:\n@@ -78,7 +77,7 @@ jobs:\n       - name: Show installed libraries and their versions\n         working-directory: /transformers\n         run: pip freeze\n-      \n+\n       - name: NVIDIA-SMI\n         run: |\n           nvidia-smi"
        },
        {
            "sha": "dee40cfbe412e758cb419cb5d434f242f7b9b14f",
            "filename": "CONTRIBUTING.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/CONTRIBUTING.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/CONTRIBUTING.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/CONTRIBUTING.md?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -344,7 +344,6 @@ RUN_SLOW=yes python -m pytest -n auto --dist=loadfile -s -v ./examples/pytorch/t\n Like the slow tests, there are other environment variables available which are not enabled by default during testing:\n - `RUN_CUSTOM_TOKENIZERS`: Enables tests for custom tokenizers.\n - `RUN_PT_FLAX_CROSS_TESTS`: Enables tests for PyTorch + Flax integration.\n-- `RUN_PT_TF_CROSS_TESTS`: Enables tests for TensorFlow + PyTorch integration.\n \n More environment variables and additional information can be found in the [testing_utils.py](https://github.com/huggingface/transformers/blob/main/src/transformers/testing_utils.py).\n "
        },
        {
            "sha": "6c4b1f3d2bf816849e8d3b8a47d811ba287b56ef",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/conftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/conftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/conftest.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -61,7 +61,6 @@\n     \"test_load_save_without_tied_weights\",\n     \"test_tied_weights_keys\",\n     \"test_model_weights_reload_no_missing_tied_weights\",\n-    \"test_pt_tf_model_equivalence\",\n     \"test_mismatched_shapes_have_properly_initialized_weights\",\n     \"test_matched_shapes_have_loaded_weights_when_some_mismatched_shapes_exist\",\n     \"test_model_is_small\",\n@@ -85,9 +84,6 @@\n \n \n def pytest_configure(config):\n-    config.addinivalue_line(\n-        \"markers\", \"is_pt_tf_cross_test: mark test to run only when PT and TF interactions are tested\"\n-    )\n     config.addinivalue_line(\n         \"markers\", \"is_pt_flax_cross_test: mark test to run only when PT and FLAX interactions are tested\"\n     )"
        },
        {
            "sha": "08ef100571c10035a74d950801bb1ed3c2cf4675",
            "filename": "docs/source/de/contributing.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/docs%2Fsource%2Fde%2Fcontributing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/docs%2Fsource%2Fde%2Fcontributing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Fcontributing.md?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -284,7 +284,6 @@ Wie bei den langsamen Tests gibt es auch andere Umgebungsvariablen, die standard\n \n * `RUN_CUSTOM_TOKENIZERS`: Aktiviert Tests fÃ¼r benutzerdefinierte Tokenizer.\n * `RUN_PT_FLAX_CROSS_TESTS`: Aktiviert Tests fÃ¼r die Integration von PyTorch + Flax.\n-* `RUN_PT_TF_CROSS_TESTS`: Aktiviert Tests fÃ¼r die Integration von TensorFlow + PyTorch.\n \n Weitere Umgebungsvariablen und zusÃ¤tzliche Informationen finden Sie in der [testing_utils.py](src/transformers/testing_utils.py).\n "
        },
        {
            "sha": "78e2f2a21697c8ce1de21a26d21a219f8889e2c9",
            "filename": "docs/source/ko/contributing.md",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/docs%2Fsource%2Fko%2Fcontributing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/docs%2Fsource%2Fko%2Fcontributing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fcontributing.md?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -85,7 +85,7 @@ python src/transformers/commands/transformers_cli.py env\n 3. í•´ë‹¹ ê¸°ëŠ¥ì˜ ì‚¬ìš©ë²•ì„ ë³´ì—¬ì£¼ëŠ” *ì½”ë“œ ìŠ¤ë‹ˆí«*ì„ ì œê³µí•´ ì£¼ì„¸ìš”.\n 4. ê¸°ëŠ¥ê³¼ ê´€ë ¨ëœ ë…¼ë¬¸ì´ ìˆëŠ” ê²½ìš° ë§í¬ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”.\n \n-ì´ìŠˆê°€ ì˜ ì‘ì„±ë˜ì—ˆë‹¤ë©´ ì´ìŠˆê°€ ìƒì„±ëœ ìˆœê°„, ì´ë¯¸ 80% ì •ë„ì˜ ì‘ì—…ì´ ì™„ë£Œëœ ê²ƒì…ë‹ˆë‹¤. \n+ì´ìŠˆê°€ ì˜ ì‘ì„±ë˜ì—ˆë‹¤ë©´ ì´ìŠˆê°€ ìƒì„±ëœ ìˆœê°„, ì´ë¯¸ 80% ì •ë„ì˜ ì‘ì—…ì´ ì™„ë£Œëœ ê²ƒì…ë‹ˆë‹¤.\n \n ì´ìŠˆë¥¼ ì œê¸°í•˜ëŠ” ë° ë„ì›€ì´ ë  ë§Œí•œ [í…œí”Œë¦¿](https://github.com/huggingface/transformers/tree/main/templates)ë„ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n \n@@ -140,7 +140,7 @@ python src/transformers/commands/transformers_cli.py env\n    ```\n \n    ë§Œì•½ ì´ë¯¸ ê°€ìƒ í™˜ê²½ì— ğŸ¤— Transformersê°€ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ë©´, `-e` í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¤ì¹˜í•˜ê¸° ì „ì— `pip uninstall transformers`ë¡œ ì œê±°í•´ì£¼ì„¸ìš”.\n-   \n+\n    ì—¬ëŸ¬ë¶„ì˜ ìš´ì˜ì²´ì œì— ë”°ë¼ì„œ, ê·¸ë¦¬ê³  ğŸ¤— Transformersì˜ ì„ íƒì  ì˜ì¡´ì„±ì˜ ìˆ˜ê°€ ì¦ê°€í•˜ë©´ì„œ, ì´ ëª…ë ¹ì´ ì‹¤íŒ¨í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ´ ê²½ìš° ì‚¬ìš©í•˜ë ¤ëŠ” ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬(PyTorch, TensorFlow, ê·¸ë¦¬ê³ /ë˜ëŠ” Flax)ë¥¼ ì„¤ì¹˜í•œ í›„ ì•„ë˜ ëª…ë ¹ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”:\n \n    ```bash\n@@ -188,7 +188,7 @@ python src/transformers/commands/transformers_cli.py env\n    ì´ëŸ¬í•œ ê²€ì‚¬ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ê³  ê´€ë ¨ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì€ [Pull Requestì— ëŒ€í•œ ê²€ì‚¬](https://huggingface.co/docs/transformers/pr_checks) ê°€ì´ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.\n \n    ë§Œì•½ `docs/source` ë””ë ‰í„°ë¦¬ ì•„ë˜ì˜ ë¬¸ì„œë¥¼ ìˆ˜ì •í•˜ëŠ” ê²½ìš°, ë¬¸ì„œê°€ ë¹Œë“œë  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ì´ ê²€ì‚¬ëŠ” Pull Requestë¥¼ ì—´ ë•Œë„ CIì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤. ë¡œì»¬ ê²€ì‚¬ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ë¬¸ì„œ ë¹Œë”ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤:\n-   \n+\n    ```bash\n    pip install \".[docs]\"\n    ```\n@@ -216,7 +216,7 @@ python src/transformers/commands/transformers_cli.py env\n    git fetch upstream\n    git rebase upstream/main\n    ```\n-   \n+\n    ë³€ê²½ ì‚¬í•­ì„ ë¸Œëœì¹˜ì— í‘¸ì‹œí•˜ì„¸ìš”:\n \n    ```bash\n@@ -238,7 +238,7 @@ python src/transformers/commands/transformers_cli.py env\n â˜ ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ëŠ” ê²½ìš°, í•´ë‹¹ ê¸°ëŠ¥ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ë„ ì¶”ê°€í•˜ì„¸ìš”.<br>\n    - ìƒˆ ëª¨ë¸ì„ ì¶”ê°€í•˜ëŠ” ê²½ìš°, `ModelTester.all_model_classes = (MyModel, MyModelWithLMHead,...)`ì„ ì‚¬ìš©í•˜ì—¬ ì¼ë°˜ì ì¸ í…ŒìŠ¤íŠ¸ë¥¼ í™œì„±í™”í•˜ì„¸ìš”.\n    - ìƒˆ `@slow` í…ŒìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•˜ëŠ” ê²½ìš°, ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ í†µê³¼í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”: `RUN_SLOW=1 python -m pytest tests/models/my_new_model/test_my_new_model.py`.\n-   - ìƒˆ í† í¬ë‚˜ì´ì €ë¥¼ ì¶”ê°€í•˜ëŠ” ê²½ìš°, í…ŒìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ê³  ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ í†µê³¼í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”: `RUN_SLOW=1 python -m pytest tests/models/{your_model_name}/test_tokenization_{your_model_name}.py`. \n+   - ìƒˆ í† í¬ë‚˜ì´ì €ë¥¼ ì¶”ê°€í•˜ëŠ” ê²½ìš°, í…ŒìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ê³  ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ í†µê³¼í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”: `RUN_SLOW=1 python -m pytest tests/models/{your_model_name}/test_tokenization_{your_model_name}.py`.\n    - CircleCIì—ì„œëŠ” ëŠë¦° í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì§€ ì•Šì§€ë§Œ, GitHub Actionsì—ì„œëŠ” ë§¤ì¼ ë°¤ ì‹¤í–‰ë©ë‹ˆë‹¤!<br>\n \n â˜ ëª¨ë“  ê³µê°œ ë©”ì†Œë“œëŠ” ìœ ìš©í•œ ê¸°ìˆ ë¬¸ì„œë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤ (ì˜ˆë¥¼ ë“¤ì–´ [`modeling_bert.py`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py) ì°¸ì¡°).<br>\n@@ -283,7 +283,6 @@ RUN_SLOW=yes python -m pytest -n auto --dist=loadfile -s -v ./examples/pytorch/t\n ëŠë¦° í…ŒìŠ¤íŠ¸ì™€ ë§ˆì°¬ê°€ì§€ë¡œ, ë‹¤ìŒê³¼ ê°™ì´ í…ŒìŠ¤íŠ¸ ì¤‘ì— ê¸°ë³¸ì ìœ¼ë¡œ í™œì„±í™”ë˜ì§€ ì•ŠëŠ” ë‹¤ë¥¸ í™˜ê²½ ë³€ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:\n - `RUN_CUSTOM_TOKENIZERS`: ì‚¬ìš©ì ì •ì˜ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.\n - `RUN_PT_FLAX_CROSS_TESTS`: PyTorch + Flax í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.\n-- `RUN_PT_TF_CROSS_TESTS`: TensorFlow + PyTorch í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.\n \n ë” ë§ì€ í™˜ê²½ ë³€ìˆ˜ì™€ ì¶”ê°€ ì •ë³´ëŠ” [testing_utils.py](src/transformers/testing_utils.py)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n "
        },
        {
            "sha": "215dd653ad85ad106da54071db16db09d09379be",
            "filename": "docs/source/zh/contributing.md",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/docs%2Fsource%2Fzh%2Fcontributing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/docs%2Fsource%2Fzh%2Fcontributing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fcontributing.md?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -33,7 +33,7 @@ limitations under the License.\n * å®ç°æ–°çš„æ¨¡å‹ã€‚\n * ä¸ºç¤ºä¾‹æˆ–æ–‡æ¡£åšè´¡çŒ®ã€‚\n \n-å¦‚æœä½ ä¸çŸ¥é“ä»å“ªé‡Œå¼€å§‹ï¼Œæœ‰ä¸€ä¸ªç‰¹åˆ«çš„ [Good First Issue](https://github.com/huggingface/transformers/contribute) åˆ—è¡¨ã€‚å®ƒä¼šåˆ—å‡ºä¸€äº›é€‚åˆåˆå­¦è€…çš„å¼€æ”¾çš„ issuesï¼Œå¹¶å¸®åŠ©ä½ å¼€å§‹ä¸ºå¼€æºé¡¹ç›®åšè´¡çŒ®ã€‚åªéœ€è¦åœ¨ä½ æƒ³è¦å¤„ç†çš„ issue ä¸‹å‘è¡¨è¯„è®ºå°±è¡Œã€‚ \n+å¦‚æœä½ ä¸çŸ¥é“ä»å“ªé‡Œå¼€å§‹ï¼Œæœ‰ä¸€ä¸ªç‰¹åˆ«çš„ [Good First Issue](https://github.com/huggingface/transformers/contribute) åˆ—è¡¨ã€‚å®ƒä¼šåˆ—å‡ºä¸€äº›é€‚åˆåˆå­¦è€…çš„å¼€æ”¾çš„ issuesï¼Œå¹¶å¸®åŠ©ä½ å¼€å§‹ä¸ºå¼€æºé¡¹ç›®åšè´¡çŒ®ã€‚åªéœ€è¦åœ¨ä½ æƒ³è¦å¤„ç†çš„ issue ä¸‹å‘è¡¨è¯„è®ºå°±è¡Œã€‚\n \n å¦‚æœæƒ³è¦ç¨å¾®æ›´æœ‰æŒ‘æˆ˜æ€§çš„å†…å®¹ï¼Œä½ ä¹Ÿå¯ä»¥æŸ¥çœ‹ [Good Second Issue](https://github.com/huggingface/transformers/labels/Good%20Second%20Issue) åˆ—è¡¨ã€‚æ€»çš„æ¥è¯´ï¼Œå¦‚æœä½ è§‰å¾—è‡ªå·±çŸ¥é“è¯¥æ€ä¹ˆåšï¼Œå°±å»åšå§ï¼Œæˆ‘ä»¬ä¼šå¸®åŠ©ä½ è¾¾åˆ°ç›®æ ‡çš„ï¼ğŸš€\n \n@@ -139,7 +139,7 @@ python src/transformers/commands/transformers_cli.py env\n    ```\n \n    å¦‚æœåœ¨è™šæ‹Ÿç¯å¢ƒä¸­å·²ç»å®‰è£…äº† ğŸ¤— Transformersï¼Œè¯·å…ˆä½¿ç”¨ `pip uninstall transformers` å¸è½½å®ƒï¼Œç„¶åå†ç”¨ `-e` å‚æ•°ä»¥å¯ç¼–è¾‘æ¨¡å¼é‡æ–°å®‰è£…ã€‚\n-   \n+\n    æ ¹æ®ä½ çš„æ“ä½œç³»ç»Ÿï¼Œä»¥åŠ Transformers çš„å¯é€‰ä¾èµ–é¡¹æ•°é‡çš„å¢åŠ ï¼Œå¯èƒ½ä¼šåœ¨æ‰§è¡Œæ­¤å‘½ä»¤æ—¶å‡ºç°å¤±è´¥ã€‚å¦‚æœå‡ºç°è¿™ç§æƒ…å†µï¼Œè¯·ç¡®ä¿å·²ç»å®‰è£…äº†ä½ æƒ³ä½¿ç”¨çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆPyTorch, TensorFlow å’Œ Flaxï¼‰ï¼Œç„¶åæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\n \n    ```bash\n@@ -187,7 +187,7 @@ python src/transformers/commands/transformers_cli.py env\n    æƒ³è¦äº†è§£æœ‰å…³è¿™äº›æ£€æŸ¥åŠå¦‚ä½•è§£å†³ç›¸å…³é—®é¢˜çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·é˜…è¯» [æ£€æŸ¥ Pull Request](https://huggingface.co/docs/transformers/pr_checks) æŒ‡å—ã€‚\n \n    å¦‚æœä½ ä¿®æ”¹äº† `docs/source` ç›®å½•ä¸‹çš„æ–‡æ¡£ï¼Œè¯·ç¡®ä¿æ–‡æ¡£ä»ç„¶èƒ½å¤Ÿè¢«æ„å»ºã€‚è¿™ä¸ªæ£€æŸ¥ä¹Ÿä¼šåœ¨ä½ åˆ›å»º PR æ—¶åœ¨ CI ä¸­è¿è¡Œã€‚å¦‚æœè¦è¿›è¡Œæœ¬åœ°æ£€æŸ¥ï¼Œè¯·ç¡®ä¿å®‰è£…äº†æ–‡æ¡£æ„å»ºå·¥å…·ï¼š\n-   \n+\n    ```bash\n    pip install \".[docs]\"\n    ```\n@@ -282,7 +282,6 @@ RUN_SLOW=yes python -m pytest -n auto --dist=loadfile -s -v ./examples/pytorch/t\n å’Œæ—¶é—´è¾ƒé•¿çš„æµ‹è¯•ä¸€æ ·ï¼Œè¿˜æœ‰å…¶ä»–ç¯å¢ƒå˜é‡åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œåœ¨é»˜è®¤æƒ…å†µä¸‹æ˜¯æœªå¯ç”¨çš„ï¼š\n - `RUN_CUSTOM_TOKENIZERS`: å¯ç”¨è‡ªå®šä¹‰åˆ†è¯å™¨çš„æµ‹è¯•ã€‚\n - `RUN_PT_FLAX_CROSS_TESTS`: å¯ç”¨ PyTorch + Flax æ•´åˆçš„æµ‹è¯•ã€‚\n-- `RUN_PT_TF_CROSS_TESTS`: å¯ç”¨ TensorFlow + PyTorch æ•´åˆçš„æµ‹è¯•ã€‚\n \n æ›´å¤šç¯å¢ƒå˜é‡å’Œé¢å¤–ä¿¡æ¯å¯ä»¥åœ¨ [testing_utils.py](src/transformers/testing_utils.py) ä¸­æ‰¾åˆ°ã€‚\n "
        },
        {
            "sha": "53dac683160d2935e62ebf68948b61a651a3de7c",
            "filename": "setup.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -473,7 +473,6 @@ def run(self):\n extras[\"tests_torch\"] = deps_list()\n extras[\"tests_tf\"] = deps_list()\n extras[\"tests_flax\"] = deps_list()\n-extras[\"tests_torch_and_tf\"] = deps_list()\n extras[\"tests_torch_and_flax\"] = deps_list()\n extras[\"tests_hub\"] = deps_list()\n extras[\"tests_pipelines_torch\"] = deps_list()"
        },
        {
            "sha": "6f7808a9b1d2aab018b3c959e248f2b386bab1c4",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -230,7 +230,6 @@ def parse_int_from_env(key, default=None):\n \n \n _run_slow_tests = parse_flag_from_env(\"RUN_SLOW\", default=False)\n-_run_pt_tf_cross_tests = parse_flag_from_env(\"RUN_PT_TF_CROSS_TESTS\", default=True)\n _run_pt_flax_cross_tests = parse_flag_from_env(\"RUN_PT_FLAX_CROSS_TESTS\", default=True)\n _run_custom_tokenizers = parse_flag_from_env(\"RUN_CUSTOM_TOKENIZERS\", default=False)\n _run_staging = parse_flag_from_env(\"HUGGINGFACE_CO_STAGING\", default=False)\n@@ -251,25 +250,6 @@ def get_device_count():\n     return num_devices\n \n \n-def is_pt_tf_cross_test(test_case):\n-    \"\"\"\n-    Decorator marking a test as a test that control interactions between PyTorch and TensorFlow.\n-\n-    PT+TF tests are skipped by default and we can run only them by setting RUN_PT_TF_CROSS_TESTS environment variable\n-    to a truthy value and selecting the is_pt_tf_cross_test pytest mark.\n-\n-    \"\"\"\n-    if not _run_pt_tf_cross_tests or not is_torch_available() or not is_tf_available():\n-        return unittest.skip(reason=\"test is PT+TF test\")(test_case)\n-    else:\n-        try:\n-            import pytest  # We don't need a hard dependency on pytest in the main library\n-        except ImportError:\n-            return test_case\n-        else:\n-            return pytest.mark.is_pt_tf_cross_test()(test_case)\n-\n-\n def is_pt_flax_cross_test(test_case):\n     \"\"\"\n     Decorator marking a test as a test that control interactions between PyTorch and Flax"
        },
        {
            "sha": "2c59c906db6b4f9c667f8f18994188fe3b759523",
            "filename": "tests/models/auto/test_modeling_tf_pytorch.py",
            "status": "removed",
            "additions": 0,
            "deletions": 228,
            "changes": 228,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a81d774b1aeeb52f607524ed2b18aa3d1b8d8dd/tests%2Fmodels%2Fauto%2Ftest_modeling_tf_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a81d774b1aeeb52f607524ed2b18aa3d1b8d8dd/tests%2Fmodels%2Fauto%2Ftest_modeling_tf_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_modeling_tf_pytorch.py?ref=1a81d774b1aeeb52f607524ed2b18aa3d1b8d8dd",
            "patch": "@@ -1,228 +0,0 @@\n-# coding=utf-8\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-from transformers import is_tf_available, is_torch_available\n-from transformers.testing_utils import DUMMY_UNKNOWN_IDENTIFIER, SMALL_MODEL_IDENTIFIER, is_pt_tf_cross_test, slow\n-\n-\n-if is_tf_available():\n-    from transformers import (\n-        AutoConfig,\n-        BertConfig,\n-        GPT2Config,\n-        T5Config,\n-        TFAutoModel,\n-        TFAutoModelForCausalLM,\n-        TFAutoModelForMaskedLM,\n-        TFAutoModelForPreTraining,\n-        TFAutoModelForQuestionAnswering,\n-        TFAutoModelForSeq2SeqLM,\n-        TFAutoModelForSequenceClassification,\n-        TFAutoModelWithLMHead,\n-        TFBertForMaskedLM,\n-        TFBertForPreTraining,\n-        TFBertForQuestionAnswering,\n-        TFBertForSequenceClassification,\n-        TFBertModel,\n-        TFGPT2LMHeadModel,\n-        TFRobertaForMaskedLM,\n-        TFT5ForConditionalGeneration,\n-    )\n-\n-if is_torch_available():\n-    from transformers import (\n-        AutoModel,\n-        AutoModelForCausalLM,\n-        AutoModelForMaskedLM,\n-        AutoModelForPreTraining,\n-        AutoModelForQuestionAnswering,\n-        AutoModelForSeq2SeqLM,\n-        AutoModelForSequenceClassification,\n-        AutoModelWithLMHead,\n-        BertForMaskedLM,\n-        BertForPreTraining,\n-        BertForQuestionAnswering,\n-        BertForSequenceClassification,\n-        BertModel,\n-        GPT2LMHeadModel,\n-        RobertaForMaskedLM,\n-        T5ForConditionalGeneration,\n-    )\n-\n-\n-@is_pt_tf_cross_test\n-class TFPTAutoModelTest(unittest.TestCase):\n-    @slow\n-    def test_model_from_pretrained(self):\n-        #     model_name = 'google-bert/bert-base-uncased'\n-        for model_name in [\"google-bert/bert-base-uncased\"]:\n-            config = AutoConfig.from_pretrained(model_name)\n-            self.assertIsNotNone(config)\n-            self.assertIsInstance(config, BertConfig)\n-\n-            model = TFAutoModel.from_pretrained(model_name, from_pt=True)\n-            self.assertIsNotNone(model)\n-            self.assertIsInstance(model, TFBertModel)\n-\n-            model = AutoModel.from_pretrained(model_name, from_tf=True)\n-            self.assertIsNotNone(model)\n-            self.assertIsInstance(model, BertModel)\n-\n-    @slow\n-    def test_model_for_pretraining_from_pretrained(self):\n-        #     model_name = 'google-bert/bert-base-uncased'\n-        for model_name in [\"google-bert/bert-base-uncased\"]:\n-            config = AutoConfig.from_pretrained(model_name)\n-            self.assertIsNotNone(config)\n-            self.assertIsInstance(config, BertConfig)\n-\n-            model = TFAutoModelForPreTraining.from_pretrained(model_name, from_pt=True)\n-            self.assertIsNotNone(model)\n-            self.assertIsInstance(model, TFBertForPreTraining)\n-\n-            model = AutoModelForPreTraining.from_pretrained(model_name, from_tf=True)\n-            self.assertIsNotNone(model)\n-            self.assertIsInstance(model, BertForPreTraining)\n-\n-    @slow\n-    def test_model_for_causal_lm(self):\n-        model_name = \"openai-community/gpt2\"\n-        config = AutoConfig.from_pretrained(model_name)\n-        self.assertIsNotNone(config)\n-        self.assertIsInstance(config, GPT2Config)\n-\n-        model = TFAutoModelForCausalLM.from_pretrained(model_name, from_pt=True)\n-        model, loading_info = TFAutoModelForCausalLM.from_pretrained(\n-            model_name, output_loading_info=True, from_pt=True\n-        )\n-        self.assertIsNotNone(model)\n-        self.assertIsInstance(model, TFGPT2LMHeadModel)\n-\n-        model = AutoModelForCausalLM.from_pretrained(model_name, from_tf=True)\n-        model, loading_info = AutoModelForCausalLM.from_pretrained(model_name, output_loading_info=True, from_tf=True)\n-        self.assertIsNotNone(model)\n-        self.assertIsInstance(model, GPT2LMHeadModel)\n-\n-    @slow\n-    def test_lmhead_model_from_pretrained(self):\n-        model_name = \"google-bert/bert-base-uncased\"\n-        config = AutoConfig.from_pretrained(model_name)\n-        self.assertIsNotNone(config)\n-        self.assertIsInstance(config, BertConfig)\n-\n-        model = TFAutoModelWithLMHead.from_pretrained(model_name, from_pt=True)\n-        self.assertIsNotNone(model)\n-        self.assertIsInstance(model, TFBertForMaskedLM)\n-\n-        model = AutoModelWithLMHead.from_pretrained(model_name, from_tf=True)\n-        self.assertIsNotNone(model)\n-        self.assertIsInstance(model, BertForMaskedLM)\n-\n-    @slow\n-    def test_model_for_masked_lm(self):\n-        model_name = \"google-bert/bert-base-uncased\"\n-        config = AutoConfig.from_pretrained(model_name)\n-        self.assertIsNotNone(config)\n-        self.assertIsInstance(config, BertConfig)\n-\n-        model = TFAutoModelForMaskedLM.from_pretrained(model_name, from_pt=True)\n-        model, loading_info = TFAutoModelForMaskedLM.from_pretrained(\n-            model_name, output_loading_info=True, from_pt=True\n-        )\n-        self.assertIsNotNone(model)\n-        self.assertIsInstance(model, TFBertForMaskedLM)\n-\n-        model = AutoModelForMaskedLM.from_pretrained(model_name, from_tf=True)\n-        model, loading_info = AutoModelForMaskedLM.from_pretrained(model_name, output_loading_info=True, from_tf=True)\n-        self.assertIsNotNone(model)\n-        self.assertIsInstance(model, BertForMaskedLM)\n-\n-    @slow\n-    def test_model_for_encoder_decoder_lm(self):\n-        model_name = \"google-t5/t5-base\"\n-        config = AutoConfig.from_pretrained(model_name)\n-        self.assertIsNotNone(config)\n-        self.assertIsInstance(config, T5Config)\n-\n-        model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name, from_pt=True)\n-        model, loading_info = TFAutoModelForSeq2SeqLM.from_pretrained(\n-            model_name, output_loading_info=True, from_pt=True\n-        )\n-        self.assertIsNotNone(model)\n-        self.assertIsInstance(model, TFT5ForConditionalGeneration)\n-\n-        model = AutoModelForSeq2SeqLM.from_pretrained(model_name, from_tf=True)\n-        model, loading_info = AutoModelForSeq2SeqLM.from_pretrained(model_name, output_loading_info=True, from_tf=True)\n-        self.assertIsNotNone(model)\n-        self.assertIsInstance(model, T5ForConditionalGeneration)\n-\n-    @slow\n-    def test_sequence_classification_model_from_pretrained(self):\n-        #     model_name = 'google-bert/bert-base-uncased'\n-        for model_name in [\"google-bert/bert-base-uncased\"]:\n-            config = AutoConfig.from_pretrained(model_name)\n-            self.assertIsNotNone(config)\n-            self.assertIsInstance(config, BertConfig)\n-\n-            model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)\n-            self.assertIsNotNone(model)\n-            self.assertIsInstance(model, TFBertForSequenceClassification)\n-\n-            model = AutoModelForSequenceClassification.from_pretrained(model_name, from_tf=True)\n-            self.assertIsNotNone(model)\n-            self.assertIsInstance(model, BertForSequenceClassification)\n-\n-    @slow\n-    def test_question_answering_model_from_pretrained(self):\n-        #     model_name = 'google-bert/bert-base-uncased'\n-        for model_name in [\"google-bert/bert-base-uncased\"]:\n-            config = AutoConfig.from_pretrained(model_name)\n-            self.assertIsNotNone(config)\n-            self.assertIsInstance(config, BertConfig)\n-\n-            model = TFAutoModelForQuestionAnswering.from_pretrained(model_name, from_pt=True)\n-            self.assertIsNotNone(model)\n-            self.assertIsInstance(model, TFBertForQuestionAnswering)\n-\n-            model = AutoModelForQuestionAnswering.from_pretrained(model_name, from_tf=True)\n-            self.assertIsNotNone(model)\n-            self.assertIsInstance(model, BertForQuestionAnswering)\n-\n-    def test_from_pretrained_identifier(self):\n-        model = TFAutoModelWithLMHead.from_pretrained(SMALL_MODEL_IDENTIFIER, from_pt=True)\n-        self.assertIsInstance(model, TFBertForMaskedLM)\n-        self.assertEqual(model.num_parameters(), 14410)\n-        self.assertEqual(model.num_parameters(only_trainable=True), 14410)\n-\n-        model = AutoModelWithLMHead.from_pretrained(SMALL_MODEL_IDENTIFIER, from_tf=True)\n-        self.assertIsInstance(model, BertForMaskedLM)\n-        self.assertEqual(model.num_parameters(), 14410)\n-        self.assertEqual(model.num_parameters(only_trainable=True), 14410)\n-\n-    def test_from_identifier_from_model_type(self):\n-        model = TFAutoModelWithLMHead.from_pretrained(DUMMY_UNKNOWN_IDENTIFIER, from_pt=True)\n-        self.assertIsInstance(model, TFRobertaForMaskedLM)\n-        self.assertEqual(model.num_parameters(), 14410)\n-        self.assertEqual(model.num_parameters(only_trainable=True), 14410)\n-\n-        model = AutoModelWithLMHead.from_pretrained(DUMMY_UNKNOWN_IDENTIFIER, from_tf=True)\n-        self.assertIsInstance(model, RobertaForMaskedLM)\n-        self.assertEqual(model.num_parameters(), 14410)\n-        self.assertEqual(model.num_parameters(only_trainable=True), 14410)"
        },
        {
            "sha": "2d0e42243ae504ee9157ec2ff4b648e6ba29f778",
            "filename": "tests/models/blip/test_modeling_blip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -375,9 +375,6 @@ def test_model_from_pretrained(self):\n         model = BlipTextModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    def test_pt_tf_model_equivalence(self):\n-        super().test_pt_tf_model_equivalence(allow_missing_keys=True)\n-\n \n class BlipModelTester:\n     def __init__(self, parent, text_kwargs=None, vision_kwargs=None, is_training=True):\n@@ -650,9 +647,6 @@ def test_get_multimodal_features(self):\n             ),\n         )\n \n-    def test_pt_tf_model_equivalence(self):\n-        super().test_pt_tf_model_equivalence(allow_missing_keys=True)\n-\n \n class BlipTextRetrievalModelTester:\n     def __init__(self, parent, text_kwargs=None, vision_kwargs=None, is_training=True):"
        },
        {
            "sha": "ad550308fa9589569bb2cf92ffd4dd7dc9c83f68",
            "filename": "tests/models/blip/test_modeling_blip_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fblip%2Ftest_modeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fblip%2Ftest_modeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip_text.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -178,6 +178,3 @@ def test_model_from_pretrained(self):\n         model_name = \"Salesforce/blip-vqa-base\"\n         model = BlipTextModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n-\n-    def test_pt_tf_model_equivalence(self):\n-        super().test_pt_tf_model_equivalence(allow_missing_keys=True)"
        },
        {
            "sha": "f8a73d19991f0fb58d7f1aa1c1f51b4eadca0694",
            "filename": "tests/models/blip/test_modeling_tf_blip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fblip%2Ftest_modeling_tf_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fblip%2Ftest_modeling_tf_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_tf_blip.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -321,9 +321,6 @@ def test_model_from_pretrained(self):\n         model = TFBlipTextModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    def test_pt_tf_model_equivalence(self, allow_missing_keys=True):\n-        super().test_pt_tf_model_equivalence(allow_missing_keys=allow_missing_keys)\n-\n \n class TFBlipModelTester:\n     def __init__(self, parent, text_kwargs=None, vision_kwargs=None, is_training=True):\n@@ -430,9 +427,6 @@ def test_model_from_pretrained(self):\n         model = TFBlipModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    def test_pt_tf_model_equivalence(self, allow_missing_keys=True):\n-        super().test_pt_tf_model_equivalence(allow_missing_keys=allow_missing_keys)\n-\n     @unittest.skip(\"Matt: Re-enable this test when we have a proper export function for TF models.\")\n     def test_saved_model_creation(self):\n         # This fails because the if return_loss: conditional can return None or a Tensor and TF hates that."
        },
        {
            "sha": "bba93551f0b03a548be3880f4b8d87bbdf30e3fb",
            "filename": "tests/models/blip/test_modeling_tf_blip_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fblip%2Ftest_modeling_tf_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fblip%2Ftest_modeling_tf_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_tf_blip_text.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -176,6 +176,3 @@ def test_model_from_pretrained(self):\n         model_name = \"Salesforce/blip-vqa-base\"\n         model = TFBlipTextModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n-\n-    def test_pt_tf_model_equivalence(self, allow_missing_keys=True):\n-        super().test_pt_tf_model_equivalence(allow_missing_keys=allow_missing_keys)"
        },
        {
            "sha": "bec4b07d3d9c4015575f14df881e16c30464118c",
            "filename": "tests/models/data2vec/test_modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -309,10 +309,6 @@ def test_initialization(self):\n                         msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                     )\n \n-    def check_pt_tf_outputs(self, tf_outputs, pt_outputs, model_class, tol=2e-4, name=\"outputs\", attributes=None):\n-        # We override with a slightly higher tol value, as semseg models tend to diverge a bit more\n-        super().check_pt_tf_outputs(tf_outputs, pt_outputs, model_class, tol, name, attributes)\n-\n     def test_for_image_classification(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_image_classification(*config_and_inputs)"
        },
        {
            "sha": "5c7ff8835517b314bee9cfe437c2625886e5ed12",
            "filename": "tests/models/data2vec/test_modeling_tf_data2vec_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_tf_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_tf_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_tf_data2vec_vision.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -385,10 +385,6 @@ def test_keras_fit(self):\n                     val_loss2 = history2.history[\"val_loss\"][0]\n                     self.assertTrue(np.allclose(val_loss1, val_loss2, atol=1e-2, rtol=1e-3))\n \n-    def check_pt_tf_outputs(self, tf_outputs, pt_outputs, model_class, tol=2e-4, name=\"outputs\", attributes=None):\n-        # We override with a slightly higher tol value, as semseg models tend to diverge a bit more\n-        super().check_pt_tf_outputs(tf_outputs, pt_outputs, model_class, tol, name, attributes)\n-\n     # Overriding this method since the base method won't be compatible with Data2VecVision.\n     def test_loss_computation(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "d6c15efa4a46ca09a9c2514df1d3b910295c810b",
            "filename": "tests/models/deberta/test_modeling_deberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -285,10 +285,6 @@ def test_torch_fx_output_loss(self):\n     def test_torch_fx(self):\n         pass\n \n-    @unittest.skip(\"This test was broken by the refactor in #22105, TODO @ArthurZucker\")\n-    def test_pt_tf_model_equivalence(self):\n-        pass\n-\n \n @require_torch\n @require_sentencepiece"
        },
        {
            "sha": "14a99ea947ec9c42801cda556b90593db22700c5",
            "filename": "tests/models/deberta/test_modeling_tf_deberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fdeberta%2Ftest_modeling_tf_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fdeberta%2Ftest_modeling_tf_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta%2Ftest_modeling_tf_deberta.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -270,10 +270,6 @@ def test_model_from_pretrained(self):\n         model = TFDebertaModel.from_pretrained(\"kamalkraj/deberta-base\")\n         self.assertIsNotNone(model)\n \n-    @unittest.skip(\"This test was broken by the refactor in #22105, TODO @ArthurZucker\")\n-    def test_pt_tf_model_equivalence(self):\n-        pass\n-\n \n @require_tf\n class TFDeBERTaModelIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "af2dfe03b33dc056e1a16e33332c1bd66f5915e1",
            "filename": "tests/models/deberta_v2/test_modeling_deberta_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -303,10 +303,6 @@ def test_torch_fx_output_loss(self):\n     def test_torch_fx(self):\n         pass\n \n-    @unittest.skip(\"This test was broken by the refactor in #22105, TODO @ArthurZucker\")\n-    def test_pt_tf_model_equivalence(self):\n-        pass\n-\n \n @require_torch\n @require_sentencepiece"
        },
        {
            "sha": "b46f68525d365db7178805a2a839eccb0dcfaea8",
            "filename": "tests/models/deberta_v2/test_modeling_tf_deberta_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_tf_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_tf_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_tf_deberta_v2.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -290,10 +290,6 @@ def test_model_from_pretrained(self):\n         model = TFDebertaV2Model.from_pretrained(\"kamalkraj/deberta-v2-xlarge\")\n         self.assertIsNotNone(model)\n \n-    @unittest.skip(\"This test was broken by the refactor in #22105, TODO @ArthurZucker\")\n-    def test_pt_tf_model_equivalence(self):\n-        pass\n-\n \n @require_tf\n class TFDeBERTaV2ModelIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "cd35578273e79c40b266b7088f4ba0cee6fdcbea",
            "filename": "tests/models/encoder_decoder/test_modeling_tf_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 374,
            "changes": 376,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_tf_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_tf_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_tf_encoder_decoder.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -16,16 +16,14 @@\n \n from __future__ import annotations\n \n-import copy\n import os\n import tempfile\n import unittest\n \n import numpy as np\n \n-from transformers import is_tf_available, is_torch_available\n-from transformers.testing_utils import is_pt_tf_cross_test, require_tf, require_torch, slow, torch_device\n-from transformers.utils.generic import ModelOutput\n+from transformers import is_tf_available\n+from transformers.testing_utils import require_tf, slow\n \n from ...test_modeling_tf_common import ids_tensor\n from ..bert.test_modeling_tf_bert import TFBertModelTester\n@@ -35,8 +33,6 @@\n \n \n if is_tf_available():\n-    import tensorflow as tf\n-\n     from transformers import (\n         AutoConfig,\n         AutoTokenizer,\n@@ -54,11 +50,6 @@\n     )\n     from transformers.modeling_tf_outputs import TFBaseModelOutput\n \n-if is_torch_available():\n-    import torch\n-\n-    from transformers import BertLMHeadModel, BertModel, EncoderDecoderModel\n-\n \n @require_tf\n class TFEncoderDecoderMixin:\n@@ -386,188 +377,6 @@ def check_encoder_decoder_model_generate(self, input_ids, config, decoder_config\n         )\n         self.assertEqual(tuple(generated_output.shape.as_list()), (input_ids.shape[0],) + (decoder_config.max_length,))\n \n-    def check_pt_tf_outputs(self, tf_outputs, pt_outputs, model_class, tol=1e-5, name=\"outputs\", attributes=None):\n-        \"\"\"Check the outputs from PyTorch and TensorFlow models are close enough. Checks are done in a recursive way.\n-\n-        Args:\n-            model_class: The class of the model that is currently testing. For example, `TFBertModel`,\n-                TFBertForMaskedLM`, `TFBertForSequenceClassification`, etc. Mainly used for providing more informative\n-                error messages.\n-            name (`str`): The name of the output. For example, `output.hidden_states`, `output.attentions`, etc.\n-            attributes (`Tuple[str]`): The names of the output's element if the output is a tuple/list with each element\n-                being a named field in the output.\n-        \"\"\"\n-\n-        self.assertEqual(type(name), str)\n-        if attributes is not None:\n-            self.assertEqual(type(attributes), tuple, f\"{name}: The argument `attributes` should be a `tuple`\")\n-\n-        # Allow `ModelOutput` (e.g. `CLIPOutput` has `text_model_output` and `vision_model_output`).\n-        if isinstance(tf_outputs, ModelOutput):\n-            self.assertTrue(\n-                isinstance(pt_outputs, ModelOutput),\n-                f\"{name}: `pt_outputs` should an instance of `ModelOutput` when `tf_outputs` is\",\n-            )\n-\n-            tf_keys = [k for k, v in tf_outputs.items() if v is not None]\n-            pt_keys = [k for k, v in pt_outputs.items() if v is not None]\n-\n-            self.assertEqual(tf_keys, pt_keys, f\"{name}: Output keys differ between TF and PyTorch\")\n-\n-            # convert to the case of `tuple`\n-            # appending each key to the current (string) `names`\n-            attributes = tuple([f\"{name}.{k}\" for k in tf_keys])\n-            self.check_pt_tf_outputs(\n-                tf_outputs.to_tuple(), pt_outputs.to_tuple(), model_class, tol=tol, name=name, attributes=attributes\n-            )\n-\n-        # Allow `list` (e.g. `TransfoXLModelOutput.mems` is a list of tensors.)\n-        elif type(tf_outputs) in [tuple, list]:\n-            self.assertEqual(type(tf_outputs), type(pt_outputs), f\"{name}: Output types differ between TF and PyTorch\")\n-            self.assertEqual(len(tf_outputs), len(pt_outputs), f\"{name}: Output lengths differ between TF and PyTorch\")\n-\n-            if attributes is not None:\n-                # case 1: each output has assigned name (e.g. a tuple form of a `ModelOutput`)\n-                self.assertEqual(\n-                    len(attributes),\n-                    len(tf_outputs),\n-                    f\"{name}: The tuple `names` should have the same length as `tf_outputs`\",\n-                )\n-            else:\n-                # case 2: each output has no assigned name (e.g. hidden states of each layer) -> add an index to `names`\n-                attributes = tuple([f\"{name}_{idx}\" for idx in range(len(tf_outputs))])\n-\n-            for tf_output, pt_output, attr in zip(tf_outputs, pt_outputs, attributes):\n-                self.check_pt_tf_outputs(tf_output, pt_output, model_class, tol=tol, name=attr)\n-\n-        elif isinstance(tf_outputs, tf.Tensor):\n-            self.assertTrue(\n-                isinstance(pt_outputs, torch.Tensor), f\"{name}: `pt_outputs` should a tensor when `tf_outputs` is\"\n-            )\n-\n-            tf_outputs = tf_outputs.numpy()\n-            pt_outputs = pt_outputs.detach().to(\"cpu\").numpy()\n-\n-            self.assertEqual(\n-                tf_outputs.shape, pt_outputs.shape, f\"{name}: Output shapes differ between TF and PyTorch\"\n-            )\n-\n-            # deal with NumPy's scalars to make replacing nan values by 0 work.\n-            if np.isscalar(tf_outputs):\n-                tf_outputs = np.array([tf_outputs])\n-                pt_outputs = np.array([pt_outputs])\n-\n-            tf_nans = np.isnan(tf_outputs)\n-            pt_nans = np.isnan(pt_outputs)\n-\n-            pt_outputs[tf_nans] = 0\n-            tf_outputs[tf_nans] = 0\n-            pt_outputs[pt_nans] = 0\n-            tf_outputs[pt_nans] = 0\n-\n-            max_diff = np.amax(np.abs(tf_outputs - pt_outputs))\n-            self.assertLessEqual(max_diff, tol, f\"{name}: Difference between torch and tf is {max_diff} (>= {tol}).\")\n-        else:\n-            raise ValueError(\n-                \"`tf_outputs` should be an instance of `tf.Tensor`, a `tuple`, or an instance of `tf.Tensor`. Got\"\n-                f\" {type(tf_outputs)} instead.\"\n-            )\n-\n-    def prepare_pt_inputs_from_tf_inputs(self, tf_inputs_dict):\n-        pt_inputs_dict = {}\n-        for name, key in tf_inputs_dict.items():\n-            if isinstance(key, bool):\n-                pt_inputs_dict[name] = key\n-            elif name == \"input_values\":\n-                pt_inputs_dict[name] = torch.from_numpy(key.numpy()).to(torch.float32)\n-            elif name == \"pixel_values\":\n-                pt_inputs_dict[name] = torch.from_numpy(key.numpy()).to(torch.float32)\n-            elif name == \"input_features\":\n-                pt_inputs_dict[name] = torch.from_numpy(key.numpy()).to(torch.float32)\n-            # other general float inputs\n-            elif tf_inputs_dict[name].dtype.is_floating:\n-                pt_inputs_dict[name] = torch.from_numpy(key.numpy()).to(torch.float32)\n-            else:\n-                pt_inputs_dict[name] = torch.from_numpy(key.numpy()).to(torch.long)\n-\n-        return pt_inputs_dict\n-\n-    def check_pt_tf_models(self, tf_model, pt_model, tf_inputs_dict):\n-        pt_inputs_dict = self.prepare_pt_inputs_from_tf_inputs(tf_inputs_dict)\n-\n-        # send pytorch inputs to the correct device\n-        pt_inputs_dict = {\n-            k: v.to(device=torch_device) if isinstance(v, torch.Tensor) else v for k, v in pt_inputs_dict.items()\n-        }\n-\n-        # send pytorch model to the correct device\n-        pt_model.to(torch_device)\n-\n-        # Check predictions on first output (logits/hidden-states) are close enough given low-level computational differences\n-        pt_model.eval()\n-\n-        with torch.no_grad():\n-            pt_outputs = pt_model(**pt_inputs_dict)\n-        tf_outputs = tf_model(tf_inputs_dict)\n-\n-        # tf models returned loss is usually a tensor rather than a scalar.\n-        # (see `hf_compute_loss`: it uses `keras.losses.Reduction.NONE`)\n-        # Change it here to a scalar to match PyTorch models' loss\n-        tf_loss = getattr(tf_outputs, \"loss\", None)\n-        if tf_loss is not None:\n-            tf_outputs.loss = tf.math.reduce_mean(tf_loss)\n-\n-        self.check_pt_tf_outputs(tf_outputs, pt_outputs, type(tf_model))\n-\n-    def check_pt_tf_equivalence(self, tf_model, pt_model, tf_inputs_dict):\n-        \"\"\"Wrap `check_pt_tf_models` to further check PT -> TF again\"\"\"\n-\n-        self.check_pt_tf_models(tf_model, pt_model, tf_inputs_dict)\n-\n-        # PT -> TF\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            pt_model.save_pretrained(tmpdirname)\n-            tf_model = TFEncoderDecoderModel.from_pretrained(tmpdirname)\n-\n-        self.check_pt_tf_models(tf_model, pt_model, tf_inputs_dict)\n-\n-    def check_pt_to_tf_equivalence(self, config, decoder_config, tf_inputs_dict):\n-        \"\"\"EncoderDecoderModel requires special way to cross load (PT -> TF)\"\"\"\n-\n-        encoder_decoder_config = EncoderDecoderConfig.from_encoder_decoder_configs(config, decoder_config)\n-        # Output all for aggressive testing\n-        encoder_decoder_config.output_hidden_states = True\n-        # All models tested in this file have attentions\n-        encoder_decoder_config.output_attentions = True\n-\n-        pt_model = EncoderDecoderModel(encoder_decoder_config)\n-\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            pt_model.save_pretrained(tmpdirname)\n-            tf_model = TFEncoderDecoderModel.from_pretrained(tmpdirname)\n-\n-        self.check_pt_tf_equivalence(tf_model, pt_model, tf_inputs_dict)\n-\n-    def check_tf_to_pt_equivalence(self, config, decoder_config, tf_inputs_dict):\n-        \"\"\"EncoderDecoderModel requires special way to cross load (TF -> PT)\"\"\"\n-\n-        encoder_decoder_config = EncoderDecoderConfig.from_encoder_decoder_configs(config, decoder_config)\n-        # Output all for aggressive testing\n-        encoder_decoder_config.output_hidden_states = True\n-        # TODO: A generalizable way to determine this attribute\n-        encoder_decoder_config.output_attentions = True\n-\n-        tf_model = TFEncoderDecoderModel(encoder_decoder_config)\n-        # Make sure model is built before saving\n-        tf_model(**tf_inputs_dict)\n-\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            # TODO Matt: PT doesn't support loading TF safetensors - remove the arg and from_tf=True when it does\n-            tf_model.save_pretrained(tmpdirname, safe_serialization=False)\n-            pt_model = EncoderDecoderModel.from_pretrained(tmpdirname, from_tf=True)\n-\n-        self.check_pt_tf_equivalence(tf_model, pt_model, tf_inputs_dict)\n-\n     def test_encoder_decoder_model(self):\n         input_ids_dict = self.prepare_config_and_inputs()\n         self.check_encoder_decoder_model(**input_ids_dict)\n@@ -608,70 +417,6 @@ def assert_almost_equals(self, a: np.ndarray, b: np.ndarray, tol: float):\n         diff = np.abs((a - b)).max()\n         self.assertLessEqual(diff, tol, f\"Difference between torch and tf is {diff} (>= {tol}).\")\n \n-    @is_pt_tf_cross_test\n-    def test_pt_tf_model_equivalence(self):\n-        config_inputs_dict = self.prepare_config_and_inputs()\n-        labels = config_inputs_dict.pop(\"decoder_token_labels\")\n-\n-        # Keep only common arguments\n-        arg_names = [\n-            \"config\",\n-            \"input_ids\",\n-            \"attention_mask\",\n-            \"decoder_config\",\n-            \"decoder_input_ids\",\n-            \"decoder_attention_mask\",\n-            \"encoder_hidden_states\",\n-        ]\n-        config_inputs_dict = {k: v for k, v in config_inputs_dict.items() if k in arg_names}\n-\n-        config = config_inputs_dict.pop(\"config\")\n-        decoder_config = config_inputs_dict.pop(\"decoder_config\")\n-\n-        # Output all for aggressive testing\n-        config.output_hidden_states = True\n-        decoder_config.output_hidden_states = True\n-        # All models tested in this file have attentions\n-        config.output_attentions = True\n-        decoder_config.output_attentions = True\n-\n-        tf_inputs_dict = config_inputs_dict\n-        # `encoder_hidden_states` is not used in model call/forward\n-        del tf_inputs_dict[\"encoder_hidden_states\"]\n-\n-        # Make sure no sequence has all zeros as attention mask, otherwise some tests fail due to the inconsistency\n-        # of the usage `1e-4`, `1e-9`, `1e-30`, `-inf`.\n-        for k in [\"attention_mask\", \"decoder_attention_mask\"]:\n-            attention_mask = tf_inputs_dict[k]\n-\n-            # Make sure no all 0s attention masks - to avoid failure at this moment.\n-            # Put `1` at the beginning of sequences to make it still work when combining causal attention masks.\n-            # TODO: remove this line once a fix regarding large negative values for attention mask is done.\n-            attention_mask = tf.concat(\n-                [tf.ones_like(attention_mask[:, :1], dtype=attention_mask.dtype), attention_mask[:, 1:]], axis=-1\n-            )\n-            tf_inputs_dict[k] = attention_mask\n-\n-        tf_inputs_dict_with_labels = copy.copy(tf_inputs_dict)\n-        tf_inputs_dict_with_labels[\"labels\"] = labels\n-\n-        self.assertTrue(decoder_config.cross_attention_hidden_size is None)\n-\n-        # Original test: check without `labels` and  without `enc_to_dec_proj` projection\n-        self.assertTrue(config.hidden_size == decoder_config.hidden_size)\n-        self.check_pt_to_tf_equivalence(config, decoder_config, tf_inputs_dict)\n-        self.check_tf_to_pt_equivalence(config, decoder_config, tf_inputs_dict)\n-\n-        # check with `labels`\n-        self.check_pt_to_tf_equivalence(config, decoder_config, tf_inputs_dict_with_labels)\n-        self.check_tf_to_pt_equivalence(config, decoder_config, tf_inputs_dict_with_labels)\n-\n-        # check `enc_to_dec_proj` work as expected\n-        decoder_config.hidden_size = decoder_config.hidden_size * 2\n-        self.assertTrue(config.hidden_size != decoder_config.hidden_size)\n-        self.check_pt_to_tf_equivalence(config, decoder_config, tf_inputs_dict)\n-        self.check_tf_to_pt_equivalence(config, decoder_config, tf_inputs_dict)\n-\n     def test_model_save_load_from_pretrained(self):\n         model_2 = self.get_pretrained_model()\n         input_ids = ids_tensor([13, 5], model_2.config.encoder.vocab_size)\n@@ -761,44 +506,6 @@ def prepare_config_and_inputs(self):\n             \"labels\": decoder_token_labels,\n         }\n \n-    @slow\n-    @is_pt_tf_cross_test\n-    def test_bert2bert_summarization(self):\n-        from transformers import EncoderDecoderModel\n-\n-        tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n-\n-        \"\"\"Not working, because pt checkpoint has `encoder.encoder.layer...` while tf model has `encoder.bert.encoder.layer...`.\n-        (For Bert decoder, there is no issue, because `BertModel` is wrapped into `decoder` as `bert`)\n-        model = TFEncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\", from_pt=True)\n-        \"\"\"\n-\n-        # workaround to load from pt\n-        _model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\")\n-        _model.encoder.save_pretrained(\"./encoder\")\n-        _model.decoder.save_pretrained(\"./decoder\")\n-        model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(\n-            \"./encoder\", \"./decoder\", encoder_from_pt=True, decoder_from_pt=True\n-        )\n-        model.config = _model.config\n-\n-        ARTICLE_STUDENTS = \"\"\"(CNN)Sigma Alpha Epsilon is under fire for a video showing party-bound fraternity members singing a racist chant. SAE's national chapter suspended the students, but University of Oklahoma President David Boren took it a step further, saying the university's affiliation with the fraternity is permanently done. The news is shocking, but it's not the first time SAE has faced controversy. SAE was founded March 9, 1856, at the University of Alabama, five years before the American Civil War, according to the fraternity website. When the war began, the group had fewer than 400 members, of which \"369 went to war for the Confederate States and seven for the Union Army,\" the website says. The fraternity now boasts more than 200,000 living alumni, along with about 15,000 undergraduates populating 219 chapters and 20 \"colonies\" seeking full membership at universities. SAE has had to work hard to change recently after a string of member deaths, many blamed on the hazing of new recruits, SAE national President Bradley Cohen wrote in a message on the fraternity's website. The fraternity's website lists more than 130 chapters cited or suspended for \"health and safety incidents\" since 2010. At least 30 of the incidents involved hazing, and dozens more involved alcohol. However, the list is missing numerous incidents from recent months. Among them, according to various media outlets: Yale University banned the SAEs from campus activities last month after members allegedly tried to interfere with a sexual misconduct investigation connected to an initiation rite. Stanford University in December suspended SAE housing privileges after finding sorority members attending a fraternity function were subjected to graphic sexual content. And Johns Hopkins University in November suspended the fraternity for underage drinking. \"The media has labeled us as the 'nation's deadliest fraternity,' \" Cohen said. In 2011, for example, a student died while being coerced into excessive alcohol consumption, according to a lawsuit. SAE's previous insurer dumped the fraternity. \"As a result, we are paying Lloyd's of London the highest insurance rates in the Greek-letter world,\" Cohen said. Universities have turned down SAE's attempts to open new chapters, and the fraternity had to close 12 in 18 months over hazing incidents.\"\"\"\n-        EXPECTED_SUMMARY_STUDENTS = \"\"\"sae was founded in 1856, five years before the civil war. the fraternity has had to work hard to change recently. the university of oklahoma president says the university's affiliation with the fraternity is permanently done. the sae has had a string of members in recent months.\"\"\"\n-\n-        input_dict = tokenizer(ARTICLE_STUDENTS, return_tensors=\"tf\")\n-        output_ids = model.generate(input_ids=input_dict[\"input_ids\"]).numpy().tolist()\n-        summary = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n-\n-        self.assertEqual(summary, [EXPECTED_SUMMARY_STUDENTS])\n-\n-        # Test with the TF checkpoint\n-        model = TFEncoderDecoderModel.from_pretrained(\"ydshieh/bert2bert-cnn_dailymail-fp16\")\n-\n-        output_ids = model.generate(input_ids=input_dict[\"input_ids\"]).numpy().tolist()\n-        summary = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n-\n-        self.assertEqual(summary, [EXPECTED_SUMMARY_STUDENTS])\n-\n \n @require_tf\n class TFGPT2EncoderDecoderModelTest(TFEncoderDecoderMixin, unittest.TestCase):\n@@ -861,37 +568,6 @@ def prepare_config_and_inputs(self):\n             \"labels\": decoder_token_labels,\n         }\n \n-    @slow\n-    @is_pt_tf_cross_test\n-    def test_bert2gpt2_summarization(self):\n-        from transformers import EncoderDecoderModel\n-\n-        tokenizer_in = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-        tokenizer_out = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n-\n-        \"\"\"Not working, because pt checkpoint has `encoder.encoder.layer...` while tf model has `encoder.bert.encoder.layer...`.\n-        (For GPT2 decoder, there is no issue)\n-        model = TFEncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2gpt2-cnn_dailymail-fp16\", from_pt=True)\n-        \"\"\"\n-\n-        # workaround to load from pt\n-        _model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2gpt2-cnn_dailymail-fp16\")\n-        _model.encoder.save_pretrained(\"./encoder\")\n-        _model.decoder.save_pretrained(\"./decoder\")\n-        model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(\n-            \"./encoder\", \"./decoder\", encoder_from_pt=True, decoder_from_pt=True\n-        )\n-        model.config = _model.config\n-\n-        ARTICLE_STUDENTS = \"\"\"(CNN)Sigma Alpha Epsilon is under fire for a video showing party-bound fraternity members singing a racist chant. SAE's national chapter suspended the students, but University of Oklahoma President David Boren took it a step further, saying the university's affiliation with the fraternity is permanently done. The news is shocking, but it's not the first time SAE has faced controversy. SAE was founded March 9, 1856, at the University of Alabama, five years before the American Civil War, according to the fraternity website. When the war began, the group had fewer than 400 members, of which \"369 went to war for the Confederate States and seven for the Union Army,\" the website says. The fraternity now boasts more than 200,000 living alumni, along with about 15,000 undergraduates populating 219 chapters and 20 \"colonies\" seeking full membership at universities. SAE has had to work hard to change recently after a string of member deaths, many blamed on the hazing of new recruits, SAE national President Bradley Cohen wrote in a message on the fraternity's website. The fraternity's website lists more than 130 chapters cited or suspended for \"health and safety incidents\" since 2010. At least 30 of the incidents involved hazing, and dozens more involved alcohol. However, the list is missing numerous incidents from recent months. Among them, according to various media outlets: Yale University banned the SAEs from campus activities last month after members allegedly tried to interfere with a sexual misconduct investigation connected to an initiation rite. Stanford University in December suspended SAE housing privileges after finding sorority members attending a fraternity function were subjected to graphic sexual content. And Johns Hopkins University in November suspended the fraternity for underage drinking. \"The media has labeled us as the 'nation's deadliest fraternity,' \" Cohen said. In 2011, for example, a student died while being coerced into excessive alcohol consumption, according to a lawsuit. SAE's previous insurer dumped the fraternity. \"As a result, we are paying Lloyd's of London the highest insurance rates in the Greek-letter world,\" Cohen said. Universities have turned down SAE's attempts to open new chapters, and the fraternity had to close 12 in 18 months over hazing incidents.\"\"\"\n-        EXPECTED_SUMMARY_STUDENTS = \"\"\"SAS Alpha Epsilon suspended the students, but university president says it's permanent.\\nThe fraternity has had to deal with a string of student deaths since 2010.\\nSAS has more than 200,000 members, many of whom are students.\\nA student died while being forced into excessive alcohol consumption.\"\"\"\n-\n-        input_dict = tokenizer_in(ARTICLE_STUDENTS, return_tensors=\"tf\")\n-        output_ids = model.generate(input_ids=input_dict[\"input_ids\"]).numpy().tolist()\n-        summary = tokenizer_out.batch_decode(output_ids, skip_special_tokens=True)\n-\n-        self.assertEqual(summary, [EXPECTED_SUMMARY_STUDENTS])\n-\n \n @require_tf\n class TFRoBertaEncoderDecoderModelTest(TFEncoderDecoderMixin, unittest.TestCase):\n@@ -1113,54 +789,6 @@ def test_encoder_decoder_save_load_from_encoder_decoder(self):\n         max_diff = np.max(np.abs(logits_2.numpy() - logits_orig.numpy()))\n         self.assertAlmostEqual(max_diff, 0.0, places=4)\n \n-    @require_torch\n-    @is_pt_tf_cross_test\n-    def test_encoder_decoder_save_load_from_encoder_decoder_from_pt(self):\n-        config = self.get_encoder_decoder_config_small()\n-\n-        # create two random BERT models for bert2bert & initialize weights (+cross_attention weights)\n-        encoder_pt = BertModel(config.encoder).to(torch_device).eval()\n-        decoder_pt = BertLMHeadModel(config.decoder).to(torch_device).eval()\n-\n-        encoder_decoder_pt = EncoderDecoderModel(encoder=encoder_pt, decoder=decoder_pt).to(torch_device).eval()\n-\n-        input_ids = ids_tensor([13, 5], encoder_pt.config.vocab_size)\n-        decoder_input_ids = ids_tensor([13, 1], decoder_pt.config.vocab_size)\n-\n-        pt_input_ids = torch.tensor(input_ids.numpy(), device=torch_device, dtype=torch.long)\n-        pt_decoder_input_ids = torch.tensor(decoder_input_ids.numpy(), device=torch_device, dtype=torch.long)\n-\n-        logits_pt = encoder_decoder_pt(input_ids=pt_input_ids, decoder_input_ids=pt_decoder_input_ids).logits\n-\n-        # PyTorch => TensorFlow\n-        with tempfile.TemporaryDirectory() as tmp_dirname_1, tempfile.TemporaryDirectory() as tmp_dirname_2:\n-            encoder_decoder_pt.encoder.save_pretrained(tmp_dirname_1)\n-            encoder_decoder_pt.decoder.save_pretrained(tmp_dirname_2)\n-            encoder_decoder_tf = TFEncoderDecoderModel.from_encoder_decoder_pretrained(tmp_dirname_1, tmp_dirname_2)\n-\n-        logits_tf = encoder_decoder_tf(input_ids=input_ids, decoder_input_ids=decoder_input_ids).logits\n-\n-        max_diff = np.max(np.abs(logits_pt.detach().cpu().numpy() - logits_tf.numpy()))\n-        self.assertAlmostEqual(max_diff, 0.0, places=3)\n-\n-        # Make sure `from_pretrained` following `save_pretrained` work and give the same result\n-        with tempfile.TemporaryDirectory() as tmp_dirname:\n-            encoder_decoder_tf.save_pretrained(tmp_dirname)\n-            encoder_decoder_tf = TFEncoderDecoderModel.from_pretrained(tmp_dirname)\n-\n-            logits_tf_2 = encoder_decoder_tf(input_ids=input_ids, decoder_input_ids=decoder_input_ids).logits\n-\n-            max_diff = np.max(np.abs(logits_tf_2.numpy() - logits_tf.numpy()))\n-            self.assertAlmostEqual(max_diff, 0.0, places=3)\n-\n-        # TensorFlow => PyTorch\n-        with tempfile.TemporaryDirectory() as tmp_dirname:\n-            encoder_decoder_tf.save_pretrained(tmp_dirname, safe_serialization=False)\n-            encoder_decoder_pt = EncoderDecoderModel.from_pretrained(tmp_dirname, from_tf=True)\n-\n-        max_diff = np.max(np.abs(logits_pt.detach().cpu().numpy() - logits_tf.numpy()))\n-        self.assertAlmostEqual(max_diff, 0.0, places=3)\n-\n     @slow\n     def test_encoder_decoder_from_pretrained(self):\n         load_weight_prefix = TFEncoderDecoderModel.load_weight_prefix"
        },
        {
            "sha": "e235033747512c517c591d37fb05d0020f136458",
            "filename": "tests/models/groupvit/test_modeling_groupvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 29,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -24,7 +24,7 @@\n import requests\n \n from transformers import GroupViTConfig, GroupViTTextConfig, GroupViTVisionConfig\n-from transformers.testing_utils import is_flaky, is_pt_tf_cross_test, require_torch, require_vision, slow, torch_device\n+from transformers.testing_utils import is_flaky, require_torch, require_vision, slow, torch_device\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_configuration_common import ConfigTester\n@@ -166,18 +166,6 @@ def test_inputs_embeds(self):\n     def test_batching_equivalence(self):\n         super().test_batching_equivalence()\n \n-    @is_pt_tf_cross_test\n-    def test_pt_tf_model_equivalence(self):\n-        import tensorflow as tf\n-\n-        seed = 338\n-        random.seed(seed)\n-        np.random.seed(seed)\n-        torch.manual_seed(seed)\n-        torch.cuda.manual_seed_all(seed)\n-        tf.random.set_seed(seed)\n-        return super().test_pt_tf_model_equivalence()\n-\n     def test_model_get_set_embeddings(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -595,22 +583,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    # overwritten from parent as this equivalent test needs a specific `seed` and hard to get a good one!\n-    def check_pt_tf_outputs(self, tf_outputs, pt_outputs, model_class, tol=2e-5, name=\"outputs\", attributes=None):\n-        super().check_pt_tf_outputs(tf_outputs, pt_outputs, model_class, tol=tol, name=name, attributes=attributes)\n-\n-    @is_pt_tf_cross_test\n-    def test_pt_tf_model_equivalence(self):\n-        import tensorflow as tf\n-\n-        seed = 163\n-        random.seed(seed)\n-        np.random.seed(seed)\n-        torch.manual_seed(seed)\n-        torch.cuda.manual_seed_all(seed)\n-        tf.random.set_seed(seed)\n-        return super().test_pt_tf_model_equivalence()\n-\n     # override as the `logit_scale` parameter initilization is different for GROUPVIT\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "06c37dcd40a8f52c82256a200a75f71dc32a7335",
            "filename": "tests/models/groupvit/test_modeling_tf_groupvit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 52,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_tf_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_tf_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_tf_groupvit.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -23,12 +23,10 @@\n import unittest\n from importlib import import_module\n \n-import numpy as np\n import requests\n \n from transformers import GroupViTConfig, GroupViTTextConfig, GroupViTVisionConfig\n from transformers.testing_utils import (\n-    is_pt_tf_cross_test,\n     require_tensorflow_probability,\n     require_tf,\n     require_vision,\n@@ -149,10 +147,6 @@ class TFGroupViTVisionModelTest(TFModelTesterMixin, unittest.TestCase):\n     test_head_masking = False\n     test_onnx = False\n \n-    def check_pt_tf_outputs(self, tf_outputs, pt_outputs, model_class, tol=1e-4, name=\"outputs\", attributes=None):\n-        # We override with a slightly higher tol value, as this model tends to diverge a bit more\n-        super().check_pt_tf_outputs(tf_outputs, pt_outputs, model_class, tol, name, attributes)\n-\n     def setUp(self):\n         self.model_tester = TFGroupViTVisionModelTester(self)\n         self.config_tester = ConfigTester(\n@@ -291,25 +285,6 @@ def check_hidden_states_output(inputs_dict, config, model_class):\n \n             check_hidden_states_output(inputs_dict, config, model_class)\n \n-    @is_pt_tf_cross_test\n-    def test_pt_tf_model_equivalence(self):\n-        # `GroupViT` computes some indices using argmax, uses them as\n-        # one-hot encoding for further computation. The problem is\n-        # while PT/TF have very small difference in `y_soft` (~ 1e-9),\n-        # the argmax could be totally different, if there are at least\n-        # 2 indices with almost identical values. This leads to very\n-        # large difference in the outputs. We need specific seeds to\n-        # avoid almost identical values happening in `y_soft`.\n-        import torch\n-\n-        seed = 338\n-        random.seed(seed)\n-        np.random.seed(seed)\n-        torch.manual_seed(seed)\n-        torch.cuda.manual_seed_all(seed)\n-        tf.random.set_seed(seed)\n-        return super().test_pt_tf_model_equivalence()\n-\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"nvidia/groupvit-gcc-yfcc\"\n@@ -462,10 +437,6 @@ class TFGroupViTTextModelTest(TFModelTesterMixin, unittest.TestCase):\n     test_head_masking = False\n     test_onnx = False\n \n-    def check_pt_tf_outputs(self, tf_outputs, pt_outputs, model_class, tol=1e-4, name=\"outputs\", attributes=None):\n-        # We override with a slightly higher tol value, as this model tends to diverge a bit more\n-        super().check_pt_tf_outputs(tf_outputs, pt_outputs, model_class, tol, name, attributes)\n-\n     def setUp(self):\n         self.model_tester = TFGroupViTTextModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=GroupViTTextConfig, hidden_size=37)\n@@ -588,10 +559,6 @@ class TFGroupViTModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.Test\n     test_attention_outputs = False\n     test_onnx = False\n \n-    def check_pt_tf_outputs(self, tf_outputs, pt_outputs, model_class, tol=1e-4, name=\"outputs\", attributes=None):\n-        # We override with a slightly higher tol value, as this model tends to diverge a bit more\n-        super().check_pt_tf_outputs(tf_outputs, pt_outputs, model_class, tol, name, attributes)\n-\n     def setUp(self):\n         self.model_tester = TFGroupViTModelTester(self)\n \n@@ -616,25 +583,6 @@ def test_model_common_attributes(self):\n     def test_keras_fit(self):\n         super().test_keras_fit()\n \n-    @is_pt_tf_cross_test\n-    def test_pt_tf_model_equivalence(self):\n-        # `GroupViT` computes some indices using argmax, uses them as\n-        # one-hot encoding for further computation. The problem is\n-        # while PT/TF have very small difference in `y_soft` (~ 1e-9),\n-        # the argmax could be totally different, if there are at least\n-        # 2 indices with almost identical values. This leads to very\n-        # large difference in the outputs. We need specific seeds to\n-        # avoid almost identical values happening in `y_soft`.\n-        import torch\n-\n-        seed = 158\n-        random.seed(seed)\n-        np.random.seed(seed)\n-        torch.manual_seed(seed)\n-        torch.cuda.manual_seed_all(seed)\n-        tf.random.set_seed(seed)\n-        return super().test_pt_tf_model_equivalence()\n-\n     # overwrite from common since `TFGroupViTModelTester` set `return_loss` to `True` and causes the preparation of\n     # `symbolic_inputs` failed.\n     def test_keras_save_load(self):"
        },
        {
            "sha": "6372fc13a140443a50b8d3e079fd7e0a793c3cfc",
            "filename": "tests/models/hubert/test_modeling_tf_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 115,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fhubert%2Ftest_modeling_tf_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fhubert%2Ftest_modeling_tf_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhubert%2Ftest_modeling_tf_hubert.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -19,15 +19,13 @@\n import copy\n import inspect\n import math\n-import os\n-import tempfile\n import unittest\n \n import numpy as np\n import pytest\n \n from transformers import is_tf_available\n-from transformers.testing_utils import is_pt_tf_cross_test, require_soundfile, require_tf, slow\n+from transformers.testing_utils import require_soundfile, require_tf, slow\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor\n@@ -337,62 +335,6 @@ def test_keras_fit(self):\n         # TODO: (Amy) - check whether skipping CTC model resolves this issue and possible resolutions for CTC\n         pass\n \n-    @is_pt_tf_cross_test\n-    def test_pt_tf_model_equivalence(self, allow_missing_keys=False):\n-        # We override the base test here to skip loss calculation for Hubert models because the loss is massive with\n-        # the default labels and frequently overflows to inf or exceeds numerical tolerances between TF/PT\n-        import torch\n-\n-        import transformers\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            # Output all for aggressive testing\n-            config.output_hidden_states = True\n-            config.output_attentions = self.has_attentions\n-\n-            # Make sure no sequence has all zeros as attention mask, otherwise some tests fail due to the inconsistency\n-            # of the usage `1e-4`, `1e-9`, `1e-30`, `-inf`.\n-            # TODO: Use a uniform value for all models, make sure all tests pass without this processing, and remove it.\n-            self._make_attention_mask_non_null(inputs_dict)\n-\n-            pt_model_class_name = model_class.__name__[2:]  # Skip the \"TF\" at the beginning\n-            pt_model_class = getattr(transformers, pt_model_class_name)\n-\n-            tf_model = model_class(config)\n-            pt_model = pt_model_class(config)\n-\n-            tf_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-\n-            # Check we can load pt model in tf and vice-versa with model => model functions\n-            tf_model = transformers.load_pytorch_model_in_tf2_model(\n-                tf_model, pt_model, tf_inputs=tf_inputs_dict, allow_missing_keys=allow_missing_keys\n-            )\n-            pt_model = transformers.load_tf2_model_in_pytorch_model(\n-                pt_model, tf_model, allow_missing_keys=allow_missing_keys\n-            )\n-\n-            # Original test: check without `labels`\n-            self.check_pt_tf_models(tf_model, pt_model, tf_inputs_dict)\n-\n-            # Check we can load pt model in tf and vice-versa with checkpoint => model functions\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_checkpoint_path = os.path.join(tmpdirname, \"pt_model.bin\")\n-                torch.save(pt_model.state_dict(), pt_checkpoint_path)\n-                tf_model = transformers.load_pytorch_checkpoint_in_tf2_model(\n-                    tf_model, pt_checkpoint_path, allow_missing_keys=allow_missing_keys\n-                )\n-\n-                tf_checkpoint_path = os.path.join(tmpdirname, \"tf_model.h5\")\n-                tf_model.save_weights(tf_checkpoint_path)\n-                pt_model = transformers.load_tf2_checkpoint_in_pytorch_model(\n-                    pt_model, tf_checkpoint_path, allow_missing_keys=allow_missing_keys\n-                )\n-\n-            # Original test: check without `labels`\n-            self.check_pt_tf_models(tf_model, pt_model, tf_inputs_dict)\n-\n \n @require_tf\n class TFHubertRobustModelTest(TFModelTesterMixin, unittest.TestCase):\n@@ -518,62 +460,6 @@ def test_keras_fit(self):\n         # TODO: (Amy) - check whether skipping CTC model resolves this issue and possible resolutions for CTC\n         pass\n \n-    @is_pt_tf_cross_test\n-    def test_pt_tf_model_equivalence(self, allow_missing_keys=False):\n-        # We override the base test here to skip loss calculation for Hubert models because the loss is massive with\n-        # the default labels and frequently overflows to inf or exceeds numerical tolerances between TF/PT\n-        import torch\n-\n-        import transformers\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            # Output all for aggressive testing\n-            config.output_hidden_states = True\n-            config.output_attentions = self.has_attentions\n-\n-            # Make sure no sequence has all zeros as attention mask, otherwise some tests fail due to the inconsistency\n-            # of the usage `1e-4`, `1e-9`, `1e-30`, `-inf`.\n-            # TODO: Use a uniform value for all models, make sure all tests pass without this processing, and remove it.\n-            self._make_attention_mask_non_null(inputs_dict)\n-\n-            pt_model_class_name = model_class.__name__[2:]  # Skip the \"TF\" at the beginning\n-            pt_model_class = getattr(transformers, pt_model_class_name)\n-\n-            tf_model = model_class(config)\n-            pt_model = pt_model_class(config)\n-\n-            tf_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-\n-            # Check we can load pt model in tf and vice-versa with model => model functions\n-            tf_model = transformers.load_pytorch_model_in_tf2_model(\n-                tf_model, pt_model, tf_inputs=tf_inputs_dict, allow_missing_keys=allow_missing_keys\n-            )\n-            pt_model = transformers.load_tf2_model_in_pytorch_model(\n-                pt_model, tf_model, allow_missing_keys=allow_missing_keys\n-            )\n-\n-            # Original test: check without `labels`\n-            self.check_pt_tf_models(tf_model, pt_model, tf_inputs_dict)\n-\n-            # Check we can load pt model in tf and vice-versa with checkpoint => model functions\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_checkpoint_path = os.path.join(tmpdirname, \"pt_model.bin\")\n-                torch.save(pt_model.state_dict(), pt_checkpoint_path)\n-                tf_model = transformers.load_pytorch_checkpoint_in_tf2_model(\n-                    tf_model, pt_checkpoint_path, allow_missing_keys=allow_missing_keys\n-                )\n-\n-                tf_checkpoint_path = os.path.join(tmpdirname, \"tf_model.h5\")\n-                tf_model.save_weights(tf_checkpoint_path)\n-                pt_model = transformers.load_tf2_checkpoint_in_pytorch_model(\n-                    pt_model, tf_checkpoint_path, allow_missing_keys=allow_missing_keys\n-                )\n-\n-            # Original test: check without `labels`\n-            self.check_pt_tf_models(tf_model, pt_model, tf_inputs_dict)\n-\n \n @require_tf\n class TFHubertUtilsTest(unittest.TestCase):"
        },
        {
            "sha": "96f0baac9070d6956f27d3abcf22cd761aa59ebc",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -23,7 +23,6 @@\n from transformers import BitsAndBytesConfig, IdeficsConfig, is_torch_available, is_vision_available\n from transformers.testing_utils import (\n     TestCasePlus,\n-    is_pt_tf_cross_test,\n     require_bitsandbytes,\n     require_torch,\n     require_torch_sdpa,\n@@ -574,11 +573,6 @@ def check_hidden_states_output(inputs_dict, config, model_class):\n \n             check_hidden_states_output(inputs_dict, config, model_class)\n \n-    @is_pt_tf_cross_test\n-    def test_pt_tf_model_equivalence(self, allow_missing_keys=False):\n-        self.has_attentions = False\n-        super().test_pt_tf_model_equivalence(allow_missing_keys=allow_missing_keys)\n-\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"HuggingFaceM4/idefics-9b\""
        },
        {
            "sha": "bbb997aa6f9cf23818315cdd76b77fb68ee3e27e",
            "filename": "tests/models/idefics/test_modeling_tf_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fidefics%2Ftest_modeling_tf_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fidefics%2Ftest_modeling_tf_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_tf_idefics.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -20,7 +20,7 @@\n from importlib import import_module\n \n from transformers import IdeficsConfig, is_tf_available, is_vision_available\n-from transformers.testing_utils import TestCasePlus, is_pt_tf_cross_test, require_tf, require_vision, slow\n+from transformers.testing_utils import TestCasePlus, require_tf, require_vision, slow\n from transformers.utils import cached_property\n \n from ...test_configuration_common import ConfigTester\n@@ -420,11 +420,6 @@ def check_hidden_states_output(inputs_dict, config, model_class):\n \n             check_hidden_states_output(inputs_dict, config, model_class)\n \n-    @is_pt_tf_cross_test\n-    def test_pt_tf_model_equivalence(self, allow_missing_keys=False):\n-        self.has_attentions = False\n-        super().test_pt_tf_model_equivalence(allow_missing_keys=allow_missing_keys)\n-\n     def test_keras_save_load(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "c7594ea78ae57f0843636b70e3fe5620ac982afb",
            "filename": "tests/models/layoutlmv2/test_tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -41,7 +41,6 @@\n     _is_whitespace,\n )\n from transformers.testing_utils import (\n-    is_pt_tf_cross_test,\n     require_detectron2,\n     require_pandas,\n     require_tokenizers,\n@@ -1497,48 +1496,6 @@ def test_layoutlmv2_truncation_integration_test(self):\n         self.assertListEqual(new_encoded_inputs, dropped_encoded_inputs)\n         self.assertLessEqual(len(new_encoded_inputs), 20)\n \n-    @is_pt_tf_cross_test\n-    def test_batch_encode_plus_tensors(self):\n-        tokenizers = self.get_tokenizers(do_lower_case=False)\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                words, boxes = self.get_words_and_boxes_batch()\n-\n-                # A Tensor cannot be build by sequences which are not the same size\n-                self.assertRaises(ValueError, tokenizer.batch_encode_plus, words, boxes=boxes, return_tensors=\"pt\")\n-                self.assertRaises(ValueError, tokenizer.batch_encode_plus, words, boxes=boxes, return_tensors=\"tf\")\n-\n-                if tokenizer.pad_token_id is None:\n-                    self.assertRaises(\n-                        ValueError,\n-                        tokenizer.batch_encode_plus,\n-                        words,\n-                        boxes=boxes,\n-                        padding=True,\n-                        return_tensors=\"pt\",\n-                    )\n-                    self.assertRaises(\n-                        ValueError,\n-                        tokenizer.batch_encode_plus,\n-                        words,\n-                        boxes=boxes,\n-                        padding=\"longest\",\n-                        return_tensors=\"tf\",\n-                    )\n-                else:\n-                    pytorch_tensor = tokenizer.batch_encode_plus(words, boxes=boxes, padding=True, return_tensors=\"pt\")\n-                    tensorflow_tensor = tokenizer.batch_encode_plus(\n-                        words, boxes=boxes, padding=\"longest\", return_tensors=\"tf\"\n-                    )\n-                    encoded_sequences = tokenizer.batch_encode_plus(words, boxes=boxes, padding=True)\n-\n-                    for key in encoded_sequences.keys():\n-                        pytorch_value = pytorch_tensor[key].tolist()\n-                        tensorflow_value = tensorflow_tensor[key].numpy().tolist()\n-                        encoded_value = encoded_sequences[key]\n-\n-                        self.assertEqual(pytorch_value, tensorflow_value, encoded_value)\n-\n     def test_sequence_ids(self):\n         tokenizers = self.get_tokenizers()\n         for tokenizer in tokenizers:"
        },
        {
            "sha": "87babd853b3d8b1fbaf12c35b6c274dfe0236725",
            "filename": "tests/models/layoutlmv3/test_tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -34,7 +34,6 @@\n )\n from transformers.models.layoutlmv3.tokenization_layoutlmv3 import VOCAB_FILES_NAMES, LayoutLMv3Tokenizer\n from transformers.testing_utils import (\n-    is_pt_tf_cross_test,\n     require_pandas,\n     require_tf,\n     require_tokenizers,\n@@ -1375,48 +1374,6 @@ def test_layoutlmv3_truncation_integration_test(self):\n         self.assertListEqual(new_encoded_inputs, dropped_encoded_inputs)\n         self.assertLessEqual(len(new_encoded_inputs), 20)\n \n-    @is_pt_tf_cross_test\n-    def test_batch_encode_plus_tensors(self):\n-        tokenizers = self.get_tokenizers(do_lower_case=False)\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                words, boxes = self.get_words_and_boxes_batch()\n-\n-                # A Tensor cannot be build by sequences which are not the same size\n-                self.assertRaises(ValueError, tokenizer.batch_encode_plus, words, boxes=boxes, return_tensors=\"pt\")\n-                self.assertRaises(ValueError, tokenizer.batch_encode_plus, words, boxes=boxes, return_tensors=\"tf\")\n-\n-                if tokenizer.pad_token_id is None:\n-                    self.assertRaises(\n-                        ValueError,\n-                        tokenizer.batch_encode_plus,\n-                        words,\n-                        boxes=boxes,\n-                        padding=True,\n-                        return_tensors=\"pt\",\n-                    )\n-                    self.assertRaises(\n-                        ValueError,\n-                        tokenizer.batch_encode_plus,\n-                        words,\n-                        boxes=boxes,\n-                        padding=\"longest\",\n-                        return_tensors=\"tf\",\n-                    )\n-                else:\n-                    pytorch_tensor = tokenizer.batch_encode_plus(words, boxes=boxes, padding=True, return_tensors=\"pt\")\n-                    tensorflow_tensor = tokenizer.batch_encode_plus(\n-                        words, boxes=boxes, padding=\"longest\", return_tensors=\"tf\"\n-                    )\n-                    encoded_sequences = tokenizer.batch_encode_plus(words, boxes=boxes, padding=True)\n-\n-                    for key in encoded_sequences.keys():\n-                        pytorch_value = pytorch_tensor[key].tolist()\n-                        tensorflow_value = tensorflow_tensor[key].numpy().tolist()\n-                        encoded_value = encoded_sequences[key]\n-\n-                        self.assertEqual(pytorch_value, tensorflow_value, encoded_value)\n-\n     def test_sequence_ids(self):\n         tokenizers = self.get_tokenizers()\n         for tokenizer in tokenizers:"
        },
        {
            "sha": "d933dca92f3d277da0f069aa57d9f051509fe7ce",
            "filename": "tests/models/layoutxlm/test_tokenization_layoutxlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -32,7 +32,6 @@\n from transformers.models.layoutxlm.tokenization_layoutxlm import LayoutXLMTokenizer\n from transformers.testing_utils import (\n     get_tests_dir,\n-    is_pt_tf_cross_test,\n     require_pandas,\n     require_sentencepiece,\n     require_tokenizers,\n@@ -1426,48 +1425,6 @@ def test_layoutxlm_truncation_integration_test(self):\n         self.assertListEqual(new_encoded_inputs, dropped_encoded_inputs)\n         self.assertLessEqual(len(new_encoded_inputs), 20)\n \n-    @is_pt_tf_cross_test\n-    def test_batch_encode_plus_tensors(self):\n-        tokenizers = self.get_tokenizers(do_lower_case=False)\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                words, boxes = self.get_words_and_boxes_batch()\n-\n-                # A Tensor cannot be build by sequences which are not the same size\n-                self.assertRaises(ValueError, tokenizer.batch_encode_plus, words, boxes=boxes, return_tensors=\"pt\")\n-                self.assertRaises(ValueError, tokenizer.batch_encode_plus, words, boxes=boxes, return_tensors=\"tf\")\n-\n-                if tokenizer.pad_token_id is None:\n-                    self.assertRaises(\n-                        ValueError,\n-                        tokenizer.batch_encode_plus,\n-                        words,\n-                        boxes=boxes,\n-                        padding=True,\n-                        return_tensors=\"pt\",\n-                    )\n-                    self.assertRaises(\n-                        ValueError,\n-                        tokenizer.batch_encode_plus,\n-                        words,\n-                        boxes=boxes,\n-                        padding=\"longest\",\n-                        return_tensors=\"tf\",\n-                    )\n-                else:\n-                    pytorch_tensor = tokenizer.batch_encode_plus(words, boxes=boxes, padding=True, return_tensors=\"pt\")\n-                    tensorflow_tensor = tokenizer.batch_encode_plus(\n-                        words, boxes=boxes, padding=\"longest\", return_tensors=\"tf\"\n-                    )\n-                    encoded_sequences = tokenizer.batch_encode_plus(words, boxes=boxes, padding=True)\n-\n-                    for key in encoded_sequences.keys():\n-                        pytorch_value = pytorch_tensor[key].tolist()\n-                        tensorflow_value = tensorflow_tensor[key].numpy().tolist()\n-                        encoded_value = encoded_sequences[key]\n-\n-                        self.assertEqual(pytorch_value, tensorflow_value, encoded_value)\n-\n     def test_sequence_ids(self):\n         tokenizers = self.get_tokenizers()\n         for tokenizer in tokenizers:"
        },
        {
            "sha": "d2e4614dfcdff7adb489d28cf7c199284f703385",
            "filename": "tests/models/markuplm/test_tokenization_markuplm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 45,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -33,7 +33,7 @@\n     logging,\n )\n from transformers.models.markuplm.tokenization_markuplm import VOCAB_FILES_NAMES, MarkupLMTokenizer\n-from transformers.testing_utils import is_pt_tf_cross_test, require_tokenizers, require_torch, slow\n+from transformers.testing_utils import require_tokenizers, require_torch, slow\n \n from ...test_tokenization_common import SMALL_TRAINING_CORPUS, TokenizerTesterMixin, merge_model_tokenizer_mappings\n \n@@ -1258,50 +1258,6 @@ def test_markuplm_truncation_integration_test(self):\n         self.assertListEqual(new_encoded_inputs, dropped_encoded_inputs)\n         self.assertLessEqual(len(new_encoded_inputs), 20)\n \n-    @is_pt_tf_cross_test\n-    def test_batch_encode_plus_tensors(self):\n-        tokenizers = self.get_tokenizers(do_lower_case=False)\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                nodes, xpaths = self.get_nodes_and_xpaths_batch()\n-\n-                # A Tensor cannot be build by sequences which are not the same size\n-                self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, return_tensors=\"pt\")\n-                self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, return_tensors=\"tf\")\n-\n-                if tokenizer.pad_token_id is None:\n-                    self.assertRaises(\n-                        ValueError,\n-                        tokenizer.batch_encode_plus,\n-                        nodes,\n-                        xpaths=xpaths,\n-                        padding=True,\n-                        return_tensors=\"pt\",\n-                    )\n-                    self.assertRaises(\n-                        ValueError,\n-                        tokenizer.batch_encode_plus,\n-                        nodes,\n-                        xpaths=xpaths,\n-                        padding=\"longest\",\n-                        return_tensors=\"tf\",\n-                    )\n-                else:\n-                    pytorch_tensor = tokenizer.batch_encode_plus(\n-                        nodes, xpaths=xpaths, padding=True, return_tensors=\"pt\"\n-                    )\n-                    tensorflow_tensor = tokenizer.batch_encode_plus(\n-                        nodes, xpaths=xpaths, padding=\"longest\", return_tensors=\"tf\"\n-                    )\n-                    encoded_sequences = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding=True)\n-\n-                    for key in encoded_sequences.keys():\n-                        pytorch_value = pytorch_tensor[key].tolist()\n-                        tensorflow_value = tensorflow_tensor[key].numpy().tolist()\n-                        encoded_value = encoded_sequences[key]\n-\n-                        self.assertEqual(pytorch_value, tensorflow_value, encoded_value)\n-\n     def test_sequence_ids(self):\n         tokenizers = self.get_tokenizers()\n         for tokenizer in tokenizers:"
        },
        {
            "sha": "551feb56d0330acabc8042b3374d222cb8dbbcda",
            "filename": "tests/models/sam/test_modeling_sam.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -433,10 +433,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_hidden_states_output(self):\n         pass\n \n-    def check_pt_tf_outputs(self, tf_outputs, pt_outputs, model_class, tol=5e-5, name=\"outputs\", attributes=None):\n-        # Use a slightly higher default tol to make the tests non-flaky\n-        super().check_pt_tf_outputs(tf_outputs, pt_outputs, model_class, tol=tol, name=name, attributes=attributes)\n-\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"facebook/sam-vit-huge\""
        },
        {
            "sha": "478c2c3873bb759c128cf41d5ca3273bde519e68",
            "filename": "tests/models/sam/test_modeling_tf_sam.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fsam%2Ftest_modeling_tf_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fsam%2Ftest_modeling_tf_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_tf_sam.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -411,16 +411,6 @@ def test_model_from_pretrained(self):\n         model = TFSamModel.from_pretrained(\"facebook/sam-vit-base\")  # sam-vit-huge blows out our memory\n         self.assertIsNotNone(model)\n \n-    def check_pt_tf_outputs(self, tf_outputs, pt_outputs, model_class, tol=5e-4, name=\"outputs\", attributes=None):\n-        super().check_pt_tf_outputs(\n-            tf_outputs=tf_outputs,\n-            pt_outputs=pt_outputs,\n-            model_class=model_class,\n-            tol=tol,\n-            name=name,\n-            attributes=attributes,\n-        )\n-\n \n def prepare_image():\n     img_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\""
        },
        {
            "sha": "8c6cf1ce39aca89856246096b6e05f090e0126c8",
            "filename": "tests/models/sam/test_processor_sam.py",
            "status": "modified",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fsam%2Ftest_processor_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fsam%2Ftest_processor_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_processor_sam.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -18,7 +18,6 @@\n import numpy as np\n \n from transformers.testing_utils import (\n-    is_pt_tf_cross_test,\n     require_tf,\n     require_torch,\n     require_torchvision,\n@@ -340,42 +339,3 @@ def tearDown(self):\n     def prepare_image_inputs(self):\n         \"\"\"This function prepares a list of PIL images.\"\"\"\n         return prepare_image_inputs()\n-\n-    @is_pt_tf_cross_test\n-    def test_post_process_masks_equivalence(self):\n-        image_processor = self.get_image_processor()\n-\n-        processor = SamProcessor(image_processor=image_processor)\n-        dummy_masks = np.random.randint(0, 2, size=(1, 3, 5, 5)).astype(np.float32)\n-        tf_dummy_masks = [tf.convert_to_tensor(dummy_masks)]\n-        pt_dummy_masks = [torch.tensor(dummy_masks)]\n-\n-        original_sizes = [[1764, 2646]]\n-\n-        reshaped_input_size = [[683, 1024]]\n-        tf_masks = processor.post_process_masks(\n-            tf_dummy_masks, original_sizes, reshaped_input_size, return_tensors=\"tf\"\n-        )\n-        pt_masks = processor.post_process_masks(\n-            pt_dummy_masks, original_sizes, reshaped_input_size, return_tensors=\"pt\"\n-        )\n-\n-        self.assertTrue(np.all(tf_masks[0].numpy() == pt_masks[0].numpy()))\n-\n-    @is_pt_tf_cross_test\n-    def test_image_processor_equivalence(self):\n-        image_processor = self.get_image_processor()\n-\n-        processor = SamProcessor(image_processor=image_processor)\n-\n-        image_input = self.prepare_image_inputs()\n-\n-        pt_input_feat_extract = image_processor(image_input, return_tensors=\"pt\")[\"pixel_values\"].numpy()\n-        pt_input_processor = processor(images=image_input, return_tensors=\"pt\")[\"pixel_values\"].numpy()\n-\n-        tf_input_feat_extract = image_processor(image_input, return_tensors=\"tf\")[\"pixel_values\"].numpy()\n-        tf_input_processor = processor(images=image_input, return_tensors=\"tf\")[\"pixel_values\"].numpy()\n-\n-        self.assertTrue(np.allclose(pt_input_feat_extract, pt_input_processor))\n-        self.assertTrue(np.allclose(pt_input_feat_extract, tf_input_feat_extract))\n-        self.assertTrue(np.allclose(pt_input_feat_extract, tf_input_processor))"
        },
        {
            "sha": "384754ecbe096a86514640a0d83bc9aecf3ab3fa",
            "filename": "tests/models/segformer/test_modeling_tf_segformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fsegformer%2Ftest_modeling_tf_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fsegformer%2Ftest_modeling_tf_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsegformer%2Ftest_modeling_tf_segformer.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -431,10 +431,6 @@ def apply(model):\n                 model = model_class(config)\n                 apply(model)\n \n-    def check_pt_tf_outputs(self, tf_outputs, pt_outputs, model_class, tol=2e-4, name=\"outputs\", attributes=None):\n-        # We override with a slightly higher tol value, as semseg models tend to diverge a bit more\n-        super().check_pt_tf_outputs(tf_outputs, pt_outputs, model_class, tol, name, attributes)\n-\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"nvidia/segformer-b0-finetuned-ade-512-512\""
        },
        {
            "sha": "2edc283fc13c811ac8e6e2ddf583fe4bdc8c1b90",
            "filename": "tests/models/speech_to_text/test_modeling_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -693,10 +693,6 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n \n             self.assertTrue(models_equal)\n \n-    def test_pt_tf_model_equivalence(self, allow_missing_keys=True):\n-        # Allow missing keys since TF doesn't cache the sinusoidal embeddings in an attribute\n-        super().test_pt_tf_model_equivalence(allow_missing_keys=allow_missing_keys)\n-\n     @unittest.skip(reason=\"Test failing,  @RocketNight is looking into it\")\n     def test_tf_from_pt_safetensors(self):\n         pass"
        },
        {
            "sha": "dc7eec0dcae2c7acff23a1be66813ab5694d9c67",
            "filename": "tests/models/speech_to_text/test_modeling_tf_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -434,10 +434,6 @@ def test_forward_signature(self):\n             ]\n             self.assertListEqual(arg_names[: len(expected_arg_names)], expected_arg_names)\n \n-    def test_pt_tf_model_equivalence(self, allow_missing_keys=True):\n-        # Allow missing keys since TF doesn't cache the sinusoidal embeddings in an attribute\n-        super().test_pt_tf_model_equivalence(allow_missing_keys=allow_missing_keys)\n-\n \n @require_tf\n @require_sentencepiece"
        },
        {
            "sha": "c6f880b86dd1d4069a040e59134da6a06f1bc2f6",
            "filename": "tests/models/tapas/test_modeling_tapas.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Ftapas%2Ftest_modeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Ftapas%2Ftest_modeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftapas%2Ftest_modeling_tapas.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -32,7 +32,7 @@\n     is_torch_available,\n )\n from transformers.models.auto import get_values\n-from transformers.testing_utils import require_tensorflow_probability, require_torch, slow, torch_device\n+from transformers.testing_utils import require_torch, slow, torch_device\n from transformers.utils import cached_property\n \n from ...test_configuration_common import ConfigTester\n@@ -522,11 +522,6 @@ def test_for_sequence_classification(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_sequence_classification(*config_and_inputs)\n \n-    @require_tensorflow_probability\n-    @unittest.skip(reason=\"tfp is not defined even if installed. FIXME @Arthur in a followup PR!\")\n-    def test_pt_tf_model_equivalence(self):\n-        pass\n-\n     @unittest.skip(reason=\"tfp is not defined even if installed. FIXME @Arthur in a followup PR!\")\n     def test_tf_from_pt_safetensors(self):\n         pass"
        },
        {
            "sha": "11915b98bbe9d770dde6507a342aaa0cbc6f25bc",
            "filename": "tests/models/tapas/test_modeling_tf_tapas.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Ftapas%2Ftest_modeling_tf_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Ftapas%2Ftest_modeling_tf_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftapas%2Ftest_modeling_tf_tapas.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -535,10 +535,6 @@ def test_keras_fit(self):\n     def test_loss_computation(self):\n         pass\n \n-    @unittest.skip(\"tfp is not defined even if installed. FIXME @Arthur in a followup PR!\")\n-    def test_pt_tf_model_equivalence(self):\n-        pass\n-\n \n def prepare_tapas_single_inputs_for_inference():\n     # Here we prepare a single table-question pair to test TAPAS inference on:"
        },
        {
            "sha": "20d56b4ed4ce897d8606d71bf12018d4d107a344",
            "filename": "tests/models/tapas/test_tokenization_tapas.py",
            "status": "modified",
            "additions": 0,
            "deletions": 49,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -34,7 +34,6 @@\n     _is_whitespace,\n )\n from transformers.testing_utils import (\n-    is_pt_tf_cross_test,\n     require_pandas,\n     require_tensorflow_probability,\n     require_tokenizers,\n@@ -1158,54 +1157,6 @@ def test_min_max_question_length(self):\n \n         self.assertListEqual(encoding.input_ids[:2], expected_results)\n \n-    @is_pt_tf_cross_test\n-    def test_batch_encode_plus_tensors(self):\n-        tokenizers = self.get_tokenizers(do_lower_case=False)\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                sequences = [\n-                    \"Testing batch encode plus\",\n-                    \"Testing batch encode plus with different sequence lengths\",\n-                    \"Testing batch encode plus with different sequence lengths correctly pads\",\n-                ]\n-\n-                table = self.get_table(tokenizer, length=0)\n-\n-                # A Tensor cannot be build by sequences which are not the same size\n-                self.assertRaises(ValueError, tokenizer.batch_encode_plus, table, sequences, return_tensors=\"pt\")\n-                self.assertRaises(ValueError, tokenizer.batch_encode_plus, table, sequences, return_tensors=\"tf\")\n-\n-                if tokenizer.pad_token_id is None:\n-                    self.assertRaises(\n-                        ValueError,\n-                        tokenizer.batch_encode_plus,\n-                        table,\n-                        sequences,\n-                        padding=True,\n-                        return_tensors=\"pt\",\n-                    )\n-                    self.assertRaises(\n-                        ValueError,\n-                        tokenizer.batch_encode_plus,\n-                        table,\n-                        sequences,\n-                        padding=\"longest\",\n-                        return_tensors=\"tf\",\n-                    )\n-                else:\n-                    pytorch_tensor = tokenizer.batch_encode_plus(table, sequences, padding=True, return_tensors=\"pt\")\n-                    tensorflow_tensor = tokenizer.batch_encode_plus(\n-                        table, sequences, padding=\"longest\", return_tensors=\"tf\"\n-                    )\n-                    encoded_sequences = tokenizer.batch_encode_plus(table, sequences, padding=True)\n-\n-                    for key in encoded_sequences.keys():\n-                        pytorch_value = pytorch_tensor[key].tolist()\n-                        tensorflow_value = tensorflow_tensor[key].numpy().tolist()\n-                        encoded_value = encoded_sequences[key]\n-\n-                        self.assertEqual(pytorch_value, tensorflow_value, encoded_value)\n-\n     @slow\n     def test_tapas_integration_test(self):\n         data = {"
        },
        {
            "sha": "470a126b892fe3c977d0d6ad38a6b9daf4ec3393",
            "filename": "tests/models/udop/test_tokenization_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 49,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -30,7 +30,6 @@\n )\n from transformers.testing_utils import (\n     get_tests_dir,\n-    is_pt_tf_cross_test,\n     require_pandas,\n     require_sentencepiece,\n     require_tokenizers,\n@@ -1374,54 +1373,6 @@ def test_udop_truncation_integration_test(self):\n         self.assertListEqual(new_encoded_inputs, dropped_encoded_inputs)\n         self.assertLessEqual(len(new_encoded_inputs), 20)\n \n-    @is_pt_tf_cross_test\n-    def test_batch_encode_plus_tensors(self):\n-        tokenizers = self.get_tokenizers(do_lower_case=False)\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                words, boxes = self.get_words_and_boxes_batch()\n-\n-                # A Tensor cannot be build by sequences which are not the same size\n-                self.assertRaises(\n-                    ValueError, tokenizer.batch_encode_plus_boxes, words, boxes=boxes, return_tensors=\"pt\"\n-                )\n-                self.assertRaises(\n-                    ValueError, tokenizer.batch_encode_plus_boxes, words, boxes=boxes, return_tensors=\"tf\"\n-                )\n-\n-                if tokenizer.pad_token_id is None:\n-                    self.assertRaises(\n-                        ValueError,\n-                        tokenizer.batch_encode_plus_boxes,\n-                        words,\n-                        boxes=boxes,\n-                        padding=True,\n-                        return_tensors=\"pt\",\n-                    )\n-                    self.assertRaises(\n-                        ValueError,\n-                        tokenizer.batch_encode_plus_boxes,\n-                        words,\n-                        boxes=boxes,\n-                        padding=\"longest\",\n-                        return_tensors=\"tf\",\n-                    )\n-                else:\n-                    pytorch_tensor = tokenizer.batch_encode_plus_boxes(\n-                        words, boxes=boxes, padding=True, return_tensors=\"pt\"\n-                    )\n-                    tensorflow_tensor = tokenizer.batch_encode_plus_boxes(\n-                        words, boxes=boxes, padding=\"longest\", return_tensors=\"tf\"\n-                    )\n-                    encoded_sequences = tokenizer.batch_encode_plus_boxes(words, boxes=boxes, padding=True)\n-\n-                    for key in encoded_sequences.keys():\n-                        pytorch_value = pytorch_tensor[key].tolist()\n-                        tensorflow_value = tensorflow_tensor[key].numpy().tolist()\n-                        encoded_value = encoded_sequences[key]\n-\n-                        self.assertEqual(pytorch_value, tensorflow_value, encoded_value)\n-\n     def test_sequence_ids(self):\n         tokenizers = self.get_tokenizers()\n         for tokenizer in tokenizers:"
        },
        {
            "sha": "337037229a1d93c868633bbbf06d1b115f6226d9",
            "filename": "tests/models/vision_encoder_decoder/test_modeling_tf_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 303,
            "changes": 304,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_tf_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_tf_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_tf_vision_encoder_decoder.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -16,23 +16,18 @@\n \n from __future__ import annotations\n \n-import copy\n import os\n import tempfile\n import unittest\n \n import numpy as np\n \n-from transformers import is_tf_available, is_torch_available, is_vision_available\n+from transformers import is_tf_available, is_vision_available\n from transformers.testing_utils import (\n-    is_pt_tf_cross_test,\n     require_tf,\n-    require_torch,\n     require_vision,\n     slow,\n-    torch_device,\n )\n-from transformers.utils.generic import ModelOutput\n \n from ...test_modeling_tf_common import floats_tensor, ids_tensor\n from ..gpt2.test_modeling_tf_gpt2 import TFGPT2ModelTester\n@@ -55,11 +50,6 @@\n     )\n     from transformers.modeling_tf_outputs import TFBaseModelOutput\n \n-if is_torch_available():\n-    import torch\n-\n-    from transformers import GPT2LMHeadModel, VisionEncoderDecoderModel, ViTModel\n-\n if is_vision_available():\n     from PIL import Image\n \n@@ -318,185 +308,6 @@ def check_encoder_decoder_model_generate(self, pixel_values, config, decoder_con\n             tuple(generated_output.shape.as_list()), (pixel_values.shape[0],) + (decoder_config.max_length,)\n         )\n \n-    def check_pt_tf_outputs(self, tf_outputs, pt_outputs, model_class, tol=1e-5, name=\"outputs\", attributes=None):\n-        \"\"\"Check the outputs from PyTorch and TensorFlow models are close enough. Checks are done in a recursive way.\n-\n-        Args:\n-            model_class: The class of the model that is currently testing. For example, `TFBertModel`,\n-                TFBertForMaskedLM`, `TFBertForSequenceClassification`, etc. Mainly used for providing more informative\n-                error messages.\n-            name (`str`): The name of the output. For example, `output.hidden_states`, `output.attentions`, etc.\n-            attributes (`Tuple[str]`): The names of the output's element if the output is a tuple/list with each element\n-                being a named field in the output.\n-        \"\"\"\n-\n-        self.assertEqual(type(name), str)\n-        if attributes is not None:\n-            self.assertEqual(type(attributes), tuple, f\"{name}: The argument `attributes` should be a `tuple`\")\n-\n-        # Allow `ModelOutput` (e.g. `CLIPOutput` has `text_model_output` and `vision_model_output`).\n-        if isinstance(tf_outputs, ModelOutput):\n-            self.assertTrue(\n-                isinstance(pt_outputs, ModelOutput),\n-                f\"{name}: `pt_outputs` should an instance of `ModelOutput` when `tf_outputs` is\",\n-            )\n-\n-            tf_keys = [k for k, v in tf_outputs.items() if v is not None]\n-            pt_keys = [k for k, v in pt_outputs.items() if v is not None]\n-\n-            self.assertEqual(tf_keys, pt_keys, f\"{name}: Output keys differ between TF and PyTorch\")\n-\n-            # convert to the case of `tuple`\n-            # appending each key to the current (string) `names`\n-            attributes = tuple([f\"{name}.{k}\" for k in tf_keys])\n-            self.check_pt_tf_outputs(\n-                tf_outputs.to_tuple(), pt_outputs.to_tuple(), model_class, tol=tol, name=name, attributes=attributes\n-            )\n-\n-        # Allow `list` (e.g. `TransfoXLModelOutput.mems` is a list of tensors.)\n-        elif type(tf_outputs) in [tuple, list]:\n-            self.assertEqual(type(tf_outputs), type(pt_outputs), f\"{name}: Output types differ between TF and PyTorch\")\n-            self.assertEqual(len(tf_outputs), len(pt_outputs), f\"{name}: Output lengths differ between TF and PyTorch\")\n-\n-            if attributes is not None:\n-                # case 1: each output has assigned name (e.g. a tuple form of a `ModelOutput`)\n-                self.assertEqual(\n-                    len(attributes),\n-                    len(tf_outputs),\n-                    f\"{name}: The tuple `names` should have the same length as `tf_outputs`\",\n-                )\n-            else:\n-                # case 2: each output has no assigned name (e.g. hidden states of each layer) -> add an index to `names`\n-                attributes = tuple([f\"{name}_{idx}\" for idx in range(len(tf_outputs))])\n-\n-            for tf_output, pt_output, attr in zip(tf_outputs, pt_outputs, attributes):\n-                self.check_pt_tf_outputs(tf_output, pt_output, model_class, tol=tol, name=attr)\n-\n-        elif isinstance(tf_outputs, tf.Tensor):\n-            self.assertTrue(\n-                isinstance(pt_outputs, torch.Tensor), f\"{name}: `pt_outputs` should a tensor when `tf_outputs` is\"\n-            )\n-\n-            tf_outputs = tf_outputs.numpy()\n-            pt_outputs = pt_outputs.detach().to(\"cpu\").numpy()\n-\n-            self.assertEqual(\n-                tf_outputs.shape, pt_outputs.shape, f\"{name}: Output shapes differ between TF and PyTorch\"\n-            )\n-\n-            # deal with NumPy's scalars to make replacing nan values by 0 work.\n-            if np.isscalar(tf_outputs):\n-                tf_outputs = np.array([tf_outputs])\n-                pt_outputs = np.array([pt_outputs])\n-\n-            tf_nans = np.isnan(tf_outputs)\n-            pt_nans = np.isnan(pt_outputs)\n-\n-            pt_outputs[tf_nans] = 0\n-            tf_outputs[tf_nans] = 0\n-            pt_outputs[pt_nans] = 0\n-            tf_outputs[pt_nans] = 0\n-\n-            max_diff = np.amax(np.abs(tf_outputs - pt_outputs))\n-            self.assertLessEqual(max_diff, tol, f\"{name}: Difference between torch and tf is {max_diff} (>= {tol}).\")\n-        else:\n-            raise ValueError(\n-                \"`tf_outputs` should be an instance of `tf.Tensor`, a `tuple`, or an instance of `tf.Tensor`. Got\"\n-                f\" {type(tf_outputs)} instead.\"\n-            )\n-\n-    def prepare_pt_inputs_from_tf_inputs(self, tf_inputs_dict):\n-        pt_inputs_dict = {}\n-        for name, key in tf_inputs_dict.items():\n-            if isinstance(key, bool):\n-                pt_inputs_dict[name] = key\n-            elif name == \"input_values\":\n-                pt_inputs_dict[name] = torch.from_numpy(key.numpy()).to(torch.float32)\n-            elif name == \"pixel_values\":\n-                pt_inputs_dict[name] = torch.from_numpy(key.numpy()).to(torch.float32)\n-            elif name == \"input_features\":\n-                pt_inputs_dict[name] = torch.from_numpy(key.numpy()).to(torch.float32)\n-            # other general float inputs\n-            elif tf_inputs_dict[name].dtype.is_floating:\n-                pt_inputs_dict[name] = torch.from_numpy(key.numpy()).to(torch.float32)\n-            else:\n-                pt_inputs_dict[name] = torch.from_numpy(key.numpy()).to(torch.long)\n-\n-        return pt_inputs_dict\n-\n-    def check_pt_tf_models(self, tf_model, pt_model, tf_inputs_dict):\n-        pt_inputs_dict = self.prepare_pt_inputs_from_tf_inputs(tf_inputs_dict)\n-\n-        # send pytorch inputs to the correct device\n-        pt_inputs_dict = {\n-            k: v.to(device=torch_device) if isinstance(v, torch.Tensor) else v for k, v in pt_inputs_dict.items()\n-        }\n-\n-        # send pytorch model to the correct device\n-        pt_model.to(torch_device)\n-\n-        # Check predictions on first output (logits/hidden-states) are close enough given low-level computational differences\n-        pt_model.eval()\n-\n-        with torch.no_grad():\n-            pt_outputs = pt_model(**pt_inputs_dict)\n-        tf_outputs = tf_model(tf_inputs_dict)\n-\n-        # tf models returned loss is usually a tensor rather than a scalar.\n-        # (see `hf_compute_loss`: it uses `keras.losses.Reduction.NONE`)\n-        # Change it here to a scalar to match PyTorch models' loss\n-        tf_loss = getattr(tf_outputs, \"loss\", None)\n-        if tf_loss is not None:\n-            tf_outputs.loss = tf.math.reduce_mean(tf_loss)\n-\n-        self.check_pt_tf_outputs(tf_outputs, pt_outputs, type(tf_model))\n-\n-    def check_pt_tf_equivalence(self, tf_model, pt_model, tf_inputs_dict):\n-        \"\"\"Wrap `check_pt_tf_models` to further check PT -> TF again\"\"\"\n-\n-        self.check_pt_tf_models(tf_model, pt_model, tf_inputs_dict)\n-\n-        # PT -> TF\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            pt_model.save_pretrained(tmpdirname)\n-            tf_model = TFVisionEncoderDecoderModel.from_pretrained(tmpdirname)\n-\n-        self.check_pt_tf_models(tf_model, pt_model, tf_inputs_dict)\n-\n-    def check_pt_to_tf_equivalence(self, config, decoder_config, tf_inputs_dict):\n-        encoder_decoder_config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config, decoder_config)\n-        # Output all for aggressive testing\n-        encoder_decoder_config.output_hidden_states = True\n-        # All models tested in this file have attentions\n-        encoder_decoder_config.output_attentions = True\n-\n-        pt_model = VisionEncoderDecoderModel(encoder_decoder_config)\n-\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            pt_model.save_pretrained(tmpdirname)\n-            tf_model = TFVisionEncoderDecoderModel.from_pretrained(tmpdirname)\n-\n-        self.check_pt_tf_equivalence(tf_model, pt_model, tf_inputs_dict)\n-\n-    def check_tf_to_pt_equivalence(self, config, decoder_config, tf_inputs_dict):\n-        encoder_decoder_config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config, decoder_config)\n-        # Output all for aggressive testing\n-        encoder_decoder_config.output_hidden_states = True\n-        # TODO: A generalizable way to determine this attribute\n-        encoder_decoder_config.output_attentions = True\n-\n-        tf_model = TFVisionEncoderDecoderModel(encoder_decoder_config)\n-        # Make sure model is built before saving\n-        tf_model(**tf_inputs_dict)\n-\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            tf_model.save_pretrained(tmpdirname, safe_serialization=False)\n-            pt_model = VisionEncoderDecoderModel.from_pretrained(\n-                tmpdirname, from_tf=True, attn_implementation=tf_model.config._attn_implementation\n-            )\n-\n-        self.check_pt_tf_equivalence(tf_model, pt_model, tf_inputs_dict)\n-\n     def test_encoder_decoder_model(self):\n         config_inputs_dict = self.prepare_config_and_inputs()\n         self.check_encoder_decoder_model(**config_inputs_dict)\n@@ -533,69 +344,6 @@ def assert_almost_equals(self, a: np.ndarray, b: np.ndarray, tol: float):\n         diff = np.abs((a - b)).max()\n         self.assertLessEqual(diff, tol, f\"Difference between torch and tf is {diff} (>= {tol}).\")\n \n-    @is_pt_tf_cross_test\n-    def test_pt_tf_model_equivalence(self):\n-        config_inputs_dict = self.prepare_config_and_inputs()\n-        labels = config_inputs_dict.pop(\"decoder_token_labels\")\n-\n-        # Keep only common arguments\n-        arg_names = [\n-            \"config\",\n-            \"pixel_values\",\n-            \"decoder_config\",\n-            \"decoder_input_ids\",\n-            \"decoder_attention_mask\",\n-            \"encoder_hidden_states\",\n-        ]\n-        config_inputs_dict = {k: v for k, v in config_inputs_dict.items() if k in arg_names}\n-\n-        config = config_inputs_dict.pop(\"config\")\n-        decoder_config = config_inputs_dict.pop(\"decoder_config\")\n-\n-        # Output all for aggressive testing\n-        config.output_hidden_states = True\n-        decoder_config.output_hidden_states = True\n-        # All models tested in this file have attentions\n-        config.output_attentions = True\n-        decoder_config.output_attentions = True\n-\n-        tf_inputs_dict = config_inputs_dict\n-        # `encoder_hidden_states` is not used in model call/forward\n-        del tf_inputs_dict[\"encoder_hidden_states\"]\n-\n-        # Make sure no sequence has all zeros as attention mask, otherwise some tests fail due to the inconsistency\n-        # of the usage `1e-4`, `1e-9`, `1e-30`, `-inf`.\n-        for k in [\"decoder_attention_mask\"]:\n-            attention_mask = tf_inputs_dict[k]\n-\n-            # Make sure no all 0s attention masks - to avoid failure at this moment.\n-            # Put `1` at the beginning of sequences to make it still work when combining causal attention masks.\n-            # TODO: remove this line once a fix regarding large negative values for attention mask is done.\n-            attention_mask = tf.concat(\n-                [tf.ones_like(attention_mask[:, :1], dtype=attention_mask.dtype), attention_mask[:, 1:]], axis=-1\n-            )\n-            tf_inputs_dict[k] = attention_mask\n-\n-        tf_inputs_dict_with_labels = copy.copy(tf_inputs_dict)\n-        tf_inputs_dict_with_labels[\"labels\"] = labels\n-\n-        self.assertTrue(decoder_config.cross_attention_hidden_size is None)\n-\n-        # Original test: check without `labels` and  without `enc_to_dec_proj` projection\n-        self.assertTrue(config.hidden_size == decoder_config.hidden_size)\n-        self.check_pt_to_tf_equivalence(config, decoder_config, tf_inputs_dict)\n-        self.check_tf_to_pt_equivalence(config, decoder_config, tf_inputs_dict)\n-\n-        # check with `labels`\n-        self.check_pt_to_tf_equivalence(config, decoder_config, tf_inputs_dict_with_labels)\n-        self.check_tf_to_pt_equivalence(config, decoder_config, tf_inputs_dict_with_labels)\n-\n-        # check `enc_to_dec_proj` work as expected\n-        decoder_config.hidden_size = decoder_config.hidden_size * 2\n-        self.assertTrue(config.hidden_size != decoder_config.hidden_size)\n-        self.check_pt_to_tf_equivalence(config, decoder_config, tf_inputs_dict)\n-        self.check_tf_to_pt_equivalence(config, decoder_config, tf_inputs_dict)\n-\n     @slow\n     def test_real_model_save_load_from_pretrained(self):\n         model_2 = self.get_pretrained_model()\n@@ -781,56 +529,6 @@ def test_encoder_decoder_save_load_from_encoder_decoder(self):\n         max_diff = np.max(np.abs(logits_2.numpy() - logits_orig.numpy()))\n         self.assertAlmostEqual(max_diff, 0.0, places=4)\n \n-    @require_torch\n-    @is_pt_tf_cross_test\n-    def test_encoder_decoder_save_load_from_encoder_decoder_from_pt(self):\n-        config = self.get_encoder_decoder_config_small()\n-\n-        # create two random ViT/GPT2 models for vit-gpt2 & initialize weights (+cross_attention weights)\n-        encoder_pt = ViTModel(config.encoder).to(torch_device).eval()\n-        decoder_pt = GPT2LMHeadModel(config.decoder).to(torch_device).eval()\n-\n-        encoder_decoder_pt = VisionEncoderDecoderModel(encoder=encoder_pt, decoder=decoder_pt).to(torch_device).eval()\n-\n-        pixel_values = floats_tensor(\n-            [\n-                13,\n-                encoder_pt.config.num_channels,\n-                encoder_pt.config.image_size,\n-                encoder_pt.config.image_size,\n-            ]\n-        )\n-        decoder_input_ids = ids_tensor([13, 1], decoder_pt.config.vocab_size)\n-\n-        pt_pixel_values = torch.tensor(pixel_values.numpy(), device=torch_device, dtype=torch.float)\n-        pt_decoder_input_ids = torch.tensor(decoder_input_ids.numpy(), device=torch_device, dtype=torch.long)\n-\n-        logits_pt = encoder_decoder_pt(pixel_values=pt_pixel_values, decoder_input_ids=pt_decoder_input_ids).logits\n-\n-        # PyTorch => TensorFlow\n-        with tempfile.TemporaryDirectory() as tmp_dirname_1, tempfile.TemporaryDirectory() as tmp_dirname_2:\n-            encoder_decoder_pt.encoder.save_pretrained(tmp_dirname_1)\n-            encoder_decoder_pt.decoder.save_pretrained(tmp_dirname_2)\n-            encoder_decoder_tf = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n-                tmp_dirname_1, tmp_dirname_2\n-            )\n-\n-        logits_tf = encoder_decoder_tf(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids).logits\n-\n-        max_diff = np.max(np.abs(logits_pt.detach().cpu().numpy() - logits_tf.numpy()))\n-        self.assertAlmostEqual(max_diff, 0.0, places=3)\n-\n-        # Make sure `from_pretrained` following `save_pretrained` work and give the same result\n-        # (See https://github.com/huggingface/transformers/pull/14016)\n-        with tempfile.TemporaryDirectory() as tmp_dirname:\n-            encoder_decoder_tf.save_pretrained(tmp_dirname, safe_serialization=False)\n-            encoder_decoder_tf = TFVisionEncoderDecoderModel.from_pretrained(tmp_dirname)\n-\n-            logits_tf_2 = encoder_decoder_tf(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids).logits\n-\n-            max_diff = np.max(np.abs(logits_tf_2.numpy() - logits_tf.numpy()))\n-            self.assertAlmostEqual(max_diff, 0.0, places=3)\n-\n     @require_vision\n     @slow\n     def test_encoder_decoder_from_pretrained(self):"
        },
        {
            "sha": "dcdf6d71e9f91fd1d44d5673545df38ad870fc8f",
            "filename": "tests/models/vit_mae/test_modeling_tf_vit_mae.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_tf_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_tf_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_tf_vit_mae.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -270,22 +270,6 @@ def prepare_numpy_arrays(inputs_dict):\n             output_for_kw_input = model(**inputs_np, noise=noise)\n             self.assert_outputs_same(output_for_dict_input, output_for_kw_input)\n \n-    # overwrite from common since TFViTMAEForPretraining has random masking, we need to fix the noise\n-    # to generate masks during test\n-    def check_pt_tf_models(self, tf_model, pt_model, tf_inputs_dict):\n-        # make masks reproducible\n-        np.random.seed(2)\n-\n-        num_patches = int((tf_model.config.image_size // tf_model.config.patch_size) ** 2)\n-        noise = np.random.uniform(size=(self.model_tester.batch_size, num_patches))\n-        tf_noise = tf.constant(noise)\n-\n-        # Add `noise` argument.\n-        # PT inputs will be prepared in `super().check_pt_tf_models()` with this added `noise` argument\n-        tf_inputs_dict[\"noise\"] = tf_noise\n-\n-        super().check_pt_tf_models(tf_model, pt_model, tf_inputs_dict)\n-\n     # overwrite from common since TFViTMAEForPretraining has random masking, we need to fix the noise\n     # to generate masks during test\n     def test_keras_save_load(self):"
        },
        {
            "sha": "d2f0424889e84d3ecfad9ebc5572181af190b483",
            "filename": "tests/models/vit_mae/test_modeling_vit_mae.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -204,22 +204,6 @@ def test_for_pretraining(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_pretraining(*config_and_inputs)\n \n-    # overwrite from common since ViTMAEForPretraining has random masking, we need to fix the noise\n-    # to generate masks during test\n-    def check_pt_tf_models(self, tf_model, pt_model, pt_inputs_dict):\n-        # make masks reproducible\n-        np.random.seed(2)\n-\n-        num_patches = int((pt_model.config.image_size // pt_model.config.patch_size) ** 2)\n-        noise = np.random.uniform(size=(self.model_tester.batch_size, num_patches))\n-        pt_noise = torch.from_numpy(noise)\n-\n-        # Add `noise` argument.\n-        # PT inputs will be prepared in `super().check_pt_tf_models()` with this added `noise` argument\n-        pt_inputs_dict[\"noise\"] = pt_noise\n-\n-        super().check_pt_tf_models(tf_model, pt_model, pt_inputs_dict)\n-\n     def test_save_load(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "7508e4fc01fbfdc0774a5cda10cbe185413b6c49",
            "filename": "tests/models/wav2vec2/test_modeling_tf_wav2vec2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 115,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_tf_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_tf_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_tf_wav2vec2.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -22,8 +22,6 @@\n import inspect\n import math\n import multiprocessing\n-import os\n-import tempfile\n import traceback\n import unittest\n \n@@ -36,7 +34,6 @@\n from transformers.testing_utils import (\n     CaptureLogger,\n     is_flaky,\n-    is_pt_tf_cross_test,\n     require_librosa,\n     require_pyctcdecode,\n     require_tf,\n@@ -438,62 +435,6 @@ def test_keras_fit(self):\n         # TODO: (Amy) - check whether skipping CTC model resolves this issue and possible resolutions for CTC\n         pass\n \n-    @is_pt_tf_cross_test\n-    def test_pt_tf_model_equivalence(self, allow_missing_keys=False):\n-        # We override the base test here to skip loss calculation for Wav2Vec2 models because the loss is massive with\n-        # the default labels and frequently overflows to inf or exceeds numerical tolerances between TF/PT\n-        import torch\n-\n-        import transformers\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            # Output all for aggressive testing\n-            config.output_hidden_states = True\n-            config.output_attentions = self.has_attentions\n-\n-            # Make sure no sequence has all zeros as attention mask, otherwise some tests fail due to the inconsistency\n-            # of the usage `1e-4`, `1e-9`, `1e-30`, `-inf`.\n-            # TODO: Use a uniform value for all models, make sure all tests pass without this processing, and remove it.\n-            self._make_attention_mask_non_null(inputs_dict)\n-\n-            pt_model_class_name = model_class.__name__[2:]  # Skip the \"TF\" at the beginning\n-            pt_model_class = getattr(transformers, pt_model_class_name)\n-\n-            tf_model = model_class(config)\n-            pt_model = pt_model_class(config)\n-\n-            tf_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-\n-            # Check we can load pt model in tf and vice-versa with model => model functions\n-            tf_model = transformers.load_pytorch_model_in_tf2_model(\n-                tf_model, pt_model, tf_inputs=tf_inputs_dict, allow_missing_keys=allow_missing_keys\n-            )\n-            pt_model = transformers.load_tf2_model_in_pytorch_model(\n-                pt_model, tf_model, allow_missing_keys=allow_missing_keys\n-            )\n-\n-            # Original test: check without `labels`\n-            self.check_pt_tf_models(tf_model, pt_model, tf_inputs_dict)\n-\n-            # Check we can load pt model in tf and vice-versa with checkpoint => model functions\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_checkpoint_path = os.path.join(tmpdirname, \"pt_model.bin\")\n-                torch.save(pt_model.state_dict(), pt_checkpoint_path)\n-                tf_model = transformers.load_pytorch_checkpoint_in_tf2_model(\n-                    tf_model, pt_checkpoint_path, allow_missing_keys=allow_missing_keys\n-                )\n-\n-                tf_checkpoint_path = os.path.join(tmpdirname, \"tf_model.h5\")\n-                tf_model.save_weights(tf_checkpoint_path)\n-                pt_model = transformers.load_tf2_checkpoint_in_pytorch_model(\n-                    pt_model, tf_checkpoint_path, allow_missing_keys=allow_missing_keys\n-                )\n-\n-            # Original test: check without `labels`\n-            self.check_pt_tf_models(tf_model, pt_model, tf_inputs_dict)\n-\n \n @require_tf\n class TFWav2Vec2RobustModelTest(TFModelTesterMixin, unittest.TestCase):\n@@ -623,62 +564,6 @@ def test_keras_fit(self):\n         # TODO: (Amy) - check whether skipping CTC model resolves this issue and possible resolutions for CTC\n         pass\n \n-    @is_pt_tf_cross_test\n-    def test_pt_tf_model_equivalence(self, allow_missing_keys=False):\n-        # We override the base test here to skip loss calculation for Wav2Vec2 models because the loss is massive with\n-        # the default labels and frequently overflows to inf or exceeds numerical tolerances between TF/PT\n-        import torch\n-\n-        import transformers\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            # Output all for aggressive testing\n-            config.output_hidden_states = True\n-            config.output_attentions = self.has_attentions\n-\n-            # Make sure no sequence has all zeros as attention mask, otherwise some tests fail due to the inconsistency\n-            # of the usage `1e-4`, `1e-9`, `1e-30`, `-inf`.\n-            # TODO: Use a uniform value for all models, make sure all tests pass without this processing, and remove it.\n-            self._make_attention_mask_non_null(inputs_dict)\n-\n-            pt_model_class_name = model_class.__name__[2:]  # Skip the \"TF\" at the beginning\n-            pt_model_class = getattr(transformers, pt_model_class_name)\n-\n-            tf_model = model_class(config)\n-            pt_model = pt_model_class(config)\n-\n-            tf_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-\n-            # Check we can load pt model in tf and vice-versa with model => model functions\n-            tf_model = transformers.load_pytorch_model_in_tf2_model(\n-                tf_model, pt_model, tf_inputs=tf_inputs_dict, allow_missing_keys=allow_missing_keys\n-            )\n-            pt_model = transformers.load_tf2_model_in_pytorch_model(\n-                pt_model, tf_model, allow_missing_keys=allow_missing_keys\n-            )\n-\n-            # Original test: check without `labels`\n-            self.check_pt_tf_models(tf_model, pt_model, tf_inputs_dict)\n-\n-            # Check we can load pt model in tf and vice-versa with checkpoint => model functions\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_checkpoint_path = os.path.join(tmpdirname, \"pt_model.bin\")\n-                torch.save(pt_model.state_dict(), pt_checkpoint_path)\n-                tf_model = transformers.load_pytorch_checkpoint_in_tf2_model(\n-                    tf_model, pt_checkpoint_path, allow_missing_keys=allow_missing_keys\n-                )\n-\n-                tf_checkpoint_path = os.path.join(tmpdirname, \"tf_model.h5\")\n-                tf_model.save_weights(tf_checkpoint_path)\n-                pt_model = transformers.load_tf2_checkpoint_in_pytorch_model(\n-                    pt_model, tf_checkpoint_path, allow_missing_keys=allow_missing_keys\n-                )\n-\n-            # Original test: check without `labels`\n-            self.check_pt_tf_models(tf_model, pt_model, tf_inputs_dict)\n-\n \n @require_tf\n class TFWav2Vec2UtilsTest(unittest.TestCase):"
        },
        {
            "sha": "20109ef2aa586170c7c3c3234ecddc666465c351",
            "filename": "tests/models/whisper/test_modeling_tf_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -433,10 +433,6 @@ def check_hidden_states_output(inputs_dict, config, model_class):\n \n             check_hidden_states_output(inputs_dict, config, model_class)\n \n-    def check_pt_tf_outputs(self, tf_outputs, pt_outputs, model_class, tol=5e-5, name=\"outputs\", attributes=None):\n-        # We override with a slightly higher tol value, as test recently became flaky\n-        super().check_pt_tf_outputs(tf_outputs, pt_outputs, model_class, tol, name, attributes)\n-\n     def test_attention_outputs(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.return_dict = True"
        },
        {
            "sha": "c5c1ba971d3e1f36c0f6614010cd65fe563f85fc",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -1069,10 +1069,6 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n \n             self.assertTrue(models_equal)\n \n-    def check_pt_tf_outputs(self, tf_outputs, pt_outputs, model_class, tol=5e-5, name=\"outputs\", attributes=None):\n-        # We override with a slightly higher tol value, as test recently became flaky\n-        super().check_pt_tf_outputs(tf_outputs, pt_outputs, model_class, tol, name, attributes)\n-\n     def check_pt_flax_outputs(self, fx_outputs, pt_outputs, model_class, tol=5e-5, name=\"outputs\", attributes=None):\n         # We override with a slightly higher tol value, as test recently became flaky\n         super().check_pt_flax_outputs(fx_outputs, pt_outputs, model_class, tol, name, attributes)"
        },
        {
            "sha": "f1f3eaeebda55b839743476907a7052bb88ff0fa",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 256,
            "changes": 257,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -76,7 +76,6 @@\n     CaptureLogger,\n     is_flaky,\n     is_pt_flax_cross_test,\n-    is_pt_tf_cross_test,\n     require_accelerate,\n     require_bitsandbytes,\n     require_deepspeed,\n@@ -129,7 +128,7 @@\n \n \n if is_tf_available():\n-    import tensorflow as tf\n+    pass\n \n if is_flax_available():\n     import jax.numpy as jnp\n@@ -2549,236 +2548,6 @@ def _postprocessing_to_ignore_test_cases(self, tf_outputs, pt_outputs, model_cla\n \n         return new_tf_outputs, new_pt_outputs\n \n-    # Copied from tests.test_modeling_tf_common.TFModelTesterMixin.check_pt_tf_outputs\n-    def check_pt_tf_outputs(self, tf_outputs, pt_outputs, model_class, tol=1e-4, name=\"outputs\", attributes=None):\n-        \"\"\"Check the outputs from PyTorch and TensorFlow models are close enough. Checks are done in a recursive way.\n-\n-        Args:\n-            model_class: The class of the model that is currently testing. For example, `TFBertModel`,\n-                TFBertForMaskedLM`, `TFBertForSequenceClassification`, etc. Mainly used for providing more informative\n-                error messages.\n-            name (`str`): The name of the output. For example, `output.hidden_states`, `output.attentions`, etc.\n-            attributes (`Tuple[str]`): The names of the output's element if the output is a tuple/list with each element\n-                being a named field in the output.\n-        \"\"\"\n-\n-        self.assertEqual(type(name), str)\n-        if attributes is not None:\n-            self.assertEqual(type(attributes), tuple, f\"{name}: The argument `attributes` should be a `tuple`\")\n-\n-        # Allow `ModelOutput` (e.g. `CLIPOutput` has `text_model_output` and `vision_model_output`).\n-        if isinstance(tf_outputs, ModelOutput):\n-            self.assertTrue(\n-                isinstance(pt_outputs, ModelOutput),\n-                f\"{name}: `pt_outputs` should an instance of `ModelOutput` when `tf_outputs` is\",\n-            )\n-\n-            # Don't copy this block to model specific test file!\n-            # TODO: remove this method and this line after issues are fixed\n-            tf_outputs, pt_outputs = self._postprocessing_to_ignore_test_cases(tf_outputs, pt_outputs, model_class)\n-\n-            tf_keys = [k for k, v in tf_outputs.items() if v is not None]\n-            pt_keys = [k for k, v in pt_outputs.items() if v is not None]\n-\n-            self.assertEqual(tf_keys, pt_keys, f\"{name}: Output keys differ between TF and PyTorch\")\n-\n-            # convert to the case of `tuple`\n-            # appending each key to the current (string) `name`\n-            attributes = tuple([f\"{name}.{k}\" for k in tf_keys])\n-            self.check_pt_tf_outputs(\n-                tf_outputs.to_tuple(), pt_outputs.to_tuple(), model_class, tol=tol, name=name, attributes=attributes\n-            )\n-\n-        # Allow `list` (e.g. `TransfoXLModelOutput.mems` is a list of tensors.)\n-        elif type(tf_outputs) in [tuple, list]:\n-            self.assertEqual(type(tf_outputs), type(pt_outputs), f\"{name}: Output types differ between TF and PyTorch\")\n-            self.assertEqual(len(tf_outputs), len(pt_outputs), f\"{name}: Output lengths differ between TF and PyTorch\")\n-\n-            if attributes is not None:\n-                # case 1: each output has assigned name (e.g. a tuple form of a `ModelOutput`)\n-                self.assertEqual(\n-                    len(attributes),\n-                    len(tf_outputs),\n-                    f\"{name}: The tuple `attributes` should have the same length as `tf_outputs`\",\n-                )\n-            else:\n-                # case 2: each output has no assigned name (e.g. hidden states of each layer) -> add an index to `name`\n-                attributes = tuple([f\"{name}_{idx}\" for idx in range(len(tf_outputs))])\n-\n-            for tf_output, pt_output, attr in zip(tf_outputs, pt_outputs, attributes):\n-                if isinstance(pt_output, DynamicCache):\n-                    pt_output = pt_output.to_legacy_cache()\n-                self.check_pt_tf_outputs(tf_output, pt_output, model_class, tol=tol, name=attr)\n-\n-        elif isinstance(tf_outputs, tf.Tensor):\n-            self.assertTrue(\n-                isinstance(pt_outputs, torch.Tensor), f\"{name}: `pt_outputs` should a tensor when `tf_outputs` is\"\n-            )\n-\n-            tf_outputs = tf_outputs.numpy()\n-            pt_outputs = pt_outputs.detach().to(\"cpu\").numpy()\n-\n-            self.assertEqual(\n-                tf_outputs.shape, pt_outputs.shape, f\"{name}: Output shapes differ between TF and PyTorch\"\n-            )\n-\n-            # deal with NumPy's scalars to make replacing nan values by 0 work.\n-            if np.isscalar(tf_outputs):\n-                tf_outputs = np.array([tf_outputs])\n-                pt_outputs = np.array([pt_outputs])\n-\n-            tf_nans = np.isnan(tf_outputs)\n-            pt_nans = np.isnan(pt_outputs)\n-\n-            pt_outputs[tf_nans] = 0\n-            tf_outputs[tf_nans] = 0\n-            pt_outputs[pt_nans] = 0\n-            tf_outputs[pt_nans] = 0\n-\n-            max_diff = np.amax(np.abs(tf_outputs - pt_outputs))\n-            self.assertLessEqual(\n-                max_diff,\n-                tol,\n-                f\"{name}: Difference between PyTorch and TF is {max_diff} (>= {tol}) for {model_class.__name__}\",\n-            )\n-        else:\n-            raise ValueError(\n-                \"`tf_outputs` should be an instance of `ModelOutput`, a `tuple`, or an instance of `tf.Tensor`. Got\"\n-                f\" {type(tf_outputs)} instead.\"\n-            )\n-\n-    def prepare_tf_inputs_from_pt_inputs(self, pt_inputs_dict):\n-        tf_inputs_dict = {}\n-        for key, tensor in pt_inputs_dict.items():\n-            # skip key that does not exist in tf\n-            if isinstance(tensor, bool):\n-                tf_inputs_dict[key] = tensor\n-            elif key == \"input_values\":\n-                tf_inputs_dict[key] = tf.convert_to_tensor(tensor.cpu().numpy(), dtype=tf.float32)\n-            elif key == \"pixel_values\":\n-                tf_inputs_dict[key] = tf.convert_to_tensor(tensor.cpu().numpy(), dtype=tf.float32)\n-            elif key == \"input_features\":\n-                tf_inputs_dict[key] = tf.convert_to_tensor(tensor.cpu().numpy(), dtype=tf.float32)\n-            # other general float inputs\n-            elif tensor.is_floating_point():\n-                tf_inputs_dict[key] = tf.convert_to_tensor(tensor.cpu().numpy(), dtype=tf.float32)\n-            else:\n-                tf_inputs_dict[key] = tf.convert_to_tensor(tensor.cpu().numpy(), dtype=tf.int32)\n-\n-        return tf_inputs_dict\n-\n-    def check_pt_tf_models(self, tf_model, pt_model, pt_inputs_dict):\n-        tf_inputs_dict = self.prepare_tf_inputs_from_pt_inputs(pt_inputs_dict)\n-\n-        # send pytorch inputs to the correct device\n-        pt_inputs_dict = {\n-            k: v.to(device=torch_device) if isinstance(v, torch.Tensor) else v for k, v in pt_inputs_dict.items()\n-        }\n-\n-        # send pytorch model to the correct device\n-        pt_model.to(torch_device)\n-\n-        # Check predictions on first output (logits/hidden-states) are close enough given low-level computational differences\n-        pt_model.eval()\n-\n-        with torch.no_grad():\n-            pt_outputs = pt_model(**pt_inputs_dict)\n-        tf_outputs = tf_model(tf_inputs_dict)\n-\n-        # tf models returned loss is usually a tensor rather than a scalar.\n-        # (see `hf_compute_loss`: it uses `tf.keras.losses.Reduction.NONE`)\n-        # Change it here to a scalar to match PyTorch models' loss\n-        tf_loss = getattr(tf_outputs, \"loss\", None)\n-        if tf_loss is not None:\n-            tf_outputs.loss = tf.math.reduce_mean(tf_loss)\n-\n-        self.check_pt_tf_outputs(tf_outputs, pt_outputs, type(pt_model))\n-\n-    @is_pt_tf_cross_test\n-    def test_pt_tf_model_equivalence(self, allow_missing_keys=False):\n-        import transformers\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            tf_model_class_name = \"TF\" + model_class.__name__  # Add the \"TF\" at the beginning\n-            if not hasattr(transformers, tf_model_class_name):\n-                self.skipTest(reason=\"transformers does not have TF version of this model yet\")\n-\n-            # Output all for aggressive testing\n-            config.output_hidden_states = True\n-            config.output_attentions = self.has_attentions\n-\n-            # Make sure no sequence has all zeros as attention mask, otherwise some tests fail due to the inconsistency\n-            # of the usage `1e-4`, `1e-9`, `1e-30`, `-inf`.\n-            # TODO: Use a uniform value for all models, make sure all tests pass without this processing, and remove it.\n-            self._make_attention_mask_non_null(inputs_dict)\n-\n-            tf_model_class = getattr(transformers, tf_model_class_name)\n-\n-            pt_model = model_class(config).eval()\n-            tf_model = tf_model_class(config)\n-\n-            pt_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-            pt_inputs_dict_with_labels = self._prepare_for_class(\n-                inputs_dict,\n-                model_class,\n-                # Not all models accept \"labels\" in the forward pass (yet :) )\n-                return_labels=True if \"labels\" in inspect.signature(model_class.forward).parameters.keys() else False,\n-            )\n-\n-            # make sure only tf inputs are forward that actually exist in function args\n-            tf_input_keys = set(inspect.signature(tf_model.call).parameters.keys())\n-\n-            # remove all head masks\n-            tf_input_keys.discard(\"head_mask\")\n-            tf_input_keys.discard(\"cross_attn_head_mask\")\n-            tf_input_keys.discard(\"decoder_head_mask\")\n-\n-            pt_inputs_dict = {k: v for k, v in pt_inputs_dict.items() if k in tf_input_keys}\n-            pt_inputs_dict_with_labels = {k: v for k, v in pt_inputs_dict_with_labels.items() if k in tf_input_keys}\n-\n-            # For some models (e.g. base models), there is no label returned.\n-            # Set the input dict to `None` to avoid check outputs twice for the same input dicts.\n-            if not set(pt_inputs_dict_with_labels.keys()).symmetric_difference(pt_inputs_dict.keys()):\n-                pt_inputs_dict_with_labels = None\n-\n-            # Check we can load pt model in tf and vice-versa with model => model functions\n-            # Here requires `tf_inputs_dict` to build `tf_model`\n-            tf_inputs_dict = self.prepare_tf_inputs_from_pt_inputs(pt_inputs_dict)\n-            tf_model = transformers.load_pytorch_model_in_tf2_model(\n-                tf_model, pt_model, tf_inputs=tf_inputs_dict, allow_missing_keys=allow_missing_keys\n-            )\n-            pt_model = transformers.load_tf2_model_in_pytorch_model(\n-                pt_model, tf_model, allow_missing_keys=allow_missing_keys\n-            )\n-\n-            # Original test: check without `labels`\n-            self.check_pt_tf_models(tf_model, pt_model, pt_inputs_dict)\n-            # check with `labels`\n-            if pt_inputs_dict_with_labels:\n-                self.check_pt_tf_models(tf_model, pt_model, pt_inputs_dict_with_labels)\n-\n-            # Check we can load pt model in tf and vice-versa with checkpoint => model functions\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_checkpoint_path = os.path.join(tmpdirname, \"pt_model.bin\")\n-                torch.save(pt_model.state_dict(), pt_checkpoint_path)\n-                tf_model = transformers.load_pytorch_checkpoint_in_tf2_model(\n-                    tf_model, pt_checkpoint_path, allow_missing_keys=allow_missing_keys\n-                )\n-\n-                tf_checkpoint_path = os.path.join(tmpdirname, \"tf_model.h5\")\n-                tf_model.save_weights(tf_checkpoint_path)\n-                pt_model = transformers.load_tf2_checkpoint_in_pytorch_model(\n-                    pt_model, tf_checkpoint_path, allow_missing_keys=allow_missing_keys\n-                )\n-\n-            # Original test: check without `labels`\n-            self.check_pt_tf_models(tf_model, pt_model, pt_inputs_dict)\n-            # check with `labels`\n-            if pt_inputs_dict_with_labels:\n-                self.check_pt_tf_models(tf_model, pt_model, pt_inputs_dict_with_labels)\n-\n     def assert_almost_equals(self, a: np.ndarray, b: np.ndarray, tol: float):\n         diff = np.abs((a - b)).max()\n         self.assertLessEqual(diff, tol, f\"Difference between torch and flax is {diff} (>= {tol}).\")\n@@ -4644,30 +4413,6 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n                 tol = torch.finfo(torch.float16).eps\n                 torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n \n-    @is_pt_tf_cross_test\n-    def test_tf_from_pt_safetensors(self):\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            tf_model_class_name = \"TF\" + model_class.__name__  # Add the \"TF\" at the beginning\n-            if not hasattr(transformers, tf_model_class_name):\n-                self.skipTest(reason=\"transformers does not have this model in TF version yet\")\n-\n-            tf_model_class = getattr(transformers, tf_model_class_name)\n-\n-            pt_model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_model.save_pretrained(tmpdirname, safe_serialization=True)\n-                tf_model_1 = tf_model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                pt_model.save_pretrained(tmpdirname, safe_serialization=False)\n-                tf_model_2 = tf_model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                # Check models are equal\n-                for p1, p2 in zip(tf_model_1.weights, tf_model_2.weights):\n-                    self.assertTrue(np.allclose(p1.numpy(), p2.numpy()))\n-\n     @is_pt_flax_cross_test\n     def test_flax_from_pt_safetensors(self):\n         for model_class in self.all_model_classes:"
        },
        {
            "sha": "1981859048bdca663ebab9018f5f456f83e5afc6",
            "filename": "tests/test_modeling_tf_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 214,
            "changes": 215,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Ftest_modeling_tf_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Ftest_modeling_tf_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_tf_common.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -29,12 +29,11 @@\n \n from datasets import Dataset\n \n-from transformers import is_tf_available, is_torch_available\n+from transformers import is_tf_available\n from transformers.models.auto import get_values\n from transformers.testing_utils import (  # noqa: F401\n     CaptureLogger,\n     _tf_gpu_memory_limit,\n-    is_pt_tf_cross_test,\n     require_tf,\n     require_tf2onnx,\n     slow,\n@@ -88,9 +87,6 @@\n                 # Virtual devices must be set before GPUs have been initialized\n                 print(e)\n \n-if is_torch_available():\n-    import torch\n-\n \n def _config_zero_init(config):\n     configs_no_init = copy.deepcopy(config)\n@@ -474,215 +470,6 @@ def _postprocessing_to_ignore_test_cases(self, tf_outputs, pt_outputs, model_cla\n \n         return new_tf_outputs, new_pt_outputs\n \n-    def check_pt_tf_outputs(self, tf_outputs, pt_outputs, model_class, tol=1e-4, name=\"outputs\", attributes=None):\n-        \"\"\"Check the outputs from PyTorch and TensorFlow models are close enough. Checks are done in a recursive way.\n-\n-        Args:\n-            model_class: The class of the model that is currently testing. For example, `TFBertModel`,\n-                TFBertForMaskedLM`, `TFBertForSequenceClassification`, etc. Mainly used for providing more informative\n-                error messages.\n-            name (`str`): The name of the output. For example, `output.hidden_states`, `output.attentions`, etc.\n-            attributes (`Tuple[str]`): The names of the output's element if the output is a tuple/list with each element\n-                being a named field in the output.\n-        \"\"\"\n-        from transformers.cache_utils import DynamicCache\n-\n-        self.assertEqual(type(name), str)\n-        if attributes is not None:\n-            self.assertEqual(type(attributes), tuple, f\"{name}: The argument `attributes` should be a `tuple`\")\n-\n-        # Allow `ModelOutput` (e.g. `CLIPOutput` has `text_model_output` and `vision_model_output`).\n-        if isinstance(tf_outputs, ModelOutput):\n-            self.assertTrue(\n-                isinstance(pt_outputs, ModelOutput),\n-                f\"{name}: `pt_outputs` should an instance of `ModelOutput` when `tf_outputs` is\",\n-            )\n-\n-            # Don't copy this block to model specific test file!\n-            # TODO: remove this method and this line after issues are fixed\n-            tf_outputs, pt_outputs = self._postprocessing_to_ignore_test_cases(tf_outputs, pt_outputs, model_class)\n-\n-            tf_keys = [k for k, v in tf_outputs.items() if v is not None]\n-            pt_keys = [k for k, v in pt_outputs.items() if v is not None]\n-\n-            self.assertEqual(tf_keys, pt_keys, f\"{name}: Output keys differ between TF and PyTorch\")\n-\n-            # convert to the case of `tuple`\n-            # appending each key to the current (string) `names`\n-            attributes = tuple([f\"{name}.{k}\" for k in tf_keys])\n-            self.check_pt_tf_outputs(\n-                tf_outputs.to_tuple(), pt_outputs.to_tuple(), model_class, tol=tol, name=name, attributes=attributes\n-            )\n-\n-        # Allow `list` (e.g. `TransfoXLModelOutput.mems` is a list of tensors.)\n-        elif type(tf_outputs) in [tuple, list]:\n-            self.assertEqual(type(tf_outputs), type(pt_outputs), f\"{name}: Output types differ between TF and PyTorch\")\n-            self.assertEqual(len(tf_outputs), len(pt_outputs), f\"{name}: Output lengths differ between TF and PyTorch\")\n-\n-            if attributes is not None:\n-                # case 1: each output has assigned name (e.g. a tuple form of a `ModelOutput`)\n-                self.assertEqual(\n-                    len(attributes),\n-                    len(tf_outputs),\n-                    f\"{name}: The tuple `names` should have the same length as `tf_outputs`\",\n-                )\n-            else:\n-                # case 2: each output has no assigned name (e.g. hidden states of each layer) -> add an index to `names`\n-                attributes = tuple([f\"{name}_{idx}\" for idx in range(len(tf_outputs))])\n-\n-            for tf_output, pt_output, attr in zip(tf_outputs, pt_outputs, attributes):\n-                if isinstance(pt_output, DynamicCache):\n-                    pt_output = pt_output.to_legacy_cache()\n-                self.check_pt_tf_outputs(tf_output, pt_output, model_class, tol=tol, name=attr)\n-\n-        elif isinstance(tf_outputs, tf.Tensor):\n-            self.assertTrue(\n-                isinstance(pt_outputs, torch.Tensor), f\"{name}: `pt_outputs` should a tensor when `tf_outputs` is\"\n-            )\n-\n-            tf_outputs = tf_outputs.numpy()\n-            pt_outputs = pt_outputs.detach().to(\"cpu\").numpy()\n-\n-            self.assertEqual(\n-                tf_outputs.shape, pt_outputs.shape, f\"{name}: Output shapes differ between TF and PyTorch\"\n-            )\n-\n-            # deal with NumPy's scalars to make replacing nan values by 0 work.\n-            if np.isscalar(tf_outputs):\n-                tf_outputs = np.array([tf_outputs])\n-                pt_outputs = np.array([pt_outputs])\n-\n-            tf_nans = np.isnan(tf_outputs)\n-            pt_nans = np.isnan(pt_outputs)\n-\n-            pt_outputs[tf_nans] = 0\n-            tf_outputs[tf_nans] = 0\n-            pt_outputs[pt_nans] = 0\n-            tf_outputs[pt_nans] = 0\n-\n-            max_diff = np.amax(np.abs(tf_outputs - pt_outputs))\n-            self.assertLessEqual(max_diff, tol, f\"{name}: Difference between torch and tf is {max_diff} (>= {tol}).\")\n-        else:\n-            raise ValueError(\n-                \"`tf_outputs` should be an instance of `tf.Tensor`, a `tuple`, or an instance of `tf.Tensor`. Got\"\n-                f\" {type(tf_outputs)} instead.\"\n-            )\n-\n-    def prepare_pt_inputs_from_tf_inputs(self, tf_inputs_dict):\n-        pt_inputs_dict = {}\n-        for name, key in tf_inputs_dict.items():\n-            if isinstance(key, bool):\n-                pt_inputs_dict[name] = key\n-            elif name == \"input_values\":\n-                pt_inputs_dict[name] = torch.from_numpy(key.numpy()).to(torch.float32)\n-            elif name == \"pixel_values\":\n-                pt_inputs_dict[name] = torch.from_numpy(key.numpy()).to(torch.float32)\n-            elif name == \"input_features\":\n-                pt_inputs_dict[name] = torch.from_numpy(key.numpy()).to(torch.float32)\n-            # other general float inputs\n-            elif tf_inputs_dict[name].dtype.is_floating:\n-                pt_inputs_dict[name] = torch.from_numpy(key.numpy()).to(torch.float32)\n-            else:\n-                pt_inputs_dict[name] = torch.from_numpy(key.numpy()).to(torch.long)\n-\n-        return pt_inputs_dict\n-\n-    def check_pt_tf_models(self, tf_model, pt_model, tf_inputs_dict):\n-        pt_inputs_dict = self.prepare_pt_inputs_from_tf_inputs(tf_inputs_dict)\n-\n-        # send pytorch inputs to the correct device\n-        pt_inputs_dict = {\n-            k: v.to(device=torch_device) if isinstance(v, torch.Tensor) else v for k, v in pt_inputs_dict.items()\n-        }\n-\n-        # send pytorch model to the correct device\n-        pt_model.to(torch_device)\n-\n-        # Check predictions on first output (logits/hidden-states) are close enough given low-level computational differences\n-        pt_model.eval()\n-\n-        with torch.no_grad():\n-            pt_outputs = pt_model(**pt_inputs_dict)\n-        tf_outputs = tf_model(tf_inputs_dict)\n-\n-        # tf models returned loss is usually a tensor rather than a scalar.\n-        # (see `hf_compute_loss`: it uses `keras.losses.Reduction.NONE`)\n-        # Change it here to a scalar to match PyTorch models' loss\n-        tf_loss = getattr(tf_outputs, \"loss\", None)\n-        if tf_loss is not None:\n-            tf_outputs.loss = tf.math.reduce_mean(tf_loss)\n-\n-        self.check_pt_tf_outputs(tf_outputs, pt_outputs, type(tf_model))\n-\n-    @is_pt_tf_cross_test\n-    def test_pt_tf_model_equivalence(self, allow_missing_keys=False):\n-        import transformers\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            # Output all for aggressive testing\n-            config.output_hidden_states = True\n-            config.output_attentions = self.has_attentions\n-\n-            # Make sure no sequence has all zeros as attention mask, otherwise some tests fail due to the inconsistency\n-            # of the usage `1e-4`, `1e-9`, `1e-30`, `-inf`.\n-            # TODO: Use a uniform value for all models, make sure all tests pass without this processing, and remove it.\n-            self._make_attention_mask_non_null(inputs_dict)\n-\n-            pt_model_class_name = model_class.__name__[2:]  # Skip the \"TF\" at the beginning\n-            pt_model_class = getattr(transformers, pt_model_class_name)\n-\n-            tf_model = model_class(config)\n-            pt_model = pt_model_class(config)\n-\n-            tf_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-            tf_inputs_dict_with_labels = self._prepare_for_class(\n-                inputs_dict,\n-                model_class,\n-                # Not all models accept \"labels\" in the forward pass (yet :) )\n-                return_labels=True if \"labels\" in inspect.signature(model_class.call).parameters.keys() else False,\n-            )\n-\n-            # For some models (e.g. base models), there is no label returned.\n-            # Set the input dict to `None` to avoid check outputs twice for the same input dicts.\n-            if not set(tf_inputs_dict_with_labels.keys()).symmetric_difference(tf_inputs_dict.keys()):\n-                tf_inputs_dict_with_labels = None\n-\n-            # Check we can load pt model in tf and vice-versa with model => model functions\n-            tf_model = transformers.load_pytorch_model_in_tf2_model(\n-                tf_model, pt_model, tf_inputs=tf_inputs_dict, allow_missing_keys=allow_missing_keys\n-            )\n-            pt_model = transformers.load_tf2_model_in_pytorch_model(\n-                pt_model, tf_model, allow_missing_keys=allow_missing_keys\n-            )\n-\n-            # Original test: check without `labels`\n-            self.check_pt_tf_models(tf_model, pt_model, tf_inputs_dict)\n-            # check with `labels`\n-            if tf_inputs_dict_with_labels:\n-                self.check_pt_tf_models(tf_model, pt_model, tf_inputs_dict_with_labels)\n-\n-            # Check we can load pt model in tf and vice-versa with checkpoint => model functions\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_checkpoint_path = os.path.join(tmpdirname, \"pt_model.bin\")\n-                torch.save(pt_model.state_dict(), pt_checkpoint_path)\n-                tf_model = transformers.load_pytorch_checkpoint_in_tf2_model(\n-                    tf_model, pt_checkpoint_path, allow_missing_keys=allow_missing_keys\n-                )\n-\n-                tf_checkpoint_path = os.path.join(tmpdirname, \"tf_model.h5\")\n-                tf_model.save_weights(tf_checkpoint_path)\n-                pt_model = transformers.load_tf2_checkpoint_in_pytorch_model(\n-                    pt_model, tf_checkpoint_path, allow_missing_keys=allow_missing_keys\n-                )\n-\n-            # Original test: check without `labels`\n-            self.check_pt_tf_models(tf_model, pt_model, tf_inputs_dict)\n-            # check with `labels`\n-            if tf_inputs_dict_with_labels:\n-                self.check_pt_tf_models(tf_model, pt_model, tf_inputs_dict_with_labels)\n-\n     @slow\n     def test_compile_tf_model(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "f2849d5ed70095670fb3813db4c666961c2021af",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -49,7 +49,6 @@\n from transformers.testing_utils import (\n     check_json_file_has_correct_format,\n     get_tests_dir,\n-    is_pt_tf_cross_test,\n     require_jinja,\n     require_read_token,\n     require_tf,\n@@ -2971,48 +2970,6 @@ def test_batch_encode_plus_overflowing_tokens(self):\n                 string_sequences, return_overflowing_tokens=True, truncation=True, padding=True, max_length=3\n             )\n \n-    @is_pt_tf_cross_test\n-    def test_batch_encode_plus_tensors(self):\n-        tokenizers = self.get_tokenizers(do_lower_case=False)\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                sequences = [\n-                    \"Testing batch encode plus\",\n-                    \"Testing batch encode plus with different sequence lengths\",\n-                    \"Testing batch encode plus with different sequence lengths correctly pads\",\n-                ]\n-\n-                # A Tensor cannot be build by sequences which are not the same size\n-                self.assertRaises(ValueError, tokenizer.batch_encode_plus, sequences, return_tensors=\"pt\")\n-                self.assertRaises(ValueError, tokenizer.batch_encode_plus, sequences, return_tensors=\"tf\")\n-\n-                if tokenizer.pad_token_id is None:\n-                    self.assertRaises(\n-                        ValueError,\n-                        tokenizer.batch_encode_plus,\n-                        sequences,\n-                        padding=True,\n-                        return_tensors=\"pt\",\n-                    )\n-                    self.assertRaises(\n-                        ValueError,\n-                        tokenizer.batch_encode_plus,\n-                        sequences,\n-                        padding=\"longest\",\n-                        return_tensors=\"tf\",\n-                    )\n-                else:\n-                    pytorch_tensor = tokenizer.batch_encode_plus(sequences, padding=True, return_tensors=\"pt\")\n-                    tensorflow_tensor = tokenizer.batch_encode_plus(sequences, padding=\"longest\", return_tensors=\"tf\")\n-                    encoded_sequences = tokenizer.batch_encode_plus(sequences, padding=True)\n-\n-                    for key in encoded_sequences.keys():\n-                        pytorch_value = pytorch_tensor[key].tolist()\n-                        tensorflow_value = tensorflow_tensor[key].numpy().tolist()\n-                        encoded_value = encoded_sequences[key]\n-\n-                        self.assertEqual(pytorch_value, tensorflow_value, encoded_value)\n-\n     def _check_no_pad_token_padding(self, tokenizer, sequences):\n         # if tokenizer does not have pad_token_id, an error should be thrown\n         if tokenizer.pad_token_id is None:"
        },
        {
            "sha": "414a0940ce49a09833cb730fd0cad21b348f7754",
            "filename": "tests/utils/test_add_new_model_like.py",
            "status": "modified",
            "additions": 0,
            "deletions": 49,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Futils%2Ftest_add_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Futils%2Ftest_add_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_add_new_model_like.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -736,55 +736,6 @@ def test_retrieve_info_for_model_with_bert(self):\n         self.assertIsNone(bert_model_patterns.feature_extractor_class)\n         self.assertIsNone(bert_model_patterns.processor_class)\n \n-    def test_retrieve_info_for_model_pt_tf_with_bert(self):\n-        bert_info = retrieve_info_for_model(\"bert\", frameworks=[\"pt\", \"tf\"])\n-        bert_classes = [\n-            \"BertForTokenClassification\",\n-            \"BertForQuestionAnswering\",\n-            \"BertForNextSentencePrediction\",\n-            \"BertForSequenceClassification\",\n-            \"BertForMaskedLM\",\n-            \"BertForMultipleChoice\",\n-            \"BertModel\",\n-            \"BertForPreTraining\",\n-            \"BertLMHeadModel\",\n-        ]\n-        expected_model_classes = {\"pt\": set(bert_classes), \"tf\": {f\"TF{m}\" for m in bert_classes}}\n-\n-        self.assertEqual(set(bert_info[\"frameworks\"]), {\"pt\", \"tf\"})\n-        model_classes = {k: set(v) for k, v in bert_info[\"model_classes\"].items()}\n-        self.assertEqual(model_classes, expected_model_classes)\n-\n-        all_bert_files = bert_info[\"model_files\"]\n-        model_files = {str(Path(f).relative_to(REPO_PATH)) for f in all_bert_files[\"model_files\"]}\n-        bert_model_files = BERT_MODEL_FILES - {\"src/transformers/models/bert/modeling_flax_bert.py\"}\n-        self.assertEqual(model_files, bert_model_files)\n-\n-        test_files = {str(Path(f).relative_to(REPO_PATH)) for f in all_bert_files[\"test_files\"]}\n-        bert_test_files = {\n-            \"tests/models/bert/test_tokenization_bert.py\",\n-            \"tests/models/bert/test_modeling_bert.py\",\n-            \"tests/models/bert/test_modeling_tf_bert.py\",\n-        }\n-        self.assertEqual(test_files, bert_test_files)\n-\n-        doc_file = str(Path(all_bert_files[\"doc_file\"]).relative_to(REPO_PATH))\n-        self.assertEqual(doc_file, \"docs/source/en/model_doc/bert.md\")\n-\n-        self.assertEqual(all_bert_files[\"module_name\"], \"bert\")\n-\n-        bert_model_patterns = bert_info[\"model_patterns\"]\n-        self.assertEqual(bert_model_patterns.model_name, \"BERT\")\n-        self.assertEqual(bert_model_patterns.checkpoint, \"google-bert/bert-base-uncased\")\n-        self.assertEqual(bert_model_patterns.model_type, \"bert\")\n-        self.assertEqual(bert_model_patterns.model_lower_cased, \"bert\")\n-        self.assertEqual(bert_model_patterns.model_camel_cased, \"Bert\")\n-        self.assertEqual(bert_model_patterns.model_upper_cased, \"BERT\")\n-        self.assertEqual(bert_model_patterns.config_class, \"BertConfig\")\n-        self.assertEqual(bert_model_patterns.tokenizer_class, \"BertTokenizer\")\n-        self.assertIsNone(bert_model_patterns.feature_extractor_class)\n-        self.assertIsNone(bert_model_patterns.processor_class)\n-\n     def test_retrieve_info_for_model_with_vit(self):\n         vit_info = retrieve_info_for_model(\"vit\")\n         vit_classes = [\"ViTForImageClassification\", \"ViTModel\"]"
        },
        {
            "sha": "abd728c43aec7b08d75bd64665f89c93f30ae4a2",
            "filename": "tests/utils/test_modeling_tf_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 125,
            "changes": 127,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/tests%2Futils%2Ftest_modeling_tf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/tests%2Futils%2Ftest_modeling_tf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_tf_utils.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -16,30 +16,27 @@\n \n from __future__ import annotations\n \n-import inspect\n import json\n import os\n import random\n import tempfile\n import unittest\n import unittest.mock as mock\n \n-from huggingface_hub import HfFolder, Repository, snapshot_download\n+from huggingface_hub import HfFolder, snapshot_download\n from requests.exceptions import HTTPError\n \n-from transformers import is_tf_available, is_torch_available\n+from transformers import is_tf_available\n from transformers.configuration_utils import PretrainedConfig\n from transformers.testing_utils import (  # noqa: F401\n     TOKEN,\n     USER,\n     CaptureLogger,\n     TemporaryHubRepo,\n     _tf_gpu_memory_limit,\n-    is_pt_tf_cross_test,\n     is_staging_test,\n     require_safetensors,\n     require_tf,\n-    require_torch,\n     slow,\n )\n from transformers.utils import (\n@@ -61,14 +58,9 @@\n \n     from transformers import (\n         BertConfig,\n-        PreTrainedModel,\n-        PushToHubCallback,\n         RagRetriever,\n-        TFAutoModel,\n-        TFBertForMaskedLM,\n         TFBertForSequenceClassification,\n         TFBertModel,\n-        TFPreTrainedModel,\n         TFRagModel,\n     )\n     from transformers.modeling_tf_utils import keras, tf_shard_checkpoint, unpack_inputs\n@@ -90,9 +82,6 @@\n                 # Virtual devices must be set before GPUs have been initialized\n                 print(e)\n \n-if is_torch_available():\n-    from transformers import BertModel\n-\n \n @require_tf\n class TFModelUtilsTest(unittest.TestCase):\n@@ -241,34 +230,6 @@ def test_sharded_checkpoint_transfer(self):\n         # If this doesn't throw an error then the test passes\n         TFBertForSequenceClassification.from_pretrained(\"ArthurZ/tiny-random-bert-sharded\")\n \n-    @is_pt_tf_cross_test\n-    def test_checkpoint_sharding_local_from_pt(self):\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            _ = Repository(local_dir=tmp_dir, clone_from=\"hf-internal-testing/tiny-random-bert-sharded\")\n-            model = TFBertModel.from_pretrained(tmp_dir, from_pt=True)\n-            # the model above is the same as the model below, just a sharded pytorch version.\n-            ref_model = TFBertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n-            for p1, p2 in zip(model.weights, ref_model.weights):\n-                assert np.allclose(p1.numpy(), p2.numpy())\n-\n-    @is_pt_tf_cross_test\n-    def test_checkpoint_loading_with_prefix_from_pt(self):\n-        model = TFBertModel.from_pretrained(\n-            \"hf-internal-testing/tiny-random-bert\", from_pt=True, load_weight_prefix=\"a/b\"\n-        )\n-        ref_model = TFBertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\", from_pt=True)\n-        for p1, p2 in zip(model.weights, ref_model.weights):\n-            self.assertTrue(np.allclose(p1.numpy(), p2.numpy()))\n-            self.assertTrue(p1.name.startswith(\"a/b/\"))\n-\n-    @is_pt_tf_cross_test\n-    def test_checkpoint_sharding_hub_from_pt(self):\n-        model = TFBertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert-sharded\", from_pt=True)\n-        # the model above is the same as the model below, just a sharded pytorch version.\n-        ref_model = TFBertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n-        for p1, p2 in zip(model.weights, ref_model.weights):\n-            assert np.allclose(p1.numpy(), p2.numpy())\n-\n     def test_shard_checkpoint(self):\n         # This is the model we will use, total size 340,000 bytes.\n         model = keras.Sequential(\n@@ -437,16 +398,6 @@ def test_safetensors_checkpoint_sharding_local(self):\n                 for p1, p2 in zip(model.weights, new_model.weights):\n                     self.assertTrue(np.allclose(p1.numpy(), p2.numpy()))\n \n-    @is_pt_tf_cross_test\n-    @require_safetensors\n-    def test_bfloat16_torch_loading(self):\n-        # Assert that neither of these raise an error - both repos contain bfloat16 tensors\n-        model1 = TFAutoModel.from_pretrained(\"Rocketknight1/tiny-random-gpt2-bfloat16-pt\", from_pt=True)\n-        model2 = TFAutoModel.from_pretrained(\"Rocketknight1/tiny-random-gpt2-bfloat16\")  # PT-format safetensors\n-        # Check that PT and safetensors loading paths end up with the same values\n-        for weight1, weight2 in zip(model1.weights, model2.weights):\n-            self.assertTrue(tf.reduce_all(weight1 == weight2))\n-\n     @slow\n     def test_save_pretrained_signatures(self):\n         model = TFBertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n@@ -522,36 +473,6 @@ def test_safetensors_sharded_save_and_load(self):\n             for p1, p2 in zip(model.weights, new_model.weights):\n                 self.assertTrue(np.allclose(p1.numpy(), p2.numpy()))\n \n-    @is_pt_tf_cross_test\n-    def test_safetensors_save_and_load_pt_to_tf(self):\n-        model = TFBertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n-        pt_model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            pt_model.save_pretrained(tmp_dir, safe_serialization=True)\n-            # Check we have a model.safetensors file\n-            self.assertTrue(os.path.isfile(os.path.join(tmp_dir, SAFE_WEIGHTS_NAME)))\n-\n-            new_model = TFBertModel.from_pretrained(tmp_dir)\n-\n-            # Check models are equal\n-            for p1, p2 in zip(model.weights, new_model.weights):\n-                self.assertTrue(np.allclose(p1.numpy(), p2.numpy()))\n-\n-    @is_pt_tf_cross_test\n-    def test_sharded_safetensors_save_and_load_pt_to_tf(self):\n-        model = TFBertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n-        pt_model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            pt_model.save_pretrained(tmp_dir, safe_serialization=True, max_shard_size=\"150kB\")\n-            # Check we have a safetensors shard index file\n-            self.assertTrue(os.path.isfile(os.path.join(tmp_dir, SAFE_WEIGHTS_INDEX_NAME)))\n-\n-            new_model = TFBertModel.from_pretrained(tmp_dir)\n-\n-            # Check models are equal\n-            for p1, p2 in zip(model.weights, new_model.weights):\n-                self.assertTrue(np.allclose(p1.numpy(), p2.numpy()))\n-\n     @require_safetensors\n     def test_safetensors_load_from_hub(self):\n         tf_model = TFBertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n@@ -581,19 +502,6 @@ def test_safetensors_tf_from_tf(self):\n         for p1, p2 in zip(model.weights, new_model.weights):\n             self.assertTrue(np.allclose(p1.numpy(), p2.numpy()))\n \n-    @require_safetensors\n-    @is_pt_tf_cross_test\n-    def test_safetensors_tf_from_torch(self):\n-        hub_model = TFBertModel.from_pretrained(\"hf-internal-testing/tiny-bert-tf-only\")\n-        model = BertModel.from_pretrained(\"hf-internal-testing/tiny-bert-pt-only\")\n-\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir, safe_serialization=True)\n-            new_model = TFBertModel.from_pretrained(tmp_dir)\n-\n-        for p1, p2 in zip(hub_model.weights, new_model.weights):\n-            self.assertTrue(np.allclose(p1.numpy(), p2.numpy()))\n-\n     @require_safetensors\n     def test_safetensors_tf_from_sharded_h5_with_sharded_safetensors_local(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n@@ -729,37 +637,6 @@ def test_push_to_hub_via_save_pretrained(self):\n                     break\n             self.assertTrue(models_equal)\n \n-    @is_pt_tf_cross_test\n-    def test_push_to_hub_callback(self):\n-        with TemporaryHubRepo(token=self._token) as tmp_repo:\n-            config = BertConfig(\n-                vocab_size=99, hidden_size=32, num_hidden_layers=5, num_attention_heads=4, intermediate_size=37\n-            )\n-            model = TFBertForMaskedLM(config)\n-            model.compile()\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir:\n-                push_to_hub_callback = PushToHubCallback(\n-                    output_dir=tmp_dir,\n-                    hub_model_id=tmp_repo.repo_id,\n-                    hub_token=self._token,\n-                )\n-                model.fit(model.dummy_inputs, model.dummy_inputs, epochs=1, callbacks=[push_to_hub_callback])\n-\n-            new_model = TFBertForMaskedLM.from_pretrained(tmp_repo.repo_id)\n-            models_equal = True\n-            for p1, p2 in zip(model.weights, new_model.weights):\n-                if not tf.math.reduce_all(p1 == p2):\n-                    models_equal = False\n-                    break\n-            self.assertTrue(models_equal)\n-\n-            tf_push_to_hub_params = dict(inspect.signature(TFPreTrainedModel.push_to_hub).parameters)\n-            tf_push_to_hub_params.pop(\"base_model_card_args\")\n-            pt_push_to_hub_params = dict(inspect.signature(PreTrainedModel.push_to_hub).parameters)\n-            pt_push_to_hub_params.pop(\"deprecated_kwargs\")\n-            self.assertDictEaual(tf_push_to_hub_params, pt_push_to_hub_params)\n-\n     def test_push_to_hub_in_organization(self):\n         with TemporaryHubRepo(namespace=\"valid_org\", token=self._token) as tmp_repo:\n             config = BertConfig("
        },
        {
            "sha": "7fb24cd540fcca2c02056e252102974090b14127",
            "filename": "utils/tests_fetcher.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0863eef248af526216260707083b37a676130027/utils%2Ftests_fetcher.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0863eef248af526216260707083b37a676130027/utils%2Ftests_fetcher.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ftests_fetcher.py?ref=0863eef248af526216260707083b37a676130027",
            "patch": "@@ -1148,7 +1148,6 @@ def parse_commit_message(commit_message: str) -> Dict[str, bool]:\n \n \n JOB_TO_TEST_FILE = {\n-    \"tests_torch_and_tf\": r\"tests/models/.*/test_modeling_(?:tf_|(?!flax)).*\",\n     \"tests_torch_and_flax\": r\"tests/models/.*/test_modeling_(?:flax|(?!tf)).*\",\n     \"tests_tf\": r\"tests/models/.*/test_modeling_tf_.*\",\n     \"tests_torch\": r\"tests/models/.*/test_modeling_(?!(?:flax_|tf_)).*\","
        }
    ],
    "stats": {
        "total": 2494,
        "additions": 56,
        "deletions": 2438
    }
}