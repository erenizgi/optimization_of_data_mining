{
    "author": "abuelnasr0",
    "message": "ðŸ”´ ðŸš¨  Resizing tokens embeddings: initialize from old embeddings' normal distribution. (#33325)\n\n* intilize new embeddings from normal distrib\r\n\r\n* Fix typo in comments\r\n\r\n* Fix typo in comments\r\n\r\n* Fix style\r\n\r\n* Fix variables naming\r\n\r\n* Add tests\r\n\r\n* Fix style\r\n\r\n* code consistency nit\r\n\r\n* Add deepspeed support\r\n\r\n* Add deepspeed support\r\n\r\n* Conver embeddings weights to float32 before computations\r\n\r\n* Add deepspeed tests\r\n\r\n* Cover when vocab_size is smaller than embedding_size\r\n\r\n* Style fix\r\n\r\n* Add tests for vocab_size smaller than hiddin_size\r\n\r\n* Style fix\r\n\r\n* Nits in tests\r\n\r\n* Nits in tests\r\n\r\n* Check for deepspeed before importing it\r\n\r\n* Increase vocab_size for positive definite covariance matrix test\r\n\r\n* Add warning\r\n\r\n* Add multivariate_resizing flag and implement resizing for lm_heads\r\n\r\n* Fix typo\r\n\r\n* Fix wrong bias indexing\r\n\r\n* Fix bias is zero check\r\n\r\n* remove multivariate_resizing flag from tests\r\n\r\n* Intialize bias from old bias normal distribution\r\n\r\n* Fixup\r\n\r\n* Code usability\r\n\r\n* Use mean_resizing instead of multivariate_resizing\r\n\r\n* Fix up\r\n\r\n* Fix comments and docs",
    "sha": "78ef58325c52971506e09d62f186ffb1c4f4ee10",
    "files": [
        {
            "sha": "466823acda5cb65d2db4fa3aa4f9d799bdc3e17a",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 147,
            "deletions": 11,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/78ef58325c52971506e09d62f186ffb1c4f4ee10/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78ef58325c52971506e09d62f186ffb1c4f4ee10/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=78ef58325c52971506e09d62f186ffb1c4f4ee10",
            "patch": "@@ -2049,7 +2049,10 @@ def _get_no_split_modules(self, device_map: str):\n         return list(_no_split_modules)\n \n     def resize_token_embeddings(\n-        self, new_num_tokens: Optional[int] = None, pad_to_multiple_of: Optional[int] = None\n+        self,\n+        new_num_tokens: Optional[int] = None,\n+        pad_to_multiple_of: Optional[int] = None,\n+        mean_resizing: bool = True,\n     ) -> nn.Embedding:\n         \"\"\"\n         Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n@@ -2069,11 +2072,19 @@ def resize_token_embeddings(\n                 `>= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128. For more\n                 details about this, or help on choosing the correct value for resizing, refer to this guide:\n                 https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n+            mean_resizing (`bool`):\n+                Whether to initialize the added embeddings from a multivariate normal distribution that has old embeddings' mean and\n+                covariance or to initialize them with a normal distribution that has a mean of zero and std equals `config.initializer_range`.\n+\n+                Setting `mean_resizing` to `True` is useful when increasing the size of the embeddings of causal language models,\n+                where the generated tokens' probabilities won't be affected by the added embeddings because initializing the new embeddings with the\n+                old embeddings' mean will reduce the kl-divergence between the next token probability before and after adding the new embeddings.\n+                Refer to this article for more information: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n \n         Return:\n             `torch.nn.Embedding`: Pointer to the input tokens Embeddings Module of the model.\n         \"\"\"\n-        model_embeds = self._resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+        model_embeds = self._resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         if new_num_tokens is None and pad_to_multiple_of is None:\n             return model_embeds\n \n@@ -2096,9 +2107,11 @@ def resize_token_embeddings(\n \n         return model_embeds\n \n-    def _resize_token_embeddings(self, new_num_tokens, pad_to_multiple_of=None):\n+    def _resize_token_embeddings(self, new_num_tokens, pad_to_multiple_of=None, mean_resizing=True):\n         old_embeddings = self.get_input_embeddings()\n-        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens, pad_to_multiple_of)\n+        new_embeddings = self._get_resized_embeddings(\n+            old_embeddings, new_num_tokens, pad_to_multiple_of, mean_resizing\n+        )\n         if hasattr(old_embeddings, \"_hf_hook\"):\n             hook = old_embeddings._hf_hook\n             add_hook_to_module(new_embeddings, hook)\n@@ -2121,9 +2134,9 @@ def _resize_token_embeddings(self, new_num_tokens, pad_to_multiple_of=None):\n         if self.get_output_embeddings() is not None and not self.config.tie_word_embeddings:\n             old_lm_head = self.get_output_embeddings()\n             if isinstance(old_lm_head, torch.nn.Embedding):\n-                new_lm_head = self._get_resized_embeddings(old_lm_head, new_num_tokens)\n+                new_lm_head = self._get_resized_embeddings(old_lm_head, new_num_tokens, mean_resizing=mean_resizing)\n             else:\n-                new_lm_head = self._get_resized_lm_head(old_lm_head, new_num_tokens)\n+                new_lm_head = self._get_resized_lm_head(old_lm_head, new_num_tokens, mean_resizing=mean_resizing)\n             if hasattr(old_lm_head, \"_hf_hook\"):\n                 hook = old_lm_head._hf_hook\n                 add_hook_to_module(new_lm_head, hook)\n@@ -2138,6 +2151,7 @@ def _get_resized_embeddings(\n         old_embeddings: nn.Embedding,\n         new_num_tokens: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n+        mean_resizing: bool = True,\n     ) -> nn.Embedding:\n         \"\"\"\n         Build a resized Embedding Module from a provided token Embedding Module. Increasing the size will add newly\n@@ -2160,6 +2174,14 @@ def _get_resized_embeddings(\n                 `>= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128. For more\n                 details about this, or help on choosing the correct value for resizing, refer to this guide:\n                 https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n+            mean_resizing (`bool`):\n+                Whether to initialize the added embeddings from a multivariate normal distribution that has old embeddings' mean and\n+                covariance or to initialize them with a normal distribution that has a mean of zero and std equals `config.initializer_range`.\n+\n+                Setting `mean_resizing` to `True` is useful when increasing the size of the embeddings of causal language models,\n+                where the generated tokens' probabilities will not be affected by the added embeddings because initializing the new embeddings with the\n+                old embeddings' mean will reduce the kl-divergence between the next token probability before and after adding the new embeddings.\n+                Refer to this article for more information: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n \n \n         Return:\n@@ -2218,8 +2240,32 @@ def _get_resized_embeddings(\n             dtype=old_embeddings.weight.dtype,\n         )\n \n-        # initialize all new embeddings (in particular added tokens)\n-        self._init_weights(new_embeddings)\n+        if new_num_tokens > old_num_tokens and not mean_resizing:\n+            # initialize new embeddings (in particular added tokens) with a mean of 0 and std equals `config.initializer_range`.\n+            self._init_weights(new_embeddings)\n+\n+        elif new_num_tokens > old_num_tokens and mean_resizing:\n+            # initialize new embeddings  (in particular added tokens). The new embeddings will be initialized\n+            # from a multivariate normal distribution that has old embeddings' mean and covariance.\n+            # as described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n+            logger.warning_once(\n+                \"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. \"\n+                \"As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. \"\n+                \"To disable this, use `mean_resizing=False`\"\n+            )\n+\n+            added_num_tokens = new_num_tokens - old_num_tokens\n+            if is_deepspeed_zero3_enabled() and not is_quantized:\n+                import deepspeed\n+\n+                with deepspeed.zero.GatheredParameters([old_embeddings.weight], modifier_rank=None):\n+                    self._init_added_embeddings_weights_with_mean(\n+                        old_embeddings, new_embeddings, old_embedding_dim, old_num_tokens, added_num_tokens\n+                    )\n+            else:\n+                self._init_added_embeddings_weights_with_mean(\n+                    old_embeddings, new_embeddings, old_embedding_dim, old_num_tokens, added_num_tokens\n+                )\n \n         # Copy token embeddings from the previous weights\n \n@@ -2259,7 +2305,11 @@ def _get_resized_embeddings(\n         return old_embeddings\n \n     def _get_resized_lm_head(\n-        self, old_lm_head: nn.Linear, new_num_tokens: Optional[int] = None, transposed: Optional[bool] = False\n+        self,\n+        old_lm_head: nn.Linear,\n+        new_num_tokens: Optional[int] = None,\n+        transposed: Optional[bool] = False,\n+        mean_resizing: bool = True,\n     ) -> nn.Linear:\n         \"\"\"\n         Build a resized Linear Module from a provided old Linear Module. Increasing the size will add newly initialized\n@@ -2276,6 +2326,14 @@ def _get_resized_lm_head(\n                 `torch.nn.Linear` module of the model without doing anything. transposed (`bool`, *optional*, defaults\n                 to `False`): Whether `old_lm_head` is transposed or not. If True `old_lm_head.size()` is `lm_head_dim,\n                 vocab_size` else `vocab_size, lm_head_dim`.\n+            mean_resizing (`bool`):\n+                Whether to initialize the added embeddings from a multivariate normal distribution that has old embeddings' mean and\n+                covariance or to initialize them with a normal distribution that has a mean of zero and std equals `config.initializer_range`.\n+\n+                Setting `mean_resizing` to `True` is useful when increasing the size of the embeddings of causal language models,\n+                where the generated tokens' probabilities will not be affected by the added embeddings because initializing the new embeddings with the\n+                old embeddings' mean will reduce the kl-divergence between the next token probability before and after adding the new embeddings.\n+                Refer to this article for more information: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n \n         Return:\n             `torch.nn.Linear`: Pointer to the resized Linear Module or the old Linear Module if `new_num_tokens` is\n@@ -2322,8 +2380,40 @@ def _get_resized_lm_head(\n             dtype=old_lm_head.weight.dtype,\n         )\n \n-        # initialize new lm head (in particular added tokens)\n-        self._init_weights(new_lm_head)\n+        if new_num_tokens > old_num_tokens and not mean_resizing:\n+            # initialize new embeddings (in particular added tokens) with a mean of 0 and std equals `config.initializer_range`.\n+            self._init_weights(new_lm_head)\n+\n+        elif new_num_tokens > old_num_tokens and mean_resizing:\n+            # initialize new lm_head weights (in particular added tokens). The new lm_head weights\n+            # will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance.\n+            # as described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n+            logger.warning_once(\n+                \"The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. \"\n+                \"As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. \"\n+                \"To disable this, use `mean_resizing=False`\"\n+            )\n+\n+            added_num_tokens = new_num_tokens - old_num_tokens\n+            if is_deepspeed_zero3_enabled() and not is_quantized:\n+                import deepspeed\n+\n+                params = [old_lm_head.weight]\n+                if has_new_lm_head_bias:\n+                    params += [old_lm_head.bias]\n+                with deepspeed.zero.GatheredParameters(params, modifier_rank=None):\n+                    self._init_added_lm_head_weights_with_mean(\n+                        old_lm_head, new_lm_head, old_lm_head_dim, old_num_tokens, added_num_tokens, transposed\n+                    )\n+                    if has_new_lm_head_bias:\n+                        self._init_added_lm_head_bias_with_mean(old_lm_head, new_lm_head, added_num_tokens)\n+\n+            else:\n+                self._init_added_lm_head_weights_with_mean(\n+                    old_lm_head, new_lm_head, old_lm_head_dim, old_num_tokens, added_num_tokens, transposed\n+                )\n+                if has_new_lm_head_bias:\n+                    self._init_added_lm_head_bias_with_mean(old_lm_head, new_lm_head, added_num_tokens)\n \n         num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n \n@@ -2342,6 +2432,52 @@ def _get_resized_lm_head(\n \n         return new_lm_head\n \n+    def _init_added_embeddings_weights_with_mean(\n+        self, old_embeddings, new_embeddings, old_embedding_dim, old_num_tokens, added_num_tokens\n+    ):\n+        old_embeddings_weight = old_embeddings.weight.data.to(torch.float32)\n+        mean_embeddings = torch.mean(old_embeddings_weight, axis=0)\n+        old_centered_embeddings = old_embeddings_weight - mean_embeddings\n+        covariance = old_centered_embeddings.T @ old_centered_embeddings / old_num_tokens\n+        if old_embedding_dim >= old_num_tokens:\n+            # Covarince matrix must be positive definite. For edge cases, when `vocab_size` is\n+            # smaller than `hidden_size`, covarince matrix won't be positive definite so we\n+            # must add the eye matrix to the covarince matrix to convert it to be positive definite.\n+            covariance = covariance + torch.eye(old_embedding_dim, device=old_embeddings.weight.device) * 1e-3\n+        distribution = torch.distributions.multivariate_normal.MultivariateNormal(\n+            mean_embeddings, covariance_matrix=1e-5 * covariance\n+        )\n+        new_embeddings.weight.data[-1 * added_num_tokens :, :] = distribution.sample(\n+            sample_shape=(added_num_tokens,)\n+        ).to(old_embeddings.weight.dtype)\n+\n+    def _init_added_lm_head_weights_with_mean(\n+        self,\n+        old_lm_head,\n+        new_lm_head,\n+        old_lm_head_dim,\n+        old_num_tokens,\n+        added_num_tokens,\n+        transposed=False,\n+    ):\n+        if transposed:\n+            # Transpose to the desired shape for the function.\n+            new_lm_head.weight.data = new_lm_head.weight.data.T\n+\n+        # The same initilization logic as Embeddings.\n+        self._init_added_embeddings_weights_with_mean(\n+            old_lm_head, new_lm_head, old_lm_head_dim, old_num_tokens, added_num_tokens\n+        )\n+\n+        if transposed:\n+            # Transpose again to the correct shape.\n+            new_lm_head.weight.data = new_lm_head.weight.data.T\n+\n+    def _init_added_lm_head_bias_with_mean(self, old_lm_head, new_lm_head, added_num_tokens):\n+        bias_mean = torch.mean(old_lm_head.bias.data, axis=0, dtype=torch.float32)\n+        bias_std = torch.std(old_lm_head.bias.data, axis=0).to(torch.float32)\n+        new_lm_head.bias.data[-1 * added_num_tokens :].normal_(mean=bias_mean, std=bias_std * 1e-5)\n+\n     def _copy_lm_head_original_to_resized(\n         self, new_lm_head, old_lm_head, num_tokens_to_copy, transposed, has_new_lm_head_bias\n     ):"
        },
        {
            "sha": "66b4e25a452655e7591684acd6aadf43b0e365ad",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 167,
            "deletions": 13,
            "changes": 180,
            "blob_url": "https://github.com/huggingface/transformers/blob/78ef58325c52971506e09d62f186ffb1c4f4ee10/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78ef58325c52971506e09d62f186ffb1c4f4ee10/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=78ef58325c52971506e09d62f186ffb1c4f4ee10",
            "patch": "@@ -25,6 +25,7 @@\n import time\n import warnings\n from collections import defaultdict\n+from contextlib import contextmanager\n from typing import Dict, List, Tuple\n \n import numpy as np\n@@ -45,6 +46,12 @@\n     logging,\n     set_seed,\n )\n+from transformers.integrations import HfDeepSpeedConfig\n+from transformers.integrations.deepspeed import (\n+    is_deepspeed_available,\n+    is_deepspeed_zero3_enabled,\n+    unset_hf_deepspeed_config,\n+)\n from transformers.models.auto import get_values\n from transformers.models.auto.modeling_auto import (\n     MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES,\n@@ -75,6 +82,7 @@\n     is_pt_tf_cross_test,\n     require_accelerate,\n     require_bitsandbytes,\n+    require_deepspeed,\n     require_flash_attn,\n     require_non_xpu,\n     require_read_token,\n@@ -134,6 +142,9 @@\n if is_torch_fx_available():\n     from transformers.utils.fx import _FX_SUPPORTED_MODELS_WITH_KV_CACHE, symbolic_trace\n \n+if is_deepspeed_available():\n+    import deepspeed\n+\n \n def _config_zero_init(config):\n     configs_no_init = copy.deepcopy(config)\n@@ -171,6 +182,15 @@ def _mock_all_init_weights(self):\n         self.tie_weights()\n \n \n+@contextmanager\n+def _deepspeed_zero3(ds_config):\n+    dschf = HfDeepSpeedConfig(ds_config)\n+    try:\n+        yield dschf\n+    finally:\n+        unset_hf_deepspeed_config()\n+\n+\n @require_torch\n class ModelTesterMixin:\n     model_tester = None\n@@ -1797,8 +1817,13 @@ def test_resize_tokens_embeddings(self):\n \n         for model_class in self.all_model_classes:\n             config = copy.deepcopy(original_config)\n-            model = model_class(config)\n-            model.to(torch_device)\n+            if is_deepspeed_zero3_enabled():\n+                with deepspeed.zero.Init():\n+                    model = model_class(config)\n+            else:\n+                model = model_class(config)\n+                model.to(torch_device)\n+\n             model_embed_pre_resize = model.get_input_embeddings()\n             type_model_embed_pre_resize = type(model_embed_pre_resize)\n \n@@ -1813,15 +1838,26 @@ def test_resize_tokens_embeddings(self):\n             # Check that resizing the token embeddings with a larger vocab size increases the model's vocab size\n             model_embed = model.resize_token_embeddings(model_vocab_size + 10)\n             new_model_vocab_size = model.config.get_text_config().vocab_size\n-\n             self.assertEqual(new_model_vocab_size, model_vocab_size + 10)\n             # Check that it actually resizes the embeddings matrix\n             self.assertEqual(model_embed.weight.shape[0], cloned_embeddings.shape[0] + 10)\n             # Check to make sure the type of embeddings returned post resizing is same as type of input\n             type_model_embed_post_resize = type(model_embed)\n             self.assertEqual(type_model_embed_pre_resize, type_model_embed_post_resize)\n+            # Check that added embeddings mean is close to the old embeddings mean\n+            if is_deepspeed_zero3_enabled():\n+                with deepspeed.zero.GatheredParameters(model_embed.weight, modifier_rank=None):\n+                    old_embeddings_mean = torch.mean(model_embed.weight.data[:-10, :], axis=0)\n+                    new_embeddings_mean = torch.mean(model_embed.weight.data[-10:, :], axis=0)\n+            else:\n+                old_embeddings_mean = torch.mean(model_embed.weight.data[:-10, :], axis=0)\n+                new_embeddings_mean = torch.mean(model_embed.weight.data[-10:, :], axis=0)\n+            torch.testing.assert_close(old_embeddings_mean, new_embeddings_mean, atol=1e-3, rtol=1e-1)\n+\n             # Check that the model can still do a forward pass successfully (every parameter should be resized)\n-            model(**self._prepare_for_class(inputs_dict, model_class))\n+            if not is_deepspeed_zero3_enabled():\n+                # A distriputed launcher is needed for the forward pass when deepspeed is enabled\n+                model(**self._prepare_for_class(inputs_dict, model_class))\n \n             # Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size\n             model_embed = model.resize_token_embeddings(model_vocab_size - 15)\n@@ -1835,9 +1871,11 @@ def test_resize_tokens_embeddings(self):\n             inputs_dict[\"input_ids\"].clamp_(max=model_vocab_size - 15 - 1)\n \n             # make sure that decoder_input_ids are resized as well\n-            if \"decoder_input_ids\" in inputs_dict:\n-                inputs_dict[\"decoder_input_ids\"].clamp_(max=model_vocab_size - 15 - 1)\n-            model(**self._prepare_for_class(inputs_dict, model_class))\n+            if not is_deepspeed_zero3_enabled():\n+                # A distriputed launcher is needed for the forward pass when deepspeed is enabled\n+                if \"decoder_input_ids\" in inputs_dict:\n+                    inputs_dict[\"decoder_input_ids\"].clamp_(max=model_vocab_size - 15 - 1)\n+                model(**self._prepare_for_class(inputs_dict, model_class))\n \n             # Check that adding and removing tokens has not modified the first part of the embedding matrix.\n             models_equal = True\n@@ -1847,9 +1885,13 @@ def test_resize_tokens_embeddings(self):\n \n             self.assertTrue(models_equal)\n \n-            config = copy.deepcopy(original_config)\n-            model = model_class(config)\n-            model.to(torch_device)\n+            del model\n+            if is_deepspeed_zero3_enabled():\n+                with deepspeed.zero.Init():\n+                    model = model_class(config)\n+            else:\n+                model = model_class(config)\n+                model.to(torch_device)\n \n             model_vocab_size = config.get_text_config().vocab_size\n             model.resize_token_embeddings(model_vocab_size + 10, pad_to_multiple_of=1)\n@@ -1877,6 +1919,63 @@ def test_resize_tokens_embeddings(self):\n             ):\n                 model.resize_token_embeddings(model_vocab_size, pad_to_multiple_of=1.3)\n \n+            # Test when `vocab_size` is smaller than `hidden_size`.\n+            del model\n+            config.vocab_size = 4\n+            if is_deepspeed_zero3_enabled():\n+                with deepspeed.zero.Init():\n+                    model = model_class(config)\n+            else:\n+                model = model_class(config)\n+                model.to(torch_device)\n+\n+            model_vocab_size = config.get_text_config().vocab_size\n+            # Retrieve the embeddings and clone theme\n+            model_embed = model.resize_token_embeddings(model_vocab_size)\n+            cloned_embeddings = model_embed.weight.clone()\n+\n+            # Check that resizing the token embeddings with a larger vocab size increases the model's vocab size\n+            model_embed = model.resize_token_embeddings(model_vocab_size + 10)\n+            new_model_vocab_size = model.config.get_text_config().vocab_size\n+            self.assertEqual(new_model_vocab_size, model_vocab_size + 10)\n+            # Check that it actually resizes the embeddings matrix\n+            self.assertEqual(model_embed.weight.shape[0], cloned_embeddings.shape[0] + 10)\n+            # Check to make sure the type of embeddings returned post resizing is same as type of input\n+            type_model_embed_post_resize = type(model_embed)\n+            self.assertEqual(type_model_embed_pre_resize, type_model_embed_post_resize)\n+            # Check that added embeddings mean is close to the old embeddings mean\n+            if is_deepspeed_zero3_enabled():\n+                with deepspeed.zero.GatheredParameters(model_embed.weight, modifier_rank=None):\n+                    old_embeddings_mean = torch.mean(model_embed.weight.data[:-10, :], axis=0)\n+                    new_embeddings_mean = torch.mean(model_embed.weight.data[-10:, :], axis=0)\n+            else:\n+                old_embeddings_mean = torch.mean(model_embed.weight.data[:-10, :], axis=0)\n+                new_embeddings_mean = torch.mean(model_embed.weight.data[-10:, :], axis=0)\n+            torch.testing.assert_close(old_embeddings_mean, new_embeddings_mean, atol=1e-3, rtol=1e-1)\n+\n+    @require_deepspeed\n+    @require_torch_gpu\n+    def test_resize_tokens_embeddings_with_deepspeed(self):\n+        ds_config = {\n+            \"zero_optimization\": {\n+                \"stage\": 3,\n+                \"offload_param\": {\"device\": \"cpu\", \"pin_memory\": True},\n+            },\n+        }\n+        with _deepspeed_zero3(ds_config):\n+            self.test_resize_tokens_embeddings()\n+\n+    @require_deepspeed\n+    @require_torch_multi_gpu\n+    def test_resize_tokens_embeddings_with_deepspeed_multi_gpu(self):\n+        ds_config = {\n+            \"zero_optimization\": {\n+                \"stage\": 3,\n+            },\n+        }\n+        with _deepspeed_zero3(ds_config):\n+            self.test_resize_tokens_embeddings()\n+\n     def test_resize_embeddings_untied(self):\n         if not self.test_resize_embeddings:\n             self.skipTest(reason=\"test_resize_embeddings is set to `False`\")\n@@ -1890,7 +1989,11 @@ def test_resize_embeddings_untied(self):\n \n         for model_class in self.all_model_classes:\n             config = copy.deepcopy(original_config)\n-            model = model_class(config).to(torch_device)\n+            if is_deepspeed_zero3_enabled():\n+                with deepspeed.zero.Init():\n+                    model = model_class(config)\n+            else:\n+                model = model_class(config).to(torch_device)\n \n             # if no output embeddings -> leave test\n             if model.get_output_embeddings() is None:\n@@ -1907,7 +2010,33 @@ def test_resize_embeddings_untied(self):\n             if output_embeds.bias is not None:\n                 self.assertEqual(output_embeds.bias.shape[0], model_vocab_size + 10)\n             # Check that the model can still do a forward pass successfully (every parameter should be resized)\n-            model(**self._prepare_for_class(inputs_dict, model_class))\n+            if not is_deepspeed_zero3_enabled():\n+                # A distriputed launcher is needed for the forward pass when deepspeed is enabled\n+                model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            # Test multivariate resizing.\n+            model.resize_token_embeddings(model_vocab_size + 10)\n+            output_embeds = model.get_output_embeddings()\n+            # Check that added embeddings mean is close to the old embeddings mean\n+            if is_deepspeed_zero3_enabled():\n+                with deepspeed.zero.GatheredParameters(output_embeds.weight, modifier_rank=None):\n+                    old_embeddings_mean = torch.mean(output_embeds.weight.data[:-10, :], axis=0)\n+                    new_embeddings_mean = torch.mean(output_embeds.weight.data[-10:, :], axis=0)\n+            else:\n+                old_embeddings_mean = torch.mean(output_embeds.weight.data[:-10, :], axis=0)\n+                new_embeddings_mean = torch.mean(output_embeds.weight.data[-10:, :], axis=0)\n+            torch.testing.assert_close(old_embeddings_mean, new_embeddings_mean, atol=1e-3, rtol=1e-1)\n+            # check if the bias is always initialized with zero.\n+            if output_embeds.bias is not None:\n+                if is_deepspeed_zero3_enabled():\n+                    with deepspeed.zero.GatheredParameters(output_embeds.bias, modifier_rank=None):\n+                        old_bias_mean = torch.mean(output_embeds.bias.data[:-10], axis=0)\n+                        new_bias_mean = torch.mean(output_embeds.bias.data[-10:], axis=0)\n+                else:\n+                    old_bias_mean = torch.mean(output_embeds.bias.data[:-10], axis=0)\n+                    new_bias_mean = torch.mean(output_embeds.bias.data[-10:], axis=0)\n+\n+                torch.testing.assert_close(old_bias_mean, new_bias_mean, atol=1e-5, rtol=1e-2)\n \n             # Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size\n             model.resize_token_embeddings(model_vocab_size - 15)\n@@ -1925,7 +2054,32 @@ def test_resize_embeddings_untied(self):\n             if \"decoder_input_ids\" in inputs_dict:\n                 inputs_dict[\"decoder_input_ids\"].clamp_(max=model_vocab_size - 15 - 1)\n             # Check that the model can still do a forward pass successfully (every parameter should be resized)\n-            model(**self._prepare_for_class(inputs_dict, model_class))\n+            if not is_deepspeed_zero3_enabled():\n+                # A distriputed launcher is needed for the forward pass when deepspeed is enabled\n+                model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+    @require_deepspeed\n+    @require_torch_gpu\n+    def test_resize_embeddings_untied_with_deepspeed(self):\n+        ds_config = {\n+            \"zero_optimization\": {\n+                \"stage\": 3,\n+                \"offload_param\": {\"device\": \"cpu\", \"pin_memory\": True},\n+            },\n+        }\n+        with _deepspeed_zero3(ds_config):\n+            self.test_resize_embeddings_untied()\n+\n+    @require_deepspeed\n+    @require_torch_multi_gpu\n+    def test_resize_embeddings_untied_with_deepspeed_multi_gpu(self):\n+        ds_config = {\n+            \"zero_optimization\": {\n+                \"stage\": 3,\n+            },\n+        }\n+        with _deepspeed_zero3(ds_config):\n+            self.test_resize_embeddings_untied()\n \n     def test_model_get_set_embeddings(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        }
    ],
    "stats": {
        "total": 338,
        "additions": 314,
        "deletions": 24
    }
}