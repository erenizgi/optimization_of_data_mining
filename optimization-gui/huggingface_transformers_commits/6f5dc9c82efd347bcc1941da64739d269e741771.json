{
    "author": "xadupre",
    "message": "Fixes DynamicCache export issues due to control flow and inplace modifications (#36652)\n\n* Remove unnecessary masked_fill in deberta models\n\n* Enable some code when exporting but not compiling\n\n* add missing import\n\n* style\n\n* replace if by torch.cond\n\n* style\n\n* use numel\n\n* style\n\n* add unit tests\n\n* style\n\n* change empty value for dynamic cache\n\n* replace != [] by numel()\n\n* fix import issue\n\n* style",
    "sha": "6f5dc9c82efd347bcc1941da64739d269e741771",
    "files": [
        {
            "sha": "3f2e02a703b78264199af1c8be427eb638e350cf",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f5dc9c82efd347bcc1941da64739d269e741771/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f5dc9c82efd347bcc1941da64739d269e741771/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=6f5dc9c82efd347bcc1941da64739d269e741771",
            "patch": "@@ -79,10 +79,10 @@ def get_usable_length(self, new_seq_length: int, layer_idx: Optional[int] = 0) -\n     def reorder_cache(self, beam_idx: torch.LongTensor):\n         \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n         for layer_idx in range(len(self.key_cache)):\n-            if self.key_cache[layer_idx] != []:\n+            if self.key_cache[layer_idx].numel():\n                 device = self.key_cache[layer_idx].device\n                 self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n-            if self.value_cache[layer_idx] != []:\n+            if self.value_cache[layer_idx].numel():\n                 device = self.value_cache[layer_idx].device\n                 self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n \n@@ -433,12 +433,12 @@ def update(\n             if len(self.key_cache) <= layer_idx:\n                 # There may be skipped layers, fill them with empty lists\n                 for _ in range(len(self.key_cache), layer_idx):\n-                    self.key_cache.append([])\n-                    self.value_cache.append([])\n+                    self.key_cache.append(torch.tensor([]))\n+                    self.value_cache.append(torch.tensor([]))\n                 self.key_cache.append(key_states)\n                 self.value_cache.append(value_states)\n             elif (\n-                len(self.key_cache[layer_idx]) == 0\n+                not self.key_cache[layer_idx].numel()  # prefers not t.numel() to len(t) == 0 to export the model\n             ):  # fills previously skipped layers; checking for tensor causes errors\n                 self.key_cache[layer_idx] = key_states\n                 self.value_cache[layer_idx] = value_states\n@@ -454,7 +454,7 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         is_empty_layer = (\n             len(self.key_cache) == 0  # no cache in any layer\n             or len(self.key_cache) <= layer_idx  # skipped `layer_idx` and hasn't run a layer with cache after it\n-            or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n+            or not self.key_cache[layer_idx].numel()  # the layer has no cache\n         )\n         layer_seq_length = self.key_cache[layer_idx].shape[-2] if not is_empty_layer else 0\n         return layer_seq_length\n@@ -494,7 +494,7 @@ def crop(self, max_length: int):\n \n         self._seen_tokens = max_length\n         for idx in range(len(self.key_cache)):\n-            if self.key_cache[idx] != []:\n+            if self.key_cache[idx].numel():\n                 self.key_cache[idx] = self.key_cache[idx][..., :max_length, :]\n                 self.value_cache[idx] = self.value_cache[idx][..., :max_length, :]\n \n@@ -516,8 +516,8 @@ def from_batch_splits(cls, splits: List[\"DynamicCache\"]) -> \"DynamicCache\":\n         `generation.utils`\"\"\"\n         cache = cls()\n         for idx in range(len(splits[0])):\n-            key_cache = [current.key_cache[idx] for current in splits if current.key_cache[idx] != []]\n-            value_cache = [current.value_cache[idx] for current in splits if current.value_cache[idx] != []]\n+            key_cache = [current.key_cache[idx] for current in splits if current.key_cache[idx].numel()]\n+            value_cache = [current.value_cache[idx] for current in splits if current.value_cache[idx].numel()]\n             if key_cache != []:\n                 layer_keys = torch.cat(key_cache, dim=0)\n                 layer_values = torch.cat(value_cache, dim=0)"
        },
        {
            "sha": "bfb404be959e31f2ae40cf71a5155ef825e3edde",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 102,
            "deletions": 15,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f5dc9c82efd347bcc1941da64739d269e741771/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f5dc9c82efd347bcc1941da64739d269e741771/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=6f5dc9c82efd347bcc1941da64739d269e741771",
            "patch": "@@ -48,6 +48,7 @@\n     is_accelerate_available,\n     is_hqq_available,\n     is_optimum_quanto_available,\n+    is_torchdynamo_exporting,\n     logging,\n )\n from .beam_constraints import DisjunctiveConstraint, PhrasalConstraint\n@@ -374,6 +375,102 @@ class GenerationMixin:\n     To learn more about decoding strategies refer to the [text generation strategies guide](../generation_strategies).\n     \"\"\"\n \n+    def _cache_dependant_input_preparation(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: Optional[torch.FloatTensor],\n+        cache_position: Optional[torch.LongTensor],\n+    ) -> Tuple[torch.FloatTensor, torch.LongTensor]:\n+        \"\"\"\n+        Generic cache-dependent input preparation\n+        The code is put in a separate function to allow granular unit testing\n+        as it needs a different implementation to be exportable.\n+\n+        If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n+        - Exception 1: when passing input_embeds, input_ids may be missing entries\n+        - Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        - Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+        - Excpetion 4: If input_embeds are passed then slice it through `cache_position`, to keep only the unprocessed tokens and\n+          generate the first token for each sequence. Later use the generated Input ids for continuation.\n+\n+        The current implementation does not rely on ``self`` and could be\n+        a class method. It is left as a standard method to be easily rewritten.\n+        \"\"\"\n+        if is_torchdynamo_exporting():\n+            return self._cache_dependant_input_preparation_exporting(input_ids, inputs_embeds, cache_position)\n+        if inputs_embeds is not None and input_ids.shape[1] == 0:  # Exception 4\n+            inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n+        elif (\n+            inputs_embeds is not None  # Exception 1\n+            or (cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+        ):\n+            input_ids = input_ids[:, -cache_position.shape[0] :]\n+        elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n+            input_ids = input_ids[:, cache_position]\n+        return inputs_embeds, input_ids\n+\n+    def _cache_dependant_input_preparation_exporting(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: Optional[torch.FloatTensor],\n+        cache_position: Optional[torch.LongTensor],\n+    ) -> Tuple[torch.FloatTensor, torch.LongTensor]:\n+        \"\"\"\n+        This method implements method ``_cache_dependant_input_preparation``\n+        with :func:`torch.cond` to make it exportable with :func:`torch.export.export`.\n+        The code is put in a separate function to allow granular unit testing.\n+        \"\"\"\n+        if inputs_embeds is None:\n+            input_ids = input_ids[:, cache_position]\n+        else:\n+            # This is the code we need to implemented with torch.cond.\n+            # if input_ids.shape[1] == 0:\n+            #     inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n+            # else:\n+            #     if cache_position[-1] >= input_ids.shape[1]:\n+            #         input_ids = input_ids[:, -cache_position.shape[0] :]\n+            #     else:\n+            #         if input_ids.shape[1] != cache_position.shape[0]:\n+            #             input_ids = input_ids[:, cache_position]\n+            def branch_1(inputs_embeds, cache_position):\n+                return inputs_embeds[:, -cache_position.shape[0] :]\n+\n+            def branch_2(input_ids, cache_position):\n+                return input_ids[:, -cache_position.shape[0] :]\n+\n+            def branch_3(input_ids, cache_position):\n+                return input_ids[:, cache_position]\n+\n+            inputs_embeds, input_ids = torch.cond(\n+                input_ids.shape[1] == 0,\n+                (\n+                    lambda input_ids, inputs_embeds, cache_position: (\n+                        branch_1(inputs_embeds, cache_position),\n+                        input_ids,\n+                    )\n+                ),\n+                (\n+                    lambda input_ids, inputs_embeds, cache_position: (\n+                        inputs_embeds,\n+                        torch.cond(\n+                            cache_position[-1] >= input_ids.shape[1],\n+                            branch_2,\n+                            lambda input_ids, cache_position: (\n+                                torch.cond(\n+                                    input_ids.shape[1] != cache_position.shape[0],\n+                                    branch_3,\n+                                    (lambda input_ids, cache_position: input_ids),\n+                                    [input_ids, cache_position],\n+                                )\n+                            ),\n+                            [input_ids, cache_position],\n+                        ),\n+                    )\n+                ),\n+                [input_ids, inputs_embeds, cache_position],\n+            )\n+        return inputs_embeds, input_ids\n+\n     def prepare_inputs_for_generation(\n         self,\n         input_ids: torch.LongTensor,\n@@ -404,23 +501,11 @@ def prepare_inputs_for_generation(\n             cache_position = torch.arange(past_length, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n \n         # 2. Generic cache-dependent input preparation\n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n-        # Excpetion 4: If input_embeds are passed then slice it through `cache_position`, to keep only the unprocessed tokens and\n-        # generate the first token for each sequence. Later use the generated Input ids for continuation.\n         if past_key_values is not None:\n             model_inputs[\"past_key_values\"] = past_key_values\n-            if inputs_embeds is not None and input_ids.shape[1] == 0:  # Exception 4\n-                inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n-            elif (\n-                inputs_embeds is not None  # Exception 1\n-                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n-            ):\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n+            inputs_embeds, input_ids = self._cache_dependant_input_preparation(\n+                input_ids, inputs_embeds, cache_position\n+            )\n \n         # 3. Prepare base model inputs\n         input_ids_key = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n@@ -1590,6 +1675,8 @@ def _prepare_generation_config(\n             generation_config = self.generation_config\n             using_model_generation_config = True\n \n+        # `torch.export.export` usually raises an exception if it is called\n+        # with ``strict=True``. deepcopy can only be processed if ``strict=False``.\n         generation_config = copy.deepcopy(generation_config)\n \n         if not using_model_generation_config:"
        },
        {
            "sha": "096db2d11bf25897a2fa5d5f749ea17152c26c03",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f5dc9c82efd347bcc1941da64739d269e741771/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f5dc9c82efd347bcc1941da64739d269e741771/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=6f5dc9c82efd347bcc1941da64739d269e741771",
            "patch": "@@ -2047,7 +2047,7 @@ def forward(\n         cross_attention_mask: Optional[torch.Tensor] = None,\n         cross_attention_states: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "567fa499208d79bd0b5b7eaac0e09ddc7b58e0f7",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f5dc9c82efd347bcc1941da64739d269e741771/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f5dc9c82efd347bcc1941da64739d269e741771/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=6f5dc9c82efd347bcc1941da64739d269e741771",
            "patch": "@@ -236,6 +236,7 @@\n     is_torchdistx_available,\n     is_torchdynamo_available,\n     is_torchdynamo_compiling,\n+    is_torchdynamo_exporting,\n     is_torchvision_available,\n     is_torchvision_v2_available,\n     is_training_run_on_sagemaker,"
        },
        {
            "sha": "1ac109c4773661e5efdd99e5bf782523d885baee",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f5dc9c82efd347bcc1941da64739d269e741771/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f5dc9c82efd347bcc1941da64739d269e741771/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=6f5dc9c82efd347bcc1941da64739d269e741771",
            "patch": "@@ -866,6 +866,23 @@ def is_torchdynamo_compiling():\n             return False\n \n \n+def is_torchdynamo_exporting():\n+    if not is_torch_available():\n+        return False\n+\n+    try:\n+        import torch\n+\n+        return torch.compiler.is_exporting()\n+    except Exception:\n+        try:\n+            import torch._dynamo as dynamo  # noqa: F401\n+\n+            return dynamo.is_exporting()\n+        except Exception:\n+            return False\n+\n+\n def is_torch_tensorrt_fx_available():\n     if importlib.util.find_spec(\"torch_tensorrt\") is None:\n         return False"
        },
        {
            "sha": "7d3e1a6e622bea97e51c20fb7af0fc259d724115",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 50,
            "deletions": 1,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f5dc9c82efd347bcc1941da64739d269e741771/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f5dc9c82efd347bcc1941da64739d269e741771/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=6f5dc9c82efd347bcc1941da64739d269e741771",
            "patch": "@@ -47,7 +47,7 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_ipex_available\n+from transformers.utils import is_ipex_available, is_torchdynamo_exporting\n \n \n if is_torch_available():\n@@ -87,6 +87,7 @@\n         GenerateDecoderOnlyOutput,\n         GenerateEncoderDecoderOutput,\n         GenerationConfig,\n+        GenerationMixin,\n         GreedySearchDecoderOnlyOutput,\n         GreedySearchEncoderDecoderOutput,\n         LogitsProcessorList,\n@@ -2703,6 +2704,54 @@ def test_speculative_sampling_target_distribution(self):\n         self.assertTrue(last_token_counts[1] > last_token_counts[3] > last_token_counts[7] > 0)\n         self.assertTrue(last_token_counts[8] > last_token_counts[3])\n \n+    def test_cache_dependant_input_preparation_exporting(self):\n+        self.assertFalse(\n+            is_torchdynamo_exporting()\n+        )  # otherwise this test does not compare two different implementation\n+        # Case 1\n+        input_ids = torch.randint(0, 16, (2, 8), dtype=torch.int64)[:, :0]\n+        inputs_embeds = torch.rand((2, 8), dtype=torch.float32)\n+        cache_position = torch.range(0, 7, dtype=torch.int64)\n+        eager1, eager2 = GenerationMixin()._cache_dependant_input_preparation(input_ids, inputs_embeds, cache_position)\n+        export1, export2 = GenerationMixin()._cache_dependant_input_preparation_exporting(\n+            input_ids, inputs_embeds, cache_position\n+        )\n+        torch.testing.assert_close(eager1, export1)\n+        torch.testing.assert_close(eager2, export2)\n+\n+        # Case 2\n+        input_ids = torch.randint(0, 16, (2, 8), dtype=torch.int64)\n+        inputs_embeds = torch.rand((2, 8), dtype=torch.float32)\n+        cache_position = torch.range(0, 7, dtype=torch.int64)\n+        eager1, eager2 = GenerationMixin()._cache_dependant_input_preparation(input_ids, inputs_embeds, cache_position)\n+        export1, export2 = GenerationMixin()._cache_dependant_input_preparation_exporting(\n+            input_ids, inputs_embeds, cache_position\n+        )\n+        torch.testing.assert_close(eager1, export1)\n+        torch.testing.assert_close(eager2, export2)\n+\n+        # Case 3\n+        input_ids = torch.randint(0, 16, (2, 12), dtype=torch.int64)\n+        inputs_embeds = None\n+        cache_position = torch.range(0, 7, dtype=torch.int64)\n+        eager1, eager2 = GenerationMixin()._cache_dependant_input_preparation(input_ids, inputs_embeds, cache_position)\n+        export1, export2 = GenerationMixin()._cache_dependant_input_preparation_exporting(\n+            input_ids, inputs_embeds, cache_position\n+        )\n+        torch.testing.assert_close(eager1, export1)\n+        torch.testing.assert_close(eager2, export2)\n+\n+        # Case 4\n+        input_ids = torch.randint(0, 16, (2, 8), dtype=torch.int64)\n+        inputs_embeds = None\n+        cache_position = torch.range(0, 7, dtype=torch.int64)\n+        eager1, eager2 = GenerationMixin()._cache_dependant_input_preparation(input_ids, inputs_embeds, cache_position)\n+        export1, export2 = GenerationMixin()._cache_dependant_input_preparation_exporting(\n+            input_ids, inputs_embeds, cache_position\n+        )\n+        torch.testing.assert_close(eager1, export1)\n+        torch.testing.assert_close(eager2, export2)\n+\n \n global_rng = random.Random()\n "
        }
    ],
    "stats": {
        "total": 206,
        "additions": 180,
        "deletions": 26
    }
}