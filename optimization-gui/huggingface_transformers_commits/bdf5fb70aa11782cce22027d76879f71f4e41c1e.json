{
    "author": "seven-mile",
    "message": "Skip non-selected experts for qwen3_moe (#38133)\n\n* fix(qwen3moe): skip experts with no workload\n\n* avoid tolist and also update other moe models\n\n* fix: should squeeze 0-dim only",
    "sha": "bdf5fb70aa11782cce22027d76879f71f4e41c1e",
    "files": [
        {
            "sha": "ae0fd74e5665705dfa22cd0fd59b0a8ab39e7a1a",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bdf5fb70aa11782cce22027d76879f71f4e41c1e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bdf5fb70aa11782cce22027d76879f71f4e41c1e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=bdf5fb70aa11782cce22027d76879f71f4e41c1e",
            "patch": "@@ -123,10 +123,10 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         # this will be used to easily index which expert is going to be sollicitated\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n-        expert_hitted = (expert_mask.sum(dim=(-1, -2)) > 0).nonzero(as_tuple=True)[0].tolist()\n+        expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n         for expert_idx in expert_hitted:\n             expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx])\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n             # Index the correct hidden states and compute the expert hidden state for\n             # the current expert. We need to make sure to multiply the output hidden\n             # states by `routing_weights` on the corresponding tokens (top-1 and top-2)"
        },
        {
            "sha": "cd774a559746dd1c1fa87180490edb5e70b465ad",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bdf5fb70aa11782cce22027d76879f71f4e41c1e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bdf5fb70aa11782cce22027d76879f71f4e41c1e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=bdf5fb70aa11782cce22027d76879f71f4e41c1e",
            "patch": "@@ -201,10 +201,10 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         # this will be used to easily index which expert is going to be sollicitated\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n-        expert_hitted = (expert_mask.sum(dim=(-1, -2)) > 0).nonzero(as_tuple=True)[0].tolist()\n+        expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n         for expert_idx in expert_hitted:\n             expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx])\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n             # Index the correct hidden states and compute the expert hidden state for\n             # the current expert. We need to make sure to multiply the output hidden\n             # states by `routing_weights` on the corresponding tokens (top-1 and top-2)"
        },
        {
            "sha": "a5118df0c07efc0b67ca808d1a26274d83cb1fd8",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bdf5fb70aa11782cce22027d76879f71f4e41c1e/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bdf5fb70aa11782cce22027d76879f71f4e41c1e/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=bdf5fb70aa11782cce22027d76879f71f4e41c1e",
            "patch": "@@ -616,10 +616,10 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n         # Loop over all available experts in the model and perform the computation on each expert\n-        expert_hitted = (expert_mask.sum(dim=(-1, -2)) > 0).nonzero(as_tuple=True)[0].tolist()\n+        expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n         for expert_idx in expert_hitted:\n             expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx])\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n \n             # Index the correct hidden states and compute the expert hidden state for\n             # the current expert. We need to make sure to multiply the output hidden"
        },
        {
            "sha": "329da67a1e6463dfb9650e5df4ebaf2d4913a343",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bdf5fb70aa11782cce22027d76879f71f4e41c1e/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bdf5fb70aa11782cce22027d76879f71f4e41c1e/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=bdf5fb70aa11782cce22027d76879f71f4e41c1e",
            "patch": "@@ -248,9 +248,10 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n         # Loop over all available experts in the model and perform the computation on each expert\n-        for expert_idx in range(self.num_experts):\n+        expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hitted:\n             expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx])\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n \n             # Index the correct hidden states and compute the expert hidden state for\n             # the current expert. We need to make sure to multiply the output hidden"
        },
        {
            "sha": "9a043f2d8d3e79fc2ea5c8a54242ffd93cd3249f",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bdf5fb70aa11782cce22027d76879f71f4e41c1e/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bdf5fb70aa11782cce22027d76879f71f4e41c1e/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=bdf5fb70aa11782cce22027d76879f71f4e41c1e",
            "patch": "@@ -99,9 +99,10 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n         # Loop over all available experts in the model and perform the computation on each expert\n-        for expert_idx in range(self.num_experts):\n+        expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hitted:\n             expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx])\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n \n             # Index the correct hidden states and compute the expert hidden state for\n             # the current expert. We need to make sure to multiply the output hidden"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 12,
        "deletions": 10
    }
}