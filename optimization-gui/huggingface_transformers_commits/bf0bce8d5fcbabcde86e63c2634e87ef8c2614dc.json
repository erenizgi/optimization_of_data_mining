{
    "author": "cyyever",
    "message": "Apply RUFF PIE rules (#41727)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
    "files": [
        {
            "sha": "731a56b2c736e66d4b8dfbd75681f66eca782f4b",
            "filename": "examples/3D_parallel.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/examples%2F3D_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/examples%2F3D_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2F3D_parallel.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -151,7 +151,6 @@ def main():\n     if dist.is_initialized() and dp_mesh.size() > 1:\n         model = FSDP(model, device_mesh=dp_mesh, sharding_strategy=ShardingStrategy.NO_SHARD)\n         use_ddp = True\n-        pass\n \n     model.train()\n "
        },
        {
            "sha": "388e69728b79600089d7a6107ecd98eb4d8683c3",
            "filename": "examples/legacy/pytorch-lightning/run_glue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/examples%2Flegacy%2Fpytorch-lightning%2Frun_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/examples%2Flegacy%2Fpytorch-lightning%2Frun_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fpytorch-lightning%2Frun_glue.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -122,7 +122,7 @@ def _eval_end(self, outputs) -> tuple:\n         out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n         preds_list = [[] for _ in range(out_label_ids.shape[0])]\n \n-        results = {**{\"val_loss\": val_loss_mean}, **compute_metrics(self.hparams.task, preds, out_label_ids)}\n+        results = {\"val_loss\": val_loss_mean, **compute_metrics(self.hparams.task, preds, out_label_ids)}\n \n         ret = dict(results.items())\n         ret[\"log\"] = results"
        },
        {
            "sha": "2d6780bb79f260b1f7747229f2879d545545bc9d",
            "filename": "src/transformers/cli/chat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fcli%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fcli%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fchat.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -249,7 +249,7 @@ def __init__(\n \n         # Generation settings\n         config = load_generation_config(generation_config)\n-        config.update(**{\"do_sample\": True, \"max_new_tokens\": 256})  # some default values\n+        config.update(do_sample=True, max_new_tokens=256)  # some default values\n         config.update(**parse_generate_flags(generate_flags))\n         self.config = config\n "
        },
        {
            "sha": "91f0f80a892c40faa63f22ba5f31a5e75f8cdacf",
            "filename": "src/transformers/generation/continuous_batching/cache_manager.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -31,7 +31,6 @@ class CacheAllocator(ABC):\n     def allocate_blocks(self, n_blocks: int, request_id: str, free_blocks: deque[int]) -> Optional[int]:\n         \"\"\"Allocates n_blocks for a given request_id. Returns the num of blocks allocated if successful and None\n         otherwise.\"\"\"\n-        pass\n \n     def free_blocks(self, request_id: str, free_blocks: deque[int]) -> None:\n         \"\"\"Frees all blocks associated with a request_id.\"\"\"\n@@ -46,17 +45,14 @@ def free_blocks(self, request_id: str, free_blocks: deque[int]) -> None:\n     @abstractmethod\n     def get_read_indices(self, request_id: str, past_length: int, query_length: int) -> list[int]:\n         \"\"\"Returns the physical indices of where to read request_id's cache in the cache tensor.\"\"\"\n-        pass\n \n     @abstractmethod\n     def get_write_indices(self, request_id: str, past_length: int, query_length: int) -> list[int]:\n         \"\"\"Returns the physical indices of where to write request_id's cache in the cache tensor.\"\"\"\n-        pass\n \n     @abstractmethod\n     def get_seqlens_k(self, request_id: str, past_length: int, query_length: int) -> tuple[str, int]:\n         \"\"\"Returns the attention type of the cache allocator and the key sequence length for the given request_id.\"\"\"\n-        pass\n \n \n class FullAttentionCacheAllocator(CacheAllocator):"
        },
        {
            "sha": "54c2f11e5c8ca0b7db55277bec9bc68d0fed3bef",
            "filename": "src/transformers/generation/continuous_batching/scheduler.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -53,7 +53,6 @@ def schedule_batch(self, token_budget: int) -> list[RequestState]:\n         \"\"\"Schedules requests for the next batch based on available token budget. This method selects which requests\n         should be processed in the current batch, considering the token budget and the scheduler's prioritization rules.\n         The token_budget is the maximum number of tokens that can be processed in this batch.\"\"\"\n-        pass\n \n     @traced\n     def has_pending_requests(self) -> bool:"
        },
        {
            "sha": "64430fefad423c3d5251b7088dae545ec27613ed",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -410,7 +410,6 @@ def adjust_generation_fn(\n                 logger.info(\n                     \"Generation config file not found, using a generation config created from the model config.\"\n                 )\n-                pass\n             # Load custom generate function if `pretrained_model_name_or_path` defines it (and override `generate`)\n             if hasattr(self, \"load_custom_generate\"):\n                 try:"
        },
        {
            "sha": "0d4910732528d7dc0afeff3659f8d9366ecb08c9",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -171,7 +171,6 @@ def forward(self, pixel_values, input_ids, cache_position):\n         Returns:\n             Output with logits for text generation\n         \"\"\"\n-        pass\n \n     def generate(\n         self, pixel_values=None, input_ids=None, max_new_tokens=50, do_sample=False, temperature=1.0, **kwargs\n@@ -189,7 +188,6 @@ def generate(\n         Returns:\n             Generated sequences\n         \"\"\"\n-        pass\n \n \n class TorchExportableModuleForDecoderOnlyLM(torch.nn.Module):"
        },
        {
            "sha": "92f101d2dc700c6712078e80d766698643bb9429",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -737,7 +737,7 @@ def setup(self, args, state, model, **kwargs):\n                 combined_dict = {**model_config, **combined_dict}\n             if hasattr(model, \"peft_config\") and model.peft_config is not None:\n                 peft_config = model.peft_config\n-                combined_dict = {**{\"peft_config\": peft_config}, **combined_dict}\n+                combined_dict = {\"peft_config\": peft_config, **combined_dict}\n             trial_name = state.trial_name\n             init_args = {}\n             if trial_name is not None:\n@@ -982,7 +982,7 @@ def setup(self, args, state, model, **kwargs):\n                 combined_dict = {**model_config, **combined_dict}\n             if hasattr(model, \"peft_config\") and model.peft_config is not None:\n                 peft_config = model.peft_config\n-                combined_dict = {**{\"peft_config\": peft_config}, **combined_dict}\n+                combined_dict = {\"peft_config\": peft_config, **combined_dict}\n \n             self._trackio.init(\n                 project=project,\n@@ -2246,7 +2246,7 @@ def setup(self, args, state, model, **kwargs):\n                 combined_dict = {**model_config, **combined_dict}\n             if hasattr(model, \"peft_config\") and model.peft_config is not None:\n                 peft_config = model.peft_config\n-                combined_dict = {**{\"peft_config\": peft_config}, **combined_dict}\n+                combined_dict = {\"peft_config\": peft_config, **combined_dict}\n             trial_name = state.trial_name\n             init_args = {}\n             if trial_name is not None and args.run_name is not None:"
        },
        {
            "sha": "dcf62769c537fce6f88df33dacb52237a8dfd113",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -5413,9 +5413,7 @@ def encode(self, input_values: torch.Tensor, *args, **kwargs):\n         \"\"\"\n         Encode raw audio retrieved from a respective `FeatureExtractor` into discrete audio codebooks (with x channels)\n         \"\"\"\n-        pass\n \n     @abstractmethod\n     def decode(self, audio_codes: torch.Tensor, *args, **kwargs):\n         \"\"\"Decode from discrete audio codebooks back to raw audio\"\"\"\n-        pass"
        },
        {
            "sha": "66483c248a2a4bad049f91502853b9d99cd8fb9f",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -1155,8 +1155,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n class AriaTextAttention(LlamaAttention):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    pass\n-\n \n class AriaTextDecoderLayer(LlamaDecoderLayer):\n     \"\"\""
        },
        {
            "sha": "622221d5a57b2bb7600a51c883483a661e2fcb82",
            "filename": "src/transformers/models/bark/generation_configuration_bark.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -235,7 +235,6 @@ def validate(self, **kwargs):\n         Overrides GenerationConfig.validate because BarkFineGenerationConfig don't use any parameters outside\n         temperature.\n         \"\"\"\n-        pass\n \n \n class BarkGenerationConfig(GenerationConfig):"
        },
        {
            "sha": "ba9cd4025dc28a3b34118b91c029d19c3cad46f5",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -688,7 +688,6 @@ def reset_memory_length(self, mem_len):\n \n     def _prune_heads(self, heads):\n         logger.info(\"Head pruning is not implemented for Transformer-XL model\")\n-        pass\n \n     def init_mems(self, bsz):\n         if self.mem_len > 0:"
        },
        {
            "sha": "6f28e572c70d60db68c9f16755593b9b5903b23f",
            "filename": "src/transformers/models/detr/image_processing_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -1652,7 +1652,7 @@ def to_tuple(tup):\n             # It may be that we have several predicted masks for the same stuff class.\n             # In the following, we track the list of masks ids for each stuff class (they are merged later on)\n             cur_masks = cur_masks.flatten(1)\n-            stuff_equiv_classes = defaultdict(lambda: [])\n+            stuff_equiv_classes = defaultdict(list)\n             for k, label in enumerate(cur_labels):\n                 if not is_thing_map[label.item()]:\n                     stuff_equiv_classes[label.item()].append(k)"
        },
        {
            "sha": "360144e983b21f9b9d384dcd7c0ab8fa7d5f5433",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -876,7 +876,7 @@ def to_tuple(tup):\n             # It may be that we have several predicted masks for the same stuff class.\n             # In the following, we track the list of masks ids for each stuff class (they are merged later on)\n             cur_masks = cur_masks.flatten(1)\n-            stuff_equiv_classes = defaultdict(lambda: [])\n+            stuff_equiv_classes = defaultdict(list)\n             for k, label in enumerate(cur_labels):\n                 if not is_thing_map[label.item()]:\n                     stuff_equiv_classes[label.item()].append(k)"
        },
        {
            "sha": "cee72924b20c9184ec7fa99d03fb3f456223d6cc",
            "filename": "src/transformers/models/falcon_mamba/modular_falcon_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -232,8 +232,6 @@ class FalconMambaCache(MambaCache):\n         ```\n     \"\"\"\n \n-    pass\n-\n \n def rms_forward(hidden_states, variance_epsilon=1e-6):\n     \"\"\""
        },
        {
            "sha": "e44831063200c631801533cfdb57931d52f1614c",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -170,8 +170,6 @@ def forward(self, hidden_states):\n \n \n class GptOssRotaryEmbedding(Qwen2RotaryEmbedding):\n-    pass\n-\n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):"
        },
        {
            "sha": "fbc9d4494e64d547ffa13a02dd0116748ab956b9",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -254,7 +254,6 @@ def forward(self, hidden_states):\n     pass\n except Exception:\n     logger.warning(\"discovered apex but it failed to load, falling back to LongT5LayerNorm\")\n-    pass\n \n \n # Copied from transformers.models.t5.modeling_t5.T5DenseActDense with T5->LongT5"
        },
        {
            "sha": "6b80e73ef8a83ce4386599fa806dbd297fe77f69",
            "filename": "src/transformers/models/metaclip_2/modular_metaclip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -93,8 +93,6 @@ class MetaClip2TextConfig(CLIPTextConfig):\n     >>> configuration = model.config\n     ```\"\"\"\n \n-    pass\n-\n \n class MetaClip2VisionConfig(CLIPVisionConfig):\n     r\"\"\"\n@@ -151,8 +149,6 @@ class MetaClip2VisionConfig(CLIPVisionConfig):\n     >>> configuration = model.config\n     ```\"\"\"\n \n-    pass\n-\n \n class MetaClip2Config(CLIPConfig):\n     r\"\"\"\n@@ -200,8 +196,6 @@ class MetaClip2Config(CLIPConfig):\n     >>> config = MetaClip2Config(text_config=config_text, vision_config=config_vision)\n     ```\"\"\"\n \n-    pass\n-\n \n class MetaClip2TextEmbeddings(CLIPTextEmbeddings):\n     pass"
        },
        {
            "sha": "09f7e5783b9caff70f52c8e3bd7515ed8eb7730e",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -92,7 +92,6 @@ def forward(self, hidden_states):\n     pass\n except Exception:\n     logger.warning(\"Discovered apex but it failed to load, falling back to Pix2StructLayerNorm\")\n-    pass\n \n \n class Pix2StructVisionEmbeddings(nn.Module):"
        },
        {
            "sha": "0fe560260d789dd05ab2f69630728600fcd413e7",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -56,7 +56,6 @@\n     pass\n except Exception:\n     logger.warning(\"Discovered apex but it failed to load, falling back to Pop2PianoLayerNorm\")\n-    pass\n \n \n # Copied from transformers.models.t5.modeling_t5.T5LayerNorm with T5->Pop2Piano"
        },
        {
            "sha": "5e259fd1cece76aff3d18ea2ed42ce39154c9c8b",
            "filename": "src/transformers/models/sam_hq/modular_sam_hq.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -68,8 +68,6 @@ class SamHQPromptEncoderConfig(SamPromptEncoderConfig):\n             The non-linear activation function in the encoder and pooler.\n     \"\"\"\n \n-    pass\n-\n \n class SamHQVisionConfig(SamVisionConfig):\n     pass\n@@ -140,8 +138,6 @@ class SamHQConfig(SamConfig):\n             Dictionary of keyword arguments.\n     \"\"\"\n \n-    pass\n-\n \n class SamHQVisionEncoderOutput(SamVisionEncoderOutput):\n     r\"\"\""
        },
        {
            "sha": "3b386091adebac4b7ef077239c61f9daebf41f8e",
            "filename": "src/transformers/models/smolvlm/modular_smolvlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -91,7 +91,6 @@ class SmolVLMVisionConfig(Idefics3VisionConfig):\n     ```\"\"\"\n \n     model_type = \"smolvlm_vision\"\n-    pass\n \n \n class SmolVLMPreTrainedModel(Idefics3PreTrainedModel):\n@@ -141,7 +140,6 @@ class SmolVLMConfig(Idefics3Config):\n     ```\"\"\"\n \n     model_type = \"smolvlm\"\n-    pass\n \n \n class SmolVLMImageProcessor(Idefics3ImageProcessor):"
        },
        {
            "sha": "4c84c45781666a8ca47121b2a9c3624c7182dc57",
            "filename": "src/transformers/models/swin2sr/convert_swin2sr_original_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fconvert_swin2sr_original_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fconvert_swin2sr_original_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fconvert_swin2sr_original_to_pytorch.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -153,7 +153,6 @@ def convert_state_dict(orig_state_dict, config):\n                 orig_state_dict[f\"swin2sr.encoder.stages.{stage_num}.layers.{block_num}.attention.self.value.bias\"] = (\n                     val[-dim:]\n                 )\n-            pass\n         else:\n             orig_state_dict[rename_key(key, config)] = val\n "
        },
        {
            "sha": "4a0f60dfacaf23c64ee332e0e9f8d298145cbafb",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -93,7 +93,6 @@ def forward(self, hidden_states):\n     pass\n except Exception:\n     logger.warning(\"discovered apex but it failed to load, falling back to T5LayerNorm\")\n-    pass\n \n \n class T5DenseActDense(nn.Module):"
        },
        {
            "sha": "50e577e1838c3be2c29a7c62992043262960cc22",
            "filename": "src/transformers/models/timm_backbone/modeling_timm_backbone.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -118,7 +118,6 @@ def _init_weights(self, module):\n         \"\"\"\n         Empty init weights function to ensure compatibility of the class in the library.\n         \"\"\"\n-        pass\n \n     def forward(\n         self,"
        },
        {
            "sha": "e8a2133d0114cfed9ba5db1ae736e46169e21e60",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -4004,13 +4004,11 @@ def _switch_to_input_mode(self):\n         \"\"\"\n         Private method to put the tokenizer in input mode (when it has different modes for input/outputs)\n         \"\"\"\n-        pass\n \n     def _switch_to_target_mode(self):\n         \"\"\"\n         Private method to put the tokenizer in target mode (when it has different modes for input/outputs)\n         \"\"\"\n-        pass\n \n     @contextmanager\n     def as_target_tokenizer(self):"
        },
        {
            "sha": "2f462dc011beb3d55cbbf000d1e25dcd42d0e0f8",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -3541,7 +3541,7 @@ def log(self, logs: dict[str, float], start_time: Optional[float] = None) -> Non\n                 )\n                 logs.update(speed_metrics(\"train\", start_time, num_tokens=current_session_num_tokens))\n \n-        output = {**logs, **{\"step\": self.state.global_step}}\n+        output = {**logs, \"step\": self.state.global_step}\n         self.state.log_history.append(output)\n         self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n "
        },
        {
            "sha": "92b4ee9c3499f3df24f1037740bf061152dba8cf",
            "filename": "src/transformers/trainer_callback.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Ftrainer_callback.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Ftrainer_callback.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_callback.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -348,93 +348,78 @@ def on_init_end(self, args: TrainingArguments, state: TrainerState, control: Tra\n         \"\"\"\n         Event called at the end of the initialization of the [`Trainer`].\n         \"\"\"\n-        pass\n \n     def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n         \"\"\"\n         Event called at the beginning of training.\n         \"\"\"\n-        pass\n \n     def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n         \"\"\"\n         Event called at the end of training.\n         \"\"\"\n-        pass\n \n     def on_epoch_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n         \"\"\"\n         Event called at the beginning of an epoch.\n         \"\"\"\n-        pass\n \n     def on_epoch_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n         \"\"\"\n         Event called at the end of an epoch.\n         \"\"\"\n-        pass\n \n     def on_step_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n         \"\"\"\n         Event called at the beginning of a training step. If using gradient accumulation, one training step might take\n         several inputs.\n         \"\"\"\n-        pass\n \n     def on_pre_optimizer_step(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n         \"\"\"\n         Event called before the optimizer step but after gradient clipping. Useful for monitoring gradients.\n         \"\"\"\n-        pass\n \n     def on_optimizer_step(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n         \"\"\"\n         Event called after the optimizer step but before gradients are zeroed out. Useful for monitoring gradients.\n         \"\"\"\n-        pass\n \n     def on_substep_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n         \"\"\"\n         Event called at the end of an substep during gradient accumulation.\n         \"\"\"\n-        pass\n \n     def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n         \"\"\"\n         Event called at the end of a training step. If using gradient accumulation, one training step might take\n         several inputs.\n         \"\"\"\n-        pass\n \n     def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n         \"\"\"\n         Event called after an evaluation phase.\n         \"\"\"\n-        pass\n \n     def on_predict(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, metrics, **kwargs):\n         \"\"\"\n         Event called after a successful prediction.\n         \"\"\"\n-        pass\n \n     def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n         \"\"\"\n         Event called after a checkpoint save.\n         \"\"\"\n-        pass\n \n     def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n         \"\"\"\n         Event called after logging the last logs.\n         \"\"\"\n-        pass\n \n     def on_prediction_step(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n         \"\"\"\n         Event called after a prediction step.\n         \"\"\"\n-        pass\n \n \n class CallbackHandler(TrainerCallback):"
        },
        {
            "sha": "6429917f87c8feadb400f88f82bc9753bf981e48",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -2223,7 +2223,7 @@ def to_sanitized_dict(self) -> dict[str, Any]:\n         Sanitized serialization to use with TensorBoard's hparams\n         \"\"\"\n         d = self.to_dict()\n-        d = {**d, **{\"train_batch_size\": self.train_batch_size, \"eval_batch_size\": self.eval_batch_size}}\n+        d = {**d, \"train_batch_size\": self.train_batch_size, \"eval_batch_size\": self.eval_batch_size}\n \n         valid_types = [bool, int, float, str]\n         if is_torch_available():"
        },
        {
            "sha": "e07228163c2b944add7585d230f1b4e3a3a0d487",
            "filename": "src/transformers/utils/chat_template_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Futils%2Fchat_template_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Futils%2Fchat_template_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fchat_template_utils.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -75,14 +75,10 @@\n class TypeHintParsingException(Exception):\n     \"\"\"Exception raised for errors in parsing type hints to generate JSON schemas\"\"\"\n \n-    pass\n-\n \n class DocstringParsingException(Exception):\n     \"\"\"Exception raised for errors in parsing docstrings to generate JSON schemas\"\"\"\n \n-    pass\n-\n \n def _get_json_schema_type(param_type: type) -> dict[str, str]:\n     type_mapping = {"
        },
        {
            "sha": "12b4da8927d9354499af040fac9c403aca5087b1",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -329,12 +329,7 @@ def __init__(\n                 self.quant_config[key] = HQQBaseQuantizeConfig(**dynamic_config[key])\n         else:\n             self.quant_config = HQQBaseQuantizeConfig(\n-                **{\n-                    \"nbits\": nbits,\n-                    \"group_size\": group_size,\n-                    \"view_as_float\": view_as_float,\n-                    \"axis\": axis,\n-                }\n+                nbits=nbits, group_size=group_size, view_as_float=view_as_float, axis=axis\n             )\n \n         self.quant_method = QuantizationMethod.HQQ\n@@ -346,7 +341,6 @@ def post_init(self):\n         r\"\"\"\n         Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.\n         \"\"\"\n-        pass\n \n     @classmethod\n     def from_dict(cls, config: dict[str, Any]):\n@@ -1908,7 +1902,6 @@ def post_init(self):\n         r\"\"\"\n         Safety checker that arguments are correct\n         \"\"\"\n-        pass\n \n \n @dataclass"
        },
        {
            "sha": "a44c9f01ded8c8aabaa9460d819e78e5af94add5",
            "filename": "utils/test_module/custom_tokenization_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/utils%2Ftest_module%2Fcustom_tokenization_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc/utils%2Ftest_module%2Fcustom_tokenization_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ftest_module%2Fcustom_tokenization_fast.py?ref=bf0bce8d5fcbabcde86e63c2634e87ef8c2614dc",
            "patch": "@@ -5,4 +5,3 @@\n \n class CustomTokenizerFast(BertTokenizerFast):\n     slow_tokenizer_class = CustomTokenizer\n-    pass"
        }
    ],
    "stats": {
        "total": 86,
        "additions": 10,
        "deletions": 76
    }
}