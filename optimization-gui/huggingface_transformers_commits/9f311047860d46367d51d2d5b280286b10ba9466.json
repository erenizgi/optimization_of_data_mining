{
    "author": "ydshieh",
    "message": "delete already deprecated models (#42235)\n\n* fix\n\n* push deleted files\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "9f311047860d46367d51d2d5b280286b10ba9466",
    "files": [
        {
            "sha": "e1fc5784a0677a9387d626b4137ea4ec51235a34",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 48,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f311047860d46367d51d2d5b280286b10ba9466/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f311047860d46367d51d2d5b280286b10ba9466/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=9f311047860d46367d51d2d5b280286b10ba9466",
            "patch": "@@ -420,8 +420,6 @@\n         title: BLOOM\n       - local: model_doc/blt\n         title: BLT\n-      - local: model_doc/bort\n-        title: BORT\n       - local: model_doc/byt5\n         title: ByT5\n       - local: model_doc/camembert\n@@ -476,8 +474,6 @@\n         title: Ernie4_5\n       - local: model_doc/ernie4_5_moe\n         title: Ernie4_5_MoE\n-      - local: model_doc/ernie_m\n-        title: ErnieM\n       - local: model_doc/esm\n         title: ESM\n       - local: model_doc/exaone4\n@@ -532,8 +528,6 @@\n         title: GPTBigCode\n       - local: model_doc/gpt_oss\n         title: GptOss\n-      - local: model_doc/gptsan-japanese\n-        title: GPTSAN Japanese\n       - local: model_doc/gpt-sw3\n         title: GPTSw3\n       - local: model_doc/granite\n@@ -558,8 +552,6 @@\n         title: Jamba\n       - local: model_doc/jetmoe\n         title: JetMoe\n-      - local: model_doc/jukebox\n-        title: Jukebox\n       - local: model_doc/led\n         title: LED\n       - local: model_doc/lfm2\n@@ -594,8 +586,6 @@\n         title: MarkupLM\n       - local: model_doc/mbart\n         title: MBart and MBart-50\n-      - local: model_doc/mega\n-        title: MEGA\n       - local: model_doc/megatron-bert\n         title: MegatronBERT\n       - local: model_doc/megatron_gpt2\n@@ -630,8 +620,6 @@\n         title: myt5\n       - local: model_doc/nemotron\n         title: Nemotron\n-      - local: model_doc/nezha\n-        title: NEZHA\n       - local: model_doc/nllb\n         title: NLLB\n       - local: model_doc/nllb-moe\n@@ -646,8 +634,6 @@\n         title: Olmo3\n       - local: model_doc/olmoe\n         title: OLMoE\n-      - local: model_doc/open-llama\n-        title: Open-Llama\n       - local: model_doc/opt\n         title: OPT\n       - local: model_doc/pegasus\n@@ -668,8 +654,6 @@\n         title: PLBart\n       - local: model_doc/prophetnet\n         title: ProphetNet\n-      - local: model_doc/qdqbert\n-        title: QDQBert\n       - local: model_doc/qwen2\n         title: Qwen2\n       - local: model_doc/qwen2_moe\n@@ -682,16 +666,12 @@\n         title: Qwen3Next\n       - local: model_doc/rag\n         title: RAG\n-      - local: model_doc/realm\n-        title: REALM\n       - local: model_doc/recurrent_gemma\n         title: RecurrentGemma\n       - local: model_doc/reformer\n         title: Reformer\n       - local: model_doc/rembert\n         title: RemBERT\n-      - local: model_doc/retribert\n-        title: RetriBERT\n       - local: model_doc/roberta\n         title: RoBERTa\n       - local: model_doc/roberta-prelayernorm\n@@ -720,10 +700,6 @@\n         title: T5Gemma\n       - local: model_doc/t5v1.1\n         title: T5v1.1\n-      - local: model_doc/tapex\n-        title: TAPEX\n-      - local: model_doc/transfo-xl\n-        title: Transformer XL\n       - local: model_doc/ul2\n         title: UL2\n       - local: model_doc/umt5\n@@ -736,8 +712,6 @@\n         title: XGLM\n       - local: model_doc/xlm\n         title: XLM\n-      - local: model_doc/xlm-prophetnet\n-        title: XLM-ProphetNet\n       - local: model_doc/xlm-roberta\n         title: XLM-RoBERTa\n       - local: model_doc/xlm-roberta-xl\n@@ -784,8 +758,6 @@\n         title: Depth Anything V2\n       - local: model_doc/depth_pro\n         title: DepthPro\n-      - local: model_doc/deta\n-        title: DETA\n       - local: model_doc/detr\n         title: DETR\n       - local: model_doc/dinat\n@@ -800,8 +772,6 @@\n         title: DiT\n       - local: model_doc/dpt\n         title: DPT\n-      - local: model_doc/efficientformer\n-        title: EfficientFormer\n       - local: model_doc/efficientloftr\n         title: EfficientLoFTR\n       - local: model_doc/efficientnet\n@@ -838,8 +808,6 @@\n         title: MobileViT\n       - local: model_doc/mobilevitv2\n         title: MobileViTV2\n-      - local: model_doc/nat\n-        title: NAT\n       - local: model_doc/poolformer\n         title: PoolFormer\n       - local: model_doc/prompt_depth_anything\n@@ -886,12 +854,8 @@\n         title: Timm Wrapper\n       - local: model_doc/upernet\n         title: UperNet\n-      - local: model_doc/van\n-        title: VAN\n       - local: model_doc/vit\n         title: Vision Transformer (ViT)\n-      - local: model_doc/vit_hybrid\n-        title: ViT Hybrid\n       - local: model_doc/vitdet\n         title: ViTDet\n       - local: model_doc/vit_mae\n@@ -930,8 +894,6 @@\n         title: Hubert\n       - local: model_doc/kyutai_speech_to_text\n         title: Kyutai Speech-To-Text\n-      - local: model_doc/mctct\n-        title: MCTCT\n       - local: model_doc/mimi\n         title: Mimi\n       - local: model_doc/mms\n@@ -958,8 +920,6 @@\n         title: SEW-D\n       - local: model_doc/speech_to_text\n         title: Speech2Text\n-      - local: model_doc/speech_to_text_2\n-        title: Speech2Text2\n       - local: model_doc/speecht5\n         title: SpeechT5\n       - local: model_doc/unispeech\n@@ -1188,8 +1148,6 @@\n         title: TAPAS\n       - local: model_doc/trocr\n         title: TrOCR\n-      - local: model_doc/tvlt\n-        title: TVLT\n       - local: model_doc/tvp\n         title: TVP\n       - local: model_doc/udop\n@@ -1216,8 +1174,6 @@\n     - sections:\n       - local: model_doc/decision_transformer\n         title: Decision Transformer\n-      - local: model_doc/trajectory_transformer\n-        title: Trajectory Transformer\n       title: Reinforcement learning models\n     - sections:\n       - local: model_doc/autoformer\n@@ -1233,10 +1189,6 @@\n       - local: model_doc/timesfm\n         title: TimesFM\n       title: Time series models\n-    - sections:\n-      - local: model_doc/graphormer\n-        title: Graphormer\n-      title: Graph models\n     title: Models\n   - sections:\n     - local: internal/modeling_utils"
        },
        {
            "sha": "159a5027f03fced74fb1b9f721b9c87dc34a83a5",
            "filename": "docs/source/en/model_doc/bort.md",
            "status": "removed",
            "additions": 0,
            "deletions": 60,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbort.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbort.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbort.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,60 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2020-10-20 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# BORT\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we do not accept any new PRs changing its code.\n-\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.\n-You can do so by running the following command: `pip install -U transformers==4.30.0`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The BORT model was proposed in [Optimal Subarchitecture Extraction for BERT](https://huggingface.co/papers/2010.10499) by\n-Adrian de Wynter and Daniel J. Perry. It is an optimal subset of architectural parameters for the BERT, which the\n-authors refer to as \"Bort\".\n-\n-The abstract from the paper is the following:\n-\n-*We extract an optimal subset of architectural parameters for the BERT architecture from Devlin et al. (2018) by\n-applying recent breakthroughs in algorithms for neural architecture search. This optimal subset, which we refer to as\n-\"Bort\", is demonstrably smaller, having an effective (that is, not counting the embedding layer) size of 5.5% the\n-original BERT-large architecture, and 16% of the net size. Bort is also able to be pretrained in 288 GPU hours, which\n-is 1.2% of the time required to pretrain the highest-performing BERT parametric architectural variant, RoBERTa-large\n-(Liu et al., 2019), and about 33% of that of the world-record, in GPU hours, required to train BERT-large on the same\n-hardware. It is also 7.9x faster on a CPU, as well as being better performing than other compressed variants of the\n-architecture, and some of the non-compressed variants: it obtains performance improvements of between 0.3% and 31%,\n-absolute, with respect to BERT-large, on multiple public natural language understanding (NLU) benchmarks.*\n-\n-This model was contributed by [stefan-it](https://huggingface.co/stefan-it). The original code can be found [here](https://github.com/alexa/bort/).\n-\n-## Usage tips\n-\n-- BORT's model architecture is based on BERT, refer to [BERT's documentation page](bert) for the\n-  model's API reference as well as usage examples.\n-- BORT uses the RoBERTa tokenizer instead of the BERT tokenizer, refer to [RoBERTa's documentation page](roberta) for the tokenizer's API reference as well as usage examples.\n-- BORT requires a specific fine-tuning algorithm, called [Agora](https://adewynter.github.io/notes/bort_algorithms_and_applications.html#fine-tuning-with-algebraic-topology) ,\n-  that is sadly not open-sourced yet. It would be very useful for the community, if someone tries to implement the\n-  algorithm to make BORT fine-tuning work."
        },
        {
            "sha": "0dda1c891791dabb4f9ab0323686298d351db387",
            "filename": "docs/source/en/model_doc/deta.md",
            "status": "removed",
            "additions": 0,
            "deletions": 78,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeta.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,78 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2022-12-12 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# DETA\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\n-You can do so by running the following command: `pip install -U transformers==4.40.2`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The DETA model was proposed in [NMS Strikes Back](https://huggingface.co/papers/2212.06137) by Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp KrÃ¤henbÃ¼hl.\n-DETA (short for Detection Transformers with Assignment) improves [Deformable DETR](deformable_detr) by replacing the one-to-one bipartite Hungarian matching loss\n-with one-to-many label assignments used in traditional detectors with non-maximum suppression (NMS). This leads to significant gains of up to 2.5 mAP.\n-\n-The abstract from the paper is the following:\n-\n-*Detection Transformer (DETR) directly transforms queries to unique objects by using one-to-one bipartite matching during training and enables end-to-end object detection. Recently, these models have surpassed traditional detectors on COCO with undeniable elegance. However, they differ from traditional detectors in multiple designs, including model architecture and training schedules, and thus the effectiveness of one-to-one matching is not fully understood. In this work, we conduct a strict comparison between the one-to-one Hungarian matching in DETRs and the one-to-many label assignments in traditional detectors with non-maximum supervision (NMS). Surprisingly, we observe one-to-many assignments with NMS consistently outperform standard one-to-one matching under the same setting, with a significant gain of up to 2.5 mAP. Our detector that trains Deformable-DETR with traditional IoU-based label assignment achieved 50.2 COCO mAP within 12 epochs (1x schedule) with ResNet50 backbone, outperforming all existing traditional or transformer-based detectors in this setting. On multiple datasets, schedules, and architectures, we consistently show bipartite matching is unnecessary for performant detection transformers. Furthermore, we attribute the success of detection transformers to their expressive transformer architecture.*\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/deta_architecture.jpg\"\n-alt=\"drawing\" width=\"600\"/>\n-\n-<small> DETA overview. Taken from the <a href=\"https://huggingface.co/papers/2212.06137\">original paper</a>. </small>\n-\n-This model was contributed by [nielsr](https://huggingface.co/nielsr).\n-The original code can be found [here](https://github.com/jozhang97/DETA).\n-\n-## Resources\n-\n-A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with DETA.\n-\n-- Demo notebooks for DETA can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETA).\n-- Scripts for finetuning [`DetaForObjectDetection`] with [`Trainer`] or [Accelerate](https://huggingface.co/docs/accelerate/index) can be found [here](https://github.com/huggingface/transformers/tree/main/examples/pytorch/object-detection).\n-- See also: [Object detection task guide](../tasks/object_detection).\n-\n-If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n-\n-## DetaConfig\n-\n-[[autodoc]] DetaConfig\n-\n-## DetaImageProcessor\n-\n-[[autodoc]] DetaImageProcessor\n-    - preprocess\n-    - post_process_object_detection\n-\n-## DetaModel\n-\n-[[autodoc]] DetaModel\n-    - forward\n-\n-## DetaForObjectDetection\n-\n-[[autodoc]] DetaForObjectDetection\n-    - forward"
        },
        {
            "sha": "f25460976d0f82ae9a975f4477f0494eb320c0e4",
            "filename": "docs/source/en/model_doc/efficientformer.md",
            "status": "removed",
            "additions": 0,
            "deletions": 85,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientformer.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,85 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2022-06-02 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# EfficientFormer\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\n-You can do so by running the following command: `pip install -U transformers==4.40.2`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The EfficientFormer model was proposed in [EfficientFormer: Vision Transformers at MobileNet Speed](https://huggingface.co/papers/2206.01191)\n-by Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.  EfficientFormer proposes a\n-dimension-consistent pure transformer that can be run on mobile devices for dense prediction tasks like image classification, object\n-detection and semantic segmentation.\n-\n-The abstract from the paper is the following:\n-\n-*Vision Transformers (ViT) have shown rapid progress in computer vision tasks, achieving promising results on various benchmarks.\n-However, due to the massive number of parameters and model design, e.g., attention mechanism, ViT-based models are generally\n-times slower than lightweight convolutional networks. Therefore, the deployment of ViT for real-time applications is particularly\n-challenging, especially on resource-constrained hardware such as mobile devices. Recent efforts try to reduce the computation\n-complexity of ViT through network architecture search or hybrid design with MobileNet block, yet the inference speed is still\n-unsatisfactory. This leads to an important question: can transformers run as fast as MobileNet while obtaining high performance?\n-To answer this, we first revisit the network architecture and operators used in ViT-based models and identify inefficient designs.\n-Then we introduce a dimension-consistent pure transformer (without MobileNet blocks) as a design paradigm.\n-Finally, we perform latency-driven slimming to get a series of final models dubbed EfficientFormer.\n-Extensive experiments show the superiority of EfficientFormer in performance and speed on mobile devices.\n-Our fastest model, EfficientFormer-L1, achieves 79.2% top-1 accuracy on ImageNet-1K with only 1.6 ms inference latency on\n-iPhone 12 (compiled with CoreML), which { runs as fast as MobileNetV2Ã—1.4 (1.6 ms, 74.7% top-1),} and our largest model,\n-EfficientFormer-L7, obtains 83.3% accuracy with only 7.0 ms latency. Our work proves that properly designed transformers can\n-reach extremely low latency on mobile devices while maintaining high performance.*\n-\n-This model was contributed by [novice03](https://huggingface.co/novice03) and [Bearnardd](https://huggingface.co/Bearnardd).\n-The original code can be found [here](https://github.com/snap-research/EfficientFormer).\n-\n-## Documentation resources\n-\n-- [Image classification task guide](../tasks/image_classification)\n-\n-## EfficientFormerConfig\n-\n-[[autodoc]] EfficientFormerConfig\n-\n-## EfficientFormerImageProcessor\n-\n-[[autodoc]] EfficientFormerImageProcessor\n-    - preprocess\n-\n-## EfficientFormerModel\n-\n-[[autodoc]] EfficientFormerModel\n-    - forward\n-\n-## EfficientFormerForImageClassification\n-\n-[[autodoc]] EfficientFormerForImageClassification\n-    - forward\n-\n-## EfficientFormerForImageClassificationWithTeacher\n-\n-[[autodoc]] EfficientFormerForImageClassificationWithTeacher\n-    - forward"
        },
        {
            "sha": "e044614e76445fb02c35f2342475a7a1f4619bae",
            "filename": "docs/source/en/model_doc/ernie_m.md",
            "status": "removed",
            "additions": 0,
            "deletions": 97,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie_m.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie_m.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie_m.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,97 +0,0 @@\n-<!--Copyright 2023 The HuggingFace and Baidu Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2020-12-31 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# ErnieM\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\n-You can do so by running the following command: `pip install -U transformers==4.40.2`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The ErnieM model was proposed in [ERNIE-M: Enhanced Multilingual Representation by Aligning\n-Cross-lingual Semantics with Monolingual Corpora](https://huggingface.co/papers/2012.15674)  by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun,\n-Hao Tian, Hua Wu, Haifeng Wang.\n-\n-The abstract from the paper is the following:\n-\n-*Recent studies have demonstrated that pre-trained cross-lingual models achieve impressive performance in downstream cross-lingual tasks. This improvement benefits from learning a large amount of monolingual and parallel corpora. Although it is generally acknowledged that parallel corpora are critical for improving the model performance, existing methods are often constrained by the size of parallel corpora, especially for lowresource languages. In this paper, we propose ERNIE-M, a new training method that encourages the model to align the representation of multiple languages with monolingual corpora, to overcome the constraint that the parallel corpus size places on the model performance. Our key insight is to integrate back-translation into the pre-training process. We generate pseudo-parallel sentence pairs on a monolingual corpus to enable the learning of semantic alignments between different languages, thereby enhancing the semantic modeling of cross-lingual models. Experimental results show that ERNIE-M outperforms existing cross-lingual models and delivers new state-of-the-art results in various cross-lingual downstream tasks.*\n-This model was contributed by [Susnato Dhar](https://huggingface.co/susnato). The original code can be found [here](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/paddlenlp/transformers/ernie_m).\n-\n-## Usage tips\n-\n-- Ernie-M is a BERT-like model so it is a stacked Transformer Encoder.\n-- Instead of using MaskedLM for pretraining (like BERT) the authors used two novel techniques: `Cross-attention Masked Language Modeling` and `Back-translation Masked Language Modeling`. For now these two LMHead objectives are not implemented here.\n-- It is a multilingual language model.\n-- Next Sentence Prediction was not used in pretraining process.\n-\n-## Resources\n-\n-- [Text classification task guide](../tasks/sequence_classification)\n-- [Token classification task guide](../tasks/token_classification)\n-- [Question answering task guide](../tasks/question_answering)\n-- [Multiple choice task guide](../tasks/multiple_choice)\n-\n-## ErnieMConfig\n-\n-[[autodoc]] ErnieMConfig\n-\n-## ErnieMTokenizer\n-\n-[[autodoc]] ErnieMTokenizer\n-    - build_inputs_with_special_tokens\n-    - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n-    - save_vocabulary\n-\n-## ErnieMModel\n-\n-[[autodoc]] ErnieMModel\n-    - forward\n-\n-## ErnieMForSequenceClassification\n-\n-[[autodoc]] ErnieMForSequenceClassification\n-    - forward\n-\n-## ErnieMForMultipleChoice\n-\n-[[autodoc]] ErnieMForMultipleChoice\n-    - forward\n-\n-## ErnieMForTokenClassification\n-\n-[[autodoc]] ErnieMForTokenClassification\n-    - forward\n-\n-## ErnieMForQuestionAnswering\n-\n-[[autodoc]] ErnieMForQuestionAnswering\n-    - forward\n-\n-## ErnieMForInformationExtraction\n-\n-[[autodoc]] ErnieMForInformationExtraction\n-    - forward"
        },
        {
            "sha": "420665d3e7e8edcf119ffdacd83d06045b6d9f8c",
            "filename": "docs/source/en/model_doc/gptsan-japanese.md",
            "status": "removed",
            "additions": 0,
            "deletions": 145,
            "changes": 145,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,145 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2023-02-07 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# GPTSAN-japanese\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\n-You can do so by running the following command: `pip install -U transformers==4.40.2`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The [GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese) model was released in the repository by Toshiyuki Sakamoto (tanreinama).\n-\n-GPTSAN is a Japanese language model using Switch Transformer. It has the same structure as the model introduced as Prefix LM\n-in the T5 paper, and support both Text Generation and Masked Language Modeling tasks. These basic tasks similarly can\n-fine-tune for translation or summarization.\n-\n-### Usage example\n-\n-The `generate()` method can be used to generate text using GPTSAN-Japanese model.\n-\n-```python\n->>> from transformers import AutoModel, AutoTokenizer\n-from accelerate import Accelerator\n->>> import torch\n-\n->>> device = Accelerator().device\n->>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n->>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").to(device)\n->>> x_tok = tokenizer(\"ã¯ã€\", prefix_text=\"ç¹”ç”°ä¿¡é•·\", return_tensors=\"pt\")\n->>> torch.manual_seed(0)\n->>> gen_tok = model.generate(x_tok.input_ids.to(model.device), token_type_ids=x_tok.token_type_ids.to(model.device), max_new_tokens=20)\n->>> tokenizer.decode(gen_tok[0])\n-'ç¹”ç”°ä¿¡é•·ã¯ã€2004å¹´ã«ã€Žæˆ¦å›½BASARAã€ã®ãŸã‚ã«ã€è±Šè‡£ç§€å‰'\n-```\n-\n-## GPTSAN Features\n-\n-GPTSAN has some unique features. It has a model structure of Prefix-LM. It works as a shifted Masked Language Model for Prefix Input tokens. Un-prefixed inputs behave like normal generative models.\n-The Spout vector is a GPTSAN specific input. Spout is pre-trained with random inputs, but you can specify a class of text or an arbitrary vector during fine-tuning. This allows you to indicate the tendency of the generated text.\n-GPTSAN has a sparse Feed Forward based on Switch-Transformer. You can also add other layers and train them partially. See the original GPTSAN repository for details.\n-\n-### Prefix-LM Model\n-\n-GPTSAN has the structure of the model named Prefix-LM in the `T5` paper. (The original GPTSAN repository calls it `hybrid`)\n-In GPTSAN, the `Prefix` part of Prefix-LM, that is, the input position that can be referenced by both tokens, can be specified with any length.\n-Arbitrary lengths can also be specified differently for each batch.\n-This length applies to the text entered in `prefix_text` for the tokenizer.\n-The tokenizer returns the mask of the `Prefix` part of Prefix-LM as `token_type_ids`.\n-The model treats the part where `token_type_ids` is 1 as a `Prefix` part, that is, the input can refer to both tokens before and after.\n-\n-## Usage tips\n-\n-Specifying the Prefix part is done with a mask passed to self-attention.\n-When token_type_ids=None or all zero, it is equivalent to regular causal mask\n-\n-for example:\n-\n->>> x_token = tokenizer(\"ï½±ï½²ï½³ï½´\")\n-\n-```text\n-input_ids:      | SOT | SEG | ï½± | ï½² | ï½³ | ï½´ |\n-token_type_ids: | 1   | 0   | 0 | 0 | 0 | 0 |\n-prefix_lm_mask:\n-SOT | 1 0 0 0 0 0 |\n-SEG | 1 1 0 0 0 0 |\n-ï½±   | 1 1 1 0 0 0 |\n-ï½²   | 1 1 1 1 0 0 |\n-ï½³   | 1 1 1 1 1 0 |\n-ï½´   | 1 1 1 1 1 1 |\n-```\n-\n->>> x_token = tokenizer(\"\", prefix_text=\"ï½±ï½²ï½³ï½´\")\n-\n-```text\n-input_ids:      | SOT | ï½± | ï½² | ï½³ | ï½´ | SEG |\n-token_type_ids: | 1   | 1 | 1 | 1 | 1 | 0  |\n-prefix_lm_mask:\n-SOT | 1 1 1 1 1 0 |\n-ï½±   | 1 1 1 1 1 0 |\n-ï½²   | 1 1 1 1 1 0 |\n-ï½³   | 1 1 1 1 1 0 |\n-ï½´   | 1 1 1 1 1 0 |\n-SEG | 1 1 1 1 1 1 |\n-```\n-\n->>> x_token = tokenizer(\"ï½³ï½´\", prefix_text=\"ï½±ï½²\")\n-\n-```text\n-input_ids:      | SOT | ï½± | ï½² | SEG | ï½³ | ï½´ |\n-token_type_ids: | 1   | 1 | 1 | 0   | 0 | 0 |\n-prefix_lm_mask:\n-SOT | 1 1 1 0 0 0 |\n-ï½±   | 1 1 1 0 0 0 |\n-ï½²   | 1 1 1 0 0 0 |\n-SEG | 1 1 1 1 0 0 |\n-ï½³   | 1 1 1 1 1 0 |\n-ï½´   | 1 1 1 1 1 1 |\n-```\n-\n-### Spout Vector\n-\n-A Spout Vector is a special vector for controlling text generation.\n-This vector is treated as the first embedding in self-attention to bring extraneous attention to the generated tokens.\n-In the pre-trained model published from `Tanrei/GPTSAN-japanese`, the Spout Vector is a 128-dimensional vector that passes through 8 fully connected layers in the model and is projected into the space acting as external attention.\n-The Spout Vector projected by the fully connected layer is split to be passed to all self-attentions.\n-\n-## GPTSanJapaneseConfig\n-\n-[[autodoc]] GPTSanJapaneseConfig\n-\n-## GPTSanJapaneseTokenizer\n-\n-[[autodoc]] GPTSanJapaneseTokenizer\n-\n-## GPTSanJapaneseModel\n-\n-[[autodoc]] GPTSanJapaneseModel\n-\n-## GPTSanJapaneseForConditionalGeneration\n-\n-[[autodoc]] GPTSanJapaneseForConditionalGeneration\n-    - forward"
        },
        {
            "sha": "851f52df09f449afd203adc9c13abcdc827f7d6b",
            "filename": "docs/source/en/model_doc/graphormer.md",
            "status": "removed",
            "additions": 0,
            "deletions": 60,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgraphormer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgraphormer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgraphormer.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,60 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team and Microsoft. All rights reserved.\n-\n-Licensed under the MIT License; you may not use this file except in compliance with\n-the License.\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2021-06-09 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# Graphormer\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\n-You can do so by running the following command: `pip install -U transformers==4.40.2`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The Graphormer model was proposed in [Do Transformers Really Perform Bad for Graph Representation?](https://huggingface.co/papers/2106.05234)  by\n-Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen and Tie-Yan Liu. It is a Graph Transformer model, modified to allow computations on graphs instead of text sequences by generating embeddings and features of interest during preprocessing and collation, then using a modified attention.\n-\n-The abstract from the paper is the following:\n-\n-*The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.*\n-\n-This model was contributed by [clefourrier](https://huggingface.co/clefourrier). The original code can be found [here](https://github.com/microsoft/Graphormer).\n-\n-## Usage tips\n-\n-This model will not work well on large graphs (more than 100 nodes/edges), as it will make the memory explode.\n-You can reduce the batch size, increase your RAM, or decrease the `UNREACHABLE_NODE_DISTANCE` parameter in algos_graphormer.pyx, but it will be hard to go above 700 nodes/edges.\n-\n-This model does not use a tokenizer, but instead a special collator during training.\n-\n-## GraphormerConfig\n-\n-[[autodoc]] GraphormerConfig\n-\n-## GraphormerModel\n-\n-[[autodoc]] GraphormerModel\n-    - forward\n-\n-## GraphormerForGraphClassification\n-\n-[[autodoc]] GraphormerForGraphClassification\n-    - forward"
        },
        {
            "sha": "11acadb547d45ff9a42565f27a7ad39786d8a549",
            "filename": "docs/source/en/model_doc/jukebox.md",
            "status": "removed",
            "additions": 0,
            "deletions": 99,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fjukebox.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fjukebox.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjukebox.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,99 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2020-04-30 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# Jukebox\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\n-You can do so by running the following command: `pip install -U transformers==4.40.2`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The Jukebox model was proposed in [Jukebox: A generative model for music](https://huggingface.co/papers/2005.00341)\n-by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford,\n-Ilya Sutskever. It introduces a generative music model which can produce minute long samples that can be conditioned on\n-an artist, genres and lyrics.\n-\n-The abstract from the paper is the following:\n-\n-*We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code.*\n-\n-As shown on the following figure, Jukebox is made of 3 `priors` which are decoder only models. They follow the architecture described in [Generating Long Sequences with Sparse Transformers](https://huggingface.co/papers/1904.10509), modified to support longer context length.\n-First, a autoencoder is used to encode the text lyrics. Next, the first (also called `top_prior`) prior attends to the last hidden states extracted from the lyrics encoder. The priors are linked to the previous priors respectively via an `AudioConditioner` module. The`AudioConditioner` upsamples the outputs of the previous prior to raw tokens at a certain audio frame per second resolution.\n-The metadata such as *artist, genre and timing* are passed to each prior, in the form of a start token and positional embedding for the timing data.  The hidden states are mapped to the closest codebook vector from the VQVAE in order to convert them to raw audio.\n-\n-![JukeboxModel](https://gist.githubusercontent.com/ArthurZucker/92c1acaae62ebf1b6a951710bdd8b6af/raw/c9c517bf4eff61393f6c7dec9366ef02bdd059a3/jukebox.svg)\n-\n-This model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ).\n-The original code can be found [here](https://github.com/openai/jukebox).\n-\n-## Usage tips\n-\n-- This model only supports inference. This is for a few reasons, mostly because it requires a crazy amount of memory to train. Feel free to open a PR and add what's missing to have a full integration with the hugging face trainer!\n-- This model is very slow, and takes 8h to generate a minute long audio using the 5b top prior on a V100 GPU. In order automaticallay handle the device on which the model should execute, use `accelerate`.\n-- Contrary to the paper, the order of the priors goes from `0` to `1` as it felt more intuitive : we sample starting from `0`.\n-- Primed sampling (conditioning the sampling on raw audio) requires more memory than ancestral sampling and should be used with `fp16` set to `True`.\n-\n-This model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ).\n-The original code can be found [here](https://github.com/openai/jukebox).\n-\n-## JukeboxConfig\n-\n-[[autodoc]] JukeboxConfig\n-\n-## JukeboxPriorConfig\n-\n-[[autodoc]] JukeboxPriorConfig\n-\n-## JukeboxVQVAEConfig\n-\n-[[autodoc]] JukeboxVQVAEConfig\n-\n-## JukeboxTokenizer\n-\n-[[autodoc]] JukeboxTokenizer\n-    - save_vocabulary\n-\n-## JukeboxModel\n-\n-[[autodoc]] JukeboxModel\n-    - ancestral_sample\n-    - primed_sample\n-    - continue_sample\n-    - upsample\n-    - _sample\n-\n-## JukeboxPrior\n-\n-[[autodoc]] JukeboxPrior\n-    - sample\n-    - forward\n-\n-## JukeboxVQVAE\n-\n-[[autodoc]] JukeboxVQVAE\n-    - forward\n-    - encode\n-    - decode"
        },
        {
            "sha": "c766b1a825d6c82243a71dd38f159cb109da8252",
            "filename": "docs/source/en/model_doc/mctct.md",
            "status": "removed",
            "additions": 0,
            "deletions": 84,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmctct.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmctct.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmctct.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,84 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2021-10-30 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# M-CTC-T\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, so we won't accept any new PRs changing its code.\n-\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.\n-You can do so by running the following command: `pip install -U transformers==4.30.0`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The M-CTC-T model was proposed in [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://huggingface.co/papers/2111.00161) by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert. The model is a 1B-param transformer encoder, with a CTC head over 8065 character labels and a language identification head over 60 language ID labels. It is trained on Common Voice (version 6.1, December 2020 release) and VoxPopuli. After training on Common Voice and VoxPopuli, the model is trained on Common Voice only. The labels are unnormalized character-level transcripts (punctuation and capitalization are not removed). The model takes as input Mel filterbank features from a 16Khz audio signal.\n-\n-The abstract from the paper is the following:\n-\n-*Semi-supervised learning through pseudo-labeling has become a staple of state-of-the-art monolingual\n-speech recognition systems. In this work, we extend pseudo-labeling to massively multilingual speech\n-recognition with 60 languages. We propose a simple pseudo-labeling recipe that works well even\n-with low-resource languages: train a supervised multilingual model, fine-tune it with semi-supervised\n-learning on a target language, generate pseudo-labels for that language, and train a final model using\n-pseudo-labels for all languages, either from scratch or by fine-tuning. Experiments on the labeled\n-Common Voice and unlabeled VoxPopuli datasets show that our recipe can yield a model with better\n-performance for many languages that also transfers well to LibriSpeech.*\n-\n-This model was contributed by [cwkeam](https://huggingface.co/cwkeam). The original code can be found [here](https://github.com/flashlight/wav2letter/tree/main/recipes/mling_pl).\n-\n-## Usage tips\n-\n-The PyTorch version of this model is only available in torch 1.9 and higher.\n-\n-## Resources\n-\n-- [Automatic speech recognition task guide](../tasks/asr)\n-\n-## MCTCTConfig\n-\n-[[autodoc]] MCTCTConfig\n-\n-## MCTCTFeatureExtractor\n-\n-[[autodoc]] MCTCTFeatureExtractor\n-    - __call__\n-\n-## MCTCTProcessor\n-\n-[[autodoc]] MCTCTProcessor\n-    - __call__\n-    - from_pretrained\n-    - save_pretrained\n-    - batch_decode\n-    - decode\n-\n-## MCTCTModel\n-\n-[[autodoc]] MCTCTModel\n-    - forward\n-\n-## MCTCTForCTC\n-\n-[[autodoc]] MCTCTForCTC\n-    - forward"
        },
        {
            "sha": "4f1b5e1befb9e0634c1cc7b003365b3e41602f8c",
            "filename": "docs/source/en/model_doc/mega.md",
            "status": "removed",
            "additions": 0,
            "deletions": 94,
            "changes": 94,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmega.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmega.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmega.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,94 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2022-09-21 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# MEGA\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\n-You can do so by running the following command: `pip install -U transformers==4.40.2`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The MEGA model was proposed in [Mega: Moving Average Equipped Gated Attention](https://huggingface.co/papers/2209.10655) by Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.\n-MEGA proposes a new approach to self-attention with each encoder layer having a multi-headed exponential moving average in addition to a single head of standard dot-product attention, giving the attention mechanism\n-stronger positional biases. This allows MEGA to perform competitively to Transformers on standard benchmarks including LRA\n-while also having significantly fewer parameters. MEGA's compute efficiency allows it to scale to very long sequences, making it an\n-attractive option for long-document NLP tasks.\n-\n-The abstract from the paper is the following:\n-\n- *The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.*\n-\n-This model was contributed by [mnaylor](https://huggingface.co/mnaylor).\n-The original code can be found [here](https://github.com/facebookresearch/mega).\n-\n-## Usage tips\n-\n-- MEGA can perform quite well with relatively few parameters. See Appendix D in the MEGA paper for examples of architectural specs which perform well in various settings. If using MEGA as a decoder, be sure to set `bidirectional=False` to avoid errors with default bidirectional.\n-- Mega-chunk is a variant of mega that reduces time and spaces complexity from quadratic to linear. Utilize chunking with MegaConfig.use_chunking and control chunk size with MegaConfig.chunk_size\n-\n-## Implementation Notes\n-\n-- The original implementation of MEGA had an inconsistent expectation of attention masks for padding and causal self-attention between the softmax attention and Laplace/squared ReLU method. This implementation addresses that inconsistency.\n-- The original implementation did not include token type embeddings; this implementation adds support for these, with the option controlled by MegaConfig.add_token_type_embeddings\n-\n-## MegaConfig\n-\n-[[autodoc]] MegaConfig\n-\n-## MegaModel\n-\n-[[autodoc]] MegaModel\n-    - forward\n-\n-## MegaForCausalLM\n-\n-[[autodoc]] MegaForCausalLM\n-    - forward\n-\n-## MegaForMaskedLM\n-\n-[[autodoc]] MegaForMaskedLM\n-    - forward\n-\n-## MegaForSequenceClassification\n-\n-[[autodoc]] MegaForSequenceClassification\n-    - forward\n-\n-## MegaForMultipleChoice\n-\n-[[autodoc]] MegaForMultipleChoice\n-    - forward\n-\n-## MegaForTokenClassification\n-\n-[[autodoc]] MegaForTokenClassification\n-    - forward\n-\n-## MegaForQuestionAnswering\n-\n-[[autodoc]] MegaForQuestionAnswering\n-    - forward"
        },
        {
            "sha": "d87633d4346eb6f0119dc0bbe51216c03ce62f74",
            "filename": "docs/source/en/model_doc/nat.md",
            "status": "removed",
            "additions": 0,
            "deletions": 101,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fnat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fnat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnat.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,101 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2022-04-14 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# Neighborhood Attention Transformer\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\n-You can do so by running the following command: `pip install -U transformers==4.40.2`.\n-\n-</Tip>\n-\n-## Overview\n-\n-NAT was proposed in [Neighborhood Attention Transformer](https://huggingface.co/papers/2204.07143)\n-by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.\n-\n-It is a hierarchical vision transformer based on Neighborhood Attention, a sliding-window self attention pattern.\n-\n-The abstract from the paper is the following:\n-\n-*We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision.\n-NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a\n-linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's\n-receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike\n-Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package\n-with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less\n-memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA\n-that boosts image classification and downstream vision performance. Experimental results on NAT are competitive;\n-NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on MS-COCO and 48.4% mIoU on ADE20K, which is 1.9%\n-ImageNet accuracy, 1.0% COCO mAP, and 2.6% ADE20K mIoU improvement over a Swin model with similar size.*\n-\n-<img\n-src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/neighborhood-attention-pattern.jpg\"\n-alt=\"drawing\" width=\"600\"/>\n-\n-<small> Neighborhood Attention compared to other attention patterns.\n-Taken from the <a href=\"https://huggingface.co/papers/2204.07143\">original paper</a>.</small>\n-\n-This model was contributed by [Ali Hassani](https://huggingface.co/alihassanijr).\n-The original code can be found [here](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer).\n-\n-## Usage tips\n-\n-- One can use the [`AutoImageProcessor`] API to prepare images for the model.\n-- NAT can be used as a *backbone*. When `output_hidden_states = True`,\n-it will output both `hidden_states` and `reshaped_hidden_states`.\n-The `reshaped_hidden_states` have a shape of `(batch, num_channels, height, width)` rather than\n-`(batch_size, height, width, num_channels)`.\n-\n-Notes:\n-\n-- NAT depends on [NATTEN](https://github.com/SHI-Labs/NATTEN/)'s implementation of Neighborhood Attention.\n-You can install it with pre-built wheels for Linux by referring to [shi-labs.com/natten](https://shi-labs.com/natten),\n-or build on your system by running `pip install natten`.\n-Note that the latter will likely take time to compile. NATTEN does not support Windows devices yet.\n-- Patch size of 4 is only supported at the moment.\n-\n-## Resources\n-\n-A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with NAT.\n-\n-<PipelineTag pipeline=\"image-classification\"/>\n-\n-- [`NatForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n-- See also: [Image classification task guide](../tasks/image_classification)\n-\n-If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n-\n-## NatConfig\n-\n-[[autodoc]] NatConfig\n-\n-## NatModel\n-\n-[[autodoc]] NatModel\n-    - forward\n-\n-## NatForImageClassification\n-\n-[[autodoc]] NatForImageClassification\n-    - forward"
        },
        {
            "sha": "37687fc25df508e7f8e058ceb5b2968a69693e3c",
            "filename": "docs/source/en/model_doc/nezha.md",
            "status": "removed",
            "additions": 0,
            "deletions": 101,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fnezha.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fnezha.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnezha.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,101 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2019-08-31 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# Nezha\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\n-You can do so by running the following command: `pip install -U transformers==4.40.2`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The Nezha model was proposed in [NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://huggingface.co/papers/1909.00204) by Junqiu Wei et al.\n-\n-The abstract from the paper is the following:\n-\n-*The pre-trained language models have achieved great successes in various natural language understanding (NLU) tasks\n-due to its capacity to capture the deep contextualized information in text by pre-training on large-scale corpora.\n-In this technical report, we present our practice of pre-training language models named NEZHA (NEural contextualiZed\n-representation for CHinese lAnguage understanding) on Chinese corpora and finetuning for the Chinese NLU tasks.\n-The current version of NEZHA is based on BERT with a collection of proven improvements, which include Functional\n-Relative Positional Encoding as an effective positional encoding scheme, Whole Word Masking strategy,\n-Mixed Precision Training and the LAMB Optimizer in training the models. The experimental results show that NEZHA\n-achieves the state-of-the-art performances when finetuned on several representative Chinese tasks, including\n-named entity recognition (People's Daily NER), sentence matching (LCQMC), Chinese sentiment classification (ChnSenti)\n-and natural language inference (XNLI).*\n-\n-This model was contributed by [sijunhe](https://huggingface.co/sijunhe). The original code can be found [here](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-PyTorch).\n-\n-## Resources\n-\n-- [Text classification task guide](../tasks/sequence_classification)\n-- [Token classification task guide](../tasks/token_classification)\n-- [Question answering task guide](../tasks/question_answering)\n-- [Masked language modeling task guide](../tasks/masked_language_modeling)\n-- [Multiple choice task guide](../tasks/multiple_choice)\n-\n-## NezhaConfig\n-\n-[[autodoc]] NezhaConfig\n-\n-## NezhaModel\n-\n-[[autodoc]] NezhaModel\n-    - forward\n-\n-## NezhaForPreTraining\n-\n-[[autodoc]] NezhaForPreTraining\n-    - forward\n-\n-## NezhaForMaskedLM\n-\n-[[autodoc]] NezhaForMaskedLM\n-    - forward\n-\n-## NezhaForNextSentencePrediction\n-\n-[[autodoc]] NezhaForNextSentencePrediction\n-    - forward\n-\n-## NezhaForSequenceClassification\n-\n-[[autodoc]] NezhaForSequenceClassification\n-    - forward\n-\n-## NezhaForMultipleChoice\n-\n-[[autodoc]] NezhaForMultipleChoice\n-    - forward\n-\n-## NezhaForTokenClassification\n-\n-[[autodoc]] NezhaForTokenClassification\n-    - forward\n-\n-## NezhaForQuestionAnswering\n-\n-[[autodoc]] NezhaForQuestionAnswering\n-    - forward"
        },
        {
            "sha": "38954cd315d0c76fc4d1edd6d4080580eb25e8ba",
            "filename": "docs/source/en/model_doc/open-llama.md",
            "status": "removed",
            "additions": 0,
            "deletions": 66,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fopen-llama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fopen-llama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fopen-llama.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,66 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2023-04-16 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# Open-Llama\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.31.0.\n-You can do so by running the following command: `pip install -U transformers==4.31.0`.\n-\n-</Tip>\n-\n-<Tip warning={true}>\n-\n-This model differs from the [OpenLLaMA models](https://huggingface.co/models?search=openllama) on the Hugging Face Hub, which primarily use the [LLaMA](llama) architecture.\n-\n-</Tip>\n-\n-## Overview\n-\n-The Open-Llama model was proposed in the open source Open-Llama project by community developer s-JoL.\n-\n-The model is mainly based on LLaMA with some modifications, incorporating memory-efficient attention from Xformers, stable embedding from Bloom, and shared input-output embedding from PaLM.\n-And the model is pre-trained on both Chinese and English, which gives it better performance on Chinese language tasks.\n-\n-This model was contributed by [s-JoL](https://huggingface.co/s-JoL).\n-The original code was released on GitHub by [s-JoL](https://github.com/s-JoL), but is now removed.\n-\n-## OpenLlamaConfig\n-\n-[[autodoc]] OpenLlamaConfig\n-\n-## OpenLlamaModel\n-\n-[[autodoc]] OpenLlamaModel\n-    - forward\n-\n-## OpenLlamaForCausalLM\n-\n-[[autodoc]] OpenLlamaForCausalLM\n-    - forward\n-\n-## OpenLlamaForSequenceClassification\n-\n-[[autodoc]] OpenLlamaForSequenceClassification\n-    - forward"
        },
        {
            "sha": "4dfb7c10bcf953c9d8420938bb3e007078521646",
            "filename": "docs/source/en/model_doc/qdqbert.md",
            "status": "removed",
            "additions": 0,
            "deletions": 183,
            "changes": 183,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,183 +0,0 @@\n-<!--Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2020-04-20 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# QDQBERT\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\n-You can do so by running the following command: `pip install -U transformers==4.40.2`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The QDQBERT model can be referenced in [Integer Quantization for Deep Learning Inference: Principles and Empirical\n-Evaluation](https://huggingface.co/papers/2004.09602) by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius\n-Micikevicius.\n-\n-The abstract from the paper is the following:\n-\n-*Quantization techniques can reduce the size of Deep Neural Networks and improve inference latency and throughput by\n-taking advantage of high throughput integer instructions. In this paper we review the mathematical aspects of\n-quantization parameters and evaluate their choices on a wide range of neural network models for different application\n-domains, including vision, speech, and language. We focus on quantization techniques that are amenable to acceleration\n-by processors with high-throughput integer math pipelines. We also present a workflow for 8-bit quantization that is\n-able to maintain accuracy within 1% of the floating-point baseline on all networks studied, including models that are\n-more difficult to quantize, such as MobileNets and BERT-large.*\n-\n-This model was contributed by [shangz](https://huggingface.co/shangz).\n-\n-## Usage tips\n-\n-- QDQBERT model adds fake quantization operations (pair of QuantizeLinear/DequantizeLinear ops) to (i) linear layer\n-  inputs and weights, (ii) matmul inputs, (iii) residual add inputs, in BERT model.\n-- QDQBERT requires the dependency of [Pytorch Quantization Toolkit](https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization). To install `pip install pytorch-quantization --extra-index-url https://pypi.ngc.nvidia.com`\n-- QDQBERT model can be loaded from any checkpoint of HuggingFace BERT model (for example *google-bert/bert-base-uncased*), and\n-  perform Quantization Aware Training/Post Training Quantization.\n-- A complete example of using QDQBERT model to perform Quatization Aware Training and Post Training Quantization for\n-  SQUAD task can be found at https://github.com/huggingface/transformers-research-projects/tree/main/quantization-qdqbert.\n-\n-### Set default quantizers\n-\n-QDQBERT model adds fake quantization operations (pair of QuantizeLinear/DequantizeLinear ops) to BERT by\n-`TensorQuantizer` in [Pytorch Quantization Toolkit](https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization). `TensorQuantizer` is the module\n-for quantizing tensors, with `QuantDescriptor` defining how the tensor should be quantized. Refer to [Pytorch\n-Quantization Toolkit userguide](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/userguide.html) for more details.\n-\n-Before creating QDQBERT model, one has to set the default `QuantDescriptor` defining default tensor quantizers.\n-\n-Example:\n-\n-```python\n->>> import pytorch_quantization.nn as quant_nn\n->>> from pytorch_quantization.tensor_quant import QuantDescriptor\n-\n->>> # The default tensor quantizer is set to use Max calibration method\n->>> input_desc = QuantDescriptor(num_bits=8, calib_method=\"max\")\n->>> # The default tensor quantizer is set to be per-channel quantization for weights\n->>> weight_desc = QuantDescriptor(num_bits=8, axis=((0,)))\n->>> quant_nn.QuantLinear.set_default_quant_desc_input(input_desc)\n->>> quant_nn.QuantLinear.set_default_quant_desc_weight(weight_desc)\n-```\n-\n-### Calibration\n-\n-Calibration is the terminology of passing data samples to the quantizer and deciding the best scaling factors for\n-tensors. After setting up the tensor quantizers, one can use the following example to calibrate the model:\n-\n-```python\n->>> # Find the TensorQuantizer and enable calibration\n->>> for name, module in model.named_modules():\n-...     if name.endswith(\"_input_quantizer\"):\n-...         module.enable_calib()\n-...         module.disable_quant()  # Use full precision data to calibrate\n-\n->>> # Feeding data samples\n->>> model(x)\n->>> # ...\n-\n->>> # Finalize calibration\n->>> for name, module in model.named_modules():\n-...     if name.endswith(\"_input_quantizer\"):\n-...         module.load_calib_amax()\n-...         module.enable_quant()\n-\n->>> # If running on accelerator, it needs to call `.to(xx)` again because new tensors will be created by calibration process\n->>> from accelerate import Accelerator\n->>> device = Accelerator().device\n->>> model.to(device)\n-\n->>> # Keep running the quantized model\n->>> # ...\n-```\n-\n-### Export to ONNX\n-\n-The goal of exporting to ONNX is to deploy inference by [TensorRT](https://developer.nvidia.com/tensorrt). Fake\n-quantization will be broken into a pair of QuantizeLinear/DequantizeLinear ONNX ops. After setting static member of\n-TensorQuantizer to use Pytorch's own fake quantization functions, fake quantized model can be exported to ONNX, follow\n-the instructions in [torch.onnx](https://pytorch.org/docs/stable/onnx.html). Example:\n-\n-```python\n->>> from pytorch_quantization.nn import TensorQuantizer\n-\n->>> TensorQuantizer.use_fb_fake_quant = True\n-\n->>> # Load the calibrated model\n->>> ...\n->>> # ONNX export\n->>> torch.onnx.export(...)\n-```\n-\n-## Resources\n-\n-- [Text classification task guide](../tasks/sequence_classification)\n-- [Token classification task guide](../tasks/token_classification)\n-- [Question answering task guide](../tasks/question_answering)\n-- [Causal language modeling task guide](../tasks/language_modeling)\n-- [Masked language modeling task guide](../tasks/masked_language_modeling)\n-- [Multiple choice task guide](../tasks/multiple_choice)\n-\n-## QDQBertConfig\n-\n-[[autodoc]] QDQBertConfig\n-\n-## QDQBertModel\n-\n-[[autodoc]] QDQBertModel\n-    - forward\n-\n-## QDQBertLMHeadModel\n-\n-[[autodoc]] QDQBertLMHeadModel\n-    - forward\n-\n-## QDQBertForMaskedLM\n-\n-[[autodoc]] QDQBertForMaskedLM\n-    - forward\n-\n-## QDQBertForSequenceClassification\n-\n-[[autodoc]] QDQBertForSequenceClassification\n-    - forward\n-\n-## QDQBertForNextSentencePrediction\n-\n-[[autodoc]] QDQBertForNextSentencePrediction\n-    - forward\n-\n-## QDQBertForMultipleChoice\n-\n-[[autodoc]] QDQBertForMultipleChoice\n-    - forward\n-\n-## QDQBertForTokenClassification\n-\n-[[autodoc]] QDQBertForTokenClassification\n-    - forward\n-\n-## QDQBertForQuestionAnswering\n-\n-[[autodoc]] QDQBertForQuestionAnswering\n-    - forward"
        },
        {
            "sha": "da3d1c140f4c7ed068f383be4ea6f59de492eed5",
            "filename": "docs/source/en/model_doc/realm.md",
            "status": "removed",
            "additions": 0,
            "deletions": 102,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Frealm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Frealm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frealm.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,102 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2020-02-10 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# REALM\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\n-You can do so by running the following command: `pip install -U transformers==4.40.2`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The REALM model was proposed in [REALM: Retrieval-Augmented Language Model Pre-Training](https://huggingface.co/papers/2002.08909) by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang. It's a\n-retrieval-augmented language model that firstly retrieves documents from a textual knowledge corpus and then\n-utilizes retrieved documents to process question answering tasks.\n-\n-The abstract from the paper is the following:\n-\n-*Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks\n-such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network,\n-requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we\n-augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend\n-over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the\n-first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language\n-modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We\n-demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the\n-challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both\n-explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous\n-methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as\n-interpretability and modularity.*\n-\n-This model was contributed by [qqaatw](https://huggingface.co/qqaatw). The original code can be found\n-[here](https://github.com/google-research/language/tree/master/language/realm).\n-\n-## RealmConfig\n-\n-[[autodoc]] RealmConfig\n-\n-## RealmTokenizer\n-\n-[[autodoc]] RealmTokenizer\n-    - build_inputs_with_special_tokens\n-    - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n-    - save_vocabulary\n-    - batch_encode_candidates\n-\n-## RealmTokenizerFast\n-\n-[[autodoc]] RealmTokenizerFast\n-    - batch_encode_candidates\n-\n-## RealmRetriever\n-\n-[[autodoc]] RealmRetriever\n-\n-## RealmEmbedder\n-\n-[[autodoc]] RealmEmbedder\n-    - forward\n-\n-## RealmScorer\n-\n-[[autodoc]] RealmScorer\n-    - forward\n-\n-## RealmKnowledgeAugEncoder\n-\n-[[autodoc]] RealmKnowledgeAugEncoder\n-    - forward\n-\n-## RealmReader\n-\n-[[autodoc]] RealmReader\n-    - forward\n-\n-## RealmForOpenQA\n-\n-[[autodoc]] RealmForOpenQA\n-    - block_embedding_to\n-    - forward"
        },
        {
            "sha": "829fed24215f9ff28584ed0cf1894c43dbf6e18a",
            "filename": "docs/source/en/model_doc/retribert.md",
            "status": "removed",
            "additions": 0,
            "deletions": 57,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fretribert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fretribert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fretribert.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,57 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2020-06-12 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# RetriBERT\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, so we won't accept any new PRs changing its code.\n-\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.\n-You can do so by running the following command: `pip install -U transformers==4.30.0`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The [RetriBERT](https://huggingface.co/yjernite/retribert-base-uncased/tree/main) model was proposed in the blog post [Explain Anything Like I'm Five: A Model for Open Domain Long Form\n-Question Answering](https://yjernite.github.io/lfqa.html). RetriBERT is a small model that uses either a single or\n-pair of BERT encoders with lower-dimension projection for dense semantic indexing of text.\n-\n-This model was contributed by [yjernite](https://huggingface.co/yjernite). Code to train and use the model can be\n-found [here](https://github.com/huggingface/transformers/tree/main/examples/research-projects/distillation).\n-\n-## RetriBertConfig\n-\n-[[autodoc]] RetriBertConfig\n-\n-## RetriBertTokenizer\n-\n-[[autodoc]] RetriBertTokenizer\n-\n-## RetriBertTokenizerFast\n-\n-[[autodoc]] RetriBertTokenizerFast\n-\n-## RetriBertModel\n-\n-[[autodoc]] RetriBertModel\n-    - forward"
        },
        {
            "sha": "a3d836455b19b1bccbffda7ee29fe769c7f02865",
            "filename": "docs/source/en/model_doc/speech_to_text_2.md",
            "status": "removed",
            "additions": 0,
            "deletions": 133,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text_2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text_2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text_2.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,133 +0,0 @@\n-<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2021-04-14 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# Speech2Text2\n-\n-  <Tip warning={true}>\n-\n-  This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-  If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\n-  You can do so by running the following command: `pip install -U transformers==4.40.2`.\n-\n-  </Tip>\n-\n-## Overview\n-\n-The Speech2Text2 model is used together with [Wav2Vec2](wav2vec2) for Speech Translation models proposed in\n-[Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://huggingface.co/papers/2104.06678) by\n-Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\n-\n-Speech2Text2 is a *decoder-only* transformer model that can be used with any speech *encoder-only*, such as\n-[Wav2Vec2](wav2vec2) or [HuBERT](hubert) for Speech-to-Text tasks. Please refer to the\n-[SpeechEncoderDecoder](speech-encoder-decoder) class on how to combine Speech2Text2 with any speech *encoder-only*\n-model.\n-\n-This model was contributed by [Patrick von Platen](https://huggingface.co/patrickvonplaten).\n-\n-The original code can be found [here](https://github.com/pytorch/fairseq/blob/1f7ef9ed1e1061f8c7f88f8b94c7186834398690/fairseq/models/wav2vec/wav2vec2_asr.py#L266).\n-\n-## Usage tips\n-\n-- Speech2Text2 achieves state-of-the-art results on the CoVoST Speech Translation dataset. For more information, see\n-  the [official models](https://huggingface.co/models?other=speech2text2) .\n-- Speech2Text2 is always used within the [SpeechEncoderDecoder](speech-encoder-decoder) framework.\n-- Speech2Text2's tokenizer is based on [fastBPE](https://github.com/glample/fastBPE).\n-\n-## Inference\n-\n-Speech2Text2's [`SpeechEncoderDecoderModel`] model accepts raw waveform input values from speech and\n-makes use of [`~generation.GenerationMixin.generate`] to translate the input speech\n-autoregressively to the target language.\n-\n-The [`Wav2Vec2FeatureExtractor`] class is responsible for preprocessing the input speech and\n-[`Speech2Text2Tokenizer`] decodes the generated target tokens to the target string. The\n-[`Speech2Text2Processor`] wraps [`Wav2Vec2FeatureExtractor`] and\n-[`Speech2Text2Tokenizer`] into a single instance to both extract the input features and decode the\n-predicted token ids.\n-\n-- Step-by-step Speech Translation\n-\n-```python\n->>> from transformers import Speech2Text2Processor, SpeechEncoderDecoderModel\n->>> from datasets import load_dataset\n-\n->>> model = SpeechEncoderDecoderModel.from_pretrained(\"facebook/s2t-wav2vec2-large-en-de\")\n->>> processor = Speech2Text2Processor.from_pretrained(\"facebook/s2t-wav2vec2-large-en-de\")\n-\n-\n->>> def map_to_array(example):\n-...     example[\"speech\"] = example[\"audio\"][\"array\"]\n-...     return example\n-\n-\n->>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n->>> ds = ds.map(map_to_array)\n-\n->>> inputs = processor(ds[\"speech\"][0], sampling_rate=16_000, return_tensors=\"pt\")\n->>> generated_ids = model.generate(inputs=inputs[\"input_values\"], attention_mask=inputs[\"attention_mask\"])\n-\n->>> transcription = processor.batch_decode(generated_ids)\n-```\n-\n-- Speech Translation via Pipelines\n-\n-  The automatic speech recognition pipeline can also be used to translate speech in just a couple lines of code\n-\n-```python\n->>> from datasets import load_dataset\n->>> from transformers import pipeline\n-\n->>> librispeech_en = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n->>> asr = pipeline(\n-...     \"automatic-speech-recognition\",\n-...     model=\"facebook/s2t-wav2vec2-large-en-de\",\n-...     feature_extractor=\"facebook/s2t-wav2vec2-large-en-de\",\n-... )\n-\n->>> translation_de = asr(librispeech_en[0][\"file\"])\n-```\n-\n-See [model hub](https://huggingface.co/models?filter=speech2text2) to look for Speech2Text2 checkpoints.\n-\n-## Resources\n-\n-- [Causal language modeling task guide](../tasks/language_modeling)\n-\n-## Speech2Text2Config\n-\n-[[autodoc]] Speech2Text2Config\n-\n-## Speech2TextTokenizer\n-\n-[[autodoc]] Speech2Text2Tokenizer\n-    - batch_decode\n-    - decode\n-    - save_vocabulary\n-\n-## Speech2Text2Processor\n-\n-[[autodoc]] Speech2Text2Processor\n-    - __call__\n-    - from_pretrained\n-    - save_pretrained\n-    - batch_decode\n-    - decode\n-\n-## Speech2Text2ForCausalLM\n-\n-[[autodoc]] Speech2Text2ForCausalLM\n-    - forward"
        },
        {
            "sha": "606d8940c4edbaccfbea9daee5e9b85973875357",
            "filename": "docs/source/en/model_doc/tapex.md",
            "status": "removed",
            "additions": 0,
            "deletions": 155,
            "changes": 155,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapex.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,155 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2021-07-16 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# TAPEX\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.\n-You can do so by running the following command: `pip install -U transformers==4.30.0`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The TAPEX model was proposed in [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://huggingface.co/papers/2107.07653) by Qian Liu,\n-Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. TAPEX pre-trains a BART model to solve synthetic SQL queries, after\n-which it can be fine-tuned to answer natural language questions related to tabular data, as well as performing table fact checking.\n-\n-TAPEX has been fine-tuned on several datasets:\n-\n-- [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) (Sequential Question Answering by Microsoft)\n-- [WTQ](https://github.com/ppasupat/WikiTableQuestions) (Wiki Table Questions by Stanford University)\n-- [WikiSQL](https://github.com/salesforce/WikiSQL) (by Salesforce)\n-- [TabFact](https://tabfact.github.io/) (by USCB NLP Lab).\n-\n-The abstract from the paper is the following:\n-\n-*Recent progress in language model pre-training has achieved a great success via leveraging large-scale unstructured textual data. However, it is\n-still a challenge to apply pre-training on structured tabular data due to the absence of large-scale high-quality tabular data. In this paper, we\n-propose TAPEX to show that table pre-training can be achieved by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically\n-synthesizing executable SQL queries and their execution outputs. TAPEX addresses the data scarcity challenge via guiding the language model to mimic a SQL\n-executor on the diverse, large-scale and high-quality synthetic corpus. We evaluate TAPEX on four benchmark datasets. Experimental results demonstrate that\n-TAPEX outperforms previous table pre-training approaches by a large margin and achieves new state-of-the-art results on all of them. This includes improvements\n-on the weakly-supervised WikiSQL denotation accuracy to 89.5% (+2.3%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA denotation accuracy\n-to 74.5% (+3.5%), and the TabFact accuracy to 84.2% (+3.2%). To our knowledge, this is the first work to exploit table pre-training via synthetic executable programs\n-and to achieve new state-of-the-art results on various downstream tasks.*\n-\n-## Usage tips\n-\n-- TAPEX is a generative (seq2seq) model. One can directly plug in the weights of TAPEX into a BART model.\n-- TAPEX has checkpoints on the hub that are either pre-trained only, or fine-tuned on WTQ, SQA, WikiSQL and TabFact.\n-- Sentences + tables are presented to the model as `sentence + \" \" + linearized table`. The linearized table has the following format:\n-  `col: col1 | col2 | col 3 row 1 : val1 | val2 | val3 row 2 : ...`.\n-- TAPEX has its own tokenizer, that allows to prepare all data for the model easily. One can pass Pandas DataFrames and strings to the tokenizer,\n-  and it will automatically create the `input_ids` and `attention_mask` (as shown in the usage examples below).\n-\n-### Usage: inference\n-\n-Below, we illustrate how to use TAPEX for table question answering. As one can see, one can directly plug in the weights of TAPEX into a BART model.\n-We use the [Auto API](auto), which will automatically instantiate the appropriate tokenizer ([`TapexTokenizer`]) and model ([`BartForConditionalGeneration`]) for us,\n-based on the configuration file of the checkpoint on the hub.\n-\n-```python\n->>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n->>> import pandas as pd\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/tapex-large-finetuned-wtq\")\n->>> model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/tapex-large-finetuned-wtq\")\n-\n->>> # prepare table + question\n->>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n->>> table = pd.DataFrame.from_dict(data)\n->>> question = \"how many movies does Leonardo Di Caprio have?\"\n-\n->>> encoding = tokenizer(table, question, return_tensors=\"pt\")\n-\n->>> # let the model generate an answer autoregressively\n->>> outputs = model.generate(**encoding)\n-\n->>> # decode back to text\n->>> predicted_answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n->>> print(predicted_answer)\n-53\n-```\n-\n-Note that [`TapexTokenizer`] also supports batched inference. Hence, one can provide a batch of different tables/questions, or a batch of a single table\n-and multiple questions, or a batch of a single query and multiple tables. Let's illustrate this:\n-\n-```python\n->>> # prepare table + question\n->>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n->>> table = pd.DataFrame.from_dict(data)\n->>> questions = [\n-...     \"how many movies does Leonardo Di Caprio have?\",\n-...     \"which actor has 69 movies?\",\n-...     \"what's the first name of the actor who has 87 movies?\",\n-... ]\n->>> encoding = tokenizer(table, questions, padding=True, return_tensors=\"pt\")\n-\n->>> # let the model generate an answer autoregressively\n->>> outputs = model.generate(**encoding)\n-\n->>> # decode back to text\n->>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-[' 53', ' george clooney', ' brad pitt']\n-```\n-\n-In case one wants to do table verification (i.e. the task of determining whether a given sentence is supported or refuted by the contents\n-of a table), one can instantiate a [`BartForSequenceClassification`] model. TAPEX has checkpoints on the hub fine-tuned on TabFact, an important\n-benchmark for table fact checking (it achieves 84% accuracy). The code example below again leverages the [Auto API](auto).\n-\n-```python\n->>> from transformers import AutoTokenizer, AutoModelForSequenceClassification\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/tapex-large-finetuned-tabfact\")\n->>> model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/tapex-large-finetuned-tabfact\")\n-\n->>> # prepare table + sentence\n->>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n->>> table = pd.DataFrame.from_dict(data)\n->>> sentence = \"George Clooney has 30 movies\"\n-\n->>> encoding = tokenizer(table, sentence, return_tensors=\"pt\")\n-\n->>> # forward pass\n->>> outputs = model(**encoding)\n-\n->>> # print prediction\n->>> predicted_class_idx = outputs.logits[0].argmax(dim=0).item()\n->>> print(model.config.id2label[predicted_class_idx])\n-Refused\n-```\n-\n-<Tip>\n-\n-TAPEX architecture is the same as BART, except for tokenization. Refer to [BART documentation](bart) for information on\n-configuration classes and their parameters. TAPEX-specific tokenizer is documented below.\n-\n-</Tip>\n-\n-## TapexTokenizer\n-\n-[[autodoc]] TapexTokenizer\n-    - __call__\n-    - save_vocabulary"
        },
        {
            "sha": "fba51b1811571f6233a3b2b515a6e63dc851b9d2",
            "filename": "docs/source/en/model_doc/trajectory_transformer.md",
            "status": "removed",
            "additions": 0,
            "deletions": 66,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Ftrajectory_transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Ftrajectory_transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftrajectory_transformer.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,66 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2021-06-03 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# Trajectory Transformer\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, so we won't accept any new PRs changing its code.\n-\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.\n-You can do so by running the following command: `pip install -U transformers==4.30.0`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The Trajectory Transformer model was proposed in [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://huggingface.co/papers/2106.02039)  by Michael Janner, Qiyang Li, Sergey Levine.\n-\n-The abstract from the paper is the following:\n-\n-*Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models,\n-leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence\n-modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards.\n-Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well\n-in other domains, such as natural-language processing, can also provide effective solutions to the RL problem.\n-To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture\n-to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence\n-modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common\n-in offline RL algorithms. We demonstrate the flexibility of this approach across long-horizon dynamics prediction,\n-imitation learning, goal-conditioned RL, and offline RL. Further, we show that this approach can be combined with\n-existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.*\n-\n-This model was contributed by [CarlCochet](https://huggingface.co/CarlCochet). The original code can be found [here](https://github.com/jannerm/trajectory-transformer).\n-\n-## Usage tips\n-\n-This Transformer is used for deep reinforcement learning. To use it, you need to create sequences from\n-actions, states and rewards from all previous timesteps. This model will treat all these elements together\n-as one big sequence (a trajectory).\n-\n-## TrajectoryTransformerConfig\n-\n-[[autodoc]] TrajectoryTransformerConfig\n-\n-## TrajectoryTransformerModel\n-\n-[[autodoc]] TrajectoryTransformerModel\n-    - forward"
        },
        {
            "sha": "0bd1b0f57e1dd91b1e5ad84a59f5bc5a9635218b",
            "filename": "docs/source/en/model_doc/transfo-xl.md",
            "status": "removed",
            "additions": 0,
            "deletions": 136,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Ftransfo-xl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Ftransfo-xl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftransfo-xl.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,136 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2019-01-09 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# Transformer XL\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, so we won't accept any new PRs changing its code. This model was deprecated due to security issues linked to `pickle.load`.\n-\n-We recommend switching to more recent models for improved security.\n-\n-In case you would still like to use `TransfoXL` in your experiments, we recommend using the [Hub checkpoint](https://huggingface.co/transfo-xl/transfo-xl-wt103) with a specific revision to ensure you are downloading safe files from the Hub.\n-\n-You will need to set the environment variable `TRUST_REMOTE_CODE` to `True` in order to allow the\n-usage of `pickle.load()`:\n-\n-```python\n-import os\n-from transformers import TransfoXLTokenizer, TransfoXLLMHeadModel\n-\n-os.environ[\"TRUST_REMOTE_CODE\"] = \"True\"\n-\n-checkpoint = 'transfo-xl/transfo-xl-wt103'\n-revision = '40a186da79458c9f9de846edfaea79c412137f97'\n-\n-tokenizer = TransfoXLTokenizer.from_pretrained(checkpoint, revision=revision)\n-model = TransfoXLLMHeadModel.from_pretrained(checkpoint, revision=revision)\n-```\n-\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.35.0.\n-You can do so by running the following command: `pip install -U transformers==4.35.0`.\n-\n-</Tip>\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=transfo-xl\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-transfo--xl-blueviolet\">\n-</a>\n-<a href=\"https://huggingface.co/spaces/docs-demos/transfo-xl-wt103\">\n-<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n-</a>\n-</div>\n-\n-## Overview\n-\n-The Transformer-XL model was proposed in [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://huggingface.co/papers/1901.02860) by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan\n-Salakhutdinov. It's a causal (uni-directional) transformer with relative positioning (sinusoÃ¯dal) embeddings which can\n-reuse previously computed hidden-states to attend to longer context (memory). This model also uses adaptive softmax\n-inputs and outputs (tied).\n-\n-The abstract from the paper is the following:\n-\n-*Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the\n-setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency\n-beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a\n-novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the\n-context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450%\n-longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+\n-times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of\n-bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn\n-Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably\n-coherent, novel text articles with thousands of tokens.*\n-\n-This model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/kimiyoung/transformer-xl).\n-\n-## Usage tips\n-\n-- Transformer-XL uses relative sinusoidal positional embeddings. Padding can be done on the left or on the right. The\n-  original implementation trains on SQuAD with padding on the left, therefore the padding defaults are set to left.\n-- Transformer-XL is one of the few models that has no sequence length limit.\n-- Same as a regular GPT model, but introduces a recurrence mechanism for two consecutive segments (similar to a regular RNNs with two consecutive inputs). In this context, a segment is a number of consecutive tokens (for instance 512) that may span across multiple documents, and segments are fed in order to the model.\n-- Basically, the hidden states of the previous segment are concatenated to the current input to compute the attention scores. This allows the model to pay attention to information that was in the previous segment as well as the current one. By stacking multiple attention layers, the receptive field can be increased to multiple previous segments.\n-- This changes the positional embeddings to positional relative embeddings (as the regular positional embeddings would give the same results in the current input and the current hidden state at a given position) and needs to make some adjustments in the way attention scores are computed.\n-\n-<Tip warning={true}>\n-\n-TransformerXL does **not** work with *torch.nn.DataParallel* due to a bug in PyTorch, see [issue #36035](https://github.com/pytorch/pytorch/issues/36035)\n-\n-</Tip>\n-\n-## Resources\n-\n-- [Text classification task guide](../tasks/sequence_classification)\n-- [Causal language modeling task guide](../tasks/language_modeling)\n-\n-## TransfoXLConfig\n-\n-[[autodoc]] TransfoXLConfig\n-\n-## TransfoXLTokenizer\n-\n-[[autodoc]] TransfoXLTokenizer\n-    - save_vocabulary\n-\n-## TransfoXL specific outputs\n-\n-[[autodoc]] models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput\n-\n-[[autodoc]] models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput\n-\n-## TransfoXLModel\n-\n-[[autodoc]] TransfoXLModel\n-    - forward\n-\n-## TransfoXLLMHeadModel\n-\n-[[autodoc]] TransfoXLLMHeadModel\n-    - forward\n-\n-## TransfoXLForSequenceClassification\n-\n-[[autodoc]] TransfoXLForSequenceClassification\n-    - forward\n-\n-## Internal Layers\n-\n-[[autodoc]] AdaptiveEmbedding"
        },
        {
            "sha": "89dd1df78b62a9a2bc66264ccbcb98a4eb0161ac",
            "filename": "docs/source/en/model_doc/tvlt.md",
            "status": "removed",
            "additions": 0,
            "deletions": 90,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvlt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvlt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvlt.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,90 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2022-09-28 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# TVLT\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\n-You can do so by running the following command: `pip install -U transformers==4.40.2`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The TVLT model was proposed in [TVLT: Textless Vision-Language Transformer](https://huggingface.co/papers/2209.14156)\n-by Zineng Tang, Jaemin Cho, Yixin Nie, Mohit Bansal (the first three authors contributed equally). The Textless Vision-Language Transformer (TVLT) is a model that uses raw visual and audio inputs for vision-and-language representation learning, without using text-specific modules such as tokenization or automatic speech recognition (ASR). It can perform various audiovisual and vision-language tasks like retrieval, question answering, etc.\n-\n-The abstract from the paper is the following:\n-\n-*In this work, we present the Textless Vision-Language Transformer (TVLT), where homogeneous transformer blocks take raw visual and audio inputs for vision-and-language representation learning with minimal modality-specific design, and do not use text-specific modules such as tokenization or automatic speech recognition (ASR). TVLT is trained by reconstructing masked patches of continuous video frames and audio spectrograms (masked autoencoding) and contrastive modeling to align video and audio. TVLT attains performance comparable to its text-based counterpart on various multimodal tasks, such as visual question answering, image retrieval, video retrieval, and multimodal sentiment analysis, with 28x faster inference speed and only 1/3 of the parameters. Our findings suggest the possibility of learning compact and efficient visual-linguistic representations from low-level visual and audio signals without assuming the prior existence of text.*\n-\n-<p align=\"center\">\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/tvlt_architecture.png\"\n-alt=\"drawing\" width=\"600\"/>\n-</p>\n-\n-<small> TVLT architecture. Taken from the <a href=\"[https://huggingface.co/papers/2102.03334](https://huggingface.co/papers/2209.14156)\">original paper</a>. </small>\n-\n-The original code can be found [here](https://github.com/zinengtang/TVLT). This model was contributed by [Zineng Tang](https://huggingface.co/ZinengTang).\n-\n-## Usage tips\n-\n-- TVLT is a model that takes both `pixel_values` and `audio_values` as input. One can use [`TvltProcessor`] to prepare data for the model.\n-  This processor wraps an image processor (for the image/video modality) and an audio feature extractor (for the audio modality) into one.\n-- TVLT is trained with images/videos and audios of various sizes: the authors resize and crop the input images/videos to 224 and limit the length of audio spectrogram to 2048. To make batching of videos and audios possible, the authors use a `pixel_mask` that indicates which pixels are real/padding and `audio_mask` that indicates which audio values are real/padding.\n-- The design of TVLT is very similar to that of a standard Vision Transformer (ViT) and masked autoencoder (MAE) as in [ViTMAE](vitmae). The difference is that the model includes embedding layers for the audio modality.\n-- The PyTorch version of this model is only available in torch 1.10 and higher.\n-\n-## TvltConfig\n-\n-[[autodoc]] TvltConfig\n-\n-## TvltProcessor\n-\n-[[autodoc]] TvltProcessor\n-    - __call__\n-\n-## TvltFeatureExtractor\n-\n-[[autodoc]] TvltFeatureExtractor\n-    - __call__\n-\n-## TvltImageProcessor\n-\n-[[autodoc]] TvltImageProcessor\n-    - preprocess\n-\n-## TvltModel\n-\n-[[autodoc]] TvltModel\n-    - forward\n-\n-## TvltForPreTraining\n-\n-[[autodoc]] TvltForPreTraining\n-    - forward\n-\n-## TvltForAudioVisualClassification\n-\n-[[autodoc]] TvltForAudioVisualClassification\n-    - forward"
        },
        {
            "sha": "0a4ded430211a2328b05c04968eaed0237e44747",
            "filename": "docs/source/en/model_doc/van.md",
            "status": "removed",
            "additions": 0,
            "deletions": 76,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvan.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvan.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvan.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,76 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2022-02-20 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# VAN\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.\n-You can do so by running the following command: `pip install -U transformers==4.30.0`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The VAN model was proposed in [Visual Attention Network](https://huggingface.co/papers/2202.09741) by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\n-\n-This paper introduces a new attention layer based on convolution operations able to capture both local and distant relationships. This is done by combining normal and large kernel convolution layers. The latter uses a dilated convolution to capture distant correlations.\n-\n-The abstract from the paper is the following:\n-\n-*While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel large kernel attention (LKA) module to enable self-adaptive and long-range correlations in self-attention while avoiding the above issues. We further introduce a novel neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN outperforms the state-of-the-art vision transformers and convolutional neural networks with a large margin in extensive experiments, including image classification, object detection, semantic segmentation, instance segmentation, etc. Code is available at [this https URL](https://github.com/Visual-Attention-Network/VAN-Classification).*\n-\n-Tips:\n-\n-- VAN does not have an embedding layer, thus the `hidden_states` will have a length equal to the number of stages.\n-\n-The figure below illustrates the architecture of a Visual Attention Layer. Taken from the [original paper](https://huggingface.co/papers/2202.09741).\n-\n-<img width=\"600\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/van_architecture.png\"/>\n-\n-This model was contributed by [Francesco](https://huggingface.co/Francesco). The original code can be found [here](https://github.com/Visual-Attention-Network/VAN-Classification).\n-\n-## Resources\n-\n-A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with VAN.\n-\n-<PipelineTag pipeline=\"image-classification\"/>\n-\n-- [`VanForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n-- See also: [Image classification task guide](../tasks/image_classification)\n-\n-If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n-\n-## VanConfig\n-\n-[[autodoc]] VanConfig\n-\n-## VanModel\n-\n-[[autodoc]] VanModel\n-    - forward\n-\n-## VanForImageClassification\n-\n-[[autodoc]] VanForImageClassification\n-    - forward"
        },
        {
            "sha": "c10d1c489b76784b0352c52aa0504441b6f27d4a",
            "filename": "docs/source/en/model_doc/vit_hybrid.md",
            "status": "removed",
            "additions": 0,
            "deletions": 112,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_hybrid.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_hybrid.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_hybrid.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,112 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2020-10-22 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# Hybrid Vision Transformer (ViT Hybrid)\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\n-You can do so by running the following command: `pip install -U transformers==4.40.2`.\n-\n-</Tip>\n-\n-## Overview\n-\n-The hybrid Vision Transformer (ViT) model was proposed in [An Image is Worth 16x16 Words: Transformers for Image Recognition\n-at Scale](https://huggingface.co/papers/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\n-Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\n-Uszkoreit, Neil Houlsby. It's the first paper that successfully trains a Transformer encoder on ImageNet, attaining\n-very good results compared to familiar convolutional architectures. ViT hybrid is a slight variant of the [plain Vision Transformer](vit),\n-by leveraging a convolutional backbone (specifically, [BiT](bit)) whose features are used as initial \"tokens\" for the Transformer.\n-\n-The abstract from the paper is the following:\n-\n-*While the Transformer architecture has become the de-facto standard for natural language processing tasks, its\n-applications to computer vision remain limited. In vision, attention is either applied in conjunction with\n-convolutional networks, or used to replace certain components of convolutional networks while keeping their overall\n-structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to\n-sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of\n-data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.),\n-Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring\n-substantially fewer computational resources to train.*\n-\n-This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code (written in JAX) can be\n-found [here](https://github.com/google-research/vision_transformer).\n-\n-## Using Scaled Dot Product Attention (SDPA)\n-\n-PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n-encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n-[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n-or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n-page for more information.\n-\n-SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n-`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n-\n-```py\n-from transformers import ViTHybridForImageClassification\n-model = ViTHybridForImageClassification.from_pretrained(\"google/vit-hybrid-base-bit-384\", attn_implementation=\"sdpa\", dtype=torch.float16)\n-...\n-```\n-\n-For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n-\n-On a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with `float32` and `google/vit-hybrid-base-bit-384` model, we saw the following speedups during inference.\n-\n-|   Batch size |   Average inference time (ms), eager mode |   Average inference time (ms), sdpa model |   Speed up, Sdpa / Eager (x) |\n-|--------------|-------------------------------------------|-------------------------------------------|------------------------------|\n-|            1 |                                        29 |                                        18 |                      1.61 |\n-|            2 |                                        26 |                                        18 |                      1.44 |\n-|            4 |                                        25 |                                        18 |                      1.39 |\n-|            8 |                                        34 |                                        24 |                      1.42 |\n-\n-## Resources\n-\n-A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with ViT Hybrid.\n-\n-<PipelineTag pipeline=\"image-classification\"/>\n-\n-- [`ViTHybridForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n-- See also: [Image classification task guide](../tasks/image_classification)\n-\n-If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n-\n-## ViTHybridConfig\n-\n-[[autodoc]] ViTHybridConfig\n-\n-## ViTHybridImageProcessor\n-\n-[[autodoc]] ViTHybridImageProcessor\n-    - preprocess\n-\n-## ViTHybridModel\n-\n-[[autodoc]] ViTHybridModel\n-    - forward\n-\n-## ViTHybridForImageClassification\n-\n-[[autodoc]] ViTHybridForImageClassification\n-    - forward"
        },
        {
            "sha": "fbf47d8c422a001bbbf91cf6654265eb46b8c725",
            "filename": "docs/source/en/model_doc/xlm-prophetnet.md",
            "status": "removed",
            "additions": 0,
            "deletions": 99,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-prophetnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-prophetnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-prophetnet.md?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,99 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-*This model was released on 2020-01-13 and added to Hugging Face Transformers on 2023-06-20.*\n-\n-# XLM-ProphetNet\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n-\n-<Tip warning={true}>\n-\n-This model is in maintenance mode only, we don't accept any new PRs changing its code.\n-If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\n-You can do so by running the following command: `pip install -U transformers==4.40.2`.\n-\n-</Tip>\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=xprophetnet\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-xprophetnet-blueviolet\">\n-</a>\n-<a href=\"https://huggingface.co/spaces/docs-demos/xprophetnet-large-wiki100-cased-xglue-ntg\">\n-<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n-</a>\n-</div>\n-\n-**DISCLAIMER:** If you see something strange, file a [Github Issue](https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title) and assign\n-@patrickvonplaten\n-\n-## Overview\n-\n-The XLM-ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,](https://huggingface.co/papers/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei\n-Zhang, Ming Zhou on 13 Jan, 2020.\n-\n-XLM-ProphetNet is an encoder-decoder model and can predict n-future tokens for \"ngram\" language modeling instead of\n-just the next token. Its architecture is identical to ProhpetNet, but the model was trained on the multi-lingual\n-\"wiki100\" Wikipedia dump. XLM-ProphetNet's model architecture and pretraining objective is same as ProphetNet, but XLM-ProphetNet was pre-trained on the cross-lingual dataset XGLUE.\n-\n-The abstract from the paper is the following:\n-\n-*In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel\n-self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of\n-the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by\n-n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time\n-step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent\n-overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale\n-dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for\n-abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new\n-state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.*\n-\n-The Authors' code can be found [here](https://github.com/microsoft/ProphetNet).\n-\n-## Resources\n-\n-- [Causal language modeling task guide](../tasks/language_modeling)\n-- [Translation task guide](../tasks/translation)\n-- [Summarization task guide](../tasks/summarization)\n-\n-## XLMProphetNetConfig\n-\n-[[autodoc]] XLMProphetNetConfig\n-\n-## XLMProphetNetTokenizer\n-\n-[[autodoc]] XLMProphetNetTokenizer\n-\n-## XLMProphetNetModel\n-\n-[[autodoc]] XLMProphetNetModel\n-\n-## XLMProphetNetEncoder\n-\n-[[autodoc]] XLMProphetNetEncoder\n-\n-## XLMProphetNetDecoder\n-\n-[[autodoc]] XLMProphetNetDecoder\n-\n-## XLMProphetNetForConditionalGeneration\n-\n-[[autodoc]] XLMProphetNetForConditionalGeneration\n-\n-## XLMProphetNetForCausalLM\n-\n-[[autodoc]] XLMProphetNetForCausalLM"
        },
        {
            "sha": "5f4cb1cf919db85d9a66abeca041ad352fbf1258",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f311047860d46367d51d2d5b280286b10ba9466/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f311047860d46367d51d2d5b280286b10ba9466/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=9f311047860d46367d51d2d5b280286b10ba9466",
            "patch": "@@ -1731,10 +1731,8 @@ def converted(self) -> Tokenizer:\n     \"OpenAIGPTTokenizer\": OpenAIGPTConverter,\n     \"PegasusTokenizer\": PegasusConverter,\n     \"Qwen2Tokenizer\": Qwen2Converter,\n-    \"RealmTokenizer\": BertConverter,\n     \"ReformerTokenizer\": ReformerConverter,\n     \"RemBertTokenizer\": RemBertConverter,\n-    \"RetriBertTokenizer\": BertConverter,\n     \"RobertaTokenizer\": RobertaConverter,\n     \"RoFormerTokenizer\": RoFormerConverter,\n     \"SeamlessM4TTokenizer\": SeamlessM4TConverter,"
        },
        {
            "sha": "cdb6cc834163e00fec8efad9f4c62d0ed747b98e",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 70,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f311047860d46367d51d2d5b280286b10ba9466/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f311047860d46367d51d2d5b280286b10ba9466/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=9f311047860d46367d51d2d5b280286b10ba9466",
            "patch": "@@ -114,7 +114,6 @@\n         (\"deit\", \"DeiTConfig\"),\n         (\"depth_anything\", \"DepthAnythingConfig\"),\n         (\"depth_pro\", \"DepthProConfig\"),\n-        (\"deta\", \"DetaConfig\"),\n         (\"detr\", \"DetrConfig\"),\n         (\"dia\", \"DiaConfig\"),\n         (\"diffllama\", \"DiffLlamaConfig\"),\n@@ -132,7 +131,6 @@\n         (\"edgetam\", \"EdgeTamConfig\"),\n         (\"edgetam_video\", \"EdgeTamVideoConfig\"),\n         (\"edgetam_vision_model\", \"EdgeTamVisionConfig\"),\n-        (\"efficientformer\", \"EfficientFormerConfig\"),\n         (\"efficientloftr\", \"EfficientLoFTRConfig\"),\n         (\"efficientnet\", \"EfficientNetConfig\"),\n         (\"electra\", \"ElectraConfig\"),\n@@ -143,7 +141,6 @@\n         (\"ernie\", \"ErnieConfig\"),\n         (\"ernie4_5\", \"Ernie4_5Config\"),\n         (\"ernie4_5_moe\", \"Ernie4_5_MoeConfig\"),\n-        (\"ernie_m\", \"ErnieMConfig\"),\n         (\"esm\", \"EsmConfig\"),\n         (\"evolla\", \"EvollaConfig\"),\n         (\"exaone4\", \"Exaone4Config\"),\n@@ -190,14 +187,12 @@\n         (\"gpt_neox_japanese\", \"GPTNeoXJapaneseConfig\"),\n         (\"gpt_oss\", \"GptOssConfig\"),\n         (\"gptj\", \"GPTJConfig\"),\n-        (\"gptsan-japanese\", \"GPTSanJapaneseConfig\"),\n         (\"granite\", \"GraniteConfig\"),\n         (\"granite_speech\", \"GraniteSpeechConfig\"),\n         (\"granitemoe\", \"GraniteMoeConfig\"),\n         (\"granitemoehybrid\", \"GraniteMoeHybridConfig\"),\n         (\"granitemoeshared\", \"GraniteMoeSharedConfig\"),\n         (\"granitevision\", \"LlavaNextConfig\"),\n-        (\"graphormer\", \"GraphormerConfig\"),\n         (\"grounding-dino\", \"GroundingDinoConfig\"),\n         (\"groupvit\", \"GroupViTConfig\"),\n         (\"helium\", \"HeliumConfig\"),\n@@ -221,7 +216,6 @@\n         (\"jamba\", \"JambaConfig\"),\n         (\"janus\", \"JanusConfig\"),\n         (\"jetmoe\", \"JetMoeConfig\"),\n-        (\"jukebox\", \"JukeboxConfig\"),\n         (\"kosmos-2\", \"Kosmos2Config\"),\n         (\"kosmos-2.5\", \"Kosmos2_5Config\"),\n         (\"kyutai_speech_to_text\", \"KyutaiSpeechToTextConfig\"),\n@@ -257,8 +251,6 @@\n         (\"maskformer\", \"MaskFormerConfig\"),\n         (\"maskformer-swin\", \"MaskFormerSwinConfig\"),\n         (\"mbart\", \"MBartConfig\"),\n-        (\"mctct\", \"MCTCTConfig\"),\n-        (\"mega\", \"MegaConfig\"),\n         (\"megatron-bert\", \"MegatronBertConfig\"),\n         (\"metaclip_2\", \"MetaClip2Config\"),\n         (\"mgp-str\", \"MgpstrConfig\"),\n@@ -287,9 +279,7 @@\n         (\"musicgen\", \"MusicgenConfig\"),\n         (\"musicgen_melody\", \"MusicgenMelodyConfig\"),\n         (\"mvp\", \"MvpConfig\"),\n-        (\"nat\", \"NatConfig\"),\n         (\"nemotron\", \"NemotronConfig\"),\n-        (\"nezha\", \"NezhaConfig\"),\n         (\"nllb-moe\", \"NllbMoeConfig\"),\n         (\"nougat\", \"VisionEncoderDecoderConfig\"),\n         (\"nystromformer\", \"NystromformerConfig\"),\n@@ -299,7 +289,6 @@\n         (\"olmoe\", \"OlmoeConfig\"),\n         (\"omdet-turbo\", \"OmDetTurboConfig\"),\n         (\"oneformer\", \"OneFormerConfig\"),\n-        (\"open-llama\", \"OpenLlamaConfig\"),\n         (\"openai-gpt\", \"OpenAIGPTConfig\"),\n         (\"opt\", \"OPTConfig\"),\n         (\"ovis2\", \"Ovis2Config\"),\n@@ -328,7 +317,6 @@\n         (\"prophetnet\", \"ProphetNetConfig\"),\n         (\"pvt\", \"PvtConfig\"),\n         (\"pvt_v2\", \"PvtV2Config\"),\n-        (\"qdqbert\", \"QDQBertConfig\"),\n         (\"qwen2\", \"Qwen2Config\"),\n         (\"qwen2_5_omni\", \"Qwen2_5OmniConfig\"),\n         (\"qwen2_5_vl\", \"Qwen2_5_VLConfig\"),\n@@ -347,13 +335,11 @@\n         (\"qwen3_vl_moe_text\", \"Qwen3VLMoeTextConfig\"),\n         (\"qwen3_vl_text\", \"Qwen3VLTextConfig\"),\n         (\"rag\", \"RagConfig\"),\n-        (\"realm\", \"RealmConfig\"),\n         (\"recurrent_gemma\", \"RecurrentGemmaConfig\"),\n         (\"reformer\", \"ReformerConfig\"),\n         (\"regnet\", \"RegNetConfig\"),\n         (\"rembert\", \"RemBertConfig\"),\n         (\"resnet\", \"ResNetConfig\"),\n-        (\"retribert\", \"RetriBertConfig\"),\n         (\"roberta\", \"RobertaConfig\"),\n         (\"roberta-prelayernorm\", \"RobertaPreLayerNormConfig\"),\n         (\"roc_bert\", \"RoCBertConfig\"),\n@@ -387,7 +373,6 @@\n         (\"smolvlm_vision\", \"SmolVLMVisionConfig\"),\n         (\"speech-encoder-decoder\", \"SpeechEncoderDecoderConfig\"),\n         (\"speech_to_text\", \"Speech2TextConfig\"),\n-        (\"speech_to_text_2\", \"Speech2Text2Config\"),\n         (\"speecht5\", \"SpeechT5Config\"),\n         (\"splinter\", \"SplinterConfig\"),\n         (\"squeezebert\", \"SqueezeBertConfig\"),\n@@ -410,18 +395,14 @@\n         (\"timesformer\", \"TimesformerConfig\"),\n         (\"timm_backbone\", \"TimmBackboneConfig\"),\n         (\"timm_wrapper\", \"TimmWrapperConfig\"),\n-        (\"trajectory_transformer\", \"TrajectoryTransformerConfig\"),\n-        (\"transfo-xl\", \"TransfoXLConfig\"),\n         (\"trocr\", \"TrOCRConfig\"),\n-        (\"tvlt\", \"TvltConfig\"),\n         (\"tvp\", \"TvpConfig\"),\n         (\"udop\", \"UdopConfig\"),\n         (\"umt5\", \"UMT5Config\"),\n         (\"unispeech\", \"UniSpeechConfig\"),\n         (\"unispeech-sat\", \"UniSpeechSatConfig\"),\n         (\"univnet\", \"UnivNetConfig\"),\n         (\"upernet\", \"UperNetConfig\"),\n-        (\"van\", \"VanConfig\"),\n         (\"vaultgemma\", \"VaultGemmaConfig\"),\n         (\"video_llama_3\", \"VideoLlama3Config\"),\n         (\"video_llama_3_vision\", \"VideoLlama3VisionConfig\"),\n@@ -433,7 +414,6 @@\n         (\"vision-text-dual-encoder\", \"VisionTextDualEncoderConfig\"),\n         (\"visual_bert\", \"VisualBertConfig\"),\n         (\"vit\", \"ViTConfig\"),\n-        (\"vit_hybrid\", \"ViTHybridConfig\"),\n         (\"vit_mae\", \"ViTMAEConfig\"),\n         (\"vit_msn\", \"ViTMSNConfig\"),\n         (\"vitdet\", \"VitDetConfig\"),\n@@ -454,7 +434,6 @@\n         (\"xcodec\", \"XcodecConfig\"),\n         (\"xglm\", \"XGLMConfig\"),\n         (\"xlm\", \"XLMConfig\"),\n-        (\"xlm-prophetnet\", \"XLMProphetNetConfig\"),\n         (\"xlm-roberta\", \"XLMRobertaConfig\"),\n         (\"xlm-roberta-xl\", \"XLMRobertaXLConfig\"),\n         (\"xlnet\", \"XLNetConfig\"),\n@@ -508,7 +487,6 @@\n         (\"blip_2_qformer\", \"BLIP-2 QFormer\"),\n         (\"bloom\", \"BLOOM\"),\n         (\"blt\", \"Blt\"),\n-        (\"bort\", \"BORT\"),\n         (\"bridgetower\", \"BridgeTower\"),\n         (\"bros\", \"BROS\"),\n         (\"byt5\", \"ByT5\"),\n@@ -560,7 +538,6 @@\n         (\"depth_anything\", \"Depth Anything\"),\n         (\"depth_anything_v2\", \"Depth Anything V2\"),\n         (\"depth_pro\", \"DepthPro\"),\n-        (\"deta\", \"DETA\"),\n         (\"detr\", \"DETR\"),\n         (\"dia\", \"Dia\"),\n         (\"dialogpt\", \"DialoGPT\"),\n@@ -580,7 +557,6 @@\n         (\"edgetam\", \"EdgeTAM\"),\n         (\"edgetam_video\", \"EdgeTamVideo\"),\n         (\"edgetam_vision_model\", \"EdgeTamVisionModel\"),\n-        (\"efficientformer\", \"EfficientFormer\"),\n         (\"efficientloftr\", \"EfficientLoFTR\"),\n         (\"efficientnet\", \"EfficientNet\"),\n         (\"electra\", \"ELECTRA\"),\n@@ -591,7 +567,6 @@\n         (\"ernie\", \"ERNIE\"),\n         (\"ernie4_5\", \"Ernie4_5\"),\n         (\"ernie4_5_moe\", \"Ernie4_5_MoE\"),\n-        (\"ernie_m\", \"ErnieM\"),\n         (\"esm\", \"ESM\"),\n         (\"evolla\", \"Evolla\"),\n         (\"exaone4\", \"EXAONE-4.0\"),\n@@ -641,14 +616,12 @@\n         (\"gpt_neox_japanese\", \"GPT NeoX Japanese\"),\n         (\"gpt_oss\", \"GptOss\"),\n         (\"gptj\", \"GPT-J\"),\n-        (\"gptsan-japanese\", \"GPTSAN-japanese\"),\n         (\"granite\", \"Granite\"),\n         (\"granite_speech\", \"GraniteSpeech\"),\n         (\"granitemoe\", \"GraniteMoeMoe\"),\n         (\"granitemoehybrid\", \"GraniteMoeHybrid\"),\n         (\"granitemoeshared\", \"GraniteMoeSharedMoe\"),\n         (\"granitevision\", \"LLaVA-NeXT\"),\n-        (\"graphormer\", \"Graphormer\"),\n         (\"grounding-dino\", \"Grounding DINO\"),\n         (\"groupvit\", \"GroupViT\"),\n         (\"helium\", \"Helium\"),\n@@ -673,7 +646,6 @@\n         (\"jamba\", \"Jamba\"),\n         (\"janus\", \"Janus\"),\n         (\"jetmoe\", \"JetMoe\"),\n-        (\"jukebox\", \"Jukebox\"),\n         (\"kosmos-2\", \"KOSMOS-2\"),\n         (\"kosmos-2.5\", \"KOSMOS-2.5\"),\n         (\"kyutai_speech_to_text\", \"KyutaiSpeechToText\"),\n@@ -714,8 +686,6 @@\n         (\"matcha\", \"MatCha\"),\n         (\"mbart\", \"mBART\"),\n         (\"mbart50\", \"mBART-50\"),\n-        (\"mctct\", \"M-CTC-T\"),\n-        (\"mega\", \"MEGA\"),\n         (\"megatron-bert\", \"Megatron-BERT\"),\n         (\"megatron_gpt2\", \"Megatron-GPT2\"),\n         (\"metaclip_2\", \"MetaCLIP 2\"),\n@@ -748,9 +718,7 @@\n         (\"musicgen_melody\", \"MusicGen Melody\"),\n         (\"mvp\", \"MVP\"),\n         (\"myt5\", \"myt5\"),\n-        (\"nat\", \"NAT\"),\n         (\"nemotron\", \"Nemotron\"),\n-        (\"nezha\", \"Nezha\"),\n         (\"nllb\", \"NLLB\"),\n         (\"nllb-moe\", \"NLLB-MOE\"),\n         (\"nougat\", \"Nougat\"),\n@@ -761,7 +729,6 @@\n         (\"olmoe\", \"OLMoE\"),\n         (\"omdet-turbo\", \"OmDet-Turbo\"),\n         (\"oneformer\", \"OneFormer\"),\n-        (\"open-llama\", \"OpenLlama\"),\n         (\"openai-gpt\", \"OpenAI GPT\"),\n         (\"opt\", \"OPT\"),\n         (\"ovis2\", \"Ovis2\"),\n@@ -792,7 +759,6 @@\n         (\"prophetnet\", \"ProphetNet\"),\n         (\"pvt\", \"PVT\"),\n         (\"pvt_v2\", \"PVTv2\"),\n-        (\"qdqbert\", \"QDQBert\"),\n         (\"qwen2\", \"Qwen2\"),\n         (\"qwen2_5_omni\", \"Qwen2_5Omni\"),\n         (\"qwen2_5_vl\", \"Qwen2_5_VL\"),\n@@ -811,13 +777,11 @@\n         (\"qwen3_vl_moe_text\", \"Qwen3VLMoe\"),\n         (\"qwen3_vl_text\", \"Qwen3VL\"),\n         (\"rag\", \"RAG\"),\n-        (\"realm\", \"REALM\"),\n         (\"recurrent_gemma\", \"RecurrentGemma\"),\n         (\"reformer\", \"Reformer\"),\n         (\"regnet\", \"RegNet\"),\n         (\"rembert\", \"RemBERT\"),\n         (\"resnet\", \"ResNet\"),\n-        (\"retribert\", \"RetriBERT\"),\n         (\"roberta\", \"RoBERTa\"),\n         (\"roberta-prelayernorm\", \"RoBERTa-PreLayerNorm\"),\n         (\"roc_bert\", \"RoCBert\"),\n@@ -851,7 +815,6 @@\n         (\"smolvlm_vision\", \"SmolVLMVisionTransformer\"),\n         (\"speech-encoder-decoder\", \"Speech Encoder decoder\"),\n         (\"speech_to_text\", \"Speech2Text\"),\n-        (\"speech_to_text_2\", \"Speech2Text2\"),\n         (\"speecht5\", \"SpeechT5\"),\n         (\"splinter\", \"Splinter\"),\n         (\"squeezebert\", \"SqueezeBERT\"),\n@@ -869,17 +832,13 @@\n         (\"t5v1.1\", \"T5v1.1\"),\n         (\"table-transformer\", \"Table Transformer\"),\n         (\"tapas\", \"TAPAS\"),\n-        (\"tapex\", \"TAPEX\"),\n         (\"textnet\", \"TextNet\"),\n         (\"time_series_transformer\", \"Time Series Transformer\"),\n         (\"timesfm\", \"TimesFm\"),\n         (\"timesformer\", \"TimeSformer\"),\n         (\"timm_backbone\", \"TimmBackbone\"),\n         (\"timm_wrapper\", \"TimmWrapperModel\"),\n-        (\"trajectory_transformer\", \"Trajectory Transformer\"),\n-        (\"transfo-xl\", \"Transformer-XL\"),\n         (\"trocr\", \"TrOCR\"),\n-        (\"tvlt\", \"TVLT\"),\n         (\"tvp\", \"TVP\"),\n         (\"udop\", \"UDOP\"),\n         (\"ul2\", \"UL2\"),\n@@ -888,7 +847,6 @@\n         (\"unispeech-sat\", \"UniSpeechSat\"),\n         (\"univnet\", \"UnivNet\"),\n         (\"upernet\", \"UPerNet\"),\n-        (\"van\", \"VAN\"),\n         (\"vaultgemma\", \"VaultGemma\"),\n         (\"video_llama_3\", \"VideoLlama3\"),\n         (\"video_llama_3_vision\", \"VideoLlama3Vision\"),\n@@ -900,7 +858,6 @@\n         (\"vision-text-dual-encoder\", \"VisionTextDualEncoder\"),\n         (\"visual_bert\", \"VisualBERT\"),\n         (\"vit\", \"ViT\"),\n-        (\"vit_hybrid\", \"ViT Hybrid\"),\n         (\"vit_mae\", \"ViTMAE\"),\n         (\"vit_msn\", \"ViTMSN\"),\n         (\"vitdet\", \"VitDet\"),\n@@ -922,7 +879,6 @@\n         (\"xcodec\", \"X-CODEC\"),\n         (\"xglm\", \"XGLM\"),\n         (\"xlm\", \"XLM\"),\n-        (\"xlm-prophetnet\", \"XLM-ProphetNet\"),\n         (\"xlm-roberta\", \"XLM-RoBERTa\"),\n         (\"xlm-roberta-xl\", \"XLM-RoBERTa-XL\"),\n         (\"xlm-v\", \"XLM-V\"),\n@@ -941,32 +897,7 @@\n \n # This is tied to the processing `-` -> `_` in `model_type_to_module_name`. For example, instead of putting\n # `transfo-xl` (as in `CONFIG_MAPPING_NAMES`), we should use `transfo_xl`.\n-DEPRECATED_MODELS = [\n-    \"bort\",\n-    \"deta\",\n-    \"efficientformer\",\n-    \"ernie_m\",\n-    \"gptsan_japanese\",\n-    \"graphormer\",\n-    \"jukebox\",\n-    \"mctct\",\n-    \"mega\",\n-    \"mmbt\",\n-    \"nat\",\n-    \"nezha\",\n-    \"open_llama\",\n-    \"qdqbert\",\n-    \"realm\",\n-    \"retribert\",\n-    \"speech_to_text_2\",\n-    \"tapex\",\n-    \"trajectory_transformer\",\n-    \"transfo_xl\",\n-    \"tvlt\",\n-    \"van\",\n-    \"vit_hybrid\",\n-    \"xlm_prophetnet\",\n-]\n+DEPRECATED_MODELS = []\n \n SPECIAL_MODEL_TYPE_TO_MODULE_NAME = OrderedDict[str, str](\n     ["
        },
        {
            "sha": "4a7ba3272238a80a8acd6f9335972a4aff688176",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f311047860d46367d51d2d5b280286b10ba9466/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f311047860d46367d51d2d5b280286b10ba9466/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=9f311047860d46367d51d2d5b280286b10ba9466",
            "patch": "@@ -50,7 +50,6 @@\n         (\"hubert\", \"Wav2Vec2FeatureExtractor\"),\n         (\"kyutai_speech_to_text\", \"KyutaiSpeechToTextFeatureExtractor\"),\n         (\"markuplm\", \"MarkupLMFeatureExtractor\"),\n-        (\"mctct\", \"MCTCTFeatureExtractor\"),\n         (\"mimi\", \"EncodecFeatureExtractor\"),\n         (\"moonshine\", \"Wav2Vec2FeatureExtractor\"),\n         (\"moshi\", \"EncodecFeatureExtractor\"),"
        },
        {
            "sha": "f6f28ad04658ebdc6006546aac7f6c9b992aa142",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f311047860d46367d51d2d5b280286b10ba9466/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f311047860d46367d51d2d5b280286b10ba9466/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=9f311047860d46367d51d2d5b280286b10ba9466",
            "patch": "@@ -89,15 +89,13 @@\n             (\"deit\", (\"DeiTImageProcessor\", \"DeiTImageProcessorFast\")),\n             (\"depth_anything\", (\"DPTImageProcessor\", \"DPTImageProcessorFast\")),\n             (\"depth_pro\", (\"DepthProImageProcessor\", \"DepthProImageProcessorFast\")),\n-            (\"deta\", (\"DetaImageProcessor\", None)),\n             (\"detr\", (\"DetrImageProcessor\", \"DetrImageProcessorFast\")),\n             (\"dinat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"dinov2\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n             (\"dinov3_vit\", (None, \"DINOv3ViTImageProcessorFast\")),\n             (\"donut-swin\", (\"DonutImageProcessor\", \"DonutImageProcessorFast\")),\n             (\"dpt\", (\"DPTImageProcessor\", \"DPTImageProcessorFast\")),\n             (\"edgetam\", (None, \"Sam2ImageProcessorFast\")),\n-            (\"efficientformer\", (\"EfficientFormerImageProcessor\", None)),\n             (\"efficientloftr\", (\"EfficientLoFTRImageProcessor\", \"EfficientLoFTRImageProcessorFast\")),\n             (\"efficientnet\", (\"EfficientNetImageProcessor\", \"EfficientNetImageProcessorFast\")),\n             (\"emu3\", (\"Emu3ImageProcessor\", None)),\n@@ -149,7 +147,6 @@\n             (\"mobilenet_v2\", (\"MobileNetV2ImageProcessor\", \"MobileNetV2ImageProcessorFast\")),\n             (\"mobilevit\", (\"MobileViTImageProcessor\", \"MobileViTImageProcessorFast\")),\n             (\"mobilevitv2\", (\"MobileViTImageProcessor\", \"MobileViTImageProcessorFast\")),\n-            (\"nat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"nougat\", (\"NougatImageProcessor\", \"NougatImageProcessorFast\")),\n             (\"omdet-turbo\", (\"DetrImageProcessor\", \"DetrImageProcessorFast\")),\n             (\"oneformer\", (\"OneFormerImageProcessor\", \"OneFormerImageProcessorFast\")),\n@@ -195,18 +192,15 @@\n             (\"timesformer\", (\"VideoMAEImageProcessor\", None)),\n             (\"timm_wrapper\", (\"TimmWrapperImageProcessor\", None)),\n             (\"trocr\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n-            (\"tvlt\", (\"TvltImageProcessor\", None)),\n             (\"tvp\", (\"TvpImageProcessor\", \"TvpImageProcessorFast\")),\n             (\"udop\", (\"LayoutLMv3ImageProcessor\", \"LayoutLMv3ImageProcessorFast\")),\n             (\"upernet\", (\"SegformerImageProcessor\", \"SegformerImageProcessorFast\")),\n-            (\"van\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"video_llama_3\", (\"VideoLlama3ImageProcessor\", \"VideoLlama3ImageProcessorFast\")),\n             (\"video_llava\", (\"VideoLlavaImageProcessor\", None)),\n             (\"videomae\", (\"VideoMAEImageProcessor\", None)),\n             (\"vilt\", (\"ViltImageProcessor\", \"ViltImageProcessorFast\")),\n             (\"vipllava\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"vit\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n-            (\"vit_hybrid\", (\"ViTHybridImageProcessor\", None)),\n             (\"vit_mae\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"vit_msn\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"vitmatte\", (\"VitMatteImageProcessor\", \"VitMatteImageProcessorFast\")),"
        },
        {
            "sha": "bedc66fc37bba661a6a14548e546190b77b4768e",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 79,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f311047860d46367d51d2d5b280286b10ba9466/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f311047860d46367d51d2d5b280286b10ba9466/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=9f311047860d46367d51d2d5b280286b10ba9466",
            "patch": "@@ -119,7 +119,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"deformable_detr\", \"DeformableDetrModel\"),\n         (\"deit\", \"DeiTModel\"),\n         (\"depth_pro\", \"DepthProModel\"),\n-        (\"deta\", \"DetaModel\"),\n         (\"detr\", \"DetrModel\"),\n         (\"dia\", \"DiaModel\"),\n         (\"diffllama\", \"DiffLlamaModel\"),\n@@ -137,7 +136,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"edgetam\", \"EdgeTamModel\"),\n         (\"edgetam_video\", \"EdgeTamVideoModel\"),\n         (\"edgetam_vision_model\", \"EdgeTamVisionModel\"),\n-        (\"efficientformer\", \"EfficientFormerModel\"),\n         (\"efficientloftr\", \"EfficientLoFTRModel\"),\n         (\"efficientnet\", \"EfficientNetModel\"),\n         (\"electra\", \"ElectraModel\"),\n@@ -146,7 +144,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"ernie\", \"ErnieModel\"),\n         (\"ernie4_5\", \"Ernie4_5Model\"),\n         (\"ernie4_5_moe\", \"Ernie4_5_MoeModel\"),\n-        (\"ernie_m\", \"ErnieMModel\"),\n         (\"esm\", \"EsmModel\"),\n         (\"evolla\", \"EvollaModel\"),\n         (\"exaone4\", \"Exaone4Model\"),\n@@ -193,12 +190,10 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"gpt_neox_japanese\", \"GPTNeoXJapaneseModel\"),\n         (\"gpt_oss\", \"GptOssModel\"),\n         (\"gptj\", \"GPTJModel\"),\n-        (\"gptsan-japanese\", \"GPTSanJapaneseForConditionalGeneration\"),\n         (\"granite\", \"GraniteModel\"),\n         (\"granitemoe\", \"GraniteMoeModel\"),\n         (\"granitemoehybrid\", \"GraniteMoeHybridModel\"),\n         (\"granitemoeshared\", \"GraniteMoeSharedModel\"),\n-        (\"graphormer\", \"GraphormerModel\"),\n         (\"grounding-dino\", \"GroundingDinoModel\"),\n         (\"groupvit\", \"GroupViTModel\"),\n         (\"helium\", \"HeliumModel\"),\n@@ -222,7 +217,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"jamba\", \"JambaModel\"),\n         (\"janus\", \"JanusModel\"),\n         (\"jetmoe\", \"JetMoeModel\"),\n-        (\"jukebox\", \"JukeboxModel\"),\n         (\"kosmos-2\", \"Kosmos2Model\"),\n         (\"kosmos-2.5\", \"Kosmos2_5Model\"),\n         (\"kyutai_speech_to_text\", \"KyutaiSpeechToTextModel\"),\n@@ -257,8 +251,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"maskformer\", \"MaskFormerModel\"),\n         (\"maskformer-swin\", \"MaskFormerSwinModel\"),\n         (\"mbart\", \"MBartModel\"),\n-        (\"mctct\", \"MCTCTModel\"),\n-        (\"mega\", \"MegaModel\"),\n         (\"megatron-bert\", \"MegatronBertModel\"),\n         (\"metaclip_2\", \"MetaClip2Model\"),\n         (\"mgp-str\", \"MgpstrForSceneTextRecognition\"),\n@@ -287,9 +279,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"musicgen\", \"MusicgenModel\"),\n         (\"musicgen_melody\", \"MusicgenMelodyModel\"),\n         (\"mvp\", \"MvpModel\"),\n-        (\"nat\", \"NatModel\"),\n         (\"nemotron\", \"NemotronModel\"),\n-        (\"nezha\", \"NezhaModel\"),\n         (\"nllb-moe\", \"NllbMoeModel\"),\n         (\"nystromformer\", \"NystromformerModel\"),\n         (\"olmo\", \"OlmoModel\"),\n@@ -298,7 +288,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"olmoe\", \"OlmoeModel\"),\n         (\"omdet-turbo\", \"OmDetTurboForObjectDetection\"),\n         (\"oneformer\", \"OneFormerModel\"),\n-        (\"open-llama\", \"OpenLlamaModel\"),\n         (\"openai-gpt\", \"OpenAIGPTModel\"),\n         (\"opt\", \"OPTModel\"),\n         (\"ovis2\", \"Ovis2Model\"),\n@@ -324,7 +313,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"prophetnet\", \"ProphetNetModel\"),\n         (\"pvt\", \"PvtModel\"),\n         (\"pvt_v2\", \"PvtV2Model\"),\n-        (\"qdqbert\", \"QDQBertModel\"),\n         (\"qwen2\", \"Qwen2Model\"),\n         (\"qwen2_5_vl\", \"Qwen2_5_VLModel\"),\n         (\"qwen2_5_vl_text\", \"Qwen2_5_VLTextModel\"),\n@@ -344,7 +332,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"regnet\", \"RegNetModel\"),\n         (\"rembert\", \"RemBertModel\"),\n         (\"resnet\", \"ResNetModel\"),\n-        (\"retribert\", \"RetriBertModel\"),\n         (\"roberta\", \"RobertaModel\"),\n         (\"roberta-prelayernorm\", \"RobertaPreLayerNormModel\"),\n         (\"roc_bert\", \"RoCBertModel\"),\n@@ -395,16 +382,12 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"timesformer\", \"TimesformerModel\"),\n         (\"timm_backbone\", \"TimmBackbone\"),\n         (\"timm_wrapper\", \"TimmWrapperModel\"),\n-        (\"trajectory_transformer\", \"TrajectoryTransformerModel\"),\n-        (\"transfo-xl\", \"TransfoXLModel\"),\n-        (\"tvlt\", \"TvltModel\"),\n         (\"tvp\", \"TvpModel\"),\n         (\"udop\", \"UdopModel\"),\n         (\"umt5\", \"UMT5Model\"),\n         (\"unispeech\", \"UniSpeechModel\"),\n         (\"unispeech-sat\", \"UniSpeechSatModel\"),\n         (\"univnet\", \"UnivNetModel\"),\n-        (\"van\", \"VanModel\"),\n         (\"vaultgemma\", \"VaultGemmaModel\"),\n         (\"video_llama_3\", \"VideoLlama3Model\"),\n         (\"video_llama_3_vision\", \"VideoLlama3VisionModel\"),\n@@ -415,7 +398,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"vision-text-dual-encoder\", \"VisionTextDualEncoderModel\"),\n         (\"visual_bert\", \"VisualBertModel\"),\n         (\"vit\", \"ViTModel\"),\n-        (\"vit_hybrid\", \"ViTHybridModel\"),\n         (\"vit_mae\", \"ViTMAEModel\"),\n         (\"vit_msn\", \"ViTMSNModel\"),\n         (\"vitdet\", \"VitDetModel\"),\n@@ -433,7 +415,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"xcodec\", \"XcodecModel\"),\n         (\"xglm\", \"XGLMModel\"),\n         (\"xlm\", \"XLMModel\"),\n-        (\"xlm-prophetnet\", \"XLMProphetNetModel\"),\n         (\"xlm-roberta\", \"XLMRobertaModel\"),\n         (\"xlm-roberta-xl\", \"XLMRobertaXLModel\"),\n         (\"xlnet\", \"XLNetModel\"),\n@@ -478,7 +459,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"gpt-sw3\", \"GPT2LMHeadModel\"),\n         (\"gpt2\", \"GPT2LMHeadModel\"),\n         (\"gpt_bigcode\", \"GPTBigCodeForCausalLM\"),\n-        (\"gptsan-japanese\", \"GPTSanJapaneseForConditionalGeneration\"),\n         (\"hiera\", \"HieraForPreTraining\"),\n         (\"ibert\", \"IBertForMaskedLM\"),\n         (\"idefics\", \"IdeficsForVisionText2Text\"),\n@@ -495,7 +475,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"lxmert\", \"LxmertForPreTraining\"),\n         (\"mamba\", \"MambaForCausalLM\"),\n         (\"mamba2\", \"Mamba2ForCausalLM\"),\n-        (\"mega\", \"MegaForMaskedLM\"),\n         (\"megatron-bert\", \"MegatronBertForPreTraining\"),\n         (\"mistral3\", \"Mistral3ForConditionalGeneration\"),\n         (\"mllama\", \"MllamaForConditionalGeneration\"),\n@@ -504,12 +483,10 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mpt\", \"MptForCausalLM\"),\n         (\"mra\", \"MraForMaskedLM\"),\n         (\"mvp\", \"MvpForConditionalGeneration\"),\n-        (\"nezha\", \"NezhaForPreTraining\"),\n         (\"nllb-moe\", \"NllbMoeForConditionalGeneration\"),\n         (\"openai-gpt\", \"OpenAIGPTLMHeadModel\"),\n         (\"paligemma\", \"PaliGemmaForConditionalGeneration\"),\n         (\"qwen2_audio\", \"Qwen2AudioForConditionalGeneration\"),\n-        (\"retribert\", \"RetriBertModel\"),\n         (\"roberta\", \"RobertaForMaskedLM\"),\n         (\"roberta-prelayernorm\", \"RobertaPreLayerNormForMaskedLM\"),\n         (\"roc_bert\", \"RoCBertForPreTraining\"),\n@@ -520,8 +497,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"t5\", \"T5ForConditionalGeneration\"),\n         (\"t5gemma\", \"T5GemmaForConditionalGeneration\"),\n         (\"tapas\", \"TapasForMaskedLM\"),\n-        (\"transfo-xl\", \"TransfoXLLMHeadModel\"),\n-        (\"tvlt\", \"TvltForPreTraining\"),\n         (\"unispeech\", \"UniSpeechForPreTraining\"),\n         (\"unispeech-sat\", \"UniSpeechSatForPreTraining\"),\n         (\"video_llava\", \"VideoLlavaForConditionalGeneration\"),\n@@ -579,7 +554,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"gpt_neox\", \"GPTNeoXForCausalLM\"),\n         (\"gpt_neox_japanese\", \"GPTNeoXJapaneseForCausalLM\"),\n         (\"gptj\", \"GPTJForCausalLM\"),\n-        (\"gptsan-japanese\", \"GPTSanJapaneseForConditionalGeneration\"),\n         (\"ibert\", \"IBertForMaskedLM\"),\n         (\"layoutlm\", \"LayoutLMForMaskedLM\"),\n         (\"led\", \"LEDForConditionalGeneration\"),\n@@ -590,22 +564,19 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mamba\", \"MambaForCausalLM\"),\n         (\"mamba2\", \"Mamba2ForCausalLM\"),\n         (\"marian\", \"MarianMTModel\"),\n-        (\"mega\", \"MegaForMaskedLM\"),\n         (\"megatron-bert\", \"MegatronBertForCausalLM\"),\n         (\"mobilebert\", \"MobileBertForMaskedLM\"),\n         (\"moonshine\", \"MoonshineForConditionalGeneration\"),\n         (\"mpnet\", \"MPNetForMaskedLM\"),\n         (\"mpt\", \"MptForCausalLM\"),\n         (\"mra\", \"MraForMaskedLM\"),\n         (\"mvp\", \"MvpForConditionalGeneration\"),\n-        (\"nezha\", \"NezhaForMaskedLM\"),\n         (\"nllb-moe\", \"NllbMoeForConditionalGeneration\"),\n         (\"nystromformer\", \"NystromformerForMaskedLM\"),\n         (\"openai-gpt\", \"OpenAIGPTLMHeadModel\"),\n         (\"pegasus_x\", \"PegasusXForConditionalGeneration\"),\n         (\"plbart\", \"PLBartForConditionalGeneration\"),\n         (\"pop2piano\", \"Pop2PianoForConditionalGeneration\"),\n-        (\"qdqbert\", \"QDQBertForMaskedLM\"),\n         (\"reformer\", \"ReformerModelWithLMHead\"),\n         (\"rembert\", \"RemBertForMaskedLM\"),\n         (\"roberta\", \"RobertaForMaskedLM\"),\n@@ -619,7 +590,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"t5\", \"T5ForConditionalGeneration\"),\n         (\"t5gemma\", \"T5GemmaForConditionalGeneration\"),\n         (\"tapas\", \"TapasForMaskedLM\"),\n-        (\"transfo-xl\", \"TransfoXLLMHeadModel\"),\n         (\"wav2vec2\", \"Wav2Vec2ForMaskedLM\"),\n         (\"whisper\", \"WhisperForConditionalGeneration\"),\n         (\"xlm\", \"XLMWithLMHeadModel\"),\n@@ -713,7 +683,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mamba2\", \"Mamba2ForCausalLM\"),\n         (\"marian\", \"MarianForCausalLM\"),\n         (\"mbart\", \"MBartForCausalLM\"),\n-        (\"mega\", \"MegaForCausalLM\"),\n         (\"megatron-bert\", \"MegatronBertForCausalLM\"),\n         (\"minimax\", \"MiniMaxForCausalLM\"),\n         (\"ministral\", \"MinistralForCausalLM\"),\n@@ -731,7 +700,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"olmo2\", \"Olmo2ForCausalLM\"),\n         (\"olmo3\", \"Olmo3ForCausalLM\"),\n         (\"olmoe\", \"OlmoeForCausalLM\"),\n-        (\"open-llama\", \"OpenLlamaForCausalLM\"),\n         (\"openai-gpt\", \"OpenAIGPTLMHeadModel\"),\n         (\"opt\", \"OPTForCausalLM\"),\n         (\"pegasus\", \"PegasusForCausalLM\"),\n@@ -742,7 +710,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"phimoe\", \"PhimoeForCausalLM\"),\n         (\"plbart\", \"PLBartForCausalLM\"),\n         (\"prophetnet\", \"ProphetNetForCausalLM\"),\n-        (\"qdqbert\", \"QDQBertLMHeadModel\"),\n         (\"qwen2\", \"Qwen2ForCausalLM\"),\n         (\"qwen2_moe\", \"Qwen2MoeForCausalLM\"),\n         (\"qwen3\", \"Qwen3ForCausalLM\"),\n@@ -758,16 +725,13 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"rwkv\", \"RwkvForCausalLM\"),\n         (\"seed_oss\", \"SeedOssForCausalLM\"),\n         (\"smollm3\", \"SmolLM3ForCausalLM\"),\n-        (\"speech_to_text_2\", \"Speech2Text2ForCausalLM\"),\n         (\"stablelm\", \"StableLmForCausalLM\"),\n         (\"starcoder2\", \"Starcoder2ForCausalLM\"),\n-        (\"transfo-xl\", \"TransfoXLLMHeadModel\"),\n         (\"trocr\", \"TrOCRForCausalLM\"),\n         (\"vaultgemma\", \"VaultGemmaForCausalLM\"),\n         (\"whisper\", \"WhisperForCausalLM\"),\n         (\"xglm\", \"XGLMForCausalLM\"),\n         (\"xlm\", \"XLMWithLMHeadModel\"),\n-        (\"xlm-prophetnet\", \"XLMProphetNetForCausalLM\"),\n         (\"xlm-roberta\", \"XLMRobertaForCausalLM\"),\n         (\"xlm-roberta-xl\", \"XLMRobertaXLForCausalLM\"),\n         (\"xlnet\", \"XLNetLMHeadModel\"),\n@@ -793,15 +757,13 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"deformable_detr\", \"DeformableDetrModel\"),\n         (\"deit\", \"DeiTModel\"),\n         (\"depth_pro\", \"DepthProModel\"),\n-        (\"deta\", \"DetaModel\"),\n         (\"detr\", \"DetrModel\"),\n         (\"dinat\", \"DinatModel\"),\n         (\"dinov2\", \"Dinov2Model\"),\n         (\"dinov2_with_registers\", \"Dinov2WithRegistersModel\"),\n         (\"dinov3_convnext\", \"DINOv3ConvNextModel\"),\n         (\"dinov3_vit\", \"DINOv3ViTModel\"),\n         (\"dpt\", \"DPTModel\"),\n-        (\"efficientformer\", \"EfficientFormerModel\"),\n         (\"efficientnet\", \"EfficientNetModel\"),\n         (\"focalnet\", \"FocalNetModel\"),\n         (\"glpn\", \"GLPNModel\"),\n@@ -816,7 +778,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mobilenet_v2\", \"MobileNetV2Model\"),\n         (\"mobilevit\", \"MobileViTModel\"),\n         (\"mobilevitv2\", \"MobileViTV2Model\"),\n-        (\"nat\", \"NatModel\"),\n         (\"poolformer\", \"PoolFormerModel\"),\n         (\"pvt\", \"PvtModel\"),\n         (\"regnet\", \"RegNetModel\"),\n@@ -831,10 +792,8 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"timesformer\", \"TimesformerModel\"),\n         (\"timm_backbone\", \"TimmBackbone\"),\n         (\"timm_wrapper\", \"TimmWrapperModel\"),\n-        (\"van\", \"VanModel\"),\n         (\"videomae\", \"VideoMAEModel\"),\n         (\"vit\", \"ViTModel\"),\n-        (\"vit_hybrid\", \"ViTHybridModel\"),\n         (\"vit_mae\", \"ViTMAEModel\"),\n         (\"vit_msn\", \"ViTMSNModel\"),\n         (\"vitdet\", \"VitDetModel\"),\n@@ -879,13 +838,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"dinov2\", \"Dinov2ForImageClassification\"),\n         (\"dinov2_with_registers\", \"Dinov2WithRegistersForImageClassification\"),\n         (\"donut-swin\", \"DonutSwinForImageClassification\"),\n-        (\n-            \"efficientformer\",\n-            (\n-                \"EfficientFormerForImageClassification\",\n-                \"EfficientFormerForImageClassificationWithTeacher\",\n-            ),\n-        ),\n         (\"efficientnet\", \"EfficientNetForImageClassification\"),\n         (\"focalnet\", \"FocalNetForImageClassification\"),\n         (\"hgnet_v2\", \"HGNetV2ForImageClassification\"),\n@@ -901,7 +853,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mobilenet_v2\", \"MobileNetV2ForImageClassification\"),\n         (\"mobilevit\", \"MobileViTForImageClassification\"),\n         (\"mobilevitv2\", \"MobileViTV2ForImageClassification\"),\n-        (\"nat\", \"NatForImageClassification\"),\n         (\n             \"perceiver\",\n             (\n@@ -924,9 +875,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"swinv2\", \"Swinv2ForImageClassification\"),\n         (\"textnet\", \"TextNetForImageClassification\"),\n         (\"timm_wrapper\", \"TimmWrapperForImageClassification\"),\n-        (\"van\", \"VanForImageClassification\"),\n         (\"vit\", \"ViTForImageClassification\"),\n-        (\"vit_hybrid\", \"ViTHybridForImageClassification\"),\n         (\"vit_msn\", \"ViTMSNForImageClassification\"),\n     ]\n )\n@@ -1097,17 +1046,14 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"longformer\", \"LongformerForMaskedLM\"),\n         (\"luke\", \"LukeForMaskedLM\"),\n         (\"mbart\", \"MBartForConditionalGeneration\"),\n-        (\"mega\", \"MegaForMaskedLM\"),\n         (\"megatron-bert\", \"MegatronBertForMaskedLM\"),\n         (\"mobilebert\", \"MobileBertForMaskedLM\"),\n         (\"modernbert\", \"ModernBertForMaskedLM\"),\n         (\"mpnet\", \"MPNetForMaskedLM\"),\n         (\"mra\", \"MraForMaskedLM\"),\n         (\"mvp\", \"MvpForConditionalGeneration\"),\n-        (\"nezha\", \"NezhaForMaskedLM\"),\n         (\"nystromformer\", \"NystromformerForMaskedLM\"),\n         (\"perceiver\", \"PerceiverForMaskedLM\"),\n-        (\"qdqbert\", \"QDQBertForMaskedLM\"),\n         (\"reformer\", \"ReformerForMaskedLM\"),\n         (\"rembert\", \"RemBertForMaskedLM\"),\n         (\"roberta\", \"RobertaForMaskedLM\"),\n@@ -1132,7 +1078,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"d_fine\", \"DFineForObjectDetection\"),\n         (\"dab-detr\", \"DabDetrForObjectDetection\"),\n         (\"deformable_detr\", \"DeformableDetrForObjectDetection\"),\n-        (\"deta\", \"DetaForObjectDetection\"),\n         (\"detr\", \"DetrForObjectDetection\"),\n         (\"rt_detr\", \"RTDetrForObjectDetection\"),\n         (\"rt_detr_v2\", \"RTDetrV2ForObjectDetection\"),\n@@ -1173,7 +1118,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"blenderbot-small\", \"BlenderbotSmallForConditionalGeneration\"),\n         (\"encoder-decoder\", \"EncoderDecoderModel\"),\n         (\"fsmt\", \"FSMTForConditionalGeneration\"),\n-        (\"gptsan-japanese\", \"GPTSanJapaneseForConditionalGeneration\"),\n         (\"granite_speech\", \"GraniteSpeechForConditionalGeneration\"),\n         (\"led\", \"LEDForConditionalGeneration\"),\n         (\"longt5\", \"LongT5ForConditionalGeneration\"),\n@@ -1195,7 +1139,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"t5gemma\", \"T5GemmaForConditionalGeneration\"),\n         (\"umt5\", \"UMT5ForConditionalGeneration\"),\n         (\"voxtral\", \"VoxtralForConditionalGeneration\"),\n-        (\"xlm-prophetnet\", \"XLMProphetNetForConditionalGeneration\"),\n     ]\n )\n \n@@ -1241,7 +1184,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"doge\", \"DogeForSequenceClassification\"),\n         (\"electra\", \"ElectraForSequenceClassification\"),\n         (\"ernie\", \"ErnieForSequenceClassification\"),\n-        (\"ernie_m\", \"ErnieMForSequenceClassification\"),\n         (\"esm\", \"EsmForSequenceClassification\"),\n         (\"exaone4\", \"Exaone4ForSequenceClassification\"),\n         (\"falcon\", \"FalconForSequenceClassification\"),\n@@ -1277,7 +1219,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"luke\", \"LukeForSequenceClassification\"),\n         (\"markuplm\", \"MarkupLMForSequenceClassification\"),\n         (\"mbart\", \"MBartForSequenceClassification\"),\n-        (\"mega\", \"MegaForSequenceClassification\"),\n         (\"megatron-bert\", \"MegatronBertForSequenceClassification\"),\n         (\"minimax\", \"MiniMaxForSequenceClassification\"),\n         (\"ministral\", \"MinistralForSequenceClassification\"),\n@@ -1292,9 +1233,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mt5\", \"MT5ForSequenceClassification\"),\n         (\"mvp\", \"MvpForSequenceClassification\"),\n         (\"nemotron\", \"NemotronForSequenceClassification\"),\n-        (\"nezha\", \"NezhaForSequenceClassification\"),\n         (\"nystromformer\", \"NystromformerForSequenceClassification\"),\n-        (\"open-llama\", \"OpenLlamaForSequenceClassification\"),\n         (\"openai-gpt\", \"OpenAIGPTForSequenceClassification\"),\n         (\"opt\", \"OPTForSequenceClassification\"),\n         (\"perceiver\", \"PerceiverForSequenceClassification\"),\n@@ -1303,7 +1242,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"phi3\", \"Phi3ForSequenceClassification\"),\n         (\"phimoe\", \"PhimoeForSequenceClassification\"),\n         (\"plbart\", \"PLBartForSequenceClassification\"),\n-        (\"qdqbert\", \"QDQBertForSequenceClassification\"),\n         (\"qwen2\", \"Qwen2ForSequenceClassification\"),\n         (\"qwen2_moe\", \"Qwen2MoeForSequenceClassification\"),\n         (\"qwen3\", \"Qwen3ForSequenceClassification\"),\n@@ -1323,7 +1261,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"t5\", \"T5ForSequenceClassification\"),\n         (\"t5gemma\", \"T5GemmaForSequenceClassification\"),\n         (\"tapas\", \"TapasForSequenceClassification\"),\n-        (\"transfo-xl\", \"TransfoXLForSequenceClassification\"),\n         (\"umt5\", \"UMT5ForSequenceClassification\"),\n         (\"xlm\", \"XLMForSequenceClassification\"),\n         (\"xlm-roberta\", \"XLMRobertaForSequenceClassification\"),\n@@ -1356,7 +1293,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"distilbert\", \"DistilBertForQuestionAnswering\"),\n         (\"electra\", \"ElectraForQuestionAnswering\"),\n         (\"ernie\", \"ErnieForQuestionAnswering\"),\n-        (\"ernie_m\", \"ErnieMForQuestionAnswering\"),\n         (\"exaone4\", \"Exaone4ForQuestionAnswering\"),\n         (\"falcon\", \"FalconForQuestionAnswering\"),\n         (\"flaubert\", \"FlaubertForQuestionAnsweringSimple\"),\n@@ -1377,7 +1313,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"lxmert\", \"LxmertForQuestionAnswering\"),\n         (\"markuplm\", \"MarkupLMForQuestionAnswering\"),\n         (\"mbart\", \"MBartForQuestionAnswering\"),\n-        (\"mega\", \"MegaForQuestionAnswering\"),\n         (\"megatron-bert\", \"MegatronBertForQuestionAnswering\"),\n         (\"minimax\", \"MiniMaxForQuestionAnswering\"),\n         (\"ministral\", \"MinistralForQuestionAnswering\"),\n@@ -1391,10 +1326,8 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mt5\", \"MT5ForQuestionAnswering\"),\n         (\"mvp\", \"MvpForQuestionAnswering\"),\n         (\"nemotron\", \"NemotronForQuestionAnswering\"),\n-        (\"nezha\", \"NezhaForQuestionAnswering\"),\n         (\"nystromformer\", \"NystromformerForQuestionAnswering\"),\n         (\"opt\", \"OPTForQuestionAnswering\"),\n-        (\"qdqbert\", \"QDQBertForQuestionAnswering\"),\n         (\"qwen2\", \"Qwen2ForQuestionAnswering\"),\n         (\"qwen2_moe\", \"Qwen2MoeForQuestionAnswering\"),\n         (\"qwen3\", \"Qwen3ForQuestionAnswering\"),\n@@ -1466,7 +1399,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"distilbert\", \"DistilBertForTokenClassification\"),\n         (\"electra\", \"ElectraForTokenClassification\"),\n         (\"ernie\", \"ErnieForTokenClassification\"),\n-        (\"ernie_m\", \"ErnieMForTokenClassification\"),\n         (\"esm\", \"EsmForTokenClassification\"),\n         (\"exaone4\", \"Exaone4ForTokenClassification\"),\n         (\"falcon\", \"FalconForTokenClassification\"),\n@@ -1493,7 +1425,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"longformer\", \"LongformerForTokenClassification\"),\n         (\"luke\", \"LukeForTokenClassification\"),\n         (\"markuplm\", \"MarkupLMForTokenClassification\"),\n-        (\"mega\", \"MegaForTokenClassification\"),\n         (\"megatron-bert\", \"MegatronBertForTokenClassification\"),\n         (\"minimax\", \"MiniMaxForTokenClassification\"),\n         (\"ministral\", \"MinistralForTokenClassification\"),\n@@ -1506,12 +1437,10 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mra\", \"MraForTokenClassification\"),\n         (\"mt5\", \"MT5ForTokenClassification\"),\n         (\"nemotron\", \"NemotronForTokenClassification\"),\n-        (\"nezha\", \"NezhaForTokenClassification\"),\n         (\"nystromformer\", \"NystromformerForTokenClassification\"),\n         (\"persimmon\", \"PersimmonForTokenClassification\"),\n         (\"phi\", \"PhiForTokenClassification\"),\n         (\"phi3\", \"Phi3ForTokenClassification\"),\n-        (\"qdqbert\", \"QDQBertForTokenClassification\"),\n         (\"qwen2\", \"Qwen2ForTokenClassification\"),\n         (\"qwen2_moe\", \"Qwen2MoeForTokenClassification\"),\n         (\"qwen3\", \"Qwen3ForTokenClassification\"),\n@@ -1553,22 +1482,18 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"distilbert\", \"DistilBertForMultipleChoice\"),\n         (\"electra\", \"ElectraForMultipleChoice\"),\n         (\"ernie\", \"ErnieForMultipleChoice\"),\n-        (\"ernie_m\", \"ErnieMForMultipleChoice\"),\n         (\"flaubert\", \"FlaubertForMultipleChoice\"),\n         (\"fnet\", \"FNetForMultipleChoice\"),\n         (\"funnel\", \"FunnelForMultipleChoice\"),\n         (\"ibert\", \"IBertForMultipleChoice\"),\n         (\"longformer\", \"LongformerForMultipleChoice\"),\n         (\"luke\", \"LukeForMultipleChoice\"),\n-        (\"mega\", \"MegaForMultipleChoice\"),\n         (\"megatron-bert\", \"MegatronBertForMultipleChoice\"),\n         (\"mobilebert\", \"MobileBertForMultipleChoice\"),\n         (\"modernbert\", \"ModernBertForMultipleChoice\"),\n         (\"mpnet\", \"MPNetForMultipleChoice\"),\n         (\"mra\", \"MraForMultipleChoice\"),\n-        (\"nezha\", \"NezhaForMultipleChoice\"),\n         (\"nystromformer\", \"NystromformerForMultipleChoice\"),\n-        (\"qdqbert\", \"QDQBertForMultipleChoice\"),\n         (\"rembert\", \"RemBertForMultipleChoice\"),\n         (\"roberta\", \"RobertaForMultipleChoice\"),\n         (\"roberta-prelayernorm\", \"RobertaPreLayerNormForMultipleChoice\"),\n@@ -1591,8 +1516,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"fnet\", \"FNetForNextSentencePrediction\"),\n         (\"megatron-bert\", \"MegatronBertForNextSentencePrediction\"),\n         (\"mobilebert\", \"MobileBertForNextSentencePrediction\"),\n-        (\"nezha\", \"NezhaForNextSentencePrediction\"),\n-        (\"qdqbert\", \"QDQBertForNextSentencePrediction\"),\n     ]\n )\n \n@@ -1619,7 +1542,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         # Model for Connectionist temporal classification (CTC) mapping\n         (\"data2vec-audio\", \"Data2VecAudioForCTC\"),\n         (\"hubert\", \"HubertForCTC\"),\n-        (\"mctct\", \"MCTCTForCTC\"),\n         (\"parakeet_ctc\", \"ParakeetForCTC\"),\n         (\"sew\", \"SEWForCTC\"),\n         (\"sew-d\", \"SEWDForCTC\"),\n@@ -1713,7 +1635,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"hgnet_v2\", \"HGNetV2Backbone\"),\n         (\"hiera\", \"HieraBackbone\"),\n         (\"maskformer-swin\", \"MaskFormerSwinBackbone\"),\n-        (\"nat\", \"NatBackbone\"),\n         (\"pvt_v2\", \"PvtV2Backbone\"),\n         (\"resnet\", \"ResNetBackbone\"),\n         (\"rt_detr_resnet\", \"RTDetrResNetBackbone\"),"
        },
        {
            "sha": "584ac914323a67b5b01631e95ec4eda6649d92b1",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f311047860d46367d51d2d5b280286b10ba9466/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f311047860d46367d51d2d5b280286b10ba9466/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=9f311047860d46367d51d2d5b280286b10ba9466",
            "patch": "@@ -102,7 +102,6 @@\n         (\"llava_next_video\", \"LlavaNextVideoProcessor\"),\n         (\"llava_onevision\", \"LlavaOnevisionProcessor\"),\n         (\"markuplm\", \"MarkupLMProcessor\"),\n-        (\"mctct\", \"MCTCTProcessor\"),\n         (\"metaclip_2\", \"CLIPProcessor\"),\n         (\"mgp-str\", \"MgpstrProcessor\"),\n         (\"mistral3\", \"PixtralProcessor\"),\n@@ -138,10 +137,8 @@\n         (\"siglip2\", \"Siglip2Processor\"),\n         (\"smolvlm\", \"SmolVLMProcessor\"),\n         (\"speech_to_text\", \"Speech2TextProcessor\"),\n-        (\"speech_to_text_2\", \"Speech2Text2Processor\"),\n         (\"speecht5\", \"SpeechT5Processor\"),\n         (\"trocr\", \"TrOCRProcessor\"),\n-        (\"tvlt\", \"TvltProcessor\"),\n         (\"tvp\", \"TvpProcessor\"),\n         (\"udop\", \"UdopProcessor\"),\n         (\"unispeech\", \"Wav2Vec2Processor\"),"
        },
        {
            "sha": "be0a1f0dd754c7e618da5f55809bcc6ec899260a",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f311047860d46367d51d2d5b280286b10ba9466/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f311047860d46367d51d2d5b280286b10ba9466/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=9f311047860d46367d51d2d5b280286b10ba9466",
            "patch": "@@ -239,7 +239,6 @@\n         (\"ernie\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"ernie4_5\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"ernie4_5_moe\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"ernie_m\", (\"ErnieMTokenizer\" if is_sentencepiece_available() else None, None)),\n         (\"esm\", (\"EsmTokenizer\", None)),\n         (\"evolla\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n@@ -321,7 +320,6 @@\n         (\"gpt_neox_japanese\", (\"GPTNeoXJapaneseTokenizer\", None)),\n         (\"gpt_oss\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"gptj\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"gptsan-japanese\", (\"GPTSanJapaneseTokenizer\", None)),\n         (\"granite\", (\"GPT2Tokenizer\", None)),\n         (\"granite_speech\", (\"GPT2Tokenizer\", None)),\n         (\"granitemoe\", (\"GPT2Tokenizer\", None)),\n@@ -354,7 +352,6 @@\n                 \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n             ),\n         ),\n-        (\"jukebox\", (\"JukeboxTokenizer\", None)),\n         (\n             \"kosmos-2\",\n             (\n@@ -426,7 +423,6 @@\n                 \"MBart50TokenizerFast\" if is_tokenizers_available() else None,\n             ),\n         ),\n-        (\"mega\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"megatron-bert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n             \"metaclip_2\",\n@@ -501,7 +497,6 @@\n         (\"mvp\", (\"MvpTokenizer\", \"MvpTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"myt5\", (\"MyT5Tokenizer\", None)),\n         (\"nemotron\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"nezha\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n             \"nllb\",\n             (\n@@ -590,7 +585,6 @@\n         (\"plbart\", (\"PLBartTokenizer\" if is_sentencepiece_available() else None, None)),\n         (\"pop2piano\", (\"Pop2PianoTokenizer\", None)),\n         (\"prophetnet\", (\"ProphetNetTokenizer\", None)),\n-        (\"qdqbert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n             \"qwen2\",\n             (\n@@ -634,7 +628,6 @@\n         (\"qwen3_vl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n         (\"qwen3_vl_moe\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n         (\"rag\", (\"RagTokenizer\", None)),\n-        (\"realm\", (\"RealmTokenizer\", \"RealmTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n             \"recurrent_gemma\",\n             (\n@@ -656,7 +649,6 @@\n                 \"RemBertTokenizerFast\" if is_tokenizers_available() else None,\n             ),\n         ),\n-        (\"retribert\", (\"RetriBertTokenizer\", \"RetriBertTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"roberta\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n             \"roberta-prelayernorm\",\n@@ -697,7 +689,6 @@\n         (\"smollm3\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"smolvlm\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"speech_to_text\", (\"Speech2TextTokenizer\" if is_sentencepiece_available() else None, None)),\n-        (\"speech_to_text_2\", (\"Speech2Text2Tokenizer\", None)),\n         (\"speecht5\", (\"SpeechT5Tokenizer\" if is_sentencepiece_available() else None, None)),\n         (\"splinter\", (\"SplinterTokenizer\", \"SplinterTokenizerFast\")),\n         (\n@@ -728,8 +719,6 @@\n             ),\n         ),\n         (\"tapas\", (\"TapasTokenizer\", None)),\n-        (\"tapex\", (\"TapexTokenizer\", None)),\n-        (\"transfo-xl\", (\"TransfoXLTokenizer\", None)),\n         (\"trocr\", (\"XLMRobertaTokenizer\", \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"tvp\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n@@ -780,7 +769,6 @@\n             ),\n         ),\n         (\"xlm\", (\"XLMTokenizer\", None)),\n-        (\"xlm-prophetnet\", (\"XLMProphetNetTokenizer\" if is_sentencepiece_available() else None, None)),\n         (\n             \"xlm-roberta\",\n             ("
        },
        {
            "sha": "0ca7ad859590028b8eea310f4362e8d0f8723e89",
            "filename": "src/transformers/models/deprecated/__init__.py",
            "status": "modified",
            "additions": 3,
            "deletions": 24,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f311047860d46367d51d2d5b280286b10ba9466/src%2Ftransformers%2Fmodels%2Fdeprecated%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f311047860d46367d51d2d5b280286b10ba9466/src%2Ftransformers%2Fmodels%2Fdeprecated%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2F__init__.py?ref=9f311047860d46367d51d2d5b280286b10ba9466",
            "patch": "@@ -18,30 +18,9 @@\n \n \n if TYPE_CHECKING:\n-    from .bort import *\n-    from .deta import *\n-    from .efficientformer import *\n-    from .ernie_m import *\n-    from .gptsan_japanese import *\n-    from .graphormer import *\n-    from .jukebox import *\n-    from .mctct import *\n-    from .mega import *\n-    from .mmbt import *\n-    from .nat import *\n-    from .nezha import *\n-    from .open_llama import *\n-    from .qdqbert import *\n-    from .realm import *\n-    from .retribert import *\n-    from .speech_to_text_2 import *\n-    from .tapex import *\n-    from .trajectory_transformer import *\n-    from .transfo_xl import *\n-    from .tvlt import *\n-    from .van import *\n-    from .vit_hybrid import *\n-    from .xlm_prophetnet import *\n+    pass\n+    # Add models to deprecate like:\n+    # from .XXX import *\n else:\n     import sys\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "src/transformers/models/deprecated/bort/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fbort%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fbort%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fbort%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "425438de25ecee986842904c932d16065649394e",
            "filename": "src/transformers/models/deprecated/bort/convert_bort_original_gluonnlp_checkpoint_to_pytorch.py",
            "status": "removed",
            "additions": 0,
            "deletions": 318,
            "changes": 318,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fbort%2Fconvert_bort_original_gluonnlp_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fbort%2Fconvert_bort_original_gluonnlp_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fbort%2Fconvert_bort_original_gluonnlp_checkpoint_to_pytorch.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,318 +0,0 @@\n-# coding=utf-8\n-# Copyright 2020, The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Convert Bort checkpoint.\"\"\"\n-\n-import argparse\n-import os\n-\n-import gluonnlp as nlp\n-import mxnet as mx\n-import numpy as np\n-import torch\n-from gluonnlp.base import get_home_dir\n-from gluonnlp.model.bert import BERTEncoder\n-from gluonnlp.model.utils import _load_vocab\n-from gluonnlp.vocab import Vocab\n-from packaging import version\n-from torch import nn\n-\n-from transformers import BertConfig, BertForMaskedLM, BertModel, RobertaTokenizer\n-from transformers.models.bert.modeling_bert import (\n-    BertIntermediate,\n-    BertLayer,\n-    BertOutput,\n-    BertSelfAttention,\n-    BertSelfOutput,\n-)\n-from transformers.utils import logging\n-\n-\n-if version.parse(nlp.__version__) != version.parse(\"0.8.3\"):\n-    raise Exception(\"requires gluonnlp == 0.8.3\")\n-\n-if version.parse(mx.__version__) != version.parse(\"1.5.0\"):\n-    raise Exception(\"requires mxnet == 1.5.0\")\n-\n-logging.set_verbosity_info()\n-logger = logging.get_logger(__name__)\n-\n-SAMPLE_TEXT = \"The Nymphenburg Palace is a beautiful palace in Munich!\"\n-\n-\n-def convert_bort_checkpoint_to_pytorch(bort_checkpoint_path: str, pytorch_dump_folder_path: str):\n-    \"\"\"\n-    Convert the original Bort checkpoint (based on MXNET and Gluonnlp) to our BERT structure-\n-    \"\"\"\n-\n-    # Original Bort configuration\n-    bort_4_8_768_1024_hparams = {\n-        \"attention_cell\": \"multi_head\",\n-        \"num_layers\": 4,\n-        \"units\": 1024,\n-        \"hidden_size\": 768,\n-        \"max_length\": 512,\n-        \"num_heads\": 8,\n-        \"scaled\": True,\n-        \"dropout\": 0.1,\n-        \"use_residual\": True,\n-        \"embed_size\": 1024,\n-        \"embed_dropout\": 0.1,\n-        \"word_embed\": None,\n-        \"layer_norm_eps\": 1e-5,\n-        \"token_type_vocab_size\": 2,\n-    }\n-\n-    predefined_args = bort_4_8_768_1024_hparams\n-\n-    # Let's construct the original Bort model here\n-    # Taken from official BERT implementation, see:\n-    # https://github.com/alexa/bort/blob/master/bort/bort.py\n-    encoder = BERTEncoder(\n-        attention_cell=predefined_args[\"attention_cell\"],\n-        num_layers=predefined_args[\"num_layers\"],\n-        units=predefined_args[\"units\"],\n-        hidden_size=predefined_args[\"hidden_size\"],\n-        max_length=predefined_args[\"max_length\"],\n-        num_heads=predefined_args[\"num_heads\"],\n-        scaled=predefined_args[\"scaled\"],\n-        dropout=predefined_args[\"dropout\"],\n-        output_attention=False,\n-        output_all_encodings=False,\n-        use_residual=predefined_args[\"use_residual\"],\n-        activation=predefined_args.get(\"activation\", \"gelu\"),\n-        layer_norm_eps=predefined_args.get(\"layer_norm_eps\", None),\n-    )\n-\n-    # Vocab information needs to be fetched first\n-    # It's the same as RoBERTa, so RobertaTokenizer can be used later\n-    vocab_name = \"openwebtext_ccnews_stories_books_cased\"\n-\n-    # Specify download folder to Gluonnlp's vocab\n-    gluon_cache_dir = os.path.join(get_home_dir(), \"models\")\n-    bort_vocab = _load_vocab(vocab_name, None, gluon_cache_dir, cls=Vocab)\n-\n-    original_bort = nlp.model.BERTModel(\n-        encoder,\n-        len(bort_vocab),\n-        units=predefined_args[\"units\"],\n-        embed_size=predefined_args[\"embed_size\"],\n-        embed_dropout=predefined_args[\"embed_dropout\"],\n-        word_embed=predefined_args[\"word_embed\"],\n-        use_pooler=False,\n-        use_token_type_embed=False,\n-        token_type_vocab_size=predefined_args[\"token_type_vocab_size\"],\n-        use_classifier=False,\n-        use_decoder=False,\n-    )\n-\n-    original_bort.load_parameters(bort_checkpoint_path, cast_dtype=True, ignore_extra=True)\n-    params = original_bort._collect_params_with_prefix()\n-\n-    # Build our config ðŸ¤—\n-    hf_bort_config_json = {\n-        \"architectures\": [\"BertForMaskedLM\"],\n-        \"attention_probs_dropout_prob\": predefined_args[\"dropout\"],\n-        \"hidden_act\": \"gelu\",\n-        \"hidden_dropout_prob\": predefined_args[\"dropout\"],\n-        \"hidden_size\": predefined_args[\"embed_size\"],\n-        \"initializer_range\": 0.02,\n-        \"intermediate_size\": predefined_args[\"hidden_size\"],\n-        \"layer_norm_eps\": predefined_args[\"layer_norm_eps\"],\n-        \"max_position_embeddings\": predefined_args[\"max_length\"],\n-        \"model_type\": \"bort\",\n-        \"num_attention_heads\": predefined_args[\"num_heads\"],\n-        \"num_hidden_layers\": predefined_args[\"num_layers\"],\n-        \"pad_token_id\": 1,  # 2 = BERT, 1 = RoBERTa\n-        \"type_vocab_size\": 1,  # 2 = BERT, 1 = RoBERTa\n-        \"vocab_size\": len(bort_vocab),\n-    }\n-\n-    hf_bort_config = BertConfig.from_dict(hf_bort_config_json)\n-    hf_bort_model = BertForMaskedLM(hf_bort_config)\n-    hf_bort_model.eval()\n-\n-    # Parameter mapping table (Gluonnlp to Transformers)\n-    # * denotes layer index\n-    #\n-    # | Gluon Parameter                                                | Transformers Parameter\n-    # | -------------------------------------------------------------- | ----------------------\n-    # | `encoder.layer_norm.beta`                                      | `bert.embeddings.LayerNorm.bias`\n-    # | `encoder.layer_norm.gamma`                                     | `bert.embeddings.LayerNorm.weight`\n-    # | `encoder.position_weight`                                      | `bert.embeddings.position_embeddings.weight`\n-    # | `word_embed.0.weight`                                          | `bert.embeddings.word_embeddings.weight`\n-    # | `encoder.transformer_cells.*.attention_cell.proj_key.bias`     | `bert.encoder.layer.*.attention.self.key.bias`\n-    # | `encoder.transformer_cells.*.attention_cell.proj_key.weight`   | `bert.encoder.layer.*.attention.self.key.weight`\n-    # | `encoder.transformer_cells.*.attention_cell.proj_query.bias`   | `bert.encoder.layer.*.attention.self.query.bias`\n-    # | `encoder.transformer_cells.*.attention_cell.proj_query.weight` | `bert.encoder.layer.*.attention.self.query.weight`\n-    # | `encoder.transformer_cells.*.attention_cell.proj_value.bias`   | `bert.encoder.layer.*.attention.self.value.bias`\n-    # | `encoder.transformer_cells.*.attention_cell.proj_value.weight` | `bert.encoder.layer.*.attention.self.value.weight`\n-    # | `encoder.transformer_cells.*.ffn.ffn_2.bias`                   | `bert.encoder.layer.*.attention.output.dense.bias`\n-    # | `encoder.transformer_cells.*.ffn.ffn_2.weight`                 | `bert.encoder.layer.*.attention.output.dense.weight`\n-    # | `encoder.transformer_cells.*.layer_norm.beta`                  | `bert.encoder.layer.*.attention.output.LayerNorm.bias`\n-    # | `encoder.transformer_cells.*.layer_norm.gamma`                 | `bert.encoder.layer.*.attention.output.LayerNorm.weight`\n-    # | `encoder.transformer_cells.*.ffn.ffn_1.bias`                   | `bert.encoder.layer.*.intermediate.dense.bias`\n-    # | `encoder.transformer_cells.*.ffn.ffn_1.weight`                 | `bert.encoder.layer.*.intermediate.dense.weight`\n-    # | `encoder.transformer_cells.*.ffn.layer_norm.beta`              | `bert.encoder.layer.*.output.LayerNorm.bias`\n-    # | `encoder.transformer_cells.*.ffn.layer_norm.gamma`             | `bert.encoder.layer.*.output.LayerNorm.weight`\n-    # | `encoder.transformer_cells.*.proj.bias`                        | `bert.encoder.layer.*.output.dense.bias`\n-    # | `encoder.transformer_cells.*.proj.weight`                      | `bert.encoder.layer.*.output.dense.weight`\n-\n-    # Helper function to convert MXNET Arrays to PyTorch\n-    def to_torch(mx_array) -> nn.Parameter:\n-        return nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))\n-\n-    # Check param shapes and map new HF param back\n-    def check_and_map_params(hf_param, gluon_param):\n-        shape_hf = hf_param.shape\n-\n-        gluon_param = to_torch(params[gluon_param])\n-        shape_gluon = gluon_param.shape\n-\n-        assert shape_hf == shape_gluon, (\n-            f\"The gluon parameter {gluon_param} has shape {shape_gluon}, but expects shape {shape_hf} for Transformers\"\n-        )\n-\n-        return gluon_param\n-\n-    hf_bort_model.bert.embeddings.word_embeddings.weight = check_and_map_params(\n-        hf_bort_model.bert.embeddings.word_embeddings.weight, \"word_embed.0.weight\"\n-    )\n-    hf_bort_model.bert.embeddings.position_embeddings.weight = check_and_map_params(\n-        hf_bort_model.bert.embeddings.position_embeddings.weight, \"encoder.position_weight\"\n-    )\n-    hf_bort_model.bert.embeddings.LayerNorm.bias = check_and_map_params(\n-        hf_bort_model.bert.embeddings.LayerNorm.bias, \"encoder.layer_norm.beta\"\n-    )\n-    hf_bort_model.bert.embeddings.LayerNorm.weight = check_and_map_params(\n-        hf_bort_model.bert.embeddings.LayerNorm.weight, \"encoder.layer_norm.gamma\"\n-    )\n-\n-    # Inspired by RoBERTa conversion script, we just zero them out (Bort does not use them)\n-    hf_bort_model.bert.embeddings.token_type_embeddings.weight.data = torch.zeros_like(\n-        hf_bort_model.bert.embeddings.token_type_embeddings.weight.data\n-    )\n-\n-    for i in range(hf_bort_config.num_hidden_layers):\n-        layer: BertLayer = hf_bort_model.bert.encoder.layer[i]\n-\n-        # self attention\n-        self_attn: BertSelfAttention = layer.attention.self\n-\n-        self_attn.key.bias.data = check_and_map_params(\n-            self_attn.key.bias.data, f\"encoder.transformer_cells.{i}.attention_cell.proj_key.bias\"\n-        )\n-\n-        self_attn.key.weight.data = check_and_map_params(\n-            self_attn.key.weight.data, f\"encoder.transformer_cells.{i}.attention_cell.proj_key.weight\"\n-        )\n-        self_attn.query.bias.data = check_and_map_params(\n-            self_attn.query.bias.data, f\"encoder.transformer_cells.{i}.attention_cell.proj_query.bias\"\n-        )\n-        self_attn.query.weight.data = check_and_map_params(\n-            self_attn.query.weight.data, f\"encoder.transformer_cells.{i}.attention_cell.proj_query.weight\"\n-        )\n-        self_attn.value.bias.data = check_and_map_params(\n-            self_attn.value.bias.data, f\"encoder.transformer_cells.{i}.attention_cell.proj_value.bias\"\n-        )\n-        self_attn.value.weight.data = check_and_map_params(\n-            self_attn.value.weight.data, f\"encoder.transformer_cells.{i}.attention_cell.proj_value.weight\"\n-        )\n-\n-        # self attention output\n-        self_output: BertSelfOutput = layer.attention.output\n-\n-        self_output.dense.bias = check_and_map_params(\n-            self_output.dense.bias, f\"encoder.transformer_cells.{i}.proj.bias\"\n-        )\n-        self_output.dense.weight = check_and_map_params(\n-            self_output.dense.weight, f\"encoder.transformer_cells.{i}.proj.weight\"\n-        )\n-        self_output.LayerNorm.bias = check_and_map_params(\n-            self_output.LayerNorm.bias, f\"encoder.transformer_cells.{i}.layer_norm.beta\"\n-        )\n-        self_output.LayerNorm.weight = check_and_map_params(\n-            self_output.LayerNorm.weight, f\"encoder.transformer_cells.{i}.layer_norm.gamma\"\n-        )\n-\n-        # intermediate\n-        intermediate: BertIntermediate = layer.intermediate\n-\n-        intermediate.dense.bias = check_and_map_params(\n-            intermediate.dense.bias, f\"encoder.transformer_cells.{i}.ffn.ffn_1.bias\"\n-        )\n-        intermediate.dense.weight = check_and_map_params(\n-            intermediate.dense.weight, f\"encoder.transformer_cells.{i}.ffn.ffn_1.weight\"\n-        )\n-\n-        # output\n-        bert_output: BertOutput = layer.output\n-\n-        bert_output.dense.bias = check_and_map_params(\n-            bert_output.dense.bias, f\"encoder.transformer_cells.{i}.ffn.ffn_2.bias\"\n-        )\n-        bert_output.dense.weight = check_and_map_params(\n-            bert_output.dense.weight, f\"encoder.transformer_cells.{i}.ffn.ffn_2.weight\"\n-        )\n-        bert_output.LayerNorm.bias = check_and_map_params(\n-            bert_output.LayerNorm.bias, f\"encoder.transformer_cells.{i}.ffn.layer_norm.beta\"\n-        )\n-        bert_output.LayerNorm.weight = check_and_map_params(\n-            bert_output.LayerNorm.weight, f\"encoder.transformer_cells.{i}.ffn.layer_norm.gamma\"\n-        )\n-\n-    # Save space and energy ðŸŽ„\n-    hf_bort_model.half()\n-\n-    # Compare output of both models\n-    tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n-\n-    input_ids = tokenizer.encode_plus(SAMPLE_TEXT)[\"input_ids\"]\n-\n-    # Get gluon output\n-    gluon_input_ids = mx.nd.array([input_ids])\n-    output_gluon = original_bort(inputs=gluon_input_ids, token_types=[])\n-\n-    # Get Transformer output (save and reload model again)\n-    hf_bort_model.save_pretrained(pytorch_dump_folder_path)\n-    hf_bort_model = BertModel.from_pretrained(pytorch_dump_folder_path)\n-    hf_bort_model.eval()\n-\n-    input_ids = tokenizer.encode_plus(SAMPLE_TEXT, return_tensors=\"pt\")\n-    output_hf = hf_bort_model(**input_ids)[0]\n-\n-    gluon_layer = output_gluon[0].asnumpy()\n-    hf_layer = output_hf[0].detach().numpy()\n-\n-    max_absolute_diff = np.max(np.abs(hf_layer - gluon_layer)).item()\n-    success = np.allclose(gluon_layer, hf_layer, atol=1e-3)\n-\n-    if success:\n-        print(\"[SUCCESS] Both models do output the same tensors\")\n-    else:\n-        print(\"[FAIL] Both models do **NOT** output the same tensors\")\n-        print(\"Absolute difference is:\", max_absolute_diff)\n-\n-\n-if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser()\n-    # Required parameters\n-    parser.add_argument(\n-        \"--bort_checkpoint_path\", default=None, type=str, required=True, help=\"Path the official Bort params file.\"\n-    )\n-    parser.add_argument(\n-        \"--pytorch_dump_folder_path\", default=None, type=str, required=True, help=\"Path to the output PyTorch model.\"\n-    )\n-    args = parser.parse_args()\n-    convert_bort_checkpoint_to_pytorch(args.bort_checkpoint_path, args.pytorch_dump_folder_path)"
        },
        {
            "sha": "6e06e186741991b2f9183f30b6591477bc60dfb5",
            "filename": "src/transformers/models/deprecated/deta/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,28 +0,0 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from typing import TYPE_CHECKING\n-\n-from ....utils import _LazyModule\n-from ....utils.import_utils import define_import_structure\n-\n-\n-if TYPE_CHECKING:\n-    from .configuration_deta import *\n-    from .image_processing_deta import *\n-    from .modeling_deta import *\n-else:\n-    import sys\n-\n-    _file = globals()[\"__file__\"]\n-    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "e4861ff0f0fa2211c52bebbe59a5054b07d49de0",
            "filename": "src/transformers/models/deprecated/deta/configuration_deta.py",
            "status": "removed",
            "additions": 0,
            "deletions": 278,
            "changes": 278,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconfiguration_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconfiguration_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconfiguration_deta.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,278 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 SenseTime and The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"DETA model configuration\"\"\"\n-\n-from ....configuration_utils import PreTrainedConfig\n-from ....utils import logging\n-from ...auto import CONFIG_MAPPING\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class DetaConfig(PreTrainedConfig):\n-    r\"\"\"\n-    This is the configuration class to store the configuration of a [`DetaModel`]. It is used to instantiate a DETA\n-    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n-    defaults will yield a similar configuration to that of the DETA\n-    [SenseTime/deformable-detr](https://huggingface.co/SenseTime/deformable-detr) architecture.\n-\n-    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PreTrainedConfig`] for more information.\n-\n-    Args:\n-        backbone_config (`PreTrainedConfig` or `dict`, *optional*, defaults to `ResNetConfig()`):\n-            The configuration of the backbone model.\n-        backbone (`str`, *optional*):\n-            Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this\n-            will load the corresponding pretrained weights from the timm or transformers library. If `use_pretrained_backbone`\n-            is `False`, this loads the backbone's config and uses that to initialize the backbone with random weights.\n-        use_pretrained_backbone (`bool`, *optional*, `False`):\n-            Whether to use pretrained weights for the backbone.\n-        use_timm_backbone (`bool`, *optional*, `False`):\n-            Whether to load `backbone` from the timm library. If `False`, the backbone is loaded from the transformers\n-            library.\n-        backbone_kwargs (`dict`, *optional*):\n-            Keyword arguments to be passed to AutoBackbone when loading from a checkpoint\n-            e.g. `{'out_indices': (0, 1, 2, 3)}`. Cannot be specified if `backbone_config` is set.\n-        num_queries (`int`, *optional*, defaults to 900):\n-            Number of object queries, i.e. detection slots. This is the maximal number of objects [`DetaModel`] can\n-            detect in a single image. In case `two_stage` is set to `True`, we use `two_stage_num_proposals` instead.\n-        d_model (`int`, *optional*, defaults to 256):\n-            Dimension of the layers.\n-        encoder_layers (`int`, *optional*, defaults to 6):\n-            Number of encoder layers.\n-        decoder_layers (`int`, *optional*, defaults to 6):\n-            Number of decoder layers.\n-        encoder_attention_heads (`int`, *optional*, defaults to 8):\n-            Number of attention heads for each attention layer in the Transformer encoder.\n-        decoder_attention_heads (`int`, *optional*, defaults to 8):\n-            Number of attention heads for each attention layer in the Transformer decoder.\n-        decoder_ffn_dim (`int`, *optional*, defaults to 2048):\n-            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n-        encoder_ffn_dim (`int`, *optional*, defaults to 2048):\n-            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n-        activation_function (`str` or `function`, *optional*, defaults to `\"relu\"`):\n-            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n-            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n-        dropout (`float`, *optional*, defaults to 0.1):\n-            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n-        attention_dropout (`float`, *optional*, defaults to 0.0):\n-            The dropout ratio for the attention probabilities.\n-        activation_dropout (`float`, *optional*, defaults to 0.0):\n-            The dropout ratio for activations inside the fully connected layer.\n-        init_std (`float`, *optional*, defaults to 0.02):\n-            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n-        init_xavier_std (`float`, *optional*, defaults to 1):\n-            The scaling factor used for the Xavier initialization gain in the HM Attention map module.\n-        encoder_layerdrop (`float`, *optional*, defaults to 0.0):\n-            The LayerDrop probability for the encoder. See the [LayerDrop paper](see https://huggingface.co/papers/1909.11556)\n-            for more details.\n-        auxiliary_loss (`bool`, *optional*, defaults to `False`):\n-            Whether auxiliary decoding losses (loss at each decoder layer) are to be used.\n-        position_embedding_type (`str`, *optional*, defaults to `\"sine\"`):\n-            Type of position embeddings to be used on top of the image features. One of `\"sine\"` or `\"learned\"`.\n-        class_cost (`float`, *optional*, defaults to 1):\n-            Relative weight of the classification error in the Hungarian matching cost.\n-        bbox_cost (`float`, *optional*, defaults to 5):\n-            Relative weight of the L1 error of the bounding box coordinates in the Hungarian matching cost.\n-        giou_cost (`float`, *optional*, defaults to 2):\n-            Relative weight of the generalized IoU loss of the bounding box in the Hungarian matching cost.\n-        mask_loss_coefficient (`float`, *optional*, defaults to 1):\n-            Relative weight of the Focal loss in the panoptic segmentation loss.\n-        dice_loss_coefficient (`float`, *optional*, defaults to 1):\n-            Relative weight of the DICE/F-1 loss in the panoptic segmentation loss.\n-        bbox_loss_coefficient (`float`, *optional*, defaults to 5):\n-            Relative weight of the L1 bounding box loss in the object detection loss.\n-        giou_loss_coefficient (`float`, *optional*, defaults to 2):\n-            Relative weight of the generalized IoU loss in the object detection loss.\n-        eos_coefficient (`float`, *optional*, defaults to 0.1):\n-            Relative classification weight of the 'no-object' class in the object detection loss.\n-        num_feature_levels (`int`, *optional*, defaults to 5):\n-            The number of input feature levels.\n-        encoder_n_points (`int`, *optional*, defaults to 4):\n-            The number of sampled keys in each feature level for each attention head in the encoder.\n-        decoder_n_points (`int`, *optional*, defaults to 4):\n-            The number of sampled keys in each feature level for each attention head in the decoder.\n-        two_stage (`bool`, *optional*, defaults to `True`):\n-            Whether to apply a two-stage deformable DETR, where the region proposals are also generated by a variant of\n-            DETA, which are further fed into the decoder for iterative bounding box refinement.\n-        two_stage_num_proposals (`int`, *optional*, defaults to 300):\n-            The number of region proposals to be generated, in case `two_stage` is set to `True`.\n-        with_box_refine (`bool`, *optional*, defaults to `True`):\n-            Whether to apply iterative bounding box refinement, where each decoder layer refines the bounding boxes\n-            based on the predictions from the previous layer.\n-        focal_alpha (`float`, *optional*, defaults to 0.25):\n-            Alpha parameter in the focal loss.\n-        assign_first_stage (`bool`, *optional*, defaults to `True`):\n-            Whether to assign each prediction i to the highest overlapping ground truth object if the overlap is larger than a threshold 0.7.\n-        assign_second_stage (`bool`, *optional*, defaults to `True`):\n-            Whether to assign second assignment procedure in the second stage closely follows the first stage assignment procedure.\n-        disable_custom_kernels (`bool`, *optional*, defaults to `True`):\n-            Disable the use of custom CUDA and CPU kernels. This option is necessary for the ONNX export, as custom\n-            kernels are not supported by PyTorch ONNX export.\n-\n-    Examples:\n-\n-    ```python\n-    >>> from transformers import DetaConfig, DetaModel\n-\n-    >>> # Initializing a DETA SenseTime/deformable-detr style configuration\n-    >>> configuration = DetaConfig()\n-\n-    >>> # Initializing a model (with random weights) from the SenseTime/deformable-detr style configuration\n-    >>> model = DetaModel(configuration)\n-\n-    >>> # Accessing the model configuration\n-    >>> configuration = model.config\n-    ```\"\"\"\n-\n-    model_type = \"deta\"\n-    attribute_map = {\n-        \"hidden_size\": \"d_model\",\n-        \"num_attention_heads\": \"encoder_attention_heads\",\n-    }\n-\n-    def __init__(\n-        self,\n-        backbone_config=None,\n-        backbone=None,\n-        use_pretrained_backbone=False,\n-        use_timm_backbone=False,\n-        backbone_kwargs=None,\n-        num_queries=900,\n-        max_position_embeddings=2048,\n-        encoder_layers=6,\n-        encoder_ffn_dim=2048,\n-        encoder_attention_heads=8,\n-        decoder_layers=6,\n-        decoder_ffn_dim=1024,\n-        decoder_attention_heads=8,\n-        encoder_layerdrop=0.0,\n-        is_encoder_decoder=True,\n-        activation_function=\"relu\",\n-        d_model=256,\n-        dropout=0.1,\n-        attention_dropout=0.0,\n-        activation_dropout=0.0,\n-        init_std=0.02,\n-        init_xavier_std=1.0,\n-        return_intermediate=True,\n-        auxiliary_loss=False,\n-        position_embedding_type=\"sine\",\n-        num_feature_levels=5,\n-        encoder_n_points=4,\n-        decoder_n_points=4,\n-        two_stage=True,\n-        two_stage_num_proposals=300,\n-        with_box_refine=True,\n-        assign_first_stage=True,\n-        assign_second_stage=True,\n-        class_cost=1,\n-        bbox_cost=5,\n-        giou_cost=2,\n-        mask_loss_coefficient=1,\n-        dice_loss_coefficient=1,\n-        bbox_loss_coefficient=5,\n-        giou_loss_coefficient=2,\n-        eos_coefficient=0.1,\n-        focal_alpha=0.25,\n-        disable_custom_kernels=True,\n-        **kwargs,\n-    ):\n-        if use_pretrained_backbone:\n-            raise ValueError(\"Pretrained backbones are not supported yet.\")\n-\n-        if backbone_config is not None and backbone is not None:\n-            raise ValueError(\"You can't specify both `backbone` and `backbone_config`.\")\n-\n-        if backbone_config is None and backbone is None:\n-            logger.info(\"`backbone_config` is `None`. Initializing the config with the default `ResNet` backbone.\")\n-            backbone_config = CONFIG_MAPPING[\"resnet\"](out_features=[\"stage2\", \"stage3\", \"stage4\"])\n-        else:\n-            if isinstance(backbone_config, dict):\n-                backbone_model_type = backbone_config.pop(\"model_type\")\n-                config_class = CONFIG_MAPPING[backbone_model_type]\n-                backbone_config = config_class.from_dict(backbone_config)\n-\n-        if backbone_kwargs is not None and backbone_kwargs and backbone_config is not None:\n-            raise ValueError(\"You can't specify both `backbone_kwargs` and `backbone_config`.\")\n-\n-        self.backbone_config = backbone_config\n-        self.backbone = backbone\n-        self.use_pretrained_backbone = use_pretrained_backbone\n-        self.use_timm_backbone = use_timm_backbone\n-        self.backbone_kwargs = backbone_kwargs\n-        self.num_queries = num_queries\n-        self.max_position_embeddings = max_position_embeddings\n-        self.d_model = d_model\n-        self.encoder_ffn_dim = encoder_ffn_dim\n-        self.encoder_layers = encoder_layers\n-        self.encoder_attention_heads = encoder_attention_heads\n-        self.decoder_ffn_dim = decoder_ffn_dim\n-        self.decoder_layers = decoder_layers\n-        self.decoder_attention_heads = decoder_attention_heads\n-        self.dropout = dropout\n-        self.attention_dropout = attention_dropout\n-        self.activation_dropout = activation_dropout\n-        self.activation_function = activation_function\n-        self.init_std = init_std\n-        self.init_xavier_std = init_xavier_std\n-        self.encoder_layerdrop = encoder_layerdrop\n-        self.auxiliary_loss = auxiliary_loss\n-        self.position_embedding_type = position_embedding_type\n-        # deformable attributes\n-        self.num_feature_levels = num_feature_levels\n-        self.encoder_n_points = encoder_n_points\n-        self.decoder_n_points = decoder_n_points\n-        self.two_stage = two_stage\n-        self.two_stage_num_proposals = two_stage_num_proposals\n-        self.with_box_refine = with_box_refine\n-        self.assign_first_stage = assign_first_stage\n-        self.assign_second_stage = assign_second_stage\n-        if two_stage is True and with_box_refine is False:\n-            raise ValueError(\"If two_stage is True, with_box_refine must be True.\")\n-        # Hungarian matcher\n-        self.class_cost = class_cost\n-        self.bbox_cost = bbox_cost\n-        self.giou_cost = giou_cost\n-        # Loss coefficients\n-        self.mask_loss_coefficient = mask_loss_coefficient\n-        self.dice_loss_coefficient = dice_loss_coefficient\n-        self.bbox_loss_coefficient = bbox_loss_coefficient\n-        self.giou_loss_coefficient = giou_loss_coefficient\n-        self.eos_coefficient = eos_coefficient\n-        self.focal_alpha = focal_alpha\n-        self.disable_custom_kernels = disable_custom_kernels\n-        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n-\n-    @property\n-    def num_attention_heads(self) -> int:\n-        return self.encoder_attention_heads\n-\n-    @property\n-    def hidden_size(self) -> int:\n-        return self.d_model\n-\n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n-\n-__all__ = [\"DetaConfig\"]"
        },
        {
            "sha": "6023be50854c5b3f9384ccb437bae956f2b77b7e",
            "filename": "src/transformers/models/deprecated/deta/convert_deta_resnet_to_pytorch.py",
            "status": "removed",
            "additions": 0,
            "deletions": 321,
            "changes": 321,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_resnet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_resnet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_resnet_to_pytorch.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,321 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Convert DETA checkpoints from the original repository.\n-\n-URL: https://github.com/jozhang97/DETA/tree/master\"\"\"\n-\n-import argparse\n-import json\n-from pathlib import Path\n-\n-import requests\n-import torch\n-from huggingface_hub import hf_hub_download\n-from PIL import Image\n-\n-from transformers import DetaConfig, DetaForObjectDetection, DetaImageProcessor\n-from transformers.utils import logging\n-\n-\n-logging.set_verbosity_info()\n-logger = logging.get_logger(__name__)\n-\n-\n-def get_deta_config():\n-    config = DetaConfig(\n-        num_queries=900,\n-        encoder_ffn_dim=2048,\n-        decoder_ffn_dim=2048,\n-        num_feature_levels=5,\n-        assign_first_stage=True,\n-        with_box_refine=True,\n-        two_stage=True,\n-    )\n-\n-    # set labels\n-    config.num_labels = 91\n-    repo_id = \"huggingface/label-files\"\n-    filename = \"coco-detection-id2label.json\"\n-    id2label = json.loads(Path(hf_hub_download(repo_id, filename, repo_type=\"dataset\")).read_text())\n-    id2label = {int(k): v for k, v in id2label.items()}\n-    config.id2label = id2label\n-    config.label2id = {v: k for k, v in id2label.items()}\n-\n-    return config\n-\n-\n-# here we list all keys to be renamed (original name on the left, our name on the right)\n-def create_rename_keys(config):\n-    rename_keys = []\n-\n-    # stem\n-    # fmt: off\n-    rename_keys.append((\"backbone.0.body.conv1.weight\", \"model.backbone.model.embedder.embedder.convolution.weight\"))\n-    rename_keys.append((\"backbone.0.body.bn1.weight\", \"model.backbone.model.embedder.embedder.normalization.weight\"))\n-    rename_keys.append((\"backbone.0.body.bn1.bias\", \"model.backbone.model.embedder.embedder.normalization.bias\"))\n-    rename_keys.append((\"backbone.0.body.bn1.running_mean\", \"model.backbone.model.embedder.embedder.normalization.running_mean\"))\n-    rename_keys.append((\"backbone.0.body.bn1.running_var\", \"model.backbone.model.embedder.embedder.normalization.running_var\"))\n-    # stages\n-    for stage_idx in range(len(config.backbone_config.depths)):\n-        for layer_idx in range(config.backbone_config.depths[stage_idx]):\n-            # shortcut\n-            if layer_idx == 0:\n-                rename_keys.append(\n-                    (\n-                        f\"backbone.0.body.layer{stage_idx + 1}.{layer_idx}.downsample.0.weight\",\n-                        f\"model.backbone.model.encoder.stages.{stage_idx}.layers.{layer_idx}.shortcut.convolution.weight\",\n-                    )\n-                )\n-                rename_keys.append(\n-                    (\n-                        f\"backbone.0.body.layer{stage_idx + 1}.{layer_idx}.downsample.1.weight\",\n-                        f\"model.backbone.model.encoder.stages.{stage_idx}.layers.{layer_idx}.shortcut.normalization.weight\",\n-                    )\n-                )\n-                rename_keys.append(\n-                    (\n-                        f\"backbone.0.body.layer{stage_idx + 1}.{layer_idx}.downsample.1.bias\",\n-                        f\"model.backbone.model.encoder.stages.{stage_idx}.layers.{layer_idx}.shortcut.normalization.bias\",\n-                    )\n-                )\n-                rename_keys.append(\n-                    (\n-                        f\"backbone.0.body.layer{stage_idx + 1}.{layer_idx}.downsample.1.running_mean\",\n-                        f\"model.backbone.model.encoder.stages.{stage_idx}.layers.{layer_idx}.shortcut.normalization.running_mean\",\n-                    )\n-                )\n-                rename_keys.append(\n-                    (\n-                        f\"backbone.0.body.layer{stage_idx + 1}.{layer_idx}.downsample.1.running_var\",\n-                        f\"model.backbone.model.encoder.stages.{stage_idx}.layers.{layer_idx}.shortcut.normalization.running_var\",\n-                    )\n-                )\n-            # 3 convs\n-            for i in range(3):\n-                rename_keys.append(\n-                    (\n-                        f\"backbone.0.body.layer{stage_idx + 1}.{layer_idx}.conv{i+1}.weight\",\n-                        f\"model.backbone.model.encoder.stages.{stage_idx}.layers.{layer_idx}.layer.{i}.convolution.weight\",\n-                    )\n-                )\n-                rename_keys.append(\n-                    (\n-                        f\"backbone.0.body.layer{stage_idx + 1}.{layer_idx}.bn{i+1}.weight\",\n-                        f\"model.backbone.model.encoder.stages.{stage_idx}.layers.{layer_idx}.layer.{i}.normalization.weight\",\n-                    )\n-                )\n-                rename_keys.append(\n-                    (\n-                        f\"backbone.0.body.layer{stage_idx + 1}.{layer_idx}.bn{i+1}.bias\",\n-                        f\"model.backbone.model.encoder.stages.{stage_idx}.layers.{layer_idx}.layer.{i}.normalization.bias\",\n-                    )\n-                )\n-                rename_keys.append(\n-                    (\n-                        f\"backbone.0.body.layer{stage_idx + 1}.{layer_idx}.bn{i+1}.running_mean\",\n-                        f\"model.backbone.model.encoder.stages.{stage_idx}.layers.{layer_idx}.layer.{i}.normalization.running_mean\",\n-                    )\n-                )\n-                rename_keys.append(\n-                    (\n-                        f\"backbone.0.body.layer{stage_idx + 1}.{layer_idx}.bn{i+1}.running_var\",\n-                        f\"model.backbone.model.encoder.stages.{stage_idx}.layers.{layer_idx}.layer.{i}.normalization.running_var\",\n-                    )\n-                )\n-    # transformer encoder\n-    for i in range(config.encoder_layers):\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.self_attn.sampling_offsets.weight\", f\"model.encoder.layers.{i}.self_attn.sampling_offsets.weight\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.self_attn.sampling_offsets.bias\", f\"model.encoder.layers.{i}.self_attn.sampling_offsets.bias\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.self_attn.attention_weights.weight\", f\"model.encoder.layers.{i}.self_attn.attention_weights.weight\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.self_attn.attention_weights.bias\", f\"model.encoder.layers.{i}.self_attn.attention_weights.bias\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.self_attn.value_proj.weight\", f\"model.encoder.layers.{i}.self_attn.value_proj.weight\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.self_attn.value_proj.bias\", f\"model.encoder.layers.{i}.self_attn.value_proj.bias\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.self_attn.output_proj.weight\", f\"model.encoder.layers.{i}.self_attn.output_proj.weight\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.self_attn.output_proj.bias\", f\"model.encoder.layers.{i}.self_attn.output_proj.bias\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.norm1.weight\", f\"model.encoder.layers.{i}.self_attn_layer_norm.weight\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.norm1.bias\", f\"model.encoder.layers.{i}.self_attn_layer_norm.bias\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.linear1.weight\", f\"model.encoder.layers.{i}.fc1.weight\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.linear1.bias\", f\"model.encoder.layers.{i}.fc1.bias\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.linear2.weight\", f\"model.encoder.layers.{i}.fc2.weight\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.linear2.bias\", f\"model.encoder.layers.{i}.fc2.bias\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.norm2.weight\", f\"model.encoder.layers.{i}.final_layer_norm.weight\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.norm2.bias\", f\"model.encoder.layers.{i}.final_layer_norm.bias\"))\n-\n-    # transformer decoder\n-    for i in range(config.decoder_layers):\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.cross_attn.sampling_offsets.weight\", f\"model.decoder.layers.{i}.encoder_attn.sampling_offsets.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.cross_attn.sampling_offsets.bias\", f\"model.decoder.layers.{i}.encoder_attn.sampling_offsets.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.cross_attn.attention_weights.weight\", f\"model.decoder.layers.{i}.encoder_attn.attention_weights.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.cross_attn.attention_weights.bias\", f\"model.decoder.layers.{i}.encoder_attn.attention_weights.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.cross_attn.value_proj.weight\", f\"model.decoder.layers.{i}.encoder_attn.value_proj.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.cross_attn.value_proj.bias\", f\"model.decoder.layers.{i}.encoder_attn.value_proj.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.cross_attn.output_proj.weight\", f\"model.decoder.layers.{i}.encoder_attn.output_proj.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.cross_attn.output_proj.bias\", f\"model.decoder.layers.{i}.encoder_attn.output_proj.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.norm1.weight\", f\"model.decoder.layers.{i}.encoder_attn_layer_norm.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.norm1.bias\", f\"model.decoder.layers.{i}.encoder_attn_layer_norm.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.self_attn.out_proj.weight\", f\"model.decoder.layers.{i}.self_attn.out_proj.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.self_attn.out_proj.bias\", f\"model.decoder.layers.{i}.self_attn.out_proj.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.norm2.weight\", f\"model.decoder.layers.{i}.self_attn_layer_norm.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.norm2.bias\", f\"model.decoder.layers.{i}.self_attn_layer_norm.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.linear1.weight\", f\"model.decoder.layers.{i}.fc1.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.linear1.bias\", f\"model.decoder.layers.{i}.fc1.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.linear2.weight\", f\"model.decoder.layers.{i}.fc2.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.linear2.bias\", f\"model.decoder.layers.{i}.fc2.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.norm3.weight\", f\"model.decoder.layers.{i}.final_layer_norm.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.norm3.bias\", f\"model.decoder.layers.{i}.final_layer_norm.bias\"))\n-\n-    # fmt: on\n-\n-    return rename_keys\n-\n-\n-def rename_key(dct, old, new):\n-    val = dct.pop(old)\n-    dct[new] = val\n-\n-\n-def read_in_decoder_q_k_v(state_dict, config):\n-    # transformer decoder self-attention layers\n-    hidden_size = config.d_model\n-    for i in range(config.decoder_layers):\n-        # read in weights + bias of input projection layer of self-attention\n-        in_proj_weight = state_dict.pop(f\"transformer.decoder.layers.{i}.self_attn.in_proj_weight\")\n-        in_proj_bias = state_dict.pop(f\"transformer.decoder.layers.{i}.self_attn.in_proj_bias\")\n-        # next, add query, keys and values (in that order) to the state dict\n-        state_dict[f\"model.decoder.layers.{i}.self_attn.q_proj.weight\"] = in_proj_weight[:hidden_size, :]\n-        state_dict[f\"model.decoder.layers.{i}.self_attn.q_proj.bias\"] = in_proj_bias[:hidden_size]\n-        state_dict[f\"model.decoder.layers.{i}.self_attn.k_proj.weight\"] = in_proj_weight[\n-            hidden_size : hidden_size * 2, :\n-        ]\n-        state_dict[f\"model.decoder.layers.{i}.self_attn.k_proj.bias\"] = in_proj_bias[hidden_size : hidden_size * 2]\n-        state_dict[f\"model.decoder.layers.{i}.self_attn.v_proj.weight\"] = in_proj_weight[-hidden_size:, :]\n-        state_dict[f\"model.decoder.layers.{i}.self_attn.v_proj.bias\"] = in_proj_bias[-hidden_size:]\n-\n-\n-# We will verify our results on an image of cute cats\n-def prepare_img():\n-    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-    im = Image.open(requests.get(url, stream=True).raw)\n-\n-    return im\n-\n-\n-@torch.no_grad()\n-def convert_deta_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub):\n-    \"\"\"\n-    Copy/paste/tweak model's weights to our DETA structure.\n-    \"\"\"\n-\n-    # load config\n-    config = get_deta_config()\n-\n-    # load original state dict\n-    if model_name == \"deta-resnet-50\":\n-        filename = \"adet_checkpoint0011.pth\"\n-    elif model_name == \"deta-resnet-50-24-epochs\":\n-        filename = \"adet_2x_checkpoint0023.pth\"\n-    else:\n-        raise ValueError(f\"Model name {model_name} not supported\")\n-    checkpoint_path = hf_hub_download(repo_id=\"nielsr/deta-checkpoints\", filename=filename)\n-    state_dict = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)[\"model\"]\n-\n-    # rename keys\n-    rename_keys = create_rename_keys(config)\n-    for src, dest in rename_keys:\n-        rename_key(state_dict, src, dest)\n-    read_in_decoder_q_k_v(state_dict, config)\n-\n-    # fix some prefixes\n-    for key in state_dict.copy():\n-        if \"transformer.decoder.class_embed\" in key or \"transformer.decoder.bbox_embed\" in key:\n-            val = state_dict.pop(key)\n-            state_dict[key.replace(\"transformer.decoder\", \"model.decoder\")] = val\n-        if \"input_proj\" in key:\n-            val = state_dict.pop(key)\n-            state_dict[\"model.\" + key] = val\n-        if \"level_embed\" in key or \"pos_trans\" in key or \"pix_trans\" in key or \"enc_output\" in key:\n-            val = state_dict.pop(key)\n-            state_dict[key.replace(\"transformer\", \"model\")] = val\n-\n-    # finally, create HuggingFace model and load state dict\n-    model = DetaForObjectDetection(config)\n-    model.load_state_dict(state_dict)\n-    model.eval()\n-\n-    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n-    model.to(device)\n-\n-    # load image processor\n-    processor = DetaImageProcessor(format=\"coco_detection\")\n-\n-    # verify our conversion on image\n-    img = prepare_img()\n-    encoding = processor(images=img, return_tensors=\"pt\")\n-    pixel_values = encoding[\"pixel_values\"]\n-    outputs = model(pixel_values.to(device))\n-\n-    # verify logits\n-    if model_name == \"deta-resnet-50\":\n-        expected_logits = torch.tensor(\n-            [[-7.3978, -2.5406, -4.1668], [-8.2684, -3.9933, -3.8096], [-7.0515, -3.7973, -5.8516]]\n-        )\n-        expected_boxes = torch.tensor([[0.5043, 0.4973, 0.9998], [0.2542, 0.5489, 0.4748], [0.5490, 0.2765, 0.0570]])\n-    elif model_name == \"deta-resnet-50-24-epochs\":\n-        expected_logits = torch.tensor(\n-            [[-7.1688, -2.4857, -4.8669], [-7.8630, -3.8154, -4.2674], [-7.2730, -4.1865, -5.5323]]\n-        )\n-        expected_boxes = torch.tensor([[0.5021, 0.4971, 0.9994], [0.2546, 0.5486, 0.4731], [0.1686, 0.1986, 0.2142]])\n-\n-    assert torch.allclose(outputs.logits[0, :3, :3], expected_logits.to(device), atol=1e-4)\n-    assert torch.allclose(outputs.pred_boxes[0, :3, :3], expected_boxes.to(device), atol=1e-4)\n-    print(\"Everything ok!\")\n-\n-    if pytorch_dump_folder_path:\n-        # Save model and processor\n-        logger.info(f\"Saving PyTorch model and processor to {pytorch_dump_folder_path}...\")\n-        Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n-        model.save_pretrained(pytorch_dump_folder_path)\n-        processor.save_pretrained(pytorch_dump_folder_path)\n-\n-    # Push to hub\n-    if push_to_hub:\n-        print(\"Pushing model and processor to hub...\")\n-        model.push_to_hub(f\"jozhang97/{model_name}\")\n-        processor.push_to_hub(f\"jozhang97/{model_name}\")\n-\n-\n-if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser()\n-\n-    parser.add_argument(\n-        \"--model_name\",\n-        type=str,\n-        default=\"deta-resnet-50\",\n-        choices=[\"deta-resnet-50\", \"deta-resnet-50-24-epochs\"],\n-        help=\"Name of the model you'd like to convert.\",\n-    )\n-    parser.add_argument(\n-        \"--pytorch_dump_folder_path\",\n-        default=None,\n-        type=str,\n-        help=\"Path to the folder to output PyTorch model.\",\n-    )\n-    parser.add_argument(\n-        \"--push_to_hub\",\n-        action=\"store_true\",\n-        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n-    )\n-    args = parser.parse_args()\n-    convert_deta_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.push_to_hub)"
        },
        {
            "sha": "cc95883c891fdbff867e72ad24beff1260da2cbe",
            "filename": "src/transformers/models/deprecated/deta/convert_deta_swin_to_pytorch.py",
            "status": "removed",
            "additions": 0,
            "deletions": 328,
            "changes": 328,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_swin_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_swin_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_swin_to_pytorch.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,328 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Convert DETA checkpoints from the original repository.\n-\n-URL: https://github.com/jozhang97/DETA/tree/master\"\"\"\n-\n-import argparse\n-import json\n-from pathlib import Path\n-\n-import requests\n-import torch\n-from huggingface_hub import hf_hub_download\n-from PIL import Image\n-\n-from transformers import DetaConfig, DetaForObjectDetection, DetaImageProcessor, SwinConfig\n-from transformers.utils import logging\n-\n-\n-logging.set_verbosity_info()\n-logger = logging.get_logger(__name__)\n-\n-\n-def get_deta_config(model_name):\n-    backbone_config = SwinConfig(\n-        embed_dim=192,\n-        depths=(2, 2, 18, 2),\n-        num_heads=(6, 12, 24, 48),\n-        window_size=12,\n-        out_features=[\"stage2\", \"stage3\", \"stage4\"],\n-    )\n-\n-    config = DetaConfig(\n-        backbone_config=backbone_config,\n-        num_queries=900,\n-        encoder_ffn_dim=2048,\n-        decoder_ffn_dim=2048,\n-        num_feature_levels=5,\n-        assign_first_stage=True,\n-        with_box_refine=True,\n-        two_stage=True,\n-    )\n-\n-    # set labels\n-    repo_id = \"huggingface/label-files\"\n-    if \"o365\" in model_name:\n-        num_labels = 366\n-        filename = \"object365-id2label.json\"\n-    else:\n-        num_labels = 91\n-        filename = \"coco-detection-id2label.json\"\n-\n-    config.num_labels = num_labels\n-    id2label = json.loads(Path(hf_hub_download(repo_id, filename, repo_type=\"dataset\")).read_text())\n-    id2label = {int(k): v for k, v in id2label.items()}\n-    config.id2label = id2label\n-    config.label2id = {v: k for k, v in id2label.items()}\n-\n-    return config\n-\n-\n-# here we list all keys to be renamed (original name on the left, our name on the right)\n-def create_rename_keys(config):\n-    rename_keys = []\n-\n-    # stem\n-    # fmt: off\n-    rename_keys.append((\"backbone.0.body.patch_embed.proj.weight\", \"model.backbone.model.embeddings.patch_embeddings.projection.weight\"))\n-    rename_keys.append((\"backbone.0.body.patch_embed.proj.bias\", \"model.backbone.model.embeddings.patch_embeddings.projection.bias\"))\n-    rename_keys.append((\"backbone.0.body.patch_embed.norm.weight\", \"model.backbone.model.embeddings.norm.weight\"))\n-    rename_keys.append((\"backbone.0.body.patch_embed.norm.bias\", \"model.backbone.model.embeddings.norm.bias\"))\n-    # stages\n-    for i in range(len(config.backbone_config.depths)):\n-        for j in range(config.backbone_config.depths[i]):\n-            rename_keys.append((f\"backbone.0.body.layers.{i}.blocks.{j}.norm1.weight\", f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.layernorm_before.weight\"))\n-            rename_keys.append((f\"backbone.0.body.layers.{i}.blocks.{j}.norm1.bias\", f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.layernorm_before.bias\"))\n-            rename_keys.append((f\"backbone.0.body.layers.{i}.blocks.{j}.attn.relative_position_bias_table\", f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.attention.self.relative_position_bias_table\"))\n-            rename_keys.append((f\"backbone.0.body.layers.{i}.blocks.{j}.attn.relative_position_index\", f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.attention.self.relative_position_index\"))\n-            rename_keys.append((f\"backbone.0.body.layers.{i}.blocks.{j}.attn.proj.weight\", f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.attention.output.dense.weight\"))\n-            rename_keys.append((f\"backbone.0.body.layers.{i}.blocks.{j}.attn.proj.bias\", f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.attention.output.dense.bias\"))\n-            rename_keys.append((f\"backbone.0.body.layers.{i}.blocks.{j}.norm2.weight\", f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.layernorm_after.weight\"))\n-            rename_keys.append((f\"backbone.0.body.layers.{i}.blocks.{j}.norm2.bias\", f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.layernorm_after.bias\"))\n-            rename_keys.append((f\"backbone.0.body.layers.{i}.blocks.{j}.mlp.fc1.weight\", f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.intermediate.dense.weight\"))\n-            rename_keys.append((f\"backbone.0.body.layers.{i}.blocks.{j}.mlp.fc1.bias\", f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.intermediate.dense.bias\"))\n-            rename_keys.append((f\"backbone.0.body.layers.{i}.blocks.{j}.mlp.fc2.weight\", f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.output.dense.weight\"))\n-            rename_keys.append((f\"backbone.0.body.layers.{i}.blocks.{j}.mlp.fc2.bias\", f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.output.dense.bias\"))\n-\n-        if i < 3:\n-            rename_keys.append((f\"backbone.0.body.layers.{i}.downsample.reduction.weight\", f\"model.backbone.model.encoder.layers.{i}.downsample.reduction.weight\"))\n-            rename_keys.append((f\"backbone.0.body.layers.{i}.downsample.norm.weight\", f\"model.backbone.model.encoder.layers.{i}.downsample.norm.weight\"))\n-            rename_keys.append((f\"backbone.0.body.layers.{i}.downsample.norm.bias\", f\"model.backbone.model.encoder.layers.{i}.downsample.norm.bias\"))\n-\n-    rename_keys.append((\"backbone.0.body.norm1.weight\", \"model.backbone.model.hidden_states_norms.stage2.weight\"))\n-    rename_keys.append((\"backbone.0.body.norm1.bias\", \"model.backbone.model.hidden_states_norms.stage2.bias\"))\n-    rename_keys.append((\"backbone.0.body.norm2.weight\", \"model.backbone.model.hidden_states_norms.stage3.weight\"))\n-    rename_keys.append((\"backbone.0.body.norm2.bias\", \"model.backbone.model.hidden_states_norms.stage3.bias\"))\n-    rename_keys.append((\"backbone.0.body.norm3.weight\", \"model.backbone.model.hidden_states_norms.stage4.weight\"))\n-    rename_keys.append((\"backbone.0.body.norm3.bias\", \"model.backbone.model.hidden_states_norms.stage4.bias\"))\n-\n-    # transformer encoder\n-    for i in range(config.encoder_layers):\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.self_attn.sampling_offsets.weight\", f\"model.encoder.layers.{i}.self_attn.sampling_offsets.weight\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.self_attn.sampling_offsets.bias\", f\"model.encoder.layers.{i}.self_attn.sampling_offsets.bias\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.self_attn.attention_weights.weight\", f\"model.encoder.layers.{i}.self_attn.attention_weights.weight\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.self_attn.attention_weights.bias\", f\"model.encoder.layers.{i}.self_attn.attention_weights.bias\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.self_attn.value_proj.weight\", f\"model.encoder.layers.{i}.self_attn.value_proj.weight\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.self_attn.value_proj.bias\", f\"model.encoder.layers.{i}.self_attn.value_proj.bias\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.self_attn.output_proj.weight\", f\"model.encoder.layers.{i}.self_attn.output_proj.weight\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.self_attn.output_proj.bias\", f\"model.encoder.layers.{i}.self_attn.output_proj.bias\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.norm1.weight\", f\"model.encoder.layers.{i}.self_attn_layer_norm.weight\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.norm1.bias\", f\"model.encoder.layers.{i}.self_attn_layer_norm.bias\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.linear1.weight\", f\"model.encoder.layers.{i}.fc1.weight\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.linear1.bias\", f\"model.encoder.layers.{i}.fc1.bias\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.linear2.weight\", f\"model.encoder.layers.{i}.fc2.weight\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.linear2.bias\", f\"model.encoder.layers.{i}.fc2.bias\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.norm2.weight\", f\"model.encoder.layers.{i}.final_layer_norm.weight\"))\n-        rename_keys.append((f\"transformer.encoder.layers.{i}.norm2.bias\", f\"model.encoder.layers.{i}.final_layer_norm.bias\"))\n-\n-    # transformer decoder\n-    for i in range(config.decoder_layers):\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.cross_attn.sampling_offsets.weight\", f\"model.decoder.layers.{i}.encoder_attn.sampling_offsets.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.cross_attn.sampling_offsets.bias\", f\"model.decoder.layers.{i}.encoder_attn.sampling_offsets.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.cross_attn.attention_weights.weight\", f\"model.decoder.layers.{i}.encoder_attn.attention_weights.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.cross_attn.attention_weights.bias\", f\"model.decoder.layers.{i}.encoder_attn.attention_weights.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.cross_attn.value_proj.weight\", f\"model.decoder.layers.{i}.encoder_attn.value_proj.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.cross_attn.value_proj.bias\", f\"model.decoder.layers.{i}.encoder_attn.value_proj.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.cross_attn.output_proj.weight\", f\"model.decoder.layers.{i}.encoder_attn.output_proj.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.cross_attn.output_proj.bias\", f\"model.decoder.layers.{i}.encoder_attn.output_proj.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.norm1.weight\", f\"model.decoder.layers.{i}.encoder_attn_layer_norm.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.norm1.bias\", f\"model.decoder.layers.{i}.encoder_attn_layer_norm.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.self_attn.out_proj.weight\", f\"model.decoder.layers.{i}.self_attn.out_proj.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.self_attn.out_proj.bias\", f\"model.decoder.layers.{i}.self_attn.out_proj.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.norm2.weight\", f\"model.decoder.layers.{i}.self_attn_layer_norm.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.norm2.bias\", f\"model.decoder.layers.{i}.self_attn_layer_norm.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.linear1.weight\", f\"model.decoder.layers.{i}.fc1.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.linear1.bias\", f\"model.decoder.layers.{i}.fc1.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.linear2.weight\", f\"model.decoder.layers.{i}.fc2.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.linear2.bias\", f\"model.decoder.layers.{i}.fc2.bias\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.norm3.weight\", f\"model.decoder.layers.{i}.final_layer_norm.weight\"))\n-        rename_keys.append((f\"transformer.decoder.layers.{i}.norm3.bias\", f\"model.decoder.layers.{i}.final_layer_norm.bias\"))\n-\n-    # fmt: on\n-\n-    return rename_keys\n-\n-\n-def rename_key(dct, old, new):\n-    val = dct.pop(old)\n-    dct[new] = val\n-\n-\n-# we split up the matrix of each encoder layer into queries, keys and values\n-def read_in_swin_q_k_v(state_dict, backbone_config):\n-    num_features = [int(backbone_config.embed_dim * 2**i) for i in range(len(backbone_config.depths))]\n-    for i in range(len(backbone_config.depths)):\n-        dim = num_features[i]\n-        for j in range(backbone_config.depths[i]):\n-            # fmt: off\n-            # read in weights + bias of input projection layer (in original implementation, this is a single matrix + bias)\n-            in_proj_weight = state_dict.pop(f\"backbone.0.body.layers.{i}.blocks.{j}.attn.qkv.weight\")\n-            in_proj_bias = state_dict.pop(f\"backbone.0.body.layers.{i}.blocks.{j}.attn.qkv.bias\")\n-            # next, add query, keys and values (in that order) to the state dict\n-            state_dict[f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.attention.self.query.weight\"] = in_proj_weight[:dim, :]\n-            state_dict[f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.attention.self.query.bias\"] = in_proj_bias[: dim]\n-            state_dict[f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.attention.self.key.weight\"] = in_proj_weight[\n-                dim : dim * 2, :\n-            ]\n-            state_dict[f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.attention.self.key.bias\"] = in_proj_bias[\n-                dim : dim * 2\n-            ]\n-            state_dict[f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.attention.self.value.weight\"] = in_proj_weight[\n-                -dim :, :\n-            ]\n-            state_dict[f\"model.backbone.model.encoder.layers.{i}.blocks.{j}.attention.self.value.bias\"] = in_proj_bias[-dim :]\n-            # fmt: on\n-\n-\n-def read_in_decoder_q_k_v(state_dict, config):\n-    # transformer decoder self-attention layers\n-    hidden_size = config.d_model\n-    for i in range(config.decoder_layers):\n-        # read in weights + bias of input projection layer of self-attention\n-        in_proj_weight = state_dict.pop(f\"transformer.decoder.layers.{i}.self_attn.in_proj_weight\")\n-        in_proj_bias = state_dict.pop(f\"transformer.decoder.layers.{i}.self_attn.in_proj_bias\")\n-        # next, add query, keys and values (in that order) to the state dict\n-        state_dict[f\"model.decoder.layers.{i}.self_attn.q_proj.weight\"] = in_proj_weight[:hidden_size, :]\n-        state_dict[f\"model.decoder.layers.{i}.self_attn.q_proj.bias\"] = in_proj_bias[:hidden_size]\n-        state_dict[f\"model.decoder.layers.{i}.self_attn.k_proj.weight\"] = in_proj_weight[\n-            hidden_size : hidden_size * 2, :\n-        ]\n-        state_dict[f\"model.decoder.layers.{i}.self_attn.k_proj.bias\"] = in_proj_bias[hidden_size : hidden_size * 2]\n-        state_dict[f\"model.decoder.layers.{i}.self_attn.v_proj.weight\"] = in_proj_weight[-hidden_size:, :]\n-        state_dict[f\"model.decoder.layers.{i}.self_attn.v_proj.bias\"] = in_proj_bias[-hidden_size:]\n-\n-\n-# We will verify our results on an image of cute cats\n-def prepare_img():\n-    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-    im = Image.open(requests.get(url, stream=True).raw)\n-\n-    return im\n-\n-\n-@torch.no_grad()\n-def convert_deta_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub):\n-    \"\"\"\n-    Copy/paste/tweak model's weights to our DETA structure.\n-    \"\"\"\n-\n-    # load config\n-    config = get_deta_config(model_name)\n-\n-    # load original state dict\n-    if model_name == \"deta-swin-large\":\n-        checkpoint_path = hf_hub_download(repo_id=\"nielsr/deta-checkpoints\", filename=\"adet_swin_ft.pth\")\n-    elif model_name == \"deta-swin-large-o365\":\n-        checkpoint_path = hf_hub_download(repo_id=\"jozhang97/deta-swin-l-o365\", filename=\"deta_swin_pt_o365.pth\")\n-    else:\n-        raise ValueError(f\"Model name {model_name} not supported\")\n-\n-    state_dict = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)[\"model\"]\n-\n-    # original state dict\n-    for name, param in state_dict.items():\n-        print(name, param.shape)\n-\n-    # rename keys\n-    rename_keys = create_rename_keys(config)\n-    for src, dest in rename_keys:\n-        rename_key(state_dict, src, dest)\n-    read_in_swin_q_k_v(state_dict, config.backbone_config)\n-    read_in_decoder_q_k_v(state_dict, config)\n-\n-    # fix some prefixes\n-    for key in state_dict.copy():\n-        if \"transformer.decoder.class_embed\" in key or \"transformer.decoder.bbox_embed\" in key:\n-            val = state_dict.pop(key)\n-            state_dict[key.replace(\"transformer.decoder\", \"model.decoder\")] = val\n-        if \"input_proj\" in key:\n-            val = state_dict.pop(key)\n-            state_dict[\"model.\" + key] = val\n-        if \"level_embed\" in key or \"pos_trans\" in key or \"pix_trans\" in key or \"enc_output\" in key:\n-            val = state_dict.pop(key)\n-            state_dict[key.replace(\"transformer\", \"model\")] = val\n-\n-    # finally, create HuggingFace model and load state dict\n-    model = DetaForObjectDetection(config)\n-    model.load_state_dict(state_dict)\n-    model.eval()\n-\n-    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n-    model.to(device)\n-\n-    # load image processor\n-    processor = DetaImageProcessor(format=\"coco_detection\")\n-\n-    # verify our conversion on image\n-    img = prepare_img()\n-    encoding = processor(images=img, return_tensors=\"pt\")\n-    pixel_values = encoding[\"pixel_values\"]\n-    outputs = model(pixel_values.to(device))\n-\n-    # verify logits\n-    print(\"Logits:\", outputs.logits[0, :3, :3])\n-    print(\"Boxes:\", outputs.pred_boxes[0, :3, :3])\n-    if model_name == \"deta-swin-large\":\n-        expected_logits = torch.tensor(\n-            [[-7.6308, -2.8485, -5.3737], [-7.2037, -4.5505, -4.8027], [-7.2943, -4.2611, -4.6617]]\n-        )\n-        expected_boxes = torch.tensor([[0.4987, 0.4969, 0.9999], [0.2549, 0.5498, 0.4805], [0.5498, 0.2757, 0.0569]])\n-    elif model_name == \"deta-swin-large-o365\":\n-        expected_logits = torch.tensor(\n-            [[-8.0122, -3.5720, -4.9717], [-8.1547, -3.6886, -4.6389], [-7.6610, -3.6194, -5.0134]]\n-        )\n-        expected_boxes = torch.tensor([[0.2523, 0.5549, 0.4881], [0.7715, 0.4149, 0.4601], [0.5503, 0.2753, 0.0575]])\n-    assert torch.allclose(outputs.logits[0, :3, :3], expected_logits.to(device), atol=1e-4)\n-    assert torch.allclose(outputs.pred_boxes[0, :3, :3], expected_boxes.to(device), atol=1e-4)\n-    print(\"Everything ok!\")\n-\n-    if pytorch_dump_folder_path:\n-        # Save model and processor\n-        logger.info(f\"Saving PyTorch model and processor to {pytorch_dump_folder_path}...\")\n-        Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n-        model.save_pretrained(pytorch_dump_folder_path)\n-        processor.save_pretrained(pytorch_dump_folder_path)\n-\n-    # Push to hub\n-    if push_to_hub:\n-        print(\"Pushing model and processor to hub...\")\n-        model.push_to_hub(f\"jozhang97/{model_name}\")\n-        processor.push_to_hub(f\"jozhang97/{model_name}\")\n-\n-\n-if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser()\n-\n-    parser.add_argument(\n-        \"--model_name\",\n-        type=str,\n-        default=\"deta-swin-large\",\n-        choices=[\"deta-swin-large\", \"deta-swin-large-o365\"],\n-        help=\"Name of the model you'd like to convert.\",\n-    )\n-    parser.add_argument(\n-        \"--pytorch_dump_folder_path\",\n-        default=None,\n-        type=str,\n-        help=\"Path to the folder to output PyTorch model.\",\n-    )\n-    parser.add_argument(\n-        \"--push_to_hub\",\n-        action=\"store_true\",\n-        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n-    )\n-    args = parser.parse_args()\n-    convert_deta_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.push_to_hub)"
        },
        {
            "sha": "b3fa7169c1105224f57cdcc7d6a3c547c68470a7",
            "filename": "src/transformers/models/deprecated/deta/image_processing_deta.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1153,
            "changes": 1153,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,1153 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Image processor class for Deformable DETR.\"\"\"\n-\n-import pathlib\n-from collections.abc import Iterable\n-from typing import Any, Optional, Union\n-\n-import numpy as np\n-\n-from ....feature_extraction_utils import BatchFeature\n-from ....image_processing_utils import BaseImageProcessor, get_size_dict\n-from ....image_transforms import (\n-    PaddingMode,\n-    center_to_corners_format,\n-    corners_to_center_format,\n-    get_size_with_aspect_ratio,\n-    pad,\n-    rescale,\n-    resize,\n-    rgb_to_id,\n-    to_channel_dimension_format,\n-)\n-from ....image_utils import (\n-    IMAGENET_DEFAULT_MEAN,\n-    IMAGENET_DEFAULT_STD,\n-    AnnotationFormat,\n-    AnnotationType,\n-    ChannelDimension,\n-    ImageInput,\n-    PILImageResampling,\n-    get_image_size,\n-    infer_channel_dimension_format,\n-    is_batched,\n-    is_scaled_image,\n-    to_numpy_array,\n-    valid_images,\n-    validate_annotations,\n-    validate_preprocess_arguments,\n-)\n-from ....utils import (\n-    is_torch_available,\n-    is_torchvision_available,\n-    is_vision_available,\n-    logging,\n-)\n-from ....utils.generic import TensorType\n-\n-\n-if is_torch_available():\n-    import torch\n-\n-\n-if is_torchvision_available():\n-    from torchvision.ops.boxes import batched_nms\n-\n-if is_vision_available():\n-    import PIL\n-\n-\n-logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n-\n-SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n-\n-\n-def get_resize_output_image_size(\n-    input_image: np.ndarray,\n-    size: Union[int, tuple[int, int], list[int]],\n-    max_size: Optional[int] = None,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-) -> tuple[int, int]:\n-    \"\"\"\n-    Computes the output image size given the input image size and the desired output size. If the desired output size\n-    is a tuple or list, the output image size is returned as is. If the desired output size is an integer, the output\n-    image size is computed by keeping the aspect ratio of the input image size.\n-\n-    Args:\n-        input_image (`np.ndarray`):\n-            The image to resize.\n-        size (`int` or `tuple[int, int]` or `list[int]`):\n-            The desired output size.\n-        max_size (`int`, *optional*):\n-            The maximum allowed output size.\n-        input_data_format (`ChannelDimension` or `str`, *optional*):\n-            The channel dimension format of the input image. If not provided, it will be inferred from the input image.\n-    \"\"\"\n-    image_size = get_image_size(input_image, input_data_format)\n-    if isinstance(size, (list, tuple)):\n-        return size\n-\n-    return get_size_with_aspect_ratio(image_size, size, max_size)\n-\n-\n-def get_image_size_for_max_height_width(\n-    input_image: np.ndarray,\n-    max_height: int,\n-    max_width: int,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-) -> tuple[int, int]:\n-    \"\"\"\n-    Computes the output image size given the input image and the maximum allowed height and width. Keep aspect ratio.\n-    Important, even if image_height < max_height and image_width < max_width, the image will be resized\n-    to at least one of the edges be equal to max_height or max_width.\n-\n-    For example:\n-        - input_size: (100, 200), max_height: 50, max_width: 50 -> output_size: (25, 50)\n-        - input_size: (100, 200), max_height: 200, max_width: 500 -> output_size: (200, 400)\n-\n-    Args:\n-        input_image (`np.ndarray`):\n-            The image to resize.\n-        max_height (`int`):\n-            The maximum allowed height.\n-        max_width (`int`):\n-            The maximum allowed width.\n-        input_data_format (`ChannelDimension` or `str`, *optional*):\n-            The channel dimension format of the input image. If not provided, it will be inferred from the input image.\n-    \"\"\"\n-    image_size = get_image_size(input_image, input_data_format)\n-    height, width = image_size\n-    height_scale = max_height / height\n-    width_scale = max_width / width\n-    min_scale = min(height_scale, width_scale)\n-    new_height = int(height * min_scale)\n-    new_width = int(width * min_scale)\n-    return new_height, new_width\n-\n-\n-def safe_squeeze(arr: np.ndarray, axis: Optional[int] = None) -> np.ndarray:\n-    \"\"\"\n-    Squeezes an array, but only if the axis specified has dim 1.\n-    \"\"\"\n-    if axis is None:\n-        return arr.squeeze()\n-\n-    try:\n-        return arr.squeeze(axis=axis)\n-    except ValueError:\n-        return arr\n-\n-\n-def normalize_annotation(annotation: dict, image_size: tuple[int, int]) -> dict:\n-    image_height, image_width = image_size\n-    norm_annotation = {}\n-    for key, value in annotation.items():\n-        if key == \"boxes\":\n-            boxes = value\n-            boxes = corners_to_center_format(boxes)\n-            boxes /= np.asarray([image_width, image_height, image_width, image_height], dtype=np.float32)\n-            norm_annotation[key] = boxes\n-        else:\n-            norm_annotation[key] = value\n-    return norm_annotation\n-\n-\n-def max_across_indices(values: Iterable[Any]) -> list[Any]:\n-    \"\"\"\n-    Return the maximum value across all indices of an iterable of values.\n-    \"\"\"\n-    return [max(values_i) for values_i in zip(*values)]\n-\n-\n-def get_max_height_width(\n-    images: list[np.ndarray], input_data_format: Optional[Union[str, ChannelDimension]] = None\n-) -> list[int]:\n-    \"\"\"\n-    Get the maximum height and width across all images in a batch.\n-    \"\"\"\n-    if input_data_format is None:\n-        input_data_format = infer_channel_dimension_format(images[0])\n-\n-    if input_data_format == ChannelDimension.FIRST:\n-        _, max_height, max_width = max_across_indices([img.shape for img in images])\n-    elif input_data_format == ChannelDimension.LAST:\n-        max_height, max_width, _ = max_across_indices([img.shape for img in images])\n-    else:\n-        raise ValueError(f\"Invalid channel dimension format: {input_data_format}\")\n-    return (max_height, max_width)\n-\n-\n-def make_pixel_mask(\n-    image: np.ndarray, output_size: tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n-) -> np.ndarray:\n-    \"\"\"\n-    Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n-\n-    Args:\n-        image (`np.ndarray`):\n-            Image to make the pixel mask for.\n-        output_size (`tuple[int, int]`):\n-            Output size of the mask.\n-    \"\"\"\n-    input_height, input_width = get_image_size(image, channel_dim=input_data_format)\n-    mask = np.zeros(output_size, dtype=np.int64)\n-    mask[:input_height, :input_width] = 1\n-    return mask\n-\n-\n-def convert_coco_poly_to_mask(segmentations, height: int, width: int) -> np.ndarray:\n-    \"\"\"\n-    Convert a COCO polygon annotation to a mask.\n-\n-    Args:\n-        segmentations (`list[list[float]]`):\n-            List of polygons, each polygon represented by a list of x-y coordinates.\n-        height (`int`):\n-            Height of the mask.\n-        width (`int`):\n-            Width of the mask.\n-    \"\"\"\n-    try:\n-        from pycocotools import mask as coco_mask\n-    except ImportError:\n-        raise ImportError(\"Pycocotools is not installed in your environment.\")\n-\n-    masks = []\n-    for polygons in segmentations:\n-        rles = coco_mask.frPyObjects(polygons, height, width)\n-        mask = coco_mask.decode(rles)\n-        if len(mask.shape) < 3:\n-            mask = mask[..., None]\n-        mask = np.asarray(mask, dtype=np.uint8)\n-        mask = np.any(mask, axis=2)\n-        masks.append(mask)\n-    if masks:\n-        masks = np.stack(masks, axis=0)\n-    else:\n-        masks = np.zeros((0, height, width), dtype=np.uint8)\n-\n-    return masks\n-\n-\n-def prepare_coco_detection_annotation(\n-    image,\n-    target,\n-    return_segmentation_masks: bool = False,\n-    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n-):\n-    \"\"\"\n-    Convert the target in COCO format into the format expected by DETA.\n-    \"\"\"\n-    image_height, image_width = get_image_size(image, channel_dim=input_data_format)\n-\n-    image_id = target[\"image_id\"]\n-    image_id = np.asarray([image_id], dtype=np.int64)\n-\n-    # Get all COCO annotations for the given image.\n-    annotations = target[\"annotations\"]\n-    annotations = [obj for obj in annotations if \"iscrowd\" not in obj or obj[\"iscrowd\"] == 0]\n-\n-    classes = [obj[\"category_id\"] for obj in annotations]\n-    classes = np.asarray(classes, dtype=np.int64)\n-\n-    # for conversion to coco api\n-    area = np.asarray([obj[\"area\"] for obj in annotations], dtype=np.float32)\n-    iscrowd = np.asarray([obj.get(\"iscrowd\", 0) for obj in annotations], dtype=np.int64)\n-\n-    boxes = [obj[\"bbox\"] for obj in annotations]\n-    # guard against no boxes via resizing\n-    boxes = np.asarray(boxes, dtype=np.float32).reshape(-1, 4)\n-    boxes[:, 2:] += boxes[:, :2]\n-    boxes[:, 0::2] = boxes[:, 0::2].clip(min=0, max=image_width)\n-    boxes[:, 1::2] = boxes[:, 1::2].clip(min=0, max=image_height)\n-\n-    keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n-\n-    new_target = {}\n-    new_target[\"image_id\"] = image_id\n-    new_target[\"class_labels\"] = classes[keep]\n-    new_target[\"boxes\"] = boxes[keep]\n-    new_target[\"area\"] = area[keep]\n-    new_target[\"iscrowd\"] = iscrowd[keep]\n-    new_target[\"orig_size\"] = np.asarray([int(image_height), int(image_width)], dtype=np.int64)\n-\n-    if annotations and \"keypoints\" in annotations[0]:\n-        keypoints = [obj[\"keypoints\"] for obj in annotations]\n-        # Converting the filtered keypoints list to a numpy array\n-        keypoints = np.asarray(keypoints, dtype=np.float32)\n-        # Apply the keep mask here to filter the relevant annotations\n-        keypoints = keypoints[keep]\n-        num_keypoints = keypoints.shape[0]\n-        keypoints = keypoints.reshape((-1, 3)) if num_keypoints else keypoints\n-        new_target[\"keypoints\"] = keypoints\n-\n-    if return_segmentation_masks:\n-        segmentation_masks = [obj[\"segmentation\"] for obj in annotations]\n-        masks = convert_coco_poly_to_mask(segmentation_masks, image_height, image_width)\n-        new_target[\"masks\"] = masks[keep]\n-\n-    return new_target\n-\n-\n-def masks_to_boxes(masks: np.ndarray) -> np.ndarray:\n-    \"\"\"\n-    Compute the bounding boxes around the provided panoptic segmentation masks.\n-\n-    Args:\n-        masks: masks in format `[number_masks, height, width]` where N is the number of masks\n-\n-    Returns:\n-        boxes: bounding boxes in format `[number_masks, 4]` in xyxy format\n-    \"\"\"\n-    if masks.size == 0:\n-        return np.zeros((0, 4))\n-\n-    h, w = masks.shape[-2:]\n-    y = np.arange(0, h, dtype=np.float32)\n-    x = np.arange(0, w, dtype=np.float32)\n-    # see https://github.com/pytorch/pytorch/issues/50276\n-    y, x = np.meshgrid(y, x, indexing=\"ij\")\n-\n-    x_mask = masks * np.expand_dims(x, axis=0)\n-    x_max = x_mask.reshape(x_mask.shape[0], -1).max(-1)\n-    x = np.ma.array(x_mask, mask=~(np.array(masks, dtype=bool)))\n-    x_min = x.filled(fill_value=1e8)\n-    x_min = x_min.reshape(x_min.shape[0], -1).min(-1)\n-\n-    y_mask = masks * np.expand_dims(y, axis=0)\n-    y_max = y_mask.reshape(x_mask.shape[0], -1).max(-1)\n-    y = np.ma.array(y_mask, mask=~(np.array(masks, dtype=bool)))\n-    y_min = y.filled(fill_value=1e8)\n-    y_min = y_min.reshape(y_min.shape[0], -1).min(-1)\n-\n-    return np.stack([x_min, y_min, x_max, y_max], 1)\n-\n-\n-def prepare_coco_panoptic_annotation(\n-    image: np.ndarray,\n-    target: dict,\n-    masks_path: Union[str, pathlib.Path],\n-    return_masks: bool = True,\n-    input_data_format: Union[ChannelDimension, str] = None,\n-) -> dict:\n-    \"\"\"\n-    Prepare a coco panoptic annotation for DETA.\n-    \"\"\"\n-    image_height, image_width = get_image_size(image, channel_dim=input_data_format)\n-    annotation_path = pathlib.Path(masks_path) / target[\"file_name\"]\n-\n-    new_target = {}\n-    new_target[\"image_id\"] = np.asarray([target[\"image_id\"] if \"image_id\" in target else target[\"id\"]], dtype=np.int64)\n-    new_target[\"size\"] = np.asarray([image_height, image_width], dtype=np.int64)\n-    new_target[\"orig_size\"] = np.asarray([image_height, image_width], dtype=np.int64)\n-\n-    if \"segments_info\" in target:\n-        masks = np.asarray(PIL.Image.open(annotation_path), dtype=np.uint32)\n-        masks = rgb_to_id(masks)\n-\n-        ids = np.array([segment_info[\"id\"] for segment_info in target[\"segments_info\"]])\n-        masks = masks == ids[:, None, None]\n-        masks = masks.astype(np.uint8)\n-        if return_masks:\n-            new_target[\"masks\"] = masks\n-        new_target[\"boxes\"] = masks_to_boxes(masks)\n-        new_target[\"class_labels\"] = np.array(\n-            [segment_info[\"category_id\"] for segment_info in target[\"segments_info\"]], dtype=np.int64\n-        )\n-        new_target[\"iscrowd\"] = np.asarray(\n-            [segment_info[\"iscrowd\"] for segment_info in target[\"segments_info\"]], dtype=np.int64\n-        )\n-        new_target[\"area\"] = np.asarray(\n-            [segment_info[\"area\"] for segment_info in target[\"segments_info\"]], dtype=np.float32\n-        )\n-\n-    return new_target\n-\n-\n-def resize_annotation(\n-    annotation: dict[str, Any],\n-    orig_size: tuple[int, int],\n-    target_size: tuple[int, int],\n-    threshold: float = 0.5,\n-    resample: PILImageResampling = PILImageResampling.NEAREST,\n-):\n-    \"\"\"\n-    Resizes an annotation to a target size.\n-\n-    Args:\n-        annotation (`dict[str, Any]`):\n-            The annotation dictionary.\n-        orig_size (`tuple[int, int]`):\n-            The original size of the input image.\n-        target_size (`tuple[int, int]`):\n-            The target size of the image, as returned by the preprocessing `resize` step.\n-        threshold (`float`, *optional*, defaults to 0.5):\n-            The threshold used to binarize the segmentation masks.\n-        resample (`PILImageResampling`, defaults to `PILImageResampling.NEAREST`):\n-            The resampling filter to use when resizing the masks.\n-    \"\"\"\n-    ratios = tuple(float(s) / float(s_orig) for s, s_orig in zip(target_size, orig_size))\n-    ratio_height, ratio_width = ratios\n-\n-    new_annotation = {}\n-    new_annotation[\"size\"] = target_size\n-\n-    for key, value in annotation.items():\n-        if key == \"boxes\":\n-            boxes = value\n-            scaled_boxes = boxes * np.asarray([ratio_width, ratio_height, ratio_width, ratio_height], dtype=np.float32)\n-            new_annotation[\"boxes\"] = scaled_boxes\n-        elif key == \"area\":\n-            area = value\n-            scaled_area = area * (ratio_width * ratio_height)\n-            new_annotation[\"area\"] = scaled_area\n-        elif key == \"masks\":\n-            masks = value[:, None]\n-            masks = np.array([resize(mask, target_size, resample=resample) for mask in masks])\n-            masks = masks.astype(np.float32)\n-            masks = masks[:, 0] > threshold\n-            new_annotation[\"masks\"] = masks\n-        elif key == \"size\":\n-            new_annotation[\"size\"] = target_size\n-        else:\n-            new_annotation[key] = value\n-\n-    return new_annotation\n-\n-\n-class DetaImageProcessor(BaseImageProcessor):\n-    r\"\"\"\n-    Constructs a Deformable DETR image processor.\n-\n-    Args:\n-        format (`str`, *optional*, defaults to `\"coco_detection\"`):\n-            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n-        do_resize (`bool`, *optional*, defaults to `True`):\n-            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be\n-            overridden by the `do_resize` parameter in the `preprocess` method.\n-        size (`dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 800, \"longest_edge\": 1333}`):\n-            Size of the image's `(height, width)` dimensions after resizing. Can be overridden by the `size` parameter\n-            in the `preprocess` method. Available options are:\n-                - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n-                    Do NOT keep the aspect ratio.\n-                - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n-                    the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n-                    less or equal to `longest_edge`.\n-                - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n-                    aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n-                    `max_width`.\n-        resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`):\n-            Resampling filter to use if resizing the image.\n-        do_rescale (`bool`, *optional*, defaults to `True`):\n-            Controls whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the\n-            `do_rescale` parameter in the `preprocess` method.\n-        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Can be overridden by the `rescale_factor` parameter in the\n-            `preprocess` method.\n-        do_normalize:\n-            Controls whether to normalize the image. Can be overridden by the `do_normalize` parameter in the\n-            `preprocess` method.\n-        image_mean (`float` or `list[float]`, *optional*, defaults to `IMAGENET_DEFAULT_MEAN`):\n-            Mean values to use when normalizing the image. Can be a single value or a list of values, one for each\n-            channel. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `list[float]`, *optional*, defaults to `IMAGENET_DEFAULT_STD`):\n-            Standard deviation values to use when normalizing the image. Can be a single value or a list of values, one\n-            for each channel. Can be overridden by the `image_std` parameter in the `preprocess` method.\n-        do_convert_annotations (`bool`, *optional*, defaults to `True`):\n-            Controls whether to convert the annotations to the format expected by the DETR model. Converts the\n-            bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n-            Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-        do_pad (`bool`, *optional*, defaults to `True`):\n-            Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-            method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n-            If `pad_size` is provided, the image will be padded to the specified dimensions.\n-            Otherwise, the image will be padded to the maximum height and width of the batch.\n-        pad_size (`dict[str, int]`, *optional*):\n-            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n-            height and width in the batch.\n-    \"\"\"\n-\n-    model_input_names = [\"pixel_values\", \"pixel_mask\"]\n-\n-    def __init__(\n-        self,\n-        format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n-        do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n-        do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_annotations: bool = True,\n-        do_pad: bool = True,\n-        pad_size: Optional[dict[str, int]] = None,\n-        **kwargs,\n-    ) -> None:\n-        size = size if size is not None else {\"shortest_edge\": 800, \"longest_edge\": 1333}\n-        size = get_size_dict(size, default_to_square=False)\n-\n-        if do_convert_annotations is None:\n-            do_convert_annotations = do_normalize\n-\n-        super().__init__(**kwargs)\n-        self.format = format\n-        self.do_resize = do_resize\n-        self.size = size\n-        self.resample = resample\n-        self.do_rescale = do_rescale\n-        self.rescale_factor = rescale_factor\n-        self.do_normalize = do_normalize\n-        self.do_convert_annotations = do_convert_annotations\n-        self.image_mean = image_mean if image_mean is not None else IMAGENET_DEFAULT_MEAN\n-        self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n-        self.do_pad = kwargs.pop(\"pad_and_return_pixel_mask\", do_pad)\n-        self.pad_size = pad_size\n-\n-    def prepare_annotation(\n-        self,\n-        image: np.ndarray,\n-        target: dict,\n-        format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> dict:\n-        \"\"\"\n-        Prepare an annotation for feeding into DETA model.\n-        \"\"\"\n-        format = format if format is not None else self.format\n-\n-        if format == AnnotationFormat.COCO_DETECTION:\n-            return_segmentation_masks = False if return_segmentation_masks is None else return_segmentation_masks\n-            target = prepare_coco_detection_annotation(\n-                image, target, return_segmentation_masks, input_data_format=input_data_format\n-            )\n-        elif format == AnnotationFormat.COCO_PANOPTIC:\n-            return_segmentation_masks = True if return_segmentation_masks is None else return_segmentation_masks\n-            target = prepare_coco_panoptic_annotation(\n-                image,\n-                target,\n-                masks_path=masks_path,\n-                return_masks=return_segmentation_masks,\n-                input_data_format=input_data_format,\n-            )\n-        else:\n-            raise ValueError(f\"Format {format} is not supported.\")\n-        return target\n-\n-    def resize(\n-        self,\n-        image: np.ndarray,\n-        size: dict[str, int],\n-        resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        **kwargs,\n-    ) -> np.ndarray:\n-        \"\"\"\n-        Resize the image to the given size. Size can be `min_size` (scalar) or `(height, width)` tuple. If size is an\n-        int, smaller edge of the image will be matched to this number.\n-\n-        Args:\n-            image (`np.ndarray`):\n-                Image to resize.\n-            size (`dict[str, int]`):\n-                Size of the image's `(height, width)` dimensions after resizing. Available options are:\n-                    - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n-                        Do NOT keep the aspect ratio.\n-                    - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n-                        the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n-                        less or equal to `longest_edge`.\n-                    - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n-                        aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n-                        `max_width`.\n-            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`):\n-                Resampling filter to use if resizing the image.\n-            data_format (`ChannelDimension`, *optional*):\n-                The channel dimension format for the output image. If unset, the channel dimension format of the input\n-                image is used.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format of the input image. If not provided, it will be inferred from the input\n-                image.\n-        \"\"\"\n-        size = get_size_dict(size, default_to_square=False)\n-        if \"shortest_edge\" in size and \"longest_edge\" in size:\n-            new_size = get_resize_output_image_size(\n-                image, size[\"shortest_edge\"], size[\"longest_edge\"], input_data_format=input_data_format\n-            )\n-        elif \"height\" in size and \"width\" in size:\n-            new_size = (size[\"height\"], size[\"width\"])\n-        elif \"max_height\" in size and \"max_width\" in size:\n-            new_size = get_image_size_for_max_height_width(\n-                image, size[\"max_height\"], size[\"max_width\"], input_data_format=input_data_format\n-            )\n-        else:\n-            raise ValueError(\n-                \"Size must contain 'height' and 'width' keys or 'shortest_edge' and 'longest_edge' keys. Got\"\n-                f\" {size.keys()}.\"\n-            )\n-        image = resize(\n-            image, size=new_size, resample=resample, data_format=data_format, input_data_format=input_data_format\n-        )\n-        return image\n-\n-    def resize_annotation(\n-        self,\n-        annotation,\n-        orig_size,\n-        size,\n-        resample: PILImageResampling = PILImageResampling.NEAREST,\n-    ) -> dict:\n-        \"\"\"\n-        Resize the annotation to match the resized image. If size is an int, smaller edge of the mask will be matched\n-        to this number.\n-        \"\"\"\n-        return resize_annotation(annotation, orig_size=orig_size, target_size=size, resample=resample)\n-\n-    def rescale(\n-        self,\n-        image: np.ndarray,\n-        rescale_factor: float,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> np.ndarray:\n-        \"\"\"\n-        Rescale the image by the given factor. image = image * rescale_factor.\n-\n-        Args:\n-            image (`np.ndarray`):\n-                Image to rescale.\n-            rescale_factor (`float`):\n-                The value to use for rescaling.\n-            data_format (`str` or `ChannelDimension`, *optional*):\n-                The channel dimension format for the output image. If unset, the channel dimension format of the input\n-                image is used. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-            input_data_format (`str` or `ChannelDimension`, *optional*):\n-                The channel dimension format for the input image. If unset, is inferred from the input image. Can be\n-                one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-        \"\"\"\n-        return rescale(image, rescale_factor, data_format=data_format, input_data_format=input_data_format)\n-\n-    def normalize_annotation(self, annotation: dict, image_size: tuple[int, int]) -> dict:\n-        \"\"\"\n-        Normalize the boxes in the annotation from `[top_left_x, top_left_y, bottom_right_x, bottom_right_y]` to\n-        `[center_x, center_y, width, height]` format and from absolute to relative pixel values.\n-        \"\"\"\n-        return normalize_annotation(annotation, image_size=image_size)\n-\n-    def _update_annotation_for_padded_image(\n-        self,\n-        annotation: dict,\n-        input_image_size: tuple[int, int],\n-        output_image_size: tuple[int, int],\n-        padding,\n-        update_bboxes,\n-    ) -> dict:\n-        \"\"\"\n-        Update the annotation for a padded image.\n-        \"\"\"\n-        new_annotation = {}\n-        new_annotation[\"size\"] = output_image_size\n-\n-        for key, value in annotation.items():\n-            if key == \"masks\":\n-                masks = value\n-                masks = pad(\n-                    masks,\n-                    padding,\n-                    mode=PaddingMode.CONSTANT,\n-                    constant_values=0,\n-                    input_data_format=ChannelDimension.FIRST,\n-                )\n-                masks = safe_squeeze(masks, 1)\n-                new_annotation[\"masks\"] = masks\n-            elif key == \"boxes\" and update_bboxes:\n-                boxes = value\n-                boxes *= np.asarray(\n-                    [\n-                        input_image_size[1] / output_image_size[1],\n-                        input_image_size[0] / output_image_size[0],\n-                        input_image_size[1] / output_image_size[1],\n-                        input_image_size[0] / output_image_size[0],\n-                    ]\n-                )\n-                new_annotation[\"boxes\"] = boxes\n-            elif key == \"size\":\n-                new_annotation[\"size\"] = output_image_size\n-            else:\n-                new_annotation[key] = value\n-        return new_annotation\n-\n-    def _pad_image(\n-        self,\n-        image: np.ndarray,\n-        output_size: tuple[int, int],\n-        annotation: Optional[dict[str, Any]] = None,\n-        constant_values: Union[float, Iterable[float]] = 0,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        update_bboxes: bool = True,\n-    ) -> np.ndarray:\n-        \"\"\"\n-        Pad an image with zeros to the given size.\n-        \"\"\"\n-        input_height, input_width = get_image_size(image, channel_dim=input_data_format)\n-        output_height, output_width = output_size\n-\n-        pad_bottom = output_height - input_height\n-        pad_right = output_width - input_width\n-        padding = ((0, pad_bottom), (0, pad_right))\n-        padded_image = pad(\n-            image,\n-            padding,\n-            mode=PaddingMode.CONSTANT,\n-            constant_values=constant_values,\n-            data_format=data_format,\n-            input_data_format=input_data_format,\n-        )\n-        if annotation is not None:\n-            annotation = self._update_annotation_for_padded_image(\n-                annotation, (input_height, input_width), (output_height, output_width), padding, update_bboxes\n-            )\n-        return padded_image, annotation\n-\n-    def pad(\n-        self,\n-        images: list[np.ndarray],\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        constant_values: Union[float, Iterable[float]] = 0,\n-        return_pixel_mask: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        update_bboxes: bool = True,\n-        pad_size: Optional[dict[str, int]] = None,\n-    ) -> BatchFeature:\n-        \"\"\"\n-        Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n-        in the batch and optionally returns their corresponding pixel mask.\n-\n-        Args:\n-            images (list[`np.ndarray`]):\n-                Images to pad.\n-            annotations (`AnnotationType` or `list[AnnotationType]`, *optional*):\n-                Annotations to transform according to the padding that is applied to the images.\n-            constant_values (`float` or `Iterable[float]`, *optional*):\n-                The value to use for the padding if `mode` is `\"constant\"`.\n-            return_pixel_mask (`bool`, *optional*, defaults to `True`):\n-                Whether to return a pixel mask.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return. Can be one of:\n-                    - Unset: Return a list of `np.ndarray`.\n-                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n-                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n-            data_format (`str` or `ChannelDimension`, *optional*):\n-                The channel dimension format of the image. If not provided, it will be the same as the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format of the input image. If not provided, it will be inferred.\n-            update_bboxes (`bool`, *optional*, defaults to `True`):\n-                Whether to update the bounding boxes in the annotations to match the padded images. If the\n-                bounding boxes have not been converted to relative coordinates and `(centre_x, centre_y, width, height)`\n-                format, the bounding boxes will not be updated.\n-            pad_size (`dict[str, int]`, *optional*):\n-                The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-                provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n-                height and width in the batch.\n-        \"\"\"\n-        pad_size = pad_size if pad_size is not None else self.pad_size\n-        if pad_size is not None:\n-            padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n-        else:\n-            padded_size = get_max_height_width(images, input_data_format=input_data_format)\n-\n-        annotation_list = annotations if annotations is not None else [None] * len(images)\n-        padded_images = []\n-        padded_annotations = []\n-        for image, annotation in zip(images, annotation_list):\n-            padded_image, padded_annotation = self._pad_image(\n-                image,\n-                padded_size,\n-                annotation,\n-                constant_values=constant_values,\n-                data_format=data_format,\n-                input_data_format=input_data_format,\n-                update_bboxes=update_bboxes,\n-            )\n-            padded_images.append(padded_image)\n-            padded_annotations.append(padded_annotation)\n-\n-        data = {\"pixel_values\": padded_images}\n-\n-        if return_pixel_mask:\n-            masks = [\n-                make_pixel_mask(image=image, output_size=padded_size, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-            data[\"pixel_mask\"] = masks\n-\n-        encoded_inputs = BatchFeature(data=data, tensor_type=return_tensors)\n-\n-        if annotations is not None:\n-            encoded_inputs[\"labels\"] = [\n-                BatchFeature(annotation, tensor_type=return_tensors) for annotation in padded_annotations\n-            ]\n-\n-        return encoded_inputs\n-\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        annotations: Optional[Union[list[dict], list[list[dict]]]] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample=None,  # PILImageResampling\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[Union[int, float]] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_annotations: Optional[bool] = None,\n-        do_pad: Optional[bool] = None,\n-        format: Optional[Union[str, AnnotationFormat]] = None,\n-        return_tensors: Optional[Union[TensorType, str]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n-        **kwargs,\n-    ) -> BatchFeature:\n-        \"\"\"\n-        Preprocess an image or a batch of images so that it can be used by the model.\n-\n-        Args:\n-            images (`ImageInput`):\n-                Image or batch of images to preprocess. Expects a single or batch of images with pixel values ranging\n-                from 0 to 255. If passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            annotations (`list[Dict]` or `list[list[Dict]]`, *optional*):\n-                List of annotations associated with the image or batch of images. If annotation is for object\n-                detection, the annotations should be a dictionary with the following keys:\n-                - \"image_id\" (`int`): The image id.\n-                - \"annotations\" (`list[Dict]`): List of annotations for an image. Each annotation should be a\n-                  dictionary. An image can have no annotations, in which case the list should be empty.\n-                If annotation is for segmentation, the annotations should be a dictionary with the following keys:\n-                - \"image_id\" (`int`): The image id.\n-                - \"segments_info\" (`list[Dict]`): List of segments for an image. Each segment should be a dictionary.\n-                  An image can have no segments, in which case the list should be empty.\n-                - \"file_name\" (`str`): The file name of the image.\n-            return_segmentation_masks (`bool`, *optional*, defaults to self.return_segmentation_masks):\n-                Whether to return segmentation masks.\n-            masks_path (`str` or `pathlib.Path`, *optional*):\n-                Path to the directory containing the segmentation masks.\n-            do_resize (`bool`, *optional*, defaults to self.do_resize):\n-                Whether to resize the image.\n-            size (`dict[str, int]`, *optional*, defaults to self.size):\n-                Size of the image's `(height, width)` dimensions after resizing. Available options are:\n-                    - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n-                        Do NOT keep the aspect ratio.\n-                    - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n-                        the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n-                        less or equal to `longest_edge`.\n-                    - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n-                        aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n-                        `max_width`.\n-            resample (`PILImageResampling`, *optional*, defaults to self.resample):\n-                Resampling filter to use when resizing the image.\n-            do_rescale (`bool`, *optional*, defaults to self.do_rescale):\n-                Whether to rescale the image.\n-            rescale_factor (`float`, *optional*, defaults to self.rescale_factor):\n-                Rescale factor to use when rescaling the image.\n-            do_normalize (`bool`, *optional*, defaults to self.do_normalize):\n-                Whether to normalize the image.\n-            image_mean (`float` or `list[float]`, *optional*, defaults to self.image_mean):\n-                Mean to use when normalizing the image.\n-            image_std (`float` or `list[float]`, *optional*, defaults to self.image_std):\n-                Standard deviation to use when normalizing the image.\n-            do_convert_annotations (`bool`, *optional*, defaults to self.do_convert_annotations):\n-                Whether to convert the annotations to the format expected by the model. Converts the bounding\n-                boxes from the format `(top_left_x, top_left_y, width, height)` to `(center_x, center_y, width, height)`\n-                and in relative coordinates.\n-            do_pad (`bool`, *optional*, defaults to self.do_pad):\n-                Whether to pad the image. If `True`, padding will be applied to the bottom and right of\n-                the image with zeros. If `pad_size` is provided, the image will be padded to the specified\n-                dimensions. Otherwise, the image will be padded to the maximum height and width of the batch.\n-            format (`str` or `AnnotationFormat`, *optional*, defaults to self.format):\n-                Format of the annotations.\n-            return_tensors (`str` or `TensorType`, *optional*, defaults to self.return_tensors):\n-                Type of tensors to return. If `None`, will return the list of images.\n-            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n-                The channel dimension format for the output image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - Unset: Use the channel dimension format of the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-            pad_size (`dict[str, int]`, *optional*):\n-                The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-                provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n-                height and width in the batch.\n-        \"\"\"\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            logger.warning_once(\n-                \"The `pad_and_return_pixel_mask` argument is deprecated and will be removed in a future version, \"\n-                \"use `do_pad` instead.\",\n-            )\n-            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n-\n-        do_resize = self.do_resize if do_resize is None else do_resize\n-        size = self.size if size is None else size\n-        size = get_size_dict(size=size, default_to_square=False)\n-        resample = self.resample if resample is None else resample\n-        do_rescale = self.do_rescale if do_rescale is None else do_rescale\n-        rescale_factor = self.rescale_factor if rescale_factor is None else rescale_factor\n-        do_normalize = self.do_normalize if do_normalize is None else do_normalize\n-        image_mean = self.image_mean if image_mean is None else image_mean\n-        image_std = self.image_std if image_std is None else image_std\n-        do_convert_annotations = (\n-            self.do_convert_annotations if do_convert_annotations is None else do_convert_annotations\n-        )\n-        do_pad = self.do_pad if do_pad is None else do_pad\n-        pad_size = self.pad_size if pad_size is None else pad_size\n-        format = self.format if format is None else format\n-\n-        # Here, the pad() method pads to the maximum of (width, height). It does not need to be validated.\n-\n-        validate_preprocess_arguments(\n-            do_rescale=do_rescale,\n-            rescale_factor=rescale_factor,\n-            do_normalize=do_normalize,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            do_resize=do_resize,\n-            size=size,\n-            resample=resample,\n-        )\n-\n-        if not is_batched(images):\n-            images = [images]\n-            annotations = [annotations] if annotations is not None else None\n-\n-        if not valid_images(images):\n-            raise ValueError(\"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor.\")\n-        if annotations is not None and len(images) != len(annotations):\n-            raise ValueError(\n-                f\"The number of images ({len(images)}) and annotations ({len(annotations)}) do not match.\"\n-            )\n-\n-        format = AnnotationFormat(format)\n-        if annotations is not None:\n-            validate_annotations(format, SUPPORTED_ANNOTATION_FORMATS, annotations)\n-\n-        if (\n-            masks_path is not None\n-            and format == AnnotationFormat.COCO_PANOPTIC\n-            and not isinstance(masks_path, (pathlib.Path, str))\n-        ):\n-            raise ValueError(\n-                \"The path to the directory containing the mask PNG files should be provided as a\"\n-                f\" `pathlib.Path` or string object, but is {type(masks_path)} instead.\"\n-            )\n-\n-        # All transformations expect numpy arrays\n-        images = [to_numpy_array(image) for image in images]\n-\n-        if do_rescale and is_scaled_image(images[0]):\n-            logger.warning_once(\n-                \"It looks like you are trying to rescale already rescaled images. If the input\"\n-                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n-            )\n-\n-        if input_data_format is None:\n-            # We assume that all images have the same channel dimension format.\n-            input_data_format = infer_channel_dimension_format(images[0])\n-\n-        # prepare (COCO annotations as a list of Dict -> DETR target as a single Dict per image)\n-        if annotations is not None:\n-            prepared_images = []\n-            prepared_annotations = []\n-            for image, target in zip(images, annotations):\n-                target = self.prepare_annotation(\n-                    image,\n-                    target,\n-                    format,\n-                    return_segmentation_masks=return_segmentation_masks,\n-                    masks_path=masks_path,\n-                    input_data_format=input_data_format,\n-                )\n-                prepared_images.append(image)\n-                prepared_annotations.append(target)\n-            images = prepared_images\n-            annotations = prepared_annotations\n-            del prepared_images, prepared_annotations\n-\n-        # transformations\n-        if do_resize:\n-            if annotations is not None:\n-                resized_images, resized_annotations = [], []\n-                for image, target in zip(images, annotations):\n-                    orig_size = get_image_size(image, input_data_format)\n-                    resized_image = self.resize(\n-                        image, size=size, resample=resample, input_data_format=input_data_format\n-                    )\n-                    resized_annotation = self.resize_annotation(\n-                        target, orig_size, get_image_size(resized_image, input_data_format)\n-                    )\n-                    resized_images.append(resized_image)\n-                    resized_annotations.append(resized_annotation)\n-                images = resized_images\n-                annotations = resized_annotations\n-                del resized_images, resized_annotations\n-            else:\n-                images = [\n-                    self.resize(image, size=size, resample=resample, input_data_format=input_data_format)\n-                    for image in images\n-                ]\n-\n-        if do_rescale:\n-            images = [self.rescale(image, rescale_factor, input_data_format=input_data_format) for image in images]\n-\n-        if do_normalize:\n-            images = [\n-                self.normalize(image, image_mean, image_std, input_data_format=input_data_format) for image in images\n-            ]\n-\n-        if do_convert_annotations and annotations is not None:\n-            annotations = [\n-                self.normalize_annotation(annotation, get_image_size(image, input_data_format))\n-                for annotation, image in zip(annotations, images)\n-            ]\n-\n-        if do_pad:\n-            # Pads images and returns their mask: {'pixel_values': ..., 'pixel_mask': ...}\n-            encoded_inputs = self.pad(\n-                images,\n-                annotations=annotations,\n-                return_pixel_mask=True,\n-                data_format=data_format,\n-                input_data_format=input_data_format,\n-                return_tensors=return_tensors,\n-                update_bboxes=do_convert_annotations,\n-                pad_size=pad_size,\n-            )\n-        else:\n-            images = [\n-                to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n-                for image in images\n-            ]\n-            encoded_inputs = BatchFeature(data={\"pixel_values\": images}, tensor_type=return_tensors)\n-            if annotations is not None:\n-                encoded_inputs[\"labels\"] = [\n-                    BatchFeature(annotation, tensor_type=return_tensors) for annotation in annotations\n-                ]\n-\n-        return encoded_inputs\n-\n-    def post_process_object_detection(\n-        self,\n-        outputs,\n-        threshold: float = 0.5,\n-        target_sizes: Union[TensorType, list[tuple]] = None,\n-        nms_threshold: float = 0.7,\n-    ):\n-        \"\"\"\n-        Converts the output of [`DetaForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n-        bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n-\n-        Args:\n-            outputs ([`DetrObjectDetectionOutput`]):\n-                Raw outputs of the model.\n-            threshold (`float`, *optional*, defaults to 0.5):\n-                Score threshold to keep object detection predictions.\n-            target_sizes (`torch.Tensor` or `list[tuple[int, int]]`, *optional*):\n-                Tensor of shape `(batch_size, 2)` or list of tuples (`tuple[int, int]`) containing the target size\n-                (height, width) of each image in the batch. If left to None, predictions will not be resized.\n-            nms_threshold (`float`, *optional*, defaults to 0.7):\n-                NMS threshold.\n-\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n-        \"\"\"\n-        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n-        batch_size, num_queries, num_labels = out_logits.shape\n-\n-        if target_sizes is not None:\n-            if len(out_logits) != len(target_sizes):\n-                raise ValueError(\n-                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n-                )\n-\n-        prob = out_logits.sigmoid()\n-\n-        all_scores = prob.view(batch_size, num_queries * num_labels).to(out_logits.device)\n-        all_indexes = torch.arange(num_queries * num_labels)[None].repeat(batch_size, 1).to(out_logits.device)\n-        all_boxes = torch.div(all_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n-        all_labels = all_indexes % out_logits.shape[2]\n-\n-        boxes = center_to_corners_format(out_bbox)\n-        boxes = torch.gather(boxes, 1, all_boxes.unsqueeze(-1).repeat(1, 1, 4))\n-\n-        # and from relative [0, 1] to absolute [0, height] coordinates\n-        if target_sizes is not None:\n-            if isinstance(target_sizes, list):\n-                img_h = torch.Tensor([i[0] for i in target_sizes])\n-                img_w = torch.Tensor([i[1] for i in target_sizes])\n-            else:\n-                img_h, img_w = target_sizes.unbind(1)\n-\n-            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n-            boxes = boxes * scale_fct[:, None, :]\n-\n-        results = []\n-        for b in range(batch_size):\n-            box = boxes[b]\n-            score = all_scores[b]\n-            lbls = all_labels[b]\n-\n-            pre_topk = score.topk(min(10000, num_queries * num_labels)).indices\n-            box = box[pre_topk]\n-            score = score[pre_topk]\n-            lbls = lbls[pre_topk]\n-\n-            # apply NMS\n-            keep_inds = batched_nms(box, score, lbls, nms_threshold)[:100]\n-            score = score[keep_inds]\n-            lbls = lbls[keep_inds]\n-            box = box[keep_inds]\n-\n-            results.append(\n-                {\n-                    \"scores\": score[score > threshold],\n-                    \"labels\": lbls[score > threshold],\n-                    \"boxes\": box[score > threshold],\n-                }\n-            )\n-\n-        return results\n-\n-\n-__all__ = [\"DetaImageProcessor\"]"
        },
        {
            "sha": "aa416298f5489347251ca22ab273c567782dfe73",
            "filename": "src/transformers/models/deprecated/deta/modeling_deta.py",
            "status": "removed",
            "additions": 0,
            "deletions": 2765,
            "changes": 2765,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "e43cb0e61df1c0242153bebd417a3bebcafc78ec",
            "filename": "src/transformers/models/deprecated/efficientformer/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,28 +0,0 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from typing import TYPE_CHECKING\n-\n-from ....utils import _LazyModule\n-from ....utils.import_utils import define_import_structure\n-\n-\n-if TYPE_CHECKING:\n-    from .configuration_efficientformer import *\n-    from .image_processing_efficientformer import *\n-    from .modeling_efficientformer import *\n-else:\n-    import sys\n-\n-    _file = globals()[\"__file__\"]\n-    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "feb173c3d8892e650e88011c5c95598227458203",
            "filename": "src/transformers/models/deprecated/efficientformer/configuration_efficientformer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 170,
            "changes": 170,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconfiguration_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconfiguration_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconfiguration_efficientformer.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,170 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"EfficientFormer model configuration\"\"\"\n-\n-from ....configuration_utils import PreTrainedConfig\n-from ....utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class EfficientFormerConfig(PreTrainedConfig):\n-    r\"\"\"\n-    This is the configuration class to store the configuration of an [`EfficientFormerModel`]. It is used to\n-    instantiate an EfficientFormer model according to the specified arguments, defining the model architecture.\n-    Instantiating a configuration with the defaults will yield a similar configuration to that of the EfficientFormer\n-    [snap-research/efficientformer-l1](https://huggingface.co/snap-research/efficientformer-l1) architecture.\n-\n-    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PreTrainedConfig`] for more information.\n-\n-    Args:\n-        depths (`List(int)`, *optional*, defaults to `[3, 2, 6, 4]`)\n-            Depth of each stage.\n-        hidden_sizes (`List(int)`, *optional*, defaults to `[48, 96, 224, 448]`)\n-            Dimensionality of each stage.\n-        downsamples (`List(bool)`, *optional*, defaults to `[True, True, True, True]`)\n-            Whether or not to downsample inputs between two stages.\n-        dim (`int`, *optional*, defaults to 448):\n-            Number of channels in Meta3D layers\n-        key_dim (`int`, *optional*, defaults to 32):\n-            The size of the key in meta3D block.\n-        attention_ratio (`int`, *optional*, defaults to 4):\n-            Ratio of the dimension of the query and value to the dimension of the key in MSHA block\n-        resolution (`int`, *optional*, defaults to 7)\n-            Size of each patch\n-        num_hidden_layers (`int`, *optional*, defaults to 5):\n-            Number of hidden layers in the Transformer encoder.\n-        num_attention_heads (`int`, *optional*, defaults to 8):\n-            Number of attention heads for each attention layer in the 3D MetaBlock.\n-        mlp_expansion_ratio (`int`, *optional*, defaults to 4):\n-            Ratio of size of the hidden dimensionality of an MLP to the dimensionality of its input.\n-        hidden_dropout_prob (`float`, *optional*, defaults to 0.1):\n-            The dropout probability for all fully connected layers in the embeddings and encoder.\n-        patch_size (`int`, *optional*, defaults to 16):\n-            The size (resolution) of each patch.\n-        num_channels (`int`, *optional*, defaults to 3):\n-            The number of input channels.\n-        pool_size (`int`, *optional*, defaults to 3):\n-            Kernel size of pooling layers.\n-        downsample_patch_size (`int`, *optional*, defaults to 3):\n-            The size of patches in downsampling layers.\n-        downsample_stride (`int`, *optional*, defaults to 2):\n-            The stride of convolution kernels in downsampling layers.\n-        downsample_pad (`int`, *optional*, defaults to 1):\n-            Padding in downsampling layers.\n-        drop_path_rate (`int`, *optional*, defaults to 0):\n-            Rate at which to increase dropout probability in DropPath.\n-        num_meta3d_blocks (`int`, *optional*, defaults to 1):\n-            The number of 3D MetaBlocks in the last stage.\n-        distillation (`bool`, *optional*, defaults to `True`):\n-            Whether to add a distillation head.\n-        use_layer_scale (`bool`, *optional*, defaults to `True`):\n-            Whether to scale outputs from token mixers.\n-        layer_scale_init_value (`float`, *optional*, defaults to 1e-5):\n-            Factor by which outputs from token mixers are scaled.\n-        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n-            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n-            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n-        initializer_range (`float`, *optional*, defaults to 0.02):\n-            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n-        layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n-            The epsilon used by the layer normalization layers.\n-        image_size (`int`, *optional*, defaults to `224`):\n-            The size (resolution) of each image.\n-\n-    Example:\n-\n-    ```python\n-    >>> from transformers import EfficientFormerConfig, EfficientFormerModel\n-\n-    >>> # Initializing a EfficientFormer efficientformer-l1 style configuration\n-    >>> configuration = EfficientFormerConfig()\n-\n-    >>> # Initializing a EfficientFormerModel (with random weights) from the efficientformer-l3 style configuration\n-    >>> model = EfficientFormerModel(configuration)\n-\n-    >>> # Accessing the model configuration\n-    >>> configuration = model.config\n-    ```\"\"\"\n-\n-    model_type = \"efficientformer\"\n-\n-    def __init__(\n-        self,\n-        depths: list[int] = [3, 2, 6, 4],\n-        hidden_sizes: list[int] = [48, 96, 224, 448],\n-        downsamples: list[bool] = [True, True, True, True],\n-        dim: int = 448,\n-        key_dim: int = 32,\n-        attention_ratio: int = 4,\n-        resolution: int = 7,\n-        num_hidden_layers: int = 5,\n-        num_attention_heads: int = 8,\n-        mlp_expansion_ratio: int = 4,\n-        hidden_dropout_prob: float = 0.0,\n-        patch_size: int = 16,\n-        num_channels: int = 3,\n-        pool_size: int = 3,\n-        downsample_patch_size: int = 3,\n-        downsample_stride: int = 2,\n-        downsample_pad: int = 1,\n-        drop_path_rate: float = 0.0,\n-        num_meta3d_blocks: int = 1,\n-        distillation: bool = True,\n-        use_layer_scale: bool = True,\n-        layer_scale_init_value: float = 1e-5,\n-        hidden_act: str = \"gelu\",\n-        initializer_range: float = 0.02,\n-        layer_norm_eps: float = 1e-12,\n-        image_size: int = 224,\n-        batch_norm_eps: float = 1e-05,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(**kwargs)\n-\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.hidden_sizes = hidden_sizes\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.initializer_range = initializer_range\n-        self.layer_norm_eps = layer_norm_eps\n-        self.patch_size = patch_size\n-        self.num_channels = num_channels\n-        self.depths = depths\n-        self.mlp_expansion_ratio = mlp_expansion_ratio\n-        self.downsamples = downsamples\n-        self.dim = dim\n-        self.key_dim = key_dim\n-        self.attention_ratio = attention_ratio\n-        self.resolution = resolution\n-        self.pool_size = pool_size\n-        self.downsample_patch_size = downsample_patch_size\n-        self.downsample_stride = downsample_stride\n-        self.downsample_pad = downsample_pad\n-        self.drop_path_rate = drop_path_rate\n-        self.num_meta3d_blocks = num_meta3d_blocks\n-        self.distillation = distillation\n-        self.use_layer_scale = use_layer_scale\n-        self.layer_scale_init_value = layer_scale_init_value\n-        self.image_size = image_size\n-        self.batch_norm_eps = batch_norm_eps\n-\n-\n-__all__ = [\n-    \"EfficientFormerConfig\",\n-]"
        },
        {
            "sha": "7b1a4aa5f207a38e9d4992a2182ff333ab399e03",
            "filename": "src/transformers/models/deprecated/efficientformer/convert_efficientformer_original_pytorch_checkpoint_to_pytorch.py",
            "status": "removed",
            "additions": 0,
            "deletions": 252,
            "changes": 252,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconvert_efficientformer_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconvert_efficientformer_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconvert_efficientformer_original_pytorch_checkpoint_to_pytorch.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,252 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\"\"\"Convert EfficientFormer checkpoints from the original repository.\n-\n-URL: https://github.com/snap-research/EfficientFormer\n-\"\"\"\n-\n-import argparse\n-import re\n-from pathlib import Path\n-\n-import requests\n-import torch\n-from PIL import Image\n-from torchvision.transforms import CenterCrop, Compose, Normalize, Resize, ToTensor\n-\n-from transformers import (\n-    EfficientFormerConfig,\n-    EfficientFormerForImageClassificationWithTeacher,\n-    EfficientFormerImageProcessor,\n-)\n-from transformers.image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling\n-\n-\n-def rename_key(old_name, num_meta4D_last_stage):\n-    new_name = old_name\n-\n-    if \"patch_embed\" in old_name:\n-        _, layer, param = old_name.split(\".\")\n-\n-        if layer == \"0\":\n-            new_name = old_name.replace(\"0\", \"convolution1\")\n-        elif layer == \"1\":\n-            new_name = old_name.replace(\"1\", \"batchnorm_before\")\n-        elif layer == \"3\":\n-            new_name = old_name.replace(\"3\", \"convolution2\")\n-        else:\n-            new_name = old_name.replace(\"4\", \"batchnorm_after\")\n-\n-    if \"network\" in old_name and re.search(r\"\\d\\.\\d\", old_name):\n-        two_digit_num = r\"\\b\\d{2}\\b\"\n-        if bool(re.search(two_digit_num, old_name)):\n-            match = re.search(r\"\\d\\.\\d\\d.\", old_name).group()\n-        else:\n-            match = re.search(r\"\\d\\.\\d.\", old_name).group()\n-        if int(match[0]) < 6:\n-            trimmed_name = old_name.replace(match, \"\")\n-            trimmed_name = trimmed_name.replace(\"network\", match[0] + \".meta4D_layers.blocks.\" + match[2:-1])\n-            new_name = \"intermediate_stages.\" + trimmed_name\n-        else:\n-            trimmed_name = old_name.replace(match, \"\")\n-            if int(match[2]) < num_meta4D_last_stage:\n-                trimmed_name = trimmed_name.replace(\"network\", \"meta4D_layers.blocks.\" + match[2])\n-            else:\n-                layer_index = str(int(match[2]) - num_meta4D_last_stage)\n-                trimmed_name = trimmed_name.replace(\"network\", \"meta3D_layers.blocks.\" + layer_index)\n-                if \"norm1\" in old_name:\n-                    trimmed_name = trimmed_name.replace(\"norm1\", \"layernorm1\")\n-                elif \"norm2\" in old_name:\n-                    trimmed_name = trimmed_name.replace(\"norm2\", \"layernorm2\")\n-                elif \"fc1\" in old_name:\n-                    trimmed_name = trimmed_name.replace(\"fc1\", \"linear_in\")\n-                elif \"fc2\" in old_name:\n-                    trimmed_name = trimmed_name.replace(\"fc2\", \"linear_out\")\n-\n-            new_name = \"last_stage.\" + trimmed_name\n-\n-    elif \"network\" in old_name and re.search(r\".\\d.\", old_name):\n-        new_name = old_name.replace(\"network\", \"intermediate_stages\")\n-\n-    if \"fc\" in new_name:\n-        new_name = new_name.replace(\"fc\", \"convolution\")\n-    elif (\"norm1\" in new_name) and (\"layernorm1\" not in new_name):\n-        new_name = new_name.replace(\"norm1\", \"batchnorm_before\")\n-    elif (\"norm2\" in new_name) and (\"layernorm2\" not in new_name):\n-        new_name = new_name.replace(\"norm2\", \"batchnorm_after\")\n-    if \"proj\" in new_name:\n-        new_name = new_name.replace(\"proj\", \"projection\")\n-    if \"dist_head\" in new_name:\n-        new_name = new_name.replace(\"dist_head\", \"distillation_classifier\")\n-    elif \"head\" in new_name:\n-        new_name = new_name.replace(\"head\", \"classifier\")\n-    elif \"patch_embed\" in new_name:\n-        new_name = \"efficientformer.\" + new_name\n-    elif new_name == \"norm.weight\" or new_name == \"norm.bias\":\n-        new_name = new_name.replace(\"norm\", \"layernorm\")\n-        new_name = \"efficientformer.\" + new_name\n-    else:\n-        new_name = \"efficientformer.encoder.\" + new_name\n-\n-    return new_name\n-\n-\n-def convert_torch_checkpoint(checkpoint, num_meta4D_last_stage):\n-    for key in checkpoint.copy():\n-        val = checkpoint.pop(key)\n-        checkpoint[rename_key(key, num_meta4D_last_stage)] = val\n-\n-    return checkpoint\n-\n-\n-# We will verify our results on a COCO image\n-def prepare_img():\n-    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-    image = Image.open(requests.get(url, stream=True).raw)\n-\n-    return image\n-\n-\n-def convert_efficientformer_checkpoint(\n-    checkpoint_path: Path, efficientformer_config_file: Path, pytorch_dump_path: Path, push_to_hub: bool\n-):\n-    orig_state_dict = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)[\"model\"]\n-    config = EfficientFormerConfig.from_json_file(efficientformer_config_file)\n-    model = EfficientFormerForImageClassificationWithTeacher(config)\n-    model_name = \"_\".join(checkpoint_path.split(\"/\")[-1].split(\".\")[0].split(\"_\")[:-1])\n-\n-    num_meta4D_last_stage = config.depths[-1] - config.num_meta3d_blocks + 1\n-    new_state_dict = convert_torch_checkpoint(orig_state_dict, num_meta4D_last_stage)\n-\n-    model.load_state_dict(new_state_dict)\n-    model.eval()\n-\n-    pillow_resamplings = {\n-        \"bilinear\": PILImageResampling.BILINEAR,\n-        \"bicubic\": PILImageResampling.BICUBIC,\n-        \"nearest\": PILImageResampling.NEAREST,\n-    }\n-\n-    # prepare image\n-    image = prepare_img()\n-    image_size = 256\n-    crop_size = 224\n-    processor = EfficientFormerImageProcessor(\n-        size={\"shortest_edge\": image_size},\n-        crop_size={\"height\": crop_size, \"width\": crop_size},\n-        resample=pillow_resamplings[\"bicubic\"],\n-    )\n-    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n-\n-    # original processing pipeline\n-    image_transforms = Compose(\n-        [\n-            Resize(image_size, interpolation=pillow_resamplings[\"bicubic\"]),\n-            CenterCrop(crop_size),\n-            ToTensor(),\n-            Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n-        ]\n-    )\n-    original_pixel_values = image_transforms(image).unsqueeze(0)\n-\n-    assert torch.allclose(original_pixel_values, pixel_values)\n-\n-    outputs = model(pixel_values)\n-    logits = outputs.logits\n-\n-    expected_shape = (1, 1000)\n-\n-    if \"l1\" in model_name:\n-        expected_logits = torch.Tensor(\n-            [-0.1312, 0.4353, -1.0499, -0.5124, 0.4183, -0.6793, -1.3777, -0.0893, -0.7358, -2.4328]\n-        )\n-        assert torch.allclose(logits[0, :10], expected_logits, atol=1e-3)\n-        assert logits.shape == expected_shape\n-    elif \"l3\" in model_name:\n-        expected_logits = torch.Tensor(\n-            [-1.3150, -1.5456, -1.2556, -0.8496, -0.7127, -0.7897, -0.9728, -0.3052, 0.3751, -0.3127]\n-        )\n-        assert torch.allclose(logits[0, :10], expected_logits, atol=1e-3)\n-        assert logits.shape == expected_shape\n-    elif \"l7\" in model_name:\n-        expected_logits = torch.Tensor(\n-            [-1.0283, -1.4131, -0.5644, -1.3115, -0.5785, -1.2049, -0.7528, 0.1992, -0.3822, -0.0878]\n-        )\n-        assert logits.shape == expected_shape\n-    else:\n-        raise ValueError(\n-            f\"Unknown model checkpoint: {checkpoint_path}. Supported version of efficientformer are l1, l3 and l7\"\n-        )\n-\n-    # Save Checkpoints\n-    Path(pytorch_dump_path).mkdir(exist_ok=True)\n-    model.save_pretrained(pytorch_dump_path)\n-    print(f\"Checkpoint successfully converted. Model saved at {pytorch_dump_path}\")\n-    processor.save_pretrained(pytorch_dump_path)\n-    print(f\"Processor successfully saved at {pytorch_dump_path}\")\n-\n-    if push_to_hub:\n-        print(\"Pushing model to the hub...\")\n-\n-        model.push_to_hub(\n-            repo_id=f\"Bearnardd/{pytorch_dump_path}\",\n-            commit_message=\"Add model\",\n-            use_temp_dir=True,\n-        )\n-        processor.push_to_hub(\n-            repo_id=f\"Bearnardd/{pytorch_dump_path}\",\n-            commit_message=\"Add image processor\",\n-            use_temp_dir=True,\n-        )\n-\n-\n-if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser()\n-    # Required parameters\n-    parser.add_argument(\n-        \"--pytorch_model_path\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Path to EfficientFormer pytorch checkpoint.\",\n-    )\n-    parser.add_argument(\n-        \"--config_file\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The json file for EfficientFormer model config.\",\n-    )\n-    parser.add_argument(\n-        \"--pytorch_dump_path\", default=None, type=str, required=True, help=\"Path to the output PyTorch model.\"\n-    )\n-\n-    parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Push model and image processor to the hub\")\n-    parser.add_argument(\n-        \"--no-push_to_hub\",\n-        dest=\"push_to_hub\",\n-        action=\"store_false\",\n-        help=\"Do not push model and image processor to the hub\",\n-    )\n-    parser.set_defaults(push_to_hub=True)\n-\n-    args = parser.parse_args()\n-    convert_efficientformer_checkpoint(\n-        checkpoint_path=args.pytorch_model_path,\n-        efficientformer_config_file=args.config_file,\n-        pytorch_dump_path=args.pytorch_dump_path,\n-        push_to_hub=args.push_to_hub,\n-    )"
        },
        {
            "sha": "a8dcedea620afd9e43dbbd5e39fb3fa2b0a58ea2",
            "filename": "src/transformers/models/deprecated/efficientformer/image_processing_efficientformer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 319,
            "changes": 319,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,319 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Image processor class for EfficientFormer.\"\"\"\n-\n-from typing import Optional, Union\n-\n-import numpy as np\n-\n-from ....image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n-from ....image_transforms import (\n-    get_resize_output_image_size,\n-    resize,\n-    to_channel_dimension_format,\n-)\n-from ....image_utils import (\n-    IMAGENET_DEFAULT_MEAN,\n-    IMAGENET_DEFAULT_STD,\n-    ChannelDimension,\n-    ImageInput,\n-    PILImageResampling,\n-    infer_channel_dimension_format,\n-    is_batched,\n-    is_scaled_image,\n-    to_numpy_array,\n-    valid_images,\n-    validate_kwargs,\n-    validate_preprocess_arguments,\n-)\n-from ....utils import TensorType, logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class EfficientFormerImageProcessor(BaseImageProcessor):\n-    r\"\"\"\n-    Constructs a EfficientFormer image processor.\n-\n-    Args:\n-        do_resize (`bool`, *optional*, defaults to `True`):\n-            Whether to resize the image's (height, width) dimensions to the specified `(size[\"height\"],\n-            size[\"width\"])`. Can be overridden by the `do_resize` parameter in the `preprocess` method.\n-        size (`dict`, *optional*, defaults to `{\"height\": 224, \"width\": 224}`):\n-            Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`\n-            method.\n-        resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`):\n-            Resampling filter to use if resizing the image. Can be overridden by the `resample` parameter in the\n-            `preprocess` method.\n-        do_center_crop (`bool`, *optional*, defaults to `True`):\n-            Whether to center crop the image to the specified `crop_size`. Can be overridden by `do_center_crop` in the\n-            `preprocess` method.\n-        crop_size (`dict[str, int]` *optional*, defaults to 224):\n-            Size of the output image after applying `center_crop`. Can be overridden by `crop_size` in the `preprocess`\n-            method.\n-        do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the `do_rescale`\n-            parameter in the `preprocess` method.\n-        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Can be overridden by the `rescale_factor` parameter in the\n-            `preprocess` method.\n-        do_normalize:\n-            Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n-            method.\n-        image_mean (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n-            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n-            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n-            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n-            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n-    \"\"\"\n-\n-    model_input_names = [\"pixel_values\"]\n-\n-    def __init__(\n-        self,\n-        do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        do_center_crop: bool = True,\n-        do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(**kwargs)\n-        size = size if size is not None else {\"height\": 224, \"width\": 224}\n-        size = get_size_dict(size)\n-        crop_size = crop_size if crop_size is not None else {\"height\": 224, \"width\": 224}\n-        crop_size = get_size_dict(crop_size, default_to_square=True, param_name=\"crop_size\")\n-\n-        self.do_resize = do_resize\n-        self.do_rescale = do_rescale\n-        self.do_normalize = do_normalize\n-        self.do_center_crop = do_center_crop\n-        self.crop_size = crop_size\n-        self.size = size\n-        self.resample = resample\n-        self.rescale_factor = rescale_factor\n-        self.image_mean = image_mean if image_mean is not None else IMAGENET_DEFAULT_MEAN\n-        self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n-        self._valid_processor_keys = [\n-            \"images\",\n-            \"do_resize\",\n-            \"size\",\n-            \"resample\",\n-            \"do_center_crop\",\n-            \"crop_size\",\n-            \"do_rescale\",\n-            \"rescale_factor\",\n-            \"do_normalize\",\n-            \"image_mean\",\n-            \"image_std\",\n-            \"return_tensors\",\n-            \"data_format\",\n-            \"input_data_format\",\n-        ]\n-\n-    def resize(\n-        self,\n-        image: np.ndarray,\n-        size: dict[str, int],\n-        resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        **kwargs,\n-    ) -> np.ndarray:\n-        \"\"\"\n-        Resize an image to `(size[\"height\"], size[\"width\"])`.\n-\n-        Args:\n-            image (`np.ndarray`):\n-                Image to resize.\n-            size (`dict[str, int]`):\n-                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n-            resample:\n-                `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BILINEAR`.\n-            data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the output image. If unset, the channel dimension format of the input\n-                image is used. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format of the input image. If not provided, it will be inferred.\n-\n-        Returns:\n-            `np.ndarray`: The resized image.\n-        \"\"\"\n-        size = get_size_dict(size)\n-\n-        if \"shortest_edge\" in size:\n-            size = get_resize_output_image_size(\n-                image, size=size[\"shortest_edge\"], default_to_square=False, input_data_format=input_data_format\n-            )\n-            # size = get_resize_output_image_size(image, size[\"shortest_edge\"], size[\"longest_edge\"])\n-        elif \"height\" in size and \"width\" in size:\n-            size = (size[\"height\"], size[\"width\"])\n-        else:\n-            raise ValueError(f\"Size must contain 'height' and 'width' keys or 'shortest_edge' key. Got {size.keys()}\")\n-        return resize(\n-            image, size=size, resample=resample, data_format=data_format, input_data_format=input_data_format, **kwargs\n-        )\n-\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        **kwargs,\n-    ) -> BatchFeature:\n-        \"\"\"\n-        Preprocess an image or batch of images.\n-\n-        Args:\n-            images (`ImageInput`):\n-                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n-                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the image.\n-            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n-                Dictionary in the format `{\"height\": h, \"width\": w}` specifying the size of the output image after\n-                resizing.\n-            resample (`PILImageResampling` filter, *optional*, defaults to `self.resample`):\n-                `PILImageResampling` filter to use if resizing the image e.g. `PILImageResampling.BILINEAR`. Only has\n-                an effect if `do_resize` is set to `True`.\n-            do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n-                Whether to center crop the image.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the image values between [0 - 1].\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n-            crop_size (`dict[str, int]`, *optional*, defaults to `self.crop_size`):\n-                Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the image.\n-            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n-                Image mean to use if `do_normalize` is set to `True`.\n-            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n-                Image standard deviation to use if `do_normalize` is set to `True`.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return. Can be one of:\n-                - Unset: Return a list of `np.ndarray`.\n-                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n-                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n-            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n-                The channel dimension format for the output image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - Unset: Use the channel dimension format of the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-        \"\"\"\n-        do_resize = do_resize if do_resize is not None else self.do_resize\n-        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n-        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n-        do_center_crop = do_center_crop if do_center_crop is not None else self.do_center_crop\n-        crop_size = crop_size if crop_size is not None else self.crop_size\n-        crop_size = get_size_dict(crop_size, param_name=\"crop_size\", default_to_square=True)\n-        resample = resample if resample is not None else self.resample\n-        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n-        image_mean = image_mean if image_mean is not None else self.image_mean\n-        image_std = image_std if image_std is not None else self.image_std\n-\n-        size = size if size is not None else self.size\n-        size_dict = get_size_dict(size)\n-\n-        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_processor_keys)\n-\n-        if not is_batched(images):\n-            images = [images]\n-\n-        if not valid_images(images):\n-            raise ValueError(\"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor\")\n-        validate_preprocess_arguments(\n-            do_rescale=do_rescale,\n-            rescale_factor=rescale_factor,\n-            do_normalize=do_normalize,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            do_center_crop=do_center_crop,\n-            crop_size=crop_size,\n-            do_resize=do_resize,\n-            size=size,\n-            resample=resample,\n-        )\n-        # All transformations expect numpy arrays.\n-        images = [to_numpy_array(image) for image in images]\n-\n-        if do_rescale and is_scaled_image(images[0]):\n-            logger.warning_once(\n-                \"It looks like you are trying to rescale already rescaled images. If the input\"\n-                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n-            )\n-\n-        if input_data_format is None:\n-            # We assume that all images have the same channel dimension format.\n-            input_data_format = infer_channel_dimension_format(images[0])\n-\n-        if do_resize:\n-            images = [\n-                self.resize(image=image, size=size_dict, resample=resample, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        if do_center_crop:\n-            images = [\n-                self.center_crop(image=image, size=crop_size, input_data_format=input_data_format) for image in images\n-            ]\n-\n-        if do_rescale:\n-            images = [\n-                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        if do_normalize:\n-            images = [\n-                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n-        ]\n-\n-        data = {\"pixel_values\": images}\n-        return BatchFeature(data=data, tensor_type=return_tensors)\n-\n-\n-__all__ = [\"EfficientFormerImageProcessor\"]"
        },
        {
            "sha": "8f1f657c5f4258c6b87ce437792f411add09bfaa",
            "filename": "src/transformers/models/deprecated/efficientformer/modeling_efficientformer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 771,
            "changes": 771,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,771 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 Snapchat Research and The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch EfficientFormer model.\"\"\"\n-\n-import itertools\n-from dataclasses import dataclass\n-from typing import Optional, Union\n-\n-import torch\n-from torch import nn\n-\n-from ....activations import ACT2FN\n-from ....modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n-from ....modeling_utils import PreTrainedModel\n-from ....utils import (\n-    ModelOutput,\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-)\n-from .configuration_efficientformer import EfficientFormerConfig\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-# General docstring\n-_CONFIG_FOR_DOC = \"EfficientFormerConfig\"\n-\n-# Base docstring\n-_CHECKPOINT_FOR_DOC = \"snap-research/efficientformer-l1-300\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 49, 448]\n-\n-# Image classification docstring\n-_IMAGE_CLASS_CHECKPOINT = \"snap-research/efficientformer-l1-300\"\n-_IMAGE_CLASS_EXPECTED_OUTPUT = \"Egyptian cat\"\n-\n-\n-class EfficientFormerPatchEmbeddings(nn.Module):\n-    \"\"\"\n-    This class performs downsampling between two stages. For the input tensor with the shape [batch_size, num_channels,\n-    height, width] it produces output tensor with the shape [batch_size, num_channels, height/stride, width/stride]\n-    \"\"\"\n-\n-    def __init__(self, config: EfficientFormerConfig, num_channels: int, embed_dim: int, apply_norm: bool = True):\n-        super().__init__()\n-        self.num_channels = num_channels\n-\n-        self.projection = nn.Conv2d(\n-            num_channels,\n-            embed_dim,\n-            kernel_size=config.downsample_patch_size,\n-            stride=config.downsample_stride,\n-            padding=config.downsample_pad,\n-        )\n-        self.norm = nn.BatchNorm2d(embed_dim, eps=config.batch_norm_eps) if apply_norm else nn.Identity()\n-\n-    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n-        batch_size, num_channels, height, width = pixel_values.shape\n-        if num_channels != self.num_channels:\n-            raise ValueError(\n-                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n-            )\n-\n-        embeddings = self.projection(pixel_values)\n-        embeddings = self.norm(embeddings)\n-\n-        return embeddings\n-\n-\n-class EfficientFormerSelfAttention(nn.Module):\n-    def __init__(self, dim: int, key_dim: int, num_heads: int, attention_ratio: int, resolution: int):\n-        super().__init__()\n-\n-        self.num_heads = num_heads\n-        self.key_dim = key_dim\n-        self.attention_ratio = attention_ratio\n-        self.scale = key_dim**-0.5\n-        self.total_key_dim = key_dim * num_heads\n-        self.expanded_key_dim = int(attention_ratio * key_dim)\n-        self.total_expanded_key_dim = int(self.expanded_key_dim * num_heads)\n-        hidden_size = self.total_expanded_key_dim + self.total_key_dim * 2\n-        self.qkv = nn.Linear(dim, hidden_size)\n-        self.projection = nn.Linear(self.total_expanded_key_dim, dim)\n-        points = list(itertools.product(range(resolution), range(resolution)))\n-        num_points = len(points)\n-        attention_offsets = {}\n-        idxs = []\n-        for point_1 in points:\n-            for point_2 in points:\n-                offset = (abs(point_1[0] - point_2[0]), abs(point_1[1] - point_2[1]))\n-                if offset not in attention_offsets:\n-                    attention_offsets[offset] = len(attention_offsets)\n-                idxs.append(attention_offsets[offset])\n-        self.attention_biases = torch.nn.Parameter(torch.zeros(num_heads, len(attention_offsets)))\n-        self.register_buffer(\"attention_bias_idxs\", torch.LongTensor(idxs).view(num_points, num_points))\n-\n-    @torch.no_grad()\n-    def train(self, mode=True):\n-        super().train(mode)\n-        if mode and hasattr(self, \"ab\"):\n-            del self.ab\n-        else:\n-            self.ab = self.attention_biases[:, self.attention_bias_idxs]\n-\n-    def forward(self, hidden_states: torch.Tensor, output_attentions: bool = False) -> tuple[torch.Tensor]:\n-        batch_size, sequence_length, num_channels = hidden_states.shape\n-        qkv = self.qkv(hidden_states)\n-        query_layer, key_layer, value_layer = qkv.reshape(batch_size, sequence_length, self.num_heads, -1).split(\n-            [self.key_dim, self.key_dim, self.expanded_key_dim], dim=3\n-        )\n-        query_layer = query_layer.permute(0, 2, 1, 3)\n-        key_layer = key_layer.permute(0, 2, 1, 3)\n-        value_layer = value_layer.permute(0, 2, 1, 3)\n-\n-        # set `model.to(torch_device)` won't change `self.ab.device`, if there is no follow-up `train` or `eval` call.\n-        # Let's do it manually here, so users won't have to do this everytime.\n-        if not self.training:\n-            self.ab = self.ab.to(self.attention_biases.device)\n-        attention_probs = (torch.matmul(query_layer, key_layer.transpose(-2, -1))) * self.scale + (\n-            self.attention_biases[:, self.attention_bias_idxs] if self.training else self.ab\n-        )\n-\n-        attention_probs = attention_probs.softmax(dim=-1)\n-\n-        context_layer = torch.matmul(attention_probs, value_layer).transpose(1, 2)\n-        context_layer = context_layer.reshape(batch_size, sequence_length, self.total_expanded_key_dim)\n-        context_layer = self.projection(context_layer)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        return outputs\n-\n-\n-class EfficientFormerConvStem(nn.Module):\n-    def __init__(self, config: EfficientFormerConfig, out_channels: int):\n-        super().__init__()\n-\n-        self.convolution1 = nn.Conv2d(config.num_channels, out_channels // 2, kernel_size=3, stride=2, padding=1)\n-        self.batchnorm_before = nn.BatchNorm2d(out_channels // 2, eps=config.batch_norm_eps)\n-\n-        self.convolution2 = nn.Conv2d(out_channels // 2, out_channels, kernel_size=3, stride=2, padding=1)\n-        self.batchnorm_after = nn.BatchNorm2d(out_channels, eps=config.batch_norm_eps)\n-\n-        self.activation = nn.ReLU()\n-\n-    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n-        features = self.batchnorm_before(self.convolution1(pixel_values))\n-        features = self.activation(features)\n-        features = self.batchnorm_after(self.convolution2(features))\n-        features = self.activation(features)\n-\n-        return features\n-\n-\n-class EfficientFormerPooling(nn.Module):\n-    def __init__(self, pool_size: int):\n-        super().__init__()\n-        self.pool = nn.AvgPool2d(pool_size, stride=1, padding=pool_size // 2, count_include_pad=False)\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        output = self.pool(hidden_states) - hidden_states\n-        return output\n-\n-\n-class EfficientFormerDenseMlp(nn.Module):\n-    def __init__(\n-        self,\n-        config: EfficientFormerConfig,\n-        in_features: int,\n-        hidden_features: Optional[int] = None,\n-        out_features: Optional[int] = None,\n-    ):\n-        super().__init__()\n-        out_features = out_features or in_features\n-        hidden_features = hidden_features or in_features\n-\n-        self.linear_in = nn.Linear(in_features, hidden_features)\n-        self.activation = ACT2FN[config.hidden_act]\n-        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-        self.linear_out = nn.Linear(hidden_features, out_features)\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        hidden_states = self.linear_in(hidden_states)\n-        hidden_states = self.activation(hidden_states)\n-        hidden_states = self.dropout(hidden_states)\n-        hidden_states = self.linear_out(hidden_states)\n-        hidden_states = self.dropout(hidden_states)\n-\n-        return hidden_states\n-\n-\n-class EfficientFormerConvMlp(nn.Module):\n-    def __init__(\n-        self,\n-        config: EfficientFormerConfig,\n-        in_features: int,\n-        hidden_features: Optional[int] = None,\n-        out_features: Optional[int] = None,\n-        drop: float = 0.0,\n-    ):\n-        super().__init__()\n-        out_features = out_features or in_features\n-        hidden_features = hidden_features or in_features\n-\n-        self.convolution1 = nn.Conv2d(in_features, hidden_features, 1)\n-        self.activation = ACT2FN[config.hidden_act]\n-        self.convolution2 = nn.Conv2d(hidden_features, out_features, 1)\n-        self.dropout = nn.Dropout(drop)\n-\n-        self.batchnorm_before = nn.BatchNorm2d(hidden_features, eps=config.batch_norm_eps)\n-        self.batchnorm_after = nn.BatchNorm2d(out_features, eps=config.batch_norm_eps)\n-\n-    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n-        hidden_state = self.convolution1(hidden_state)\n-        hidden_state = self.batchnorm_before(hidden_state)\n-\n-        hidden_state = self.activation(hidden_state)\n-        hidden_state = self.dropout(hidden_state)\n-        hidden_state = self.convolution2(hidden_state)\n-\n-        hidden_state = self.batchnorm_after(hidden_state)\n-        hidden_state = self.dropout(hidden_state)\n-\n-        return hidden_state\n-\n-\n-def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:\n-    \"\"\"\n-    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n-\n-    \"\"\"\n-    if drop_prob == 0.0 or not training:\n-        return input\n-    keep_prob = 1 - drop_prob\n-    shape = (input.shape[0],) + (1,) * (input.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n-    random_tensor = keep_prob + torch.rand(shape, dtype=input.dtype, device=input.device)\n-    random_tensor.floor_()  # binarize\n-    output = input.div(keep_prob) * random_tensor\n-    return output\n-\n-\n-class EfficientFormerDropPath(nn.Module):\n-    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n-\n-    def __init__(self, drop_prob: Optional[float] = None) -> None:\n-        super().__init__()\n-        self.drop_prob = drop_prob\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        return drop_path(hidden_states, self.drop_prob, self.training)\n-\n-    def extra_repr(self) -> str:\n-        return f\"p={self.drop_prob}\"\n-\n-\n-class EfficientFormerFlat(nn.Module):\n-    def __init__(self):\n-        super().__init__()\n-\n-    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor]:\n-        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n-        return hidden_states\n-\n-\n-class EfficientFormerMeta3D(nn.Module):\n-    def __init__(self, config: EfficientFormerConfig, dim: int, drop_path: float = 0.0):\n-        super().__init__()\n-\n-        self.token_mixer = EfficientFormerSelfAttention(\n-            dim=config.dim,\n-            key_dim=config.key_dim,\n-            num_heads=config.num_attention_heads,\n-            attention_ratio=config.attention_ratio,\n-            resolution=config.resolution,\n-        )\n-\n-        self.layernorm1 = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n-        self.layernorm2 = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n-\n-        mlp_hidden_dim = int(dim * config.mlp_expansion_ratio)\n-        self.mlp = EfficientFormerDenseMlp(config, in_features=dim, hidden_features=mlp_hidden_dim)\n-\n-        self.drop_path = EfficientFormerDropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n-        self.use_layer_scale = config.use_layer_scale\n-        if config.use_layer_scale:\n-            self.layer_scale_1 = nn.Parameter(config.layer_scale_init_value * torch.ones(dim), requires_grad=True)\n-            self.layer_scale_2 = nn.Parameter(config.layer_scale_init_value * torch.ones(dim), requires_grad=True)\n-\n-    def forward(self, hidden_states: torch.Tensor, output_attentions: bool = False) -> tuple[torch.Tensor]:\n-        self_attention_outputs = self.token_mixer(self.layernorm1(hidden_states), output_attentions)\n-        attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        if self.use_layer_scale:\n-            layer_output = hidden_states + self.drop_path(\n-                self.layer_scale_1.unsqueeze(0).unsqueeze(0) * attention_output\n-            )\n-            layer_output = layer_output + self.drop_path(\n-                self.layer_scale_2.unsqueeze(0).unsqueeze(0) * self.mlp(self.layernorm2(layer_output))\n-            )\n-        else:\n-            layer_output = hidden_states + self.drop_path(attention_output)\n-            layer_output = layer_output + self.drop_path(self.mlp(self.layernorm2(layer_output)))\n-\n-        outputs = (layer_output,) + outputs\n-\n-        return outputs\n-\n-\n-class EfficientFormerMeta3DLayers(nn.Module):\n-    def __init__(self, config: EfficientFormerConfig):\n-        super().__init__()\n-        drop_paths = [\n-            config.drop_path_rate * (block_idx + sum(config.depths[:-1]))\n-            for block_idx in range(config.num_meta3d_blocks)\n-        ]\n-        self.blocks = nn.ModuleList(\n-            [EfficientFormerMeta3D(config, config.hidden_sizes[-1], drop_path=drop_path) for drop_path in drop_paths]\n-        )\n-\n-    def forward(self, hidden_states: torch.Tensor, output_attentions: bool = False) -> tuple[torch.Tensor]:\n-        all_attention_outputs = () if output_attentions else None\n-\n-        for layer_module in self.blocks:\n-            if isinstance(hidden_states, tuple):\n-                hidden_states = hidden_states[0]\n-\n-            hidden_states = layer_module(hidden_states, output_attentions)\n-\n-            if output_attentions:\n-                all_attention_outputs = all_attention_outputs + (hidden_states[1],)\n-\n-        if output_attentions:\n-            outputs = (hidden_states[0],) + all_attention_outputs\n-            return outputs\n-\n-        return hidden_states\n-\n-\n-class EfficientFormerMeta4D(nn.Module):\n-    def __init__(self, config: EfficientFormerConfig, dim: int, drop_path: float = 0.0):\n-        super().__init__()\n-        pool_size = config.pool_size if config.pool_size is not None else 3\n-        self.token_mixer = EfficientFormerPooling(pool_size=pool_size)\n-        mlp_hidden_dim = int(dim * config.mlp_expansion_ratio)\n-        self.mlp = EfficientFormerConvMlp(\n-            config, in_features=dim, hidden_features=mlp_hidden_dim, drop=config.hidden_dropout_prob\n-        )\n-\n-        self.drop_path = EfficientFormerDropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n-        self.use_layer_scale = config.use_layer_scale\n-        if config.use_layer_scale:\n-            self.layer_scale_1 = nn.Parameter(config.layer_scale_init_value * torch.ones(dim), requires_grad=True)\n-            self.layer_scale_2 = nn.Parameter(config.layer_scale_init_value * torch.ones(dim), requires_grad=True)\n-\n-    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor]:\n-        outputs = self.token_mixer(hidden_states)\n-\n-        if self.use_layer_scale:\n-            layer_output = hidden_states + self.drop_path(self.layer_scale_1.unsqueeze(-1).unsqueeze(-1) * outputs)\n-\n-            layer_output = layer_output + self.drop_path(\n-                self.layer_scale_2.unsqueeze(-1).unsqueeze(-1) * self.mlp(layer_output)\n-            )\n-        else:\n-            layer_output = hidden_states + self.drop_path(outputs)\n-            layer_output = layer_output + self.drop_path(self.mlp(layer_output))\n-\n-        return layer_output\n-\n-\n-class EfficientFormerMeta4DLayers(nn.Module):\n-    def __init__(self, config: EfficientFormerConfig, stage_idx: int):\n-        super().__init__()\n-        num_layers = (\n-            config.depths[stage_idx] if stage_idx != -1 else config.depths[stage_idx] - config.num_meta3d_blocks\n-        )\n-        drop_paths = [\n-            config.drop_path_rate * (block_idx + sum(config.depths[:stage_idx])) for block_idx in range(num_layers)\n-        ]\n-\n-        self.blocks = nn.ModuleList(\n-            [\n-                EfficientFormerMeta4D(config, config.hidden_sizes[stage_idx], drop_path=drop_path)\n-                for drop_path in drop_paths\n-            ]\n-        )\n-\n-    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor]:\n-        for layer_module in self.blocks:\n-            hidden_states = layer_module(hidden_states)\n-        return hidden_states\n-\n-\n-class EfficientFormerIntermediateStage(nn.Module):\n-    def __init__(self, config: EfficientFormerConfig, index: int):\n-        super().__init__()\n-        self.meta4D_layers = EfficientFormerMeta4DLayers(config, index)\n-\n-    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor]:\n-        hidden_states = self.meta4D_layers(hidden_states)\n-        return hidden_states\n-\n-\n-class EfficientFormerLastStage(nn.Module):\n-    def __init__(self, config: EfficientFormerConfig):\n-        super().__init__()\n-        self.meta4D_layers = EfficientFormerMeta4DLayers(config, -1)\n-        self.flat = EfficientFormerFlat()\n-        self.meta3D_layers = EfficientFormerMeta3DLayers(config)\n-\n-    def forward(self, hidden_states: torch.Tensor, output_attentions: bool = False) -> tuple[torch.Tensor]:\n-        hidden_states = self.meta4D_layers(hidden_states)\n-        hidden_states = self.flat(hidden_states)\n-        hidden_states = self.meta3D_layers(hidden_states, output_attentions)\n-\n-        return hidden_states\n-\n-\n-class EfficientFormerEncoder(nn.Module):\n-    def __init__(self, config: EfficientFormerConfig):\n-        super().__init__()\n-        self.config = config\n-        num_intermediate_stages = len(config.depths) - 1\n-        downsamples = [\n-            config.downsamples[i] or config.hidden_sizes[i] != config.hidden_sizes[i + 1]\n-            for i in range(num_intermediate_stages)\n-        ]\n-        intermediate_stages = []\n-\n-        for i in range(num_intermediate_stages):\n-            intermediate_stages.append(EfficientFormerIntermediateStage(config, i))\n-            if downsamples[i]:\n-                intermediate_stages.append(\n-                    EfficientFormerPatchEmbeddings(config, config.hidden_sizes[i], config.hidden_sizes[i + 1])\n-                )\n-\n-        self.intermediate_stages = nn.ModuleList(intermediate_stages)\n-        self.last_stage = EfficientFormerLastStage(config)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        output_hidden_states: bool = False,\n-        output_attentions: bool = False,\n-        return_dict: bool = True,\n-    ) -> BaseModelOutput:\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-\n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        for layer_module in self.intermediate_stages:\n-            hidden_states = layer_module(hidden_states)\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        layer_output = self.last_stage(hidden_states, output_attentions=output_attentions)\n-\n-        if output_attentions:\n-            all_self_attentions = all_self_attentions + layer_output[1:]\n-\n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (layer_output[0],)\n-\n-        if not return_dict:\n-            return tuple(v for v in [layer_output[0], all_hidden_states, all_self_attentions] if v is not None)\n-\n-        return BaseModelOutput(\n-            last_hidden_state=layer_output[0],\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-        )\n-\n-\n-class EfficientFormerPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config: EfficientFormerConfig\n-    base_model_prefix = \"efficientformer\"\n-    main_input_name = \"pixel_values\"\n-    supports_gradient_checkpointing = False\n-\n-\n-EFFICIENTFORMER_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module) subclass. Use it as a\n-    regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.\n-\n-    Parameters:\n-        config ([`EfficientFormerConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-EFFICIENTFORMER_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`ViTImageProcessor`]. See\n-            [`ViTImageProcessor.preprocess`] for details.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare EfficientFormer Model transformer outputting raw hidden-states without any specific head on top.\",\n-    EFFICIENTFORMER_START_DOCSTRING,\n-)\n-class EfficientFormerModel(EfficientFormerPreTrainedModel):\n-    def __init__(self, config: EfficientFormerConfig):\n-        super().__init__(config)\n-        self.config = config\n-        _no_split_modules = [\"EfficientFormerMeta4D\"]\n-\n-        self.patch_embed = EfficientFormerConvStem(config, config.hidden_sizes[0])\n-        self.encoder = EfficientFormerEncoder(config)\n-        self.layernorm = nn.LayerNorm(config.hidden_sizes[-1], eps=config.layer_norm_eps)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(EFFICIENTFORMER_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPooling,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"vision\",\n-        expected_output=_EXPECTED_OUTPUT_SHAPE,\n-    )\n-    def forward(\n-        self,\n-        pixel_values: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, BaseModelOutput]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        if pixel_values is None:\n-            raise ValueError(\"You have to specify pixel_values\")\n-\n-        embedding_output = self.patch_embed(pixel_values)\n-        encoder_outputs = self.encoder(\n-            embedding_output, output_attentions=output_attentions, output_hidden_states=output_hidden_states\n-        )\n-\n-        sequence_output = encoder_outputs[0]\n-        sequence_output = self.layernorm(sequence_output)\n-\n-        if not return_dict:\n-            head_outputs = (sequence_output,)\n-            return head_outputs + encoder_outputs[1:]\n-\n-        return BaseModelOutput(\n-            last_hidden_state=sequence_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    EfficientFormer Model transformer with an image classification head on top (a linear layer on top of the final\n-    hidden state of the [CLS] token) e.g. for ImageNet.\n-    \"\"\",\n-    EFFICIENTFORMER_START_DOCSTRING,\n-)\n-class EfficientFormerForImageClassification(EfficientFormerPreTrainedModel):\n-    def __init__(self, config: EfficientFormerConfig):\n-        super().__init__(config)\n-\n-        self.num_labels = config.num_labels\n-        self.efficientformer = EfficientFormerModel(config)\n-\n-        # Classifier head\n-        self.classifier = (\n-            nn.Linear(config.hidden_sizes[-1], config.num_labels) if config.num_labels > 0 else nn.Identity()\n-        )\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(EFFICIENTFORMER_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_IMAGE_CLASS_CHECKPOINT,\n-        output_type=ImageClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,\n-    )\n-    def forward(\n-        self,\n-        pixel_values: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, ImageClassifierOutput]:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        outputs = self.efficientformer(\n-            pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        sequence_output = outputs[0]\n-\n-        logits = self.classifier(sequence_output.mean(-2))\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(labels, logits, self.config)\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n-        return ImageClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@dataclass\n-class EfficientFormerForImageClassificationWithTeacherOutput(ModelOutput):\n-    \"\"\"\n-    Output type of [`EfficientFormerForImageClassificationWithTeacher`].\n-\n-    Args:\n-        logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Prediction scores as the average of the cls_logits and distillation logits.\n-        cls_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Prediction scores of the classification head (i.e. the linear layer on top of the final hidden state of the\n-            class token).\n-        distillation_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Prediction scores of the distillation head (i.e. the linear layer on top of the final hidden state of the\n-            distillation token).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-    \"\"\"\n-\n-    logits: Optional[torch.FloatTensor] = None\n-    cls_logits: Optional[torch.FloatTensor] = None\n-    distillation_logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    EfficientFormer Model transformer with image classification heads on top (a linear layer on top of the final hidden\n-    state of the [CLS] token and a linear layer on top of the final hidden state of the distillation token) e.g. for\n-    ImageNet.\n-\n-    <Tip warning={true}>\n-\n-           This model supports inference-only. Fine-tuning with distillation (i.e. with a teacher) is not yet\n-           supported.\n-\n-    </Tip>\n-    \"\"\",\n-    EFFICIENTFORMER_START_DOCSTRING,\n-)\n-class EfficientFormerForImageClassificationWithTeacher(EfficientFormerPreTrainedModel):\n-    def __init__(self, config: EfficientFormerConfig):\n-        super().__init__(config)\n-\n-        self.num_labels = config.num_labels\n-        self.efficientformer = EfficientFormerModel(config)\n-\n-        # Classifier head\n-        self.classifier = nn.Linear(config.hidden_size, config.num_labels) if config.num_labels > 0 else nn.Identity()\n-        # Distillation head\n-        self.distillation_classifier = (\n-            nn.Linear(config.hidden_size, config.num_labels) if config.num_labels > 0 else nn.Identity()\n-        )\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(EFFICIENTFORMER_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_IMAGE_CLASS_CHECKPOINT,\n-        output_type=EfficientFormerForImageClassificationWithTeacherOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,\n-    )\n-    def forward(\n-        self,\n-        pixel_values: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, EfficientFormerForImageClassificationWithTeacherOutput]:\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        outputs = self.efficientformer(\n-            pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        sequence_output = outputs[0]\n-\n-        cls_logits = self.classifier(sequence_output.mean(-2))\n-        distillation_logits = self.distillation_classifier(sequence_output.mean(-2))\n-\n-        # during inference, return the average of both classifier predictions\n-        logits = (cls_logits + distillation_logits) / 2\n-\n-        if not return_dict:\n-            output = (logits, cls_logits, distillation_logits) + outputs[1:]\n-            return output\n-\n-        return EfficientFormerForImageClassificationWithTeacherOutput(\n-            logits=logits,\n-            cls_logits=cls_logits,\n-            distillation_logits=distillation_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-__all__ = [\n-    \"EfficientFormerForImageClassification\",\n-    \"EfficientFormerForImageClassificationWithTeacher\",\n-    \"EfficientFormerModel\",\n-    \"EfficientFormerPreTrainedModel\",\n-]"
        },
        {
            "sha": "2beb8f463ff10af33ea980599ea4d5fc05888acb",
            "filename": "src/transformers/models/deprecated/ernie_m/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,28 +0,0 @@\n-# Copyright 2023 The HuggingFace and Baidu Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from typing import TYPE_CHECKING\n-\n-from ....utils import _LazyModule\n-from ....utils.import_utils import define_import_structure\n-\n-\n-if TYPE_CHECKING:\n-    from .configuration_ernie_m import *\n-    from .modeling_ernie_m import *\n-    from .tokenization_ernie_m import *\n-else:\n-    import sys\n-\n-    _file = globals()[\"__file__\"]\n-    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "aa08a1d05c7559d7dbffdb184d66cb8028d3869d",
            "filename": "src/transformers/models/deprecated/ernie_m/configuration_ernie_m.py",
            "status": "removed",
            "additions": 0,
            "deletions": 112,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fconfiguration_ernie_m.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fconfiguration_ernie_m.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fconfiguration_ernie_m.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,112 +0,0 @@\n-# coding=utf-8\n-# Copyright 2023 Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang and The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"ErnieM model configuration\"\"\"\n-# Adapted from original paddlenlp repository.(https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/transformers/ernie_m/configuration.py)\n-\n-from __future__ import annotations\n-\n-from ....configuration_utils import PreTrainedConfig\n-\n-\n-class ErnieMConfig(PreTrainedConfig):\n-    r\"\"\"\n-    This is the configuration class to store the configuration of a [`ErnieMModel`]. It is used to instantiate a\n-    Ernie-M model according to the specified arguments, defining the model architecture. Instantiating a configuration\n-    with the defaults will yield a similar configuration to that of the `Ernie-M`\n-    [susnato/ernie-m-base_pytorch](https://huggingface.co/susnato/ernie-m-base_pytorch) architecture.\n-\n-\n-    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PreTrainedConfig`] for more information.\n-\n-    Args:\n-        vocab_size (`int`, *optional*, defaults to 250002):\n-            Vocabulary size of `inputs_ids` in [`ErnieMModel`]. Also is the vocab size of token embedding matrix.\n-            Defines the number of different tokens that can be represented by the `inputs_ids` passed when calling\n-            [`ErnieMModel`].\n-        hidden_size (`int`, *optional*, defaults to 768):\n-            Dimensionality of the embedding layer, encoder layers and pooler layer.\n-        num_hidden_layers (`int`, *optional*, defaults to 12):\n-            Number of hidden layers in the Transformer encoder.\n-        num_attention_heads (`int`, *optional*, defaults to 12):\n-            Number of attention heads for each attention layer in the Transformer encoder.\n-        intermediate_size (`int`, *optional*, defaults to 3072):\n-            Dimensionality of the feed-forward (ff) layer in the encoder. Input tensors to feed-forward layers are\n-            firstly projected from hidden_size to intermediate_size, and then projected back to hidden_size. Typically\n-            intermediate_size is larger than hidden_size.\n-        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n-            The non-linear activation function in the feed-forward layer. `\"gelu\"`, `\"relu\"` and any other torch\n-            supported activation functions are supported.\n-        hidden_dropout_prob (`float`, *optional*, defaults to 0.1):\n-            The dropout probability for all fully connected layers in the embeddings and encoder.\n-        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.1):\n-            The dropout probability used in `MultiHeadAttention` in all encoder layers to drop some attention target.\n-        max_position_embeddings (`int`, *optional*, defaults to 514):\n-            The maximum value of the dimensionality of position encoding, which dictates the maximum supported length\n-            of an input sequence.\n-        initializer_range (`float`, *optional*, defaults to 0.02):\n-            The standard deviation of the normal initializer for initializing all weight matrices. The index of padding\n-            token in the token vocabulary.\n-        pad_token_id (`int`, *optional*, defaults to 1):\n-            Padding token id.\n-        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n-            The epsilon used by the layer normalization layers.\n-        classifier_dropout (`float`, *optional*):\n-            The dropout ratio for the classification head.\n-        act_dropout (`float`, *optional*, defaults to 0.0):\n-            This dropout probability is used in `ErnieMEncoderLayer` after activation.\n-\n-    A normal_initializer initializes weight matrices as normal distributions. See\n-    `ErnieMPretrainedModel._init_weights()` for how weights are initialized in `ErnieMModel`.\n-    \"\"\"\n-\n-    model_type = \"ernie_m\"\n-    attribute_map: dict[str, str] = {\"dropout\": \"classifier_dropout\", \"num_classes\": \"num_labels\"}\n-\n-    def __init__(\n-        self,\n-        vocab_size: int = 250002,\n-        hidden_size: int = 768,\n-        num_hidden_layers: int = 12,\n-        num_attention_heads: int = 12,\n-        intermediate_size: int = 3072,\n-        hidden_act: str = \"gelu\",\n-        hidden_dropout_prob: float = 0.1,\n-        attention_probs_dropout_prob: float = 0.1,\n-        max_position_embeddings: int = 514,\n-        initializer_range: float = 0.02,\n-        pad_token_id: int = 1,\n-        layer_norm_eps: float = 1e-05,\n-        classifier_dropout=None,\n-        act_dropout=0.0,\n-        **kwargs,\n-    ):\n-        super().__init__(pad_token_id=pad_token_id, **kwargs)\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.initializer_range = initializer_range\n-        self.layer_norm_eps = layer_norm_eps\n-        self.classifier_dropout = classifier_dropout\n-        self.act_dropout = act_dropout\n-\n-\n-__all__ = [\"ErnieMConfig\"]"
        },
        {
            "sha": "a22e21bd7a49117359365498388b642d43ab5cc2",
            "filename": "src/transformers/models/deprecated/ernie_m/modeling_ernie_m.py",
            "status": "removed",
            "additions": 0,
            "deletions": 987,
            "changes": 987,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,987 +0,0 @@\n-# coding=utf-8\n-# Copyright 2023 Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch ErnieM model.\"\"\"\n-\n-import math\n-from typing import Optional, Union\n-\n-import torch\n-from torch import nn, tensor\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n-\n-from ....activations import ACT2FN\n-from ....cache_utils import Cache\n-from ....modeling_outputs import (\n-    BaseModelOutputWithPastAndCrossAttentions,\n-    BaseModelOutputWithPoolingAndCrossAttentions,\n-    MultipleChoiceModelOutput,\n-    QuestionAnsweringModelOutput,\n-    SequenceClassifierOutput,\n-    TokenClassifierOutput,\n-)\n-from ....modeling_utils import PreTrainedModel\n-from ....utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n-from .configuration_ernie_m import ErnieMConfig\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-_CHECKPOINT_FOR_DOC = \"susnato/ernie-m-base_pytorch\"\n-_CONFIG_FOR_DOC = \"ErnieMConfig\"\n-_TOKENIZER_FOR_DOC = \"ErnieMTokenizer\"\n-\n-\n-# Adapted from paddlenlp.transformers.ernie_m.modeling.ErnieEmbeddings\n-class ErnieMEmbeddings(nn.Module):\n-    \"\"\"Construct the embeddings from word and position embeddings.\"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-        self.hidden_size = config.hidden_size\n-        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n-        self.position_embeddings = nn.Embedding(\n-            config.max_position_embeddings, config.hidden_size, padding_idx=config.pad_token_id\n-        )\n-        self.layer_norm = nn.LayerNorm(normalized_shape=config.hidden_size, eps=config.layer_norm_eps)\n-        self.dropout = nn.Dropout(p=config.hidden_dropout_prob)\n-        self.padding_idx = config.pad_token_id\n-\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.LongTensor] = None,\n-        past_key_values_length: int = 0,\n-    ) -> torch.Tensor:\n-        if inputs_embeds is None:\n-            inputs_embeds = self.word_embeddings(input_ids)\n-        if position_ids is None:\n-            input_shape = inputs_embeds.size()[:-1]\n-            ones = torch.ones(input_shape, dtype=torch.int64, device=inputs_embeds.device)\n-            seq_length = torch.cumsum(ones, dim=1)\n-            position_ids = seq_length - ones\n-\n-            if past_key_values_length > 0:\n-                position_ids = position_ids + past_key_values_length\n-        # to mimic paddlenlp implementation\n-        position_ids += 2\n-        position_embeddings = self.position_embeddings(position_ids)\n-        embeddings = inputs_embeds + position_embeddings\n-        embeddings = self.layer_norm(embeddings)\n-        embeddings = self.dropout(embeddings)\n-\n-        return embeddings\n-\n-\n-class ErnieMSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n-        super().__init__()\n-        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n-            raise ValueError(\n-                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n-                f\"heads ({config.num_attention_heads})\"\n-            )\n-\n-        self.num_attention_heads = config.num_attention_heads\n-        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n-        self.all_head_size = self.num_attention_heads * self.attention_head_size\n-\n-        self.q_proj = nn.Linear(config.hidden_size, self.all_head_size)\n-        self.k_proj = nn.Linear(config.hidden_size, self.all_head_size)\n-        self.v_proj = nn.Linear(config.hidden_size, self.all_head_size)\n-\n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n-\n-        self.is_decoder = config.is_decoder\n-\n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.q_proj(hidden_states)\n-\n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n-        is_cross_attention = encoder_hidden_states is not None\n-\n-        if is_cross_attention and past_key_values is not None:\n-            # reuse k,v, cross_attentions\n-            key_layer = past_key_values[0]\n-            value_layer = past_key_values[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.k_proj(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.v_proj(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_values is not None:\n-            key_layer = self.transpose_for_scores(self.k_proj(hidden_states))\n-            value_layer = self.transpose_for_scores(self.v_proj(hidden_states))\n-            key_layer = torch.cat([past_key_values[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_values[1], value_layer], dim=2)\n-        else:\n-            key_layer = self.transpose_for_scores(self.k_proj(hidden_states))\n-            value_layer = self.transpose_for_scores(self.v_proj(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        use_cache = past_key_values is not None\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_values` is always `None`\n-            past_key_values = (key_layer, value_layer)\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if use_cache:\n-                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n-                    -1, 1\n-                )\n-            else:\n-                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in ErnieMModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        context_layer = torch.matmul(attention_probs, value_layer)\n-\n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_values,)\n-        return outputs\n-\n-\n-class ErnieMAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n-        super().__init__()\n-        self.self_attn = ErnieMSelfAttention(config, position_embedding_type=position_embedding_type)\n-        self.out_proj = nn.Linear(config.hidden_size, config.hidden_size)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self_attn(\n-            hidden_states,\n-            attention_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_values,\n-            output_attentions,\n-        )\n-        attention_output = self.out_proj(self_outputs[0])\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n-\n-\n-class ErnieMEncoderLayer(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        # to mimic paddlenlp implementation\n-        dropout = 0.1 if config.hidden_dropout_prob is None else config.hidden_dropout_prob\n-        act_dropout = config.hidden_dropout_prob if config.act_dropout is None else config.act_dropout\n-\n-        self.self_attn = ErnieMAttention(config)\n-        self.linear1 = nn.Linear(config.hidden_size, config.intermediate_size)\n-        self.dropout = nn.Dropout(act_dropout)\n-        self.linear2 = nn.Linear(config.intermediate_size, config.hidden_size)\n-        self.norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.dropout1 = nn.Dropout(dropout)\n-        self.dropout2 = nn.Dropout(dropout)\n-        if isinstance(config.hidden_act, str):\n-            self.activation = ACT2FN[config.hidden_act]\n-        else:\n-            self.activation = config.hidden_act\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = True,\n-    ):\n-        residual = hidden_states\n-        if output_attentions:\n-            hidden_states, attention_opt_weights = self.self_attn(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-            )\n-\n-        else:\n-            hidden_states = self.self_attn(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-            )\n-        hidden_states = residual + self.dropout1(hidden_states)\n-        hidden_states = self.norm1(hidden_states)\n-        residual = hidden_states\n-\n-        hidden_states = self.linear1(hidden_states)\n-        hidden_states = self.activation(hidden_states)\n-        hidden_states = self.dropout(hidden_states)\n-        hidden_states = self.linear2(hidden_states)\n-        hidden_states = residual + self.dropout2(hidden_states)\n-        hidden_states = self.norm2(hidden_states)\n-\n-        if output_attentions:\n-            return hidden_states, attention_opt_weights\n-        else:\n-            return hidden_states\n-\n-\n-class ErnieMEncoder(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.config = config\n-        self.layers = nn.ModuleList([ErnieMEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n-\n-    def forward(\n-        self,\n-        input_embeds: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n-        hidden_states = () if output_hidden_states else None\n-        attentions = () if output_attentions else None\n-\n-        output = input_embeds\n-        if output_hidden_states:\n-            hidden_states = hidden_states + (output,)\n-        for i, layer in enumerate(self.layers):\n-            output, opt_attn_weights = layer(\n-                hidden_states=output,\n-                attention_mask=attention_mask,\n-                past_key_values=past_key_values[i] if past_key_values is not None else None,\n-            )\n-\n-            if output_hidden_states:\n-                hidden_states = hidden_states + (output,)\n-            if output_attentions:\n-                attentions = attentions + (opt_attn_weights,)\n-\n-        last_hidden_state = output\n-        if not return_dict:\n-            return tuple(v for v in [last_hidden_state, hidden_states, attentions] if v is not None)\n-\n-        return BaseModelOutputWithPastAndCrossAttentions(\n-            last_hidden_state=last_hidden_state, hidden_states=hidden_states, attentions=attentions\n-        )\n-\n-\n-class ErnieMPooler(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n-        self.activation = nn.Tanh()\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        # We \"pool\" the model by simply taking the hidden state corresponding\n-        # to the first token.\n-        first_token_tensor = hidden_states[:, 0]\n-        pooled_output = self.dense(first_token_tensor)\n-        pooled_output = self.activation(pooled_output)\n-        return pooled_output\n-\n-\n-class ErnieMPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config: ErnieMConfig\n-    base_model_prefix = \"ernie_m\"\n-\n-\n-ERNIE_M_START_DOCSTRING = r\"\"\"\n-\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n-    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`ErnieMConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-ERNIE_M_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`ErnieMTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare ErnieM Model transformer outputting raw hidden-states without any specific head on top.\",\n-    ERNIE_M_START_DOCSTRING,\n-)\n-class ErnieMModel(ErnieMPreTrainedModel):\n-    def __init__(self, config, add_pooling_layer=True):\n-        super().__init__(config)\n-        self.initializer_range = config.initializer_range\n-        self.embeddings = ErnieMEmbeddings(config)\n-        self.encoder = ErnieMEncoder(config)\n-        self.pooler = ErnieMPooler(config) if add_pooling_layer else None\n-        self.post_init()\n-\n-    def get_input_embeddings(self):\n-        return self.embeddings.word_embeddings\n-\n-    def set_input_embeddings(self, value):\n-        self.embeddings.word_embeddings = value\n-\n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layers[layer].self_attn.prune_heads(heads)\n-\n-    @add_start_docstrings_to_model_forward(ERNIE_M_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        processor_class=_TOKENIZER_FOR_DOC,\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPastAndCrossAttentions,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n-    def forward(\n-        self,\n-        input_ids: Optional[tensor] = None,\n-        position_ids: Optional[tensor] = None,\n-        attention_mask: Optional[tensor] = None,\n-        inputs_embeds: Optional[tensor] = None,\n-        past_key_values: Optional[tuple[tuple[tensor]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple[torch.FloatTensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time.\")\n-\n-        # init the default bool value\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.return_dict\n-\n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = past_key_values.get_seq_length()\n-\n-        # Adapted from paddlenlp.transformers.ernie_m.ErnieMModel\n-        if attention_mask is None:\n-            attention_mask = (input_ids == self.config.pad_token_id).to(torch.float32)\n-            attention_mask *= torch.finfo(attention_mask.dtype).min\n-            if past_key_values is not None:\n-                batch_size = past_key_values[0][0].shape[0]\n-                past_mask = torch.zeros([batch_size, 1, 1, past_key_values_length], dtype=attention_mask.dtype)\n-                attention_mask = torch.concat([past_mask, attention_mask], dim=-1)\n-        # For 2D attention_mask from tokenizer\n-        elif attention_mask.ndim == 2:\n-            attention_mask = attention_mask.to(torch.float32)\n-            attention_mask = 1.0 - attention_mask\n-            attention_mask *= torch.finfo(attention_mask.dtype).min\n-\n-        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)\n-\n-        embedding_output = self.embeddings(\n-            input_ids=input_ids,\n-            position_ids=position_ids,\n-            inputs_embeds=inputs_embeds,\n-            past_key_values_length=past_key_values_length,\n-        )\n-        encoder_outputs = self.encoder(\n-            embedding_output,\n-            attention_mask=extended_attention_mask,\n-            past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        if not return_dict:\n-            sequence_output = encoder_outputs[0]\n-            pooler_output = self.pooler(sequence_output) if self.pooler is not None else None\n-            return (sequence_output, pooler_output) + encoder_outputs[1:]\n-\n-        sequence_output = encoder_outputs[\"last_hidden_state\"]\n-        pooler_output = self.pooler(sequence_output) if self.pooler is not None else None\n-        hidden_states = None if not output_hidden_states else encoder_outputs[\"hidden_states\"]\n-        attentions = None if not output_attentions else encoder_outputs[\"attentions\"]\n-\n-        return BaseModelOutputWithPoolingAndCrossAttentions(\n-            last_hidden_state=sequence_output,\n-            pooler_output=pooler_output,\n-            hidden_states=hidden_states,\n-            attentions=attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"ErnieM Model transformer with a sequence classification/regression head on top (a linear layer on top of\n-    the pooled output) e.g. for GLUE tasks.\"\"\",\n-    ERNIE_M_START_DOCSTRING,\n-)\n-class ErnieMForSequenceClassification(ErnieMPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.config = config\n-\n-        self.ernie_m = ErnieMModel(config)\n-        classifier_dropout = (\n-            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n-        )\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(ERNIE_M_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        processor_class=_TOKENIZER_FOR_DOC,\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=SequenceClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        return_dict: Optional[bool] = True,\n-        labels: Optional[torch.Tensor] = None,\n-    ) -> Union[tuple[torch.FloatTensor], SequenceClassifierOutput]:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        outputs = self.ernie_m(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            inputs_embeds=inputs_embeds,\n-            past_key_values=past_key_values,\n-            output_hidden_states=output_hidden_states,\n-            output_attentions=output_attentions,\n-            return_dict=return_dict,\n-        )\n-\n-        pooled_output = outputs[1]\n-\n-        pooled_output = self.dropout(pooled_output)\n-        logits = self.classifier(pooled_output)\n-\n-        loss = None\n-        if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n-        return SequenceClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"ErnieM Model with a multiple choice classification head on top (a linear layer on top of\n-    the pooled output and a softmax) e.g. for RocStories/SWAG tasks.\"\"\",\n-    ERNIE_M_START_DOCSTRING,\n-)\n-class ErnieMForMultipleChoice(ErnieMPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-\n-        self.ernie_m = ErnieMModel(config)\n-        classifier_dropout = (\n-            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n-        )\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.classifier = nn.Linear(config.hidden_size, 1)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(ERNIE_M_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=MultipleChoiceModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = True,\n-    ) -> Union[tuple[torch.FloatTensor], MultipleChoiceModelOutput]:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n-            num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n-            `input_ids` above)\n-        \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n-\n-        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n-        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n-        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n-        inputs_embeds = (\n-            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n-            if inputs_embeds is not None\n-            else None\n-        )\n-\n-        outputs = self.ernie_m(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        pooled_output = outputs[1]\n-\n-        pooled_output = self.dropout(pooled_output)\n-        logits = self.classifier(pooled_output)\n-        reshaped_logits = logits.view(-1, num_choices)\n-\n-        loss = None\n-        if labels is not None:\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(reshaped_logits, labels)\n-\n-        if not return_dict:\n-            output = (reshaped_logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n-        return MultipleChoiceModelOutput(\n-            loss=loss,\n-            logits=reshaped_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"ErnieM Model with a token classification head on top (a linear layer on top of\n-    the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\"\"\",\n-    ERNIE_M_START_DOCSTRING,\n-)\n-class ErnieMForTokenClassification(ErnieMPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-\n-        self.ernie_m = ErnieMModel(config, add_pooling_layer=False)\n-        classifier_dropout = (\n-            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n-        )\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(ERNIE_M_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        processor_class=_TOKENIZER_FOR_DOC,\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=TokenClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        return_dict: Optional[bool] = True,\n-        labels: Optional[torch.Tensor] = None,\n-    ) -> Union[tuple[torch.FloatTensor], TokenClassifierOutput]:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n-        \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        outputs = self.ernie_m(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            inputs_embeds=inputs_embeds,\n-            past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        sequence_output = outputs[0]\n-\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.classifier(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"ErnieM Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n-    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\"\"\",\n-    ERNIE_M_START_DOCSTRING,\n-)\n-class ErnieMForQuestionAnswering(ErnieMPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-\n-        self.ernie_m = ErnieMModel(config, add_pooling_layer=False)\n-        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(ERNIE_M_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        processor_class=_TOKENIZER_FOR_DOC,\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=QuestionAnsweringModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        start_positions: Optional[torch.Tensor] = None,\n-        end_positions: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = True,\n-    ) -> Union[tuple[torch.FloatTensor], QuestionAnsweringModelOutput]:\n-        r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        outputs = self.ernie_m(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        sequence_output = outputs[0]\n-\n-        logits = self.qa_outputs(sequence_output)\n-        start_logits, end_logits = logits.split(1, dim=-1)\n-        start_logits = start_logits.squeeze(-1).contiguous()\n-        end_logits = end_logits.squeeze(-1).contiguous()\n-\n-        total_loss = None\n-        if start_positions is not None and end_positions is not None:\n-            # If we are on multi-GPU, split add a dimension\n-            if len(start_positions.size()) > 1:\n-                start_positions = start_positions.squeeze(-1)\n-            if len(end_positions.size()) > 1:\n-                end_positions = end_positions.squeeze(-1)\n-            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n-            ignored_index = start_logits.size(1)\n-            start_positions = start_positions.clamp(0, ignored_index)\n-            end_positions = end_positions.clamp(0, ignored_index)\n-\n-            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n-            start_loss = loss_fct(start_logits, start_positions)\n-            end_loss = loss_fct(end_logits, end_positions)\n-            total_loss = (start_loss + end_loss) / 2\n-\n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n-\n-        return QuestionAnsweringModelOutput(\n-            loss=total_loss,\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"ErnieMForInformationExtraction is a Ernie-M Model with two linear layer on top of the hidden-states output to\n-    compute `start_prob` and `end_prob`, designed for Universal Information Extraction.\"\"\",\n-    ERNIE_M_START_DOCSTRING,\n-)\n-class ErnieMForInformationExtraction(ErnieMPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.ernie_m = ErnieMModel(config)\n-        self.linear_start = nn.Linear(config.hidden_size, 1)\n-        self.linear_end = nn.Linear(config.hidden_size, 1)\n-        self.sigmoid = nn.Sigmoid()\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(ERNIE_M_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        start_positions: Optional[torch.Tensor] = None,\n-        end_positions: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = True,\n-    ) -> Union[tuple[torch.FloatTensor], QuestionAnsweringModelOutput]:\n-        r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for position (index) for computing the start_positions loss. Position outside of the sequence are\n-            not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) for computing the end_positions loss. Position outside of the sequence are not\n-            taken into account for computing the loss.\n-        \"\"\"\n-\n-        result = self.ernie_m(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-        if return_dict:\n-            sequence_output = result.last_hidden_state\n-        elif not return_dict:\n-            sequence_output = result[0]\n-\n-        start_logits = self.linear_start(sequence_output)\n-        start_logits = start_logits.squeeze(-1)\n-        end_logits = self.linear_end(sequence_output)\n-        end_logits = end_logits.squeeze(-1)\n-\n-        total_loss = None\n-        if start_positions is not None and end_positions is not None:\n-            # If we are on multi-GPU, split add a dimension\n-            if len(start_positions.size()) > 1:\n-                start_positions = start_positions.squeeze(-1)\n-            if len(end_positions.size()) > 1:\n-                end_positions = end_positions.squeeze(-1)\n-            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n-            ignored_index = start_logits.size(1)\n-            start_positions = start_positions.clamp(0, ignored_index)\n-            end_positions = end_positions.clamp(0, ignored_index)\n-\n-            loss_fct = BCEWithLogitsLoss()\n-            start_loss = loss_fct(start_logits, start_positions)\n-            end_loss = loss_fct(end_logits, end_positions)\n-            total_loss = (start_loss + end_loss) / 2\n-\n-        if not return_dict:\n-            return tuple(\n-                i\n-                for i in [total_loss, start_logits, end_logits, result.hidden_states, result.attentions]\n-                if i is not None\n-            )\n-\n-        return QuestionAnsweringModelOutput(\n-            loss=total_loss,\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=result.hidden_states,\n-            attentions=result.attentions,\n-        )\n-\n-\n-__all__ = [\n-    \"ErnieMForMultipleChoice\",\n-    \"ErnieMForQuestionAnswering\",\n-    \"ErnieMForSequenceClassification\",\n-    \"ErnieMForTokenClassification\",\n-    \"ErnieMModel\",\n-    \"ErnieMPreTrainedModel\",\n-    \"ErnieMForInformationExtraction\",\n-]"
        },
        {
            "sha": "4c33abd043e6d170485e6d01298788d461275aa6",
            "filename": "src/transformers/models/deprecated/ernie_m/tokenization_ernie_m.py",
            "status": "removed",
            "additions": 0,
            "deletions": 409,
            "changes": 409,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Ftokenization_ernie_m.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Ftokenization_ernie_m.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Ftokenization_ernie_m.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,409 +0,0 @@\n-# coding=utf-8\n-# Copyright 2023 Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang and The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Tokenization classes for Ernie-M.\"\"\"\n-\n-import os\n-import unicodedata\n-from typing import Any, Optional\n-\n-import sentencepiece as spm\n-\n-from ....tokenization_utils import PreTrainedTokenizer\n-from ....utils import logging\n-from ....utils.import_utils import requires\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-SPIECE_UNDERLINE = \"â–\"\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\", \"sentencepiece_model_ckpt\": \"sentencepiece.bpe.model\"}\n-\n-RESOURCE_FILES_NAMES = {\n-    \"sentencepiece_model_file\": \"sentencepiece.bpe.model\",\n-    \"vocab_file\": \"vocab.txt\",\n-}\n-\n-\n-# Adapted from paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer\n-@requires(backends=(\"sentencepiece\",))\n-class ErnieMTokenizer(PreTrainedTokenizer):\n-    r\"\"\"\n-    Constructs a Ernie-M tokenizer. It uses the `sentencepiece` tools to cut the words to sub-words.\n-\n-    Args:\n-        sentencepiece_model_file (`str`):\n-            The file path of sentencepiece model.\n-        vocab_file (`str`, *optional*):\n-            The file path of the vocabulary.\n-        do_lower_case (`str`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n-            A special token representing the `unknown (out-of-vocabulary)` token. An unknown token is set to be\n-            `unk_token` inorder to be converted to an ID.\n-        sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            A special token separating two different sentences in the same input.\n-        pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n-            A special token used to make arrays of tokens the same size for batching purposes.\n-        cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n-            A special token used for sequence classification. It is the last token of the sequence when built with\n-            special tokens.\n-        mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n-            A special token representing a masked token. This is the token used in the masked language modeling task\n-            which the model tries to predict the original unmasked ones.\n-    \"\"\"\n-\n-    # Ernie-M model doesn't have token_type embedding.\n-    model_input_names: list[str] = [\"input_ids\"]\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    resource_files_names = RESOURCE_FILES_NAMES\n-\n-    def __init__(\n-        self,\n-        sentencepiece_model_ckpt,\n-        vocab_file=None,\n-        do_lower_case=False,\n-        encoding=\"utf8\",\n-        unk_token=\"[UNK]\",\n-        sep_token=\"[SEP]\",\n-        pad_token=\"[PAD]\",\n-        cls_token=\"[CLS]\",\n-        mask_token=\"[MASK]\",\n-        sp_model_kwargs: Optional[dict[str, Any]] = None,\n-        **kwargs,\n-    ) -> None:\n-        # Mask token behave like a normal word, i.e. include the space before it and\n-        # is included in the raw text, there should be a match in a non-normalized sentence.\n-\n-        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n-\n-        self.do_lower_case = do_lower_case\n-        self.sentencepiece_model_ckpt = sentencepiece_model_ckpt\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(sentencepiece_model_ckpt)\n-\n-        # to mimic paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer functioning\n-        if vocab_file is not None:\n-            self.vocab = self.load_vocab(filepath=vocab_file)\n-        else:\n-            self.vocab = {self.sp_model.id_to_piece(id): id for id in range(self.sp_model.get_piece_size())}\n-        self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n-\n-        super().__init__(\n-            do_lower_case=do_lower_case,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            vocab_file=vocab_file,\n-            encoding=encoding,\n-            sp_model_kwargs=self.sp_model_kwargs,\n-            **kwargs,\n-        )\n-\n-    def get_offset_mapping(self, text):\n-        if text is None:\n-            return None\n-\n-        split_tokens = self.tokenize(text)\n-        normalized_text, char_mapping = \"\", []\n-\n-        for i, ch in enumerate(text):\n-            if ch in self.SP_CHAR_MAPPING:\n-                ch = self.SP_CHAR_MAPPING.get(ch)\n-            else:\n-                ch = unicodedata.normalize(\"NFKC\", ch)\n-            if self.is_whitespace(ch):\n-                continue\n-            normalized_text += ch\n-            char_mapping.extend([i] * len(ch))\n-\n-        text, token_mapping, offset = normalized_text, [], 0\n-\n-        if self.do_lower_case:\n-            text = text.lower()\n-\n-        for token in split_tokens:\n-            if token[:1] == \"â–\":\n-                token = token[1:]\n-            start = text[offset:].index(token) + offset\n-            end = start + len(token)\n-\n-            token_mapping.append((char_mapping[start], char_mapping[end - 1] + 1))\n-            offset = end\n-        return token_mapping\n-\n-    @property\n-    def vocab_size(self):\n-        return len(self.vocab)\n-\n-    def get_vocab(self):\n-        return dict(self.vocab, **self.added_tokens_encoder)\n-\n-    def __getstate__(self):\n-        state = self.__dict__.copy()\n-        state[\"sp_model\"] = None\n-        return state\n-\n-    def __setstate__(self, d):\n-        self.__dict__ = d\n-\n-        # for backward compatibility\n-        if not hasattr(self, \"sp_model_kwargs\"):\n-            self.sp_model_kwargs = {}\n-\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(self.sentencepiece_model_ckpt)\n-\n-    def clean_text(self, text):\n-        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n-        return \"\".join(self.SP_CHAR_MAPPING.get(c, c) for c in text)\n-\n-    def _tokenize(self, text, enable_sampling=False, nbest_size=64, alpha=0.1):\n-        \"\"\"Tokenize a string.\"\"\"\n-\n-        if self.sp_model_kwargs.get(\"enable_sampling\") is True:\n-            enable_sampling = True\n-        if self.sp_model_kwargs.get(\"alpha\") is not None:\n-            alpha = self.sp_model_kwargs.get(\"alpha\")\n-        if self.sp_model_kwargs.get(\"nbest_size\") is not None:\n-            nbest_size = self.sp_model_kwargs.get(\"nbest_size\")\n-\n-        if not enable_sampling:\n-            pieces = self.sp_model.EncodeAsPieces(text)\n-        else:\n-            pieces = self.sp_model.SampleEncodeAsPieces(text, nbest_size, alpha)\n-        new_pieces = []\n-        for pi, piece in enumerate(pieces):\n-            if piece == SPIECE_UNDERLINE:\n-                if not pieces[pi + 1].startswith(SPIECE_UNDERLINE) and pi != 0:\n-                    new_pieces.append(SPIECE_UNDERLINE)\n-                    continue\n-                else:\n-                    continue\n-            lst_i = 0\n-            for i, chunk in enumerate(piece):\n-                if chunk == SPIECE_UNDERLINE:\n-                    continue\n-                if self.is_ch_char(chunk) or self.is_punct(chunk):\n-                    if i > lst_i and piece[lst_i:i] != SPIECE_UNDERLINE:\n-                        new_pieces.append(piece[lst_i:i])\n-                    new_pieces.append(chunk)\n-                    lst_i = i + 1\n-                elif chunk.isdigit() and i > 0 and not piece[i - 1].isdigit():\n-                    if i > lst_i and piece[lst_i:i] != SPIECE_UNDERLINE:\n-                        new_pieces.append(piece[lst_i:i])\n-                    lst_i = i\n-                elif not chunk.isdigit() and i > 0 and piece[i - 1].isdigit():\n-                    if i > lst_i and piece[lst_i:i] != SPIECE_UNDERLINE:\n-                        new_pieces.append(piece[lst_i:i])\n-                    lst_i = i\n-            if len(piece) > lst_i:\n-                new_pieces.append(piece[lst_i:])\n-        return new_pieces\n-\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n-        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n-        return out_string\n-\n-    def convert_ids_to_string(self, ids):\n-        \"\"\"\n-        Converts a sequence of tokens (strings for sub-words) in a single string.\n-        \"\"\"\n-        tokens = self.convert_ids_to_tokens(ids)\n-        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n-        return out_string\n-\n-    # to mimic paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer functioning\n-    def _convert_token_to_id(self, token):\n-        return self.vocab.get(token, self.vocab.get(self.unk_token))\n-\n-    # to mimic paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer functioning\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.reverse_vocab.get(index, self.unk_token)\n-\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        r\"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. An ErnieM sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-        Returns:\n-            `list[int]`: List of input_id with the appropriate special tokens.\n-        \"\"\"\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        _cls = [self.cls_token_id]\n-        _sep = [self.sep_token_id]\n-        return _cls + token_ids_0 + _sep + _sep + token_ids_1 + _sep\n-\n-    def build_offset_mapping_with_special_tokens(self, offset_mapping_0, offset_mapping_1=None):\n-        r\"\"\"\n-        Build offset map from a pair of offset map by concatenating and adding offsets of special tokens. An Ernie-M\n-        offset_mapping has the following format:\n-\n-        - single sequence: `(0,0) X (0,0)`\n-        - pair of sequences: `(0,0) A (0,0) (0,0) B (0,0)`\n-\n-        Args:\n-            offset_mapping_ids_0 (`list[tuple]`):\n-                List of char offsets to which the special tokens will be added.\n-            offset_mapping_ids_1 (`list[tuple]`, *optional*):\n-                Optional second list of wordpiece offsets for offset mapping pairs.\n-        Returns:\n-            `list[tuple]`: List of wordpiece offsets with the appropriate offsets of special tokens.\n-        \"\"\"\n-        if offset_mapping_1 is None:\n-            return [(0, 0)] + offset_mapping_0 + [(0, 0)]\n-\n-        return [(0, 0)] + offset_mapping_0 + [(0, 0), (0, 0)] + offset_mapping_1 + [(0, 0)]\n-\n-    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n-        r\"\"\"\n-        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `encode` method.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of ids of the first sequence.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`str`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-        Returns:\n-            `list[int]`:\n-                The list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-\n-        if already_has_special_tokens:\n-            if token_ids_1 is not None:\n-                raise ValueError(\n-                    \"You should not supply a second sequence if the provided sequence of \"\n-                    \"ids is already formatted with special tokens for the model.\"\n-                )\n-            return [1 if x in [self.sep_token_id, self.cls_token_id] else 0 for x in token_ids_0]\n-\n-        if token_ids_1 is not None:\n-            return [1] + ([0] * len(token_ids_0)) + [1, 1] + ([0] * len(token_ids_1)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1]\n-\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Create the token type IDs corresponding to the sequences passed. [What are token type\n-        IDs?](../glossary#token-type-ids) Should be overridden in a subclass if the model has a special way of\n-        building: those.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                The first tokenized sequence.\n-            token_ids_1 (`list[int]`, *optional*):\n-                The second tokenized sequence.\n-        Returns:\n-            `list[int]`: The token type ids.\n-        \"\"\"\n-        # called when `add_special_tokens` is True, so align with `build_inputs_with_special_tokens` method\n-        if token_ids_1 is None:\n-            # [CLS] X [SEP]\n-            return (len(token_ids_0) + 2) * [0]\n-\n-        # [CLS] A [SEP] [SEP] B [SEP]\n-        return [0] * (len(token_ids_0) + 1) + [1] * (len(token_ids_1) + 3)\n-\n-    def is_ch_char(self, char):\n-        \"\"\"\n-        is_ch_char\n-        \"\"\"\n-        if \"\\u4e00\" <= char <= \"\\u9fff\":\n-            return True\n-        return False\n-\n-    def is_alpha(self, char):\n-        \"\"\"\n-        is_alpha\n-        \"\"\"\n-        if (\"a\" <= char <= \"z\") or (\"A\" <= char <= \"Z\"):\n-            return True\n-        return False\n-\n-    def is_punct(self, char):\n-        \"\"\"\n-        is_punct\n-        \"\"\"\n-        if char in \",;:.?!~ï¼Œï¼›ï¼šã€‚ï¼Ÿï¼ã€Šã€‹ã€ã€‘\":\n-            return True\n-        return False\n-\n-    def is_whitespace(self, char):\n-        \"\"\"\n-        is whitespace\n-        \"\"\"\n-        if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n-            return True\n-        if len(char) == 1:\n-            cat = unicodedata.category(char)\n-            if cat == \"Zs\":\n-                return True\n-        return False\n-\n-    def load_vocab(self, filepath):\n-        token_to_idx = {}\n-        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n-            for index, line in enumerate(f):\n-                token = line.rstrip(\"\\n\")\n-                token_to_idx[token] = int(index)\n-\n-        return token_to_idx\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        index = 0\n-        if os.path.isdir(save_directory):\n-            vocab_file = os.path.join(\n-                save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-            )\n-        else:\n-            vocab_file = (filename_prefix + \"-\" if filename_prefix else \"\") + save_directory\n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n-            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n-                if index != token_index:\n-                    logger.warning(\n-                        f\"Saving vocabulary to {vocab_file}: vocabulary indices are not consecutive.\"\n-                        \" Please check that the vocabulary is not corrupted!\"\n-                    )\n-                    index = token_index\n-                writer.write(token + \"\\n\")\n-                index += 1\n-\n-        tokenizer_model_file = os.path.join(save_directory, \"sentencepiece.bpe.model\")\n-        with open(tokenizer_model_file, \"wb\") as fi:\n-            content_spiece_model = self.sp_model.serialized_model_proto()\n-            fi.write(content_spiece_model)\n-\n-        return (vocab_file,)\n-\n-\n-__all__ = [\"ErnieMTokenizer\"]"
        },
        {
            "sha": "3c23b58f35012f1ddc00e2275c484810287de6f8",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,28 +0,0 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from typing import TYPE_CHECKING\n-\n-from ....utils import _LazyModule\n-from ....utils.import_utils import define_import_structure\n-\n-\n-if TYPE_CHECKING:\n-    from .configuration_gptsan_japanese import *\n-    from .modeling_gptsan_japanese import *\n-    from .tokenization_gptsan_japanese import *\n-else:\n-    import sys\n-\n-    _file = globals()[\"__file__\"]\n-    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "424a35b241dfc65404a6137b97010caa900f5490",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/configuration_gptsan_japanese.py",
            "status": "removed",
            "additions": 0,
            "deletions": 157,
            "changes": 157,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconfiguration_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconfiguration_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconfiguration_gptsan_japanese.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,157 +0,0 @@\n-# coding=utf-8\n-# Copyright 2023, HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"GPTSAN-japanese model configuration\"\"\"\n-\n-from ....configuration_utils import PreTrainedConfig\n-from ....utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class GPTSanJapaneseConfig(PreTrainedConfig):\n-    r\"\"\"\n-    This is the configuration class to store the configuration of a [`GPTSanJapaneseModel`]. It is used to instantiate\n-    a GPTSANJapanese model according to the specified arguments, defining the model architecture. Instantiating a\n-    configuration with the defaults will yield a similar configuration to that of the GPTSANJapanese\n-    [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese) architecture.\n-\n-    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PreTrainedConfig`] for more information.\n-\n-    Arguments:\n-        vocab_size (`int`, *optional*, defaults to 36000):\n-            Vocabulary size of the GPTSANJapanese model. Defines the number of different tokens that can be represented\n-            by the `inputs_ids` passed when calling [`GPTSanJapaneseModel`].\n-        max_position_embeddings (`int`, *optional*, defaults to 1280):\n-            The maximum sequence length that this model might ever be used with. Defaults set this to 1280.\n-        d_model (`int`, *optional*, defaults to 1024):\n-            Size of the encoder layers and the pooler layer.\n-        d_ff (`int`, *optional*, defaults to 8192):\n-            Size of the intermediate feed forward layer in each `SwitchTransformersBlock`.\n-        d_ext (`int`, *optional*, defaults to 4096):\n-            Size of the intermediate feed forward layer in each Extra-layers.\n-        d_spout (`int`, *optional*, defaults to 128):\n-            Size of the `spout` vector.\n-        num_switch_layers (`int`, *optional*, defaults to 10):\n-            Number of layers in the Switch Transformer layer.\n-        num_ext_layers (`int`, *optional*, defaults to 0):\n-            Number of layers in the Extra-layers.\n-        num_heads (`int`, *optional*, defaults to 16):\n-            Number of attention heads for each attention layer in the Transformer encoder.\n-        num_experts (`int`, *optional*, defaults to 16):\n-            Number of experts for each SwitchTransformer layer.\n-        expert_capacity (`int`, *optional*, defaults to 128):\n-            Number of tokens that can be stored in each expert. If set to 1, the model will behave like a regular\n-            Transformer.\n-        dropout_rate (`float`, *optional*, defaults to 0.0):\n-            The ratio for all dropout layers.\n-        layer_norm_eps (`float`, *optional*, defaults to 1e-5):\n-            The epsilon used by the layer normalization layers.\n-        router_bias (`bool`, *optional*, defaults to `False`):\n-            Whether to add a bias to the router.\n-        router_jitter_noise (`float`, *optional*, defaults to 0.0):\n-            Amount of noise to add to the router. Set it to 0.0 during prediction or set small value (usually 1e-2)\n-            during training.\n-        router_dtype (`str`, *optional*, default to `\"float32\"`):\n-            The `dtype` used for the routers. It is preferable to keep the `dtype` to `\"float32\"` as specified in the\n-            *selective precision* discussion in [the paper](https://huggingface.co/papers/2101.03961).\n-        router_ignore_padding_tokens (`bool`, *optional*, defaults to `False`):\n-            Whether to ignore padding tokens when routing.\n-        output_hidden_states (`bool`, *optional*, default to `False`):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        output_attentions (`bool`, *optional*, defaults to `False`):\n-            Whether or not to return the attentions tensors of all attention layers.\n-        initializer_factor (`float`, *optional*, defaults to 0.002):\n-            A factor for initializing all weight matrices.\n-        output_router_logits (`bool`, *optional*, default to `False`):\n-            Whether or not to return the router logits of all experts.\n-        use_cache (`bool`, *optional*, defaults to `True`):\n-            Whether or not the model should return the last key/values attentions (not used by all models)\n-    \"\"\"\n-\n-    model_type = \"gptsan-japanese\"\n-    keys_to_ignore_at_inference = [\n-        \"past_key_values\",\n-    ]\n-    attribute_map = {\n-        \"hidden_size\": \"d_model\",\n-        \"num_attention_heads\": \"num_heads\",\n-        \"num_hidden_layers\": \"num_layers\",\n-    }\n-\n-    def __init__(\n-        self,\n-        vocab_size=36000,\n-        max_position_embeddings=1280,\n-        d_model=1024,\n-        d_ff=8192,\n-        d_ext=4096,\n-        d_spout=128,\n-        num_switch_layers=10,\n-        num_ext_layers=0,\n-        num_heads=16,\n-        num_experts=16,\n-        expert_capacity=128,\n-        dropout_rate=0.0,\n-        layer_norm_epsilon=1e-5,\n-        router_bias=False,\n-        router_jitter_noise=0.0,\n-        router_dtype=\"float32\",\n-        router_ignore_padding_tokens=False,\n-        output_hidden_states=False,\n-        output_attentions=False,\n-        initializer_factor=0.002,\n-        output_router_logits=False,\n-        use_cache=True,\n-        separator_token_id=35998,\n-        pad_token_id=35995,\n-        eos_token_id=35999,\n-        **kwargs,\n-    ):\n-        self.vocab_size = vocab_size\n-        self.max_position_embeddings = max_position_embeddings\n-        self.d_model = d_model\n-        self.d_ff = d_ff\n-        self.d_ext = d_ext\n-        self.d_spout = d_spout\n-        self.num_switch_layers = num_switch_layers\n-        self.num_ext_layers = num_ext_layers\n-        self.num_layers = num_switch_layers + num_ext_layers\n-        self.num_heads = num_heads\n-        self.num_experts = num_experts\n-        self.expert_capacity = expert_capacity\n-        self.dropout_rate = dropout_rate\n-        self.layer_norm_epsilon = layer_norm_epsilon\n-        self.router_bias = router_bias\n-        self.router_jitter_noise = router_jitter_noise\n-        self.router_dtype = router_dtype\n-        self.router_ignore_padding_tokens = router_ignore_padding_tokens\n-        self.output_hidden_states = output_hidden_states\n-        self.output_attentions = output_attentions\n-        self.initializer_factor = initializer_factor\n-        self.output_router_logits = output_router_logits\n-        self.use_cache = use_cache\n-\n-        super().__init__(\n-            separator_token_id=separator_token_id,\n-            pad_token_id=pad_token_id,\n-            eos_token_id=eos_token_id,\n-            **kwargs,\n-        )\n-\n-\n-__all__ = [\"GPTSanJapaneseConfig\"]"
        },
        {
            "sha": "76b9c9cf328cdc61b9a3df893386b5be1874f920",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/convert_gptsan_tf_checkpoint_to_pytorch.py",
            "status": "removed",
            "additions": 0,
            "deletions": 181,
            "changes": 181,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconvert_gptsan_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconvert_gptsan_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconvert_gptsan_tf_checkpoint_to_pytorch.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,181 +0,0 @@\n-# coding=utf-8\n-# Copyright 2023 The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\"\"\"Convert GPTSANJapanese checkpoints from the original repository to pytorch model.\"\"\"\n-\n-import argparse\n-import json\n-import os\n-from collections import OrderedDict\n-\n-import numpy as np\n-import tensorflow as tf\n-import torch\n-\n-\n-def convert_tf_gptsan_to_pt(args):\n-    parameter_file = os.path.join(args.tf_model_dir, \"parameters.json\")\n-    params = json.loads(open(parameter_file).read())\n-    if not params:\n-        raise ValueError(\n-            f\"It seems that the json file at {parameter_file} is empty. Make sure you have a correct json file.\"\n-        )\n-    if not args.output.endswith(\".pt\"):\n-        args.output = args.output + \".pt\"\n-    new_state = OrderedDict()\n-    with tf.device(\"/CPU:0\"):\n-        reader = tf.train.load_checkpoint(args.tf_model_dir)\n-        shapes = reader.get_variable_to_shape_map()\n-        for key_name in shapes:\n-            vnp = reader.get_tensor(key_name).astype(np.float16)\n-            if key_name.endswith(\"/adam_m\") or key_name.endswith(\"/adam_v\"):\n-                continue\n-            if key_name.startswith(\"pasts/\"):\n-                if key_name.startswith(\"pasts/mlp\"):\n-                    player = int(key_name[9])\n-                elif key_name.startswith(\"pasts/out\"):\n-                    player = 8\n-                name = \"model.sqout.%d.weight\" % (player * 2)  # enter to nn.Sequential with Tanh, so 2 at a time\n-                state = vnp.transpose([1, 0]).copy()  # Mesh-Tensorflow is a diagonal matrix\n-                new_state[name] = torch.tensor(state)\n-            elif key_name.startswith(\"model/moe\"):\n-                player = int(key_name[9:].split(\"/\")[0])\n-                if key_name.endswith(\"/switch_gating/kernel\"):\n-                    name = \"model.blocks.%d.feed_forward.mlp.router.classifier.weight\" % player\n-                    state = vnp.transpose([1, 0]).copy()  # Mesh-Tensorflow is a diagonal matrix\n-                    new_state[name] = torch.tensor(state)\n-                elif key_name.endswith(\"/softmlp/kernel\"):\n-                    name = \"model.blocks.%d.feed_forward.soft_bypass_mlp.weight\" % player\n-                    state = vnp.transpose([1, 0]).copy()  # Mesh-Tensorflow is a diagonal matrix\n-                    new_state[name] = torch.tensor(state)\n-                elif key_name.endswith(\"/wo/kernel\") or key_name.endswith(\"/wi/kernel\"):\n-                    nlayer = key_name[-9:-7]\n-                    for i in range(16):\n-                        name = \"model.blocks.%d.feed_forward.mlp.experts.expert_%d.%s.weight\" % (player, i, nlayer)\n-                        state = (\n-                            vnp[i].transpose([1, 0]).copy()\n-                        )  # In Mesh-Tensorflow, it is one array, so it is divided\n-                        new_state[name] = torch.tensor(state)\n-            elif key_name.startswith(\"model/mlp\"):\n-                player = int(key_name[9:].split(\"/\")[0])\n-                if key_name.endswith(\"/p1/kernel\"):\n-                    name = \"model.blocks.%d.feed_forward.mlp.wi.weight\" % player\n-                    state = vnp.transpose([1, 0]).copy()  # Mesh-Tensorflow is a diagonal matrix\n-                    new_state[name] = torch.tensor(state)\n-                elif key_name.endswith(\"/p1/bias\"):\n-                    name = \"model.blocks.%d.feed_forward.mlp.wi.bias\" % player\n-                    state = vnp.copy()  # same because it is one dimensional\n-                    new_state[name] = torch.tensor(state)\n-                elif key_name.endswith(\"/p2/kernel\"):\n-                    name = \"model.blocks.%d.feed_forward.mlp.wo.weight\" % player\n-                    state = vnp.transpose([1, 0]).copy()  # Mesh-Tensorflow is a diagonal matrix\n-                    new_state[name] = torch.tensor(state)\n-                elif key_name.endswith(\"/p2/bias\"):\n-                    name = \"model.blocks.%d.feed_forward.mlp.wo.bias\" % player\n-                    state = vnp.copy()  # same because it is one dimensional\n-                    new_state[name] = torch.tensor(state)\n-            elif key_name.startswith(\"model/ln\"):\n-                player = int(key_name[8:].split(\"/\")[0])\n-                if key_name.endswith(\"/b\"):\n-                    name = \"model.blocks.%d.feed_forward.norm.bias\" % player\n-                    state = vnp.copy()  # same because it is one dimensional\n-                    new_state[name] = torch.tensor(state)\n-                elif key_name.endswith(\"/g\"):\n-                    name = \"model.blocks.%d.feed_forward.norm.weight\" % player\n-                    state = vnp.copy()  # same because it is one dimensional\n-                    new_state[name] = torch.tensor(state)\n-            elif key_name.startswith(\"model/att\"):\n-                player = int(key_name[9:].split(\"/\")[0])\n-                if key_name.endswith(\"/qkv/kernel\"):\n-                    state = vnp.copy()  # Compute same dimension as Mesh-tensorflow using einsum\n-                    state_q = state[:, 0, :, :]\n-                    state_k = state[:, 1, :, :]\n-                    state_v = state[:, 2, :, :]\n-                    state_q = (\n-                        state_q.reshape([state_q.shape[0], state_q.shape[1] * state_q.shape[2]])\n-                        .transpose([1, 0])\n-                        .copy()\n-                    )  # Mesh-Tensorflow is a diagonal matrix\n-                    state_k = (\n-                        state_k.reshape([state_k.shape[0], state_k.shape[1] * state_k.shape[2]])\n-                        .transpose([1, 0])\n-                        .copy()\n-                    )  # Mesh-Tensorflow is a diagonal matrix\n-                    state_v = (\n-                        state_v.reshape([state_v.shape[0], state_v.shape[1] * state_v.shape[2]])\n-                        .transpose([1, 0])\n-                        .copy()\n-                    )  # Mesh-Tensorflow is a diagonal matrix\n-                    name = \"model.blocks.%d.self_attn.self_attn.q_proj.weight\" % player\n-                    new_state[name] = torch.tensor(state_q)\n-                    name = \"model.blocks.%d.self_attn.self_attn.k_proj.weight\" % player\n-                    new_state[name] = torch.tensor(state_k)\n-                    name = \"model.blocks.%d.self_attn.self_attn.v_proj.weight\" % player\n-                    new_state[name] = torch.tensor(state_v)\n-                elif key_name.endswith(\"/o/kernel\"):\n-                    name = \"model.blocks.%d.self_attn.self_attn.out_proj.weight\" % player\n-                    state = (\n-                        vnp.reshape([vnp.shape[0] * vnp.shape[1], vnp.shape[2]]).transpose([1, 0]).copy()\n-                    )  # Mesh-Tensorflow is a diagonal matrix\n-                    new_state[name] = torch.tensor(state)\n-            elif key_name.startswith(\"model/an\"):\n-                player = int(key_name[8:].split(\"/\")[0])\n-                if key_name.endswith(\"/b\"):\n-                    name = \"model.blocks.%d.self_attn.norm.bias\" % player\n-                    state = vnp.copy()  # same because it is one dimensional\n-                    new_state[name] = torch.tensor(state)\n-                elif key_name.endswith(\"/g\"):\n-                    name = \"model.blocks.%d.self_attn.norm.weight\" % player\n-                    state = vnp.copy()  # same because it is one dimensional\n-                    new_state[name] = torch.tensor(state)\n-            elif (\n-                key_name.startswith(\"model/wte\")\n-                or key_name.startswith(\"model/wpe\")\n-                or key_name.startswith(\"model/ete\")\n-            ):\n-                nlayer = {\"wte\": \"embed_tokens\", \"wpe\": \"position_embeddings\", \"ete\": \"extra_position_embeddings\"}[\n-                    key_name[-3:]\n-                ]\n-                name = \"model.%s.weight\" % nlayer\n-                state = vnp.copy()  # same in embedded\n-                new_state[name] = torch.tensor(state)\n-                if key_name.startswith(\"model/wte\"):\n-                    name = \"lm_head.weight\"\n-                    state = vnp.copy()  # same in embedded\n-                    new_state[name] = torch.tensor(state)\n-            elif key_name.startswith(\"model/wob\"):\n-                name = \"final_logits_bias\"\n-                state = vnp.copy()  # same in embedded\n-                state = state.reshape((1, -1))\n-                new_state[name] = torch.tensor(state)\n-            elif key_name == \"model/dense/kernel\":\n-                name = \"model.last_project.weight\"\n-                state = vnp.transpose([1, 0]).copy()  # Mesh-Tensorflow is a diagonal matrix\n-                new_state[name] = torch.tensor(state)\n-            elif key_name == \"model/dense_1/bias\":\n-                name = \"model.last_project.bias\"\n-                state = vnp.copy()  # same because it is one dimensional\n-                new_state[name] = torch.tensor(state)\n-    torch.save(new_state, args.output)\n-\n-\n-if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser(\n-        description=\"model converter.\", formatter_class=argparse.ArgumentDefaultsHelpFormatter\n-    )\n-    parser.add_argument(\"--tf_model_dir\", metavar=\"PATH\", type=str, required=True, help=\"import model\")\n-    parser.add_argument(\"--output\", metavar=\"PATH\", type=str, required=True, help=\"output model\")\n-    args = parser.parse_args()\n-    convert_tf_gptsan_to_pt(args)"
        },
        {
            "sha": "291677c6caa4eab21c44de0604e3a5bd5e057093",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1071,
            "changes": 1071,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,1071 +0,0 @@\n-# coding=utf-8\n-# Copyright 2023 Toshiyuki Sakamoto(tanreinama) and HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from typing import Optional, Union\n-\n-import torch\n-import torch.nn as nn\n-\n-from .... import initialization as init\n-from ....activations import ACT2FN\n-from ....cache_utils import Cache\n-from ....modeling_outputs import MoECausalLMOutputWithPast, MoEModelOutputWithPastAndCrossAttentions\n-from ....modeling_utils import PreTrainedModel\n-from ....utils import DUMMY_INPUTS, DUMMY_MASK\n-from .configuration_gptsan_japanese import GPTSanJapaneseConfig\n-\n-\n-class GPTSanJapaneseDenseActDense(nn.Module):\n-    \"\"\"\n-    FFN Layer for Switch Transformer and Extra layers\n-\n-    GPTSAN can mix Switch Transformer layers and normal Transformer layers This class is used as Expert in Switch\n-    Transformer layers and as FFN in regular Transformer layers. RELU is used in the Switch Transformer layer, and\n-    Swish is used in the normal Transformer layer, so there is a choice of which is used in the argument.\n-\n-    \"\"\"\n-\n-    def __init__(self, config: GPTSanJapaneseConfig, ext_layer=False):\n-        super().__init__()\n-        d_inter = config.d_ext if ext_layer else config.d_ff\n-        self.wi = nn.Linear(config.d_model, d_inter, bias=ext_layer)\n-        self.wo = nn.Linear(d_inter, config.d_model, bias=ext_layer)\n-        self.dropout = nn.Identity() if ext_layer else nn.Dropout(config.dropout_rate)\n-        self.act = ACT2FN[\"swish\" if ext_layer else \"relu\"]\n-\n-    def forward(self, hidden_states):\n-        r\"\"\"\n-        Args:\n-            hidden_states (`torch.Tensor`) :\n-                [num_groups, tokens_per_group, hidden_dim] inputs to send to experts.\n-        Returns:\n-            torch.Tensor[num_groups, tokens_per_group, hidden_dim]\n-\n-        \"\"\"\n-        hidden_states = self.wi(hidden_states)\n-        hidden_states = self.act(hidden_states)\n-        hidden_states = self.dropout(hidden_states)\n-        hidden_states = self.wo(hidden_states)\n-        return hidden_states\n-\n-\n-class GPTSanJapaneseTop1Router(nn.Module):\n-    \"\"\"\n-    Router using tokens choose top-1 experts assignment.\n-\n-    This router uses the same mechanism as in Switch Transformer (https://huggingface.co/papers/2101.03961) and V-MoE\n-    (https://huggingface.co/papers/2106.05974): tokens choose their top experts. Items are sorted by router_probs and then\n-    routed to their choice of expert until the expert's expert_capacity is reached. **There is no guarantee that each\n-    token is processed by an expert**, or that each expert receives at least one token.\n-\n-    \"\"\"\n-\n-    def __init__(self, config: GPTSanJapaneseConfig):\n-        super().__init__()\n-        self.num_experts = config.num_experts\n-        self.expert_capacity = config.expert_capacity\n-        self.classifier = nn.Linear(config.hidden_size, self.num_experts, bias=config.router_bias)\n-        self.jitter_noise = config.router_jitter_noise\n-        self.ignore_padding_tokens = config.router_ignore_padding_tokens\n-        self.dtype = getattr(torch, config.router_dtype)\n-\n-    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n-        r\"\"\"\n-        Computes router probabilities from input hidden states.\n-\n-        Args:\n-            hidden_states (`torch.Tensor`):\n-                (batch_size, sequence_length, hidden_dim) from which router probabilities are computed.\n-        Returns:\n-            router_probabilities (`torch.Tensor`):\n-                Tensor of shape (batch_size, sequence_length, num_experts) corresponding to the probabilities for each\n-                token and expert. Used for routing tokens to experts.\n-            router_logits (`torch.Tensor`):\n-                Logits tensor of shape (batch_size, sequence_length, num_experts) corresponding to raw router logits.\n-                This is used later for computing router z-loss.\n-        \"\"\"\n-        # float32 is used to ensure stability. See the discussion of \"selective precision\" in\n-        # https://huggingface.co/papers/2101.03961.\n-        # We also store the previous dtype to cast back the output to the previous dtype\n-        self.input_dtype = hidden_states.dtype\n-        hidden_states = hidden_states.to(self.dtype)\n-        if self.training and self.jitter_noise > 0:\n-            # Multiply the token inputs by the uniform distribution - adding some noise\n-            hidden_states *= torch.empty_like(hidden_states).uniform_(1.0 - self.jitter_noise, 1.0 + self.jitter_noise)\n-        router_logits = self.classifier(hidden_states)\n-\n-        # Apply Softmax and cast back to the original `dtype`\n-        router_probs = nn.functional.softmax(router_logits, dim=-1, dtype=self.dtype).to(self.input_dtype)\n-        router_logits, expert_index = torch.max(router_probs, dim=-1, keepdim=True)\n-        expert_index = torch.nn.functional.one_hot(expert_index, num_classes=self.num_experts)\n-        token_priority = torch.cumsum(expert_index, dim=-2)\n-        # mask if the token routed to to the expert will overflow\n-        expert_capacity_mask = token_priority <= self.expert_capacity\n-        expert_index = expert_index * expert_capacity_mask\n-        router_probs = torch.max(router_probs, dim=-1).values.unsqueeze(-1)\n-        return router_probs, expert_index, router_logits\n-\n-\n-class GPTSanExperts(nn.ModuleDict):\n-    def __init__(self, config: GPTSanJapaneseConfig):\n-        super().__init__()\n-        for idx in range(config.num_experts):\n-            self.add_module(f\"expert_{idx}\", GPTSanJapaneseDenseActDense(config))\n-\n-\n-class GPTSanJapaneseSparseMLP(nn.Module):\n-    def __init__(self, config: GPTSanJapaneseConfig):\n-        super().__init__()\n-        self.router = GPTSanJapaneseTop1Router(config)\n-        self.experts = GPTSanExperts(config)\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        batch_size, sequence_length, hidden_dim = hidden_states.shape\n-        hidden_states = hidden_states.view(-1, hidden_dim)\n-        _, selected_experts, routing_weights = self.router(hidden_states)\n-        hidden_states = self.experts(hidden_states, selected_experts, routing_weights)\n-        hidden_states = hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n-        return hidden_states\n-\n-\n-class GPTSanJapaneseLayerSparseFF(nn.Module):\n-    r\"\"\"\n-    Switch Transformers Feed Forward layer module. This is a wrapper around the Mixture of Experts module.\n-\n-    Parameters:\n-        config : ([`GPTSanJapaneseConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-    \"\"\"\n-\n-    def __init__(self, config: GPTSanJapaneseConfig):\n-        super().__init__()\n-        self.mlp = GPTSanJapaneseSparseMLP(config)\n-        self.soft_bypass_mlp = nn.Linear(config.d_model, config.d_model, bias=False)\n-        self.norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n-\n-    def forward(self, hidden_states, output_router_logits):\n-        r\"\"\"\n-        Args:\n-            hidden_states (`torch.Tensor`) :\n-                [num_groups, tokens_per_group, hidden_dim] inputs to send to experts.\n-            output_router_logits (`bool`) :\n-                output experts router output.\n-        Returns:\n-            torch.Tensor[num_groups, tokens_per_group, hidden_dim]\n-\n-        \"\"\"\n-        forwarded_states, router_tuple = self.mlp(hidden_states)\n-        forwarded_states += torch.tanh(self.soft_bypass_mlp(hidden_states))\n-        output = hidden_states + self.norm(forwarded_states)\n-\n-        if output_router_logits and router_tuple is not None:\n-            return output, router_tuple\n-        else:\n-            return output\n-\n-\n-class GPTSanJapaneseLayerDenseFF(nn.Module):\n-    r\"\"\"\n-    Extra Transformers Feed Forward layer module.\n-\n-    Parameters:\n-        config : ([`GPTSanJapaneseConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-    \"\"\"\n-\n-    def __init__(self, config: GPTSanJapaneseConfig):\n-        super().__init__()\n-        # Check if it is a sparse layer, if not then it is a dense layer\n-        self.mlp = GPTSanJapaneseDenseActDense(config, ext_layer=True)\n-        self.norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n-\n-    def forward(self, hidden_states):\n-        r\"\"\"\n-        Args:\n-            hidden_states (`torch.Tensor`) :\n-                [num_groups, tokens_per_group, hidden_dim] inputs to send to experts.\n-        Returns:\n-            torch.Tensor[num_groups, tokens_per_group, hidden_dim]\n-\n-        \"\"\"\n-        forwarded_states = self.mlp(hidden_states)\n-        output = hidden_states + self.norm(forwarded_states)\n-        return output\n-\n-\n-class GPTSanJapaneseAttention(nn.Module):\n-    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n-\n-    def __init__(\n-        self,\n-        embed_dim: int,\n-        num_heads: int,\n-        dropout: float = 0.0,\n-        is_decoder: bool = False,\n-        bias: bool = True,\n-        is_causal: bool = False,\n-        config: Optional[GPTSanJapaneseConfig] = None,\n-    ):\n-        super().__init__()\n-        self.embed_dim = embed_dim\n-        self.num_heads = num_heads\n-        self.dropout = dropout\n-        self.head_dim = embed_dim // num_heads\n-        self.config = config\n-\n-        if (self.head_dim * num_heads) != self.embed_dim:\n-            raise ValueError(\n-                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n-                f\" and `num_heads`: {num_heads}).\"\n-            )\n-        self.scaling = self.head_dim**-0.5\n-        self.is_decoder = is_decoder\n-        self.is_causal = is_causal\n-\n-        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n-        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n-        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n-        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n-\n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_values[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_values` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_values is not None\n-            and past_key_values[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_values[0]\n-            value_states = past_key_values[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_values is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_values[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_values[1], value_states], dim=2)\n-        else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_values` is always `None`\n-            past_key_values = (key_states, value_states)\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.reshape(*proj_shape)\n-        value_states = value_states.reshape(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped, past_key_values\n-\n-\n-class GPTSanJapaneseLayerSelfAttention(nn.Module):\n-    \"\"\"\n-    Self Attention and Normalization Unit\n-    \"\"\"\n-\n-    def __init__(self, config, has_relative_attention_bias=False):\n-        super().__init__()\n-        self.self_attn = GPTSanJapaneseAttention(\n-            embed_dim=config.d_model,\n-            num_heads=config.num_heads,\n-            is_decoder=True,\n-            bias=has_relative_attention_bias,\n-        )\n-        self.norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n-\n-    def forward(\n-        self,\n-        hidden_states: Optional[tuple[torch.FloatTensor]],\n-        past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = False,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[Union[torch.Tensor, tuple[torch.Tensor]], ...]:\n-        r\"\"\"\n-        Self-attention and normalize block.\n-\n-        Args:\n-            hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n-                if the model is configured as a decoder.\n-            past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-                Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up\n-                decoding. If `past_key_values` are used, the user can optionally input only the last\n-                `decoder_input_ids` (those that don't have their past key value states given to this model) of shape\n-                `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-            attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used\n-                in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        Returns:\n-            tuple[torch.Tensor[num_groups, tokens_per_group, hidden_dim],...]\n-        \"\"\"\n-        # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_values = past_key_values[:2] if past_key_values is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        atten_out = self.self_attn(\n-            hidden_states=hidden_states,\n-            past_key_values=self_attn_past_key_values,\n-            attention_mask=(1 - attention_mask) * torch.finfo(hidden_states.dtype).min,\n-            output_attentions=output_attentions,\n-        )\n-        if output_attentions:\n-            attn_weights = (atten_out[1],)\n-        else:\n-            attn_weights = ()\n-\n-        attention_output = atten_out[0]\n-\n-        hidden = hidden_states + self.norm(attention_output)\n-\n-        if use_cache:\n-            outputs = (hidden, atten_out[2])  # hidden, present, (attentions)\n-        else:\n-            outputs = (hidden,)  # hidden, (attentions)\n-\n-        return outputs + attn_weights\n-\n-\n-class GPTSanJapaneseBlock(nn.Module):\n-    \"\"\"\n-    Self Attention and FFN Unit\n-    \"\"\"\n-\n-    def __init__(self, config, ext_layer=False):\n-        super().__init__()\n-        self.self_attn = GPTSanJapaneseLayerSelfAttention(config)\n-        self.feed_forward = GPTSanJapaneseLayerDenseFF(config) if ext_layer else GPTSanJapaneseLayerSparseFF(config)\n-\n-    def forward(\n-        self,\n-        hidden_states: Optional[tuple[torch.FloatTensor]],\n-        past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = False,\n-        output_attentions: Optional[bool] = False,\n-        output_router_tuple: Optional[bool] = False,\n-    ) -> tuple[Union[torch.Tensor, tuple[torch.Tensor]], ...]:\n-        r\"\"\"\n-        GPTSAN transformer block.\n-\n-        Args:\n-            hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n-                if the model is configured as a decoder.\n-            past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-                Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up\n-                decoding. If `past_key_values` are used, the user can optionally input only the last\n-                `decoder_input_ids` (those that don't have their past key value states given to this model) of shape\n-                `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-            attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used\n-                in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            output_attentions (`bool`) :\n-                output attention probabirities.\n-            output_router_tuple:\n-                output experts router logits and expert id.\n-        Returns:\n-            tuple[torch.Tensor[num_groups, tokens_per_group, hidden_dim],...]\n-        \"\"\"\n-        atten_out = self.self_attn(\n-            hidden_states=hidden_states,\n-            past_key_values=past_key_values,\n-            attention_mask=attention_mask,\n-            use_cache=use_cache,\n-            output_attentions=output_attentions,\n-        )\n-        attention_output = atten_out[0]\n-\n-        if isinstance(self.feed_forward, GPTSanJapaneseLayerSparseFF):\n-            sparse_out = self.feed_forward(attention_output, output_router_tuple)\n-            if output_router_tuple:\n-                hidden, router_tuple = sparse_out\n-            else:\n-                hidden = sparse_out\n-        else:\n-            hidden = self.feed_forward(attention_output)\n-\n-        outputs = (hidden,) + atten_out[1:]\n-\n-        if isinstance(self.feed_forward, GPTSanJapaneseLayerSparseFF) and output_router_tuple:\n-            outputs += (router_tuple,)\n-\n-        return outputs\n-\n-\n-class GPTSanJapanesePreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config: GPTSanJapaneseConfig\n-    base_model_prefix = \"gptsan_japanese\"\n-    supports_gradient_checkpointing = False\n-    _no_split_modules = [\"GPTSanJapaneseBlock\"]\n-    _skip_keys_device_placement = \"past_key_values\"\n-\n-    @property\n-    def dummy_inputs(self):\n-        input_ids = torch.tensor(DUMMY_INPUTS)\n-        input_mask = torch.tensor(DUMMY_MASK)\n-        dummy_inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-        }\n-        return dummy_inputs\n-\n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        factor = self.config.initializer_factor  # Used for testing weights initialization\n-        if isinstance(module, nn.LayerNorm):\n-            init.constant_(module.weight, factor * 1.0)\n-            init.zeros_(module.bias)\n-        elif isinstance(module, nn.Linear):\n-            init.normal_(module.weight, mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n-            if hasattr(module, \"bias\") and module.bias is not None:\n-                init.zeros_(module.bias)\n-        elif isinstance(module, nn.Embedding):\n-            init.normal_(module.weight, mean=0.0, std=factor * 1.0)\n-        elif isinstance(module, GPTSanJapaneseModel):\n-            # Mesh TensorFlow embeddings initialization\n-            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\n-            init.normal_(module.embed_tokens.weight, mean=0.0, std=factor * 1.0)\n-            init.normal_(module.position_embeddings.weight, mean=0.0, std=factor * 1.0)\n-            if hasattr(module, \"extra_position_embeddings\") and module.extra_position_embeddings is not None:\n-                init.normal_(module.extra_position_embeddings.weight, mean=0.0, std=factor * 1.0)\n-        elif isinstance(module, (GPTSanJapaneseModel, GPTSanJapaneseForConditionalGeneration)):\n-            # Mesh TensorFlow embeddings initialization\n-            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\n-            init.normal_(module.final_logits_bias, mean=0.0, std=factor * 1.0)\n-            if hasattr(module, \"lm_head\") and not self.config.tie_word_embeddings:\n-                init.normal_(module.lm_head.weight, mean=0.0, std=factor * 1.0)\n-        elif isinstance(module, GPTSanJapaneseDenseActDense):\n-            # Mesh TensorFlow FF initialization\n-            # See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56\n-            # and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89\n-            init.normal_(module.wi.weight, mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n-            if hasattr(module.wi, \"bias\") and module.wi.bias is not None:\n-                init.zeros_(module.wi.bias)\n-            init.normal_(module.wo.weight, mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n-            if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n-                init.zeros_(module.wo.bias)\n-        elif isinstance(module, GPTSanJapaneseAttention):\n-            # Multi-headed attention\n-            d_model = self.config.d_model\n-            key_value_proj_dim = self.config.d_model\n-            n_heads = self.config.num_heads\n-            init.normal_(module.k_proj.weight, mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n-            init.normal_(module.v_proj.weight, mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n-            init.normal_(module.q_proj.weight, mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n-            init.normal_(module.out_proj.weight, mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5))\n-        elif isinstance(module, GPTSanJapaneseSparseMLP):\n-            # Mesh TensorFlow attention initialization to avoid scaling before softmax\n-            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136\n-            d_model = self.config.d_model\n-            key_value_proj_dim = self.config.d_model\n-            n_heads = self.config.num_heads\n-            init.normal_(module.router.classifier.weight, mean=0.0, std=factor * 1)\n-            for idx in range(self.config.num_experts):\n-                init.normal_(module.experts[f\"expert_{idx}\"].wi.weight, mean=0.0, std=factor * (d_model**-0.5))\n-                init.normal_(module.experts[f\"expert_{idx}\"].wo.weight, mean=0.0, std=factor * (d_model**-0.5))\n-\n-    def _shift_right(self, input_ids):\n-        decoder_start_token_id = self.config.decoder_start_token_id\n-        pad_token_id = self.config.pad_token_id\n-\n-        if decoder_start_token_id is None:\n-            raise ValueError(\n-                \"self.model.config.decoder_start_token_id has to be defined. In T5 it is usually set to the pad_token_id. \"\n-                \"See T5 docs for more information.\"\n-            )\n-\n-        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n-        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n-        shifted_input_ids[..., 0] = decoder_start_token_id\n-\n-        if pad_token_id is None:\n-            raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n-        # replace possible -100 values in labels by `pad_token_id`\n-        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n-\n-        return shifted_input_ids\n-\n-\n-class GPTSanJapaneseModel(GPTSanJapanesePreTrainedModel):\n-    def __init__(self, config: GPTSanJapaneseConfig):\n-        super().__init__(config)\n-        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.d_model)\n-        self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model)\n-        self.last_project = nn.Linear(config.d_model, config.d_model, bias=True)\n-        self.act = ACT2FN[\"swish\"]\n-\n-        self.blocks = torch.nn.ModuleList([])\n-        for _ in range(config.num_switch_layers):\n-            self.blocks.append(GPTSanJapaneseBlock(config))\n-        for _ in range(config.num_ext_layers):\n-            self.blocks.append(GPTSanJapaneseBlock(config, ext_layer=True))\n-\n-        if config.num_ext_layers > 0:\n-            self.extra_position_embeddings = nn.Embedding(config.max_position_embeddings, config.d_model)\n-\n-        if config.d_spout:\n-            spouts = []\n-            for _ in range(8):\n-                spouts.append(nn.Linear(config.d_spout, config.d_spout, bias=False))\n-                spouts.append(nn.Tanh())\n-            spouts.append(nn.Linear(config.d_spout, config.num_layers * 2 * config.d_model, bias=False))\n-            self.spout = nn.Sequential(*spouts)\n-\n-        self.post_init()\n-\n-    def set_input_embeddings(self, new_embeddings):\n-        self.embed_tokens = new_embeddings\n-\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.FloatTensor] = None,\n-        spout: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = False,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n-        num_precontext: Optional[torch.LongTensor] = None,\n-    ) -> Union[MoEModelOutputWithPastAndCrossAttentions, tuple[torch.FloatTensor]]:\n-        r\"\"\"\n-        num_precontext (`torch.LongTensor` of shape `(batch_size,1)`):\n-            length of `hybrid` input tokens in the input. Tokens up to this length refer to both front and back like\n-            BERT, tokens after that refer only to front like GPT. see also:\n-            https://github.com/tanreinama/GPTSAN/blob/main/report/model.md\n-\n-        Returns:\n-            `MoEModelOutputWithPastAndCrossAttentions` or `tuple` if `return_dict` returns\n-            MoEModelOutputWithPastAndCrossAttentions instead of tuple\n-        \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        device = self.position_embeddings.weight.device\n-        if input_ids is None:\n-            input_ids = torch.zeros([1, 1]).int().to(device)  # dummy for input_ids was None\n-        if inputs_embeds is not None:\n-            raise NotImplementedError(\n-                \"GPTSanJapaneseModel does not use `inputs_embeds`. Make sure to pass in `input_ids` instead.\"\n-            )\n-        num_pasts_contexts = 0\n-        num_batch = input_ids.shape[0]\n-        pasts_or_spout_value = None\n-        if past_key_values is not None:\n-            num_pasts_contexts = past_key_values.get_seq_length()\n-        elif self.config.d_spout and spout is not None:\n-            # `spout` is a special input vector specific to GPTSAN\n-            # This controls the output by projecting embedded information such as the class of sentences during learning.\n-            # It should passed instead of the first past_key_values.\n-            # See the original GPTSAN repository for details\n-            num_pasts_contexts += 1\n-\n-        # If there is an attention_mask, increase first one for spout\n-        if self.config.d_spout and spout is not None and attention_mask is not None:\n-            attention_mask_with_spout = torch.ones(num_batch, attention_mask.shape[1] + 1, device=device)\n-            attention_mask_with_spout[:, 1:] -= 1 - attention_mask  # 1st token should be spout\n-            attention_mask = attention_mask_with_spout  # update attention_mask\n-\n-        if num_precontext is not None:\n-            # `num_precontext` is the number of tokens that refer to each other in prefix-lm\n-            # created per batch, so dimension of num_precontext should be [batch, 1]\n-            if not (\n-                len(num_precontext.shape) == 2 and num_precontext.shape[1] == 1\n-            ):  # num_precontext Should be [batch,1]\n-                raise ValueError(\"num_precontext should be [batch, 1] size.\")\n-            num_precontext = torch.reshape(num_precontext, [-1])\n-        else:\n-            num_precontext = torch.zeros([num_batch]).int().to(device)\n-\n-        num_input_contexts = input_ids.shape[1]\n-        num_output_contexts = num_input_contexts + num_pasts_contexts\n-\n-        hidden_states = self.embed_tokens(input_ids)\n-\n-        if past_key_values is not None:\n-            pasts_or_spout_value = past_key_values\n-        elif self.config.d_spout and spout is not None:\n-            # Make vector from `spout` of GPTSAN to the same shape as past_key_values\n-            pasts_or_spout_value = self.spout(spout)  # projecting `spout` vector\n-            pasts_or_spout_value = torch.reshape(\n-                pasts_or_spout_value,\n-                [\n-                    num_batch,\n-                    self.config.num_layers,\n-                    2,\n-                    self.config.num_heads,\n-                    num_pasts_contexts,\n-                    self.config.d_model // self.config.num_heads,\n-                ],\n-            )\n-            pasts_or_spout_value = torch.split(pasts_or_spout_value, [1] * self.config.num_layers, dim=1)\n-            # make same shape as past_key_values\n-            pasts_or_spout_value = tuple(\n-                tuple(b.squeeze(1) for b in torch.split(a.squeeze(1), [1, 1], dim=1)) for a in pasts_or_spout_value\n-            )\n-        else:\n-            pasts_or_spout_value = [None] * self.config.num_layers\n-\n-        # Token position considering spout and pasts\n-        token_position = torch.arange(num_input_contexts).to(device) + num_pasts_contexts\n-\n-        if attention_mask is None:\n-            attention_mask = torch.ones(num_batch, num_input_contexts, device=device)\n-\n-        # positions for get position_embeddings\n-        gather_position = (\n-            (\n-                torch.zeros((num_batch, self.config.d_model, num_input_contexts)).to(device)\n-                + token_position.unsqueeze(0)\n-            )\n-            .transpose(1, 2)\n-            .long()\n-        )\n-        # When padding with padding_side=\"left\", zeros line up on the left side of attention_mask, so position_embeddings is shifted accordingly\n-        gather_position -= (1 - attention_mask).argmin(dim=-1).unsqueeze(1).unsqueeze(2)\n-        gather_position = torch.clip(gather_position, num_pasts_contexts, self.config.max_position_embeddings - 1)\n-\n-        # attention_mask is applied per batch\n-        for i in range(num_batch):\n-            hidden_states[i] += torch.gather(self.position_embeddings.weight, dim=0, index=gather_position[i])\n-\n-        # Create a mask to be used when making the prefix Input length of Prefix-LM variable\n-        causal_mask = (\n-            torch.tril(torch.ones((num_output_contexts, num_output_contexts), dtype=torch.uint8))\n-            .view(1, 1, num_output_contexts, num_output_contexts)\n-            .to(device)\n-        )\n-        prefix_lm_mask = causal_mask[:, :, -num_input_contexts:, :]\n-        if token_type_ids is not None:\n-            token_type_ids = token_type_ids.unsqueeze(1).unsqueeze(2)\n-            prefix_lm_mask = ((prefix_lm_mask + token_type_ids) > 0).float()\n-        # Merge prefix_lm_mask and attention_mask\n-        extended_attention_mask = prefix_lm_mask * attention_mask.unsqueeze(1).unsqueeze(2)\n-\n-        for layer, past in enumerate(pasts_or_spout_value):\n-            if layer == self.config.num_switch_layers:\n-                if self.config.num_ext_layers > 0:\n-                    # extra_position_embeddings are extra position embeddings that are only created when extending the model with code from the original GPTSAN repository. Not used in the default model.\n-                    # However, it is created when you create an additional layer and partially train only that location.\n-                    # Therefore, convert_gptsan_tf_checkpoint_to_pytorch.py is used when converting and loading models created in the original GPTSAN repository.\n-                    for i in range(num_batch):\n-                        hidden_states[i] += torch.gather(\n-                            self.extra_position_embeddings.weight, dim=0, index=gather_position[i]\n-                        )\n-            hidden_states = self.blocks[layer](\n-                hidden_states=hidden_states,\n-                past_key_values=past,\n-                attention_mask=extended_attention_mask,\n-            )\n-\n-        hidden_states = self.last_project(hidden_states)\n-        hidden_states = self.act(hidden_states)\n-\n-        return MoEModelOutputWithPastAndCrossAttentions(last_hidden_state=hidden_states)\n-\n-\n-####################################################\n-# This dict contains ids and associated url\n-# for the pretrained weights provided with the models\n-####################################################\n-\n-\n-# TODO import from switch\n-def router_z_loss_func(router_logits: torch.Tensor) -> float:\n-    r\"\"\"\n-    Compute the router z-loss implemented in PyTorch.\n-\n-    The router z-loss was introduced in [Designing Effective Sparse Expert Models](https://huggingface.co/papers/2202.08906).\n-    It encourages router logits to remain small in an effort to improve stability.\n-\n-    Args:\n-        router_logits (`float`):\n-            Input logits of shape [batch_size, sequence_length, num_experts]\n-\n-    Returns:\n-        Scalar router z-loss.\n-    \"\"\"\n-    num_groups, tokens_per_group, _ = router_logits.shape\n-    log_z = torch.logsumexp(router_logits, dim=-1)\n-    z_loss = log_z**2\n-    return torch.sum(z_loss) / (num_groups * tokens_per_group)\n-\n-\n-def load_balancing_loss_func(router_probs: torch.Tensor, expert_indices: torch.Tensor) -> float:\n-    r\"\"\"\n-    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n-\n-    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n-    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n-    experts is too unbalanced.\n-\n-    Args:\n-        router_probs (`torch.Tensor`):\n-            Probability assigned to each expert per token. Shape: [batch_size, sequence_length, num_experts].\n-        expert_indices (`torch.Tensor`):\n-            Indices tensor of shape [batch_size, sequence_length] identifying the selected expert for a given token.\n-\n-    Returns:\n-        The auxiliary loss.\n-    \"\"\"\n-    num_experts = router_probs.shape[-1]\n-\n-    # cast the expert indices to int64, otherwise one-hot encoding will fail\n-    if expert_indices.dtype != torch.int64:\n-        expert_indices = expert_indices.to(torch.int64)\n-\n-    if len(expert_indices.shape) == 2:\n-        expert_indices = expert_indices.unsqueeze(2)\n-\n-    expert_mask = torch.nn.functional.one_hot(expert_indices, num_experts)\n-\n-    # For a given token, determine if it was routed to a given expert.\n-    expert_mask = torch.max(expert_mask, axis=-2).values\n-\n-    # cast to float32 otherwise mean will fail\n-    expert_mask = expert_mask.to(torch.float32)\n-    tokens_per_group_and_expert = torch.mean(expert_mask, axis=-2)\n-\n-    router_prob_per_group_and_expert = torch.mean(router_probs, axis=-2)\n-    return torch.mean(tokens_per_group_and_expert * router_prob_per_group_and_expert) * (num_experts**2)\n-\n-\n-class GPTSanJapaneseForConditionalGeneration(GPTSanJapanesePreTrainedModel):\n-    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n-\n-    def __init__(self, config: GPTSanJapaneseConfig):\n-        super().__init__(config)\n-        self.model = GPTSanJapaneseModel(config)\n-        self.register_buffer(\"final_logits_bias\", torch.zeros([1, config.vocab_size]))\n-        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n-        self.lm_head.weight = self.model.embed_tokens.weight\n-\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.FloatTensor] = None,\n-        spout: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = False,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-    ) -> Union[tuple[torch.FloatTensor], MoECausalLMOutputWithPast]:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification loss. Indices should be in `[-100, 0, ...,\n-            config.vocab_size - 1]`. All labels set to `-100` are ignored (masked), the loss is only computed for\n-            labels in `[0, ..., config.vocab_size]`\n-\n-        Returns:\n-            `MoECausalLMOutputWithPast` or `tuple` if `return_dict` returns MoECausalLMOutputWithPast instead of tuple\n-\n-        Example:\n-\n-        Text Generation with regular LM Model\n-        ```python\n-        >>> from transformers import AutoModel, AutoTokenizer, trainer_utils\n-\n-        >>> device = \"cuda\"\n-        >>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").to(device)\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n-        >>> x_token = tokenizer(\"ç¹”ç”°ä¿¡é•·ã¯ã€\", return_tensors=\"pt\")\n-        >>> trainer_utils.set_seed(30)\n-        >>> input_ids = x_token.input_ids.to(device)\n-        >>> gen_token = model.generate(input_ids, max_new_tokens=50)\n-        >>> tokenizer.decode(gen_token[0])\n-        \"ç¹”ç”°ä¿¡é•·ã¯ã€æ”¿æ²»ãƒ»è»äº‹ã®ä¸­æž¢ã¾ã§æŽŒæ¡ã—ãŸæ”¿æ²»å®¶ã§ã‚ã‚Šã€æ—¥æœ¬å²ä¸Šé¡žã‚’è¦‹ãªã„é©šç•°çš„ãªè»äº‹ä¾µæ”»ã‚’ç¶šã‘...\"\n-        ```\n-\n-        Text Generation with Prefix-LM Model\n-        ```python\n-        >>> from transformers import AutoModel, AutoTokenizer, trainer_utils\n-\n-        >>> device = \"cuda\"\n-        >>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").to(device)\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n-        >>> x_token = tokenizer(\"\", prefix_text=\"ç¹”ç”°ä¿¡é•·ã¯ã€\", return_tensors=\"pt\")\n-        >>> trainer_utils.set_seed(30)\n-        >>> input_ids = x_token.input_ids.to(device)\n-        >>> token_type_ids = x_token.token_type_ids.to(device)\n-        >>> gen_token = model.generate(input_ids, token_type_ids=token_type_ids, max_new_tokens=50)\n-        >>> tokenizer.decode(gen_token[0])\n-        \"ç¹”ç”°ä¿¡é•·ã¯ã€æ”¿æ²»ãƒ»å¤–äº¤ã§æ•°ã€…ã®æˆ¦æžœã‚’ä¸Šã’ã‚‹ãŒã€1568å¹´ã‹ã‚‰ã¯ã€ã„ã‚ã‚†ã‚‹æœ¬èƒ½å¯ºã®å¤‰ã§ç´°å·æ™´å…ƒã«æš—æ®ºã•ã‚Œã‚‹...\"\n-        ```\n-\n-        Simultaneously Text Generation And Masked Language Model\n-        ```python\n-        >>> from transformers import AutoModel, AutoTokenizer, trainer_utils\n-\n-        >>> device = \"cuda\"\n-        >>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").to(device)\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n-        >>> masked_sentence = \"æ­¦ç”°ä¿¡çŽ„ã¯ã€<|inputmask|>æ™‚ä»£ãƒ•ã‚¡ãƒ³ãªã‚‰ãœã²æŠ¼ã•ãˆ<|inputmask|>ããŸã„åå°†ã®ä¸€äººã€‚\"\n-        >>> x_token = tokenizer(\"\", prefix_text=masked_sentence, return_tensors=\"pt\")\n-        >>> trainer_utils.set_seed(30)\n-        >>> input_ids = x_token.input_ids.to(device)\n-        >>> token_type_ids = x_token.token_type_ids.to(device)\n-        >>> out_lm_token = model.generate(input_ids, token_type_ids=token_type_ids, max_new_tokens=50)\n-        >>> out_mlm_token = model(input_ids, token_type_ids=token_type_ids).logits.argmax(axis=-1)\n-        >>> tokenizer.decode(out_mlm_token[0])\n-        \"æ­¦ç”°ä¿¡çŽ„ã¯ã€æˆ¦å›½æ™‚ä»£ãƒ•ã‚¡ãƒ³ãªã‚‰ãœã²æŠ¼ã•ãˆã¦ãŠããŸã„åå°†ã®ä¸€äººã€‚\"\n-\n-        >>> tokenizer.decode(out_lm_token[0][input_ids.shape[1] :])\n-        \"æ­¦ç”°æ°ã®ä¸‰ä»£ã«æ¸¡ã£ãŸæ­¦ç”°å®¶ã®ã²ã¨ã‚Š\\nç”²æ–å¸‚ã«ä½ã‚€ã€æ—¥æœ¬å²ä¸Šæœ€å¤§ã®æˆ¦å›½å¤§åã€‚...\"\n-        ```\"\"\"\n-        SEG_TOKEN = self.config.separator_token_id\n-        use_cache = use_cache or self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        model_return_dict = True\n-        num_precontext = None\n-        if input_ids is not None:\n-            num_batch = input_ids.shape[0]\n-            num_precontext = torch.zeros([num_batch]).int().to(input_ids.device)\n-            where_separators = torch.where(input_ids == SEG_TOKEN)\n-            num_precontext[where_separators[0]] += where_separators[1]\n-            num_precontext = num_precontext.unsqueeze(1)\n-\n-        outputs = self.model(\n-            input_ids,\n-            attention_mask,\n-            token_type_ids,\n-            spout,\n-            past_key_values,\n-            use_cache,\n-            inputs_embeds,\n-            decoder_inputs_embeds,\n-            output_attentions,\n-            output_hidden_states,\n-            model_return_dict,\n-            output_router_logits,\n-            num_precontext,\n-        )\n-\n-        lm_logits = self.lm_head(outputs[0])\n-        if lm_logits.shape[-1] == self.final_logits_bias.shape[-1]:\n-            lm_logits = lm_logits + self.final_logits_bias\n-\n-        loss = None\n-        z_loss = None\n-        router_probs = None\n-        aux_loss = None\n-        if labels is not None:\n-            # move labels to correct device to enable model parallelism\n-            labels = labels.to(lm_logits.device)\n-\n-            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n-\n-            if output_router_logits:\n-                # Compute the router loss (z_loss + auxiliary loss) for each router in the encoder and decoder\n-                router_logits, expert_indexes = self._unpack_router_logits(outputs.router_probs)\n-                z_loss = router_z_loss_func(router_logits)\n-                router_probs = nn.Softmax(dim=-1)(router_logits)\n-                aux_loss = load_balancing_loss_func(router_probs, expert_indexes)\n-\n-            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n-\n-        return MoECausalLMOutputWithPast(\n-            loss=loss,\n-            logits=lm_logits,\n-            past_key_values=outputs.past_key_values,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-            router_logits=outputs.router_probs,\n-            z_loss=z_loss,\n-            aux_loss=aux_loss,\n-        )\n-\n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids: torch.LongTensor,\n-        attention_mask: torch.FloatTensor,\n-        token_type_ids: Optional[torch.FloatTensor] = None,\n-        spout: Optional[Union[list, torch.FloatTensor]] = None,\n-        past_key_values: Optional[Cache] = None,\n-        **kwargs,\n-    ):\n-        if isinstance(spout, list):\n-            spout = torch.tensor(spout).float()\n-            if input_ids is not None:\n-                spout = spout.to(input_ids.device)\n-        if past_key_values is not None:\n-            return {\n-                \"input_ids\": input_ids[:, -1:] if input_ids is not None else None,\n-                \"attention_mask\": attention_mask,\n-                \"token_type_ids\": token_type_ids[:, -1:] if token_type_ids is not None else None,\n-                \"spout\": spout,\n-                \"past_key_values\": past_key_values,\n-            }\n-        return {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"token_type_ids\": token_type_ids,\n-            \"spout\": spout,\n-            \"past_key_values\": None,\n-        }\n-\n-    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n-        return self._shift_right(labels)\n-\n-    def resize_token_embeddings(\n-        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n-    ) -> nn.Embedding:\n-        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n-        self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n-        return new_embeddings\n-\n-    def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n-        old_num_tokens = self.final_logits_bias.shape[-1]\n-        if new_num_tokens <= old_num_tokens:\n-            new_bias = self.final_logits_bias[:, :new_num_tokens]\n-        else:\n-            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n-            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n-        self.register_buffer(\"final_logits_bias\", new_bias)\n-\n-    def get_input_embeddings(self):\n-        return self.model.get_input_embeddings()\n-\n-    def set_input_embeddings(self, new_embeddings):\n-        self.model.set_input_embeddings(new_embeddings)\n-\n-    def _unpack_router_logits(self, router_outputs):\n-        total_router_logits = []\n-        total_expert_indexes = []\n-        for router_output in router_outputs:\n-            if len(router_output[0].shape) > 1:\n-                router_logits, expert_indexes = router_output\n-                total_router_logits.append(router_logits)\n-                total_expert_indexes.append(expert_indexes)\n-        return torch.cat(total_router_logits, dim=1), torch.cat(total_expert_indexes, dim=1)\n-\n-\n-__all__ = [\"GPTSanJapaneseForConditionalGeneration\", \"GPTSanJapaneseModel\", \"GPTSanJapanesePreTrainedModel\"]"
        },
        {
            "sha": "1025fdf75fb46a0d350359a66c09315bd42effd3",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/tokenization_gptsan_japanese.py",
            "status": "removed",
            "additions": 0,
            "deletions": 518,
            "changes": 518,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,518 +0,0 @@\n-# coding=utf-8\n-# Copyright 2023 HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Tokenization classes for GPTSANJapanese.\"\"\"\n-\n-import collections\n-import json\n-import os\n-import re\n-import sys\n-from typing import Optional, Union\n-\n-import numpy as np\n-\n-from ....tokenization_utils import PreTrainedTokenizer\n-from ....tokenization_utils_base import (\n-    BatchEncoding,\n-    PreTokenizedInput,\n-    PreTokenizedInputPair,\n-    TextInput,\n-    TextInputPair,\n-    TruncationStrategy,\n-)\n-from ....utils import PaddingStrategy, logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\", \"emoji_file\": \"emoji.json\"}\n-\n-\n-def load_vocab_and_emoji(vocab_file, emoji_file):\n-    \"\"\"Loads a vocabulary file and emoji file into a dictionary.\"\"\"\n-    with open(emoji_file, \"r\", encoding=\"utf-8\") as f:\n-        emoji = json.loads(f.read())\n-\n-    vocab = collections.OrderedDict()\n-    raw_vocab = collections.OrderedDict()\n-    ids_to_tokens = collections.OrderedDict()\n-    with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n-        token = f.readlines()\n-    token = [[t.rstrip(\"\\n\")] if (t == \",\\n\" or \",\" not in t) else t.rstrip(\"\\n\").split(\",\") for t in token]\n-    for idx, b in enumerate(token):\n-        ids_to_tokens[idx] = b\n-        raw_vocab[\",\".join(b)] = idx\n-        for wd in b:\n-            vocab[wd] = idx\n-\n-    return vocab, raw_vocab, ids_to_tokens, emoji\n-\n-\n-class GPTSanJapaneseTokenizer(PreTrainedTokenizer):\n-    \"\"\"\n-    This tokenizer is based on GPTNeoXJapaneseTokenizer and has the following modifications\n-    - Decoding byte0~byte255 tokens correctly\n-    - Added bagofword token handling\n-    - Return token_type_ids for Prefix-LM model\n-    The bagofword token represents a repetition of the previous token and is converted to 3 consecutive tokens when\n-    decoding In addition, the original Japanese special Sub-Word-Encoding has been released in this repository\n-    (https://github.com/tanreinama/Japanese-BPEEncoder_V2). The token_type_ids is a mask indicating the prefix input\n-    position of the Prefix-LM model. To specify a prefix position, specify a prefix input for prefix_text, or specify a\n-    sentence of the prefix part and the part after it as a text pair of batch input.\n-\n-    Example:\n-\n-    ```python\n-    >>> from transformers import GPTSanJapaneseTokenizer\n-\n-    >>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n-    >>> # You can confirm both æ…¶å¿œ and æ…¶æ‡‰ are encoded to 17750\n-    >>> tokenizer(\"å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ðŸ¯ã€‚å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«\")[\"input_ids\"]\n-    [35993, 35998, 34347, 31459, 30647, 31448, 25, 30659, 35729, 35676, 32417, 30647, 17750, 35589, 17750, 35590, 321, 1281]\n-\n-    >>> # Both æ…¶å¿œ and æ…¶æ‡‰ are decoded to æ…¶å¿œ\n-    >>> tokenizer.decode(tokenizer(\"å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ðŸ¯ã€‚å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«\")[\"input_ids\"])\n-    'å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ðŸ¯ã€‚å®Ÿã¯æ…¶å¿œ(æ…¶å¿œ)å¤§å­¦å‡ºèº«'\n-    ```\n-\n-    Example for Prefix-LM:\n-\n-    ```python\n-    >>> from transformers import GPTSanJapaneseTokenizer\n-\n-    >>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n-    >>> tokenizer(\"å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«\", prefix_text=\"å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ðŸ¯ã€‚\")[\"input_ids\"]\n-    [35993, 34347, 31459, 30647, 31448, 25, 30659, 35729, 35676, 35998, 32417, 30647, 17750, 35589, 17750, 35590, 321, 1281]\n-\n-    >>> # Mask for Prefix-LM inputs\n-    >>> tokenizer(\"å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«\", prefix_text=\"å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ðŸ¯ã€‚\")[\"token_type_ids\"]\n-    [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n-    ```\n-\n-    Example for batch encode:\n-\n-    ```python\n-    >>> from transformers import GPTSanJapaneseTokenizer\n-\n-    >>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n-    >>> tokenizer([[\"æ­¦ç”°ä¿¡çŽ„\", \"ã¯ã€\"], [\"ç¹”ç”°ä¿¡é•·\", \"ã®é…ä¸‹ã®ã€\"]], padding=True)[\"input_ids\"]\n-    [[35993, 35998, 8640, 25948, 35993, 35998, 30647, 35675, 35999, 35999], [35993, 35998, 10382, 9868, 35993, 35998, 30646, 9459, 30646, 35675]]\n-\n-    >>> # Mask for Prefix-LM inputs\n-    >>> tokenizer([[\"æ­¦ç”°ä¿¡çŽ„\", \"ã¯ã€\"], [\"ç¹”ç”°ä¿¡é•·\", \"ã®é…ä¸‹ã®ã€\"]], padding=True)[\"token_type_ids\"]\n-    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n-\n-    >>> # Mask for padding\n-    >>> tokenizer([[\"æ­¦ç”°ä¿¡çŽ„\", \"ã¯ã€\"], [\"ç¹”ç”°ä¿¡é•·\", \"ã®é…ä¸‹ã®ã€\"]], padding=True)[\"attention_mask\"]\n-    [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n-    ```\n-\n-    Args:\n-        vocab_file (`str`):\n-            File containing the vocabulary.\n-        emoji_file (`str`):\n-            File containing the emoji.\n-        unk_token (`str`, *optional*, defaults to `\"<|nottoken|>\"`):\n-            The token used for unknown character\n-        pad_token (`str`, *optional*, defaults to `\"<|separator|>\"`):\n-            The token used for padding\n-        bos_token (`str`, *optional*, defaults to `\"<|startoftext|>\"`):\n-            The beginning of sequence token.\n-        eos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n-            The end of sequence token.\n-        sep_token (`str`, *optional*, defaults to `\"<|segmenter|>\"`):\n-            A special token to separate token to prefix part and general input part.\n-        do_clean_text (`bool`, *optional*, defaults to `False`):\n-            Whether or not to clean text for URL, EMAIL, TEL, Japanese DATE and Japanese PRICE.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    model_input_names = [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n-\n-    def __init__(\n-        self,\n-        vocab_file,\n-        emoji_file,\n-        unk_token=\"<|nottoken|>\",\n-        pad_token=\"<|separator|>\",\n-        bos_token=\"<|startoftext|>\",\n-        eos_token=\"<|endoftext|>\",\n-        sep_token=\"<|segmenter|>\",\n-        do_clean_text=False,\n-        **kwargs,\n-    ):\n-        if not os.path.isfile(vocab_file):\n-            raise ValueError(\n-                f\"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained\"\n-                \" model use `tokenizer = GPTSanJapaneseTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n-            )\n-        if not os.path.isfile(emoji_file):\n-            raise ValueError(\n-                f\"Can't find a emoji file at path '{emoji_file}'. To load the emoji information from a Google\"\n-                \" pretrained model use `tokenizer = GPTSanJapaneseTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n-            )\n-        self.do_clean_text = do_clean_text\n-        self.vocab, self.raw_vocab, self.ids_to_tokens, self.emoji = load_vocab_and_emoji(vocab_file, emoji_file)\n-        self.subword_tokenizer = SubWordJapaneseTokenizer(\n-            vocab=self.vocab, ids_to_tokens=self.ids_to_tokens, emoji=self.emoji\n-        )\n-\n-        super().__init__(\n-            unk_token=unk_token,\n-            pad_token=pad_token,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            sep_token=sep_token,\n-            do_clean_text=do_clean_text,\n-            **kwargs,\n-        )\n-\n-    @property\n-    def vocab_size(self):\n-        # self.vocab contains support for character fluctuation unique to Japanese, and has a large number of vocab\n-        return len(self.raw_vocab)\n-\n-    def get_vocab(self):\n-        return dict(self.raw_vocab, **self.added_tokens_encoder)\n-\n-    def _tokenize(self, text):\n-        return self.subword_tokenizer.tokenize(text, clean=self.do_clean_text)\n-\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.vocab.get(token, self.vocab.get(self.unk_token))\n-\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.subword_tokenizer.convert_id_to_token(index)\n-\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        words = []\n-        byte_tokens = []\n-        for word in tokens:\n-            if word[:6] == \"<|byte\" and word[-2:] == \"|>\":\n-                byte_tokens.append(int(word[6:-2]))\n-            else:\n-                if len(byte_tokens) > 0:\n-                    words.append(bytearray(byte_tokens).decode(\"utf-8\", errors=\"replace\"))\n-                    byte_tokens = []\n-                if word[:7] == \"<|emoji\" and word[-2:] == \"|>\":\n-                    words.append(self.emoji[\"emoji_inv\"][word])\n-                elif word == \"<SP>\":\n-                    words.append(\" \")\n-                elif word == \"<BR>\":\n-                    words.append(\"\\n\")\n-                elif word == \"<TAB>\":\n-                    words.append(\"\\t\")\n-                elif word == \"<BLOCK>\":\n-                    words.append(\"â–€\")\n-                elif word == \"<KIGOU>\":\n-                    words.append(\"Ç€\")\n-                elif word == \"<U2000U2BFF>\":\n-                    words.append(\"â€–\")\n-                elif word == \"<|bagoftoken|>\":\n-                    if len(words) > 0:\n-                        words.append(words[-1])\n-                        words.append(words[-1])\n-                        words.append(words[-1])\n-                elif word.startswith(\"<|\") and word.endswith(\"|>\"):\n-                    words.append(\"\")\n-                else:\n-                    words.append(word)\n-        if len(byte_tokens) > 0:\n-            words.append(bytearray(byte_tokens).decode(\"utf-8\", errors=\"replace\"))\n-        text = \"\".join(words)\n-        return text\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        index = 0\n-        if os.path.isdir(save_directory):\n-            vocab_file = os.path.join(\n-                save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-            )\n-            emoji_file = os.path.join(\n-                save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"emoji_file\"]\n-            )\n-        else:\n-            vocab_file = (\n-                (filename_prefix + \"-\" if filename_prefix else \"\") + save_directory + VOCAB_FILES_NAMES[\"vocab_file\"]\n-            )\n-            emoji_file = (\n-                (filename_prefix + \"-\" if filename_prefix else \"\") + save_directory + VOCAB_FILES_NAMES[\"emoji_file\"]\n-            )\n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n-            for token_index, token in self.ids_to_tokens.items():\n-                if index != token_index:\n-                    logger.warning(\n-                        f\"Saving vocabulary to {vocab_file}: vocabulary indices are not consecutive.\"\n-                        \" Please check that the vocabulary is not corrupted!\"\n-                    )\n-                    index = token_index\n-                writer.write(\",\".join(token) + \"\\n\")\n-                index += 1\n-        with open(emoji_file, \"w\", encoding=\"utf-8\") as writer:\n-            json.dump(self.emoji, writer)\n-        return vocab_file, emoji_file\n-\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        # docstyle-ignore\n-        \"\"\"\n-        The tokenizer returns token_type_ids as separators between the Prefix part and the rest.\n-        token_type_ids is 1 for the Prefix part and 0 for the rest of the token.\n-\n-        Example:\n-        ```python\n-        >>> from transformers import GPTSanJapaneseTokenizer\n-\n-        >>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n-        >>> x_token = tokenizer(\"ï½±ï½²ï½³ï½´\")\n-        >>> # input_ids:      | SOT | SEG | ï½± | ï½² | ï½³ | ï½´ |\n-        >>> # token_type_ids: | 1   | 0   | 0 | 0 | 0 | 0 |\n-\n-        >>> x_token = tokenizer(\"\", prefix_text=\"ï½±ï½²ï½³ï½´\")\n-        >>> # input_ids:      | SOT | ï½± | ï½² | ï½³ | ï½´ | SEG |\n-        >>> # token_type_ids: | 1   | 1 | 1 | 1 | 1 | 0  |\n-\n-        >>> x_token = tokenizer(\"ï½³ï½´\", prefix_text=\"ï½±ï½²\")\n-        >>> # input_ids:      | SOT | ï½± | ï½² | SEG | ï½³ | ï½´ |\n-        >>> # token_type_ids: | 1   | 1 | 1 | 0   | 0 | 0 |\n-        ```\"\"\"\n-        prefix_len = 0\n-        if self.sep_token in self.vocab:\n-            segid = self.vocab[self.sep_token]\n-            if segid in token_ids_0:\n-                prefix_len = token_ids_0.index(segid)\n-        if token_ids_1 is None:\n-            total_len = len(token_ids_0)\n-        else:\n-            total_len = len(token_ids_0 + token_ids_1)\n-        return prefix_len * [1] + (total_len - prefix_len) * [0]\n-\n-    def prepare_for_tokenization(self, text, prefix_text=None, add_sep_token=None, **kwargs):\n-        # GPTSAN inserts extra SEP tokens in Prefix-LM in addition to SOT for text generation.\n-        # SOT at the beginning of the text, and SEP at the separator between the Prefix part and the rest.\n-        if add_sep_token is None:\n-            add_sep_token = self.sep_token not in text  # If insert un-prefix position explicitly\n-        prepared = self.bos_token if self.bos_token in self.vocab else \"\"\n-        prepared += prefix_text if prefix_text is not None else \"\"\n-        if add_sep_token:\n-            prepared += self.sep_token if self.sep_token in self.vocab else \"\"\n-        prepared += text\n-        return (prepared, kwargs)\n-\n-    def _batch_encode_plus(\n-        self,\n-        batch_text_or_text_pairs: Union[\n-            list[TextInput], list[TextInputPair], list[PreTokenizedInput], list[PreTokenizedInputPair]\n-        ],\n-        add_special_tokens: bool = True,\n-        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n-        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n-        max_length: Optional[int] = None,\n-        stride: int = 0,\n-        is_split_into_words: bool = False,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_tensors: Optional[str] = None,\n-        return_token_type_ids: Optional[bool] = None,\n-        return_attention_mask: Optional[bool] = None,\n-        return_overflowing_tokens: bool = False,\n-        return_special_tokens_mask: bool = False,\n-        return_offsets_mapping: bool = False,\n-        return_length: bool = False,\n-        verbose: bool = True,\n-        **kwargs,\n-    ) -> BatchEncoding:\n-        # This tokenizer converts input text pairs into Prefix input and subsequent input\n-        if isinstance(batch_text_or_text_pairs[0], tuple) or isinstance(tuple(batch_text_or_text_pairs[0]), list):\n-            # As a single text with an explicit un-prefix position\n-            batch_prefix_texts = []\n-            for pref, txt in batch_text_or_text_pairs:\n-                batch_prefix_texts.append(pref + self.sep_token + txt)\n-            batch_text_or_text_pairs = batch_prefix_texts\n-\n-        return super()._batch_encode_plus(\n-            batch_text_or_text_pairs,\n-            add_special_tokens,\n-            padding_strategy,\n-            truncation_strategy,\n-            max_length,\n-            stride,\n-            is_split_into_words,\n-            pad_to_multiple_of,\n-            return_tensors,\n-            return_token_type_ids,\n-            return_attention_mask,\n-            return_overflowing_tokens,\n-            return_special_tokens_mask,\n-            return_offsets_mapping,\n-            return_length,\n-            verbose,\n-            **kwargs,\n-        )\n-\n-\n-class SubWordJapaneseTokenizer:\n-    \"\"\"\n-    This tokenizer is based on GPTNeoXJapaneseTokenizer and has the following modifications\n-    - Decoding byte0~byte255 tokens correctly\n-    - Added bagofword token handling\n-\n-    https://github.com/tanreinama/Japanese-BPEEncoder_V2 This tokenizer class is under MIT License according to the\n-    original repository.\n-\n-    MIT License\n-\n-    Copyright (c) 2020 tanreinama\n-\n-    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n-    documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n-    rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\n-    permit persons to whom the Software is furnished to do so, subject to the following conditions:\n-\n-    The above copyright notice and this permission notice shall be included in all copies or substantial portions of\n-    the Software.\n-\n-    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO\n-    THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n-    TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-    SOFTWARE.\n-    \"\"\"\n-\n-    def __init__(self, vocab, ids_to_tokens, emoji):\n-        self.vocab = vocab  # same as swe\n-        self.ids_to_tokens = ids_to_tokens  # same as bpe\n-        self.emoji = emoji\n-        self.maxlen = np.max([len(w) for w in self.vocab])\n-        self.content_repatter1 = re.compile(r\"(https?|ftp)(:\\/\\/[-_\\.!~*\\'()a-zA-Z0-9;\\/?:\\@&=\\+$,%#]+)\")\n-        self.content_repatter2 = re.compile(r\"[A-Za-z0-9\\._+]*@[\\-_0-9A-Za-z]+(\\.[A-Za-z]+)*\")\n-        self.content_repatter3 = re.compile(r\"[\\(]{0,1}[0-9]{2,4}[\\)\\-\\(]{0,1}[0-9]{2,4}[\\)\\-]{0,1}[0-9]{3,4}\")\n-        self.content_repatter4 = re.compile(\n-            r\"([12]\\d{3}[/\\-å¹´])*(0?[1-9]|1[0-2])[/\\-æœˆ]((0?[1-9]|[12][0-9]|3[01])æ—¥?)*(\\d{1,2}|:|\\d{1,2}æ™‚|\\d{1,2}åˆ†|\\(æ—¥\\)|\\(æœˆ\\)|\\(ç«\\)|\\(æ°´\\)|\\(æœ¨\\)|\\(é‡‘\\)|\\(åœŸ\\)|ãˆ°|ãˆª|ãˆ«|ãˆ¬|ãˆ­|ãˆ®|ãˆ¯)*\"\n-        )\n-        self.content_repatter5 = re.compile(\n-            r\"(æ˜Žæ²»|å¤§æ­£|æ˜­å’Œ|å¹³æˆ|ä»¤å’Œ|ã¾|ã½|ã¼|ã»|\\u32ff)\\d{1,2}å¹´(0?[1-9]|1[0-2])æœˆ(0?[1-9]|[12][0-9]|3[01])æ—¥(\\d{1,2}|:|\\d{1,2}æ™‚|\\d{1,2}åˆ†|\\(æ—¥\\)|\\(æœˆ\\)|\\(ç«\\)|\\(æ°´\\)|\\(æœ¨\\)|\\(é‡‘\\)|\\(åœŸ\\)|ãˆ°|ãˆª|ãˆ«|ãˆ¬|ãˆ­|ãˆ®|ãˆ¯)*\"\n-        )\n-        # The original version of this regex displays catastrophic backtracking behaviour. We avoid this using\n-        # possessive quantifiers in Py >= 3.11. In versions below this, we avoid the vulnerability using a slightly\n-        # different regex that should generally have the same behaviour in most non-pathological cases.\n-        if sys.version_info >= (3, 11):\n-            self.content_repatter6 = re.compile(\n-                r\"(?:\\d,\\d{3}|[\\då„„])*+\"\n-                r\"(?:\\d,\\d{3}|[\\dä¸‡])*+\"\n-                r\"(?:\\d,\\d{3}|[\\dåƒ])*+\"\n-                r\"(?:åƒå††|ä¸‡å††|åƒä¸‡å††|å††|åƒãƒ‰ãƒ«|ä¸‡ãƒ‰ãƒ«|åƒä¸‡ãƒ‰ãƒ«|ãƒ‰ãƒ«|åƒãƒ¦ãƒ¼ãƒ­|ä¸‡ãƒ¦ãƒ¼ãƒ­|åƒä¸‡ãƒ¦ãƒ¼ãƒ­|ãƒ¦ãƒ¼ãƒ­)+\"\n-                r\"(?:\\(ç¨Žè¾¼\\)|\\(ç¨ŽæŠœ\\)|\\+tax)*\"\n-            )\n-        else:\n-            self.content_repatter6 = re.compile(\n-                r\"(?:\\d,\\d{3}|[\\då„„ä¸‡åƒ])*\"\n-                r\"(?:åƒå††|ä¸‡å††|åƒä¸‡å††|å††|åƒãƒ‰ãƒ«|ä¸‡ãƒ‰ãƒ«|åƒä¸‡ãƒ‰ãƒ«|ãƒ‰ãƒ«|åƒãƒ¦ãƒ¼ãƒ­|ä¸‡ãƒ¦ãƒ¼ãƒ­|åƒä¸‡ãƒ¦ãƒ¼ãƒ­|ãƒ¦ãƒ¼ãƒ­)+\"\n-                r\"(?:\\(ç¨Žè¾¼\\)|\\(ç¨ŽæŠœ\\)|\\+tax)*\"\n-            )\n-        keisen = \"â”€â”â”‚â”ƒâ”„â”…â”†â”‡â”ˆâ”‰â”Šâ”‹â”Œâ”â”Žâ”â”â”‘â”’â”“â””â”•â”–â”—â”˜â”™â”šâ”›â”œâ”â”žâ”Ÿâ” â”¡â”¢â”£â”¤â”¥â”¦â”§â”¨â”©â”ªâ”«â”¬â”­â”®â”¯â”°â”±â”²â”³â”´â”µâ”¶â”·â”¸â”¹â”ºâ”»â”¼â”½â”¾â”¿â•€â•â•‚â•ƒâ•„â•…â•†â•‡â•ˆâ•‰â•Šâ•‹â•Œâ•â•Žâ•â•â•‘â•’â•“â•”â••â•–â•—â•˜â•™â•šâ•›â•œâ•â•žâ•Ÿâ• â•¡â•¢â•£â•¤â•¥â•¦â•§â•¨â•©â•ªâ•«â•¬â•­â•®â•¯â•°â•±â•²â•³â•´â•µâ•¶â•·â•¸â•¹â•ºâ•»â•¼â•½â•¾â•¿\"\n-        blocks = \"â–€â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–‰â–Šâ–‹â–Œâ–â–Žâ–â–â–‘â–’â–“â–”â–•â––â–—â–˜â–™â–šâ–›â–œâ–â–žâ–Ÿ\"\n-        self.content_trans1 = str.maketrans(dict.fromkeys(keisen + blocks, \"<BLOCK>\"))\n-\n-    def __len__(self):\n-        return len(self.ids_to_tokens)\n-\n-    def clean_text(self, content):\n-        content = self.content_repatter1.sub(\"<URL>\", content)\n-        content = self.content_repatter2.sub(\"<EMAIL>\", content)\n-        content = self.content_repatter3.sub(\"<TEL>\", content)\n-        content = self.content_repatter4.sub(\"<DATE>\", content)\n-        content = self.content_repatter5.sub(\"<DATE>\", content)\n-        content = self.content_repatter6.sub(\"<PRICE>\", content)\n-        content = content.translate(self.content_trans1)\n-        while \"<BLOCK><BLOCK>\" in content:\n-            content = content.replace(\"<BLOCK><BLOCK>\", \"<BLOCK>\")\n-        return content\n-\n-    def tokenize(self, text, clean=False):\n-        text = text.replace(\" \", \"<SP>\")\n-        text = text.replace(\"ã€€\", \"<SP>\")\n-        text = text.replace(\"\\r\\n\", \"<BR>\")\n-        text = text.replace(\"\\n\", \"<BR>\")\n-        text = text.replace(\"\\r\", \"<BR>\")\n-        text = text.replace(\"\\t\", \"<TAB>\")\n-        text = text.replace(\"â€”\", \"ãƒ¼\")\n-        text = text.replace(\"âˆ’\", \"ãƒ¼\")\n-        for k, v in self.emoji[\"emoji\"].items():\n-            if k in text:\n-                text = text.replace(k, v)\n-        if clean:\n-            text = self.clean_text(text)\n-\n-        def check_simbol(x):\n-            e = x.encode()\n-            if len(x) == 1 and len(e) == 2:\n-                c = (int(e[0]) << 8) + int(e[1])\n-                if (\n-                    (c >= 0xC2A1 and c <= 0xC2BF)\n-                    or (c >= 0xC780 and c <= 0xC783)\n-                    or (c >= 0xCAB9 and c <= 0xCBBF)\n-                    or (c >= 0xCC80 and c <= 0xCDA2)\n-                ):\n-                    return True\n-            return False\n-\n-        def checku2e(x):\n-            e = x.encode()\n-            if len(x) == 1 and len(e) == 3:\n-                c = (int(e[0]) << 16) + (int(e[1]) << 8) + int(e[2])\n-                if c >= 0xE28080 and c <= 0xE2B07F:\n-                    return True\n-            return False\n-\n-        pos = 0\n-        result = []\n-        while pos < len(text):\n-            end = min(len(text), pos + self.maxlen + 1) if text[pos] == \"<\" else pos + 3\n-            candidates = []  # (token_id, token, pos)\n-            for e in range(end, pos, -1):\n-                wd = text[pos:e]\n-                if wd in self.vocab:\n-                    if wd[0] == \"<\" and len(wd) > 2:\n-                        candidates = [(self.vocab[wd], wd, e)]\n-                        break\n-                    else:\n-                        candidates.append((self.vocab[wd], wd, e))\n-            if len(candidates) > 0:\n-                # the smallest token_id is adopted\n-                _, wd, e = min(candidates, key=lambda x: x[0])\n-                result.append(wd)\n-                pos = e\n-            else:\n-                end = pos + 1\n-                wd = text[pos:end]\n-                if check_simbol(wd):\n-                    result.append(\"<KIGOU>\")\n-                elif checku2e(wd):\n-                    result.append(\"<U2000U2BFF>\")\n-                else:\n-                    for i in wd.encode(\"utf-8\"):\n-                        result.append(\"<|byte%d|>\" % i)\n-                pos = end\n-        return result\n-\n-    def convert_id_to_token(self, index):\n-        return self.ids_to_tokens[index][0]\n-\n-\n-__all__ = [\"GPTSanJapaneseTokenizer\"]"
        },
        {
            "sha": "3a4b3eb1be2b4e69dcad1540abdd91f412de0ca2",
            "filename": "src/transformers/models/deprecated/graphormer/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,27 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from typing import TYPE_CHECKING\n-\n-from ....utils import _LazyModule\n-from ....utils.import_utils import define_import_structure\n-\n-\n-if TYPE_CHECKING:\n-    from .configuration_graphormer import *\n-    from .modeling_graphormer import *\n-else:\n-    import sys\n-\n-    _file = globals()[\"__file__\"]\n-    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "bcbff6445fe109fbf5cef82b131222cc3e6eb508",
            "filename": "src/transformers/models/deprecated/graphormer/algos_graphormer.pyx",
            "status": "removed",
            "additions": 0,
            "deletions": 107,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Falgos_graphormer.pyx",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Falgos_graphormer.pyx",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Falgos_graphormer.pyx?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,107 +0,0 @@\n-# Copyright (c) Microsoft Corporation and HuggingFace\n-# Licensed under the MIT License.\n-\n-import cython\n-\n-cimport numpy\n-from cython.parallel cimport parallel, prange\n-\n-import numpy as np\n-\n-\n-# Reduce this number if matrices are too big for large graphs\n-UNREACHABLE_NODE_DISTANCE = 510 \n-\n-def floyd_warshall(adjacency_matrix):\n-    \"\"\"\n-    Applies the Floyd-Warshall algorithm to the adjacency matrix, to compute the \n-    shortest paths distance between all nodes, up to UNREACHABLE_NODE_DISTANCE.\n-    \"\"\"\n-    (nrows, ncols) = adjacency_matrix.shape\n-    assert nrows == ncols\n-    cdef unsigned int n = nrows\n-\n-    adj_mat_copy = adjacency_matrix.astype(np.int32, order='C', casting='safe', copy=True)\n-    assert adj_mat_copy.flags['C_CONTIGUOUS']\n-    cdef numpy.ndarray[numpy.int32_t, ndim=2, mode='c'] M = adj_mat_copy\n-    cdef numpy.ndarray[numpy.int32_t, ndim=2, mode='c'] path = -1 * np.ones([n, n], dtype=np.int32)\n-\n-    cdef unsigned int i, j, k\n-    cdef numpy.int32_t M_ij, M_ik, cost_ikkj\n-    cdef numpy.int32_t* M_ptr = &M[0,0]\n-    cdef numpy.int32_t* M_i_ptr\n-    cdef numpy.int32_t* M_k_ptr\n-\n-    # set unreachable nodes distance to UNREACHABLE_NODE_DISTANCE\n-    for i in range(n):\n-        for j in range(n):\n-            if i == j:\n-                M[i][j] = 0\n-            elif M[i][j] == 0:\n-                M[i][j] = UNREACHABLE_NODE_DISTANCE\n-\n-    # floyed algo\n-    for k in range(n):\n-        M_k_ptr = M_ptr + n*k\n-        for i in range(n):\n-            M_i_ptr = M_ptr + n*i\n-            M_ik = M_i_ptr[k]\n-            for j in range(n):\n-                cost_ikkj = M_ik + M_k_ptr[j]\n-                M_ij = M_i_ptr[j]\n-                if M_ij > cost_ikkj:\n-                    M_i_ptr[j] = cost_ikkj\n-                    path[i][j] = k\n-\n-    # set unreachable path to UNREACHABLE_NODE_DISTANCE\n-    for i in range(n):\n-        for j in range(n):\n-            if M[i][j] >= UNREACHABLE_NODE_DISTANCE:\n-                path[i][j] = UNREACHABLE_NODE_DISTANCE\n-                M[i][j] = UNREACHABLE_NODE_DISTANCE\n-\n-    return M, path\n-\n-\n-def get_all_edges(path, i, j):\n-    \"\"\"\n-    Recursive function to compute all possible paths between two nodes from the graph adjacency matrix.\n-    \"\"\"\n-    cdef int k = path[i][j]\n-    if k == -1:\n-        return []\n-    else:\n-        return get_all_edges(path, i, k) + [k] + get_all_edges(path, k, j)\n-\n-\n-def gen_edge_input(max_dist, path, edge_feat):\n-    \"\"\"\n-    Generates the full edge feature and adjacency matrix.\n-    Shape: num_nodes * num_nodes * max_distance_between_nodes * num_edge_features\n-    Dim 1 is the input node, dim 2 the output node of the edge, dim 3 the depth of the edge, dim 4 the feature\n-    \"\"\"\n-    (nrows, ncols) = path.shape\n-    assert nrows == ncols\n-    cdef unsigned int n = nrows\n-    cdef unsigned int max_dist_copy = max_dist\n-\n-    path_copy = path.astype(int, order='C', casting='safe', copy=True)\n-    edge_feat_copy = edge_feat.astype(int, order='C', casting='safe', copy=True)\n-    assert path_copy.flags['C_CONTIGUOUS']\n-    assert edge_feat_copy.flags['C_CONTIGUOUS']\n-\n-    cdef numpy.ndarray[numpy.int32_t, ndim=4, mode='c'] edge_fea_all = -1 * np.ones([n, n, max_dist_copy, edge_feat.shape[-1]], dtype=np.int32)\n-    cdef unsigned int i, j, k, num_path, cur\n-\n-    for i in range(n):\n-        for j in range(n):\n-            if i == j:\n-                continue\n-            if path_copy[i][j] == UNREACHABLE_NODE_DISTANCE:\n-                continue\n-            path = [i] + get_all_edges(path_copy, i, j) + [j]\n-            num_path = len(path) - 1\n-            for k in range(num_path):\n-                edge_fea_all[i, j, k, :] = edge_feat_copy[path[k], path[k+1], :]\n-\n-    return edge_fea_all"
        },
        {
            "sha": "88657bab435d95cf5f06014524b184939e8582c7",
            "filename": "src/transformers/models/deprecated/graphormer/collating_graphormer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 135,
            "changes": 135,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fcollating_graphormer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fcollating_graphormer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fcollating_graphormer.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,135 +0,0 @@\n-# Copyright (c) Microsoft Corporation and HuggingFace\n-# Licensed under the MIT License.\n-\n-from collections.abc import Mapping\n-from typing import Any\n-\n-import numpy as np\n-import torch\n-\n-from ....utils import is_cython_available, requires_backends\n-\n-\n-if is_cython_available():\n-    import pyximport\n-\n-    pyximport.install(setup_args={\"include_dirs\": np.get_include()})\n-    from . import algos_graphormer\n-\n-\n-def convert_to_single_emb(x, offset: int = 512):\n-    feature_num = x.shape[1] if len(x.shape) > 1 else 1\n-    feature_offset = 1 + np.arange(0, feature_num * offset, offset, dtype=np.int64)\n-    x = x + feature_offset\n-    return x\n-\n-\n-def preprocess_item(item, keep_features=True):\n-    requires_backends(preprocess_item, [\"cython\"])\n-\n-    if keep_features and \"edge_attr\" in item:  # edge_attr\n-        edge_attr = np.asarray(item[\"edge_attr\"], dtype=np.int64)\n-    else:\n-        edge_attr = np.ones((len(item[\"edge_index\"][0]), 1), dtype=np.int64)  # same embedding for all\n-\n-    if keep_features and \"node_feat\" in item:  # input_nodes\n-        node_feature = np.asarray(item[\"node_feat\"], dtype=np.int64)\n-    else:\n-        node_feature = np.ones((item[\"num_nodes\"], 1), dtype=np.int64)  # same embedding for all\n-\n-    edge_index = np.asarray(item[\"edge_index\"], dtype=np.int64)\n-\n-    input_nodes = convert_to_single_emb(node_feature) + 1\n-    num_nodes = item[\"num_nodes\"]\n-\n-    if len(edge_attr.shape) == 1:\n-        edge_attr = edge_attr[:, None]\n-    attn_edge_type = np.zeros([num_nodes, num_nodes, edge_attr.shape[-1]], dtype=np.int64)\n-    attn_edge_type[edge_index[0], edge_index[1]] = convert_to_single_emb(edge_attr) + 1\n-\n-    # node adj matrix [num_nodes, num_nodes] bool\n-    adj = np.zeros([num_nodes, num_nodes], dtype=bool)\n-    adj[edge_index[0], edge_index[1]] = True\n-\n-    shortest_path_result, path = algos_graphormer.floyd_warshall(adj)\n-    max_dist = np.amax(shortest_path_result)\n-\n-    input_edges = algos_graphormer.gen_edge_input(max_dist, path, attn_edge_type)\n-    attn_bias = np.zeros([num_nodes + 1, num_nodes + 1], dtype=np.single)  # with graph token\n-\n-    # combine\n-    item[\"input_nodes\"] = input_nodes + 1  # we shift all indices by one for padding\n-    item[\"attn_bias\"] = attn_bias\n-    item[\"attn_edge_type\"] = attn_edge_type\n-    item[\"spatial_pos\"] = shortest_path_result.astype(np.int64) + 1  # we shift all indices by one for padding\n-    item[\"in_degree\"] = np.sum(adj, axis=1).reshape(-1) + 1  # we shift all indices by one for padding\n-    item[\"out_degree\"] = item[\"in_degree\"]  # for undirected graph\n-    item[\"input_edges\"] = input_edges + 1  # we shift all indices by one for padding\n-    if \"labels\" not in item:\n-        item[\"labels\"] = item[\"y\"]\n-\n-    return item\n-\n-\n-class GraphormerDataCollator:\n-    def __init__(self, spatial_pos_max=20, on_the_fly_processing=False):\n-        if not is_cython_available():\n-            raise ImportError(\"Graphormer preprocessing needs Cython (pyximport)\")\n-\n-        self.spatial_pos_max = spatial_pos_max\n-        self.on_the_fly_processing = on_the_fly_processing\n-\n-    def __call__(self, features: list[dict]) -> dict[str, Any]:\n-        if self.on_the_fly_processing:\n-            features = [preprocess_item(i) for i in features]\n-\n-        if not isinstance(features[0], Mapping):\n-            features = [vars(f) for f in features]\n-        batch = {}\n-\n-        max_node_num = max(len(i[\"input_nodes\"]) for i in features)\n-        node_feat_size = len(features[0][\"input_nodes\"][0])\n-        edge_feat_size = len(features[0][\"attn_edge_type\"][0][0])\n-        max_dist = max(len(i[\"input_edges\"][0][0]) for i in features)\n-        edge_input_size = len(features[0][\"input_edges\"][0][0][0])\n-        batch_size = len(features)\n-\n-        batch[\"attn_bias\"] = torch.zeros(batch_size, max_node_num + 1, max_node_num + 1, dtype=torch.float)\n-        batch[\"attn_edge_type\"] = torch.zeros(batch_size, max_node_num, max_node_num, edge_feat_size, dtype=torch.long)\n-        batch[\"spatial_pos\"] = torch.zeros(batch_size, max_node_num, max_node_num, dtype=torch.long)\n-        batch[\"in_degree\"] = torch.zeros(batch_size, max_node_num, dtype=torch.long)\n-        batch[\"input_nodes\"] = torch.zeros(batch_size, max_node_num, node_feat_size, dtype=torch.long)\n-        batch[\"input_edges\"] = torch.zeros(\n-            batch_size, max_node_num, max_node_num, max_dist, edge_input_size, dtype=torch.long\n-        )\n-\n-        for ix, f in enumerate(features):\n-            for k in [\"attn_bias\", \"attn_edge_type\", \"spatial_pos\", \"in_degree\", \"input_nodes\", \"input_edges\"]:\n-                f[k] = torch.tensor(f[k])\n-\n-            if len(f[\"attn_bias\"][1:, 1:][f[\"spatial_pos\"] >= self.spatial_pos_max]) > 0:\n-                f[\"attn_bias\"][1:, 1:][f[\"spatial_pos\"] >= self.spatial_pos_max] = float(\"-inf\")\n-\n-            batch[\"attn_bias\"][ix, : f[\"attn_bias\"].shape[0], : f[\"attn_bias\"].shape[1]] = f[\"attn_bias\"]\n-            batch[\"attn_edge_type\"][ix, : f[\"attn_edge_type\"].shape[0], : f[\"attn_edge_type\"].shape[1], :] = f[\n-                \"attn_edge_type\"\n-            ]\n-            batch[\"spatial_pos\"][ix, : f[\"spatial_pos\"].shape[0], : f[\"spatial_pos\"].shape[1]] = f[\"spatial_pos\"]\n-            batch[\"in_degree\"][ix, : f[\"in_degree\"].shape[0]] = f[\"in_degree\"]\n-            batch[\"input_nodes\"][ix, : f[\"input_nodes\"].shape[0], :] = f[\"input_nodes\"]\n-            batch[\"input_edges\"][\n-                ix, : f[\"input_edges\"].shape[0], : f[\"input_edges\"].shape[1], : f[\"input_edges\"].shape[2], :\n-            ] = f[\"input_edges\"]\n-\n-        batch[\"out_degree\"] = batch[\"in_degree\"]\n-\n-        sample = features[0][\"labels\"]\n-        if len(sample) == 1:  # one task\n-            if isinstance(sample[0], float):  # regression\n-                batch[\"labels\"] = torch.from_numpy(np.concatenate([i[\"labels\"] for i in features]))\n-            else:  # binary classification\n-                batch[\"labels\"] = torch.from_numpy(np.concatenate([i[\"labels\"] for i in features]))\n-        else:  # multi task classification, left to float to keep the NaNs\n-            batch[\"labels\"] = torch.from_numpy(np.stack([i[\"labels\"] for i in features], axis=0))\n-\n-        return batch"
        },
        {
            "sha": "1a2bb9587d56ccd90fb452b01025a5fe70273261",
            "filename": "src/transformers/models/deprecated/graphormer/configuration_graphormer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 220,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fconfiguration_graphormer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fconfiguration_graphormer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fconfiguration_graphormer.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,220 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 Microsoft, clefourrier and The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Graphormer model configuration\"\"\"\n-\n-from typing import Optional\n-\n-from ....configuration_utils import PreTrainedConfig\n-from ....utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class GraphormerConfig(PreTrainedConfig):\n-    r\"\"\"\n-    This is the configuration class to store the configuration of a [`~GraphormerModel`]. It is used to instantiate an\n-    Graphormer model according to the specified arguments, defining the model architecture. Instantiating a\n-    configuration with the defaults will yield a similar configuration to that of the Graphormer\n-    [graphormer-base-pcqm4mv1](https://huggingface.co/graphormer-base-pcqm4mv1) architecture.\n-\n-    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PreTrainedConfig`] for more information.\n-\n-\n-    Args:\n-        num_classes (`int`, *optional*, defaults to 1):\n-            Number of target classes or labels, set to n for binary classification of n tasks.\n-        num_atoms (`int`, *optional*, defaults to 512*9):\n-            Number of node types in the graphs.\n-        num_edges (`int`, *optional*, defaults to 512*3):\n-            Number of edges types in the graph.\n-        num_in_degree (`int`, *optional*, defaults to 512):\n-            Number of in degrees types in the input graphs.\n-        num_out_degree (`int`, *optional*, defaults to 512):\n-            Number of out degrees types in the input graphs.\n-        num_edge_dis (`int`, *optional*, defaults to 128):\n-            Number of edge dis in the input graphs.\n-        multi_hop_max_dist (`int`, *optional*, defaults to 20):\n-            Maximum distance of multi hop edges between two nodes.\n-        spatial_pos_max (`int`, *optional*, defaults to 1024):\n-            Maximum distance between nodes in the graph attention bias matrices, used during preprocessing and\n-            collation.\n-        edge_type (`str`, *optional*, defaults to multihop):\n-            Type of edge relation chosen.\n-        max_nodes (`int`, *optional*, defaults to 512):\n-            Maximum number of nodes which can be parsed for the input graphs.\n-        share_input_output_embed (`bool`, *optional*, defaults to `False`):\n-            Shares the embedding layer between encoder and decoder - careful, True is not implemented.\n-        num_layers (`int`, *optional*, defaults to 12):\n-            Number of layers.\n-        embedding_dim (`int`, *optional*, defaults to 768):\n-            Dimension of the embedding layer in encoder.\n-        ffn_embedding_dim (`int`, *optional*, defaults to 768):\n-            Dimension of the \"intermediate\" (often named feed-forward) layer in encoder.\n-        num_attention_heads (`int`, *optional*, defaults to 32):\n-            Number of attention heads in the encoder.\n-        self_attention (`bool`, *optional*, defaults to `True`):\n-            Model is self attentive (False not implemented).\n-        activation_function (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n-            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n-            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n-        dropout (`float`, *optional*, defaults to 0.1):\n-            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n-        attention_dropout (`float`, *optional*, defaults to 0.1):\n-            The dropout probability for the attention weights.\n-        activation_dropout (`float`, *optional*, defaults to 0.1):\n-            The dropout probability for the activation of the linear transformer layer.\n-        layerdrop (`float`, *optional*, defaults to 0.0):\n-            The LayerDrop probability for the encoder. See the [LayerDrop paper](see https://huggingface.co/papers/1909.11556)\n-            for more details.\n-        bias (`bool`, *optional*, defaults to `True`):\n-            Uses bias in the attention module - unsupported at the moment.\n-        embed_scale(`float`, *optional*, defaults to None):\n-            Scaling factor for the node embeddings.\n-        num_trans_layers_to_freeze (`int`, *optional*, defaults to 0):\n-            Number of transformer layers to freeze.\n-        encoder_normalize_before (`bool`, *optional*, defaults to `False`):\n-            Normalize features before encoding the graph.\n-        pre_layernorm (`bool`, *optional*, defaults to `False`):\n-            Apply layernorm before self attention and the feed forward network. Without this, post layernorm will be\n-            used.\n-        apply_graphormer_init (`bool`, *optional*, defaults to `False`):\n-            Apply a custom graphormer initialisation to the model before training.\n-        freeze_embeddings (`bool`, *optional*, defaults to `False`):\n-            Freeze the embedding layer, or train it along the model.\n-        encoder_normalize_before (`bool`, *optional*, defaults to `False`):\n-            Apply the layer norm before each encoder block.\n-        q_noise (`float`, *optional*, defaults to 0.0):\n-            Amount of quantization noise (see \"Training with Quantization Noise for Extreme Model Compression\"). (For\n-            more detail, see fairseq's documentation on quant_noise).\n-        qn_block_size (`int`, *optional*, defaults to 8):\n-            Size of the blocks for subsequent quantization with iPQ (see q_noise).\n-        kdim (`int`, *optional*, defaults to None):\n-            Dimension of the key in the attention, if different from the other values.\n-        vdim (`int`, *optional*, defaults to None):\n-            Dimension of the value in the attention, if different from the other values.\n-        use_cache (`bool`, *optional*, defaults to `True`):\n-            Whether or not the model should return the last key/values attentions (not used by all models).\n-        traceable (`bool`, *optional*, defaults to `False`):\n-            Changes return value of the encoder's inner_state to stacked tensors.\n-\n-        Example:\n-            ```python\n-            >>> from transformers import GraphormerForGraphClassification, GraphormerConfig\n-\n-            >>> # Initializing a Graphormer graphormer-base-pcqm4mv2 style configuration\n-            >>> configuration = GraphormerConfig()\n-\n-            >>> # Initializing a model from the graphormer-base-pcqm4mv1 style configuration\n-            >>> model = GraphormerForGraphClassification(configuration)\n-\n-            >>> # Accessing the model configuration\n-            >>> configuration = model.config\n-            ```\n-    \"\"\"\n-\n-    model_type = \"graphormer\"\n-    keys_to_ignore_at_inference = [\"past_key_values\"]\n-\n-    def __init__(\n-        self,\n-        num_classes: int = 1,\n-        num_atoms: int = 512 * 9,\n-        num_edges: int = 512 * 3,\n-        num_in_degree: int = 512,\n-        num_out_degree: int = 512,\n-        num_spatial: int = 512,\n-        num_edge_dis: int = 128,\n-        multi_hop_max_dist: int = 5,  # sometimes is 20\n-        spatial_pos_max: int = 1024,\n-        edge_type: str = \"multi_hop\",\n-        max_nodes: int = 512,\n-        share_input_output_embed: bool = False,\n-        num_hidden_layers: int = 12,\n-        embedding_dim: int = 768,\n-        ffn_embedding_dim: int = 768,\n-        num_attention_heads: int = 32,\n-        dropout: float = 0.1,\n-        attention_dropout: float = 0.1,\n-        activation_dropout: float = 0.1,\n-        layerdrop: float = 0.0,\n-        encoder_normalize_before: bool = False,\n-        pre_layernorm: bool = False,\n-        apply_graphormer_init: bool = False,\n-        activation_fn: str = \"gelu\",\n-        embed_scale: Optional[float] = None,\n-        freeze_embeddings: bool = False,\n-        num_trans_layers_to_freeze: int = 0,\n-        traceable: bool = False,\n-        q_noise: float = 0.0,\n-        qn_block_size: int = 8,\n-        kdim: Optional[int] = None,\n-        vdim: Optional[int] = None,\n-        bias: bool = True,\n-        self_attention: bool = True,\n-        pad_token_id=0,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        **kwargs,\n-    ):\n-        self.num_classes = num_classes\n-        self.num_atoms = num_atoms\n-        self.num_in_degree = num_in_degree\n-        self.num_out_degree = num_out_degree\n-        self.num_edges = num_edges\n-        self.num_spatial = num_spatial\n-        self.num_edge_dis = num_edge_dis\n-        self.edge_type = edge_type\n-        self.multi_hop_max_dist = multi_hop_max_dist\n-        self.spatial_pos_max = spatial_pos_max\n-        self.max_nodes = max_nodes\n-        self.num_hidden_layers = num_hidden_layers\n-        self.embedding_dim = embedding_dim\n-        self.hidden_size = embedding_dim\n-        self.ffn_embedding_dim = ffn_embedding_dim\n-        self.num_attention_heads = num_attention_heads\n-        self.dropout = dropout\n-        self.attention_dropout = attention_dropout\n-        self.activation_dropout = activation_dropout\n-        self.layerdrop = layerdrop\n-        self.encoder_normalize_before = encoder_normalize_before\n-        self.pre_layernorm = pre_layernorm\n-        self.apply_graphormer_init = apply_graphormer_init\n-        self.activation_fn = activation_fn\n-        self.embed_scale = embed_scale\n-        self.freeze_embeddings = freeze_embeddings\n-        self.num_trans_layers_to_freeze = num_trans_layers_to_freeze\n-        self.share_input_output_embed = share_input_output_embed\n-        self.traceable = traceable\n-        self.q_noise = q_noise\n-        self.qn_block_size = qn_block_size\n-\n-        # These parameters are here for future extensions\n-        # atm, the model only supports self attention\n-        self.kdim = kdim\n-        self.vdim = vdim\n-        self.self_attention = self_attention\n-        self.bias = bias\n-\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            **kwargs,\n-        )\n-\n-\n-__all__ = [\"GraphormerConfig\"]"
        },
        {
            "sha": "0069abd2ce6fe1e5fdba82e07bb41f79f470bfb8",
            "filename": "src/transformers/models/deprecated/graphormer/modeling_graphormer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 895,
            "changes": 895,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fmodeling_graphormer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fmodeling_graphormer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fmodeling_graphormer.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,895 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 Microsoft, clefourrier The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch Graphormer model.\"\"\"\n-\n-import math\n-from collections.abc import Iterable, Iterator\n-from typing import Optional, Union\n-\n-import torch\n-import torch.nn as nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n-\n-from .... import initialization as init\n-from ....activations import ACT2FN\n-from ....modeling_outputs import (\n-    BaseModelOutputWithNoAttention,\n-    SequenceClassifierOutput,\n-)\n-from ....modeling_utils import PreTrainedModel\n-from ....utils import logging\n-from .configuration_graphormer import GraphormerConfig\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-_CHECKPOINT_FOR_DOC = \"graphormer-base-pcqm4mv1\"\n-_CONFIG_FOR_DOC = \"GraphormerConfig\"\n-\n-\n-def quant_noise(module: nn.Module, p: float, block_size: int):\n-    \"\"\"\n-    From:\n-    https://github.com/facebookresearch/fairseq/blob/dd0079bde7f678b0cd0715cbd0ae68d661b7226d/fairseq/modules/quant_noise.py\n-\n-    Wraps modules and applies quantization noise to the weights for subsequent quantization with Iterative Product\n-    Quantization as described in \"Training with Quantization Noise for Extreme Model Compression\"\n-\n-    Args:\n-        - module: nn.Module\n-        - p: amount of Quantization Noise\n-        - block_size: size of the blocks for subsequent quantization with iPQ\n-\n-    Remarks:\n-        - Module weights must have the right sizes wrt the block size\n-        - Only Linear, Embedding and Conv2d modules are supported for the moment\n-        - For more detail on how to quantize by blocks with convolutional weights, see \"And the Bit Goes Down:\n-          Revisiting the Quantization of Neural Networks\"\n-        - We implement the simplest form of noise here as stated in the paper which consists in randomly dropping\n-          blocks\n-    \"\"\"\n-\n-    # if no quantization noise, don't register hook\n-    if p <= 0:\n-        return module\n-\n-    # supported modules\n-    if not isinstance(module, (nn.Linear, nn.Embedding, nn.Conv2d)):\n-        raise NotImplementedError(\"Module unsupported for quant_noise.\")\n-\n-    # test whether module.weight has the right sizes wrt block_size\n-    is_conv = module.weight.ndim == 4\n-\n-    # 2D matrix\n-    if not is_conv:\n-        if module.weight.size(1) % block_size != 0:\n-            raise AssertionError(\"Input features must be a multiple of block sizes\")\n-\n-    # 4D matrix\n-    else:\n-        # 1x1 convolutions\n-        if module.kernel_size == (1, 1):\n-            if module.in_channels % block_size != 0:\n-                raise AssertionError(\"Input channels must be a multiple of block sizes\")\n-        # regular convolutions\n-        else:\n-            k = module.kernel_size[0] * module.kernel_size[1]\n-            if k % block_size != 0:\n-                raise AssertionError(\"Kernel size must be a multiple of block size\")\n-\n-    def _forward_pre_hook(mod, input):\n-        # no noise for evaluation\n-        if mod.training:\n-            if not is_conv:\n-                # gather weight and sizes\n-                weight = mod.weight\n-                in_features = weight.size(1)\n-                out_features = weight.size(0)\n-\n-                # split weight matrix into blocks and randomly drop selected blocks\n-                mask = torch.zeros(in_features // block_size * out_features, device=weight.device)\n-                mask.bernoulli_(p)\n-                mask = mask.repeat_interleave(block_size, -1).view(-1, in_features)\n-\n-            else:\n-                # gather weight and sizes\n-                weight = mod.weight\n-                in_channels = mod.in_channels\n-                out_channels = mod.out_channels\n-\n-                # split weight matrix into blocks and randomly drop selected blocks\n-                if mod.kernel_size == (1, 1):\n-                    mask = torch.zeros(\n-                        int(in_channels // block_size * out_channels),\n-                        device=weight.device,\n-                    )\n-                    mask.bernoulli_(p)\n-                    mask = mask.repeat_interleave(block_size, -1).view(-1, in_channels)\n-                else:\n-                    mask = torch.zeros(weight.size(0), weight.size(1), device=weight.device)\n-                    mask.bernoulli_(p)\n-                    mask = mask.unsqueeze(2).unsqueeze(3).repeat(1, 1, mod.kernel_size[0], mod.kernel_size[1])\n-\n-            # scale weights and apply mask\n-            mask = mask.to(torch.bool)  # x.bool() is not currently supported in TorchScript\n-            s = 1 / (1 - p)\n-            mod.weight.data = s * weight.masked_fill(mask, 0)\n-\n-    module.register_forward_pre_hook(_forward_pre_hook)\n-    return module\n-\n-\n-class LayerDropModuleList(nn.ModuleList):\n-    \"\"\"\n-    From:\n-    https://github.com/facebookresearch/fairseq/blob/dd0079bde7f678b0cd0715cbd0ae68d661b7226d/fairseq/modules/layer_drop.py\n-    A LayerDrop implementation based on [`torch.nn.ModuleList`]. LayerDrop as described in\n-    https://huggingface.co/papers/1909.11556.\n-\n-    We refresh the choice of which layers to drop every time we iterate over the LayerDropModuleList instance. During\n-    evaluation we always iterate over all layers.\n-\n-    Usage:\n-\n-    ```python\n-    layers = LayerDropList(p=0.5, modules=[layer1, layer2, layer3])\n-    for layer in layers:  # this might iterate over layers 1 and 3\n-        x = layer(x)\n-    for layer in layers:  # this might iterate over all layers\n-        x = layer(x)\n-    for layer in layers:  # this might not iterate over any layers\n-        x = layer(x)\n-    ```\n-\n-    Args:\n-        p (float): probability of dropping out each layer\n-        modules (iterable, optional): an iterable of modules to add\n-    \"\"\"\n-\n-    def __init__(self, p: float, modules: Optional[Iterable[nn.Module]] = None):\n-        super().__init__(modules)\n-        self.p = p\n-\n-    def __iter__(self) -> Iterator[nn.Module]:\n-        dropout_probs = torch.empty(len(self)).uniform_()\n-        for i, m in enumerate(super().__iter__()):\n-            if not self.training or (dropout_probs[i] > self.p):\n-                yield m\n-\n-\n-class GraphormerGraphNodeFeature(nn.Module):\n-    \"\"\"\n-    Compute node features for each node in the graph.\n-    \"\"\"\n-\n-    def __init__(self, config: GraphormerConfig):\n-        super().__init__()\n-        self.num_heads = config.num_attention_heads\n-        self.num_atoms = config.num_atoms\n-\n-        self.atom_encoder = nn.Embedding(config.num_atoms + 1, config.hidden_size, padding_idx=config.pad_token_id)\n-        self.in_degree_encoder = nn.Embedding(\n-            config.num_in_degree, config.hidden_size, padding_idx=config.pad_token_id\n-        )\n-        self.out_degree_encoder = nn.Embedding(\n-            config.num_out_degree, config.hidden_size, padding_idx=config.pad_token_id\n-        )\n-\n-        self.graph_token = nn.Embedding(1, config.hidden_size)\n-\n-    def forward(\n-        self,\n-        input_nodes: torch.LongTensor,\n-        in_degree: torch.LongTensor,\n-        out_degree: torch.LongTensor,\n-    ) -> torch.Tensor:\n-        n_graph, n_node = input_nodes.size()[:2]\n-\n-        node_feature = (  # node feature + graph token\n-            self.atom_encoder(input_nodes).sum(dim=-2)  # [n_graph, n_node, n_hidden]\n-            + self.in_degree_encoder(in_degree)\n-            + self.out_degree_encoder(out_degree)\n-        )\n-\n-        graph_token_feature = self.graph_token.weight.unsqueeze(0).repeat(n_graph, 1, 1)\n-\n-        graph_node_feature = torch.cat([graph_token_feature, node_feature], dim=1)\n-\n-        return graph_node_feature\n-\n-\n-class GraphormerGraphAttnBias(nn.Module):\n-    \"\"\"\n-    Compute attention bias for each head.\n-    \"\"\"\n-\n-    def __init__(self, config: GraphormerConfig):\n-        super().__init__()\n-        self.num_heads = config.num_attention_heads\n-        self.multi_hop_max_dist = config.multi_hop_max_dist\n-\n-        # We do not change edge feature embedding learning, as edge embeddings are represented as a combination of the original features\n-        # + shortest path\n-        self.edge_encoder = nn.Embedding(config.num_edges + 1, config.num_attention_heads, padding_idx=0)\n-\n-        self.edge_type = config.edge_type\n-        if self.edge_type == \"multi_hop\":\n-            self.edge_dis_encoder = nn.Embedding(\n-                config.num_edge_dis * config.num_attention_heads * config.num_attention_heads,\n-                1,\n-            )\n-\n-        self.spatial_pos_encoder = nn.Embedding(config.num_spatial, config.num_attention_heads, padding_idx=0)\n-\n-        self.graph_token_virtual_distance = nn.Embedding(1, config.num_attention_heads)\n-\n-    def forward(\n-        self,\n-        input_nodes: torch.LongTensor,\n-        attn_bias: torch.Tensor,\n-        spatial_pos: torch.LongTensor,\n-        input_edges: torch.LongTensor,\n-        attn_edge_type: torch.LongTensor,\n-    ) -> torch.Tensor:\n-        n_graph, n_node = input_nodes.size()[:2]\n-        graph_attn_bias = attn_bias.clone()\n-        graph_attn_bias = graph_attn_bias.unsqueeze(1).repeat(\n-            1, self.num_heads, 1, 1\n-        )  # [n_graph, n_head, n_node+1, n_node+1]\n-\n-        # spatial pos\n-        # [n_graph, n_node, n_node, n_head] -> [n_graph, n_head, n_node, n_node]\n-        spatial_pos_bias = self.spatial_pos_encoder(spatial_pos).permute(0, 3, 1, 2)\n-        graph_attn_bias[:, :, 1:, 1:] = graph_attn_bias[:, :, 1:, 1:] + spatial_pos_bias\n-\n-        # reset spatial pos here\n-        t = self.graph_token_virtual_distance.weight.view(1, self.num_heads, 1)\n-        graph_attn_bias[:, :, 1:, 0] = graph_attn_bias[:, :, 1:, 0] + t\n-        graph_attn_bias[:, :, 0, :] = graph_attn_bias[:, :, 0, :] + t\n-\n-        # edge feature\n-        if self.edge_type == \"multi_hop\":\n-            spatial_pos_ = spatial_pos.clone()\n-\n-            spatial_pos_[spatial_pos_ == 0] = 1  # set pad to 1\n-            # set 1 to 1, input_nodes > 1 to input_nodes - 1\n-            spatial_pos_ = torch.where(spatial_pos_ > 1, spatial_pos_ - 1, spatial_pos_)\n-            if self.multi_hop_max_dist > 0:\n-                spatial_pos_ = spatial_pos_.clamp(0, self.multi_hop_max_dist)\n-                input_edges = input_edges[:, :, :, : self.multi_hop_max_dist, :]\n-            # [n_graph, n_node, n_node, max_dist, n_head]\n-\n-            input_edges = self.edge_encoder(input_edges).mean(-2)\n-            max_dist = input_edges.size(-2)\n-            edge_input_flat = input_edges.permute(3, 0, 1, 2, 4).reshape(max_dist, -1, self.num_heads)\n-            edge_input_flat = torch.bmm(\n-                edge_input_flat,\n-                self.edge_dis_encoder.weight.reshape(-1, self.num_heads, self.num_heads)[:max_dist, :, :],\n-            )\n-            input_edges = edge_input_flat.reshape(max_dist, n_graph, n_node, n_node, self.num_heads).permute(\n-                1, 2, 3, 0, 4\n-            )\n-            input_edges = (input_edges.sum(-2) / (spatial_pos_.float().unsqueeze(-1))).permute(0, 3, 1, 2)\n-        else:\n-            # [n_graph, n_node, n_node, n_head] -> [n_graph, n_head, n_node, n_node]\n-            input_edges = self.edge_encoder(attn_edge_type).mean(-2).permute(0, 3, 1, 2)\n-\n-        graph_attn_bias[:, :, 1:, 1:] = graph_attn_bias[:, :, 1:, 1:] + input_edges\n-        graph_attn_bias = graph_attn_bias + attn_bias.unsqueeze(1)  # reset\n-\n-        return graph_attn_bias\n-\n-\n-class GraphormerMultiheadAttention(nn.Module):\n-    \"\"\"Multi-headed attention.\n-\n-    See \"Attention Is All You Need\" for more details.\n-    \"\"\"\n-\n-    def __init__(self, config: GraphormerConfig):\n-        super().__init__()\n-        self.embedding_dim = config.embedding_dim\n-        self.kdim = config.kdim if config.kdim is not None else config.embedding_dim\n-        self.vdim = config.vdim if config.vdim is not None else config.embedding_dim\n-        self.qkv_same_dim = self.kdim == config.embedding_dim and self.vdim == config.embedding_dim\n-\n-        self.num_heads = config.num_attention_heads\n-        self.attention_dropout_module = torch.nn.Dropout(p=config.attention_dropout, inplace=False)\n-\n-        self.head_dim = config.embedding_dim // config.num_attention_heads\n-        if not (self.head_dim * config.num_attention_heads == self.embedding_dim):\n-            raise AssertionError(\"The embedding_dim must be divisible by num_heads.\")\n-        self.scaling = self.head_dim**-0.5\n-\n-        self.self_attention = True  # config.self_attention\n-        if not (self.self_attention):\n-            raise NotImplementedError(\"The Graphormer model only supports self attention for now.\")\n-        if self.self_attention and not self.qkv_same_dim:\n-            raise AssertionError(\"Self-attention requires query, key and value to be of the same size.\")\n-\n-        self.k_proj = quant_noise(\n-            nn.Linear(self.kdim, config.embedding_dim, bias=config.bias),\n-            config.q_noise,\n-            config.qn_block_size,\n-        )\n-        self.v_proj = quant_noise(\n-            nn.Linear(self.vdim, config.embedding_dim, bias=config.bias),\n-            config.q_noise,\n-            config.qn_block_size,\n-        )\n-        self.q_proj = quant_noise(\n-            nn.Linear(config.embedding_dim, config.embedding_dim, bias=config.bias),\n-            config.q_noise,\n-            config.qn_block_size,\n-        )\n-\n-        self.out_proj = quant_noise(\n-            nn.Linear(config.embedding_dim, config.embedding_dim, bias=config.bias),\n-            config.q_noise,\n-            config.qn_block_size,\n-        )\n-\n-        self.onnx_trace = False\n-\n-    def reset_parameters(self):\n-        if self.qkv_same_dim:\n-            # Empirically observed the convergence to be much better with\n-            # the scaled initialization\n-            init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n-            init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n-            init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n-        else:\n-            init.xavier_uniform_(self.k_proj.weight)\n-            init.xavier_uniform_(self.v_proj.weight)\n-            init.xavier_uniform_(self.q_proj.weight)\n-\n-        init.xavier_uniform_(self.out_proj.weight)\n-        if self.out_proj.bias is not None:\n-            init.constant_(self.out_proj.bias, 0.0)\n-\n-    def forward(\n-        self,\n-        query: torch.LongTensor,\n-        key: Optional[torch.Tensor],\n-        value: Optional[torch.Tensor],\n-        attn_bias: Optional[torch.Tensor],\n-        key_padding_mask: Optional[torch.Tensor] = None,\n-        need_weights: bool = True,\n-        attn_mask: Optional[torch.Tensor] = None,\n-        before_softmax: bool = False,\n-        need_head_weights: bool = False,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n-        \"\"\"\n-        Args:\n-            key_padding_mask (Bytetorch.Tensor, optional): mask to exclude\n-                keys that are pads, of shape `(batch, src_len)`, where padding elements are indicated by 1s.\n-            need_weights (bool, optional): return the attention weights,\n-                averaged over heads (default: False).\n-            attn_mask (Bytetorch.Tensor, optional): typically used to\n-                implement causal attention, where the mask prevents the attention from looking forward in time\n-                (default: None).\n-            before_softmax (bool, optional): return the raw attention\n-                weights and values before the attention softmax.\n-            need_head_weights (bool, optional): return the attention\n-                weights for each head. Implies *need_weights*. Default: return the average attention weights over all\n-                heads.\n-        \"\"\"\n-        if need_head_weights:\n-            need_weights = True\n-\n-        tgt_len, bsz, embedding_dim = query.size()\n-        src_len = tgt_len\n-        if not (embedding_dim == self.embedding_dim):\n-            raise AssertionError(\n-                f\"The query embedding dimension {embedding_dim} is not equal to the expected embedding_dim\"\n-                f\" {self.embedding_dim}.\"\n-            )\n-        if not (list(query.size()) == [tgt_len, bsz, embedding_dim]):\n-            raise AssertionError(\"Query size incorrect in Graphormer, compared to model dimensions.\")\n-\n-        if key is not None:\n-            src_len, key_bsz, _ = key.size()\n-            if not torch.jit.is_scripting():\n-                if (key_bsz != bsz) or (value is None) or not (src_len, bsz == value.shape[:2]):\n-                    raise AssertionError(\n-                        \"The batch shape does not match the key or value shapes provided to the attention.\"\n-                    )\n-\n-        q = self.q_proj(query)\n-        k = self.k_proj(query)\n-        v = self.v_proj(query)\n-\n-        q *= self.scaling\n-\n-        q = q.contiguous().view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n-        if k is not None:\n-            k = k.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n-        if v is not None:\n-            v = v.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n-\n-        if (k is None) or not (k.size(1) == src_len):\n-            raise AssertionError(\"The shape of the key generated in the attention is incorrect\")\n-\n-        # This is part of a workaround to get around fork/join parallelism\n-        # not supporting Optional types.\n-        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n-            key_padding_mask = None\n-\n-        if key_padding_mask is not None:\n-            if key_padding_mask.size(0) != bsz or key_padding_mask.size(1) != src_len:\n-                raise AssertionError(\n-                    \"The shape of the generated padding mask for the key does not match expected dimensions.\"\n-                )\n-        attn_weights = torch.bmm(q, k.transpose(1, 2))\n-        attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n-\n-        if list(attn_weights.size()) != [bsz * self.num_heads, tgt_len, src_len]:\n-            raise AssertionError(\"The attention weights generated do not match the expected dimensions.\")\n-\n-        if attn_bias is not None:\n-            attn_weights += attn_bias.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if attn_mask is not None:\n-            attn_mask = attn_mask.unsqueeze(0)\n-            attn_weights += attn_mask\n-\n-        if key_padding_mask is not None:\n-            # don't attend to padding symbols\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.masked_fill(\n-                key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), float(\"-inf\")\n-            )\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if before_softmax:\n-            return attn_weights, v\n-\n-        attn_weights_float = torch.nn.functional.softmax(attn_weights, dim=-1)\n-        attn_weights = attn_weights_float.type_as(attn_weights)\n-        attn_probs = self.attention_dropout_module(attn_weights)\n-\n-        if v is None:\n-            raise AssertionError(\"No value generated\")\n-        attn = torch.bmm(attn_probs, v)\n-        if list(attn.size()) != [bsz * self.num_heads, tgt_len, self.head_dim]:\n-            raise AssertionError(\"The attention generated do not match the expected dimensions.\")\n-\n-        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embedding_dim)\n-        attn: torch.Tensor = self.out_proj(attn)\n-\n-        attn_weights = None\n-        if need_weights:\n-            attn_weights = attn_weights_float.contiguous().view(bsz, self.num_heads, tgt_len, src_len).transpose(1, 0)\n-            if not need_head_weights:\n-                # average attention weights over heads\n-                attn_weights = attn_weights.mean(dim=0)\n-\n-        return attn, attn_weights\n-\n-    def apply_sparse_mask(self, attn_weights: torch.Tensor, tgt_len: int, src_len: int, bsz: int) -> torch.Tensor:\n-        return attn_weights\n-\n-\n-class GraphormerGraphEncoderLayer(nn.Module):\n-    def __init__(self, config: GraphormerConfig) -> None:\n-        super().__init__()\n-\n-        # Initialize parameters\n-        self.embedding_dim = config.embedding_dim\n-        self.num_attention_heads = config.num_attention_heads\n-        self.q_noise = config.q_noise\n-        self.qn_block_size = config.qn_block_size\n-        self.pre_layernorm = config.pre_layernorm\n-\n-        self.dropout_module = torch.nn.Dropout(p=config.dropout, inplace=False)\n-\n-        self.activation_dropout_module = torch.nn.Dropout(p=config.activation_dropout, inplace=False)\n-\n-        # Initialize blocks\n-        self.activation_fn = ACT2FN[config.activation_fn]\n-        self.self_attn = GraphormerMultiheadAttention(config)\n-\n-        # layer norm associated with the self attention layer\n-        self.self_attn_layer_norm = nn.LayerNorm(self.embedding_dim)\n-\n-        self.fc1 = self.build_fc(\n-            self.embedding_dim,\n-            config.ffn_embedding_dim,\n-            q_noise=config.q_noise,\n-            qn_block_size=config.qn_block_size,\n-        )\n-        self.fc2 = self.build_fc(\n-            config.ffn_embedding_dim,\n-            self.embedding_dim,\n-            q_noise=config.q_noise,\n-            qn_block_size=config.qn_block_size,\n-        )\n-\n-        # layer norm associated with the position wise feed-forward NN\n-        self.final_layer_norm = nn.LayerNorm(self.embedding_dim)\n-\n-    def build_fc(\n-        self, input_dim: int, output_dim: int, q_noise: float, qn_block_size: int\n-    ) -> Union[nn.Module, nn.Linear, nn.Embedding, nn.Conv2d]:\n-        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n-\n-    def forward(\n-        self,\n-        input_nodes: torch.Tensor,\n-        self_attn_bias: Optional[torch.Tensor] = None,\n-        self_attn_mask: Optional[torch.Tensor] = None,\n-        self_attn_padding_mask: Optional[torch.Tensor] = None,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n-        \"\"\"\n-        nn.LayerNorm is applied either before or after the self-attention/ffn modules similar to the original\n-        Transformer implementation.\n-        \"\"\"\n-        residual = input_nodes\n-        if self.pre_layernorm:\n-            input_nodes = self.self_attn_layer_norm(input_nodes)\n-\n-        input_nodes, attn = self.self_attn(\n-            query=input_nodes,\n-            key=input_nodes,\n-            value=input_nodes,\n-            attn_bias=self_attn_bias,\n-            key_padding_mask=self_attn_padding_mask,\n-            need_weights=False,\n-            attn_mask=self_attn_mask,\n-        )\n-        input_nodes = self.dropout_module(input_nodes)\n-        input_nodes = residual + input_nodes\n-        if not self.pre_layernorm:\n-            input_nodes = self.self_attn_layer_norm(input_nodes)\n-\n-        residual = input_nodes\n-        if self.pre_layernorm:\n-            input_nodes = self.final_layer_norm(input_nodes)\n-        input_nodes = self.activation_fn(self.fc1(input_nodes))\n-        input_nodes = self.activation_dropout_module(input_nodes)\n-        input_nodes = self.fc2(input_nodes)\n-        input_nodes = self.dropout_module(input_nodes)\n-        input_nodes = residual + input_nodes\n-        if not self.pre_layernorm:\n-            input_nodes = self.final_layer_norm(input_nodes)\n-\n-        return input_nodes, attn\n-\n-\n-class GraphormerGraphEncoder(nn.Module):\n-    def __init__(self, config: GraphormerConfig):\n-        super().__init__()\n-\n-        self.dropout_module = torch.nn.Dropout(p=config.dropout, inplace=False)\n-        self.layerdrop = config.layerdrop\n-        self.embedding_dim = config.embedding_dim\n-        self.apply_graphormer_init = config.apply_graphormer_init\n-        self.traceable = config.traceable\n-\n-        self.graph_node_feature = GraphormerGraphNodeFeature(config)\n-        self.graph_attn_bias = GraphormerGraphAttnBias(config)\n-\n-        self.embed_scale = config.embed_scale\n-\n-        if config.q_noise > 0:\n-            self.quant_noise = quant_noise(\n-                nn.Linear(self.embedding_dim, self.embedding_dim, bias=False),\n-                config.q_noise,\n-                config.qn_block_size,\n-            )\n-        else:\n-            self.quant_noise = None\n-\n-        if config.encoder_normalize_before:\n-            self.emb_layer_norm = nn.LayerNorm(self.embedding_dim)\n-        else:\n-            self.emb_layer_norm = None\n-\n-        if config.pre_layernorm:\n-            self.final_layer_norm = nn.LayerNorm(self.embedding_dim)\n-\n-        if self.layerdrop > 0.0:\n-            self.layers = LayerDropModuleList(p=self.layerdrop)\n-        else:\n-            self.layers = nn.ModuleList([])\n-        self.layers.extend([GraphormerGraphEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n-\n-        # Apply initialization of model params after building the model\n-        if config.freeze_embeddings:\n-            raise NotImplementedError(\"Freezing embeddings is not implemented yet.\")\n-\n-        for layer in range(config.num_trans_layers_to_freeze):\n-            m = self.layers[layer]\n-            if m is not None:\n-                for p in m.parameters():\n-                    p.requires_grad = False\n-\n-    def forward(\n-        self,\n-        input_nodes: torch.LongTensor,\n-        input_edges: torch.LongTensor,\n-        attn_bias: torch.Tensor,\n-        in_degree: torch.LongTensor,\n-        out_degree: torch.LongTensor,\n-        spatial_pos: torch.LongTensor,\n-        attn_edge_type: torch.LongTensor,\n-        perturb=None,\n-        last_state_only: bool = False,\n-        token_embeddings: Optional[torch.Tensor] = None,\n-        attn_mask: Optional[torch.Tensor] = None,\n-    ) -> tuple[Union[torch.Tensor, list[torch.LongTensor]], torch.Tensor]:\n-        # compute padding mask. This is needed for multi-head attention\n-        data_x = input_nodes\n-        n_graph, n_node = data_x.size()[:2]\n-        padding_mask = (data_x[:, :, 0]).eq(0)\n-        padding_mask_cls = torch.zeros(n_graph, 1, device=padding_mask.device, dtype=padding_mask.dtype)\n-        padding_mask = torch.cat((padding_mask_cls, padding_mask), dim=1)\n-\n-        attn_bias = self.graph_attn_bias(input_nodes, attn_bias, spatial_pos, input_edges, attn_edge_type)\n-\n-        if token_embeddings is not None:\n-            input_nodes = token_embeddings\n-        else:\n-            input_nodes = self.graph_node_feature(input_nodes, in_degree, out_degree)\n-\n-        if perturb is not None:\n-            input_nodes[:, 1:, :] += perturb\n-\n-        if self.embed_scale is not None:\n-            input_nodes = input_nodes * self.embed_scale\n-\n-        if self.quant_noise is not None:\n-            input_nodes = self.quant_noise(input_nodes)\n-\n-        if self.emb_layer_norm is not None:\n-            input_nodes = self.emb_layer_norm(input_nodes)\n-\n-        input_nodes = self.dropout_module(input_nodes)\n-\n-        input_nodes = input_nodes.transpose(0, 1)\n-\n-        inner_states = []\n-        if not last_state_only:\n-            inner_states.append(input_nodes)\n-\n-        for layer in self.layers:\n-            input_nodes, _ = layer(\n-                input_nodes,\n-                self_attn_padding_mask=padding_mask,\n-                self_attn_mask=attn_mask,\n-                self_attn_bias=attn_bias,\n-            )\n-            if not last_state_only:\n-                inner_states.append(input_nodes)\n-\n-        graph_rep = input_nodes[0, :, :]\n-\n-        if last_state_only:\n-            inner_states = [input_nodes]\n-\n-        if self.traceable:\n-            return torch.stack(inner_states), graph_rep\n-        else:\n-            return inner_states, graph_rep\n-\n-\n-class GraphormerDecoderHead(nn.Module):\n-    def __init__(self, embedding_dim: int, num_classes: int):\n-        super().__init__()\n-        \"\"\"num_classes should be 1 for regression, or the number of classes for classification\"\"\"\n-        self.lm_output_learned_bias = nn.Parameter(torch.zeros(1))\n-        self.classifier = nn.Linear(embedding_dim, num_classes, bias=False)\n-        self.num_classes = num_classes\n-\n-    def forward(self, input_nodes: torch.Tensor, **unused) -> torch.Tensor:\n-        input_nodes = self.classifier(input_nodes)\n-        input_nodes = input_nodes + self.lm_output_learned_bias\n-        return input_nodes\n-\n-\n-class GraphormerPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config: GraphormerConfig\n-    base_model_prefix = \"graphormer\"\n-    main_input_name_nodes = \"input_nodes\"\n-    main_input_name_edges = \"input_edges\"\n-\n-    def init_graphormer_params(self, module: Union[nn.Linear, nn.Embedding, GraphormerMultiheadAttention]):\n-        \"\"\"\n-        Initialize the weights specific to the Graphormer Model.\n-        \"\"\"\n-        if isinstance(module, nn.Linear):\n-            init.normal_(module.weight.data, mean=0.0, std=0.02)\n-            if module.bias is not None:\n-                init.zeros_(module.bias)\n-        if isinstance(module, nn.Embedding):\n-            init.normal_(module.weight.data, mean=0.0, std=0.02)\n-            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n-            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n-                init.zeros_(module.weight[module.padding_idx])\n-        if isinstance(module, GraphormerMultiheadAttention):\n-            init.normal_(module.q_proj.weight.data, mean=0.0, std=0.02)\n-            init.normal_(module.k_proj.weight.data, mean=0.0, std=0.02)\n-            init.normal_(module.v_proj.weight.data, mean=0.0, std=0.02)\n-\n-    @torch.no_grad()\n-    def _init_weights(\n-        self,\n-        module: Union[\n-            nn.Linear, nn.Conv2d, nn.Embedding, nn.LayerNorm, GraphormerMultiheadAttention, GraphormerGraphEncoder\n-        ],\n-    ):\n-        \"\"\"\n-        Initialize the weights\n-        \"\"\"\n-        super()._init_weights(module)\n-        if isinstance(module, GraphormerMultiheadAttention):\n-            init.normal_(module.q_proj.weight, mean=0.0, std=0.02)\n-            init.normal_(module.k_proj.weight, mean=0.0, std=0.02)\n-            init.normal_(module.v_proj.weight, mean=0.0, std=0.02)\n-            module.reset_parameters()\n-        elif isinstance(module, GraphormerGraphEncoder):\n-            if module.apply_graphormer_init:\n-                module.apply(self.init_graphormer_params)\n-\n-\n-class GraphormerModel(GraphormerPreTrainedModel):\n-    \"\"\"The Graphormer model is a graph-encoder model.\n-\n-    It goes from a graph to its representation. If you want to use the model for a downstream classification task, use\n-    GraphormerForGraphClassification instead. For any other downstream task, feel free to add a new class, or combine\n-    this model with a downstream model of your choice, following the example in GraphormerForGraphClassification.\n-    \"\"\"\n-\n-    def __init__(self, config: GraphormerConfig):\n-        super().__init__(config)\n-        self.max_nodes = config.max_nodes\n-\n-        self.graph_encoder = GraphormerGraphEncoder(config)\n-\n-        self.share_input_output_embed = config.share_input_output_embed\n-        self.lm_output_learned_bias = None\n-\n-        # Remove head is set to true during fine-tuning\n-        self.load_softmax = not getattr(config, \"remove_head\", False)\n-\n-        self.lm_head_transform_weight = nn.Linear(config.embedding_dim, config.embedding_dim)\n-        self.activation_fn = ACT2FN[config.activation_fn]\n-        self.layer_norm = nn.LayerNorm(config.embedding_dim)\n-\n-        self.post_init()\n-\n-    def reset_output_layer_parameters(self):\n-        self.lm_output_learned_bias = nn.Parameter(torch.zeros(1))\n-\n-    def forward(\n-        self,\n-        input_nodes: torch.LongTensor,\n-        input_edges: torch.LongTensor,\n-        attn_bias: torch.Tensor,\n-        in_degree: torch.LongTensor,\n-        out_degree: torch.LongTensor,\n-        spatial_pos: torch.LongTensor,\n-        attn_edge_type: torch.LongTensor,\n-        perturb: Optional[torch.FloatTensor] = None,\n-        masked_tokens: None = None,\n-        return_dict: Optional[bool] = None,\n-        **unused,\n-    ) -> Union[tuple[torch.LongTensor], BaseModelOutputWithNoAttention]:\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        inner_states, graph_rep = self.graph_encoder(\n-            input_nodes, input_edges, attn_bias, in_degree, out_degree, spatial_pos, attn_edge_type, perturb=perturb\n-        )\n-\n-        # last inner state, then revert Batch and Graph len\n-        input_nodes = inner_states[-1].transpose(0, 1)\n-\n-        # project masked tokens only\n-        if masked_tokens is not None:\n-            raise NotImplementedError\n-\n-        input_nodes = self.layer_norm(self.activation_fn(self.lm_head_transform_weight(input_nodes)))\n-\n-        # project back to size of vocabulary\n-        if self.share_input_output_embed and hasattr(self.graph_encoder.embed_tokens, \"weight\"):\n-            input_nodes = torch.nn.functional.linear(input_nodes, self.graph_encoder.embed_tokens.weight)\n-\n-        if not return_dict:\n-            return tuple(x for x in [input_nodes, inner_states] if x is not None)\n-        return BaseModelOutputWithNoAttention(last_hidden_state=input_nodes, hidden_states=inner_states)\n-\n-    def max_nodes(self):\n-        \"\"\"Maximum output length supported by the encoder.\"\"\"\n-        return self.max_nodes\n-\n-\n-class GraphormerForGraphClassification(GraphormerPreTrainedModel):\n-    \"\"\"\n-    This model can be used for graph-level classification or regression tasks.\n-\n-    It can be trained on\n-    - regression (by setting config.num_classes to 1); there should be one float-type label per graph\n-    - one task classification (by setting config.num_classes to the number of classes); there should be one integer\n-      label per graph\n-    - binary multi-task classification (by setting config.num_classes to the number of labels); there should be a list\n-      of integer labels for each graph.\n-    \"\"\"\n-\n-    def __init__(self, config: GraphormerConfig):\n-        super().__init__(config)\n-        self.encoder = GraphormerModel(config)\n-        self.embedding_dim = config.embedding_dim\n-        self.num_classes = config.num_classes\n-        self.classifier = GraphormerDecoderHead(self.embedding_dim, self.num_classes)\n-        self.is_encoder_decoder = True\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def forward(\n-        self,\n-        input_nodes: torch.LongTensor,\n-        input_edges: torch.LongTensor,\n-        attn_bias: torch.Tensor,\n-        in_degree: torch.LongTensor,\n-        out_degree: torch.LongTensor,\n-        spatial_pos: torch.LongTensor,\n-        attn_edge_type: torch.LongTensor,\n-        labels: Optional[torch.LongTensor] = None,\n-        return_dict: Optional[bool] = None,\n-        **unused,\n-    ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        encoder_outputs = self.encoder(\n-            input_nodes,\n-            input_edges,\n-            attn_bias,\n-            in_degree,\n-            out_degree,\n-            spatial_pos,\n-            attn_edge_type,\n-            return_dict=True,\n-        )\n-        outputs, hidden_states = encoder_outputs[\"last_hidden_state\"], encoder_outputs[\"hidden_states\"]\n-\n-        head_outputs = self.classifier(outputs)\n-        logits = head_outputs[:, 0, :].contiguous()\n-\n-        loss = None\n-        if labels is not None:\n-            mask = ~torch.isnan(labels)\n-\n-            if self.num_classes == 1:  # regression\n-                loss_fct = MSELoss()\n-                loss = loss_fct(logits[mask].squeeze(), labels[mask].squeeze().float())\n-            elif self.num_classes > 1 and len(labels.shape) == 1:  # One task classification\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits[mask].view(-1, self.num_classes), labels[mask].view(-1))\n-            else:  # Binary multi-task classification\n-                loss_fct = BCEWithLogitsLoss(reduction=\"sum\")\n-                loss = loss_fct(logits[mask], labels[mask])\n-\n-        if not return_dict:\n-            return tuple(x for x in [loss, logits, hidden_states] if x is not None)\n-        return SequenceClassifierOutput(loss=loss, logits=logits, hidden_states=hidden_states, attentions=None)\n-\n-\n-__all__ = [\"GraphormerForGraphClassification\", \"GraphormerModel\", \"GraphormerPreTrainedModel\"]"
        },
        {
            "sha": "826bdbddc1f182de43a795ab0d78ad4009507c14",
            "filename": "src/transformers/models/deprecated/jukebox/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,28 +0,0 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from typing import TYPE_CHECKING\n-\n-from ....utils import _LazyModule\n-from ....utils.import_utils import define_import_structure\n-\n-\n-if TYPE_CHECKING:\n-    from .configuration_jukebox import *\n-    from .modeling_jukebox import *\n-    from .tokenization_jukebox import *\n-else:\n-    import sys\n-\n-    _file = globals()[\"__file__\"]\n-    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "57fae4d9d605fb23e313437b7cd737420e3bfb84",
            "filename": "src/transformers/models/deprecated/jukebox/configuration_jukebox.py",
            "status": "removed",
            "additions": 0,
            "deletions": 599,
            "changes": 599,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,599 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The OpenAI Team Authors and HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Jukebox configuration\"\"\"\n-\n-import os\n-from typing import Union\n-\n-from ....configuration_utils import PreTrainedConfig\n-from ....utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-_LARGE_ATTENTION = [\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"cross_attention\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"cross_attention\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"cross_attention\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"cross_attention\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"cross_attention\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"cross_attention\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"block_attn\",\n-    \"transpose_block_attn\",\n-    \"prev_block_attn\",\n-    \"cross_attention\",\n-]\n-_RawColumnPreviousRowAttention = [\"block_attn\", \"transpose_block_attn\", \"prev_block_attn\"]\n-_FullDenseAttention = [\"dense_attention\"]\n-_PrimePrimeDenseAttention = [\"prime_attn\", \"prime_attn\", \"dense_attn\"]\n-\n-\n-def full_dense_attention(layer):\n-    return _FullDenseAttention[0]\n-\n-\n-def raw_column_previous_row_attention(layer):\n-    return _RawColumnPreviousRowAttention[layer % 3]\n-\n-\n-def large_separated_enc_dec_w_lyrics(layer):\n-    return _LARGE_ATTENTION[layer % 79]\n-\n-\n-def enc_dec_with_lyrics(layer):\n-    if layer % 16 == 15:\n-        return _PrimePrimeDenseAttention[layer % 3]\n-    return _RawColumnPreviousRowAttention[layer % 3]\n-\n-\n-ATTENTION_PATTERNS = {\n-    \"full_dense_attention\": full_dense_attention,\n-    \"raw_column_previous_row_attention\": raw_column_previous_row_attention,  # Alternate row, column and previous row attn\n-    \"large_separated_enc_dec_w_lyrics\": large_separated_enc_dec_w_lyrics,  # Used by large separated_enc_dec model with lyrics\n-    \"enc_dec_with_lyrics\": enc_dec_with_lyrics,  # Used by encoder_decoder model with lyrics\n-}\n-\n-\n-class JukeboxPriorConfig(PreTrainedConfig):\n-    \"\"\"\n-        This is the configuration class to store the configuration of a [`JukeboxPrior`]. It is used to instantiate a\n-        `JukeboxPrior` according to the specified arguments, defining the model architecture. Instantiating a\n-        configuration with the defaults will yield a similar configuration to that of the top level prior from the\n-        [openai/jukebox-1b-lyrics](https://huggingface.co/openai/jukebox\n-    -1b-lyrics) architecture.\n-\n-        Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n-        documentation from [`PreTrainedConfig`] for more information.\n-\n-\n-\n-    Args:\n-        act_fn (`str`, *optional*, defaults to `\"quick_gelu\"`):\n-            Activation function.\n-        alignment_head (`int`, *optional*, defaults to 2):\n-            Head that is responsible of the alignment between lyrics and music. Only used to compute the lyric to audio\n-            alignment\n-        alignment_layer (`int`, *optional*, defaults to 68):\n-            Index of the layer that is responsible of the alignment between lyrics and music. Only used to compute the\n-            lyric to audio alignment\n-        attention_multiplier (`float`, *optional*, defaults to 0.25):\n-            Multiplier coefficient used to define the hidden dimension of the attention layers. 0.25 means that\n-            0.25*width of the model will be used.\n-        attention_pattern (`str`, *optional*, defaults to `\"enc_dec_with_lyrics\"`):\n-            Which attention pattern to use for the decoder/\n-        attn_dropout (`int`, *optional*, defaults to 0):\n-            Dropout probability for the post-attention layer dropout in the decoder.\n-        attn_res_scale (`bool`, *optional*, defaults to `False`):\n-            Whether or not to scale the residuals in the attention conditioner block.\n-        blocks (`int`, *optional*, defaults to 64):\n-            Number of blocks used in the `block_attn`. A sequence of length seq_len is factored as `[blocks, seq_len //\n-            blocks]` in the `JukeboxAttention` layer.\n-        conv_res_scale (`int`, *optional*):\n-            Whether or not to scale the residuals in the conditioner block. Since the top level prior does not have a\n-            conditioner, the default value is to None and should not be modified.\n-        num_layers (`int`, *optional*, defaults to 72):\n-            Number of layers of the transformer architecture.\n-        emb_dropout (`int`, *optional*, defaults to 0):\n-            Embedding dropout used in the lyric decoder.\n-        encoder_config (`JukeboxPriorConfig`, *optional*) :\n-            Configuration of the encoder which models the prior on the lyrics.\n-        encoder_loss_fraction (`float`, *optional*, defaults to 0.4):\n-            Multiplication factor used in front of the lyric encoder loss.\n-        hidden_size (`int`, *optional*, defaults to 2048):\n-            Hidden dimension of the attention layers.\n-        init_scale (`float`, *optional*, defaults to 0.2):\n-            Initialization scales for the prior modules.\n-        is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n-            Whether or not the prior is an encoder-decoder model. In case it is not, and `nb_relevant_lyric_tokens` is\n-            greater than 0, the `encoder` args should be specified for the lyric encoding.\n-        mask (`bool`, *optional*, defaults to `False`):\n-            Whether or not to mask the previous positions in the attention.\n-        max_duration (`int`, *optional*, defaults to 600):\n-            Maximum supported duration of the generated song in seconds.\n-        max_nb_genres (`int`, *optional*, defaults to 1):\n-            Maximum number of genres that can be used to condition the model.\n-        merged_decoder (`bool`, *optional*, defaults to `True`):\n-            Whether or not the decoder and the encoder inputs are merged. This is used for the separated\n-            encoder-decoder architecture\n-        metadata_conditioning (`bool`, *optional*, defaults to `True)`:\n-            Whether or not to condition on the artist and genre metadata.\n-        metadata_dims (`List[int]`, *optional*, defaults to `[604, 7898]`):\n-            Number of genres and the number of artists that were used to train the embedding layers of the prior\n-            models.\n-        min_duration (`int`, *optional*, defaults to 0):\n-            Minimum duration of the generated audio on which the model was trained.\n-        mlp_multiplier (`float`, *optional*, defaults to 1.0):\n-            Multiplier coefficient used to define the hidden dimension of the MLP layers. 0.25 means that 0.25*width of\n-            the model will be used.\n-        music_vocab_size (`int`, *optional*, defaults to 2048):\n-            Number of different music tokens. Should be similar to the `JukeboxVQVAEConfig.nb_discrete_codes`.\n-        n_ctx (`int`, *optional*, defaults to 6144):\n-            Number of context tokens for each prior. The context tokens are the music tokens that are attended to when\n-            generating music tokens.\n-        n_heads (`int`, *optional*, defaults to 2):\n-                Number of attention heads.\n-        nb_relevant_lyric_tokens (`int`, *optional*, defaults to 384):\n-            Number of lyric tokens that are used when sampling a single window of length `n_ctx`\n-        res_conv_depth (`int`, *optional*, defaults to 3):\n-            Depth of the `JukeboxDecoderConvBock` used to upsample the previously sampled audio in the\n-            `JukeboxMusicTokenConditioner`.\n-        res_conv_width (`int`, *optional*, defaults to 128):\n-            Width of the `JukeboxDecoderConvBock` used to upsample the previously sampled audio in the\n-            `JukeboxMusicTokenConditioner`.\n-        res_convolution_multiplier (`int`, *optional*, defaults to 1):\n-            Multiplier used to scale the `hidden_dim` of the `JukeboxResConv1DBlock`.\n-        res_dilation_cycle (`int`, *optional*):\n-            Dilation cycle used to define the `JukeboxMusicTokenConditioner`. Usually similar to the ones used in the\n-            corresponding level of the VQVAE. The first prior does not use it as it is not conditioned on upper level\n-            tokens.\n-        res_dilation_growth_rate (`int`, *optional*, defaults to 1):\n-            Dilation grow rate used between each convolutionnal block of the `JukeboxMusicTokenConditioner`\n-        res_downs_t (`List[int]`, *optional*, defaults to `[3, 2, 2]`):\n-            Downsampling rates used in the audio conditioning network\n-        res_strides_t (`List[int]`, *optional*, defaults to `[2, 2, 2]`):\n-            Striding used in the audio conditioning network\n-        resid_dropout (`int`, *optional*, defaults to 0):\n-            Residual dropout used in the attention pattern.\n-        sampling_rate (`int`, *optional*, defaults to 44100):\n-            Sampling rate used for training.\n-        spread (`int`, *optional*):\n-            Spread used in the `summary_spread_attention` pattern\n-        timing_dims (`int`, *optional*, defaults to 64):\n-            Dimension of the timing embedding.\n-        zero_out (`bool`, *optional*, defaults to `False`):\n-            Whether or not to zero out convolution weights when initializing.\n-    \"\"\"\n-\n-    model_type = \"jukebox_prior\"\n-    attribute_map = {\n-        \"max_position_embeddings\": \"n_positions\",\n-        \"num_attention_heads\": \"n_head\",\n-    }\n-\n-    def __init__(\n-        self,\n-        act_fn=\"quick_gelu\",\n-        level=0,\n-        alignment_head=2,\n-        alignment_layer=68,\n-        attention_multiplier=0.25,\n-        attention_pattern=\"enc_dec_with_lyrics\",\n-        attn_dropout=0,\n-        attn_res_scale=False,\n-        blocks=64,\n-        conv_res_scale=None,\n-        num_layers=72,\n-        emb_dropout=0,\n-        encoder_config=None,\n-        encoder_loss_fraction=0.4,\n-        hidden_size=2048,\n-        init_scale=0.2,\n-        is_encoder_decoder=True,\n-        lyric_vocab_size=80,\n-        mask=False,\n-        max_duration=600,\n-        max_nb_genres=1,\n-        merged_decoder=True,\n-        metadata_conditioning=True,\n-        metadata_dims=[604, 7898],\n-        min_duration=0,\n-        mlp_multiplier=1.0,\n-        music_vocab_size=2048,\n-        n_ctx=6144,\n-        n_heads=2,\n-        nb_relevant_lyric_tokens=384,\n-        res_conv_depth=3,\n-        res_conv_width=128,\n-        res_convolution_multiplier=1,\n-        res_dilation_cycle=None,\n-        res_dilation_growth_rate=1,\n-        res_downs_t=[3, 2, 2],\n-        res_strides_t=[2, 2, 2],\n-        resid_dropout=0,\n-        sampling_rate=44100,\n-        spread=None,\n-        timing_dims=64,\n-        zero_out=False,\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs)\n-        self.act_fn = act_fn\n-        self.alignment_head = alignment_head\n-        self.alignment_layer = alignment_layer\n-        self.attention_multiplier = attention_multiplier\n-        self.attention_pattern = attention_pattern\n-        self.attn_dropout = attn_dropout\n-        self.attn_res_scale = attn_res_scale\n-        self.blocks = blocks\n-        self.conv_res_scale = conv_res_scale\n-        self.num_layers = num_layers\n-        self.emb_dropout = emb_dropout\n-        self.music_vocab_size = music_vocab_size\n-        if encoder_config is not None:\n-            self.encoder_config = JukeboxPriorConfig(**encoder_config)\n-        else:\n-            self.encoder_config = None\n-        self.encoder_loss_fraction = encoder_loss_fraction\n-        self.init_scale = init_scale\n-        self.is_encoder_decoder = is_encoder_decoder\n-        self.lyric_vocab_size = lyric_vocab_size\n-        self.level = level\n-        self.mask = mask\n-        self.max_duration = max_duration\n-        self.max_nb_genres = max_nb_genres\n-        self.merged_decoder = merged_decoder\n-        self.metadata_conditioning = metadata_conditioning\n-        self.metadata_dims = metadata_dims\n-        self.min_duration = min_duration\n-        self.mlp_multiplier = mlp_multiplier\n-        self.n_ctx = n_ctx\n-        self.n_heads = n_heads\n-        self.nb_relevant_lyric_tokens = nb_relevant_lyric_tokens\n-        self.res_conv_depth = res_conv_depth\n-        self.res_conv_width = res_conv_width\n-        self.res_convolution_multiplier = res_convolution_multiplier\n-        self.res_dilation_cycle = res_dilation_cycle\n-        self.res_dilation_growth_rate = res_dilation_growth_rate\n-        self.res_downs_t = res_downs_t\n-        self.res_strides_t = res_strides_t\n-        self.resid_dropout = resid_dropout\n-        self.sampling_rate = sampling_rate\n-        self.spread = spread\n-        self.timing_dims = timing_dims\n-        self.hidden_size = hidden_size\n-        self.zero_out = zero_out\n-\n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], level=0, **kwargs):\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the prior config dict if we are loading from JukeboxConfig\n-        if config_dict.get(\"model_type\") == \"jukebox\":\n-            config_dict = config_dict[f\"prior_{level}\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n-\n-class JukeboxVQVAEConfig(PreTrainedConfig):\n-    \"\"\"\n-    This is the configuration class to store the configuration of a [`JukeboxVQVAE`]. It is used to instantiate a\n-    `JukeboxVQVAE` according to the specified arguments, defining the model architecture. Instantiating a configuration\n-    with the defaults will yield a similar configuration to that of the VQVAE from\n-    [openai/jukebox-1b-lyrics](https://huggingface.co/openai/jukebox-1b-lyrics) architecture.\n-\n-    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PreTrainedConfig`] for more information.\n-\n-    Args:\n-        act_fn (`str`, *optional*, defaults to `\"relu\"`):\n-            Activation function of the model.\n-        nb_discrete_codes (`int`, *optional*, defaults to 2048):\n-            Number of codes of the VQVAE.\n-        commit (`float`, *optional*, defaults to 0.02):\n-            Commit loss multiplier.\n-        conv_input_shape (`int`, *optional*, defaults to 1):\n-            Number of audio channels.\n-        conv_res_scale (`bool`, *optional*, defaults to `False`):\n-            Whether or not to scale the residuals of the `JukeboxResConv1DBlock`.\n-        embed_dim (`int`, *optional*, defaults to 64):\n-            Embedding dimension of the codebook vectors.\n-        hop_fraction (`List[int]`, *optional*, defaults to `[0.125, 0.5, 0.5]`):\n-            Fraction of non-intersecting window used when continuing the sampling process.\n-        levels (`int`, *optional*, defaults to 3):\n-            Number of hierarchical levels that used in the VQVAE.\n-        lmu (`float`, *optional*, defaults to 0.99):\n-            Used in the codebook update, exponential moving average coefficient. For more detail refer to Appendix A.1\n-            of the original [VQVAE paper](https://huggingface.co/papers/1711.00937v2.pdf)\n-        multipliers (`List[int]`, *optional*, defaults to `[2, 1, 1]`):\n-            Depth and width multipliers used for each level. Used on the `res_conv_width` and `res_conv_depth`\n-        res_conv_depth (`int`, *optional*, defaults to 4):\n-            Depth of the encoder and decoder block. If no `multipliers` are used, this is the same for each level.\n-        res_conv_width (`int`, *optional*, defaults to 32):\n-            Width of the encoder and decoder block. If no `multipliers` are used, this is the same for each level.\n-        res_convolution_multiplier (`int`, *optional*, defaults to 1):\n-            Scaling factor of the hidden dimension used in the `JukeboxResConv1DBlock`.\n-        res_dilation_cycle (`int`, *optional*):\n-            Dilation cycle value used in the `JukeboxResnet`. If an int is used, each new Conv1 block will have a depth\n-            reduced by a power of `res_dilation_cycle`.\n-        res_dilation_growth_rate (`int`, *optional*, defaults to 3):\n-            Resnet dilation growth rate used in the VQVAE (dilation_growth_rate ** depth)\n-        res_downs_t (`List[int]`, *optional*, defaults to `[3, 2, 2]`):\n-            Downsampling rate for each level of the hierarchical VQ-VAE.\n-        res_strides_t (`List[int]`, *optional*, defaults to `[2, 2, 2]`):\n-            Stride used for each level of the hierarchical VQ-VAE.\n-        sample_length (`int`, *optional*, defaults to 1058304):\n-            Provides the max input shape of the VQVAE. Is used to compute the input shape of each level.\n-        init_scale (`float`, *optional*, defaults to 0.2):\n-            Initialization scale.\n-        zero_out (`bool`, *optional*, defaults to `False`):\n-            Whether or not to zero out convolution weights when initializing.\n-    \"\"\"\n-\n-    model_type = \"jukebox_vqvae\"\n-\n-    def __init__(\n-        self,\n-        act_fn=\"relu\",\n-        nb_discrete_codes=2048,\n-        commit=0.02,\n-        conv_input_shape=1,\n-        conv_res_scale=False,\n-        embed_dim=64,\n-        hop_fraction=[0.125, 0.5, 0.5],\n-        levels=3,\n-        lmu=0.99,\n-        multipliers=[2, 1, 1],\n-        res_conv_depth=4,\n-        res_conv_width=32,\n-        res_convolution_multiplier=1,\n-        res_dilation_cycle=None,\n-        res_dilation_growth_rate=3,\n-        res_downs_t=[3, 2, 2],\n-        res_strides_t=[2, 2, 2],\n-        sample_length=1058304,\n-        init_scale=0.2,\n-        zero_out=False,\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs)\n-        self.hop_fraction = hop_fraction\n-        self.conv_input_shape = conv_input_shape\n-        self.sample_length = sample_length\n-\n-        # VQVAE parameters (all used)\n-        self.levels = levels\n-        self.embed_dim = embed_dim\n-        self.nb_discrete_codes = nb_discrete_codes\n-        self.res_conv_width = res_conv_width\n-        self.res_conv_depth = res_conv_depth\n-        self.res_convolution_multiplier = res_convolution_multiplier\n-        self.res_dilation_growth_rate = res_dilation_growth_rate\n-        self.res_dilation_cycle = res_dilation_cycle\n-        self.multipliers = multipliers\n-        self.res_downs_t = res_downs_t\n-        self.res_strides_t = res_strides_t\n-        self.lmu = lmu\n-        self.commit = commit\n-        self.conv_res_scale = conv_res_scale\n-        self.act_fn = act_fn\n-        self.init_scale = init_scale\n-        self.zero_out = zero_out\n-\n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs):\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the text config dict if we are loading from CLIPConfig\n-        if config_dict.get(\"model_type\") == \"jukebox\":\n-            config_dict = config_dict[\"vqvae_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n-\n-class JukeboxConfig(PreTrainedConfig):\n-    \"\"\"\n-    This is the configuration class to store the configuration of a [`JukeboxModel`].\n-\n-    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PreTrainedConfig`] for more information. Instantiating a configuration with the defaults will\n-    yield a similar configuration to that of\n-    [openai/jukebox-1b-lyrics](https://huggingface.co/openai/jukebox-1b-lyrics) architecture.\n-\n-\n-    The downsampling and stride are used to determine downsampling of the input sequence. For example, downsampling =\n-    (5,3), and strides = (2, 2) will downsample the audio by 2^5 = 32 to get the first level of codes, and 2**8 = 256\n-    to get the second level codes. This is mostly true for training the top level prior and the upsamplers.\n-\n-    Args:\n-        vqvae_config (`JukeboxVQVAEConfig`, *optional*):\n-            Configuration for the `JukeboxVQVAE` model.\n-        prior_config_list (`List[JukeboxPriorConfig]`, *optional*):\n-            List of the configs for each of the `JukeboxPrior` of the model. The original architecture uses 3 priors.\n-        nb_priors (`int`, *optional*, defaults to 3):\n-            Number of prior models that will sequentially sample tokens. Each prior is conditional auto regressive\n-            (decoder) model, apart from the top prior, which can include a lyric encoder. The available models were\n-            trained using a top prior and 2 upsampler priors.\n-        sampling_rate (`int`, *optional*, defaults to 44100):\n-            Sampling rate of the raw audio.\n-        timing_dims (`int`, *optional*, defaults to 64):\n-            Dimensions of the JukeboxRangeEmbedding layer which is equivalent to traditional positional embedding\n-            layer. The timing embedding layer converts the absolute and relative position in the currently sampled\n-            audio to a tensor of length `timing_dims` that will be added to the music tokens.\n-        min_duration (`int`, *optional*, defaults to 0):\n-            Minimum duration of the audios to generate\n-        max_duration (`float`, *optional*, defaults to 600.0):\n-            Maximum duration of the audios to generate\n-        max_nb_genres (`int`, *optional*, defaults to 5):\n-            Maximum number of genres that can be used to condition a single sample.\n-        metadata_conditioning (`bool`, *optional*, defaults to `True`):\n-            Whether or not to use metadata conditioning, corresponding to the artist, the genre and the min/maximum\n-            duration.\n-\n-    Example:\n-\n-    ```python\n-    >>> from transformers import JukeboxModel, JukeboxConfig\n-\n-    >>> # Initializing a Jukebox configuration\n-    >>> configuration = JukeboxConfig()\n-\n-    >>> # Initializing a model from the configuration\n-    >>> model = JukeboxModel(configuration)\n-\n-    >>> # Accessing the model configuration\n-    >>> configuration = model.config\n-    ```\n-    \"\"\"\n-\n-    model_type = \"jukebox\"\n-\n-    def __init__(\n-        self,\n-        vqvae_config=None,\n-        prior_config_list=None,\n-        nb_priors=3,\n-        sampling_rate=44100,\n-        timing_dims=64,\n-        min_duration=0,\n-        max_duration=600.0,\n-        max_nb_genres=5,\n-        metadata_conditioning=True,\n-        **kwargs,\n-    ):\n-        if vqvae_config is None:\n-            vqvae_config = JukeboxVQVAEConfig()\n-            logger.info(\"vqvae_config is None. initializing the JukeboxVQVAE with default values.\")\n-        elif isinstance(vqvae_config, dict):\n-            vqvae_config = JukeboxVQVAEConfig(**vqvae_config)\n-        self.vqvae_config = vqvae_config\n-\n-        if prior_config_list is not None and isinstance(prior_config_list[0], dict):\n-            prior_configs = [JukeboxPriorConfig(**prior_config) for prior_config in prior_config_list]\n-        elif prior_config_list is None:\n-            prior_configs = []\n-            for prior_idx in range(nb_priors):\n-                prior_config = kwargs.pop(f\"prior_{prior_idx}\", None)\n-                if prior_config is None:\n-                    prior_config = {}\n-                    logger.info(\n-                        f\"prior_{prior_idx}'s  config is None. Initializing the JukeboxPriorConfig list with default\"\n-                        \" values.\"\n-                    )\n-                prior_configs.append(JukeboxPriorConfig(**prior_config))\n-        self.prior_configs = prior_configs\n-\n-        self.hop_fraction = self.vqvae_config.hop_fraction\n-        self.nb_priors = nb_priors\n-\n-        # Metadata conditioning\n-        self.max_nb_genres = max_nb_genres\n-        self.sampling_rate = sampling_rate\n-        self.timing_dims = timing_dims\n-        self.min_duration = min_duration\n-        self.max_duration = max_duration\n-        self.metadata_conditioning = metadata_conditioning\n-\n-        super().__init__(**kwargs)\n-\n-    def to_dict(self):\n-        # Override the default to_dict to apply to_dict to the list of prior configs.\n-        result = super().to_dict()\n-        result[\"prior_config_list\"] = [config.to_dict() for config in result.pop(\"prior_configs\")]\n-        return result\n-\n-\n-__all__ = [\"JukeboxConfig\", \"JukeboxPriorConfig\", \"JukeboxVQVAEConfig\"]"
        },
        {
            "sha": "29763daaa30a145b5b2156cc95e515a38a291cb5",
            "filename": "src/transformers/models/deprecated/jukebox/convert_jukebox.py",
            "status": "removed",
            "additions": 0,
            "deletions": 279,
            "changes": 279,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconvert_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconvert_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconvert_jukebox.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,279 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Convert Jukebox checkpoints\"\"\"\n-\n-import argparse\n-import json\n-import os\n-from pathlib import Path\n-\n-import requests\n-import torch\n-\n-from transformers import JukeboxConfig, JukeboxModel\n-from transformers.utils import logging\n-\n-\n-logging.set_verbosity_info()\n-logger = logging.get_logger(__name__)\n-\n-\n-PREFIX = \"https://openaipublic.azureedge.net/jukebox/models/\"\n-MODEL_MAPPING = {\n-    \"jukebox-1b-lyrics\": [\n-        \"5b/vqvae.pth.tar\",\n-        \"5b/prior_level_0.pth.tar\",\n-        \"5b/prior_level_1.pth.tar\",\n-        \"1b_lyrics/prior_level_2.pth.tar\",\n-    ],\n-    \"jukebox-5b-lyrics\": [\n-        \"5b/vqvae.pth.tar\",\n-        \"5b/prior_level_0.pth.tar\",\n-        \"5b/prior_level_1.pth.tar\",\n-        \"5b_lyrics/prior_level_2.pth.tar\",\n-    ],\n-}\n-\n-\n-def replace_key(key):\n-    if key.endswith(\".model.1.bias\") and len(key.split(\".\")) > 10:\n-        key = key.replace(\".model.1.bias\", \".conv1d_1.bias\")\n-    elif key.endswith(\".model.1.weight\") and len(key.split(\".\")) > 10:\n-        key = key.replace(\".model.1.weight\", \".conv1d_1.weight\")\n-    elif key.endswith(\".model.3.bias\") and len(key.split(\".\")) > 10:\n-        key = key.replace(\".model.3.bias\", \".conv1d_2.bias\")\n-    elif key.endswith(\".model.3.weight\") and len(key.split(\".\")) > 10:\n-        key = key.replace(\".model.3.weight\", \".conv1d_2.weight\")\n-\n-    if \"conditioner_blocks.0.\" in key:\n-        key = key.replace(\"conditioner_blocks.0\", \"conditioner_blocks\")\n-\n-    if \"prime_prior\" in key:\n-        key = key.replace(\"prime_prior\", \"encoder\")\n-\n-    if \".emb.\" in key and \"total\" not in key and \"absolute\" not in key and \"relative\" not in key:\n-        key = key.replace(\".emb.\", \".\")\n-\n-    if key.endswith(\"k\"):  # replace vqvae.X.k with vqvae.X.codebook\n-        return key.replace(\".k\", \".codebook\")\n-    if \"y_emb.\" in key:\n-        return key.replace(\"y_emb.\", \"metadata_embedding.\")\n-\n-    if \"x_emb.emb.\" in key:\n-        key = key.replace(\"0.x_emb.emb\", \"embed_tokens\")\n-\n-    if \"prime_state_ln\" in key:\n-        return key.replace(\"prime_state_ln\", \"encoder.final_layer_norm\")\n-    if \".ln\" in key:\n-        return key.replace(\".ln\", \".layer_norm\")\n-    if \"_ln\" in key:\n-        return key.replace(\"_ln\", \"_layer_norm\")\n-\n-    if \"prime_state_proj\" in key:\n-        return key.replace(\"prime_state_proj\", \"encoder.proj_in\")\n-    if \"prime_x_out\" in key:\n-        return key.replace(\"prime_x_out\", \"encoder.lm_head\")\n-    if \"prior.x_out\" in key:\n-        return key.replace(\"x_out\", \"fc_proj_out\")\n-    if \"x_emb\" in key:\n-        return key.replace(\"x_emb\", \"embed_tokens\")\n-\n-    return key\n-\n-\n-def fix_jukebox_keys(state_dict, model_state_dict, key_prefix, mapping):\n-    new_dict = {}\n-    import re\n-\n-    re_encoder_block_conv_in = re.compile(r\"encoders.(\\d*).level_blocks.(\\d*).model.(\\d*).(\\d).(bias|weight)\")\n-    re_encoder_block_resnet = re.compile(\n-        r\"encoders.(\\d*).level_blocks.(\\d*).model.(\\d*).(\\d).model.(\\d*).model.(\\d*).(bias|weight)\"\n-    )\n-    re_encoder_block_proj_out = re.compile(r\"encoders.(\\d*).level_blocks.(\\d*).model.(\\d*).(bias|weight)\")\n-\n-    re_decoder_block_conv_out = re.compile(r\"decoders.(\\d*).level_blocks.(\\d*).model.(\\d*).(\\d).(bias|weight)\")\n-    re_decoder_block_resnet = re.compile(\n-        r\"decoders.(\\d*).level_blocks.(\\d*).model.(\\d*).(\\d).model.(\\d*).model.(\\d*).(bias|weight)\"\n-    )\n-    re_decoder_block_proj_in = re.compile(r\"decoders.(\\d*).level_blocks.(\\d*).model.(\\d*).(bias|weight)\")\n-\n-    re_prior_cond_conv_out = re.compile(r\"conditioner_blocks.(\\d*).cond.model.(\\d*).(\\d).(bias|weight)\")\n-    re_prior_cond_resnet = re.compile(\n-        r\"conditioner_blocks.(\\d*).cond.model.(\\d*).(\\d).model.(\\d*).model.(\\d*).(bias|weight)\"\n-    )\n-    re_prior_cond_proj_in = re.compile(r\"conditioner_blocks.(\\d*).cond.model.(\\d*).(bias|weight)\")\n-\n-    for original_key, value in state_dict.items():\n-        # rename vqvae.encoder keys\n-        if re_encoder_block_conv_in.fullmatch(original_key):\n-            regex_match = re_encoder_block_conv_in.match(original_key)\n-            groups = regex_match.groups()\n-            block_index = int(groups[2]) * 2 + int(groups[3])\n-            re_new_key = f\"encoders.{groups[0]}.level_blocks.{groups[1]}.downsample_block.{block_index}.{groups[-1]}\"\n-            key = re_encoder_block_conv_in.sub(re_new_key, original_key)\n-\n-        elif re_encoder_block_resnet.fullmatch(original_key):\n-            regex_match = re_encoder_block_resnet.match(original_key)\n-            groups = regex_match.groups()\n-            block_index = int(groups[2]) * 2 + int(groups[3])\n-            conv_index = {\"1\": 1, \"3\": 2}[groups[-2]]\n-            prefix = f\"encoders.{groups[0]}.level_blocks.{groups[1]}.downsample_block.{block_index}.\"\n-            resnet_block = f\"resnet_block.{groups[-3]}.conv1d_{conv_index}.{groups[-1]}\"\n-            re_new_key = prefix + resnet_block\n-            key = re_encoder_block_resnet.sub(re_new_key, original_key)\n-\n-        elif re_encoder_block_proj_out.fullmatch(original_key):\n-            regex_match = re_encoder_block_proj_out.match(original_key)\n-            groups = regex_match.groups()\n-            re_new_key = f\"encoders.{groups[0]}.level_blocks.{groups[1]}.proj_out.{groups[-1]}\"\n-            key = re_encoder_block_proj_out.sub(re_new_key, original_key)\n-\n-        # rename vqvae.decoder keys\n-        elif re_decoder_block_conv_out.fullmatch(original_key):\n-            regex_match = re_decoder_block_conv_out.match(original_key)\n-            groups = regex_match.groups()\n-            block_index = int(groups[2]) * 2 + int(groups[3]) - 2\n-            re_new_key = f\"decoders.{groups[0]}.level_blocks.{groups[1]}.upsample_block.{block_index}.{groups[-1]}\"\n-            key = re_decoder_block_conv_out.sub(re_new_key, original_key)\n-\n-        elif re_decoder_block_resnet.fullmatch(original_key):\n-            regex_match = re_decoder_block_resnet.match(original_key)\n-            groups = regex_match.groups()\n-            block_index = int(groups[2]) * 2 + int(groups[3]) - 2\n-            conv_index = {\"1\": 1, \"3\": 2}[groups[-2]]\n-            prefix = f\"decoders.{groups[0]}.level_blocks.{groups[1]}.upsample_block.{block_index}.\"\n-            resnet_block = f\"resnet_block.{groups[-3]}.conv1d_{conv_index}.{groups[-1]}\"\n-            re_new_key = prefix + resnet_block\n-            key = re_decoder_block_resnet.sub(re_new_key, original_key)\n-\n-        elif re_decoder_block_proj_in.fullmatch(original_key):\n-            regex_match = re_decoder_block_proj_in.match(original_key)\n-            groups = regex_match.groups()\n-            re_new_key = f\"decoders.{groups[0]}.level_blocks.{groups[1]}.proj_in.{groups[-1]}\"\n-            key = re_decoder_block_proj_in.sub(re_new_key, original_key)\n-\n-        # rename prior cond.model to upsampler.upsample_block and resnet\n-        elif re_prior_cond_conv_out.fullmatch(original_key):\n-            regex_match = re_prior_cond_conv_out.match(original_key)\n-            groups = regex_match.groups()\n-            block_index = int(groups[1]) * 2 + int(groups[2]) - 2\n-            re_new_key = f\"conditioner_blocks.upsampler.upsample_block.{block_index}.{groups[-1]}\"\n-            key = re_prior_cond_conv_out.sub(re_new_key, original_key)\n-\n-        elif re_prior_cond_resnet.fullmatch(original_key):\n-            regex_match = re_prior_cond_resnet.match(original_key)\n-            groups = regex_match.groups()\n-            block_index = int(groups[1]) * 2 + int(groups[2]) - 2\n-            conv_index = {\"1\": 1, \"3\": 2}[groups[-2]]\n-            prefix = f\"conditioner_blocks.upsampler.upsample_block.{block_index}.\"\n-            resnet_block = f\"resnet_block.{groups[-3]}.conv1d_{conv_index}.{groups[-1]}\"\n-            re_new_key = prefix + resnet_block\n-            key = re_prior_cond_resnet.sub(re_new_key, original_key)\n-\n-        elif re_prior_cond_proj_in.fullmatch(original_key):\n-            regex_match = re_prior_cond_proj_in.match(original_key)\n-            groups = regex_match.groups()\n-            re_new_key = f\"conditioner_blocks.upsampler.proj_in.{groups[-1]}\"\n-            key = re_prior_cond_proj_in.sub(re_new_key, original_key)\n-\n-        # keep original key\n-        else:\n-            key = original_key\n-\n-        key = replace_key(key)\n-\n-        if f\"{key_prefix}.{key}\" not in model_state_dict or key is None:\n-            print(f\"failed converting {original_key} to {key}, does not match\")\n-\n-        # handle mismatched shape\n-        elif value.shape != model_state_dict[f\"{key_prefix}.{key}\"].shape:\n-            val = model_state_dict[f\"{key_prefix}.{key}\"]\n-            print(f\"{original_key}-> {key} : \\nshape {val.shape} and {value.shape}, do not match\")\n-            key = original_key\n-\n-        mapping[key] = original_key\n-        new_dict[key] = value\n-\n-    return new_dict\n-\n-\n-@torch.no_grad()\n-def convert_openai_checkpoint(model_name=None, pytorch_dump_folder_path=None):\n-    \"\"\"\n-    Copy/paste/tweak model's weights to our Jukebox structure.\n-    \"\"\"\n-    for file in MODEL_MAPPING[model_name]:\n-        if not os.path.isfile(f\"{pytorch_dump_folder_path}/{file.split('/')[-1]}\"):\n-            r = requests.get(f\"{PREFIX}{file}\", allow_redirects=True)\n-            os.makedirs(f\"{pytorch_dump_folder_path}/\", exist_ok=True)\n-            open(f\"{pytorch_dump_folder_path}/{file.split('/')[-1]}\", \"wb\").write(r.content)\n-\n-    model_to_convert = MODEL_MAPPING[model_name.split(\"/\")[-1]]\n-\n-    config = JukeboxConfig.from_pretrained(model_name)\n-    model = JukeboxModel(config)\n-\n-    weight_dict = []\n-    mapping = {}\n-    for i, dict_name in enumerate(model_to_convert):\n-        old_dic = torch.load(f\"{pytorch_dump_folder_path}/{dict_name.split('/')[-1]}\", weights_only=True)[\"model\"]\n-\n-        new_dic = {}\n-        for k in old_dic:\n-            if k.endswith(\".b\"):\n-                new_dic[k.replace(\"b\", \"bias\")] = old_dic[k]\n-            elif k.endswith(\".w\"):\n-                new_dic[k.replace(\"w\", \"weight\")] = old_dic[k]\n-            elif \"level_2\" not in dict_name and \"cond.model.\" in k:\n-                new_dic[k.replace(\".blocks.\", \".model.\")] = old_dic[k]\n-            else:\n-                new_dic[k] = old_dic[k]\n-\n-        key_prefix = \"vqvae\" if i == 0 else f\"priors.{3 - i}\"\n-        new_dic = fix_jukebox_keys(new_dic, model.state_dict(), key_prefix, mapping)\n-        weight_dict.append(new_dic)\n-\n-    vqvae_state_dict = weight_dict.pop(0)\n-    model.vqvae.load_state_dict(vqvae_state_dict)\n-    for i in range(len(weight_dict)):\n-        model.priors[i].load_state_dict(weight_dict[2 - i])\n-\n-    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n-    with open(f\"{pytorch_dump_folder_path}/mapping.json\", \"w\") as txtfile:\n-        json.dump(mapping, txtfile)\n-\n-    print(f\"Saving model {model_name} to {pytorch_dump_folder_path}\")\n-    model.save_pretrained(pytorch_dump_folder_path)\n-\n-    return weight_dict\n-\n-\n-if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser()\n-    # Required parameters\n-    parser.add_argument(\n-        \"--model_name\",\n-        default=\"jukebox-5b-lyrics\",\n-        type=str,\n-        help=\"Name of the model you'd like to convert.\",\n-    )\n-    parser.add_argument(\n-        \"--pytorch_dump_folder_path\",\n-        default=\"jukebox-5b-lyrics-converted\",\n-        type=str,\n-        help=\"Path to the output PyTorch model directory.\",\n-    )\n-    args = parser.parse_args()\n-    convert_openai_checkpoint(args.model_name, args.pytorch_dump_folder_path)"
        },
        {
            "sha": "e71d82b745e3942e626576eeee44cec2fd731330",
            "filename": "src/transformers/models/deprecated/jukebox/modeling_jukebox.py",
            "status": "removed",
            "additions": 0,
            "deletions": 2674,
            "changes": 2674,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "b9948c5c354e2c090db0236ed228f27af45b62b2",
            "filename": "src/transformers/models/deprecated/jukebox/tokenization_jukebox.py",
            "status": "removed",
            "additions": 0,
            "deletions": 390,
            "changes": 390,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Ftokenization_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Ftokenization_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Ftokenization_jukebox.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,390 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The Open AI Team Authors and The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Tokenization classes for OpenAI Jukebox.\"\"\"\n-\n-import json\n-import os\n-import re\n-import unicodedata\n-from json.encoder import INFINITY\n-from typing import Any, Optional, Union\n-\n-import numpy as np\n-import regex\n-\n-from ....tokenization_utils import AddedToken, PreTrainedTokenizer\n-from ....tokenization_utils_base import BatchEncoding\n-from ....utils import TensorType, is_torch_available, logging\n-from ....utils.generic import is_numpy_array\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\n-    \"artists_file\": \"artists.json\",\n-    \"lyrics_file\": \"lyrics.json\",\n-    \"genres_file\": \"genres.json\",\n-}\n-\n-\n-class JukeboxTokenizer(PreTrainedTokenizer):\n-    \"\"\"\n-    Constructs a Jukebox tokenizer. Jukebox can be conditioned on 3 different inputs :\n-        - Artists, unique ids are associated to each artist from the provided dictionary.\n-        - Genres, unique ids are associated to each genre from the provided dictionary.\n-        - Lyrics, character based tokenization. Must be initialized with the list of characters that are inside the\n-        vocabulary.\n-\n-    This tokenizer does not require training. It should be able to process a different number of inputs:\n-    as the conditioning of the model can be done on the three different queries. If None is provided, defaults values will be used.:\n-\n-    Depending on the number of genres on which the model should be conditioned (`n_genres`).\n-    ```python\n-    >>> from transformers import JukeboxTokenizer\n-\n-    >>> tokenizer = JukeboxTokenizer.from_pretrained(\"openai/jukebox-1b-lyrics\")\n-    >>> tokenizer(\"Alan Jackson\", \"Country Rock\", \"old town road\")[\"input_ids\"]\n-    [tensor([[   0,    0,    0, 6785,  546,   41,   38,   30,   76,   46,   41,   49,\n-               40,   76,   44,   41,   27,   30]]), tensor([[  0,   0,   0, 145,   0]]), tensor([[  0,   0,   0, 145,   0]])]\n-    ```\n-\n-    You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer or when you\n-    call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.\n-\n-    <Tip>\n-\n-    If nothing is provided, the genres and the artist will either be selected randomly or set to None\n-\n-    </Tip>\n-\n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to:\n-    this superclass for more information regarding those methods.\n-\n-    However the code does not allow that and only supports composing from various genres.\n-\n-    Args:\n-        artists_file (`str`):\n-            Path to the vocabulary file which contains a mapping between artists and ids. The default file supports\n-            both \"v2\" and \"v3\"\n-        genres_file (`str`):\n-            Path to the vocabulary file which contain a mapping between genres and ids.\n-        lyrics_file (`str`):\n-            Path to the vocabulary file which contains the accepted characters for the lyrics tokenization.\n-        version (`list[str]`, `optional`, default to `[\"v3\", \"v2\", \"v2\"]`) :\n-            List of the tokenizer versions. The `5b-lyrics`'s top level prior model was trained using `v3` instead of\n-            `v2`.\n-        n_genres (`int`, `optional`, defaults to 1):\n-            Maximum number of genres to use for composition.\n-        max_n_lyric_tokens (`int`, `optional`, defaults to 512):\n-            Maximum number of lyric tokens to keep.\n-        unk_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    model_input_names = [\"input_ids\", \"attention_mask\"]\n-\n-    def __init__(\n-        self,\n-        artists_file,\n-        genres_file,\n-        lyrics_file,\n-        version=[\"v3\", \"v2\", \"v2\"],\n-        max_n_lyric_tokens=512,\n-        n_genres=5,\n-        unk_token=\"<|endoftext|>\",\n-        **kwargs,\n-    ):\n-        unk_token = AddedToken(unk_token, lstrip=False, rstrip=False) if isinstance(unk_token, str) else unk_token\n-        self.version = version\n-        self.max_n_lyric_tokens = max_n_lyric_tokens\n-        self.n_genres = n_genres\n-        self._added_tokens_decoder = {0: unk_token}\n-\n-        with open(artists_file, encoding=\"utf-8\") as vocab_handle:\n-            self.artists_encoder = json.load(vocab_handle)\n-\n-        with open(genres_file, encoding=\"utf-8\") as vocab_handle:\n-            self.genres_encoder = json.load(vocab_handle)\n-\n-        with open(lyrics_file, encoding=\"utf-8\") as vocab_handle:\n-            self.lyrics_encoder = json.load(vocab_handle)\n-\n-        oov = r\"[^A-Za-z0-9.,:;!?\\-'\\\"()\\[\\] \\t\\n]+\"\n-        # In v2, we had a n_vocab=80 and in v3 we missed + and so n_vocab=79 of characters.\n-        if len(self.lyrics_encoder) == 79:\n-            oov = oov.replace(r\"\\-'\", r\"\\-+'\")\n-\n-        self.out_of_vocab = regex.compile(oov)\n-        self.artists_decoder = {v: k for k, v in self.artists_encoder.items()}\n-        self.genres_decoder = {v: k for k, v in self.genres_encoder.items()}\n-        self.lyrics_decoder = {v: k for k, v in self.lyrics_encoder.items()}\n-        super().__init__(\n-            unk_token=unk_token,\n-            n_genres=n_genres,\n-            version=version,\n-            max_n_lyric_tokens=max_n_lyric_tokens,\n-            **kwargs,\n-        )\n-\n-    @property\n-    def vocab_size(self):\n-        return len(self.artists_encoder) + len(self.genres_encoder) + len(self.lyrics_encoder)\n-\n-    def get_vocab(self):\n-        return {\n-            \"artists_encoder\": self.artists_encoder,\n-            \"genres_encoder\": self.genres_encoder,\n-            \"lyrics_encoder\": self.lyrics_encoder,\n-        }\n-\n-    def _convert_token_to_id(self, list_artists, list_genres, list_lyrics):\n-        \"\"\"Converts the artist, genre and lyrics tokens to their index using the vocabulary.\n-        The total_length, offset and duration have to be provided in order to select relevant lyrics and add padding to\n-        the lyrics token sequence.\n-        \"\"\"\n-        artists_id = [self.artists_encoder.get(artist, 0) for artist in list_artists]\n-        for genres in range(len(list_genres)):\n-            list_genres[genres] = [self.genres_encoder.get(genre, 0) for genre in list_genres[genres]]\n-            list_genres[genres] = list_genres[genres] + [-1] * (self.n_genres - len(list_genres[genres]))\n-\n-        lyric_ids = [[self.lyrics_encoder.get(character, 0) for character in list_lyrics[0]], [], []]\n-        return artists_id, list_genres, lyric_ids\n-\n-    def _tokenize(self, lyrics):\n-        \"\"\"\n-        Converts a string into a sequence of tokens (string), using the tokenizer. Split in words for word-based\n-        vocabulary or sub-words for sub-word-based vocabularies (BPE/SentencePieces/WordPieces).\n-\n-        Do NOT take care of added tokens. Only the lyrics are split into character for the character-based vocabulary.\n-        \"\"\"\n-        # only lyrics are not tokenized, but character based is easily handled\n-        return list(lyrics)\n-\n-    def tokenize(self, artist, genre, lyrics, **kwargs):\n-        \"\"\"\n-        Converts three strings in a 3 sequence of tokens using the tokenizer\n-        \"\"\"\n-        artist, genre, lyrics = self.prepare_for_tokenization(artist, genre, lyrics)\n-        lyrics = self._tokenize(lyrics)\n-        return artist, genre, lyrics\n-\n-    def prepare_for_tokenization(\n-        self, artists: str, genres: str, lyrics: str, is_split_into_words: bool = False\n-    ) -> tuple[str, str, str, dict[str, Any]]:\n-        \"\"\"\n-        Performs any necessary transformations before tokenization.\n-\n-        Args:\n-            artist (`str`):\n-                The artist name to prepare. This will mostly lower the string\n-            genres (`str`):\n-                The genre name to prepare. This will mostly lower the string.\n-            lyrics (`str`):\n-                The lyrics to prepare.\n-            is_split_into_words (`bool`, *optional*, defaults to `False`):\n-                Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n-                tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n-                which it will tokenize. This is useful for NER or token classification.\n-        \"\"\"\n-        for idx in range(len(self.version)):\n-            if self.version[idx] == \"v3\":\n-                artists[idx] = artists[idx].lower()\n-                genres[idx] = [genres[idx].lower()]\n-            else:\n-                artists[idx] = self._normalize(artists[idx]) + \".v2\"\n-                genres[idx] = [\n-                    self._normalize(genre) + \".v2\" for genre in genres[idx].split(\"_\")\n-                ]  # split is for the full dictionary with combined genres\n-\n-        if self.version[0] == \"v2\":\n-            self.out_of_vocab = regex.compile(r\"[^A-Za-z0-9.,:;!?\\-'\\\"()\\[\\] \\t\\n]+\")\n-            vocab = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789.,:;!?-+'\\\"()[] \\t\\n\"\n-            self.vocab = {vocab[index]: index + 1 for index in range(len(vocab))}\n-            self.vocab[\"<unk>\"] = 0\n-            self.n_vocab = len(vocab) + 1\n-            self.lyrics_encoder = self.vocab\n-            self.lyrics_decoder = {v: k for k, v in self.vocab.items()}\n-            self.lyrics_decoder[0] = \"\"\n-        else:\n-            self.out_of_vocab = regex.compile(r\"[^A-Za-z0-9.,:;!?\\-+'\\\"()\\[\\] \\t\\n]+\")\n-\n-        lyrics = self._run_strip_accents(lyrics)\n-        lyrics = lyrics.replace(\"\\\\\", \"\\n\")\n-        lyrics = self.out_of_vocab.sub(\"\", lyrics), [], []\n-        return artists, genres, lyrics\n-\n-    def _run_strip_accents(self, text):\n-        \"\"\"Strips accents from a piece of text.\"\"\"\n-        text = unicodedata.normalize(\"NFD\", text)\n-        output = []\n-        for char in text:\n-            cat = unicodedata.category(char)\n-            if cat == \"Mn\":\n-                continue\n-            output.append(char)\n-        return \"\".join(output)\n-\n-    def _normalize(self, text: str) -> str:\n-        \"\"\"\n-        Normalizes the input text. This process is for the genres and the artist\n-\n-        Args:\n-            text (`str`):\n-                Artist or Genre string to normalize\n-        \"\"\"\n-\n-        accepted = (\n-            [chr(i) for i in range(ord(\"a\"), ord(\"z\") + 1)]\n-            + [chr(i) for i in range(ord(\"A\"), ord(\"Z\") + 1)]\n-            + [chr(i) for i in range(ord(\"0\"), ord(\"9\") + 1)]\n-            + [\".\"]\n-        )\n-        accepted = frozenset(accepted)\n-        pattern = re.compile(r\"_+\")\n-        text = \"\".join([c if c in accepted else \"_\" for c in text.lower()])\n-        text = pattern.sub(\"_\", text).strip(\"_\")\n-        return text\n-\n-    def convert_lyric_tokens_to_string(self, lyrics: list[str]) -> str:\n-        return \" \".join(lyrics)\n-\n-    def convert_to_tensors(\n-        self, inputs, tensor_type: Optional[Union[str, TensorType]] = None, prepend_batch_axis: bool = False\n-    ):\n-        \"\"\"\n-        Convert the inner content to tensors.\n-\n-        Args:\n-            tensor_type (`str` or [`~utils.TensorType`], *optional*):\n-                The type of tensors to use. If `str`, should be one of the values of the enum [`~utils.TensorType`]. If\n-                unset, no modification is done.\n-            prepend_batch_axis (`int`, *optional*, defaults to `False`):\n-                Whether or not to add the batch dimension during the conversion.\n-        \"\"\"\n-        # Convert to TensorType\n-        if not isinstance(tensor_type, TensorType):\n-            tensor_type = TensorType(tensor_type)\n-\n-        if tensor_type == TensorType.PYTORCH:\n-            if not is_torch_available():\n-                raise ImportError(\"Unable to convert output to PyTorch tensors format, PyTorch is not installed.\")\n-            import torch\n-\n-            as_tensor = torch.tensor\n-            is_tensor = torch.is_tensor\n-        else:\n-            as_tensor = np.asarray\n-            is_tensor = is_numpy_array\n-\n-        # Do the tensor conversion in batch\n-\n-        try:\n-            if prepend_batch_axis:\n-                inputs = [inputs]\n-\n-            if not is_tensor(inputs):\n-                inputs = as_tensor(inputs)\n-        except:  # noqa E722\n-            raise ValueError(\n-                \"Unable to create tensor, you should probably activate truncation and/or padding \"\n-                \"with 'padding=True' 'truncation=True' to have batched tensors with the same length.\"\n-            )\n-\n-        return inputs\n-\n-    def __call__(self, artist, genres, lyrics=\"\", return_tensors=\"pt\") -> BatchEncoding:\n-        \"\"\"Convert the raw string to a list of token ids\n-\n-        Args:\n-            artist (`str`):\n-                Name of the artist.\n-            genres (`str`):\n-                List of genres that will be mixed to condition the audio\n-            lyrics (`str`, *optional*, defaults to `\"\"`):\n-                Lyrics used to condition the generation\n-        \"\"\"\n-        input_ids = [0, 0, 0]\n-        artist = [artist] * len(self.version)\n-        genres = [genres] * len(self.version)\n-\n-        artists_tokens, genres_tokens, lyrics_tokens = self.tokenize(artist, genres, lyrics)\n-        artists_id, genres_ids, full_tokens = self._convert_token_to_id(artists_tokens, genres_tokens, lyrics_tokens)\n-\n-        attention_masks = [-INFINITY] * len(full_tokens[-1])\n-        input_ids = [\n-            self.convert_to_tensors(\n-                [input_ids + [artists_id[i]] + genres_ids[i] + full_tokens[i]], tensor_type=return_tensors\n-            )\n-            for i in range(len(self.version))\n-        ]\n-        return BatchEncoding({\"input_ids\": input_ids, \"attention_masks\": attention_masks})\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        \"\"\"\n-        Saves the tokenizer's vocabulary dictionary to the provided save_directory.\n-\n-        Args:\n-            save_directory (`str`):\n-                A path to the directory where to saved. It will be created if it doesn't exist.\n-\n-            filename_prefix (`Optional[str]`, *optional*):\n-                A prefix to add to the names of the files saved by the tokenizer.\n-\n-        \"\"\"\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-\n-        artists_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"artists_file\"]\n-        )\n-        with open(artists_file, \"w\", encoding=\"utf-8\") as f:\n-            f.write(json.dumps(self.artists_encoder, ensure_ascii=False))\n-\n-        genres_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"genres_file\"]\n-        )\n-        with open(genres_file, \"w\", encoding=\"utf-8\") as f:\n-            f.write(json.dumps(self.genres_encoder, ensure_ascii=False))\n-\n-        lyrics_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"lyrics_file\"]\n-        )\n-        with open(lyrics_file, \"w\", encoding=\"utf-8\") as f:\n-            f.write(json.dumps(self.lyrics_encoder, ensure_ascii=False))\n-\n-        return (artists_file, genres_file, lyrics_file)\n-\n-    def _convert_id_to_token(self, artists_index, genres_index, lyric_index):\n-        \"\"\"\n-        Converts an index (integer) in a token (str) using the vocab.\n-\n-        Args:\n-            artists_index (`int`):\n-                Index of the artist in its corresponding dictionary.\n-            genres_index (`Union[list[int], int]`):\n-               Index of the genre in its corresponding dictionary.\n-            lyric_index (`list[int]`):\n-                List of character indices, which each correspond to a character.\n-        \"\"\"\n-        artist = self.artists_decoder.get(artists_index)\n-        genres = [self.genres_decoder.get(genre) for genre in genres_index]\n-        lyrics = [self.lyrics_decoder.get(character) for character in lyric_index]\n-        return artist, genres, lyrics\n-\n-\n-__all__ = [\"JukeboxTokenizer\"]"
        },
        {
            "sha": "53ec5ed37c13614266d04cde838ff7360946a451",
            "filename": "src/transformers/models/deprecated/mctct/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,29 +0,0 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from typing import TYPE_CHECKING\n-\n-from ....utils import _LazyModule\n-from ....utils.import_utils import define_import_structure\n-\n-\n-if TYPE_CHECKING:\n-    from .configuration_mctct import *\n-    from .feature_extraction_mctct import *\n-    from .modeling_mctct import *\n-    from .processing_mctct import *\n-else:\n-    import sys\n-\n-    _file = globals()[\"__file__\"]\n-    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "b6767ec9b65cfb90e5838aaf2d0ba77b8d31dcfe",
            "filename": "src/transformers/models/deprecated/mctct/configuration_mctct.py",
            "status": "removed",
            "additions": 0,
            "deletions": 184,
            "changes": 184,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fconfiguration_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fconfiguration_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fconfiguration_mctct.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,184 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"M-CTC-T model configuration\"\"\"\n-\n-from ....configuration_utils import PreTrainedConfig\n-from ....utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class MCTCTConfig(PreTrainedConfig):\n-    r\"\"\"\n-    This is the configuration class to store the configuration of a [`MCTCTModel`]. It is used to instantiate an\n-    M-CTC-T model according to the specified arguments, defining the model architecture. Instantiating a configuration\n-    with the defaults will yield a similar configuration to that of the M-CTC-T\n-    [speechbrain/m-ctc-t-large](https://huggingface.co/speechbrain/m-ctc-t-large) architecture.\n-\n-    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PreTrainedConfig`] for more information.\n-\n-\n-    Args:\n-        vocab_size (`int`, *optional*, defaults to 8065):\n-            Vocabulary size of the M-CTC-T model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`MCTCTModel`].\n-        hidden_size (`int`, *optional*, defaults to 1536):\n-            Dimension of the encoder layers and the pooler layer.\n-        num_hidden_layers (`int`, *optional*, defaults to 36):\n-            Number of hidden layers in the Transformer encoder.\n-        intermediate_size (`int`, *optional*, defaults to 6144):\n-            Dimension of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n-        num_attention_heads (`int`, *optional*, defaults to 4):\n-            Number of attention heads for each attention layer in the Transformer encoder.\n-        attention_head_dim (`int`, *optional*, defaults to 384):\n-            Dimensions of each attention head for each attention layer in the Transformer encoder.\n-        max_position_embeddings (`int`, *optional*, defaults to 920):\n-            The maximum sequence length that this model might ever be used with (after log-mel spectrogram extraction).\n-        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n-            The epsilon used by the layer normalization layers.\n-        layerdrop (`float`, *optional*, defaults to 0.3):\n-            The probability of dropping an encoder layer during training. The default 0.3 value is used in the original\n-            implementation.\n-        hidden_act (`str` or `function`, *optional*, defaults to `\"relu\"`):\n-            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n-            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n-        initializer_range (`float`, *optional*, defaults to 0.02):\n-            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n-        hidden_dropout_prob (`float`, *optional*, defaults to 0.3):\n-            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n-        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.3):\n-            The dropout ratio for the attention probabilities.\n-        pad_token_id (`int`, *optional*, defaults to 1):\n-            The tokenizer index of the pad token.\n-        bos_token_id (`int`, *optional*, defaults to 0):\n-            The tokenizer index of the bos token.\n-        eos_token_id (`int`, *optional*, defaults to 2):\n-            The tokenizer index of the eos token.\n-        conv_glu_dim (`int`, *optional*, defaults to 1):\n-            The dimension of the output of the `Conv1dSubsampler` layer in which GLU is applied on. Though the original\n-            Flashlight code uses the value of 2, here it's adapted to 1 due to transposition differences.\n-        conv_dropout (`int`, *optional*, defaults to 0.3):\n-            The probability of randomly dropping the `Conv1dSubsampler` layer during training.\n-        num_conv_layers (`int`, *optional*, defaults to 1):\n-            Number of convolution layers before applying transformer encoder layers.\n-        conv_kernel (`Sequence[int]`, *optional*, defaults to `(7,)`):\n-            The kernel size of the 1D convolution applied before transformer layers. `len(conv_kernel)` must be equal\n-            to `num_conv_layers`.\n-        conv_stride (`Sequence[int]`, *optional*, defaults to `(3,)`):\n-            The stride length of the 1D convolution applied before transformer layers. `len(conv_stride)` must be equal\n-            to `num_conv_layers`.\n-        input_feat_per_channel (`int`, *optional*, defaults to 80):\n-            Feature dimensions of the channels of the input to the Conv1D layer.\n-        input_channels (`int`, *optional*, defaults to 1):\n-            Number of input channels of the input to the Conv1D layer.\n-        conv_channels (`list[int]`, *optional*):\n-            Channel sizes of intermediate Conv1D layers.\n-        ctc_loss_reduction (`str`, *optional*, defaults to `\"sum\"`):\n-            Specifies the reduction to apply to the output of `torch.nn.CTCLoss`. Only relevant when training an\n-            instance of [`MCTCTForCTC`].\n-        ctc_zero_infinity (`bool`, *optional*, defaults to `False`):\n-            Whether to zero infinite losses and the associated gradients of `torch.nn.CTCLoss`. Infinite losses mainly\n-            occur when the inputs are too short to be aligned to the targets. Only relevant when training an instance\n-            of [`MCTCTForCTC`].\n-\n-    Example:\n-\n-    ```python\n-    >>> from transformers import MCTCTConfig, MCTCTModel\n-\n-    >>> # Initializing a M-CTC-T mctct-large style configuration\n-    >>> configuration = MCTCTConfig()\n-\n-    >>> # Initializing a model (with random weights) from the mctct-large style configuration\n-    >>> model = MCTCTModel(configuration)\n-\n-    >>> # Accessing the model configuration\n-    >>> configuration = model.config\n-    ```\"\"\"\n-\n-    model_type = \"mctct\"\n-\n-    def __init__(\n-        self,\n-        vocab_size=8065,\n-        hidden_size=1536,\n-        num_hidden_layers=36,\n-        intermediate_size=6144,\n-        num_attention_heads=4,\n-        attention_head_dim=384,\n-        max_position_embeddings=920,\n-        layer_norm_eps=1e-5,\n-        layerdrop=0.3,\n-        hidden_act=\"relu\",\n-        initializer_range=0.02,\n-        hidden_dropout_prob=0.3,\n-        attention_probs_dropout_prob=0.3,\n-        pad_token_id=1,\n-        bos_token_id=0,\n-        eos_token_id=2,\n-        conv_glu_dim=1,\n-        conv_dropout=0.3,\n-        num_conv_layers=1,\n-        conv_kernel=(7,),\n-        conv_stride=(3,),\n-        input_feat_per_channel=80,\n-        input_channels=1,\n-        conv_channels=None,\n-        ctc_loss_reduction=\"sum\",\n-        ctc_zero_infinity=False,\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs, pad_token_id=pad_token_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id)\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.intermediate_size = intermediate_size\n-        self.num_attention_heads = num_attention_heads\n-        self.attention_head_dim = attention_head_dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.layer_norm_eps = layer_norm_eps\n-        self.layerdrop = layerdrop\n-        self.hidden_act = hidden_act\n-        self.initializer_range = initializer_range\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.pad_token_id = pad_token_id\n-        self.bos_token_id = bos_token_id\n-        self.eos_token_id = eos_token_id\n-        self.conv_glu_dim = conv_glu_dim\n-        self.conv_dropout = conv_dropout\n-        self.num_conv_layers = num_conv_layers\n-        self.input_feat_per_channel = input_feat_per_channel\n-        self.input_channels = input_channels\n-        self.conv_channels = conv_channels\n-        self.ctc_loss_reduction = ctc_loss_reduction\n-        self.ctc_zero_infinity = ctc_zero_infinity\n-\n-        # prevents config testing fail with exporting to json\n-        self.conv_kernel = list(conv_kernel)\n-        self.conv_stride = list(conv_stride)\n-\n-        if len(self.conv_kernel) != self.num_conv_layers:\n-            raise ValueError(\n-                \"Configuration for convolutional module is incorrect. \"\n-                \"It is required that `len(config.conv_kernel)` == `config.num_conv_layers` \"\n-                f\"but is `len(config.conv_kernel) = {len(self.conv_kernel)}`, \"\n-                f\"`config.num_conv_layers = {self.num_conv_layers}`.\"\n-            )\n-\n-\n-__all__ = [\"MCTCTConfig\"]"
        },
        {
            "sha": "0ce7c1da31a2a239d5c52ccddf78879ca63b69fa",
            "filename": "src/transformers/models/deprecated/mctct/feature_extraction_mctct.py",
            "status": "removed",
            "additions": 0,
            "deletions": 290,
            "changes": 290,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Ffeature_extraction_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Ffeature_extraction_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Ffeature_extraction_mctct.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,290 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Feature extractor class for M-CTC-T\n-\"\"\"\n-\n-from typing import Optional, Union\n-\n-import numpy as np\n-\n-from ....audio_utils import mel_filter_bank, optimal_fft_length, spectrogram, window_function\n-from ....feature_extraction_sequence_utils import SequenceFeatureExtractor\n-from ....feature_extraction_utils import BatchFeature\n-from ....file_utils import PaddingStrategy, TensorType\n-from ....utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class MCTCTFeatureExtractor(SequenceFeatureExtractor):\n-    r\"\"\"\n-    Constructs a M-CTC-T feature extractor.\n-\n-    This feature extractor inherits from [`~feature_extraction_sequence_utils.SequenceFeatureExtractor`] which contains\n-    most of the main methods. Users should refer to this superclass for more information regarding those methods. This\n-    code has been adapted from Flashlight's C++ code. For more information about the implementation, one can refer to\n-    this [notebook](https://colab.research.google.com/drive/1GLtINkkhzms-IsdcGy_-tVCkv0qNF-Gt#scrollTo=pMCRGMmUC_an)\n-    that takes the user step-by-step in the implementation.\n-\n-    Args:\n-        feature_size (`int`, defaults to 80):\n-            The feature dimension of the extracted features. This is the number of mel_frequency\n-        sampling_rate (`int`, defaults to 16000):\n-            The sampling rate at which the audio files should be digitalized expressed in hertz (Hz).\n-        padding_value (`float`, defaults to 0.0):\n-            The value that is used to fill the padding values.\n-        hop_length (`int`, defaults to 10):\n-            Number of audio samples between windows. Otherwise referred to as \"shift\" in many papers.\n-        win_length (`int`, defaults to 25):\n-            Number of ms per window\n-        win_function (`str`, defaults to `\"hamming_window\"`):\n-            Name for the window function used for windowing, must be accessible via `torch.{win_function}`\n-        frame_signal_scale (`float`, defaults to 32768.0):\n-            Constant multiplied in creating the frames before applying DFT.\n-        preemphasis_coeff (`float`, defaults to 0.97):\n-            Constant multiplied in applying Pre-emphasis before DFT.\n-        mel_floor (`float` defaults to 1.0):\n-            Minimum value of mel frequency banks.\n-        normalize_means (`bool`, *optional*, defaults to `True`):\n-            Whether or not to zero-mean normalize the extracted features.\n-        normalize_vars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to unit-variance normalize the extracted features.\n-    \"\"\"\n-\n-    model_input_names = [\"input_features\", \"attention_mask\"]\n-\n-    def __init__(\n-        self,\n-        feature_size=80,\n-        sampling_rate=16000,\n-        padding_value=0.0,\n-        hop_length=10,\n-        win_length=25,\n-        win_function=\"hamming_window\",\n-        frame_signal_scale=32768.0,\n-        preemphasis_coeff=0.97,\n-        mel_floor=1.0,\n-        normalize_means=True,\n-        normalize_vars=True,\n-        return_attention_mask=False,\n-        **kwargs,\n-    ):\n-        super().__init__(feature_size=feature_size, sampling_rate=sampling_rate, padding_value=padding_value, **kwargs)\n-\n-        self.feature_size = feature_size\n-        self.sampling_rate = sampling_rate\n-        self.padding_value = padding_value\n-        self.hop_length = hop_length\n-        self.win_length = win_length\n-        self.frame_signal_scale = frame_signal_scale\n-        self.preemphasis_coeff = preemphasis_coeff\n-        self.mel_floor = mel_floor\n-        self.normalize_means = normalize_means\n-        self.normalize_vars = normalize_vars\n-        self.win_function = win_function\n-        self.return_attention_mask = return_attention_mask\n-\n-        self.sample_size = win_length * sampling_rate // 1000\n-        self.sample_stride = hop_length * sampling_rate // 1000\n-\n-        self.n_fft = optimal_fft_length(self.sample_size)\n-        self.n_freqs = (self.n_fft // 2) + 1\n-\n-    def _extract_mfsc_features(self, one_waveform: np.ndarray) -> np.ndarray:\n-        \"\"\"\n-        Extracts MFSC Features for one waveform vector (unbatched). Adapted from Flashlight's C++ MFSC code.\n-        \"\"\"\n-        if self.win_function == \"hamming_window\":\n-            window = window_function(window_length=self.sample_size, name=self.win_function, periodic=False)\n-        else:\n-            window = window_function(window_length=self.sample_size, name=self.win_function)\n-\n-        fbanks = mel_filter_bank(\n-            num_frequency_bins=self.n_freqs,\n-            num_mel_filters=self.feature_size,\n-            min_frequency=0.0,\n-            max_frequency=self.sampling_rate / 2.0,\n-            sampling_rate=self.sampling_rate,\n-        )\n-\n-        msfc_features = spectrogram(\n-            one_waveform * self.frame_signal_scale,\n-            window=window,\n-            frame_length=self.sample_size,\n-            hop_length=self.sample_stride,\n-            fft_length=self.n_fft,\n-            center=False,\n-            preemphasis=self.preemphasis_coeff,\n-            mel_filters=fbanks,\n-            mel_floor=self.mel_floor,\n-            log_mel=\"log\",\n-        )\n-        return msfc_features.T\n-\n-    def _normalize_one(self, x, input_length, padding_value):\n-        # make sure we normalize float32 arrays\n-        if self.normalize_means:\n-            mean = x[:input_length].mean(axis=0)\n-            x = np.subtract(x, mean)\n-        if self.normalize_vars:\n-            std = x[:input_length].std(axis=0)\n-            x = np.divide(x, std)\n-\n-        if input_length < x.shape[0]:\n-            x[input_length:] = padding_value\n-\n-        # make sure array is in float32\n-        x = x.astype(np.float32)\n-\n-        return x\n-\n-    def normalize(\n-        self, input_features: list[np.ndarray], attention_mask: Optional[np.ndarray] = None\n-    ) -> list[np.ndarray]:\n-        lengths = attention_mask.sum(-1) if attention_mask is not None else [x.shape[0] for x in input_features]\n-        return [self._normalize_one(x, n, self.padding_value) for x, n in zip(input_features, lengths)]\n-\n-    def __call__(\n-        self,\n-        raw_speech: Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]],\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        max_length: Optional[int] = None,\n-        truncation: bool = False,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_attention_mask: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        sampling_rate: Optional[int] = None,\n-        **kwargs,\n-    ) -> BatchFeature:\n-        \"\"\"\n-        Main method to featurize and prepare for the model one or several sequence(s). sequences. It returns the\n-        log-mel spectrogram of the input audio, as implemented in the original Flashlight MFSC feature extraction code.\n-\n-        Args:\n-            raw_speech (`torch.Tensor`, `np.ndarray`, `list[float]`, `list[torch.Tensor]`, `list[np.ndarray]`, `list[list[float]]`):\n-                The sequence or batch of sequences to be padded. Each sequence can be a tensor, a numpy array, a list\n-                of float values, a list of tensors, a list of numpy arrays or a list of list of float values. Must be\n-                mono channel audio, not stereo, i.e. single float per timestep.\n-            padding (`bool`, `str` or [`~file_utils.PaddingStrategy`], *optional*, defaults to `False`):\n-                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n-                index) among:\n-\n-                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n-                  sequence if provided).\n-                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n-                  acceptable input length for the model if that argument is not provided.\n-                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n-                  lengths).\n-            max_length (`int`, *optional*):\n-                Maximum length of the returned list and optionally padding length (see above).\n-            truncation (`bool`):\n-                Activates truncation to cut input sequences longer than *max_length* to *max_length*.\n-            pad_to_multiple_of (`int`, *optional*):\n-                If set will pad the sequence to a multiple of the provided value.\n-\n-                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n-                `>= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.\n-            return_attention_mask (`bool`, *optional*):\n-                Whether to return the attention mask. If left to the default, will return the attention mask according\n-                to the specific feature_extractor's default.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-\n-            return_tensors (`str` or [`~file_utils.TensorType`], *optional*):\n-                If set, will return tensors instead of list of python integers. Acceptable values are:\n-\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return Numpy `np.ndarray` objects.\n-            sampling_rate (`int`, *optional*):\n-                The sampling rate at which the `raw_speech` input was sampled. It is strongly recommended to pass\n-                `sampling_rate` at the forward call to prevent silent errors.\n-            padding_value (`float`, defaults to 0.0):\n-        \"\"\"\n-\n-        if sampling_rate is not None:\n-            if sampling_rate != self.sampling_rate:\n-                raise ValueError(\n-                    f\"The model corresponding to this feature extractor: {self} was trained using a sampling rate of\"\n-                    f\" {self.sampling_rate}. Please make sure that the provided `raw_speech` input was sampled with\"\n-                    f\" {self.sampling_rate} and not {sampling_rate}.\"\n-                )\n-        else:\n-            logger.warning(\n-                \"It is strongly recommended to pass the ``sampling_rate`` argument to this function. \"\n-                \"Failing to do so can result in silent errors that might be hard to debug.\"\n-            )\n-\n-        is_batched_numpy = isinstance(raw_speech, np.ndarray) and len(raw_speech.shape) > 1\n-        if is_batched_numpy and len(raw_speech.shape) > 2:\n-            raise ValueError(f\"Only mono-channel audio is supported for input to {self}\")\n-        is_batched = is_batched_numpy or (\n-            isinstance(raw_speech, (list, tuple)) and (isinstance(raw_speech[0], (np.ndarray, tuple, list)))\n-        )\n-\n-        if is_batched:\n-            raw_speech = [np.asarray(speech, dtype=np.float32) for speech in raw_speech]\n-        elif not is_batched and not isinstance(raw_speech, np.ndarray):\n-            raw_speech = np.asarray(raw_speech, dtype=np.float32)\n-        elif isinstance(raw_speech, np.ndarray) and raw_speech.dtype is np.dtype(np.float64):\n-            raw_speech = raw_speech.astype(np.float32)\n-\n-        # always return batch\n-        if not is_batched:\n-            raw_speech = [raw_speech]\n-\n-        # extract fbank features\n-        features = [self._extract_mfsc_features(one_waveform) for one_waveform in raw_speech]\n-\n-        # convert into correct format for padding\n-        encoded_inputs = BatchFeature({\"input_features\": features})\n-\n-        padded_inputs = self.pad(\n-            encoded_inputs,\n-            padding=padding,\n-            max_length=max_length,\n-            truncation=truncation,\n-            pad_to_multiple_of=pad_to_multiple_of,\n-            return_attention_mask=True,\n-            **kwargs,\n-        )\n-        # make sure list is in array format\n-        input_features = padded_inputs.get(\"input_features\")\n-        if isinstance(input_features[0], list):\n-            padded_inputs[\"input_features\"] = [np.asarray(feature, dtype=np.float32) for feature in input_features]\n-\n-        attention_mask = padded_inputs.get(\"attention_mask\")\n-        if attention_mask is not None:\n-            padded_inputs[\"attention_mask\"] = [np.asarray(array, dtype=np.int32) for array in attention_mask]\n-\n-        if self.normalize_means or self.normalize_vars:\n-            attention_mask = (\n-                np.array(attention_mask, dtype=np.int32)\n-                if self._get_padding_strategies(padding, max_length=max_length) is not PaddingStrategy.DO_NOT_PAD\n-                and padding\n-                else None\n-            )\n-            padded_inputs[\"input_features\"] = self.normalize(\n-                padded_inputs[\"input_features\"], attention_mask=attention_mask\n-            )\n-\n-        if return_tensors is not None:\n-            padded_inputs = padded_inputs.convert_to_tensors(return_tensors)\n-\n-        return padded_inputs\n-\n-\n-__all__ = [\"MCTCTFeatureExtractor\"]"
        },
        {
            "sha": "8f0530c407569f0ee92a5316d030ce2c1e3767a9",
            "filename": "src/transformers/models/deprecated/mctct/modeling_mctct.py",
            "status": "removed",
            "additions": 0,
            "deletions": 714,
            "changes": 714,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,714 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch M-CTC-T model.\"\"\"\n-\n-import math\n-from typing import Optional, Union\n-\n-import torch\n-from torch import nn\n-\n-from .... import initialization as init\n-from ....activations import ACT2FN\n-from ....file_utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward\n-from ....integrations.deepspeed import is_deepspeed_zero3_enabled\n-from ....integrations.fsdp import is_fsdp_managed_module\n-from ....modeling_attn_mask_utils import _prepare_4d_attention_mask\n-from ....modeling_layers import GradientCheckpointingLayer\n-from ....modeling_outputs import BaseModelOutput, CausalLMOutput\n-from ....modeling_utils import PreTrainedModel\n-from ....pytorch_utils import apply_chunking_to_forward\n-from ....utils import logging\n-from .configuration_mctct import MCTCTConfig\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-_HIDDEN_STATES_START_POSITION = 1\n-\n-_CONFIG_FOR_DOC = \"MCTCTConfig\"\n-\n-# Base docstring\n-_CHECKPOINT_FOR_DOC = \"speechbrain/m-ctc-t-large\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 195, 1536]\n-\n-# CTC docstring\n-_CTC_EXPECTED_OUTPUT = '\"Mr. Quilter is the apostle of the middle classes, and we\\'re glad to welcome his gospel.\"'\n-_CTC_EXPECTED_LOSS = 1885.65\n-\n-\n-class MCTCTConv1dSubsampler(nn.Module):\n-    \"\"\"\n-    Convolutional subsampler: a stack of 1D convolution (along temporal dimension) followed by non-linear activation\n-    via gated linear units (https://huggingface.co/papers/1911.08460)\n-    \"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-        self.config = config\n-        self.glu_dim = config.conv_glu_dim\n-\n-        self.dropout = nn.Dropout(config.conv_dropout)\n-\n-        self.num_layers = config.num_conv_layers\n-        self.in_channels = config.input_feat_per_channel * config.input_channels\n-\n-        if self.num_layers > 1:\n-            if config.conv_channels is None:\n-                raise ValueError(\n-                    \"Need to specify `conv_channels` configuration in `MCTCTConfig` to use multiple convolution\"\n-                    \" layers.\"\n-                )\n-\n-            self.mid_channels = config.conv_channels\n-        else:\n-            self.mid_channels = None\n-\n-        self.out_channels = config.hidden_size * 2  # considering GLU halving\n-        self.kernel_size = config.conv_kernel\n-        self.stride = config.conv_stride\n-\n-        # NOTE: MCTCT by construction only uses one convolution kernel. I've made this flexible to allow for\n-        # multiple layers of convolutions, but not sure if this model definition should just restrict it\n-        # to one layer. This becomes especially relevant when considering the padding like line 1 of forward().\n-        self.conv_layers = nn.ModuleList(\n-            nn.Conv1d(\n-                self.in_channels if i == 0 else self.mid_channels[i],\n-                self.mid_channels[i] if i < self.num_layers - 1 else self.out_channels,\n-                kernel_size=k,\n-                stride=self.stride[i],\n-                padding=\"valid\",\n-            )\n-            for i, k in enumerate(self.kernel_size)\n-        )\n-\n-    def forward(self, input_features):\n-        # NOTE: in reference to the NOTE in __init__, right now it just calculates padding as if\n-        # there will be just one conv layer.\n-        padding = sum(size // 2 for size in self.kernel_size)  # (7, 7) -> (3, 3)\n-\n-        input_features = torch.nn.functional.pad(input_features, (0, 0, padding, padding), \"constant\", 0)\n-        hidden_states = input_features.transpose(1, 2).contiguous()  # -> Batch x Frame x Time\n-        for conv in self.conv_layers:\n-            hidden_states = conv(hidden_states)\n-            hidden_states = nn.functional.glu(hidden_states, dim=self.glu_dim)\n-            hidden_states = self.dropout(hidden_states)\n-\n-        hidden_states = hidden_states.transpose(1, 2).contiguous()  # -> Batch x Time x Frame\n-        return hidden_states\n-\n-\n-class MCTCTEmbeddings(nn.Module):\n-    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n-        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n-        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n-\n-        # self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.LayerNorm = MCTCTLayerNorm()\n-        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-\n-        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.register_buffer(\n-            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n-        )\n-        self.register_buffer(\n-            \"token_type_ids\",\n-            torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n-            persistent=False,\n-        )\n-\n-    def forward(\n-        self, input_features=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n-    ):\n-        input_shape = input_features.size() if input_features is not None else inputs_embeds.size()[:-1]\n-\n-        seq_length = input_shape[1]\n-\n-        if position_ids is None:\n-            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n-\n-        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n-        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n-        # issue #5664\n-        if token_type_ids is None:\n-            if hasattr(self, \"token_type_ids\"):\n-                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n-            else:\n-                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.word_embeddings(input_features)\n-\n-        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n-        embeddings = inputs_embeds + token_type_embeddings\n-\n-        embeddings = self.LayerNorm(embeddings)\n-        embeddings = self.dropout(embeddings)\n-        return embeddings\n-\n-\n-class MCTCTSelfAttention(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n-            raise ValueError(\n-                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n-                f\"heads ({config.num_attention_heads})\"\n-            )\n-\n-        self.num_attention_heads = config.num_attention_heads\n-        self.attention_head_size = config.attention_head_dim\n-        self.all_head_size = self.num_attention_heads * self.attention_head_size\n-\n-        self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=False)\n-        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=False)\n-        self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=False)\n-\n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n-\n-        self.is_decoder = config.is_decoder\n-\n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n-    def reshape_fortran(self, x, shape):\n-        if len(x.shape) > 0:\n-            x = x.permute(*reversed(range(len(x.shape))))\n-        return x.reshape(*reversed(shape)).permute(*reversed(range(len(shape))))\n-\n-    def relative_position_embedding_rotate(self, scores):\n-        # NOTE: should re-evaluate whether this re-implementation was truly necessary\n-        # or the reason why my complete re-haul worked was due to some other part\n-        # of the code. Adding this and the reshape fortrain code seems very undesirable.\n-        scores = scores.permute(0, 2, 3, 1)  # e.g. [10, 1839, 14, 4]\n-\n-        batch, hidden_state, seq_len, heads = scores.shape\n-\n-        # e.g. [10, 1853, 14, 4]\n-        scores = torch.cat((scores, torch.zeros((batch, seq_len, seq_len, heads), device=scores.device)), dim=1)\n-\n-        # e.g. [10, 25942, 1, 4]\n-        scores = self.reshape_fortran(scores, [batch, (hidden_state + seq_len) * seq_len, 1, heads])\n-\n-        # e.g. [10, 25928, 1, 4]\n-        scores = scores[:, : (seq_len + hidden_state - 1) * seq_len]\n-\n-        # e.g. [10, 1852, 14, 4]\n-        scores = self.reshape_fortran(scores, [batch, hidden_state + seq_len - 1, seq_len, heads])\n-\n-        halfpoint = hidden_state // 2\n-        scores = scores[:, halfpoint : halfpoint + seq_len].transpose(1, 2)  # e.g. [10, 14, 14, 4]\n-\n-        return scores.permute(0, 3, 1, 2)\n-\n-    def forward(\n-        self,\n-        hidden_states,\n-        attention_mask=None,\n-        output_attentions=False,\n-    ):\n-        mixed_query_layer = self.query(hidden_states)\n-        mixed_query_layer = mixed_query_layer / math.sqrt(self.attention_head_size)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        # relative key position embeddings\n-        positional_embedding = self.distance_embedding.weight\n-        relative_position_scores = torch.einsum(\"lh, bche -> bcle\", positional_embedding, query_layer.transpose(2, 3))\n-\n-        relative_position_scores = self.relative_position_embedding_rotate(relative_position_scores)\n-        attention_scores = attention_scores + relative_position_scores\n-\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in MCTCTModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        context_layer = torch.matmul(attention_probs, value_layer)\n-\n-        context_layer = context_layer.permute(0, 2, 1, 3).flatten(start_dim=-2)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        return outputs\n-\n-\n-class MCTCTLayerNorm(nn.Module):\n-    def __init__(self):\n-        super().__init__()\n-        self.singleton_weight = nn.Parameter(torch.ones(1))\n-        self.singleton_bias = nn.Parameter(torch.zeros(1))\n-\n-    def forward(self, hidden_states):\n-        return (hidden_states * self.singleton_weight) + self.singleton_bias\n-\n-\n-class MCTCTSelfOutput(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.config = config\n-        self.dense = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-\n-    def forward(self, hidden_states, input_tensor):\n-        hidden_states = self.dense(hidden_states)\n-        hidden_states = self.dropout(hidden_states)\n-        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n-        return hidden_states\n-\n-\n-class MCTCTAttention(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.self = MCTCTSelfAttention(config)\n-        self.output = MCTCTSelfOutput(config)\n-\n-    def forward(\n-        self,\n-        hidden_states,\n-        attention_mask=None,\n-        output_attentions=False,\n-    ):\n-        self_outputs = self.self(\n-            hidden_states,\n-            attention_mask,\n-            output_attentions,\n-        )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-\n-        return outputs\n-\n-\n-class MCTCTIntermediate(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n-        if isinstance(config.hidden_act, str):\n-            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n-        else:\n-            self.intermediate_act_fn = config.hidden_act\n-\n-    def forward(self, hidden_states):\n-        hidden_states = self.dense(hidden_states)\n-        hidden_states = self.intermediate_act_fn(hidden_states)\n-        return hidden_states\n-\n-\n-class MCTCTOutput(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.dense = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-\n-    def forward(self, hidden_states, input_tensor):\n-        hidden_states = self.dense(hidden_states)\n-        hidden_states = self.dropout(hidden_states)\n-        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n-        return hidden_states\n-\n-\n-class MCTCTLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: MCTCTConfig):\n-        super().__init__()\n-\n-        self.seq_len_dim = 1\n-        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n-\n-        self.intermediate = MCTCTIntermediate(config)\n-        self.attention = MCTCTAttention(config)\n-        self.is_decoder = config.is_decoder\n-        self.output = MCTCTOutput(config)\n-\n-    def forward(\n-        self,\n-        hidden_states,\n-        attention_mask=None,\n-        output_attentions=False,\n-    ):\n-        self_attention_outputs = self.attention(hidden_states, attention_mask, output_attentions=output_attentions)\n-        attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        layer_output = apply_chunking_to_forward(\n-            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n-        )\n-\n-        outputs = (layer_output,) + outputs\n-\n-        return outputs\n-\n-    def feed_forward_chunk(self, attention_output):\n-        intermediate_output = self.intermediate(attention_output)\n-        layer_output = self.output(intermediate_output, attention_output)\n-        return layer_output\n-\n-\n-class MCTCTPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config: MCTCTConfig\n-    base_model_prefix = \"mctct\"\n-    main_input_name = \"input_features\"\n-    supports_gradient_checkpointing = True\n-\n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        super()._init_weights(module)\n-        if isinstance(module, MCTCTLayerNorm):\n-            init.ones_(module.singleton_weight)\n-            init.zeros_(module.singleton_bias)\n-\n-    def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):\n-        \"\"\"\n-        Computes the output length of the convolutional layers\n-        \"\"\"\n-        dilation = 1\n-        for _, kernel_sz, stride in zip(\n-            range(self.config.num_conv_layers), self.config.conv_kernel, self.config.conv_stride\n-        ):\n-            padding = kernel_sz // 2\n-            input_lengths = input_lengths + 2 * padding - dilation * (kernel_sz - 1) - 1\n-            input_lengths = torch.div(input_lengths, stride, rounding_mode=\"trunc\") + 1\n-\n-        return input_lengths\n-\n-    def _get_feature_vector_attention_mask(self, feature_vector_length, attention_mask):\n-        # generate creates 3D attention mask, because of the shape of input_features\n-        # convert it to 2D if that's the case\n-        if len(attention_mask.shape) > 2:\n-            attention_mask = attention_mask[:, :, -1]\n-\n-        # subsampled_lengths = attention_mask.sum(-1)\n-        subsampled_lengths = self._get_feat_extract_output_lengths(attention_mask.sum(-1))\n-        bsz = attention_mask.size()[0]\n-        attention_mask = torch.zeros(\n-            (bsz, feature_vector_length), dtype=attention_mask.dtype, device=attention_mask.device\n-        )\n-\n-        # these two operations makes sure that all values\n-        # before the output lengths indices are attended to\n-        attention_mask[(torch.arange(bsz, device=attention_mask.device), subsampled_lengths - 1)] = 1\n-        attention_mask = attention_mask.flip([-1]).cumsum(-1).flip([-1]).long()\n-        return attention_mask\n-\n-\n-MCTCT_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n-    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`MCTCTConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-MCTCT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_features (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`Wav2Vec2CTCTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-class MCTCTEncoder(MCTCTPreTrainedModel):\n-    def __init__(self, config: MCTCTConfig):\n-        super().__init__(config)\n-        self.hidden_dropout_prob = config.hidden_dropout_prob\n-\n-        self.layer_norm = MCTCTLayerNorm()\n-        self.conv = MCTCTConv1dSubsampler(config)\n-        self.layers = nn.ModuleList([MCTCTLayer(config) for _ in range(config.num_hidden_layers)])\n-\n-        self.gradient_checkpointing = False\n-\n-    def forward(\n-        self,\n-        input_features: torch.Tensor,\n-        attention_mask: torch.Tensor,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-    ) -> Union[tuple, BaseModelOutput]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        input_features = self.layer_norm(input_features)\n-\n-        inputs_embeds = self.conv(input_features)\n-\n-        # subsample attention mask if necessary\n-        if attention_mask is not None:\n-            attention_mask = self._get_feature_vector_attention_mask(inputs_embeds.shape[1], attention_mask)\n-\n-        hidden_states = nn.functional.dropout(inputs_embeds, p=self.hidden_dropout_prob, training=self.training)\n-\n-        # expand attention_mask\n-        if attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n-        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n-        for idx, encoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-\n-            # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n-            dropout_probability = torch.rand([])\n-\n-            skip_the_layer = self.training and dropout_probability < self.config.layerdrop\n-            if not skip_the_layer or synced_gpus:\n-                # under fsdp or deepspeed zero3 all gpus must run in sync\n-                layer_outputs = encoder_layer(\n-                    hidden_states=hidden_states,\n-                    attention_mask=attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n-\n-                hidden_states = layer_outputs[0]\n-\n-            if skip_the_layer:\n-                layer_outputs = (None, None)\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"The bare M-CTC-T Model transformer outputting raw hidden-states without any specific head on top.\",\n-    MCTCT_START_DOCSTRING,\n-)\n-class MCTCTModel(MCTCTPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.config = config\n-\n-        self.encoder = MCTCTEncoder(config)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(MCTCT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"audio\",\n-        expected_output=_EXPECTED_OUTPUT_SHAPE,\n-    )\n-    def forward(\n-        self,\n-        input_features: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, BaseModelOutput]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        if input_features is None:\n-            raise ValueError(\"You have to specify input_features.\")\n-\n-        encoder_outputs = self.encoder(\n-            input_features,\n-            attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-        sequence_output = encoder_outputs[0]\n-\n-        if not return_dict:\n-            return (sequence_output,) + encoder_outputs[1:]\n-\n-        return BaseModelOutput(\n-            last_hidden_state=sequence_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"MCTCT Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n-    MCTCT_START_DOCSTRING,\n-)\n-class MCTCTForCTC(MCTCTPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-\n-        self.mctct = MCTCTModel(config)\n-\n-        if config.vocab_size is None:\n-            raise ValueError(\n-                f\"You are trying to instantiate {self.__class__} with a configuration that \"\n-                \"does not define the vocabulary size of the language model head. Please \"\n-                \"instantiate the model as follows: `MCTCTForCTC.from_pretrained(..., vocab_size=vocab_size)`. \"\n-                \"or define `vocab_size` of your model's configuration.\"\n-            )\n-        output_hidden_size = config.hidden_size\n-\n-        self.ctc_head = nn.Linear(output_hidden_size, config.vocab_size)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(MCTCT_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=CausalLMOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_CTC_EXPECTED_OUTPUT,\n-        expected_loss=_CTC_EXPECTED_LOSS,\n-    )\n-    def forward(\n-        self,\n-        input_features: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-    ) -> Union[tuple, CausalLMOutput]:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n-            Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to\n-            the sequence length of the output logits. Indices are selected in `[-100, 0, ..., config.vocab_size - 1]`.\n-            All labels set to `-100` are ignored (masked), the loss is only computed for labels in `[0, ...,\n-            config.vocab_size - 1]`.\n-        \"\"\"\n-        if labels is not None and labels.max() >= self.config.vocab_size:\n-            raise ValueError(f\"Label values must be <= vocab_size: {self.config.vocab_size}\")\n-\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        outputs = self.mctct(\n-            input_features,\n-            attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        hidden_states = outputs[0]\n-\n-        logits = self.ctc_head(hidden_states)\n-\n-        loss = None\n-        if labels is not None:\n-            # retrieve loss input_lengths from attention_mask\n-            attention_mask = (\n-                attention_mask\n-                if attention_mask is not None\n-                else torch.ones(input_features.shape[:-1], dtype=torch.long)\n-            )\n-            input_lengths = self._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)\n-            # assuming that padded tokens are filled with -100\n-            # when not being attended to\n-            labels_mask = labels >= 0\n-            target_lengths = labels_mask.sum(-1)\n-            flattened_targets = labels.masked_select(labels_mask)\n-\n-            # ctc_loss doesn't support fp16\n-            log_probs = nn.functional.log_softmax(logits, dim=-1, dtype=torch.float32).transpose(0, 1)\n-\n-            with torch.backends.cudnn.flags(enabled=False):\n-                loss = nn.functional.ctc_loss(\n-                    log_probs,\n-                    flattened_targets,\n-                    input_lengths,\n-                    target_lengths,\n-                    blank=self.config.pad_token_id,\n-                    reduction=self.config.ctc_loss_reduction,\n-                    zero_infinity=self.config.ctc_zero_infinity,\n-                )\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[_HIDDEN_STATES_START_POSITION:]\n-            return ((loss,) + output) if loss is not None else output\n-\n-        return CausalLMOutput(\n-            loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions\n-        )\n-\n-\n-__all__ = [\"MCTCTForCTC\", \"MCTCTModel\", \"MCTCTPreTrainedModel\"]"
        },
        {
            "sha": "9661a5082d4838b4e60042be2ceb747c03e045ce",
            "filename": "src/transformers/models/deprecated/mctct/processing_mctct.py",
            "status": "removed",
            "additions": 0,
            "deletions": 139,
            "changes": 139,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fprocessing_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fprocessing_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fprocessing_mctct.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,139 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Speech processor class for M-CTC-T\n-\"\"\"\n-\n-import warnings\n-from contextlib import contextmanager\n-\n-from ....processing_utils import ProcessorMixin\n-\n-\n-class MCTCTProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a MCTCT processor which wraps a MCTCT feature extractor and a MCTCT tokenizer into a single processor.\n-\n-    [`MCTCTProcessor`] offers all the functionalities of [`MCTCTFeatureExtractor`] and [`AutoTokenizer`]. See the\n-    [`~MCTCTProcessor.__call__`] and [`~MCTCTProcessor.decode`] for more information.\n-\n-    Args:\n-        feature_extractor (`MCTCTFeatureExtractor`):\n-            An instance of [`MCTCTFeatureExtractor`]. The feature extractor is a required input.\n-        tokenizer (`AutoTokenizer`):\n-            An instance of [`AutoTokenizer`]. The tokenizer is a required input.\n-    \"\"\"\n-\n-    feature_extractor_class = \"MCTCTFeatureExtractor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n-    def __init__(self, feature_extractor, tokenizer):\n-        super().__init__(feature_extractor, tokenizer)\n-        self.current_processor = self.feature_extractor\n-        self._in_target_context_manager = False\n-\n-    def __call__(self, *args, **kwargs):\n-        \"\"\"\n-        When used in normal mode, this method forwards all its arguments to MCTCTFeatureExtractor's\n-        [`~MCTCTFeatureExtractor.__call__`] and returns its output. If used in the context\n-        [`~MCTCTProcessor.as_target_processor`] this method forwards all its arguments to AutoTokenizer's\n-        [`~AutoTokenizer.__call__`]. Please refer to the docstring of the above two methods for more information.\n-        \"\"\"\n-        # For backward compatibility\n-        if self._in_target_context_manager:\n-            return self.current_processor(*args, **kwargs)\n-\n-        if \"raw_speech\" in kwargs:\n-            warnings.warn(\"Using `raw_speech` as a keyword argument is deprecated. Use `audio` instead.\")\n-            audio = kwargs.pop(\"raw_speech\")\n-        else:\n-            audio = kwargs.pop(\"audio\", None)\n-        sampling_rate = kwargs.pop(\"sampling_rate\", None)\n-        text = kwargs.pop(\"text\", None)\n-        if len(args) > 0:\n-            audio = args[0]\n-            args = args[1:]\n-\n-        if audio is None and text is None:\n-            raise ValueError(\"You need to specify either an `audio` or `text` input to process.\")\n-\n-        if audio is not None:\n-            inputs = self.feature_extractor(audio, *args, sampling_rate=sampling_rate, **kwargs)\n-        if text is not None:\n-            encodings = self.tokenizer(text, **kwargs)\n-\n-        if text is None:\n-            return inputs\n-        elif audio is None:\n-            return encodings\n-        else:\n-            inputs[\"labels\"] = encodings[\"input_ids\"]\n-            return inputs\n-\n-    def pad(self, *args, **kwargs):\n-        \"\"\"\n-        When used in normal mode, this method forwards all its arguments to MCTCTFeatureExtractor's\n-        [`~MCTCTFeatureExtractor.pad`] and returns its output. If used in the context\n-        [`~MCTCTProcessor.as_target_processor`] this method forwards all its arguments to PreTrainedTokenizer's\n-        [`~PreTrainedTokenizer.pad`]. Please refer to the docstring of the above two methods for more information.\n-        \"\"\"\n-        # For backward compatibility\n-        if self._in_target_context_manager:\n-            return self.current_processor.pad(*args, **kwargs)\n-\n-        input_features = kwargs.pop(\"input_features\", None)\n-        labels = kwargs.pop(\"labels\", None)\n-        if len(args) > 0:\n-            input_features = args[0]\n-            args = args[1:]\n-\n-        if input_features is not None:\n-            input_features = self.feature_extractor.pad(input_features, *args, **kwargs)\n-        if labels is not None:\n-            labels = self.tokenizer.pad(labels, **kwargs)\n-\n-        if labels is None:\n-            return input_features\n-        elif input_features is None:\n-            return labels\n-        else:\n-            input_features[\"labels\"] = labels[\"input_ids\"]\n-            return input_features\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to AutoTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to the\n-        docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @contextmanager\n-    def as_target_processor(self):\n-        \"\"\"\n-        Temporarily sets the tokenizer for processing the input. Useful for encoding the labels when fine-tuning MCTCT.\n-        \"\"\"\n-        warnings.warn(\n-            \"`as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your \"\n-            \"labels by using the argument `text` of the regular `__call__` method (either in the same call as \"\n-            \"your audio inputs, or in a separate call.\"\n-        )\n-        self._in_target_context_manager = True\n-        self.current_processor = self.tokenizer\n-        yield\n-        self.current_processor = self.feature_extractor\n-        self._in_target_context_manager = False\n-\n-\n-__all__ = [\"MCTCTProcessor\"]"
        },
        {
            "sha": "cff2c19505f9306c28edb5bdabb66f668363f5fa",
            "filename": "src/transformers/models/deprecated/mega/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,27 +0,0 @@\n-# Copyright 2023 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from typing import TYPE_CHECKING\n-\n-from ....utils import _LazyModule\n-from ....utils.import_utils import define_import_structure\n-\n-\n-if TYPE_CHECKING:\n-    from .configuration_mega import *\n-    from .modeling_mega import *\n-else:\n-    import sys\n-\n-    _file = globals()[\"__file__\"]\n-    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "07a54f08f6533ca7c301b3bfcd5be9c67dbdd523",
            "filename": "src/transformers/models/deprecated/mega/configuration_mega.py",
            "status": "removed",
            "additions": 0,
            "deletions": 224,
            "changes": 224,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconfiguration_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconfiguration_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconfiguration_mega.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,224 +0,0 @@\n-# coding=utf-8\n-# Copyright 2023 The Mega Authors and The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"MEGA configuration\"\"\"\n-\n-from ....configuration_utils import PreTrainedConfig\n-from ....utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class MegaConfig(PreTrainedConfig):\n-    r\"\"\"\n-    This is the configuration class to store the configuration of a [`MegaModel`]. It is used to instantiate a Mega\n-    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n-    defaults will yield a similar configuration to that of the Mega\n-    [mnaylor/mega-base-wikitext](https://huggingface.co/mnaylor/mega-base-wikitext) architecture.\n-\n-    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PreTrainedConfig`] for more information.\n-\n-\n-    Args:\n-        vocab_size (`int`, *optional*, defaults to 30522):\n-            Vocabulary size of the Mega model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`MegaModel`].\n-        hidden_size (`int`, *optional*, defaults to 128):\n-            Dimensionality of the encoder layers and the pooler layer.\n-        num_hidden_layers (`int`, *optional*, defaults to 4):\n-            Number of hidden layers in the Mega encoder.\n-        intermediate_size (`int`, *optional*, defaults to 256):\n-            Dimensionality of the hidden size (self-attention value projection) within the Mega encoder\n-        ema_projection_size (`int`, *optional*, defaults to 16):\n-            Dimensionality of the MegaMultiDimensionDampedEma\n-        bidirectional (`bool`, *optional*, defaults to `True`):\n-            Whether the MegaMultiDimensionDampedEma used in Mega's self-attention should work bidirectionally (`True`)\n-            or unidirectionally (`False`). Bidirectional EMA is incompatible with causal decoding, so this should be\n-            False if you intend to use the model as a decoder.\n-        shared_representation_size (`int`, *optional*, defaults to 64):\n-            Dimensionality of the linear projection for shared representation of self-attention queries and keys\n-        use_chunking (`bool`, *optional*, defaults to `False`):\n-            Whether to chunk inputs for linear self-attention complexity (described as Mega-chunk in the paper)\n-        chunk_size (`int`, *optional*, defaults to -1):\n-            If `use_chunking` is set to `True`, determines the size of the chunks to apply to the input sequence. If\n-            chunking is used, input sequences must be padded to a multiple of `chunk_size`\n-        truncation (`int`, *optional*):\n-            If specified, the sequence length for which to truncate MegaMultiDimensionDampedEma\n-        normalize_before_mega (`bool`, *optional*, defaults to `True`):\n-            Whether to normalize before (`True`) or after (`False`) passing through Mega encoder blocks\n-        normalization_type (`str`, *optional*, defaults to `\"scalenorm\"`):\n-            Type of normalization to use in Mega encoder blocks. Choose one of `\"scalenorm\"`, `\"layernorm\"`,\n-            `\"rmsnorm\"`, `\"batchnorm\"`, or `\"syncbatchnorm\"` (GPU required for syncbatchnorm)\n-        norm_affine (`bool`, *optional*, defaults to `True`):\n-            If `True`, applies a parameterized affine transformation to inputs during normalization\n-        activation (`str`, *optional*, defaults to `\"silu\"`):\n-            Activation function to apply within Mega encoder blocks. Choose one of `\"silu\"`, `\"relu\"`, `\"linear\"`,\n-            `\"gelu\"`, or `\"gelu_accurate\"`\n-        attention_activation (`str`, *optional*, defaults to `\"softmax\"`):\n-            Activation function to apply for single-headed self-attention (a la Transformer). Choose one of\n-            `\"softmax\"`, `\"laplace\"`, or `\"relu2\"`\n-        dropout_prob (`float`, *optional*, defaults to 0.1):\n-            The dropout probability for EMA self-attention\n-        hidden_dropout_prob (`float`, *optional*, defaults to 0.1):\n-            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n-        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.1):\n-            The dropout ratio for the attention probabilities.\n-        use_feature_dropout (`bool`, *optional*, defaults to `False`):\n-            Whether to use feature-based (`True`) or standard dropout (`False`)\n-        use_normalized_ffn (`bool`, *optional*, defaults to `True`):\n-            Whether to use the normalized feed-forward sub-layer in Mega blocks (`True`) or pass Mega encoder output\n-            as-is (`False`)\n-        nffn_hidden_size (`int`, *optional*, defaults to 256):\n-            If using the normalized feed-forward network (NFFN) layer within Mega (`use_normalized_ffn = True`), this\n-            is the hidden size of the NFFN\n-        normalize_before_ffn (`bool`, *optional*, defaults to `True`):\n-            Whether to normalize before (`True`) or after (`False`) the feed-forward portion of NFFN\n-        nffn_activation_dropout_prob (`float`, *optional*, defaults to 0.1):\n-            The dropout ratio for the NFFN component.\n-        max_positions (`int`, *optional*, defaults to 2048):\n-            The maximum sequence length to use for positional representations. For `\"simple\"` relative positional bias,\n-            this is a hard limit on input length; `\"rotary\"` relative positional bias will extrapolate to longer\n-            sequences\n-        add_token_type_embeddings (`bool`, *optional*, defaults to `True`):\n-            Whether to account for token types in embeddings. Left as optional to maintain compatibility with original\n-            implementation while adding support for token types.\n-        type_vocab_size (`int`, *optional*, defaults to 2):\n-            The vocabulary size of the `token_type_ids` passed when calling [`MegaModel`]. Only used if\n-            `add_token_type_embeddings = True`\n-        initializer_range (`float`, *optional*, defaults to 0.02):\n-            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n-        ema_delta_alpha_range (`float`, *optional*, defaults to 0.2):\n-            The standard deviation for initializing the delta (damping factor) and alpha (decay factor) parameters in\n-            MegaMultiDimensionDampedEma.\n-        ema_beta_range (`float`, *optional*, defaults to 0.02):\n-            The standard deviation for initializing the beta parameter (expansion matrix) in\n-            MegaMultiDimensionDampedEma.\n-        ema_gamma_omega_range (`float`, *optional*, defaults to 1.0):\n-            The standard deviation for initializing the gamma (projection matrix) and omega (residual weight)\n-            parameters in MultiDimensionEMA.\n-        relative_positional_bias (`str`, *optional*, defaults to `\"rotary\"`):\n-            Type of relative positional encoding. Choose one of `\"rotary\"` or `\"simple\"`. If `\"simple\"` is selected,\n-            `max_positions` is used as a limit on input size, while `\"rotary\"` extrapolates beyond `max_positions`.\n-        is_decoder (`bool`, *optional*, defaults to `False`):\n-            Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n-        use_cache (`bool`, *optional*, defaults to `True`):\n-            Whether or not the model should return the last key/values attentions (not used by all models). Only\n-            relevant if `config.is_decoder=True`.\n-        classifier_dropout (`float`, *optional*):\n-            The dropout ratio for the classification head.\n-        add_lm_hidden_dense_layer (`bool`, *optional*, defaults to `True`):\n-            Whether to include a hidden layer for projection between encoder outputs and LM heads (`True`) or pass\n-            hidden states directly to LM head (`False`). Remains optional for compatibility with original\n-            implementation\n-\n-    Examples:\n-\n-    ```python\n-    >>> from transformers import MegaConfig, MegaModel\n-\n-    >>> # Initializing a Mega configuration\n-    >>> configuration = MegaConfig()\n-\n-    >>> # Initializing a model (with random weights) from the configuration\n-    >>> model = MegaModel(configuration)\n-\n-    >>> # Accessing the model configuration\n-    >>> configuration = model.config\n-    ```\"\"\"\n-\n-    model_type = \"mega\"\n-\n-    def __init__(\n-        self,\n-        vocab_size=30522,\n-        hidden_size=128,\n-        num_hidden_layers=4,\n-        intermediate_size=256,\n-        ema_projection_size=16,\n-        bidirectional=True,\n-        shared_representation_size=64,\n-        use_chunking=False,\n-        chunk_size=-1,\n-        truncation=None,\n-        normalize_before_mega=True,\n-        normalization_type=\"scalenorm\",\n-        norm_affine=True,\n-        activation=\"silu\",\n-        attention_activation=\"softmax\",\n-        dropout_prob=0.1,\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        use_feature_dropout=False,\n-        use_normalized_ffn=True,\n-        nffn_hidden_size=256,\n-        normalize_before_ffn=True,\n-        nffn_activation_dropout_prob=0.1,\n-        max_positions=2048,\n-        add_token_type_embeddings=False,\n-        type_vocab_size=2,\n-        initializer_range=0.02,\n-        ema_delta_alpha_range=0.2,\n-        ema_beta_range=0.02,\n-        ema_gamma_omega_range=1.0,\n-        pad_token_id=1,\n-        bos_token_id=0,\n-        eos_token_id=2,\n-        relative_positional_bias=\"rotary\",\n-        classifier_dropout=None,\n-        use_cache=True,\n-        add_lm_hidden_dense_layer=True,\n-        **kwargs,\n-    ):\n-        super().__init__(pad_token_id=pad_token_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n-\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.activation = activation\n-        self.attention_activation = attention_activation\n-        self.intermediate_size = intermediate_size\n-        self.ema_projection_size = ema_projection_size\n-        self.bidirectional = bidirectional\n-        self.shared_representation_size = shared_representation_size\n-        self.use_chunking = use_chunking\n-        self.chunk_size = chunk_size\n-        self.truncation = truncation\n-        self.normalize_before_mega = normalize_before_mega\n-        self.normalization_type = normalization_type\n-        self.norm_affine = norm_affine\n-        self.dropout_prob = dropout_prob\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.use_feature_dropout = use_feature_dropout\n-        self.use_normalized_ffn = use_normalized_ffn\n-        self.nffn_hidden_size = nffn_hidden_size\n-        self.normalize_before_ffn = normalize_before_ffn\n-        self.nffn_activation_dropout_prob = nffn_activation_dropout_prob\n-        self.max_positions = max_positions\n-        self.add_token_type_embeddings = add_token_type_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.initializer_range = initializer_range\n-        self.ema_delta_alpha_range = ema_delta_alpha_range\n-        self.ema_beta_range = ema_beta_range\n-        self.ema_gamma_omega_range = ema_gamma_omega_range\n-        self.relative_positional_bias = relative_positional_bias\n-        self.use_cache = use_cache\n-        self.classifier_dropout = classifier_dropout\n-        self.add_lm_hidden_dense_layer = add_lm_hidden_dense_layer\n-        self.num_attention_heads = 1  # not used but required by Hugging Face\n-\n-\n-__all__ = [\"MegaConfig\"]"
        },
        {
            "sha": "63c3ebd1370db150d1c15f16836e716d340709be",
            "filename": "src/transformers/models/deprecated/mega/convert_mega_original_pytorch_checkpoint_to_pytorch.py",
            "status": "removed",
            "additions": 0,
            "deletions": 307,
            "changes": 307,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconvert_mega_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconvert_mega_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconvert_mega_original_pytorch_checkpoint_to_pytorch.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,307 +0,0 @@\n-# coding=utf-8\n-# Copyright 2023 The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\"\"\"\n-Convert Mega pretrained checkpoint. Built to convert the Masked LM checkpoint located at\n-https://huggingface.co/mnaylor/mega-wikitext-103\n-\n-Requirements:\n-  - clone the Mega repo and install fairseq from there\n-    1. git clone https://github.com/facebookresearch/mega.git\n-    2. cd mega && pip install -e\n-  - clone the pretrained weights for the original implementation from the hugging face repo\n-    * use this location as the path for pretrained weights\n-\"\"\"\n-\n-import argparse\n-\n-# utilities to import the model weights and config file\n-import os\n-import pickle\n-\n-# PyTorch + new model classes\n-import torch\n-from torch import nn\n-\n-from transformers import AutoTokenizer, MegaConfig, MegaForMaskedLM\n-\n-from ....utils import strtobool\n-\n-\n-# import the EncoderLayer class used to pretrain\n-# !! NOTE !! this requires the version of fairseq that is built when you install the Mega source\n-try:\n-    from fairseq.modules.mega_layer import MegaEncoderLayer\n-except ImportError:\n-    raise ImportError(\"You need to install the version of fairseq from the Mega repo!\")\n-\n-\n-# define the wrapper classes used to train the MLM  (see colab notebook below)\n-# https://colab.research.google.com/drive/1qfUO6o5HRdxBblWlw058HVyvaEPhPpH8?usp=sharing\n-# MegaLM outputs hidden states\n-class MegaLM(nn.Module):\n-    \"The base class for our Mega encoder - given input IDs, embed text and return encoder output\"\n-\n-    def __init__(self, mega_args, depth, vocab_size):\n-        super().__init__()\n-        self.mega_args = mega_args\n-        self.embedding_layer = nn.Embedding(vocab_size, self.mega_args.encoder_embed_dim)\n-        self.encoders = nn.ModuleList([MegaEncoderLayer(self.mega_args) for _ in range(depth)])\n-        self.depth = depth\n-\n-    def forward(self, input_ids, attention_mask, batch_first=True, ignore_mask_value=0):\n-        \"\"\"\n-        Code for a forward pass - expects input_ids and attention_mask to come from a Hugging Face tokenizer as PyTorch\n-        tensors, and returns a tensor of size (batch, n_classes) containing classification logits\n-\n-        Other options:\n-          - batch_first: boolean indicating whether the batch dimension is first in input_ids (default: True, which\n-            aligns with the HF tokenizer behavior)\n-          - ignore_mask_value: the value in attention_mask that identifies tokens that should be ignored (default: 0,\n-            which aligns with HF tokenizer)\n-        \"\"\"\n-\n-        # Mega expects embeddings to be (time, batch, embedding size), but\n-        # Hugging Face returns tokens as (batch, time)\n-        if batch_first:\n-            input_ids = input_ids.T\n-\n-        # to make things more confusing, Mega expects the attention mask to\n-        # be (batch, time), but with values of 0 (normal token) and 1 (ignore token)\n-        # which is the opposite of what HF returns\n-        if ignore_mask_value == 0:\n-            attention_mask = 1 - attention_mask\n-\n-        # get token embeddings from IDs\n-        embeds = self.embedding_layer(input_ids)\n-\n-        # pass through the Mega layers\n-        # input is (time, batch, encoder dim) and output is the same\n-        for encoder in self.encoders:\n-            embeds = encoder(embeds, attention_mask)\n-\n-        # return according to the shape specified\n-        if batch_first:\n-            # (T, B, H) --> (B, T, H)\n-            return torch.transpose(embeds, 0, 1)\n-        else:\n-            return embeds\n-\n-\n-# renamed from MegaForMaskedLM to avoid confusion with new module\n-class OriginalMegaForMaskedLM(nn.Module):\n-    \"A wrapper class for doing masked language modeling with Mega\"\n-\n-    def __init__(self, mega_args, depth, vocab_size):\n-        super().__init__()\n-        self.mega = MegaLM(mega_args, depth, vocab_size)\n-        self.mlm_head = nn.Linear(mega_args.encoder_embed_dim, vocab_size)\n-        self.dropout = nn.Dropout(p=0.1)\n-\n-    def forward(self, input_ids, attention_mask, batch_first=True, ignore_mask_value=0):\n-        \"\"\"\n-        Perform a forward pass through the Mega encoder and the masked LM head. Returns logits for each vocabulary\n-        entry.\n-\n-        If `batch_first` (default to align with Hugging Face tokenizer behavior), output will have the shape (Batch\n-        size, Sequence length, Vocab size); otherwise (S, B, V)\n-        \"\"\"\n-        encoder_output = self.mega(input_ids, attention_mask, batch_first, ignore_mask_value)\n-        return self.mlm_head(self.dropout(encoder_output))\n-\n-\n-# code to convert the checkpoint located in the user-specified location\n-def convert_checkpoint_to_huggingface(pretrained_checkpoint_path, output_path, includes_tokenizer):\n-    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n-        raise ValueError(\n-            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n-            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n-            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n-            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n-        )\n-    with open(os.path.join(pretrained_checkpoint_path, \"model_args.pkl\"), \"rb\") as f:\n-        mega_original_args = pickle.load(f)\n-\n-    # load the original encoder\n-    original_mlm = OriginalMegaForMaskedLM(**mega_original_args).eval()\n-\n-    # load its weights\n-    print(\n-        \"Original Mega encoder:\",\n-        original_mlm.mega.load_state_dict(\n-            torch.load(\n-                os.path.join(pretrained_checkpoint_path, \"encoder_weights.pt\"), map_location=\"cpu\", weights_only=True\n-            )\n-        ),\n-    )\n-    print(\n-        \"Original Mega MLM layer:\",\n-        original_mlm.mlm_head.load_state_dict(\n-            torch.load(\n-                os.path.join(pretrained_checkpoint_path, \"mlm_head_weights.pt\"), map_location=\"cpu\", weights_only=True\n-            )\n-        ),\n-    )\n-\n-    # create a new config from the old one\n-    hf_config = MegaConfig(\n-        num_hidden_layers=mega_original_args[\"depth\"],\n-        vocab_size=mega_original_args[\"vocab_size\"],\n-        hidden_size=mega_original_args[\"mega_args\"].encoder_embed_dim,\n-        shared_representation_size=mega_original_args[\"mega_args\"].encoder_z_dim,\n-        intermediate_size=mega_original_args[\"mega_args\"].encoder_hidden_dim,\n-        ema_projection_size=mega_original_args[\"mega_args\"].encoder_n_dim,\n-        dropout_prob=mega_original_args[\"mega_args\"].dropout,\n-        attention_probs_dropout_prob=mega_original_args[\"mega_args\"].attention_dropout,\n-        hidden_dropout_prob=mega_original_args[\"mega_args\"].hidden_dropout,\n-        activation=mega_original_args[\"mega_args\"].activation_fn,\n-        attention_activation=mega_original_args[\"mega_args\"].attention_activation_fn,\n-        bidirectional=mega_original_args[\"mega_args\"].bidirectional,\n-        use_chunking=mega_original_args[\"mega_args\"].encoder_chunk_size > 0,\n-        chunk_size=mega_original_args[\"mega_args\"].encoder_chunk_size,\n-        truncation=mega_original_args[\"mega_args\"].truncation_length,\n-        normalization_type=mega_original_args[\"mega_args\"].normalization_type,\n-        normalize_before_mega=True,\n-        norm_affine=True,\n-        use_feature_dropout=mega_original_args[\"mega_args\"].feature_dropout,\n-        relative_positional_bias=mega_original_args[\"mega_args\"].rel_pos_bias,\n-        max_positions=mega_original_args[\"mega_args\"].max_source_positions,\n-        nffn_hidden_size=mega_original_args[\"mega_args\"].encoder_ffn_embed_dim,\n-        normalize_before_ffn=mega_original_args[\"mega_args\"].normalize_before,\n-        # new arguments added for HF implementation\n-        nffn_activation_dropout_prob=0.0,\n-        add_token_type_embeddings=False,\n-        add_lm_hidden_dense_layer=False,\n-    )\n-\n-    hf_mlm = MegaForMaskedLM(hf_config).eval()\n-\n-    # the original checkpoint just uses nn.Embedding for the word embeddings\n-    # we use a wrapper module for embeddings to add support for positional embeddings\n-    hf_mlm.mega.embedding_layer.word_embeddings.weight = original_mlm.mega.embedding_layer.weight\n-\n-    # modify the state dictionary of the original checkpoint to account for naming issues in the Hugging Face\n-    # ecosystem -- any names containing \"beta\" or \"gamma\" aren't safe to use and are renamed upon _load_pretrained,\n-    # also renaming previously confusing parameter names\n-    original_state_dict = original_mlm.mega.encoders.state_dict()\n-    updated_keys = {}\n-    for module_name in original_state_dict:\n-        new_module_name = None\n-        # have to handle gamma, beta, and alpha differently due to their use\n-        # in multiple modules within the original repository;\n-        # beta is used in EMA, MovingAverageGatedAttention, and RotaryRelativePositionalBias, and must be renamed due to flax/tf weights\n-        # the EMA sublayer was renamed from \"move\" to \"ema_gate\" for readability, so that is also done here\n-        if \"beta\" in module_name:\n-            # EMA sub-layers were always called \"move\" in the original repo\n-            if \"move.beta\" in module_name:\n-                new_module_name = module_name.replace(\"move.beta\", \"ema_gate.ema_expansion_matrix\")\n-            elif \"mega_layer.beta\" in module_name:\n-                new_module_name = module_name.replace(\"beta\", \"qk_bias\")\n-            else:\n-                new_module_name = module_name.replace(\"beta\", \"b_param\")\n-        # beta is used in EMA and MovingAverageGatedAttention, and must be renamed due to flax/tf weights\n-        elif \"gamma\" in module_name:\n-            if \"move.gamma\" in module_name:\n-                new_module_name = module_name.replace(\"move.gamma\", \"ema_gate.kernel_projection_matrix\")\n-            elif \"mega_layer.gamma\" in module_name:\n-                new_module_name = module_name.replace(\"gamma\", \"qk_weight\")\n-            else:\n-                new_module_name = module_name.replace(\"gamma\", \"g_param\")\n-        # alpha is used in EMA and positional bias; renaming to improve readability\n-        elif \"move.alpha\" in module_name:\n-            new_module_name = module_name.replace(\"move.alpha\", \"ema_gate.decay_factor\")\n-        # delta is only used in EMA; renaming to improve readability\n-        elif \"move.delta\" in module_name:\n-            new_module_name = module_name.replace(\"move.delta\", \"ema_gate.damping_factor\")\n-        # omega is only used in EMA; renaming to improve readability\n-        elif \"omega\" in module_name:\n-            new_module_name = module_name.replace(\"move.omega\", \"ema_gate.residual_weight\")\n-\n-        if new_module_name:\n-            updated_keys[module_name] = new_module_name\n-\n-    if len(updated_keys) != 0:\n-        print(f\"Renaming these keys: {updated_keys.keys()}\")\n-    else:\n-        print(\"No need to rename state dict entries\")\n-    for old, new in updated_keys.items():\n-        original_state_dict[new] = original_state_dict.pop(old)\n-\n-    # now attempt to load the state dictionary with updated names\n-    # note that we now call it `mega.layers` instead of `mega.encoders` due to hugging face style\n-    print(\"HF Mega encoder:\", hf_mlm.mega.layers.load_state_dict(original_state_dict))\n-\n-    # load the MLM head weights directly\n-    print(\n-        \"HF Mega MLM layer:\",\n-        hf_mlm.mlm_head.load_state_dict(\n-            torch.load(\n-                os.path.join(pretrained_checkpoint_path, \"mlm_head_weights.pt\"), map_location=\"cpu\", weights_only=True\n-            )\n-        ),\n-    )\n-\n-    # test on a randomly generated input sequence\n-    input_ids = torch.randint(0, hf_config.vocab_size, size=(4, 256))\n-    input_mask = torch.ones_like(input_ids)\n-    # mask a few tokens to make sure masking is applied appropriately :)\n-    input_mask[:, -10:] = 0\n-\n-    # run forward passes\n-    original_output = original_mlm(input_ids, input_mask, batch_first=True, ignore_mask_value=0)\n-    hf_output = hf_mlm(input_ids, input_mask)[0]\n-\n-    # print shapes and diff\n-    print(f\"original output {original_output.shape}\")\n-    print(f\"hf output {hf_output.shape}\")\n-    print(f\"max diff: {(original_output - hf_output).max()}\")  # 0.0\n-    success = torch.allclose(original_output, hf_output, atol=1e-3)\n-\n-    if success:\n-        print(\"Yay!\")\n-        hf_mlm.save_pretrained(output_path)\n-    else:\n-        raise RuntimeError(f\"Something's broken :(\\nOriginal:\\n{original_output}\\n\\nHF\\n{hf_output}\\n{hf_mlm}\")\n-\n-    if includes_tokenizer:\n-        print(\"Transferring tokenizer\")\n-        tokenizer = AutoTokenizer.from_pretrained(pretrained_checkpoint_path)\n-        tokenizer.save_pretrained(output_path)\n-\n-\n-if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser()\n-\n-    parser.add_argument(\n-        \"--pretrained_checkpoint_path\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Point to the directory containing your model weights using the official Mega repo\",\n-    )\n-\n-    parser.add_argument(\n-        \"--output_path\", default=None, type=str, required=True, help=\"Location to save the Hugging Face version\"\n-    )\n-\n-    parser.add_argument(\n-        \"--includes_tokenizer\",\n-        action=\"store_true\",\n-        help=\"Use this flag if there is a Hugging Face tokenizer in the original checkpoint repo\",\n-    )\n-\n-    args = parser.parse_args()\n-\n-    convert_checkpoint_to_huggingface(args.pretrained_checkpoint_path, args.output_path, args.includes_tokenizer)"
        },
        {
            "sha": "eb0e93d636db336fd06f15a8bf0b788705e561e3",
            "filename": "src/transformers/models/deprecated/mega/modeling_mega.py",
            "status": "removed",
            "additions": 0,
            "deletions": 2271,
            "changes": 2271,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "03b556e2eddf6d5f81f6f4a15346596dd5a32f85",
            "filename": "src/transformers/models/deprecated/mmbt/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,27 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from typing import TYPE_CHECKING\n-\n-from ....utils import _LazyModule\n-from ....utils.import_utils import define_import_structure\n-\n-\n-if TYPE_CHECKING:\n-    from .configuration_mmbt import *\n-    from .modeling_mmbt import *\n-else:\n-    import sys\n-\n-    _file = globals()[\"__file__\"]\n-    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "1c58e4e6cdd0d0a8c87b9b94ca896c87970b5654",
            "filename": "src/transformers/models/deprecated/mmbt/configuration_mmbt.py",
            "status": "removed",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2Fconfiguration_mmbt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2Fconfiguration_mmbt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2Fconfiguration_mmbt.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,45 +0,0 @@\n-# coding=utf-8\n-# Copyright (c) Facebook, Inc. and its affiliates.\n-# Copyright (c) HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"MMBT configuration\"\"\"\n-\n-from ....utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class MMBTConfig:\n-    \"\"\"\n-    This is the configuration class to store the configuration of a [`MMBTModel`]. It is used to instantiate a MMBT\n-    model according to the specified arguments, defining the model architecture.\n-\n-    Args:\n-        config ([`PreTrainedConfig`]):\n-            Config of the underlying Transformer models. Its values are copied over to use a single config.\n-        num_labels (`int`, *optional*):\n-            Size of final Linear layer for classification.\n-        modal_hidden_size (`int`, *optional*, defaults to 2048):\n-            Embedding dimension of the non-text modality encoder.\n-    \"\"\"\n-\n-    def __init__(self, config, num_labels=None, modal_hidden_size=2048):\n-        self.__dict__ = config.__dict__\n-        self.modal_hidden_size = modal_hidden_size\n-        if num_labels:\n-            self.num_labels = num_labels\n-\n-\n-__all__ = [\"MMBTConfig\"]"
        },
        {
            "sha": "ed8e3847b578cdc8fe105616670575bbfee4ecbb",
            "filename": "src/transformers/models/deprecated/mmbt/modeling_mmbt.py",
            "status": "removed",
            "additions": 0,
            "deletions": 399,
            "changes": 399,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2Fmodeling_mmbt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2Fmodeling_mmbt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmmbt%2Fmodeling_mmbt.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,399 +0,0 @@\n-# coding=utf-8\n-# Copyright (c) Facebook, Inc. and its affiliates.\n-# Copyright (c) HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch MMBT model.\"\"\"\n-\n-import torch\n-from torch import nn\n-from torch.nn import CrossEntropyLoss, MSELoss\n-\n-from ....modeling_outputs import BaseModelOutputWithPooling, SequenceClassifierOutput\n-from ....modeling_utils import ModuleUtilsMixin\n-from ....utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-_CONFIG_FOR_DOC = \"MMBTConfig\"\n-\n-\n-class ModalEmbeddings(nn.Module):\n-    \"\"\"Generic Modal Embeddings which takes in an encoder, and a transformer embedding.\"\"\"\n-\n-    def __init__(self, config, encoder, embeddings):\n-        super().__init__()\n-        self.config = config\n-        self.encoder = encoder\n-        self.proj_embeddings = nn.Linear(config.modal_hidden_size, config.hidden_size)\n-        self.position_embeddings = embeddings.position_embeddings\n-        self.token_type_embeddings = embeddings.token_type_embeddings\n-        self.word_embeddings = embeddings.word_embeddings\n-        self.LayerNorm = embeddings.LayerNorm\n-        self.dropout = nn.Dropout(p=config.hidden_dropout_prob)\n-\n-    def forward(self, input_modal, start_token=None, end_token=None, position_ids=None, token_type_ids=None):\n-        token_embeddings = self.proj_embeddings(self.encoder(input_modal))\n-        seq_length = token_embeddings.size(1)\n-\n-        if start_token is not None:\n-            start_token_embeds = self.word_embeddings(start_token)\n-            seq_length += 1\n-            token_embeddings = torch.cat([start_token_embeds.unsqueeze(1), token_embeddings], dim=1)\n-\n-        if end_token is not None:\n-            end_token_embeds = self.word_embeddings(end_token)\n-            seq_length += 1\n-            token_embeddings = torch.cat([token_embeddings, end_token_embeds.unsqueeze(1)], dim=1)\n-\n-        if position_ids is None:\n-            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_modal.device)\n-            position_ids = position_ids.unsqueeze(0).expand(input_modal.size(0), seq_length)\n-\n-        if token_type_ids is None:\n-            token_type_ids = torch.zeros(\n-                (input_modal.size(0), seq_length), dtype=torch.long, device=input_modal.device\n-            )\n-\n-        position_embeddings = self.position_embeddings(position_ids)\n-        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-        embeddings = token_embeddings + position_embeddings + token_type_embeddings\n-        embeddings = self.LayerNorm(embeddings)\n-        embeddings = self.dropout(embeddings)\n-        return embeddings\n-\n-\n-MMBT_START_DOCSTRING = r\"\"\"\n-    MMBT model was proposed in [Supervised Multimodal Bitransformers for Classifying Images and\n-    Text](https://github.com/facebookresearch/mmbt) by Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, Davide Testuggine.\n-    It's a supervised multimodal bitransformer model that fuses information from text and other image encoders, and\n-    obtain state-of-the-art performance on various multimodal classification benchmark tasks.\n-\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`MMBTConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration.\n-        transformer (`nn.Module`): A text transformer that is used by MMBT.\n-            It should have embeddings, encoder, and pooler attributes.\n-        encoder (`nn.Module`): Encoder for the second modality.\n-            It should take in a batch of modal inputs and return k, n dimension embeddings.\n-\"\"\"\n-\n-MMBT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_modal (`torch.FloatTensor` of shape `(batch_size, ***)`):\n-            The other modality data. It will be the shape that the encoder for that type expects. e.g. With an Image\n-            Encoder, the shape would be (batch_size, channels, height, width)\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. It does not expect [CLS] token to be added as it's\n-            appended to the end of other modality embeddings. Indices can be obtained using [`AutoTokenizer`]. See\n-            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        modal_start_tokens (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Optional start token to be added to Other Modality Embedding. [CLS] Most commonly used for classification\n-            tasks.\n-        modal_end_tokens (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Optional end token to be added to Other Modality Embedding. [SEP] Most commonly used.\n-        attention_mask (*optional*) `torch.FloatTensor` of shape `(batch_size, sequence_length)`:\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (*optional*) `torch.LongTensor` of shape `(batch_size, sequence_length)`:\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        modal_token_type_ids (*optional*) `torch.LongTensor` of shape `(batch_size, modal_sequence_length)`:\n-            Segment token indices to indicate different portions of the non-text modality. The embeddings from these\n-            tokens will be summed with the respective token embeddings for the non-text modality.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        modal_position_ids (`torch.LongTensor` of shape `(batch_size, modal_sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings for the non-text modality.\n-            Selected in the range `[0, config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, embedding_dim)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare MMBT Model outputting raw hidden-states without any specific head on top.\",\n-    MMBT_START_DOCSTRING,\n-)\n-class MMBTModel(nn.Module, ModuleUtilsMixin):\n-    def __init__(self, config, transformer, encoder):\n-        super().__init__()\n-        self.config = config\n-        self.transformer = transformer\n-        self.modal_encoder = ModalEmbeddings(config, encoder, transformer.embeddings)\n-\n-    @add_start_docstrings_to_model_forward(MMBT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=_CONFIG_FOR_DOC)\n-    def forward(\n-        self,\n-        input_modal,\n-        input_ids=None,\n-        modal_start_tokens=None,\n-        modal_end_tokens=None,\n-        attention_mask=None,\n-        token_type_ids=None,\n-        modal_token_type_ids=None,\n-        position_ids=None,\n-        modal_position_ids=None,\n-        inputs_embeds=None,\n-        encoder_hidden_states=None,\n-        encoder_attention_mask=None,\n-        output_attentions=None,\n-        output_hidden_states=None,\n-        return_dict=None,\n-    ):\n-        r\"\"\"\n-        Returns:\n-\n-        Examples:\n-\n-        ```python\n-        # For example purposes. Not runnable.\n-        transformer = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n-        encoder = ImageEncoder(args)\n-        mmbt = MMBTModel(config, transformer, encoder)\n-        ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input_txt_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_txt_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n-\n-        modal_embeddings = self.modal_encoder(\n-            input_modal,\n-            start_token=modal_start_tokens,\n-            end_token=modal_end_tokens,\n-            position_ids=modal_position_ids,\n-            token_type_ids=modal_token_type_ids,\n-        )\n-\n-        input_modal_shape = modal_embeddings.size()[:-1]\n-\n-        if token_type_ids is None:\n-            token_type_ids = torch.ones(input_txt_shape, dtype=torch.long, device=device)\n-\n-        txt_embeddings = self.transformer.embeddings(\n-            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n-        )\n-\n-        embedding_output = torch.cat([modal_embeddings, txt_embeddings], 1)\n-\n-        input_shape = embedding_output.size()[:-1]\n-\n-        if attention_mask is None:\n-            attention_mask = torch.ones(input_shape, device=device)\n-        else:\n-            attention_mask = torch.cat(\n-                [torch.ones(input_modal_shape, device=device, dtype=torch.long), attention_mask], dim=1\n-            )\n-        if encoder_attention_mask is None:\n-            encoder_attention_mask = torch.ones(input_shape, device=device)\n-        else:\n-            encoder_attention_mask = torch.cat(\n-                [torch.ones(input_modal_shape, device=device), encoder_attention_mask], dim=1\n-            )\n-\n-        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n-        encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n-\n-        encoder_outputs = self.transformer.encoder(\n-            embedding_output,\n-            attention_mask=extended_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_extended_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        sequence_output = encoder_outputs[0]\n-        pooled_output = self.transformer.pooler(sequence_output)\n-\n-        if not return_dict:\n-            return (sequence_output, pooled_output) + encoder_outputs[1:]\n-\n-        return BaseModelOutputWithPooling(\n-            last_hidden_state=sequence_output,\n-            pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-        )\n-\n-    def get_input_embeddings(self):\n-        return self.embeddings.word_embeddings\n-\n-    def set_input_embeddings(self, value):\n-        self.embeddings.word_embeddings = value\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    MMBT Model with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n-    \"\"\",\n-    MMBT_START_DOCSTRING,\n-    MMBT_INPUTS_DOCSTRING,\n-)\n-class MMBTForClassification(nn.Module):\n-    r\"\"\"\n-    **labels**: (*optional*) `torch.LongTensor` of shape `(batch_size,)`:\n-        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-\n-    Returns: *Tuple* comprising various elements depending on the configuration (config) and inputs: **loss**:\n-    (*optional*, returned when `labels` is provided) `torch.FloatTensor` of shape `(1,)`: Classification (or\n-    regression if config.num_labels==1) loss. **logits**:\n-        `torch.FloatTensor` of shape `(batch_size, config.num_labels)` Classification (or regression if\n-        config.num_labels==1) scores (before SoftMax).\n-    **hidden_states**: (*optional*, returned when `output_hidden_states=True`) list of `torch.FloatTensor` (one for\n-    the output of each layer + the output of the embeddings) of shape `(batch_size, sequence_length, hidden_size)`:\n-    Hidden-states of the model at the output of each layer plus the initial embedding outputs. **attentions**:\n-    (*optional*, returned when `output_attentions=True`) list of `torch.FloatTensor` (one for each layer) of shape\n-    `(batch_size, num_heads, sequence_length, sequence_length)`: Attentions weights after the attention softmax, used\n-    to compute the weighted average in the self-attention heads.\n-\n-    Examples:\n-\n-    ```python\n-    # For example purposes. Not runnable.\n-    transformer = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n-    encoder = ImageEncoder(args)\n-    model = MMBTForClassification(config, transformer, encoder)\n-    outputs = model(input_modal, input_ids, labels=labels)\n-    loss, logits = outputs[:2]\n-    ```\"\"\"\n-\n-    def __init__(self, config, transformer, encoder):\n-        super().__init__()\n-        self.num_labels = config.num_labels\n-\n-        self.mmbt = MMBTModel(config, transformer, encoder)\n-        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n-\n-    def forward(\n-        self,\n-        input_modal,\n-        input_ids=None,\n-        modal_start_tokens=None,\n-        modal_end_tokens=None,\n-        attention_mask=None,\n-        token_type_ids=None,\n-        modal_token_type_ids=None,\n-        position_ids=None,\n-        modal_position_ids=None,\n-        inputs_embeds=None,\n-        labels=None,\n-        return_dict=None,\n-    ):\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        outputs = self.mmbt(\n-            input_modal=input_modal,\n-            input_ids=input_ids,\n-            modal_start_tokens=modal_start_tokens,\n-            modal_end_tokens=modal_end_tokens,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            modal_token_type_ids=modal_token_type_ids,\n-            position_ids=position_ids,\n-            modal_position_ids=modal_position_ids,\n-            inputs_embeds=inputs_embeds,\n-            return_dict=return_dict,\n-        )\n-\n-        pooled_output = outputs[1]\n-\n-        pooled_output = self.dropout(pooled_output)\n-        logits = self.classifier(pooled_output)\n-\n-        loss = None\n-        if labels is not None:\n-            if self.num_labels == 1:\n-                #  We are doing regression\n-                loss_fct = MSELoss()\n-                loss = loss_fct(logits.view(-1), labels.view(-1))\n-            else:\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n-        return SequenceClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-__all__ = [\"MMBTForClassification\", \"MMBTModel\", \"ModalEmbeddings\"]"
        },
        {
            "sha": "c5373969ce7831491b0d5fa5495078fb1d3f6e4e",
            "filename": "src/transformers/models/deprecated/nat/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,27 +0,0 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from typing import TYPE_CHECKING\n-\n-from ....utils import _LazyModule\n-from ....utils.import_utils import define_import_structure\n-\n-\n-if TYPE_CHECKING:\n-    from .configuration_nat import *\n-    from .modeling_nat import *\n-else:\n-    import sys\n-\n-    _file = globals()[\"__file__\"]\n-    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "c4de795e05796369f3ca74c7eeb18f0118f18f95",
            "filename": "src/transformers/models/deprecated/nat/configuration_nat.py",
            "status": "removed",
            "additions": 0,
            "deletions": 148,
            "changes": 148,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fconfiguration_nat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fconfiguration_nat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fconfiguration_nat.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,148 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Neighborhood Attention Transformer model configuration\"\"\"\n-\n-from ....configuration_utils import PreTrainedConfig\n-from ....utils import logging\n-from ....utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class NatConfig(BackboneConfigMixin, PreTrainedConfig):\n-    r\"\"\"\n-    This is the configuration class to store the configuration of a [`NatModel`]. It is used to instantiate a Nat model\n-    according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n-    defaults will yield a similar configuration to that of the Nat\n-    [shi-labs/nat-mini-in1k-224](https://huggingface.co/shi-labs/nat-mini-in1k-224) architecture.\n-\n-    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PreTrainedConfig`] for more information.\n-\n-    Args:\n-        patch_size (`int`, *optional*, defaults to 4):\n-            The size (resolution) of each patch. NOTE: Only patch size of 4 is supported at the moment.\n-        num_channels (`int`, *optional*, defaults to 3):\n-            The number of input channels.\n-        embed_dim (`int`, *optional*, defaults to 64):\n-            Dimensionality of patch embedding.\n-        depths (`list[int]`, *optional*, defaults to `[3, 4, 6, 5]`):\n-            Number of layers in each level of the encoder.\n-        num_heads (`list[int]`, *optional*, defaults to `[2, 4, 8, 16]`):\n-            Number of attention heads in each layer of the Transformer encoder.\n-        kernel_size (`int`, *optional*, defaults to 7):\n-            Neighborhood Attention kernel size.\n-        mlp_ratio (`float`, *optional*, defaults to 3.0):\n-            Ratio of MLP hidden dimensionality to embedding dimensionality.\n-        qkv_bias (`bool`, *optional*, defaults to `True`):\n-            Whether or not a learnable bias should be added to the queries, keys and values.\n-        hidden_dropout_prob (`float`, *optional*, defaults to 0.0):\n-            The dropout probability for all fully connected layers in the embeddings and encoder.\n-        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.0):\n-            The dropout ratio for the attention probabilities.\n-        drop_path_rate (`float`, *optional*, defaults to 0.1):\n-            Stochastic depth rate.\n-        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n-            The non-linear activation function (function or string) in the encoder. If string, `\"gelu\"`, `\"relu\"`,\n-            `\"selu\"` and `\"gelu_new\"` are supported.\n-        initializer_range (`float`, *optional*, defaults to 0.02):\n-            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n-        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n-            The epsilon used by the layer normalization layers.\n-        layer_scale_init_value (`float`, *optional*, defaults to 0.0):\n-            The initial value for the layer scale. Disabled if <=0.\n-        out_features (`list[str]`, *optional*):\n-            If used as backbone, list of features to output. Can be any of `\"stem\"`, `\"stage1\"`, `\"stage2\"`, etc.\n-            (depending on how many stages the model has). If unset and `out_indices` is set, will default to the\n-            corresponding stages. If unset and `out_indices` is unset, will default to the last stage. Must be in the\n-            same order as defined in the `stage_names` attribute.\n-        out_indices (`list[int]`, *optional*):\n-            If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how\n-            many stages the model has). If unset and `out_features` is set, will default to the corresponding stages.\n-            If unset and `out_features` is unset, will default to the last stage. Must be in the\n-            same order as defined in the `stage_names` attribute.\n-\n-    Example:\n-\n-    ```python\n-    >>> from transformers import NatConfig, NatModel\n-\n-    >>> # Initializing a Nat shi-labs/nat-mini-in1k-224 style configuration\n-    >>> configuration = NatConfig()\n-\n-    >>> # Initializing a model (with random weights) from the shi-labs/nat-mini-in1k-224 style configuration\n-    >>> model = NatModel(configuration)\n-\n-    >>> # Accessing the model configuration\n-    >>> configuration = model.config\n-    ```\"\"\"\n-\n-    model_type = \"nat\"\n-\n-    attribute_map = {\n-        \"num_attention_heads\": \"num_heads\",\n-        \"num_hidden_layers\": \"num_layers\",\n-    }\n-\n-    def __init__(\n-        self,\n-        patch_size=4,\n-        num_channels=3,\n-        embed_dim=64,\n-        depths=[3, 4, 6, 5],\n-        num_heads=[2, 4, 8, 16],\n-        kernel_size=7,\n-        mlp_ratio=3.0,\n-        qkv_bias=True,\n-        hidden_dropout_prob=0.0,\n-        attention_probs_dropout_prob=0.0,\n-        drop_path_rate=0.1,\n-        hidden_act=\"gelu\",\n-        initializer_range=0.02,\n-        layer_norm_eps=1e-5,\n-        layer_scale_init_value=0.0,\n-        out_features=None,\n-        out_indices=None,\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs)\n-\n-        self.patch_size = patch_size\n-        self.num_channels = num_channels\n-        self.embed_dim = embed_dim\n-        self.depths = depths\n-        self.num_layers = len(depths)\n-        self.num_heads = num_heads\n-        self.kernel_size = kernel_size\n-        self.mlp_ratio = mlp_ratio\n-        self.qkv_bias = qkv_bias\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.drop_path_rate = drop_path_rate\n-        self.hidden_act = hidden_act\n-        self.layer_norm_eps = layer_norm_eps\n-        self.initializer_range = initializer_range\n-        # we set the hidden_size attribute in order to make Nat work with VisionEncoderDecoderModel\n-        # this indicates the channel dimension after the last stage of the model\n-        self.hidden_size = int(embed_dim * 2 ** (len(depths) - 1))\n-        self.layer_scale_init_value = layer_scale_init_value\n-        self.stage_names = [\"stem\"] + [f\"stage{idx}\" for idx in range(1, len(depths) + 1)]\n-        self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n-            out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n-        )\n-\n-\n-__all__ = [\"NatConfig\"]"
        },
        {
            "sha": "20e2d62ee57f18636d8547d4f28b342e4e314ae4",
            "filename": "src/transformers/models/deprecated/nat/modeling_nat.py",
            "status": "removed",
            "additions": 0,
            "deletions": 895,
            "changes": 895,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,895 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 SHI Labs and The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch Neighborhood Attention Transformer model.\"\"\"\n-\n-import math\n-from dataclasses import dataclass\n-from typing import Optional, Union\n-\n-import torch\n-from torch import nn\n-\n-from ....activations import ACT2FN\n-from ....modeling_outputs import BackboneOutput\n-from ....modeling_utils import PreTrainedModel\n-from ....utils import (\n-    ModelOutput,\n-    OptionalDependencyNotAvailable,\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    is_natten_available,\n-    logging,\n-    replace_return_docstrings,\n-    requires_backends,\n-)\n-from ....utils.backbone_utils import BackboneMixin\n-from .configuration_nat import NatConfig\n-\n-\n-if is_natten_available():\n-    from natten.functional import natten2dav, natten2dqkrpb\n-else:\n-\n-    def natten2dqkrpb(*args, **kwargs):\n-        raise OptionalDependencyNotAvailable()\n-\n-    def natten2dav(*args, **kwargs):\n-        raise OptionalDependencyNotAvailable()\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-# General docstring\n-_CONFIG_FOR_DOC = \"NatConfig\"\n-\n-# Base docstring\n-_CHECKPOINT_FOR_DOC = \"shi-labs/nat-mini-in1k-224\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 7, 7, 512]\n-\n-# Image classification docstring\n-_IMAGE_CLASS_CHECKPOINT = \"shi-labs/nat-mini-in1k-224\"\n-_IMAGE_CLASS_EXPECTED_OUTPUT = \"tiger cat\"\n-\n-\n-# drop_path and NatDropPath are from the timm library.\n-\n-\n-@dataclass\n-class NatEncoderOutput(ModelOutput):\n-    \"\"\"\n-    Nat encoder's outputs, with potential hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each stage) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, hidden_size, height, width)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n-            include the spatial dimensions.\n-    \"\"\"\n-\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    reshaped_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-\n-\n-@dataclass\n-class NatModelOutput(ModelOutput):\n-    \"\"\"\n-    Nat model's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`, *optional*, returned when `add_pooling_layer=True` is passed):\n-            Average pooling of the last layer hidden-state.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each stage) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, hidden_size, height, width)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n-            include the spatial dimensions.\n-    \"\"\"\n-\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    pooler_output: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    reshaped_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-\n-\n-@dataclass\n-class NatImageClassifierOutput(ModelOutput):\n-    \"\"\"\n-    Nat outputs for image classification.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Classification (or regression if config.num_labels==1) loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each stage) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, hidden_size, height, width)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n-            include the spatial dimensions.\n-    \"\"\"\n-\n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    reshaped_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-\n-\n-class NatEmbeddings(nn.Module):\n-    \"\"\"\n-    Construct the patch and position embeddings.\n-    \"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-\n-        self.patch_embeddings = NatPatchEmbeddings(config)\n-\n-        self.norm = nn.LayerNorm(config.embed_dim)\n-        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-\n-    def forward(self, pixel_values: Optional[torch.FloatTensor]) -> tuple[torch.Tensor]:\n-        embeddings = self.patch_embeddings(pixel_values)\n-        embeddings = self.norm(embeddings)\n-\n-        embeddings = self.dropout(embeddings)\n-\n-        return embeddings\n-\n-\n-class NatPatchEmbeddings(nn.Module):\n-    \"\"\"\n-    This class turns `pixel_values` of shape `(batch_size, num_channels, height, width)` into the initial\n-    `hidden_states` (patch embeddings) of shape `(batch_size, height, width, hidden_size)` to be consumed by a\n-    Transformer.\n-    \"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-        patch_size = config.patch_size\n-        num_channels, hidden_size = config.num_channels, config.embed_dim\n-        self.num_channels = num_channels\n-\n-        if patch_size == 4:\n-            pass\n-        else:\n-            # TODO: Support arbitrary patch sizes.\n-            raise ValueError(\"Dinat only supports patch size of 4 at the moment.\")\n-\n-        self.projection = nn.Sequential(\n-            nn.Conv2d(self.num_channels, hidden_size // 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n-            nn.Conv2d(hidden_size // 2, hidden_size, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n-        )\n-\n-    def forward(self, pixel_values: Optional[torch.FloatTensor]) -> torch.Tensor:\n-        _, num_channels, height, width = pixel_values.shape\n-        if num_channels != self.num_channels:\n-            raise ValueError(\n-                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n-            )\n-        embeddings = self.projection(pixel_values)\n-        embeddings = embeddings.permute(0, 2, 3, 1)\n-\n-        return embeddings\n-\n-\n-class NatDownsampler(nn.Module):\n-    \"\"\"\n-    Convolutional Downsampling Layer.\n-\n-    Args:\n-        dim (`int`):\n-            Number of input channels.\n-        norm_layer (`nn.Module`, *optional*, defaults to `nn.LayerNorm`):\n-            Normalization layer class.\n-    \"\"\"\n-\n-    def __init__(self, dim: int, norm_layer: nn.Module = nn.LayerNorm) -> None:\n-        super().__init__()\n-        self.dim = dim\n-        self.reduction = nn.Conv2d(dim, 2 * dim, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n-        self.norm = norm_layer(2 * dim)\n-\n-    def forward(self, input_feature: torch.Tensor) -> torch.Tensor:\n-        input_feature = self.reduction(input_feature.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n-        input_feature = self.norm(input_feature)\n-        return input_feature\n-\n-\n-def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:\n-    \"\"\"\n-    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n-\n-    \"\"\"\n-    if drop_prob == 0.0 or not training:\n-        return input\n-    keep_prob = 1 - drop_prob\n-    shape = (input.shape[0],) + (1,) * (input.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n-    random_tensor = keep_prob + torch.rand(shape, dtype=input.dtype, device=input.device)\n-    random_tensor.floor_()  # binarize\n-    output = input.div(keep_prob) * random_tensor\n-    return output\n-\n-\n-class NatDropPath(nn.Module):\n-    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n-\n-    def __init__(self, drop_prob: Optional[float] = None) -> None:\n-        super().__init__()\n-        self.drop_prob = drop_prob\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        return drop_path(hidden_states, self.drop_prob, self.training)\n-\n-    def extra_repr(self) -> str:\n-        return f\"p={self.drop_prob}\"\n-\n-\n-class NeighborhoodAttention(nn.Module):\n-    def __init__(self, config, dim, num_heads, kernel_size):\n-        super().__init__()\n-        if dim % num_heads != 0:\n-            raise ValueError(\n-                f\"The hidden size ({dim}) is not a multiple of the number of attention heads ({num_heads})\"\n-            )\n-\n-        self.num_attention_heads = num_heads\n-        self.attention_head_size = int(dim / num_heads)\n-        self.all_head_size = self.num_attention_heads * self.attention_head_size\n-        self.kernel_size = kernel_size\n-\n-        # rpb is learnable relative positional biases; same concept is used Swin.\n-        self.rpb = nn.Parameter(torch.zeros(num_heads, (2 * self.kernel_size - 1), (2 * self.kernel_size - 1)))\n-\n-        self.query = nn.Linear(self.all_head_size, self.all_head_size, bias=config.qkv_bias)\n-        self.key = nn.Linear(self.all_head_size, self.all_head_size, bias=config.qkv_bias)\n-        self.value = nn.Linear(self.all_head_size, self.all_head_size, bias=config.qkv_bias)\n-\n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-\n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 3, 1, 2, 4)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n-        query_layer = self.transpose_for_scores(self.query(hidden_states))\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        # Apply the scale factor before computing attention weights. It's usually more efficient because\n-        # attention weights are typically a bigger tensor compared to query.\n-        # It gives identical results because scalars are commutable in matrix multiplication.\n-        query_layer = query_layer / math.sqrt(self.attention_head_size)\n-\n-        # Compute NA between \"query\" and \"key\" to get the raw attention scores, and add relative positional biases.\n-        attention_scores = natten2dqkrpb(query_layer, key_layer, self.rpb, self.kernel_size, 1)\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        context_layer = natten2dav(attention_probs, value_layer, self.kernel_size, 1)\n-        context_layer = context_layer.permute(0, 2, 3, 1, 4).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        return outputs\n-\n-\n-class NeighborhoodAttentionOutput(nn.Module):\n-    def __init__(self, config, dim):\n-        super().__init__()\n-        self.dense = nn.Linear(dim, dim)\n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-\n-    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n-        hidden_states = self.dense(hidden_states)\n-        hidden_states = self.dropout(hidden_states)\n-\n-        return hidden_states\n-\n-\n-class NeighborhoodAttentionModule(nn.Module):\n-    def __init__(self, config, dim, num_heads, kernel_size):\n-        super().__init__()\n-        self.self = NeighborhoodAttention(config, dim, num_heads, kernel_size)\n-        self.output = NeighborhoodAttentionOutput(config, dim)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self(hidden_states, output_attentions)\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n-\n-\n-class NatIntermediate(nn.Module):\n-    def __init__(self, config, dim):\n-        super().__init__()\n-        self.dense = nn.Linear(dim, int(config.mlp_ratio * dim))\n-        if isinstance(config.hidden_act, str):\n-            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n-        else:\n-            self.intermediate_act_fn = config.hidden_act\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        hidden_states = self.dense(hidden_states)\n-        hidden_states = self.intermediate_act_fn(hidden_states)\n-        return hidden_states\n-\n-\n-class NatOutput(nn.Module):\n-    def __init__(self, config, dim):\n-        super().__init__()\n-        self.dense = nn.Linear(int(config.mlp_ratio * dim), dim)\n-        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        hidden_states = self.dense(hidden_states)\n-        hidden_states = self.dropout(hidden_states)\n-        return hidden_states\n-\n-\n-class NatLayer(nn.Module):\n-    def __init__(self, config, dim, num_heads, drop_path_rate=0.0):\n-        super().__init__()\n-        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n-        self.kernel_size = config.kernel_size\n-        self.layernorm_before = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n-        self.attention = NeighborhoodAttentionModule(config, dim, num_heads, kernel_size=self.kernel_size)\n-        self.drop_path = NatDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n-        self.layernorm_after = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n-        self.intermediate = NatIntermediate(config, dim)\n-        self.output = NatOutput(config, dim)\n-        self.layer_scale_parameters = (\n-            nn.Parameter(config.layer_scale_init_value * torch.ones((2, dim)), requires_grad=True)\n-            if config.layer_scale_init_value > 0\n-            else None\n-        )\n-\n-    def maybe_pad(self, hidden_states, height, width):\n-        window_size = self.kernel_size\n-        pad_values = (0, 0, 0, 0, 0, 0)\n-        if height < window_size or width < window_size:\n-            pad_l = pad_t = 0\n-            pad_r = max(0, window_size - width)\n-            pad_b = max(0, window_size - height)\n-            pad_values = (0, 0, pad_l, pad_r, pad_t, pad_b)\n-            hidden_states = nn.functional.pad(hidden_states, pad_values)\n-        return hidden_states, pad_values\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n-        batch_size, height, width, channels = hidden_states.size()\n-        shortcut = hidden_states\n-\n-        hidden_states = self.layernorm_before(hidden_states)\n-        # pad hidden_states if they are smaller than kernel size\n-        hidden_states, pad_values = self.maybe_pad(hidden_states, height, width)\n-\n-        _, height_pad, width_pad, _ = hidden_states.shape\n-\n-        attention_outputs = self.attention(hidden_states, output_attentions=output_attentions)\n-\n-        attention_output = attention_outputs[0]\n-\n-        was_padded = pad_values[3] > 0 or pad_values[5] > 0\n-        if was_padded:\n-            attention_output = attention_output[:, :height, :width, :].contiguous()\n-\n-        if self.layer_scale_parameters is not None:\n-            attention_output = self.layer_scale_parameters[0] * attention_output\n-\n-        hidden_states = shortcut + self.drop_path(attention_output)\n-\n-        layer_output = self.layernorm_after(hidden_states)\n-        layer_output = self.output(self.intermediate(layer_output))\n-\n-        if self.layer_scale_parameters is not None:\n-            layer_output = self.layer_scale_parameters[1] * layer_output\n-\n-        layer_output = hidden_states + self.drop_path(layer_output)\n-\n-        layer_outputs = (layer_output, attention_outputs[1]) if output_attentions else (layer_output,)\n-        return layer_outputs\n-\n-\n-class NatStage(nn.Module):\n-    def __init__(self, config, dim, depth, num_heads, drop_path_rate, downsample):\n-        super().__init__()\n-        self.config = config\n-        self.dim = dim\n-        self.layers = nn.ModuleList(\n-            [\n-                NatLayer(\n-                    config=config,\n-                    dim=dim,\n-                    num_heads=num_heads,\n-                    drop_path_rate=drop_path_rate[i],\n-                )\n-                for i in range(depth)\n-            ]\n-        )\n-\n-        # patch merging layer\n-        if downsample is not None:\n-            self.downsample = downsample(dim=dim, norm_layer=nn.LayerNorm)\n-        else:\n-            self.downsample = None\n-\n-        self.pointing = False\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n-        _, height, width, _ = hidden_states.size()\n-        for i, layer_module in enumerate(self.layers):\n-            layer_outputs = layer_module(hidden_states, output_attentions)\n-            hidden_states = layer_outputs[0]\n-\n-        hidden_states_before_downsampling = hidden_states\n-        if self.downsample is not None:\n-            hidden_states = self.downsample(hidden_states_before_downsampling)\n-\n-        stage_outputs = (hidden_states, hidden_states_before_downsampling)\n-\n-        if output_attentions:\n-            stage_outputs += layer_outputs[1:]\n-        return stage_outputs\n-\n-\n-class NatEncoder(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.num_levels = len(config.depths)\n-        self.config = config\n-        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\")]\n-        self.levels = nn.ModuleList(\n-            [\n-                NatStage(\n-                    config=config,\n-                    dim=int(config.embed_dim * 2**i_layer),\n-                    depth=config.depths[i_layer],\n-                    num_heads=config.num_heads[i_layer],\n-                    drop_path_rate=dpr[sum(config.depths[:i_layer]) : sum(config.depths[: i_layer + 1])],\n-                    downsample=NatDownsampler if (i_layer < self.num_levels - 1) else None,\n-                )\n-                for i_layer in range(self.num_levels)\n-            ]\n-        )\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        output_hidden_states_before_downsampling: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n-    ) -> Union[tuple, NatEncoderOutput]:\n-        all_hidden_states = () if output_hidden_states else None\n-        all_reshaped_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-\n-        if output_hidden_states:\n-            # rearrange b h w c -> b c h w\n-            reshaped_hidden_state = hidden_states.permute(0, 3, 1, 2)\n-            all_hidden_states += (hidden_states,)\n-            all_reshaped_hidden_states += (reshaped_hidden_state,)\n-\n-        for i, layer_module in enumerate(self.levels):\n-            layer_outputs = layer_module(hidden_states, output_attentions)\n-\n-            hidden_states = layer_outputs[0]\n-            hidden_states_before_downsampling = layer_outputs[1]\n-\n-            if output_hidden_states and output_hidden_states_before_downsampling:\n-                # rearrange b h w c -> b c h w\n-                reshaped_hidden_state = hidden_states_before_downsampling.permute(0, 3, 1, 2)\n-                all_hidden_states += (hidden_states_before_downsampling,)\n-                all_reshaped_hidden_states += (reshaped_hidden_state,)\n-            elif output_hidden_states and not output_hidden_states_before_downsampling:\n-                # rearrange b h w c -> b c h w\n-                reshaped_hidden_state = hidden_states.permute(0, 3, 1, 2)\n-                all_hidden_states += (hidden_states,)\n-                all_reshaped_hidden_states += (reshaped_hidden_state,)\n-\n-            if output_attentions:\n-                all_self_attentions += layer_outputs[2:]\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n-\n-        return NatEncoderOutput(\n-            last_hidden_state=hidden_states,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-            reshaped_hidden_states=all_reshaped_hidden_states,\n-        )\n-\n-\n-class NatPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config: NatConfig\n-    base_model_prefix = \"nat\"\n-    main_input_name = \"pixel_values\"\n-\n-\n-NAT_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n-    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`NatConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-NAT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See [`ViTImageProcessor.__call__`]\n-            for details.\n-\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Nat Model transformer outputting raw hidden-states without any specific head on top.\",\n-    NAT_START_DOCSTRING,\n-)\n-class NatModel(NatPreTrainedModel):\n-    def __init__(self, config, add_pooling_layer=True):\n-        super().__init__(config)\n-\n-        requires_backends(self, [\"natten\"])\n-\n-        self.config = config\n-        self.num_levels = len(config.depths)\n-        self.num_features = int(config.embed_dim * 2 ** (self.num_levels - 1))\n-\n-        self.embeddings = NatEmbeddings(config)\n-        self.encoder = NatEncoder(config)\n-\n-        self.layernorm = nn.LayerNorm(self.num_features, eps=config.layer_norm_eps)\n-        self.pooler = nn.AdaptiveAvgPool1d(1) if add_pooling_layer else None\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def get_input_embeddings(self):\n-        return self.embeddings.patch_embeddings\n-\n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n-    @add_start_docstrings_to_model_forward(NAT_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=NatModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"vision\",\n-        expected_output=_EXPECTED_OUTPUT_SHAPE,\n-    )\n-    def forward(\n-        self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, NatModelOutput]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        if pixel_values is None:\n-            raise ValueError(\"You have to specify pixel_values\")\n-\n-        embedding_output = self.embeddings(pixel_values)\n-\n-        encoder_outputs = self.encoder(\n-            embedding_output,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        sequence_output = encoder_outputs[0]\n-        sequence_output = self.layernorm(sequence_output)\n-\n-        pooled_output = None\n-        if self.pooler is not None:\n-            pooled_output = self.pooler(sequence_output.flatten(1, 2).transpose(1, 2))\n-            pooled_output = torch.flatten(pooled_output, 1)\n-\n-        if not return_dict:\n-            output = (sequence_output, pooled_output) + encoder_outputs[1:]\n-\n-            return output\n-\n-        return NatModelOutput(\n-            last_hidden_state=sequence_output,\n-            pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-            reshaped_hidden_states=encoder_outputs.reshaped_hidden_states,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Nat Model transformer with an image classification head on top (a linear layer on top of the final hidden state of\n-    the [CLS] token) e.g. for ImageNet.\n-    \"\"\",\n-    NAT_START_DOCSTRING,\n-)\n-class NatForImageClassification(NatPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-\n-        requires_backends(self, [\"natten\"])\n-\n-        self.num_labels = config.num_labels\n-        self.nat = NatModel(config)\n-\n-        # Classifier head\n-        self.classifier = (\n-            nn.Linear(self.nat.num_features, config.num_labels) if config.num_labels > 0 else nn.Identity()\n-        )\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(NAT_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_IMAGE_CLASS_CHECKPOINT,\n-        output_type=NatImageClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,\n-    )\n-    def forward(\n-        self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, NatImageClassifierOutput]:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        outputs = self.nat(\n-            pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        pooled_output = outputs[1]\n-\n-        logits = self.classifier(pooled_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(labels, logits, self.config)\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n-        return NatImageClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-            reshaped_hidden_states=outputs.reshaped_hidden_states,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"NAT backbone, to be used with frameworks like DETR and MaskFormer.\",\n-    NAT_START_DOCSTRING,\n-)\n-class NatBackbone(NatPreTrainedModel, BackboneMixin):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        super()._init_backbone(config)\n-\n-        requires_backends(self, [\"natten\"])\n-\n-        self.embeddings = NatEmbeddings(config)\n-        self.encoder = NatEncoder(config)\n-        self.num_features = [config.embed_dim] + [int(config.embed_dim * 2**i) for i in range(len(config.depths))]\n-\n-        # Add layer norms to hidden states of out_features\n-        hidden_states_norms = {}\n-        for stage, num_channels in zip(self.out_features, self.channels):\n-            hidden_states_norms[stage] = nn.LayerNorm(num_channels)\n-        self.hidden_states_norms = nn.ModuleDict(hidden_states_norms)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def get_input_embeddings(self):\n-        return self.embeddings.patch_embeddings\n-\n-    @add_start_docstrings_to_model_forward(NAT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BackboneOutput, config_class=_CONFIG_FOR_DOC)\n-    def forward(\n-        self,\n-        pixel_values: torch.Tensor,\n-        output_hidden_states: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> BackboneOutput:\n-        \"\"\"\n-        Returns:\n-\n-        Examples:\n-\n-        ```python\n-        >>> from transformers import AutoImageProcessor, AutoBackbone\n-        >>> import torch\n-        >>> from PIL import Image\n-        >>> import requests\n-\n-        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n-\n-        >>> processor = AutoImageProcessor.from_pretrained(\"shi-labs/nat-mini-in1k-224\")\n-        >>> model = AutoBackbone.from_pretrained(\n-        ...     \"shi-labs/nat-mini-in1k-224\", out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"]\n-        ... )\n-\n-        >>> inputs = processor(image, return_tensors=\"pt\")\n-\n-        >>> outputs = model(**inputs)\n-\n-        >>> feature_maps = outputs.feature_maps\n-        >>> list(feature_maps[-1].shape)\n-        [1, 512, 7, 7]\n-        ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n-        embedding_output = self.embeddings(pixel_values)\n-\n-        outputs = self.encoder(\n-            embedding_output,\n-            output_attentions=output_attentions,\n-            output_hidden_states=True,\n-            output_hidden_states_before_downsampling=True,\n-            return_dict=True,\n-        )\n-\n-        hidden_states = outputs.reshaped_hidden_states\n-\n-        feature_maps = ()\n-        for stage, hidden_state in zip(self.stage_names, hidden_states):\n-            if stage in self.out_features:\n-                # TODO can we simplify this?\n-                batch_size, num_channels, height, width = hidden_state.shape\n-                hidden_state = hidden_state.permute(0, 2, 3, 1).contiguous()\n-                hidden_state = hidden_state.view(batch_size, height * width, num_channels)\n-                hidden_state = self.hidden_states_norms[stage](hidden_state)\n-                hidden_state = hidden_state.view(batch_size, height, width, num_channels)\n-                hidden_state = hidden_state.permute(0, 3, 1, 2).contiguous()\n-                feature_maps += (hidden_state,)\n-\n-        if not return_dict:\n-            output = (feature_maps,)\n-            if output_hidden_states:\n-                output += (outputs.hidden_states,)\n-            return output\n-\n-        return BackboneOutput(\n-            feature_maps=feature_maps,\n-            hidden_states=outputs.hidden_states if output_hidden_states else None,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-__all__ = [\"NatForImageClassification\", \"NatModel\", \"NatPreTrainedModel\", \"NatBackbone\"]"
        },
        {
            "sha": "f0690129ae9edf84829278790b5e065bbd6608ee",
            "filename": "src/transformers/models/deprecated/nezha/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,27 +0,0 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from typing import TYPE_CHECKING\n-\n-from ....utils import _LazyModule\n-from ....utils.import_utils import define_import_structure\n-\n-\n-if TYPE_CHECKING:\n-    from .configuration_nezha import *\n-    from .modeling_nezha import *\n-else:\n-    import sys\n-\n-    _file = globals()[\"__file__\"]\n-    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "77ed0c2f66d7fdb304fe3e6775c58d635896ab74",
            "filename": "src/transformers/models/deprecated/nezha/configuration_nezha.py",
            "status": "removed",
            "additions": 0,
            "deletions": 105,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fconfiguration_nezha.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fconfiguration_nezha.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fconfiguration_nezha.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,105 +0,0 @@\n-from .... import PreTrainedConfig\n-\n-\n-class NezhaConfig(PreTrainedConfig):\n-    r\"\"\"\n-    This is the configuration class to store the configuration of an [`NezhaModel`]. It is used to instantiate an Nezha\n-    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n-    defaults will yield a similar configuration to that of the Nezha\n-    [sijunhe/nezha-cn-base](https://huggingface.co/sijunhe/nezha-cn-base) architecture.\n-\n-    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PreTrainedConfig`] for more information.\n-\n-\n-    Args:\n-        vocab_size (`int`, optional, defaults to 21128):\n-            Vocabulary size of the NEZHA model. Defines the different tokens that can be represented by the\n-            *inputs_ids* passed to the forward method of [`NezhaModel`].\n-        hidden_size (`int`, optional, defaults to 768):\n-            Dimensionality of the encoder layers and the pooler layer.\n-        num_hidden_layers (`int`, optional, defaults to 12):\n-            Number of hidden layers in the Transformer encoder.\n-        num_attention_heads (`int`, optional, defaults to 12):\n-            Number of attention heads for each attention layer in the Transformer encoder.\n-        intermediate_size (`int`, optional, defaults to 3072):\n-            The dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n-        hidden_act (`str` or `function`, optional, defaults to \"gelu\"):\n-            The non-linear activation function (function or string) in the encoder and pooler.\n-        hidden_dropout_prob (`float`, optional, defaults to 0.1):\n-            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n-        attention_probs_dropout_prob (`float`, optional, defaults to 0.1):\n-            The dropout ratio for the attention probabilities.\n-        max_position_embeddings (`int`, optional, defaults to 512):\n-            The maximum sequence length that this model might ever be used with. Typically set this to something large\n-            (e.g., 512 or 1024 or 2048).\n-        type_vocab_size (`int`, optional, defaults to 2):\n-            The vocabulary size of the *token_type_ids* passed into [`NezhaModel`].\n-        initializer_range (`float`, optional, defaults to 0.02):\n-            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n-        layer_norm_eps (`float`, optional, defaults to 1e-12):\n-            The epsilon used by the layer normalization layers.\n-        classifier_dropout (`float`, optional, defaults to 0.1):\n-            The dropout ratio for attached classifiers.\n-        is_decoder (`bool`, *optional*, defaults to `False`):\n-            Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n-\n-    Example:\n-\n-    ```python\n-    >>> from transformers import NezhaConfig, NezhaModel\n-\n-    >>> # Initializing an Nezha configuration\n-    >>> configuration = NezhaConfig()\n-\n-    >>> # Initializing a model (with random weights) from the Nezha-base style configuration model\n-    >>> model = NezhaModel(configuration)\n-\n-    >>> # Accessing the model configuration\n-    >>> configuration = model.config\n-    ```\"\"\"\n-\n-    model_type = \"nezha\"\n-\n-    def __init__(\n-        self,\n-        vocab_size=21128,\n-        hidden_size=768,\n-        num_hidden_layers=12,\n-        num_attention_heads=12,\n-        intermediate_size=3072,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        max_relative_position=64,\n-        type_vocab_size=2,\n-        initializer_range=0.02,\n-        layer_norm_eps=1e-12,\n-        classifier_dropout=0.1,\n-        pad_token_id=0,\n-        bos_token_id=2,\n-        eos_token_id=3,\n-        use_cache=True,\n-        **kwargs,\n-    ):\n-        super().__init__(pad_token_id=pad_token_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n-\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.hidden_act = hidden_act\n-        self.intermediate_size = intermediate_size\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.max_relative_position = max_relative_position\n-        self.type_vocab_size = type_vocab_size\n-        self.initializer_range = initializer_range\n-        self.layer_norm_eps = layer_norm_eps\n-        self.classifier_dropout = classifier_dropout\n-        self.use_cache = use_cache\n-\n-\n-__all__ = [\"NezhaConfig\"]"
        },
        {
            "sha": "3bb957339e42e46ba0d01ff056abb594794d8985",
            "filename": "src/transformers/models/deprecated/nezha/modeling_nezha.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1530,
            "changes": 1530,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,1530 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch Nezha model.\"\"\"\n-\n-import math\n-import warnings\n-from dataclasses import dataclass\n-from typing import Optional, Union\n-\n-import torch\n-from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n-\n-from ....activations import ACT2FN\n-from ....cache_utils import Cache\n-from ....modeling_layers import GradientCheckpointingLayer\n-from ....modeling_outputs import (\n-    BaseModelOutputWithPastAndCrossAttentions,\n-    BaseModelOutputWithPoolingAndCrossAttentions,\n-    MaskedLMOutput,\n-    MultipleChoiceModelOutput,\n-    NextSentencePredictorOutput,\n-    QuestionAnsweringModelOutput,\n-    SequenceClassifierOutput,\n-    TokenClassifierOutput,\n-)\n-from ....modeling_utils import PreTrainedModel\n-from ....pytorch_utils import apply_chunking_to_forward\n-from ....utils import (\n-    ModelOutput,\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-)\n-from .configuration_nezha import NezhaConfig\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-_CHECKPOINT_FOR_DOC = \"sijunhe/nezha-cn-base\"\n-_CONFIG_FOR_DOC = \"NezhaConfig\"\n-\n-\n-class NezhaRelativePositionsEncoding(nn.Module):\n-    \"\"\"Implement the Functional Relative Position Encoding\"\"\"\n-\n-    def __init__(self, length, depth, max_relative_position=127):\n-        super().__init__()\n-        vocab_size = max_relative_position * 2 + 1\n-        range_vec = torch.arange(length)\n-        range_mat = range_vec.repeat(length).view(length, length)\n-        distance_mat = range_mat - torch.t(range_mat)\n-        distance_mat_clipped = torch.clamp(distance_mat, -max_relative_position, max_relative_position)\n-        final_mat = distance_mat_clipped + max_relative_position\n-\n-        embeddings_table = torch.zeros(vocab_size, depth)\n-        position = torch.arange(0, vocab_size, dtype=torch.int64).float().unsqueeze(1)\n-        div_term = torch.exp(torch.arange(0, depth, 2).float() * (-math.log(10000.0) / depth))\n-        embeddings_table[:, 0::2] = torch.sin(position * div_term)\n-        embeddings_table[:, 1::2] = torch.cos(position * div_term)\n-\n-        flat_relative_positions_matrix = final_mat.view(-1)\n-        one_hot_relative_positions_matrix = torch.nn.functional.one_hot(\n-            flat_relative_positions_matrix, num_classes=vocab_size\n-        ).float()\n-        positions_encoding = torch.matmul(one_hot_relative_positions_matrix, embeddings_table)\n-        my_shape = list(final_mat.size())\n-        my_shape.append(depth)\n-        positions_encoding = positions_encoding.view(my_shape)\n-        self.register_buffer(\"positions_encoding\", positions_encoding, persistent=False)\n-\n-    def forward(self, length):\n-        return self.positions_encoding[:length, :length, :]\n-\n-\n-class NezhaEmbeddings(nn.Module):\n-    \"\"\"Construct the embeddings from word and token_type embeddings.\"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n-        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n-\n-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-        self.register_buffer(\n-            \"token_type_ids\", torch.zeros((1, config.max_position_embeddings), dtype=torch.long), persistent=False\n-        )\n-\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-    ) -> torch.Tensor:\n-        if input_ids is not None:\n-            input_shape = input_ids.size()\n-        else:\n-            input_shape = inputs_embeds.size()[:-1]\n-\n-        seq_length = input_shape[1]\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.word_embeddings(input_ids)\n-\n-        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n-        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n-        # issue #5664\n-        if token_type_ids is None:\n-            if hasattr(self, \"token_type_ids\"):\n-                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n-            else:\n-                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=inputs_embeds.device)\n-\n-        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n-        embeddings = inputs_embeds + token_type_embeddings\n-        embeddings = self.LayerNorm(embeddings)\n-        embeddings = self.dropout(embeddings)\n-        return embeddings\n-\n-\n-class NezhaSelfAttention(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        if config.hidden_size % config.num_attention_heads != 0:\n-            raise ValueError(\n-                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n-                f\"heads ({config.num_attention_heads})\"\n-            )\n-\n-        self.num_attention_heads = config.num_attention_heads\n-        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n-        self.all_head_size = self.num_attention_heads * self.attention_head_size\n-\n-        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n-        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n-        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n-\n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.relative_positions_encoding = NezhaRelativePositionsEncoding(\n-            length=config.max_position_embeddings,\n-            depth=self.attention_head_size,\n-            max_relative_position=config.max_relative_position,\n-        )\n-        self.is_decoder = config.is_decoder\n-\n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n-        is_cross_attention = encoder_hidden_states is not None\n-\n-        if is_cross_attention and past_key_values is not None:\n-            # reuse k,v, cross_attentions\n-            key_layer = past_key_values[0]\n-            value_layer = past_key_values[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_values is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_values[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_values[1], value_layer], dim=2)\n-        else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_values` is always `None`\n-            past_key_values = (key_layer, value_layer)\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        batch_size, num_attention_heads, from_seq_length, to_seq_length = attention_scores.size()\n-        relations_keys = self.relative_positions_encoding(to_seq_length)\n-        query_layer_t = query_layer.permute(2, 0, 1, 3)\n-\n-        query_layer_r = query_layer_t.contiguous().view(\n-            from_seq_length, batch_size * num_attention_heads, self.attention_head_size\n-        )\n-        key_position_scores = torch.matmul(query_layer_r, relations_keys.permute(0, 2, 1))\n-        key_position_scores_r = key_position_scores.view(\n-            from_seq_length, batch_size, num_attention_heads, from_seq_length\n-        )\n-        key_position_scores_r_t = key_position_scores_r.permute(1, 2, 0, 3)\n-        attention_scores = attention_scores + key_position_scores_r_t\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in NezhaModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        context_layer = torch.matmul(attention_probs, value_layer)\n-        relations_values = self.relative_positions_encoding(to_seq_length)\n-        attention_probs_t = attention_probs.permute(2, 0, 1, 3)\n-        attentions_probs_r = attention_probs_t.contiguous().view(\n-            from_seq_length, batch_size * num_attention_heads, to_seq_length\n-        )\n-        value_position_scores = torch.matmul(attentions_probs_r, relations_values)\n-        value_position_scores_r = value_position_scores.view(\n-            from_seq_length, batch_size, num_attention_heads, self.attention_head_size\n-        )\n-        value_position_scores_r_t = value_position_scores_r.permute(1, 2, 0, 3)\n-        context_layer = context_layer + value_position_scores_r_t\n-\n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_values,)\n-        return outputs\n-\n-\n-class NezhaSelfOutput(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-\n-    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n-        hidden_states = self.dense(hidden_states)\n-        hidden_states = self.dropout(hidden_states)\n-        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n-        return hidden_states\n-\n-\n-class NezhaAttention(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.self = NezhaSelfAttention(config)\n-        self.output = NezhaSelfOutput(config)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self(\n-            hidden_states,\n-            attention_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_values,\n-            output_attentions,\n-        )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n-\n-\n-class NezhaIntermediate(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n-        if isinstance(config.hidden_act, str):\n-            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n-        else:\n-            self.intermediate_act_fn = config.hidden_act\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        hidden_states = self.dense(hidden_states)\n-        hidden_states = self.intermediate_act_fn(hidden_states)\n-        return hidden_states\n-\n-\n-class NezhaOutput(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-\n-    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n-        hidden_states = self.dense(hidden_states)\n-        hidden_states = self.dropout(hidden_states)\n-        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n-        return hidden_states\n-\n-\n-class NezhaLayer(GradientCheckpointingLayer):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n-        self.seq_len_dim = 1\n-        self.attention = NezhaAttention(config)\n-        self.is_decoder = config.is_decoder\n-        self.add_cross_attention = config.add_cross_attention\n-        if self.add_cross_attention:\n-            if not self.is_decoder:\n-                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = NezhaAttention(config)\n-        self.intermediate = NezhaIntermediate(config)\n-        self.output = NezhaOutput(config)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_values = past_key_values[:2] if past_key_values is not None else None\n-        self_attention_outputs = self.attention(\n-            hidden_states,\n-            attention_mask,\n-            output_attentions=output_attentions,\n-            past_key_values=self_attn_past_key_values,\n-        )\n-        attention_output = self_attention_outputs[0]\n-\n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-            present_key_value = self_attention_outputs[-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        cross_attn_present_key_value = None\n-        if self.is_decoder and encoder_hidden_states is not None:\n-            if not hasattr(self, \"crossattention\"):\n-                raise ValueError(\n-                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n-                    \" by setting `config.add_cross_attention=True`\"\n-                )\n-\n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_values tuple\n-            cross_attn_past_key_values = past_key_values[-2:] if past_key_values is not None else None\n-            cross_attention_outputs = self.crossattention(\n-                attention_output,\n-                attention_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                cross_attn_past_key_values,\n-                output_attentions,\n-            )\n-            attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n-\n-            # add cross-attn cache to positions 3,4 of present_key_value tuple\n-            cross_attn_present_key_value = cross_attention_outputs[-1]\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n-        layer_output = apply_chunking_to_forward(\n-            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n-        )\n-        outputs = (layer_output,) + outputs\n-\n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (present_key_value,)\n-\n-        return outputs\n-\n-    def feed_forward_chunk(self, attention_output):\n-        intermediate_output = self.intermediate(attention_output)\n-        layer_output = self.output(intermediate_output, attention_output)\n-        return layer_output\n-\n-\n-class NezhaEncoder(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.config = config\n-        self.layer = nn.ModuleList([NezhaLayer(config) for _ in range(config.num_hidden_layers)])\n-        self.gradient_checkpointing = False\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n-\n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        next_decoder_cache = () if use_cache else None\n-        for i, layer_module in enumerate(self.layer):\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-            layer_outputs = layer_module(\n-                hidden_states,\n-                attention_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                past_key_values[i] if past_key_values is not None else None,\n-                output_attentions,\n-            )\n-\n-            hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n-\n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    next_decoder_cache,\n-                    all_hidden_states,\n-                    all_self_attentions,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n-        return BaseModelOutputWithPastAndCrossAttentions(\n-            last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-            cross_attentions=all_cross_attentions,\n-        )\n-\n-\n-class NezhaPooler(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n-        self.activation = nn.Tanh()\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        # We \"pool\" the model by simply taking the hidden state corresponding\n-        # to the first token.\n-        first_token_tensor = hidden_states[:, 0]\n-        pooled_output = self.dense(first_token_tensor)\n-        pooled_output = self.activation(pooled_output)\n-        return pooled_output\n-\n-\n-class NezhaPredictionHeadTransform(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n-        if isinstance(config.hidden_act, str):\n-            self.transform_act_fn = ACT2FN[config.hidden_act]\n-        else:\n-            self.transform_act_fn = config.hidden_act\n-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        hidden_states = self.dense(hidden_states)\n-        hidden_states = self.transform_act_fn(hidden_states)\n-        hidden_states = self.LayerNorm(hidden_states)\n-        return hidden_states\n-\n-\n-class NezhaLMPredictionHead(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.transform = NezhaPredictionHeadTransform(config)\n-\n-        # The output weights are the same as the input embeddings, but there is\n-        # an output-only bias for each token.\n-        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n-\n-        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n-\n-    def forward(self, hidden_states):\n-        hidden_states = self.transform(hidden_states)\n-        hidden_states = self.decoder(hidden_states)\n-        return hidden_states\n-\n-\n-class NezhaOnlyMLMHead(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.predictions = NezhaLMPredictionHead(config)\n-\n-    def forward(self, sequence_output: torch.Tensor) -> torch.Tensor:\n-        prediction_scores = self.predictions(sequence_output)\n-        return prediction_scores\n-\n-\n-class NezhaOnlyNSPHead(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n-\n-    def forward(self, pooled_output):\n-        seq_relationship_score = self.seq_relationship(pooled_output)\n-        return seq_relationship_score\n-\n-\n-class NezhaPreTrainingHeads(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.predictions = NezhaLMPredictionHead(config)\n-        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n-\n-    def forward(self, sequence_output, pooled_output):\n-        prediction_scores = self.predictions(sequence_output)\n-        seq_relationship_score = self.seq_relationship(pooled_output)\n-        return prediction_scores, seq_relationship_score\n-\n-\n-class NezhaPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config: NezhaConfig\n-    base_model_prefix = \"nezha\"\n-    supports_gradient_checkpointing = True\n-\n-\n-@dataclass\n-class NezhaForPreTrainingOutput(ModelOutput):\n-    \"\"\"\n-    Output type of [`NezhaForPreTraining`].\n-\n-    Args:\n-        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n-            (classification) loss.\n-        prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n-            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n-            before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    loss: Optional[torch.FloatTensor] = None\n-    prediction_logits: Optional[torch.FloatTensor] = None\n-    seq_relationship_logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n-\n-\n-NEZHA_START_DOCSTRING = r\"\"\"\n-\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`NezhaConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-NEZHA_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Nezha Model transformer outputting raw hidden-states without any specific head on top.\",\n-    NEZHA_START_DOCSTRING,\n-)\n-class NezhaModel(NezhaPreTrainedModel):\n-    \"\"\"\n-\n-    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n-    cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n-    all you need](https://huggingface.co/papers/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n-    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n-\n-    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n-    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n-    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n-    \"\"\"\n-\n-    def __init__(self, config, add_pooling_layer=True):\n-        super().__init__(config)\n-        self.config = config\n-\n-        self.embeddings = NezhaEmbeddings(config)\n-        self.encoder = NezhaEncoder(config)\n-\n-        self.pooler = NezhaPooler(config) if add_pooling_layer else None\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def get_input_embeddings(self):\n-        return self.embeddings.word_embeddings\n-\n-    def set_input_embeddings(self, value):\n-        self.embeddings.word_embeddings = value\n-\n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n-    @add_start_docstrings_to_model_forward(NEZHA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        if self.config.is_decoder:\n-            use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        else:\n-            use_cache = False\n-\n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        batch_size, seq_length = input_shape\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n-\n-        # past_key_values_length\n-        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n-\n-        if attention_mask is None:\n-            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n-\n-        if token_type_ids is None:\n-            if hasattr(self.embeddings, \"token_type_ids\"):\n-                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n-            else:\n-                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n-\n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n-\n-        # If a 2D or 3D attention mask is provided for the cross-attention\n-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-        if self.config.is_decoder and encoder_hidden_states is not None:\n-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n-            if encoder_attention_mask is None:\n-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n-            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n-        else:\n-            encoder_extended_attention_mask = None\n-\n-        embedding_output = self.embeddings(\n-            input_ids=input_ids,\n-            token_type_ids=token_type_ids,\n-            inputs_embeds=inputs_embeds,\n-        )\n-        encoder_outputs = self.encoder(\n-            embedding_output,\n-            attention_mask=extended_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_extended_attention_mask,\n-            past_key_values=past_key_values,\n-            use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-        sequence_output = encoder_outputs[0]\n-        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n-\n-        if not return_dict:\n-            return (sequence_output, pooled_output) + encoder_outputs[1:]\n-\n-        return BaseModelOutputWithPoolingAndCrossAttentions(\n-            last_hidden_state=sequence_output,\n-            pooler_output=pooled_output,\n-            past_key_values=encoder_outputs.past_key_values,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-            cross_attentions=encoder_outputs.cross_attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Nezha Model with two heads on top as done during the pretraining: a `masked language modeling` head and a `next\n-    sentence prediction (classification)` head.\n-    \"\"\",\n-    NEZHA_START_DOCSTRING,\n-)\n-class NezhaForPreTraining(NezhaPreTrainedModel):\n-    _tied_weights_keys = {\n-        \"cls.predictions.decoder.weight\": \"nezha.embeddings.word_embeddings.weight\",\n-        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n-    }\n-\n-    def __init__(self, config):\n-        super().__init__(config)\n-\n-        self.nezha = NezhaModel(config)\n-        self.cls = NezhaPreTrainingHeads(config)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def get_output_embeddings(self):\n-        return self.cls.predictions.decoder\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.cls.predictions.decoder = new_embeddings\n-        self.cls.predictions.bias = new_embeddings.bias\n-\n-    @add_start_docstrings_to_model_forward(NEZHA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=NezhaForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        next_sentence_label: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple[torch.Tensor], NezhaForPreTrainingOutput]:\n-        r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n-                config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked),\n-                the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n-            next_sentence_label (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-                Labels for computing the next sequence prediction (classification) loss. Input should be a sequence\n-                pair (see `input_ids` docstring) Indices should be in `[0, 1]`:\n-\n-                - 0 indicates sequence B is a continuation of sequence A,\n-                - 1 indicates sequence B is a random sequence.\n-            kwargs (`dict[str, any]`, optional, defaults to *{}*):\n-                Used to hide legacy arguments that have been deprecated.\n-\n-        Returns:\n-\n-        Example:\n-\n-        ```python\n-        >>> from transformers import AutoTokenizer, NezhaForPreTraining\n-        >>> import torch\n-\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"sijunhe/nezha-cn-base\")\n-        >>> model = NezhaForPreTraining.from_pretrained(\"sijunhe/nezha-cn-base\")\n-\n-        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n-        >>> outputs = model(**inputs)\n-\n-        >>> prediction_logits = outputs.prediction_logits\n-        >>> seq_relationship_logits = outputs.seq_relationship_logits\n-        ```\n-        \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        outputs = self.nezha(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        sequence_output, pooled_output = outputs[:2]\n-        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n-\n-        total_loss = None\n-        if labels is not None and next_sentence_label is not None:\n-            loss_fct = CrossEntropyLoss()\n-            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n-            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n-            total_loss = masked_lm_loss + next_sentence_loss\n-\n-        if not return_dict:\n-            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n-\n-        return NezhaForPreTrainingOutput(\n-            loss=total_loss,\n-            prediction_logits=prediction_scores,\n-            seq_relationship_logits=seq_relationship_score,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\"\"\"Nezha Model with a `language modeling` head on top.\"\"\", NEZHA_START_DOCSTRING)\n-class NezhaForMaskedLM(NezhaPreTrainedModel):\n-    _tied_weights_keys = {\n-        \"cls.predictions.decoder.weight\": \"nezha.embeddings.word_embeddings.weight\",\n-        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n-    }\n-\n-    def __init__(self, config):\n-        super().__init__(config)\n-\n-        if config.is_decoder:\n-            logger.warning(\n-                \"If you want to use `NezhaForMaskedLM` make sure `config.is_decoder=False` for \"\n-                \"bi-directional self-attention.\"\n-            )\n-\n-        self.nezha = NezhaModel(config, add_pooling_layer=False)\n-        self.cls = NezhaOnlyMLMHead(config)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def get_output_embeddings(self):\n-        return self.cls.predictions.decoder\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.cls.predictions.decoder = new_embeddings\n-        self.cls.predictions.bias = new_embeddings.bias\n-\n-    @add_start_docstrings_to_model_forward(NEZHA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=MaskedLMOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n-            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n-            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n-        \"\"\"\n-\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        outputs = self.nezha(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            inputs_embeds=inputs_embeds,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        sequence_output = outputs[0]\n-        prediction_scores = self.cls(sequence_output)\n-\n-        masked_lm_loss = None\n-        if labels is not None:\n-            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n-            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n-\n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n-\n-        return MaskedLMOutput(\n-            loss=masked_lm_loss,\n-            logits=prediction_scores,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_kwargs):\n-        input_shape = input_ids.shape\n-        effective_batch_size = input_shape[0]\n-\n-        #  add a dummy token\n-        if self.config.pad_token_id is None:\n-            raise ValueError(\"The PAD token should be defined for generation\")\n-\n-        attention_mask = torch.cat([attention_mask, attention_mask.new_zeros((attention_mask.shape[0], 1))], dim=-1)\n-        dummy_token = torch.full(\n-            (effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device\n-        )\n-        input_ids = torch.cat([input_ids, dummy_token], dim=1)\n-\n-        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-\n-\n-@add_start_docstrings(\n-    \"\"\"Nezha Model with a `next sentence prediction (classification)` head on top.\"\"\",\n-    NEZHA_START_DOCSTRING,\n-)\n-class NezhaForNextSentencePrediction(NezhaPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-\n-        self.nezha = NezhaModel(config)\n-        self.cls = NezhaOnlyNSPHead(config)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(NEZHA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=NextSentencePredictorOutput, config_class=_CONFIG_FOR_DOC)\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        **kwargs,\n-    ) -> Union[tuple[torch.Tensor], NextSentencePredictorOutput]:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n-            (see `input_ids` docstring). Indices should be in `[0, 1]`:\n-\n-            - 0 indicates sequence B is a continuation of sequence A,\n-            - 1 indicates sequence B is a random sequence.\n-\n-        Returns:\n-\n-        Example:\n-\n-        ```python\n-        >>> from transformers import AutoTokenizer, NezhaForNextSentencePrediction\n-        >>> import torch\n-\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"sijunhe/nezha-cn-base\")\n-        >>> model = NezhaForNextSentencePrediction.from_pretrained(\"sijunhe/nezha-cn-base\")\n-\n-        >>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n-        >>> next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n-        >>> encoding = tokenizer(prompt, next_sentence, return_tensors=\"pt\")\n-\n-        >>> outputs = model(**encoding, labels=torch.LongTensor([1]))\n-        >>> logits = outputs.logits\n-        >>> assert logits[0, 0] < logits[0, 1]  # next sentence was random\n-        ```\n-        \"\"\"\n-\n-        if \"next_sentence_label\" in kwargs:\n-            warnings.warn(\n-                \"The `next_sentence_label` argument is deprecated and will be removed in a future version, use\"\n-                \" `labels` instead.\",\n-                FutureWarning,\n-            )\n-            labels = kwargs.pop(\"next_sentence_label\")\n-\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        outputs = self.nezha(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        pooled_output = outputs[1]\n-\n-        seq_relationship_scores = self.cls(pooled_output)\n-\n-        next_sentence_loss = None\n-        if labels is not None:\n-            loss_fct = CrossEntropyLoss()\n-            next_sentence_loss = loss_fct(seq_relationship_scores.view(-1, 2), labels.view(-1))\n-\n-        if not return_dict:\n-            output = (seq_relationship_scores,) + outputs[2:]\n-            return ((next_sentence_loss,) + output) if next_sentence_loss is not None else output\n-\n-        return NextSentencePredictorOutput(\n-            loss=next_sentence_loss,\n-            logits=seq_relationship_scores,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Nezha Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n-    output) e.g. for GLUE tasks.\n-    \"\"\",\n-    NEZHA_START_DOCSTRING,\n-)\n-class NezhaForSequenceClassification(NezhaPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.config = config\n-\n-        self.nezha = NezhaModel(config)\n-        classifier_dropout = (\n-            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n-        )\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(NEZHA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=SequenceClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        outputs = self.nezha(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        pooled_output = outputs[1]\n-\n-        pooled_output = self.dropout(pooled_output)\n-        logits = self.classifier(pooled_output)\n-\n-        loss = None\n-        if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n-        return SequenceClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Nezha Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n-    softmax) e.g. for RocStories/SWAG tasks.\n-    \"\"\",\n-    NEZHA_START_DOCSTRING,\n-)\n-class NezhaForMultipleChoice(NezhaPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-\n-        self.nezha = NezhaModel(config)\n-        classifier_dropout = (\n-            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n-        )\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.classifier = nn.Linear(config.hidden_size, 1)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(NEZHA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=MultipleChoiceModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n-            num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n-            `input_ids` above)\n-        \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n-        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n-        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n-        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n-        inputs_embeds = (\n-            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n-            if inputs_embeds is not None\n-            else None\n-        )\n-\n-        outputs = self.nezha(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        pooled_output = outputs[1]\n-        print(pooled_output.shape)\n-        pooled_output = self.dropout(pooled_output)\n-        logits = self.classifier(pooled_output)\n-        print(logits.shape)\n-        print(num_choices)\n-        reshaped_logits = logits.view(-1, num_choices)\n-\n-        loss = None\n-        if labels is not None:\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(reshaped_logits, labels)\n-\n-        if not return_dict:\n-            output = (reshaped_logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n-        return MultipleChoiceModelOutput(\n-            loss=loss,\n-            logits=reshaped_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Nezha Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n-    Named-Entity-Recognition (NER) tasks.\n-    \"\"\",\n-    NEZHA_START_DOCSTRING,\n-)\n-class NezhaForTokenClassification(NezhaPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-\n-        self.nezha = NezhaModel(config, add_pooling_layer=False)\n-        classifier_dropout = (\n-            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n-        )\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(NEZHA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=TokenClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n-        \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        outputs = self.nezha(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        sequence_output = outputs[0]\n-\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.classifier(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Nezha Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n-    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n-    \"\"\",\n-    NEZHA_START_DOCSTRING,\n-)\n-class NezhaForQuestionAnswering(NezhaPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-\n-        self.nezha = NezhaModel(config, add_pooling_layer=False)\n-        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(NEZHA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=QuestionAnsweringModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        start_positions: Optional[torch.Tensor] = None,\n-        end_positions: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n-        r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        outputs = self.nezha(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        sequence_output = outputs[0]\n-\n-        logits = self.qa_outputs(sequence_output)\n-        start_logits, end_logits = logits.split(1, dim=-1)\n-        start_logits = start_logits.squeeze(-1).contiguous()\n-        end_logits = end_logits.squeeze(-1).contiguous()\n-\n-        total_loss = None\n-        if start_positions is not None and end_positions is not None:\n-            # If we are on multi-GPU, split add a dimension\n-            if len(start_positions.size()) > 1:\n-                start_positions = start_positions.squeeze(-1)\n-            if len(end_positions.size()) > 1:\n-                end_positions = end_positions.squeeze(-1)\n-            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n-            ignored_index = start_logits.size(1)\n-            start_positions = start_positions.clamp(0, ignored_index)\n-            end_positions = end_positions.clamp(0, ignored_index)\n-\n-            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n-            start_loss = loss_fct(start_logits, start_positions)\n-            end_loss = loss_fct(end_logits, end_positions)\n-            total_loss = (start_loss + end_loss) / 2\n-\n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n-\n-        return QuestionAnsweringModelOutput(\n-            loss=total_loss,\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-__all__ = [\n-    \"NezhaForNextSentencePrediction\",\n-    \"NezhaForMaskedLM\",\n-    \"NezhaForPreTraining\",\n-    \"NezhaForMultipleChoice\",\n-    \"NezhaForQuestionAnswering\",\n-    \"NezhaForSequenceClassification\",\n-    \"NezhaForTokenClassification\",\n-    \"NezhaModel\",\n-    \"NezhaPreTrainedModel\",\n-]"
        },
        {
            "sha": "2b3964d194bed041987c6236b5c60bfcf3b7caf4",
            "filename": "src/transformers/models/deprecated/open_llama/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,27 +0,0 @@\n-# Copyright 2023 EleutherAI and The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from typing import TYPE_CHECKING\n-\n-from ....utils import _LazyModule\n-from ....utils.import_utils import define_import_structure\n-\n-\n-if TYPE_CHECKING:\n-    from .configuration_open_llama import *\n-    from .modeling_open_llama import *\n-else:\n-    import sys\n-\n-    _file = globals()[\"__file__\"]\n-    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "64545d7abcf6c6ba2ff920c9d13df9ab550200e8",
            "filename": "src/transformers/models/deprecated/open_llama/configuration_open_llama.py",
            "status": "removed",
            "additions": 0,
            "deletions": 171,
            "changes": 171,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fconfiguration_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fconfiguration_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fconfiguration_open_llama.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,171 +0,0 @@\n-# coding=utf-8\n-# Copyright 2023 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n-#\n-# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n-# and OPT implementations in this library. It has been modified from its\n-# original forms to accommodate minor architectural differences compared\n-# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Open-Llama model configuration\"\"\"\n-\n-from ....configuration_utils import PreTrainedConfig\n-from ....utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class OpenLlamaConfig(PreTrainedConfig):\n-    r\"\"\"\n-    This is the configuration class to store the configuration of a [`OpenLlamaModel`]. It is used to instantiate an\n-    Open-Llama model according to the specified arguments, defining the model architecture. Instantiating a\n-    configuration with the defaults will yield a similar configuration to that of the\n-    [s-JoL/Open-Llama-V1](https://huggingface.co/s-JoL/Open-Llama-V1).\n-\n-    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PreTrainedConfig`] for more information.\n-\n-\n-    Args:\n-        vocab_size (`int`, *optional*, defaults to 32000):\n-            Vocabulary size of the Open-Llama model. Defines the number of different tokens that can be represented by\n-            the `inputs_ids` passed when calling [`OpenLlamaModel`]\n-        hidden_size (`int`, *optional*, defaults to 4096):\n-            Dimension of the hidden representations.\n-        intermediate_size (`int`, *optional*, defaults to 11008):\n-            Dimension of the MLP representations.\n-        num_hidden_layers (`int`, *optional*, defaults to 32):\n-            Number of hidden layers in the Transformer encoder.\n-        num_attention_heads (`int`, *optional*, defaults to 32):\n-            Number of attention heads for each attention layer in the Transformer encoder.\n-        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n-            The non-linear activation function (function or string) in the decoder.\n-        max_position_embeddings (`int`, *optional*, defaults to 2048):\n-            The maximum sequence length that this model might ever be used with. Typically set this to something large\n-            just in case (e.g., 512 or 1024 or 2048).\n-        initializer_range (`float`, *optional*, defaults to 0.02):\n-            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n-        rms_norm_eps (`float`, *optional*, defaults to 1e-12):\n-            The epsilon used by the rms normalization layers.\n-        use_cache (`bool`, *optional*, defaults to `True`):\n-            Whether or not the model should return the last key/values attentions (not used by all models). Only\n-            relevant if `config.is_decoder=True`.\n-        tie_word_embeddings(`bool`, *optional*, defaults to `False`):\n-            Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n-            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n-            with longer `max_position_embeddings`.\n-\n-        Example:\n-\n-    ```python\n-    >>> from transformers import OpenLlamaModel, OpenLlamaConfig\n-\n-    >>> # Initializing a Open-Llama open_llama-7b style configuration\n-    >>> configuration = OpenLlamaConfig()\n-\n-    >>> # Initializing a model from the open_llama-7b style configuration\n-    >>> model = OpenLlamaModel(configuration)\n-\n-    >>> # Accessing the model configuration\n-    >>> configuration = model.config\n-    ```\"\"\"\n-\n-    model_type = \"open-llama\"\n-\n-    def __init__(\n-        self,\n-        vocab_size=100000,\n-        hidden_size=4096,\n-        intermediate_size=11008,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=0,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        use_memory_efficient_attention=True,\n-        hidden_dropout_prob=0.1,\n-        attention_dropout_prob=0.1,\n-        use_stable_embedding=True,\n-        shared_input_output_embedding=True,\n-        rope_theta=10000.0,\n-        rope_parameters=None,\n-        **kwargs,\n-    ):\n-        self.vocab_size = vocab_size\n-        self.max_position_embeddings = max_position_embeddings\n-        self.hidden_size = hidden_size\n-        self.intermediate_size = intermediate_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.hidden_act = hidden_act\n-        self.initializer_range = initializer_range\n-        self.rms_norm_eps = rms_norm_eps\n-        self.use_cache = use_cache\n-        self.use_memory_efficient_attention = kwargs.pop(\n-            \"use_memorry_efficient_attention\", use_memory_efficient_attention\n-        )\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_dropout_prob = attention_dropout_prob\n-        self.use_stable_embedding = use_stable_embedding\n-        self.shared_input_output_embedding = shared_input_output_embedding\n-        self.rope_theta = rope_theta\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-        self._rope_parameters_validation()\n-\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n-\n-    def _rope_parameters_validation(self):\n-        \"\"\"\n-        Validate the `rope_parameters` configuration.\n-        \"\"\"\n-        if self.rope_parameters is None:\n-            return\n-\n-        if not isinstance(self.rope_parameters, dict) or len(self.rope_parameters) != 2:\n-            raise ValueError(\n-                f\"`rope_parameters` must be a dictionary with two fields, `type` and `factor`, got {self.rope_parameters}\"\n-            )\n-        rope_parameters_type = self.rope_parameters.get(\"type\", None)\n-        rope_parameters_factor = self.rope_parameters.get(\"factor\", None)\n-        if rope_parameters_type is None or rope_parameters_type not in [\"linear\", \"dynamic\"]:\n-            raise ValueError(\n-                f\"`rope_parameters`'s type field must be one of ['linear', 'dynamic'], got {rope_parameters_type}\"\n-            )\n-        if (\n-            rope_parameters_factor is None\n-            or not isinstance(rope_parameters_factor, float)\n-            or rope_parameters_factor <= 1.0\n-        ):\n-            raise ValueError(f\"`rope_parameters`'s factor field must be a float > 1, got {rope_parameters_factor}\")\n-\n-\n-__all__ = [\"OpenLlamaConfig\"]"
        },
        {
            "sha": "261fa5c9770821fdf5c4fb7e659836d04a22617d",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "removed",
            "additions": 0,
            "deletions": 929,
            "changes": 929,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,929 +0,0 @@\n-# coding=utf-8\n-# Copyright 2023 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n-#\n-# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n-# and OPT implementations in this library. It has been modified from its\n-# original forms to accommodate minor architectural differences compared\n-# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch Open-Llama model.\"\"\"\n-\n-import math\n-from typing import Optional, Union\n-\n-import torch\n-from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n-\n-from .... import initialization as init\n-from ....activations import ACT2FN\n-from ....cache_utils import Cache\n-from ....modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n-from ....modeling_layers import GradientCheckpointingLayer\n-from ....modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n-from ....modeling_utils import PreTrainedModel\n-from ....utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n-from .configuration_open_llama import OpenLlamaConfig\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-try:\n-    from xformers import ops as xops\n-except ImportError:\n-    xops = None\n-\n-\n-_CONFIG_FOR_DOC = \"OpenLlamaConfig\"\n-\n-\n-class OpenLlamaRMSNorm(nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n-        \"\"\"\n-        OpenLlamaRMSNorm is equivalent to T5LayerNorm\n-        \"\"\"\n-        super().__init__()\n-        self.weight = nn.Parameter(torch.ones(hidden_size))\n-        self.variance_epsilon = eps\n-\n-    def forward(self, hidden_states):\n-        input_dtype = hidden_states.dtype\n-        hidden_states = hidden_states.to(torch.float32)\n-        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n-        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n-        return self.weight * hidden_states.to(input_dtype)\n-\n-    def extra_repr(self):\n-        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n-\n-\n-class OpenLlamaRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-    cos_cached: torch.Tensor\n-    sin_cached: torch.Tensor\n-\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n-        super().__init__()\n-\n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (\n-            self.base\n-            ** (torch.arange(0, self.dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / self.dim)\n-        )\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-        # Build here to make `torch.jit.trace` work.\n-        self._set_cos_sin_cache(\n-            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n-        )\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n-\n-    def forward(self, x, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        if seq_len > self.max_seq_len_cached:\n-            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n-\n-        return (\n-            self.cos_cached[:seq_len].to(dtype=x.dtype),\n-            self.sin_cached[:seq_len].to(dtype=x.dtype),\n-        )\n-\n-\n-class OpenLlamaLinearScalingRotaryEmbedding(OpenLlamaRotaryEmbedding):\n-    \"\"\"OpenLlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n-\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n-        self.scaling_factor = scaling_factor\n-        super().__init__(dim, max_position_embeddings, base, device)\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-        t = t / self.scaling_factor\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n-\n-\n-class OpenLlamaDynamicNTKScalingRotaryEmbedding(OpenLlamaRotaryEmbedding):\n-    \"\"\"OpenLlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n-\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n-        self.scaling_factor = scaling_factor\n-        super().__init__(dim, max_position_embeddings, base, device)\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-\n-        if seq_len > self.max_position_embeddings:\n-            base = self.base * (\n-                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n-            ) ** (self.dim / (self.dim - 2))\n-            inv_freq = 1.0 / (\n-                base\n-                ** (torch.arange(0, self.dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / self.dim)\n-            )\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n-\n-\n-def rotate_half(x):\n-    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n-    x1 = x[..., : x.shape[-1] // 2]\n-    x2 = x[..., x.shape[-1] // 2 :]\n-    return torch.cat((-x2, x1), dim=-1)\n-\n-\n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n-    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n-\n-    Args:\n-        q (`torch.Tensor`): The query tensor.\n-        k (`torch.Tensor`): The key tensor.\n-        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n-        sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`):\n-            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n-            used to pass offsetted position ids when working with a KV-cache.\n-        unsqueeze_dim (`int`, *optional*, defaults to 1):\n-            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n-            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n-            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n-            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n-            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n-            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n-    Returns:\n-        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n-    \"\"\"\n-    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n-    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n-    q_embed = (q * cos) + (rotate_half(q) * sin)\n-    k_embed = (k * cos) + (rotate_half(k) * sin)\n-    return q_embed, k_embed\n-\n-\n-class OpenLlamaMLP(nn.Module):\n-    def __init__(\n-        self,\n-        hidden_size: int,\n-        intermediate_size: int,\n-        hidden_act: str,\n-        dropout_prob: float,\n-    ):\n-        super().__init__()\n-        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n-        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n-        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n-        self.act_fn = ACT2FN[hidden_act]\n-        self.dropout = nn.Dropout(dropout_prob)\n-\n-    def forward(self, x):\n-        out = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n-        return self.dropout(out)\n-\n-\n-class OpenLlamaAttention(nn.Module):\n-    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n-\n-    def __init__(self, config: OpenLlamaConfig):\n-        super().__init__()\n-        self.config = config\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.dropout_prob = config.attention_dropout_prob\n-\n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n-        self._init_rope()\n-\n-    def _init_rope(self):\n-        if self.config.rope_parameters is None:\n-            self.rotary_emb = OpenLlamaRotaryEmbedding(\n-                self.head_dim,\n-                max_position_embeddings=self.max_position_embeddings,\n-                base=self.rope_theta,\n-            )\n-        else:\n-            scaling_type = self.config.rope_parameters[\"type\"]\n-            scaling_factor = self.config.rope_parameters[\"factor\"]\n-            if scaling_type == \"linear\":\n-                self.rotary_emb = OpenLlamaLinearScalingRotaryEmbedding(\n-                    self.head_dim,\n-                    max_position_embeddings=self.max_position_embeddings,\n-                    scaling_factor=scaling_factor,\n-                    base=self.rope_theta,\n-                )\n-            elif scaling_type == \"dynamic\":\n-                self.rotary_emb = OpenLlamaDynamicNTKScalingRotaryEmbedding(\n-                    self.head_dim,\n-                    max_position_embeddings=self.max_position_embeddings,\n-                    scaling_factor=scaling_factor,\n-                    base=self.rope_theta,\n-                )\n-            else:\n-                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n-\n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_values is not None:\n-            kv_seq_len += past_key_values[0].shape[-2]\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n-        # [bsz, nh, t, hd]\n-\n-        if past_key_values is not None:\n-            # reuse k, v, self_attention\n-            key_states = torch.cat([past_key_values[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_values[1], value_states], dim=2)\n-\n-        past_key_values = (key_states, value_states) if use_cache else None\n-\n-        if self.config.use_memory_efficient_attention and xops is not None and self.training:\n-            attn_weights = None\n-            query_states = query_states.transpose(1, 2)\n-            key_states = key_states.transpose(1, 2)\n-            value_states = value_states.transpose(1, 2)\n-            attn_output = xops.memory_efficient_attention(\n-                query_states, key_states, value_states, attn_bias=xops.LowerTriangularMask(), p=self.dropout_prob\n-            )\n-        else:\n-            attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-            if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n-                raise ValueError(\n-                    f\"Attention weights should be of size {(bsz * self.num_heads, q_len, kv_seq_len)}, but is\"\n-                    f\" {attn_weights.size()}\"\n-                )\n-\n-            if attention_mask is not None:\n-                if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n-                    raise ValueError(\n-                        f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n-                    )\n-                attn_weights = attn_weights + attention_mask\n-                attn_weights = torch.max(\n-                    attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min, device=attn_weights.device)\n-                )\n-\n-            # upcast attention to fp32\n-            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-            attn_output = torch.matmul(attn_weights, value_states)\n-\n-            if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-                raise ValueError(\n-                    f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                    f\" {attn_output.size()}\"\n-                )\n-\n-            attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_values\n-\n-\n-class OpenLlamaDecoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: OpenLlamaConfig):\n-        super().__init__()\n-        self.hidden_size = config.hidden_size\n-        self.self_attn = OpenLlamaAttention(config=config)\n-        self.mlp = OpenLlamaMLP(\n-            hidden_size=self.hidden_size,\n-            intermediate_size=config.intermediate_size,\n-            hidden_act=config.hidden_act,\n-            dropout_prob=config.hidden_dropout_prob,\n-        )\n-        self.input_layernorm = OpenLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.post_attention_layernorm = OpenLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n-                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-        \"\"\"\n-\n-        residual = hidden_states\n-\n-        hidden_states = self.input_layernorm(hidden_states)\n-\n-        # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n-            hidden_states=hidden_states,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n-        )\n-        hidden_states = residual + hidden_states\n-\n-        # Fully Connected\n-        residual = hidden_states\n-        hidden_states = self.post_attention_layernorm(hidden_states)\n-        hidden_states = self.mlp(hidden_states)\n-        hidden_states = residual + hidden_states\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n-        return outputs\n-\n-\n-OPEN_LLAMA_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`OpenLlamaConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Open-Llama Model outputting raw hidden-states without any specific head on top.\",\n-    OPEN_LLAMA_START_DOCSTRING,\n-)\n-class OpenLlamaPreTrainedModel(PreTrainedModel):\n-    config: OpenLlamaConfig\n-    base_model_prefix = \"model\"\n-    supports_gradient_checkpointing = True\n-    _no_split_modules = [\"OpenLlamaDecoderLayer\"]\n-\n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            init.normal_(module.weight, mean=0.0, std=std)\n-            if module.bias is not None:\n-                init.zeros_(module.bias)\n-        elif isinstance(module, nn.Embedding):\n-            if self.config.use_stable_embedding:\n-                init.xavier_normal_(module.weight)\n-            else:\n-                init.normal_(module.weight, mean=0.0, std=std)\n-            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n-            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n-                init.zeros_(module.weight[module.padding_idx])\n-\n-\n-OPEN_LLAMA_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://huggingface.co/papers/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Open-Llama Model outputting raw hidden-states without any specific head on top.\",\n-    OPEN_LLAMA_START_DOCSTRING,\n-)\n-class OpenLlamaModel(OpenLlamaPreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`OpenLlamaDecoderLayer`]\n-\n-    Args:\n-        config: OpenLlamaConfig\n-    \"\"\"\n-\n-    def __init__(self, config: OpenLlamaConfig):\n-        super().__init__(config)\n-        self.padding_idx = config.pad_token_id\n-        self.vocab_size = config.vocab_size\n-\n-        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n-        if config.use_stable_embedding:\n-            self.embed_layer_norm = nn.LayerNorm(config.hidden_size)\n-        else:\n-            self.embed_layer_norm = None\n-        self.layers = nn.ModuleList([OpenLlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n-        self.norm = OpenLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n-        self.gradient_checkpointing = False\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(OPEN_LLAMA_INPUTS_DOCSTRING)\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, BaseModelOutputWithPast]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        # retrieve input_ids and inputs_embeds\n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            batch_size, seq_length = input_ids.shape\n-        elif inputs_embeds is not None:\n-            batch_size, seq_length, _ = inputs_embeds.shape\n-        else:\n-            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n-\n-        seq_length_with_past = seq_length\n-        past_key_values_length = 0\n-\n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        if past_key_values is not None:\n-            past_key_values_length = past_key_values.get_seq_length()\n-            seq_length_with_past = seq_length_with_past + past_key_values_length\n-\n-        if position_ids is None:\n-            device = input_ids.device if input_ids is not None else inputs_embeds.device\n-            position_ids = torch.arange(\n-                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n-            )\n-            position_ids = position_ids.unsqueeze(0)\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n-            if self.embed_layer_norm:\n-                inputs_embeds = self.embed_layer_norm(inputs_embeds)\n-        # embed positions\n-        if self.config.use_memory_efficient_attention and self.training:\n-            attention_mask = None\n-        elif attention_mask is None:\n-            attention_mask = torch.ones(\n-                (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n-            )\n-\n-        input_shape = (batch_size, seq_length)\n-        attention_mask = _prepare_4d_causal_attention_mask(\n-            attention_mask, input_shape, inputs_embeds, past_key_values_length\n-        )\n-\n-        hidden_states = inputs_embeds\n-\n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        next_decoder_cache = () if use_cache else None\n-\n-        for idx, decoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            layer_outputs = decoder_layer(\n-                hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_values=past_key_values[idx] if past_key_values is not None else None,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-            )\n-\n-            hidden_states = layer_outputs[0]\n-\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n-        hidden_states = self.norm(hidden_states)\n-\n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n-        next_cache = next_decoder_cache if use_cache else None\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n-            last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-        )\n-\n-\n-class OpenLlamaForCausalLM(OpenLlamaPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.model = OpenLlamaModel(config)\n-        if config.shared_input_output_embedding:\n-            self.lm_head = None\n-        else:\n-            self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(OPEN_LLAMA_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, CausalLMOutputWithPast]:\n-        r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-        Returns:\n-\n-        Example:\n-\n-        ```python\n-        >>> from transformers import AutoTokenizer, OpenLlamaForCausalLM\n-\n-        >>> model = OpenLlamaForCausalLM.from_pretrained(\"openlm-research/open_llama_7b\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"openlm-research/open_llama_7b\")\n-\n-        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n-        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n-\n-        >>> # Generate\n-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n-        ```\"\"\"\n-\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        hidden_states = outputs[0]\n-        if self.config.shared_input_output_embedding:\n-            logits = torch.einsum(\n-                \"blh,vh->blv\", hidden_states.to(self.model.embed_tokens.weight.device), self.model.embed_tokens.weight\n-            )\n-        else:\n-            logits = self.lm_head(hidden_states)\n-\n-        loss = None\n-        if labels is not None:\n-            # move labels to correct device\n-            labels = labels.to(logits.device)\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n-        return CausalLMOutputWithPast(\n-            loss=loss,\n-            logits=logits,\n-            past_key_values=outputs.past_key_values,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-    def prepare_inputs_for_generation(\n-        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n-    ):\n-        if past_key_values is not None:\n-            past_length = past_key_values.get_seq_length()\n-\n-            # Some generation methods already pass only the last input ID\n-            if input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = input_ids.shape[1] - 1\n-\n-            input_ids = input_ids[:, remove_prefix_length:]\n-\n-        position_ids = kwargs.get(\"position_ids\")\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and past_key_values is None:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids}\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": kwargs.get(\"use_cache\"),\n-                \"attention_mask\": attention_mask,\n-            }\n-        )\n-        return model_inputs\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    The LLaMa Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`OpenLlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal\n-    models (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\",\n-    OPEN_LLAMA_START_DOCSTRING,\n-)\n-class OpenLlamaForSequenceClassification(OpenLlamaPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = OpenLlamaModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(OPEN_LLAMA_INPUTS_DOCSTRING)\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        transformer_outputs = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-        hidden_states = transformer_outputs[0]\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n-        else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n-\n-        loss = None\n-        if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-__all__ = [\"OpenLlamaPreTrainedModel\", \"OpenLlamaModel\", \"OpenLlamaForCausalLM\", \"OpenLlamaForSequenceClassification\"]"
        },
        {
            "sha": "864b321bc2ee3a521e3d6da5403cab7363dea56b",
            "filename": "src/transformers/models/deprecated/qdqbert/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d",
            "patch": "@@ -1,27 +0,0 @@\n-# Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from typing import TYPE_CHECKING\n-\n-from ....utils import _LazyModule\n-from ....utils.import_utils import define_import_structure\n-\n-\n-if TYPE_CHECKING:\n-    from .configuration_qdqbert import *\n-    from .modeling_qdqbert import *\n-else:\n-    import sys\n-\n-    _file = globals()[\"__file__\"]\n-    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "20e65ba2340aae80120790ab146bd2e662ed673f",
            "filename": "src/transformers/models/deprecated/qdqbert/configuration_qdqbert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fconfiguration_qdqbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fconfiguration_qdqbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fconfiguration_qdqbert.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "ca7f491508f24b64250d82cf89ef841ecbed4784",
            "filename": "src/transformers/models/deprecated/qdqbert/modeling_qdqbert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1563,
            "changes": 1563,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "cdfdeb5d179c8e954c9c6822d3f154d632394964",
            "filename": "src/transformers/models/deprecated/realm/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "6c6f9d4cea79203b322a92bd8bfe462b183091cf",
            "filename": "src/transformers/models/deprecated/realm/configuration_realm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 169,
            "changes": 169,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fconfiguration_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fconfiguration_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fconfiguration_realm.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "d4ea5198965a915ce71eae17dbc238282dc4f666",
            "filename": "src/transformers/models/deprecated/realm/modeling_realm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1665,
            "changes": 1665,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "354ca2aba63ad853cf7cb6962c75eb8b50551294",
            "filename": "src/transformers/models/deprecated/realm/retrieval_realm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 166,
            "changes": 166,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fretrieval_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fretrieval_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fretrieval_realm.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "af7cdc1a5bac3afea8ed0651ae2892adcc03cbc3",
            "filename": "src/transformers/models/deprecated/realm/tokenization_realm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 534,
            "changes": 534,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "53a1d99e0ac804ae196e9c65f0e2979dcb2401c8",
            "filename": "src/transformers/models/deprecated/realm/tokenization_realm_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 223,
            "changes": 223,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm_fast.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "a875576607430c041abab01eccaf468a6cc9272e",
            "filename": "src/transformers/models/deprecated/retribert/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "cfa30aa2ad66c14965b22e7e5494c2cf7bd79d1b",
            "filename": "src/transformers/models/deprecated/retribert/configuration_retribert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 108,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fconfiguration_retribert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fconfiguration_retribert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fconfiguration_retribert.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "9158e8cb83e377d28ed9121c3dc86eaeec964d27",
            "filename": "src/transformers/models/deprecated/retribert/modeling_retribert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 200,
            "changes": 200,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "288b46e267bccb358fa86e74ac2b98dcb99e8904",
            "filename": "src/transformers/models/deprecated/retribert/tokenization_retribert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 475,
            "changes": 475,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "5fcbc395a1b7ba7f659a9befae14c8ba658c27d0",
            "filename": "src/transformers/models/deprecated/retribert/tokenization_retribert_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 150,
            "changes": 150,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert_fast.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "78c549b6e294e1ca97a718ef601472d4d400e12a",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "9128840ce9d0cc84963635dc74544bfc49bbb2d9",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/configuration_speech_to_text_2.py",
            "status": "removed",
            "additions": 0,
            "deletions": 134,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fconfiguration_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fconfiguration_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fconfiguration_speech_to_text_2.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "f0ca446cc8381d58d4e32651495a7ee10db17f44",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/modeling_speech_to_text_2.py",
            "status": "removed",
            "additions": 0,
            "deletions": 826,
            "changes": 826,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "b69420bf540cbe737d6509b79d943af9d41e6c24",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/processing_speech_to_text_2.py",
            "status": "removed",
            "additions": 0,
            "deletions": 105,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fprocessing_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fprocessing_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fprocessing_speech_to_text_2.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "05c10080b7732601efd476a8c6e2640e74450a76",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/tokenization_speech_to_text_2.py",
            "status": "removed",
            "additions": 0,
            "deletions": 252,
            "changes": 252,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Ftokenization_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Ftokenization_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Ftokenization_speech_to_text_2.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "b535eb1df2c40a7680bbf8d57fc70b78e23437f4",
            "filename": "src/transformers/models/deprecated/tapex/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "fa74d8aa3b557c48cd9561c9f1b28aa2008b1b7f",
            "filename": "src/transformers/models/deprecated/tapex/tokenization_tapex.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1469,
            "changes": 1469,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "b4bccdd12c9703313951e223d0ac1955f9ca1581",
            "filename": "src/transformers/models/deprecated/trajectory_transformer/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "1e3a5f5ddb15dc1d066bef6a584bda6f04b913d7",
            "filename": "src/transformers/models/deprecated/trajectory_transformer/configuration_trajectory_transformer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 155,
            "changes": 155,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fconfiguration_trajectory_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fconfiguration_trajectory_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fconfiguration_trajectory_transformer.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "da7f7806671dbace1a10bd60d93e6782e27a5136",
            "filename": "src/transformers/models/deprecated/trajectory_transformer/convert_trajectory_transformer_original_pytorch_checkpoint_to_pytorch.py",
            "status": "removed",
            "additions": 0,
            "deletions": 70,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fconvert_trajectory_transformer_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fconvert_trajectory_transformer_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fconvert_trajectory_transformer_original_pytorch_checkpoint_to_pytorch.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "3b8cb4e2edf0e3965e16d623d844bea57deadf03",
            "filename": "src/transformers/models/deprecated/trajectory_transformer/modeling_trajectory_transformer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 528,
            "changes": 528,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "9bd3dd7b8838be6c8ad766aff370d675c3ad15ff",
            "filename": "src/transformers/models/deprecated/transfo_xl/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "4b349a76eb394e8080cc2e4783b4b08efda86ee6",
            "filename": "src/transformers/models/deprecated/transfo_xl/configuration_transfo_xl.py",
            "status": "removed",
            "additions": 0,
            "deletions": 189,
            "changes": 189,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconfiguration_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconfiguration_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconfiguration_transfo_xl.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "989a70ef71bd9156b353c0b4b9080a264aff8692",
            "filename": "src/transformers/models/deprecated/transfo_xl/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py",
            "status": "removed",
            "additions": 0,
            "deletions": 249,
            "changes": 249,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconvert_transfo_xl_original_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconvert_transfo_xl_original_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconvert_transfo_xl_original_tf_checkpoint_to_pytorch.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "d890edab89d9bebf9122d8747dc94f98ad0798f6",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1131,
            "changes": 1131,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "f76f3ccc6259fcb033b44eb43dd98be23482221c",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl_utilities.py",
            "status": "removed",
            "additions": 0,
            "deletions": 251,
            "changes": 251,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl_utilities.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl_utilities.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl_utilities.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "e7081cd46d423f86ae797d8c52079844197b89f8",
            "filename": "src/transformers/models/deprecated/transfo_xl/tokenization_transfo_xl.py",
            "status": "removed",
            "additions": 0,
            "deletions": 824,
            "changes": 824,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Ftokenization_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Ftokenization_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Ftokenization_transfo_xl.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "941db2f6ac5fefe66e06d598e1adbe0db1cf1769",
            "filename": "src/transformers/models/deprecated/tvlt/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "e3cd88760cbf03cbc9bfd39a0e03f6751ae91488",
            "filename": "src/transformers/models/deprecated/tvlt/configuration_tvlt.py",
            "status": "removed",
            "additions": 0,
            "deletions": 187,
            "changes": 187,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fconfiguration_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fconfiguration_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fconfiguration_tvlt.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "b9350d31a0197926ba96e04dacfdb7bc71adbfc6",
            "filename": "src/transformers/models/deprecated/tvlt/feature_extraction_tvlt.py",
            "status": "removed",
            "additions": 0,
            "deletions": 233,
            "changes": 233,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Ffeature_extraction_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Ffeature_extraction_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Ffeature_extraction_tvlt.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "7a7ae16ebc9f56b68a1db0bbc81f77332145f61d",
            "filename": "src/transformers/models/deprecated/tvlt/image_processing_tvlt.py",
            "status": "removed",
            "additions": 0,
            "deletions": 435,
            "changes": 435,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "99872fe5ea52f3d7d4019a5f121659e41d4ee862",
            "filename": "src/transformers/models/deprecated/tvlt/modeling_tvlt.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1234,
            "changes": 1234,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "1ecbac0dfa16f7c2cf70d07775bba2beeb456973",
            "filename": "src/transformers/models/deprecated/tvlt/processing_tvlt.py",
            "status": "removed",
            "additions": 0,
            "deletions": 86,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fprocessing_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fprocessing_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fprocessing_tvlt.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "9552c827365d3913539be3eafaf822146ada5829",
            "filename": "src/transformers/models/deprecated/van/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "33ffea192ddf9abc0bceceb7c26f5c7a207310fe",
            "filename": "src/transformers/models/deprecated/van/configuration_van.py",
            "status": "removed",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconfiguration_van.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconfiguration_van.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconfiguration_van.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "9f97d1c0c2967aebb8cd22377c2d3ac8f2529555",
            "filename": "src/transformers/models/deprecated/van/convert_van_to_pytorch.py",
            "status": "removed",
            "additions": 0,
            "deletions": 288,
            "changes": 288,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconvert_van_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconvert_van_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconvert_van_to_pytorch.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "3b2b484487a13fd48000884ecf8c220fdf8c03cf",
            "filename": "src/transformers/models/deprecated/van/modeling_van.py",
            "status": "removed",
            "additions": 0,
            "deletions": 517,
            "changes": 517,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "f5bd93aa4dabea9b2adbc54eeeb3664de589f43a",
            "filename": "src/transformers/models/deprecated/vit_hybrid/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "534c5f463b82d2ae44dfc6f560749e2bc57fa88d",
            "filename": "src/transformers/models/deprecated/vit_hybrid/configuration_vit_hybrid.py",
            "status": "removed",
            "additions": 0,
            "deletions": 180,
            "changes": 180,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fconfiguration_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fconfiguration_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fconfiguration_vit_hybrid.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "1d717d74c961e509697adab7623d2bc3fe64a1cf",
            "filename": "src/transformers/models/deprecated/vit_hybrid/convert_vit_hybrid_timm_to_pytorch.py",
            "status": "removed",
            "additions": 0,
            "deletions": 282,
            "changes": 282,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fconvert_vit_hybrid_timm_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fconvert_vit_hybrid_timm_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fconvert_vit_hybrid_timm_to_pytorch.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "662382be43dff424c857254d1b7a3e17c2791917",
            "filename": "src/transformers/models/deprecated/vit_hybrid/image_processing_vit_hybrid.py",
            "status": "removed",
            "additions": 0,
            "deletions": 336,
            "changes": 336,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "d414277e3e70f36587e8063e570687531294b59e",
            "filename": "src/transformers/models/deprecated/vit_hybrid/modeling_vit_hybrid.py",
            "status": "removed",
            "additions": 0,
            "deletions": 681,
            "changes": 681,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "c13c67012fa157a94bae21734a1f59b26c78588d",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2F__init__.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "4f0f40dda2328b1f69fe590443f53595858505d0",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/configuration_xlm_prophetnet.py",
            "status": "removed",
            "additions": 0,
            "deletions": 182,
            "changes": 182,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fconfiguration_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fconfiguration_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fconfiguration_xlm_prophetnet.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "05fa850a176bccf23ebda07780502ffd857475ad",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py",
            "status": "removed",
            "additions": 0,
            "deletions": 2160,
            "changes": 2160,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "77431b13c49f47ac9df31b3d4e76ac9e2060895d",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/tokenization_xlm_prophetnet.py",
            "status": "removed",
            "additions": 0,
            "deletions": 322,
            "changes": 322,
            "blob_url": "https://github.com/huggingface/transformers/blob/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Ftokenization_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d372b827547e0414809daf120cea4ba525bf601d/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Ftokenization_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Ftokenization_xlm_prophetnet.py?ref=d372b827547e0414809daf120cea4ba525bf601d"
        },
        {
            "sha": "9c0be2abb8ac645318be1a75cc11472ac7065109",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f311047860d46367d51d2d5b280286b10ba9466/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f311047860d46367d51d2d5b280286b10ba9466/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=9f311047860d46367d51d2d5b280286b10ba9466"
        },
        {
            "sha": "bf3b4dd21f88eff516f630b74b20d631523bdfc7",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f311047860d46367d51d2d5b280286b10ba9466/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f311047860d46367d51d2d5b280286b10ba9466/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=9f311047860d46367d51d2d5b280286b10ba9466"
        },
        {
            "sha": "89a24e16c3428d3ad003b26bb6d3d4e4642a723c",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f311047860d46367d51d2d5b280286b10ba9466/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f311047860d46367d51d2d5b280286b10ba9466/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=9f311047860d46367d51d2d5b280286b10ba9466"
        }
    ],
    "stats": {
        "total": 45767,
        "additions": 12,
        "deletions": 45755
    }
}