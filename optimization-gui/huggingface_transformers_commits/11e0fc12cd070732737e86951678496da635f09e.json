{
    "author": "ydshieh",
    "message": "another way of fix for #42400 (#42509)\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "11e0fc12cd070732737e86951678496da635f09e",
    "files": [
        {
            "sha": "f5df6e4bf4970bbdeae213c11b4a6a82bd1a0636",
            "filename": "tests/models/ernie4_5_moe/test_modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/11e0fc12cd070732737e86951678496da635f09e/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11e0fc12cd070732737e86951678496da635f09e/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py?ref=11e0fc12cd070732737e86951678496da635f09e",
            "patch": "@@ -89,15 +89,14 @@ def test_flash_attn_2_equivalence(self):\n                 # higher tolerance, not sure where it stems from\n                 assert torch.allclose(logits_fa, logits, atol=1e-2, rtol=1e-2)\n \n-    # Ignore copy\n-    @unittest.skip(\"TODO @ArthurZucker investigate later on\")\n+    @is_flaky(max_attempts=2)\n     def test_load_balancing_loss(self):\n         r\"\"\"\n         Let's make sure we can actually compute the loss and do a backward on it.\n         \"\"\"\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.num_labels = 3\n-        config.num_experts = 8\n+        config.num_experts = 3\n         config.expert_interval = 2\n         config.output_router_logits = True\n         input_ids = input_dict[\"input_ids\"]\n@@ -111,7 +110,7 @@ def test_load_balancing_loss(self):\n \n         # First, we make sure that adding padding tokens doesn't change the loss\n         # loss(input_ids, attention_mask=None) == loss(input_ids + padding, attention_mask=attention_mask_with_padding)\n-        pad_length = 1000\n+        pad_length = input_ids.shape[1] * 4\n         # Add padding tokens (assume that pad_token_id=1) to input_ids\n         padding_block = torch.ones(input_ids.shape[0], pad_length, dtype=torch.int32).to(torch_device)\n         padded_input_ids = torch.cat((padding_block, input_ids), dim=1)  # this is to simulate padding to the left"
        },
        {
            "sha": "9312d57e7fa6a91d6eff81f29be044bed1e64cf2",
            "filename": "tests/models/minimax/test_modeling_minimax.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/11e0fc12cd070732737e86951678496da635f09e/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11e0fc12cd070732737e86951678496da635f09e/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py?ref=11e0fc12cd070732737e86951678496da635f09e",
            "patch": "@@ -18,6 +18,7 @@\n from transformers import is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n+    is_flaky,\n     require_torch,\n     require_torch_accelerator,\n     slow,\n@@ -63,13 +64,14 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n+    @is_flaky(max_attempts=2)\n     def test_load_balancing_loss(self):\n         r\"\"\"\n         Let's make sure we can actually compute the loss and do a backward on it.\n         \"\"\"\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.num_labels = 3\n-        config.num_local_experts = 8\n+        config.num_local_experts = 3\n         config.output_router_logits = True\n         input_ids = input_dict[\"input_ids\"]\n         attention_mask = input_ids.ne(1).to(torch_device)\n@@ -82,7 +84,7 @@ def test_load_balancing_loss(self):\n \n         # First, we make sure that adding padding tokens doesn't change the loss\n         # loss(input_ids, attention_mask=None) == loss(input_ids + padding, attention_mask=attention_mask_with_padding)\n-        pad_length = 1000\n+        pad_length = input_ids.shape[1] * 4\n         # Add padding tokens (assume that pad_token_id=1) to input_ids\n         padding_block = torch.ones(input_ids.shape[0], pad_length, dtype=torch.int32).to(torch_device)\n         padded_input_ids = torch.cat((padding_block, input_ids), dim=1)  # this is to simulate padding to the left"
        },
        {
            "sha": "bf674b60c72227548e5493ebca3ec238ab44a100",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/11e0fc12cd070732737e86951678496da635f09e/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11e0fc12cd070732737e86951678496da635f09e/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=11e0fc12cd070732737e86951678496da635f09e",
            "patch": "@@ -20,6 +20,7 @@\n from transformers import is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n+    is_flaky,\n     require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n@@ -68,14 +69,14 @@ def is_pipeline_test_to_skip(\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         self.skipTest(reason=\"Mixtral flash attention does not support right padding\")\n \n-    # Ignore copy\n+    @is_flaky(max_attempts=2)\n     def test_load_balancing_loss(self):\n         r\"\"\"\n         Let's make sure we can actually compute the loss and do a backward on it.\n         \"\"\"\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.num_labels = 3\n-        config.num_local_experts = 8\n+        config.num_local_experts = 3\n         config.output_router_logits = True\n         input_ids = input_dict[\"input_ids\"]\n         attention_mask = input_ids.ne(1).to(torch_device)\n@@ -88,7 +89,7 @@ def test_load_balancing_loss(self):\n \n         # First, we make sure that adding padding tokens doesn't change the loss\n         # loss(input_ids, attention_mask=None) == loss(input_ids + padding, attention_mask=attention_mask_with_padding)\n-        pad_length = 1000\n+        pad_length = input_ids.shape[1] * 4\n         # Add padding tokens (assume that pad_token_id=1) to input_ids\n         padding_block = torch.ones(input_ids.shape[0], pad_length, dtype=torch.int32).to(torch_device)\n         padded_input_ids = torch.cat((padding_block, input_ids), dim=1)  # this is to simulate padding to the left"
        },
        {
            "sha": "61d33463fb415eb273a1bee69f541771240ce1e4",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/11e0fc12cd070732737e86951678496da635f09e/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11e0fc12cd070732737e86951678496da635f09e/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=11e0fc12cd070732737e86951678496da635f09e",
            "patch": "@@ -20,6 +20,7 @@\n from transformers import AutoTokenizer, is_torch_available, set_seed\n from transformers.testing_utils import (\n     cleanup,\n+    is_flaky,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n@@ -72,14 +73,14 @@ def is_pipeline_test_to_skip(\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         self.skipTest(reason=\"Qwen2Moe flash attention does not support right padding\")\n \n-    # Ignore copy\n+    @is_flaky(max_attempts=2)\n     def test_load_balancing_loss(self):\n         r\"\"\"\n         Let's make sure we can actually compute the loss and do a backward on it.\n         \"\"\"\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.num_labels = 3\n-        config.num_experts = 8\n+        config.num_experts = 3\n         config.expert_interval = 2\n         config.output_router_logits = True\n         input_ids = input_dict[\"input_ids\"]\n@@ -93,7 +94,7 @@ def test_load_balancing_loss(self):\n \n         # First, we make sure that adding padding tokens doesn't change the loss\n         # loss(input_ids, attention_mask=None) == loss(input_ids + padding, attention_mask=attention_mask_with_padding)\n-        pad_length = 1000\n+        pad_length = input_ids.shape[1] * 4\n         # Add padding tokens (assume that pad_token_id=1) to input_ids\n         padding_block = torch.ones(input_ids.shape[0], pad_length, dtype=torch.int32).to(torch_device)\n         padded_input_ids = torch.cat((padding_block, input_ids), dim=1)  # this is to simulate padding to the left"
        },
        {
            "sha": "eba6978e1e6e7f4adcb94a07e36ca3bb63572d04",
            "filename": "tests/models/qwen3_moe/test_modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/11e0fc12cd070732737e86951678496da635f09e/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11e0fc12cd070732737e86951678496da635f09e/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py?ref=11e0fc12cd070732737e86951678496da635f09e",
            "patch": "@@ -20,6 +20,7 @@\n from transformers import AutoTokenizer, BitsAndBytesConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n     cleanup,\n+    is_flaky,\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n@@ -63,14 +64,14 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    # Ignore copy\n+    @is_flaky(max_attempts=2)\n     def test_load_balancing_loss(self):\n         r\"\"\"\n         Let's make sure we can actually compute the loss and do a backward on it.\n         \"\"\"\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.num_labels = 3\n-        config.num_experts = 8\n+        config.num_experts = 3\n         config.expert_interval = 2\n         config.output_router_logits = True\n         input_ids = input_dict[\"input_ids\"]\n@@ -84,7 +85,7 @@ def test_load_balancing_loss(self):\n \n         # First, we make sure that adding padding tokens doesn't change the loss\n         # loss(input_ids, attention_mask=None) == loss(input_ids + padding, attention_mask=attention_mask_with_padding)\n-        pad_length = 1000\n+        pad_length = input_ids.shape[1] * 4\n         # Add padding tokens (assume that pad_token_id=1) to input_ids\n         padding_block = torch.ones(input_ids.shape[0], pad_length, dtype=torch.int32).to(torch_device)\n         padded_input_ids = torch.cat((padding_block, input_ids), dim=1)  # this is to simulate padding to the left"
        }
    ],
    "stats": {
        "total": 34,
        "additions": 19,
        "deletions": 15
    }
}