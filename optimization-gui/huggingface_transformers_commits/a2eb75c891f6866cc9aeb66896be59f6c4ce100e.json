{
    "author": "EduardDurech",
    "message": "Support for Flash Attention 3 (#38972)\n\n* Support `flash_attn_3`\nImplements fwd and tests for Flash Attention 3 https://github.com/Dao-AILab/flash-attention/commits/main/hopper\n\n- Includes checks for dropout>0 and ALiBi in `modeling_utils.PreTrainedModel._check_and_enable_flash_attn_3` (Dropout will likely be supported soon, so this will need to be updated and `modeling_flash_attention_utils._flash_attention_forward` at the `if _IS_FLASH_ATTN_3_AVAILABLE: ...`\n\nAn example Llama implementation is included in `modeling_llama.py` but other models would still need to be updated\n\nBased on https://github.com/huggingface/transformers/pull/36190 which has model implementations and examples which could be merged\n\n* Add tests for Flash Attention 2 and 3 parity\n\n* ci fix\n\n* FA2 compatibiity\n- `_prepare_flash_attention_from_position_ids` ->`prepare_fa2_from_position_ids`\n- Remove bettertransformer check in Flash Attention 3\n- Merge tests\n- Add licensing\n\n* ci fix\n\n* Test naming consistency\n\n* ci fix\n\n* Deprecation warning for `prepare_fa2_from_position_ids`\n\n* ci fix",
    "sha": "a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
    "files": [
        {
            "sha": "4e7a0c62d0fc38a9a805a47b7d7eda948de81a58",
            "filename": "pyproject.toml",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/pyproject.toml",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/pyproject.toml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/pyproject.toml?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -52,6 +52,7 @@ line-ending = \"auto\"\n addopts = \"--doctest-glob='**/*.md'\"\n doctest_optionflags=\"NUMBER NORMALIZE_WHITESPACE ELLIPSIS\"\n markers = [\n+    \"flash_attn_3_test: marks tests related to flash attention 3 (deselect with '-m \\\"not flash_attn_3_test\\\"')\",\n     \"flash_attn_test: marks tests related to flash attention (deselect with '-m \\\"not flash_attn_test\\\"')\",\n     \"bitsandbytes: select (or deselect with `not`) bitsandbytes integration tests\",\n     \"generate: marks tests that use the GenerationTesterMixin\""
        },
        {
            "sha": "00df0ef0fd66b5f9fb21fc85969bcbb9a5a6558e",
            "filename": "src/transformers/integrations/flash_attention.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_attention.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -75,6 +75,7 @@ def flash_attention_forward(\n         softcap=softcap,\n         use_top_left_mask=_use_top_left_mask,\n         target_dtype=target_dtype,\n+        attn_implementation=module.config._attn_implementation,\n         **kwargs,\n     )\n "
        },
        {
            "sha": "649447ca8f7b79500b9402d68bcb9daf2a4a44f1",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 174,
            "deletions": 24,
            "changes": 198,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -14,13 +14,15 @@\n \n import inspect\n import os\n+import warnings\n from typing import Optional, TypedDict\n \n import torch\n import torch.nn.functional as F\n \n from .utils import (\n     is_flash_attn_2_available,\n+    is_flash_attn_3_available,\n     is_flash_attn_greater_or_equal,\n     is_flash_attn_greater_or_equal_2_10,\n     is_torch_npu_available,\n@@ -32,18 +34,123 @@\n flash_attn_func = None\n \n \n-if is_flash_attn_2_available():\n-    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n-    from flash_attn import flash_attn_func, flash_attn_varlen_func\n-    from flash_attn.layers.rotary import apply_rotary_emb  # noqa\n+def _index_first_axis(tensor, indices):\n+    \"\"\"\n+    A local implementation of the PyTorch indexing operation `tensor[indices]` on the first axis,\n+    after flattening the first two dimensions of the tensor. This is functionally equivalent to\n+    FA2's `index_first_axis` and replaces the need to import it.\n+    \"\"\"\n+    # The input tensor is expected to be of shape (batch, seq_len, ...). We flatten the first\n+    # two dimensions to get (total_tokens, ...) before indexing.\n+    reshaped_tensor = tensor.reshape(-1, *tensor.shape[2:])\n+    return reshaped_tensor[indices]\n+\n+\n+def _fa3_unpad_input(hidden_states, attention_mask, unused_mask=None):\n+    \"\"\"\n+    FA3-compatible unpad_input function.\n \n+    Arguments:\n+        hidden_states: (batch, seqlen, ...)\n+        attention_mask: (batch, seqlen), bool / int, 1 means valid and 0 means not valid.\n+        unused_mask: (batch, seqlen), bool / int, 1 means the element is allocated but unused.\n+    Return:\n+        hidden_states: (total_nnz, ...), where total_nnz = number of tokens selected in attention_mask + unused_mask.\n+        indices: (total_nnz), the indices of masked tokens from the flattened input sequence.\n+        cu_seqlens: (batch + 1), the cumulative sequence lengths, used to index into hidden_states.\n+        max_seqlen_in_batch: int\n+        seqused: (batch), returns the number of tokens selected in attention_mask + unused_mask.\n+    \"\"\"\n+    all_masks = (attention_mask + unused_mask) if unused_mask is not None else attention_mask\n+    seqlens_in_batch = all_masks.sum(dim=-1, dtype=torch.int32)\n+    used_seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n+    indices = torch.nonzero(all_masks.flatten(), as_tuple=False).flatten()\n+    max_seqlen_in_batch = seqlens_in_batch.max().item()\n+    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n+\n+    return (\n+        _index_first_axis(hidden_states, indices),\n+        indices,\n+        cu_seqlens,\n+        max_seqlen_in_batch,\n+        used_seqlens_in_batch,\n+    )\n+\n+\n+def _fa3_pad_input(hidden_states, indices, batch, seqlen):\n+    \"\"\"\n+    FA3-compatible pad_input function.\n+\n+    Arguments:\n+        hidden_states: (total_nnz, ...), where total_nnz = number of tokens in selected in attention_mask.\n+        indices: (total_nnz), the indices that represent the non-masked tokens of the original padded input sequence.\n+        batch: int, batch size for the padded sequence.\n+        seqlen: int, maximum sequence length for the padded sequence.\n+    Return:\n+        hidden_states: (batch, seqlen, ...)\n+    \"\"\"\n+    dim = hidden_states.shape[1:]\n+    output = torch.zeros((batch * seqlen), *dim, device=hidden_states.device, dtype=hidden_states.dtype)\n+    output[indices] = hidden_states\n+    return output.view(batch, seqlen, *dim)\n+\n+\n+FA_VERSION = None\n+if is_flash_attn_2_available():\n+    from flash_attn import flash_attn_func as flash_attn_2_func\n+    from flash_attn import flash_attn_varlen_func as flash_attn_2_varlen_func\n+    from flash_attn.bert_padding import pad_input as pad_input_fa2\n+    from flash_attn.bert_padding import unpad_input as unpad_input_fa2\n+    from flash_attn.layers.rotary import apply_rotary_emb\n+\n+    HAS_FA2 = True\n+    FA_VERSION = 2\n+else:\n+    flash_attn_2_func = None\n+    flash_attn_2_varlen_func = None\n+    pad_input_fa2 = None\n+    unpad_input_fa2 = None\n+    apply_rotary_emb = None\n+    HAS_FA2 = False\n+\n+if is_flash_attn_3_available():\n+    from flash_attn_interface import flash_attn_func as flash_attn_3_func\n+    from flash_attn_interface import flash_attn_varlen_func as flash_attn_3_varlen_func\n+\n+    pad_input_fa3 = _fa3_pad_input\n+    unpad_input_fa3 = _fa3_unpad_input\n+    HAS_FA3 = True\n+    FA_VERSION = 3\n+else:\n+    flash_attn_3_func = None\n+    flash_attn_3_varlen_func = None\n+    pad_input_fa3 = None\n+    unpad_input_fa3 = None\n+    HAS_FA3 = False\n+\n+\n+# Current Flash Attention implementations\n+if FA_VERSION:\n+    flash_attn_func = globals()[f\"flash_attn_{FA_VERSION}_func\"]\n+    flash_attn_varlen_func = globals()[f\"flash_attn_{FA_VERSION}_varlen_func\"]\n+    unpad_input = globals()[f\"unpad_input_fa{FA_VERSION}\"]\n+    pad_input = globals()[f\"pad_input_fa{FA_VERSION}\"]\n \n # patch functions in package `flash-attn` when using flash-attention on Ascend NPU.\n if is_torch_npu_available():\n-    from .integrations.npu_flash_attention import index_first_axis, pad_input, unpad_input\n-    from .integrations.npu_flash_attention import npu_apply_rotary_emb as apply_rotary_emb  # noqa\n-    from .integrations.npu_flash_attention import npu_flash_attn_func as flash_attn_func\n-    from .integrations.npu_flash_attention import npu_flash_attn_varlen_func as flash_attn_varlen_func\n+    from .integrations.npu_flash_attention import (\n+        npu_apply_rotary_emb as apply_rotary_emb,  # noqa: F401\n+    )\n+    from .integrations.npu_flash_attention import (\n+        npu_flash_attn_func as flash_attn_func,\n+    )\n+    from .integrations.npu_flash_attention import (\n+        npu_flash_attn_varlen_func as flash_attn_varlen_func,\n+    )\n+    from .integrations.npu_flash_attention import (\n+        pad_input,\n+        unpad_input,\n+    )\n \n \n _flash_supports_window_size = False\n@@ -56,6 +163,9 @@\n def is_flash_attn_available():\n     \"\"\"Determine whether flash-attention can be used or not.\"\"\"\n \n+    if is_flash_attn_3_available():\n+        return True\n+\n     # if package `flash-attn` is available, flash-attention can be used natively.\n     if is_flash_attn_2_available():\n         return True\n@@ -70,6 +180,9 @@ def is_flash_attn_available():\n def flash_attn_supports_top_left_mask():\n     \"\"\"Determine whether flash-attention uses top-left or down-right mask\"\"\"\n \n+    if is_flash_attn_3_available():\n+        return False\n+\n     if is_flash_attn_2_available():\n         # top-left mask is used in package `flash-attn` with version lower than 2.1.0\n         return not is_flash_attn_greater_or_equal_2_10()\n@@ -116,6 +229,7 @@ def _upad_input(\n     value_layer: torch.Tensor,\n     attention_mask: torch.Tensor,\n     query_length: int,\n+    unpad_input_func,\n ):\n     \"\"\"\n     Unpads query, key, and values tensors, using a single dimension for all tokens even though they belong to different batches.\n@@ -134,6 +248,8 @@ def _upad_input(\n             Boolean or int tensor of shape (batch_size, sequence_length), 1 means valid and 0 means not valid.\n         query_length (`int`):\n             Target length.\n+        unpad_input_func:\n+            The function to use for unpadding the input tensors.\n \n     Return:\n         query_layer (`torch.Tensor`):\n@@ -158,12 +274,10 @@ def _upad_input(\n \n     batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n \n-    key_layer = index_first_axis(key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)\n-    value_layer = index_first_axis(\n-        value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n-    )\n+    key_layer = _index_first_axis(key_layer, indices_k)\n+    value_layer = _index_first_axis(value_layer, indices_k)\n     if query_length == kv_seq_len:\n-        query_layer = index_first_axis(query_layer.reshape(batch_size * kv_seq_len, -1, head_dim), indices_k)\n+        query_layer = _index_first_axis(query_layer, indices_k)\n         cu_seqlens_q = cu_seqlens_k\n         max_seqlen_in_batch_q = max_seqlen_in_batch_k\n         indices_q = indices_k\n@@ -177,7 +291,7 @@ def _upad_input(\n     else:\n         # The -q_len: slice assumes left padding.\n         attention_mask = attention_mask[:, -query_length:]\n-        query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q, *_ = unpad_input(query_layer, attention_mask)\n+        query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q, *_ = unpad_input_func(query_layer, attention_mask)\n \n     return (\n         query_layer,\n@@ -189,7 +303,7 @@ def _upad_input(\n     )\n \n \n-def prepare_fa2_from_position_ids(query, key, value, position_ids):\n+def _prepare_flash_attention_from_position_ids(query, key, value, position_ids):\n     \"\"\"\n     This function returns necessary arguments to call `flash_attn_varlen_func`.\n     All three query, key, value states will be flattened.\n@@ -239,6 +353,14 @@ def prepare_fa2_from_position_ids(query, key, value, position_ids):\n     return (query, key, value, indices_q, (cu_seq_lens, cu_seq_lens), (max_length, max_length))\n \n \n+def prepare_fa2_from_position_ids(*args, **kwargs):\n+    warnings.warn(\n+        \"The function `prepare_fa2_from_position_ids` in `transformers.modeling_flash_attention_utils` is deprecated and will be removed in a future version. Please use `_prepare_flash_attention_from_position_ids` instead.\",\n+        FutureWarning,\n+    )\n+    return _prepare_flash_attention_from_position_ids(*args, **kwargs)\n+\n+\n def fa_peft_integration_check(\n     query: torch.Tensor,\n     key: torch.Tensor,\n@@ -303,6 +425,7 @@ def _flash_attention_forward(\n     max_length_q: Optional[int] = None,\n     max_length_k: Optional[int] = None,\n     target_dtype: Optional[torch.dtype] = None,\n+    attn_implementation: Optional[str] = None,\n     **kwargs,\n ):\n     \"\"\"\n@@ -329,7 +452,28 @@ def _flash_attention_forward(\n             Softcap for the attention logits, used e.g. in gemma2.\n         deterministic (`bool`, *optional*):\n             Determines if the deterministic option introduced in flash_attn>=2.4.1 is enabled.\n+        attn_implementation (`str`, *optional*):\n+            The attention implementation to use. If None, will default to the one based on the environment.\n     \"\"\"\n+    if attn_implementation is None:\n+        _flash_attn_varlen_func = flash_attn_varlen_func\n+        _flash_attn_func = flash_attn_func\n+        _pad_input = pad_input\n+        _unpad_input = unpad_input\n+        _is_fa3 = HAS_FA3\n+    elif attn_implementation == \"flash_attention_3\":\n+        _flash_attn_varlen_func = flash_attn_3_varlen_func\n+        _flash_attn_func = flash_attn_3_func\n+        _pad_input = pad_input_fa3\n+        _unpad_input = unpad_input_fa3\n+        _is_fa3 = True\n+    elif attn_implementation == \"flash_attention_2\":\n+        _flash_attn_varlen_func = flash_attn_2_varlen_func\n+        _flash_attn_func = flash_attn_2_func\n+        _pad_input = pad_input_fa2\n+        _unpad_input = unpad_input_fa2\n+        _is_fa3 = False\n+\n     if not use_top_left_mask:\n         causal = is_causal\n     else:\n@@ -342,6 +486,12 @@ def _flash_attention_forward(\n     )\n     flash_kwargs = {\"window_size\": (sliding_window, sliding_window)} if use_sliding_windows else {}\n \n+    if _is_fa3:\n+        if dropout > 0.0:\n+            logger.warning_once(\"Flash Attention 3 does not support dropout. Setting dropout to 0.0.\")\n+    else:\n+        flash_kwargs[\"dropout_p\"] = dropout\n+\n     if flash_241:\n         if deterministic is None:\n             global deterministic_g\n@@ -362,25 +512,24 @@ def _flash_attention_forward(\n     if attention_mask is not None:\n         batch_size = query_states.shape[0]\n         query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = _upad_input(\n-            query_states, key_states, value_states, attention_mask, query_length\n+            query_states, key_states, value_states, attention_mask, query_length, _unpad_input\n         )\n         cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n         max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n \n-        attn_output_unpad = flash_attn_varlen_func(\n+        attn_output_unpad = _flash_attn_varlen_func(\n             query_states,\n             key_states,\n             value_states,\n             cu_seqlens_q=cu_seqlens_q,\n             cu_seqlens_k=cu_seqlens_k,\n             max_seqlen_q=max_seqlen_in_batch_q,\n             max_seqlen_k=max_seqlen_in_batch_k,\n-            dropout_p=dropout,\n             softmax_scale=softmax_scale,\n             causal=causal,\n             **flash_kwargs,\n         )\n-        attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n+        attn_output = _pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n \n     # If position_ids is provided and check all examples do not contain only 1 sequence, If tensor in increasing\n     # then we probably have one sequence, otherwise it is packed. Additionally check we are in pre-fill/training stage.\n@@ -394,7 +543,7 @@ def _flash_attention_forward(\n \n         if cu_seq_lens_q is None or cu_seq_lens_k is None:\n             query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = (\n-                prepare_fa2_from_position_ids(query_states, key_states, value_states, position_ids)\n+                _prepare_flash_attention_from_position_ids(query_states, key_states, value_states, position_ids)\n             )\n \n             cu_seq_lens_q, cu_seq_lens_k = cu_seq_lens\n@@ -405,15 +554,14 @@ def _flash_attention_forward(\n             key_states = key_states.reshape(-1, key_states.size(-2), key_states.size(-1))\n             value_states = value_states.reshape(-1, value_states.size(-2), value_states.size(-1))\n \n-        attn_output = flash_attn_varlen_func(\n+        attn_output = _flash_attn_varlen_func(\n             query_states,\n             key_states,\n             value_states,\n             cu_seqlens_q=cu_seq_lens_q,\n             cu_seqlens_k=cu_seq_lens_k,\n             max_seqlen_q=max_length_q,\n             max_seqlen_k=max_length_k,\n-            dropout_p=dropout,\n             softmax_scale=softmax_scale,\n             causal=causal,\n             **flash_kwargs,\n@@ -422,10 +570,12 @@ def _flash_attention_forward(\n         attn_output = attn_output.view(batch_size, -1, attn_output.size(-2), attn_output.size(-1))\n \n     else:\n-        attn_output = flash_attn_func(\n-            query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=causal, **flash_kwargs\n+        attn_output = _flash_attn_func(\n+            query_states, key_states, value_states, softmax_scale=softmax_scale, causal=causal, **flash_kwargs\n         )\n \n+    if isinstance(attn_output, tuple):\n+        return attn_output[0]\n     return attn_output\n \n "
        },
        {
            "sha": "a5d1be345d107b840a402c968348a0fdb3b5f0bc",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 105,
            "deletions": 2,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -105,6 +105,7 @@\n     is_accelerate_available,\n     is_bitsandbytes_available,\n     is_flash_attn_2_available,\n+    is_flash_attn_3_available,\n     is_kernels_available,\n     is_offline_mode,\n     is_optimum_available,\n@@ -1957,6 +1958,9 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, PushToHubMixin, PeftAdapterMi\n     # Flash Attention 2 support\n     _supports_flash_attn_2 = False\n \n+    # Flash Attention 3 support\n+    _supports_flash_attn_3 = False\n+\n     # SDPA support\n     _supports_sdpa = False\n \n@@ -2247,6 +2251,8 @@ def _autoset_attn_implementation(\n                 and config._attn_implementation not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys()\n             ):\n                 message = f'Specified `attn_implementation=\"{config._attn_implementation}\"` is not supported. The only possible arguments are `attn_implementation=\"eager\"` (manual attention implementation)'\n+                if cls._supports_flash_attn_3:\n+                    message += ', `\"attn_implementation=flash_attention_3\"` (implementation using flash attention 3)'\n                 if cls._supports_flash_attn_2:\n                     message += ', `\"attn_implementation=flash_attention_2\"` (implementation using flash attention 2)'\n                 if cls._supports_sdpa:\n@@ -2282,7 +2288,15 @@ def _autoset_attn_implementation(\n             ):\n                 sub_config._attn_implementation_internal = curr_attn_implementation\n \n-        if config._attn_implementation == \"flash_attention_2\":\n+        if config._attn_implementation == \"flash_attention_3\":\n+            cls._check_and_enable_flash_attn_3(\n+                config,\n+                torch_dtype=torch_dtype,\n+                device_map=device_map,\n+                hard_check_only=False,\n+                check_device_map=check_device_map,\n+            )\n+        elif config._attn_implementation == \"flash_attention_2\":\n             cls._check_and_enable_flash_attn_2(\n                 config,\n                 torch_dtype=torch_dtype,\n@@ -2498,6 +2512,94 @@ def _check_and_enable_flash_attn_2(\n             config._attn_implementation = \"flash_attention_2\"\n         return config\n \n+    @classmethod\n+    def _check_and_enable_flash_attn_3(\n+        cls,\n+        config,\n+        torch_dtype: Optional[torch.dtype] = None,\n+        device_map: Optional[Union[str, dict[str, int]]] = None,\n+        check_device_map: bool = True,\n+        hard_check_only: bool = False,\n+    ) -> PretrainedConfig:\n+        \"\"\"\n+        Checks the availability of Flash Attention 3 and compatibility with the current model.\n+\n+        If all checks pass and `hard_check_only` is False, the method will set the config attribute `attn_implementation` to \"flash_attention_3\" so that the model can initialize the correct attention module.\n+        \"\"\"\n+        if not cls._supports_flash_attn_3:\n+            raise ValueError(\n+                f\"{cls.__name__} does not support Flash Attention 3.0 yet. Please request to add support where\"\n+                f\" the model is hosted, on its model hub page: https://huggingface.co/{config._name_or_path}/discussions/new\"\n+                \" or in the Transformers GitHub repo: https://github.com/huggingface/transformers/issues/new\"\n+            )\n+\n+        if not is_flash_attn_3_available():\n+            preface = \"FlashAttention3 has been toggled on, but it cannot be used due to the following error:\"\n+\n+            if importlib.util.find_spec(\"flash_attn_3\") is None:\n+                raise ImportError(f\"{preface} the package flash_attn_3 seems to be not installed.\")\n+\n+            if torch.cuda.is_available():\n+                major, _ = torch.cuda.get_device_capability()\n+                if major < 9:\n+                    raise ValueError(\n+                        f\"{preface} Flash Attention 3 requires compute capability >= 9.0, but found {torch.cuda.get_device_capability()} with compute capability {major}.0.\"\n+                    )\n+                else:\n+                    raise ImportError(f\"{preface} Flash Attention 3 is not available.\")\n+            else:\n+                raise ValueError(\n+                    f\"{preface} Flash Attention 3 is not available on CPU. Please make sure torch can access a CUDA device.\"\n+                )\n+\n+        if torch_dtype is None:\n+            logger.warning_once(\n+                \"You are attempting to use Flash Attention 3 without specifying a torch dtype. This might lead to unexpected behaviour\"\n+            )\n+        elif torch_dtype is not None and torch_dtype not in [torch.float16, torch.bfloat16]:\n+            logger.warning_once(\n+                \"Flash Attention 3 only supports torch.float16 and torch.bfloat16 dtypes, but\"\n+                f\" the current dype in {cls.__name__} is {torch_dtype}. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator,\"\n+                ' or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"meta-llama/Llama-3.2-1B\", attn_implementation=\"flash_attention_3\", torch_dtype=torch.float16)`'\n+            )\n+\n+        if getattr(config, \"alibi\", False) or getattr(config, \"use_alibi\", False):\n+            raise ValueError(\"Model is configured to use ALiBi, which is not supported by Flash Attention 3.\")\n+\n+        # Check for attention dropout, which is incompatible with FA3\n+        if hasattr(config, \"attention_dropout\") and config.attention_dropout > 0:\n+            raise ValueError(\n+                f\"Model has attention_dropout={config.attention_dropout}, which is not supported by Flash Attention 3.\"\n+            )\n+\n+        # The check `torch.empty(0).device.type != \"cuda\"` is needed as the model may be initialized after `torch.set_default_device` has been called,\n+        # or the model may be initialized under the context manager `with torch.device(\"cuda\"):`.\n+        if check_device_map and device_map is None and torch.empty(0).device.type not in [\"cuda\", \"mlu\"]:\n+            if torch.cuda.is_available():\n+                logger.warning_once(\n+                    \"You are attempting to use Flash Attention 3 with a model not initialized on GPU. Make sure to move the model to GPU\"\n+                    \" after initializing it on CPU with `model.to('cuda')`.\"\n+                )\n+            else:\n+                raise ValueError(\n+                    \"You are attempting to use Flash Attention 3 with a model not initialized on GPU and with no GPU available. \"\n+                    \"This is not supported yet. Please make sure to have access to a GPU and either initialise the model on a GPU by passing a device_map \"\n+                    \"or initialising the model on CPU and then moving it to GPU.\"\n+                )\n+        elif (\n+            check_device_map\n+            and device_map is not None\n+            and isinstance(device_map, dict)\n+            and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n+        ):\n+            raise ValueError(\n+                \"You are attempting to use Flash Attention 3 with a model dispatched on CPU or disk. This is not supported. Please make sure to \"\n+                \"initialise the model on a GPU by passing a device_map that contains only GPU devices as keys.\"\n+            )\n+        if not hard_check_only:\n+            config._attn_implementation = \"flash_attention_3\"\n+        return config\n+\n     @classmethod\n     def _check_and_enable_sdpa(cls, config, hard_check_only: bool = False) -> PretrainedConfig:\n         \"\"\"\n@@ -4134,7 +4236,7 @@ def from_pretrained(\n \n                 </Tip>\n             attn_implementation (`str`, *optional*):\n-                The attention implementation to use in the model (if relevant). Can be any of `\"eager\"` (manual implementation of the attention), `\"sdpa\"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `\"flash_attention_2\"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `\"eager\"` implementation.\n+                The attention implementation to use in the model (if relevant). Can be any of `\"eager\"` (manual implementation of the attention), `\"sdpa\"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), `\"flash_attention_2\"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)), or `\"flash_attention_3\"` (using [Dao-AILab/flash-attention/hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `\"eager\"` implementation.\n \n             > Parameters for big model inference\n \n@@ -5770,6 +5872,7 @@ class AttentionInterface(GeneralInterface):\n     # Class instance object, so that a call to `register` can be reflected into all other files correctly, even if\n     # a new instance is created (in order to locally override a given function)\n     _global_mapping = {\n+        \"flash_attention_3\": flash_attention_forward,\n         \"flash_attention_2\": flash_attention_forward,\n         \"flex_attention\": flex_attention_forward,\n         \"paged_attention\": paged_attention_forward,"
        },
        {
            "sha": "c224c4300eb46bef9bc2ee50ef13abcf1b210f7b",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -321,6 +321,7 @@ class ArceePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ArceeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "87f11d192693ba4338d26e4b4ed2cf6a9a2599fa",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -667,6 +667,7 @@ class AriaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"AriaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "afafd3f9118805f4b686f5e012bb97e5bed77106",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -318,6 +318,7 @@ class BitNetPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"BitNetDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "ad1604bed4a379f22bc3ec6f30e6c48325a4bd0c",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -355,6 +355,7 @@ class CoherePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"CohereDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "3fec29e976096ccdd99e9288608782909b89603d",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -334,6 +334,7 @@ class Cohere2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Cohere2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "541ae6669e92d0cf2608c6ae6536b670c4abac65",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -504,6 +504,7 @@ class DeepseekV3PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DeepseekV3DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "383c329c9909c1c682d82ffb9750850fca7c442e",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -556,6 +556,7 @@ class DiffLlamaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DiffLlamaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = False"
        },
        {
            "sha": "58b805cca613b752d70c87a5b14d4547365b1822",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -424,6 +424,7 @@ class Dots1PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Dots1DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "04b438c5ab4faf168d87a3f82321c5c61a37bb02",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -318,6 +318,7 @@ class GemmaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"GemmaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "bfd3317946be6c6a355e80192eb39518373e8c32",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -339,6 +339,7 @@ class Gemma2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Gemma2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "084ef0893a7cdfabcb4bca8d3e3145fddc77926a",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -422,6 +422,7 @@ class Gemma3PreTrainedModel(PreTrainedModel):\n         \"SiglipMultiheadAttentionPoolingHead\",\n     ]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "86538fc25e58a9021b6067db0e6a50034d5c48d8",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -335,6 +335,7 @@ class GlmPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"GlmDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "55cc8869d9525be4f498497e722b6c079aefb7b7",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -343,6 +343,7 @@ class Glm4PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Glm4DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "2e563e401f23231a7d4a335f88070c05a8a0a641",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -292,6 +292,7 @@ class GPTNeoXPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"GPTNeoXLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "b65530c40613f8d9ae9d1a6f69c26c19f121ceb3",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -305,6 +305,7 @@ class GranitePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"GraniteDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "3a48d931ca1e645d508f8b497e183e3138af3118",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -320,6 +320,7 @@ class HeliumPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"HeliumDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "e79a76976028fef00215ed5363bc4626abd63b9b",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -320,6 +320,7 @@ class LlamaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "66ed4adcea4cd17d924d665c36832d9d424799bf",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -590,6 +590,7 @@ class MiniMaxPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MiniMaxDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "4b222eabe2371fda8a5487b7930998433e87391d",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -262,6 +262,7 @@ class MistralPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MistralDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "526bf2bbd756c4d23a64081d23476db7f804fc65",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -417,6 +417,7 @@ class MixtralPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MixtralDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "fc6a7188623aaad7b9485462f85ba7a3d147991d",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -301,6 +301,7 @@ class OlmoPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"OlmoDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "84f5e5ad4e8ab8a02cc9ea0c5fdd6c4da7ec81eb",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -305,6 +305,7 @@ class Olmo2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Olmo2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "1c513604406522c1c1a43d26b7aa5364dd087525",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -295,6 +295,7 @@ class PhiPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PhiDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "54fd3d1caf7336ad7f2ab9f9568a050ed80042de",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -316,6 +316,7 @@ class Phi3PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Phi3DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "27c199bf50a084ba0b8c462d08dfdb706d382f49",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -1622,6 +1622,7 @@ class Phi4MultimodalPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Phi4MultimodalDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "4ba0b43e134f74d788aef7ddcaf222c096122c07",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -266,6 +266,7 @@ class Qwen2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "e64f96675977ee40c2527ce4c3cb71febea2285e",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -292,6 +292,7 @@ class Qwen3PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen3DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "47ec0d10ab1247c549f457e0758a679e97f03c0c",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -424,6 +424,7 @@ class Qwen3MoePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen3MoeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "1e1d9c64363250012cf8304b27d9dfc77e59758d",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -299,6 +299,7 @@ class Starcoder2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Starcoder2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "a6cec1c0997239bef7f2ba826bd12c6d5d096d6c",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -561,6 +561,7 @@ class T5GemmaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"T5GemmaBlock\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "2ddbd51d41401349bc9bae4c809ebed7fbc03294",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -86,6 +86,7 @@\n     is_faiss_available,\n     is_fbgemm_gpu_available,\n     is_flash_attn_2_available,\n+    is_flash_attn_3_available,\n     is_flax_available,\n     is_flute_available,\n     is_fsdp_available,\n@@ -571,6 +572,15 @@ def require_flash_attn(test_case):\n     return unittest.skipUnless(is_flash_attn_2_available(), \"test requires Flash Attention\")(test_case)\n \n \n+def require_flash_attn_3(test_case):\n+    \"\"\"\n+    Decorator marking a test that requires Flash Attention 3.\n+\n+    These tests are skipped when Flash Attention 3 isn't installed.\n+    \"\"\"\n+    return unittest.skipUnless(is_flash_attn_3_available(), \"test requires Flash Attention 3\")(test_case)\n+\n+\n def require_torch_sdpa(test_case):\n     \"\"\"\n     Decorator marking a test that requires PyTorch's SDPA."
        },
        {
            "sha": "7ca4c3552808f96fe5183ac7d9346bd33067d0e2",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -153,6 +153,7 @@\n     is_faiss_available,\n     is_fbgemm_gpu_available,\n     is_flash_attn_2_available,\n+    is_flash_attn_3_available,\n     is_flash_attn_greater_or_equal,\n     is_flash_attn_greater_or_equal_2_10,\n     is_flax_available,"
        },
        {
            "sha": "61f947516ff7b0e548e46761916c3ed5be9acbf5",
            "filename": "src/transformers/utils/args_doc.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Futils%2Fargs_doc.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Futils%2Fargs_doc.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fargs_doc.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -926,6 +926,9 @@ class ClassAttrs:\n     _skip_keys_device_placement = r\"\"\"\n     A list of keys to ignore when moving inputs or outputs between devices when using the `accelerate` library.\n     \"\"\"\n+    _supports_flash_attn_3 = r\"\"\"\n+    Whether the model's attention implementation supports FlashAttention 3.0.\n+    \"\"\"\n     _supports_flash_attn_2 = r\"\"\"\n     Whether the model's attention implementation supports FlashAttention 2.0.\n     \"\"\""
        },
        {
            "sha": "014366cc97785b49909fc81607a423a7fa410c89",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -1120,6 +1120,25 @@ def is_flash_attn_2_available():\n         return False\n \n \n+@lru_cache()\n+def is_flash_attn_3_available():\n+    if not is_torch_available():\n+        return False\n+\n+    if not _is_package_available(\"flash_attn_3\"):\n+        return False\n+\n+    import torch\n+\n+    if not torch.cuda.is_available():\n+        return False\n+\n+    # TODO: Check for a minimum version when FA3 is stable\n+    # return version.parse(importlib.metadata.version(\"flash_attn_3\")) >= version.parse(\"3.0.0\")\n+\n+    return True\n+\n+\n @lru_cache\n def is_flash_attn_greater_or_equal_2_10():\n     if not _is_package_available(\"flash_attn\"):"
        },
        {
            "sha": "187bdfe24cd95e8ecf405223c59a9c511d86b857",
            "filename": "tests/generation/test_flash_attention_parity.py",
            "status": "added",
            "additions": 144,
            "deletions": 0,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/tests%2Fgeneration%2Ftest_flash_attention_parity.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/tests%2Fgeneration%2Ftest_flash_attention_parity.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_flash_attention_parity.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -0,0 +1,144 @@\n+# Copyright 2025 Eduard Durech and SGLang team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+# Usage:\n+# RUN_SLOW=1 pytest -s tests/generation/test_flash_attention_parity.py\n+\n+import unittest\n+\n+import pytest\n+import torch\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers.testing_utils import require_flash_attn, require_flash_attn_3, require_torch_gpu, slow\n+\n+\n+class FlashAttentionParityTest(unittest.TestCase):\n+    # From https://github.com/sgl-project/sglang/blob/main/python/sglang/test/test_utils.py\n+    def _lcs(self, X, Y):\n+        m = len(X)\n+        n = len(Y)\n+        L = [[0] * (n + 1) for _ in range(m + 1)]\n+\n+        for i in range(m + 1):\n+            for j in range(n + 1):\n+                if i == 0 or j == 0:\n+                    L[i][j] = 0\n+                elif X[i - 1] == Y[j - 1]:\n+                    L[i][j] = L[i - 1][j - 1] + 1\n+                else:\n+                    L[i][j] = max(L[i - 1][j], L[i][j - 1])\n+\n+        return L[m][n]\n+\n+    # From https://github.com/sgl-project/sglang/blob/main/python/sglang/test/test_utils.py\n+    def _calculate_rouge_l(self, output_strs_list1, output_strs_list2):\n+        rouge_l_scores = []\n+\n+        for s1, s2 in zip(output_strs_list1, output_strs_list2):\n+            lcs_len = self._lcs(s1, s2)\n+            precision = lcs_len / len(s1) if len(s1) > 0 else 0\n+            recall = lcs_len / len(s2) if len(s2) > 0 else 0\n+            if precision + recall > 0:\n+                fmeasure = (2 * precision * recall) / (precision + recall)\n+            else:\n+                fmeasure = 0.0\n+            rouge_l_scores.append(fmeasure)\n+\n+        return rouge_l_scores\n+\n+    def _benchmark_generation(self, model, inputs, n_warmup=3, n_runs=5):\n+        for _ in range(n_warmup):\n+            model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+        torch.cuda.synchronize()\n+\n+        start_time = torch.cuda.Event(enable_timing=True)\n+        end_time = torch.cuda.Event(enable_timing=True)\n+\n+        start_time.record()\n+        for _ in range(n_runs):\n+            model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+        end_time.record()\n+        torch.cuda.synchronize()\n+\n+        return start_time.elapsed_time(end_time) / n_runs\n+\n+    @pytest.mark.flash_attn_3_test\n+    @require_torch_gpu\n+    @require_flash_attn\n+    @require_flash_attn_3\n+    @slow\n+    def test_flash_attention_2_3_parity(self):\n+        model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n+        prompt = \"The ETH AI Center is\"\n+\n+        # 1. Load FA2 model and tokenizer\n+        model_2 = AutoModelForCausalLM.from_pretrained(\n+            model_id,\n+            torch_dtype=torch.bfloat16,\n+            attn_implementation=\"flash_attention_2\",\n+        ).to(\"cuda\")\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+        # 2. Load FA3 model\n+        try:\n+            model_3 = AutoModelForCausalLM.from_pretrained(\n+                model_id,\n+                torch_dtype=torch.bfloat16,\n+                attn_implementation=\"flash_attention_3\",\n+            ).to(\"cuda\")\n+        except (ValueError, ImportError) as e:\n+            pytest.skip(f\"Could not load Flash Attention 3 model, skipping test. Error: {e}\")\n+\n+        # 3. Generate with both models\n+        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n+\n+        with torch.no_grad():\n+            output_2 = model_2.generate(\n+                **inputs, max_new_tokens=20, do_sample=False, output_scores=True, return_dict_in_generate=True\n+            )\n+            output_3 = model_3.generate(\n+                **inputs, max_new_tokens=20, do_sample=False, output_scores=True, return_dict_in_generate=True\n+            )\n+\n+        # 4. Correctness check\n+        # 4a. Logits\n+        logits_2 = torch.stack(output_2.scores)\n+        logits_3 = torch.stack(output_3.scores)\n+        torch.testing.assert_close(logits_2, logits_3, atol=1e-3, rtol=1e-3)\n+        logprobs_2 = torch.nn.functional.log_softmax(logits_2, dim=-1)\n+        logprobs_3 = torch.nn.functional.log_softmax(logits_3, dim=-1)\n+        max_logprob_diff = torch.max(torch.abs(logprobs_2 - logprobs_3)).item()\n+\n+        # 4b. Generated text\n+        text_2 = tokenizer.decode(output_2.sequences[0], skip_special_tokens=True)\n+        text_3 = tokenizer.decode(output_3.sequences[0], skip_special_tokens=True)\n+        rouge_score = self._calculate_rouge_l([text_2], [text_3])[0]\n+        assert rouge_score > 0.99, f\"Generated texts do not match (ROUGE-L: {rouge_score})\"\n+\n+        # 5. Performance check\n+        with torch.no_grad():\n+            time_2 = self._benchmark_generation(model_2, inputs)\n+            time_3 = self._benchmark_generation(model_3, inputs)\n+\n+        print(f\"\\n--- Flash Attention {2, 3} Parity Test on {model_id} ---\")\n+        print(f\"Prompt: '{prompt}'\")\n+        print(f\"Generated text with Flash Attention 2: {text_2}\")\n+        print(f\"Generated text with Flash Attention 3: {text_3}\")\n+        print(f\"ROUGE-L: {rouge_score}\")\n+        print(f\"Max absolute difference in logprobs: {max_logprob_diff:.5e}\")\n+        print(f\"Flash Attention 2 latency: {time_2:.2f} ms\")\n+        print(f\"Flash Attention 3 latency: {time_3:.2f} ms\")\n+        print(f\"Speed-up: {time_2 / time_3:.2f}x\")\n+        print(\"---\")"
        },
        {
            "sha": "840d2e66e753f663da2caa5993fd82ed8fc1d0f4",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -34,6 +34,7 @@\n     is_flaky,\n     require_accelerate,\n     require_flash_attn,\n+    require_flash_attn_3,\n     require_optimum_quanto,\n     require_read_token,\n     require_torch,\n@@ -2292,6 +2293,7 @@ def _test_attention_implementation(self, attn_implementation):\n         support_flag = {\n             \"sdpa\": \"_supports_sdpa\",\n             \"flash_attention_2\": \"_supports_flash_attn_2\",\n+            \"flash_attention_3\": \"_supports_flash_attn_3\",\n         }\n \n         for model_class in self.all_generative_model_classes:\n@@ -2369,6 +2371,14 @@ def test_eager_matches_fa2_generate(self):\n         \"\"\"Tests that generate has equivalent outputs with FA2 and eager attention implementations.\"\"\"\n         self._test_attention_implementation(\"flash_attention_2\")\n \n+    @pytest.mark.flash_attn_3_test\n+    @require_flash_attn_3\n+    @require_torch_gpu\n+    @slow\n+    def test_eager_matches_fa3_generate(self):\n+        \"\"\"Tests that generate has equivalent outputs with FA3 and eager attention implementations.\"\"\"\n+        self._test_attention_implementation(\"flash_attention_3\")\n+\n     def _check_generate_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n         input_batch_size = int(output.sequences.shape[0] / num_return_sequences)\n         internal_batch_size = ("
        },
        {
            "sha": "a5d9c90068090886506c0674857edcc392d506e5",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 192,
            "deletions": 235,
            "changes": 427,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -84,6 +84,7 @@\n     require_bitsandbytes,\n     require_deepspeed,\n     require_flash_attn,\n+    require_flash_attn_3,\n     require_non_hpu,\n     require_safetensors,\n     require_torch,\n@@ -3129,26 +3130,27 @@ def test_model_is_small(self):\n                 f\"{model_class} is too big for the common tests ({num_params})! It should have 1M max.\"\n             )\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    @is_flaky()\n-    def test_flash_attn_2_inference_equivalence(self):\n+    def flash_attn_inference_equivalence(self, attn_implementation: str, padding_side: str):\n+        r\"\"\"\n+        Tests the equivalence between the eager and flash attention implementations.\n+        This test is only for inference and runs with `torch_dtype=torch.bfloat16`.\n+        \"\"\"\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n+            if (attn_implementation == \"flash_attention_2\" and not model_class._supports_flash_attn_2) or (\n+                attn_implementation == \"flash_attention_3\" and not model_class._supports_flash_attn_3\n+            ):\n+                self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             model = model_class(config)\n \n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n                 model_fa = model_class.from_pretrained(\n-                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=attn_implementation\n                 )\n                 model_fa.to(torch_device)\n \n@@ -3163,9 +3165,12 @@ def test_flash_attn_2_inference_equivalence(self):\n \n                 if dummy_attention_mask is not None:\n                     dummy_attention_mask = dummy_attention_mask[:1]\n-                    dummy_attention_mask[:, 1:] = 1\n-                    dummy_attention_mask[:, :1] = 0\n-\n+                    if padding_side == \"left\":\n+                        dummy_attention_mask[:, 1:] = 1\n+                        dummy_attention_mask[:, :1] = 0\n+                    else:\n+                        dummy_attention_mask[:, :-1] = 1\n+                        dummy_attention_mask[:, -1:] = 0\n                 if model.config.is_encoder_decoder:\n                     decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)[:1]\n \n@@ -3220,104 +3225,46 @@ def test_flash_attn_2_inference_equivalence(self):\n                     else outputs_fa.decoder_hidden_states[-1]\n                 )\n \n-                assert torch.allclose(logits_fa[1:], logits[1:], atol=4e-2, rtol=4e-2)\n+                if padding_side == \"left\":\n+                    assert torch.allclose(logits_fa[1:], logits[1:], atol=4e-2, rtol=4e-2)\n \n-                # check with inference + dropout\n-                model.train()\n-                _ = model_fa(dummy_input, **other_inputs)\n+                    # check with inference + dropout\n+                    model.train()\n+                    _ = model_fa(dummy_input, **other_inputs)\n+                else:\n+                    assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n \n     @require_flash_attn\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n     @is_flaky()\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.bfloat16)\n-                model.to(torch_device)\n-\n-                dummy_input = inputs_dict[model.main_input_name][:1]\n-                if dummy_input.dtype in [torch.float32, torch.float16]:\n-                    dummy_input = dummy_input.to(torch.bfloat16)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n-\n-                if dummy_attention_mask is not None:\n-                    dummy_attention_mask = dummy_attention_mask[:1]\n-                    dummy_attention_mask[:, :-1] = 1\n-                    dummy_attention_mask[:, -1:] = 0\n-\n-                if model.config.is_encoder_decoder:\n-                    decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)[:1]\n-\n-                    outputs = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-                    outputs_fa = model_fa(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-                else:\n-                    outputs = model(dummy_input, output_hidden_states=True)\n-                    outputs_fa = model_fa(dummy_input, output_hidden_states=True)\n-\n-                logits = (\n-                    outputs.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs.decoder_hidden_states[-1]\n-                )\n-                logits_fa = (\n-                    outputs_fa.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs_fa.decoder_hidden_states[-1]\n-                )\n-\n-                assert torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2)\n-\n-                if model.config.is_encoder_decoder:\n-                    other_inputs = {\n-                        \"decoder_input_ids\": decoder_input_ids,\n-                        \"decoder_attention_mask\": dummy_attention_mask,\n-                        \"output_hidden_states\": True,\n-                    }\n-                    if dummy_attention_mask is not None:\n-                        other_inputs[\"attention_mask\"] = dummy_attention_mask\n-\n-                    outputs = model(dummy_input, **other_inputs)\n-                    outputs_fa = model_fa(dummy_input, **other_inputs)\n-                else:\n-                    other_inputs = {\n-                        \"output_hidden_states\": True,\n-                    }\n-                    if dummy_attention_mask is not None:\n-                        other_inputs[\"attention_mask\"] = dummy_attention_mask\n+    def test_flash_attn_2_inference_equivalence(self):\n+        self.flash_attn_inference_equivalence(attn_implementation=\"flash_attention_2\", padding_side=\"left\")\n \n-                    outputs = model(dummy_input, **other_inputs)\n-                    outputs_fa = model_fa(dummy_input, **other_inputs)\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    @is_flaky()\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        self.flash_attn_inference_equivalence(attn_implementation=\"flash_attention_2\", padding_side=\"right\")\n \n-                logits = (\n-                    outputs.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs.decoder_hidden_states[-1]\n-                )\n-                logits_fa = (\n-                    outputs_fa.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs_fa.decoder_hidden_states[-1]\n-                )\n+    @require_flash_attn_3\n+    @require_torch_gpu\n+    @mark.flash_attn_3_test\n+    @slow\n+    @is_flaky()\n+    def test_flash_attn_3_inference_equivalence(self):\n+        self.flash_attn_inference_equivalence(attn_implementation=\"flash_attention_3\", padding_side=\"left\")\n \n-                assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n+    @require_flash_attn_3\n+    @require_torch_gpu\n+    @mark.flash_attn_3_test\n+    @slow\n+    @is_flaky()\n+    def test_flash_attn_3_inference_equivalence_right_padding(self):\n+        self.flash_attn_inference_equivalence(attn_implementation=\"flash_attention_3\", padding_side=\"right\")\n \n     def test_attn_implementation_composite_models(self):\n         \"\"\"\n@@ -3959,24 +3906,21 @@ def test_sdpa_matches_eager_sliding_window(self):\n                     torch.allclose(res_eager[attention_mask == 1], res_sdpa[attention_mask == 1], rtol=1e-4, atol=1e-4)\n                 )\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    def test_flash_attn_2_can_dispatch_composite_models(self):\n+    def flash_attn_can_dispatch_composite_models(self, attn_implementation: str):\n         \"\"\"\n-        Tests if composite models can dispatch on FA2 if the sub-models support FA2.\n+        Tests if composite models can dispatch on flash attention if the sub-models support it.\n         The tests is needed as we handle differently composite models and we cannot check them\n-        with above tests. If any of the sub-models does not support FA2, we'll raise an error when dispatching\n+        with above tests. If any of the sub-models does not support flash attention, we'll raise an error when dispatching\n         that particular sub-model. Otherwise we dispatch safely in all sub-models, where \"sub-models\" are specific\n         backbone models (LM/vision/audio/etc)\n         \"\"\"\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n-        if not is_torch_fp16_available_on_device(torch_device):\n-            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n+        if not is_torch_bf16_available_on_device(torch_device):\n+            self.skipTest(f\"bfloat16 not supported on {torch_device} (on the specific device currently used)\")\n \n-        torch_dtype = torch.float16\n+        torch_dtype = torch.bfloat16\n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             model = model_class(config)\n@@ -3987,44 +3931,64 @@ def test_flash_attn_2_can_dispatch_composite_models(self):\n                 model.save_pretrained(tmpdirname)\n                 model = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n \n-                sub_models_supporting_fa2 = [\n-                    module._supports_flash_attn_2\n+                sub_models_supporting_fa = [\n+                    (\n+                        module._supports_flash_attn_3\n+                        if attn_implementation == \"flash_attention_3\"\n+                        else module._supports_flash_attn_2\n+                    )\n                     for name, module in model.named_modules()\n                     if isinstance(module, PreTrainedModel) and name != \"\"\n                 ]\n-                supports_fa2_all_modules = (\n-                    all(sub_models_supporting_fa2)\n-                    if len(sub_models_supporting_fa2) > 0\n-                    else model._supports_flash_attn_2\n+                supports_fa_all_modules = (\n+                    all(sub_models_supporting_fa)\n+                    if len(sub_models_supporting_fa) > 0\n+                    else (\n+                        model._supports_flash_attn_3\n+                        if attn_implementation == \"flash_attention_3\"\n+                        else model._supports_flash_attn_2\n+                    )\n                 )\n-                if not supports_fa2_all_modules:\n+                if not supports_fa_all_modules:\n                     with self.assertRaises(ValueError):\n-                        model_fa2 = model_class.from_pretrained(\n+                        model_fa = model_class.from_pretrained(\n                             tmpdirname,\n                             torch_dtype=torch_dtype,\n-                            attn_implementation=\"flash_attention_2\",\n+                            attn_implementation=attn_implementation,\n                         )\n                 else:\n-                    model_fa2 = model_class.from_pretrained(\n-                        tmpdirname, torch_dtype=torch_dtype, attn_implementation=\"flash_attention_2\"\n+                    model_fa = model_class.from_pretrained(\n+                        tmpdirname, torch_dtype=torch_dtype, attn_implementation=attn_implementation\n                     )\n-                    for key in model_fa2.config:\n-                        if isinstance(getattr(model_fa2.config, key), PretrainedConfig):\n-                            sub_config = getattr(model_fa2.config, key)\n-                            self.assertTrue(sub_config._attn_implementation == \"flash_attention_2\")\n+                    for key in model_fa.config:\n+                        if isinstance(getattr(model_fa.config, key), PretrainedConfig):\n+                            sub_config = getattr(model_fa.config, key)\n+                            self.assertTrue(sub_config._attn_implementation == attn_implementation)\n \n-                    has_fa2 = False\n-                    for name, submodule in model_fa2.named_modules():\n+                    has_fa = False\n+                    for name, submodule in model_fa.named_modules():\n                         class_name = submodule.__class__.__name__\n                         if (\n                             \"Attention\" in class_name\n                             and getattr(submodule, \"config\", None)\n-                            and submodule.config._attn_implementation == \"flash_attention_2\"\n+                            and submodule.config._attn_implementation == attn_implementation\n                         ):\n-                            has_fa2 = True\n+                            has_fa = True\n                             break\n-                    if not has_fa2:\n-                        raise ValueError(\"The FA2 model should have FA2 layers\")\n+                    if not has_fa:\n+                        raise ValueError(f\"The {attn_implementation} model should have {attn_implementation} layers\")\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    def test_flash_attn_2_can_dispatch_composite_models(self):\n+        self.flash_attn_can_dispatch_composite_models(attn_implementation=\"flash_attention_2\")\n+\n+    @require_flash_attn_3\n+    @require_torch_gpu\n+    @mark.flash_attn_3_test\n+    def test_flash_attn_3_can_dispatch_composite_models(self):\n+        self.flash_attn_can_dispatch_composite_models(attn_implementation=\"flash_attention_3\")\n \n     @require_flash_attn\n     @require_torch_gpu\n@@ -4121,27 +4085,29 @@ def test_flash_attn_2_can_compile_with_attention_mask_None_without_graph_break(s\n \n         assert not loss.isnan().any()\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+    def flash_attention_padding_matches_padding_free_with_position_ids(\n+        self, attn_implementation: str, fa_kwargs: bool = False\n+    ):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n         max_new_tokens = 30\n \n         for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n+            if not (\n+                model_class._supports_flash_attn_2\n+                if attn_implementation == \"flash_attention_2\"\n+                else model_class._supports_flash_attn_3\n+            ):\n+                self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             if 0 not in inputs_dict.get(\"attention_mask\", []) or \"attention_mask\" not in inputs_dict:\n                 self.skipTest(\"Model dummy inputs should contain padding in their attention mask\")\n \n             dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n+            if dummy_input.dtype in [torch.float32, torch.float16]:\n+                dummy_input = dummy_input.to(torch.bfloat16)\n \n             # make sure that all models have enough positions for generation\n             if hasattr(config, \"max_position_embeddings\"):\n@@ -4151,7 +4117,7 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n             if \"position_ids\" not in inspect.signature(model.forward).parameters:\n                 self.skipTest(\"Model does not support position_ids\")\n \n-            if \"position_ids\" not in inspect.signature(model.forward).parameters:\n+            if (not fa_kwargs) and \"position_ids\" not in inspect.signature(model.forward).parameters:\n                 continue  # this model doesn't accept position ids as input\n \n             with tempfile.TemporaryDirectory() as tmpdirname:\n@@ -4166,26 +4132,40 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n                 model = (\n                     model_class.from_pretrained(\n                         tmpdirname,\n-                        torch_dtype=torch.float16,\n-                        attn_implementation=\"flash_attention_2\",\n+                        torch_dtype=torch.bfloat16,\n+                        attn_implementation=attn_implementation,\n                     )\n                     .to(torch_device)\n                     .eval()\n                 )\n \n-                # flatten\n-                padfree_inputs_dict = {\n-                    k: v[dummy_attention_mask.bool()].unsqueeze(0)\n-                    for k, v in inputs_dict.items()\n-                    if not k == \"attention_mask\"\n-                }\n-                # add position_ids\n-                padfree_inputs_dict[\"position_ids\"] = (\n-                    torch.cat([torch.arange(length) for length in dummy_attention_mask.sum(1).tolist()])\n-                    .long()\n-                    .unsqueeze(0)\n-                    .to(torch_device)\n-                )\n+                if fa_kwargs:\n+                    # flatten\n+                    features = [\n+                        {\"input_ids\": i[a.bool()].tolist()}\n+                        for i, a in zip(inputs_dict[\"input_ids\"], inputs_dict[\"attention_mask\"])\n+                    ]\n+\n+                    # add position_ids + fa_kwargs\n+                    data_collator = DataCollatorWithFlattening(return_tensors=\"pt\", return_flash_attn_kwargs=True)\n+                    batch = data_collator(features)\n+                    padfree_inputs_dict = {\n+                        k: t.to(torch_device) if torch.is_tensor(t) else t for k, t in batch.items()\n+                    }\n+                else:\n+                    # flatten\n+                    padfree_inputs_dict = {\n+                        k: v[dummy_attention_mask.bool()].unsqueeze(0)\n+                        for k, v in inputs_dict.items()\n+                        if not k == \"attention_mask\"\n+                    }\n+                    # add position_ids\n+                    padfree_inputs_dict[\"position_ids\"] = (\n+                        torch.cat([torch.arange(length) for length in dummy_attention_mask.sum(1).tolist()])\n+                        .long()\n+                        .unsqueeze(0)\n+                        .to(torch_device)\n+                    )\n \n                 res_padded = model(**inputs_dict)\n                 res_padfree = model(**padfree_inputs_dict)\n@@ -4195,119 +4175,96 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n \n                 torch.testing.assert_close(logits_padded.argmax(-1), logits_padfree.argmax(-1), rtol=0, atol=0)\n                 # acceptable numerical instability\n-                tol = torch.finfo(torch.float16).eps\n+                tol = torch.finfo(torch.bfloat16).eps\n                 torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n \n     @require_flash_attn\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n-    def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        max_new_tokens = 30\n-\n-        for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            if 0 not in inputs_dict.get(\"attention_mask\", []) or \"attention_mask\" not in inputs_dict:\n-                self.skipTest(\"Model dummy inputs should contain padding in their attention mask\")\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-            if \"position_ids\" not in inspect.signature(model.forward).parameters:\n-                self.skipTest(\"Model does not support position_ids\")\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                # ensure left padding, to adapt for some models\n-                if 0 in inputs_dict[\"attention_mask\"][:, -1]:\n-                    inputs_dict[\"attention_mask\"] = inputs_dict[\"attention_mask\"].flip(1)\n-                dummy_attention_mask = inputs_dict[\"attention_mask\"]\n-                inputs_dict[\"input_ids\"][~dummy_attention_mask.bool()] = config.get_text_config().pad_token_id\n-\n-                model = (\n-                    model_class.from_pretrained(\n-                        tmpdirname,\n-                        torch_dtype=torch.float16,\n-                        attn_implementation=\"flash_attention_2\",\n-                    )\n-                    .to(torch_device)\n-                    .eval()\n-                )\n-\n-                # flatten\n-                features = [\n-                    {\"input_ids\": i[a.bool()].tolist()}\n-                    for i, a in zip(inputs_dict[\"input_ids\"], inputs_dict[\"attention_mask\"])\n-                ]\n-\n-                # add position_ids + fa_kwargs\n-                data_collator = DataCollatorWithFlattening(return_tensors=\"pt\", return_flash_attn_kwargs=True)\n-                batch = data_collator(features)\n-                batch_accelerator = {k: t.to(torch_device) if torch.is_tensor(t) else t for k, t in batch.items()}\n-\n-                res_padded = model(**inputs_dict)\n-                res_padfree = model(**batch_accelerator)\n-\n-                logits_padded = res_padded.logits[inputs_dict[\"attention_mask\"].bool()]\n-                logits_padfree = res_padfree.logits[0]\n-\n-                torch.testing.assert_close(logits_padded.argmax(-1), logits_padfree.argmax(-1), rtol=0, atol=0)\n-                # acceptable numerical instability\n-                tol = torch.finfo(torch.float16).eps\n-                torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        self.flash_attention_padding_matches_padding_free_with_position_ids(attn_implementation=\"flash_attention_2\")\n \n     @require_flash_attn\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n-    def test_flash_attn_2_from_config(self):\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n+        self.flash_attention_padding_matches_padding_free_with_position_ids(\n+            attn_implementation=\"flash_attention_2\", fa_kwargs=True\n+        )\n+\n+    @require_flash_attn_3\n+    @require_torch_gpu\n+    @mark.flash_attn_3_test\n+    @slow\n+    def test_flash_attention_3_padding_matches_padding_free_with_position_ids(self):\n+        self.flash_attention_padding_matches_padding_free_with_position_ids(attn_implementation=\"flash_attention_3\")\n+\n+    @require_flash_attn_3\n+    @require_torch_gpu\n+    @mark.flash_attn_3_test\n+    @slow\n+    def test_flash_attention_3_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n+        self.flash_attention_padding_matches_padding_free_with_position_ids(\n+            attn_implementation=\"flash_attention_3\", fa_kwargs=True\n+        )\n+\n+    def flash_attn_from_config(self, attn_implementation: str):\n+        r\"\"\"\n+        Tests if the model can be loaded with `attn_implementation` from the config and if the\n+        weights are not randomly initialized.\n+        \"\"\"\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n         for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n+            if (attn_implementation == \"flash_attention_2\" and not model_class._supports_flash_attn_2) or (\n+                attn_implementation == \"flash_attention_3\" and not model_class._supports_flash_attn_3\n+            ):\n+                self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             # TODO: to change it in the future with other relevant auto classes\n-            fa2_model = model_class._from_config(\n-                config, attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16\n+            fa_model = model_class._from_config(\n+                config, attn_implementation=attn_implementation, torch_dtype=torch.bfloat16\n             ).to(torch_device)\n \n-            dummy_input = inputs_dict[fa2_model.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n+            dummy_input = inputs_dict[fa_model.main_input_name]\n+            if dummy_input.dtype in [torch.float32, torch.float16]:\n+                dummy_input = dummy_input.to(torch.bfloat16)\n             dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n \n-            if fa2_model.config.is_encoder_decoder:\n+            if fa_model.config.is_encoder_decoder:\n                 dummy_decoder_input_ids = inputs_dict[\"decoder_input_ids\"]\n                 dummy_decoder_attention_mask = inputs_dict[\"decoder_attention_mask\"]\n-                _ = fa2_model(\n+                _ = fa_model(\n                     dummy_input,\n                     attention_mask=dummy_attention_mask,\n                     decoder_input_ids=dummy_decoder_input_ids,\n                     decoder_attention_mask=dummy_decoder_attention_mask,\n                 )\n             else:\n-                _ = fa2_model(dummy_input, attention_mask=dummy_attention_mask)\n+                _ = fa_model(dummy_input, attention_mask=dummy_attention_mask)\n \n             with tempfile.TemporaryDirectory() as tmpdirname:\n-                fa2_model.save_pretrained(tmpdirname)\n+                fa_model.save_pretrained(tmpdirname)\n                 model_from_pretrained = model_class.from_pretrained(tmpdirname)\n-                self.assertTrue(model_from_pretrained.config._attn_implementation != \"flash_attention_2\")\n+                self.assertTrue(model_from_pretrained.config._attn_implementation != attn_implementation)\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    def test_flash_attn_2_from_config(self):\n+        self.flash_attn_from_config(attn_implementation=\"flash_attention_2\")\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_3_test\n+    @slow\n+    def test_flash_attn_3_from_config(self):\n+        self.flash_attn_from_config(attn_implementation=\"flash_attention_3\")\n \n     def _get_custom_4d_mask_test_data(self):\n         # Sequence in which all but the last token is the same"
        },
        {
            "sha": "7df23e02959150ca767893fd795a6a9fdae23b94",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2eb75c891f6866cc9aeb66896be59f6c4ce100e/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=a2eb75c891f6866cc9aeb66896be59f6c4ce100e",
            "patch": "@@ -77,6 +77,7 @@\n )\n from transformers.utils.import_utils import (\n     is_flash_attn_2_available,\n+    is_flash_attn_3_available,\n     is_flax_available,\n     is_tf_available,\n     is_torch_npu_available,\n@@ -676,6 +677,9 @@ def test_model_from_pretrained_attn_implementation(self):\n         if is_flash_attn_available():\n             attn_implementation_available.append(\"flash_attention_2\")\n \n+        if is_flash_attn_3_available():\n+            attn_implementation_available.append(\"flash_attention_3\")\n+\n         for requested_attn_implementation in attn_implementation_available:\n             model = AutoModelForCausalLM.from_pretrained(\n                 TINY_MISTRAL, attn_implementation=requested_attn_implementation\n@@ -700,6 +704,9 @@ def test_model_from_config_attn_implementation(self):\n         if is_flash_attn_available():\n             attn_implementation_available.append(\"flash_attention_2\")\n \n+        if is_flash_attn_3_available():\n+            attn_implementation_available.append(\"flash_attention_3\")\n+\n         for requested_attn_implementation in attn_implementation_available:\n             config = AutoConfig.from_pretrained(TINY_MISTRAL, attn_implementation=requested_attn_implementation)\n             # Ensure the config was set correctly"
        }
    ],
    "stats": {
        "total": 958,
        "additions": 697,
        "deletions": 261
    }
}