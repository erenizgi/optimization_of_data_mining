{
    "author": "yonigozlan",
    "message": "Add OmDet-Turbo (#31843)\n\n* Add template with add-new-model-like\r\n\r\n* Add rough OmDetTurboEncoder and OmDetTurboDecoder\r\n\r\n* Add working OmDetTurbo convert to hf\r\n\r\n* Change OmDetTurbo encoder to RT-DETR encoder\r\n\r\n* Add swin timm backbone as default, add always partition fix for swin timm\r\n\r\n* Add labels and tasks caching\r\n\r\n* Fix make fix-copies\r\n\r\n* Format omdet_turbo\r\n\r\n* fix Tokenizer tests\r\n\r\n* Fix style and quality\r\n\r\n* Reformat omdet_turbo\r\n\r\n* Fix quality, style, copies\r\n\r\n* Standardize processor kwargs\r\n\r\n* Fix style\r\n\r\n* Add output_hidden_states and ouput_attentions\r\n\r\n* Add personalize multi-head attention, improve docstrings\r\n\r\n* Add integrated test and fix copy, style, quality\r\n\r\n* Fix unprotected import\r\n\r\n* Cleanup comments and fix unprotected imports\r\n\r\n* Add fix different prompts in batch (key_padding_mask)\r\n\r\n* Add key_padding_mask to custom multi-head attention module\r\n\r\n* Replace attention_mask by key_padding_mask\r\n\r\n* Remove OmDetTurboModel and refactor\r\n\r\n* Refactor processing of classes and abstract use of timm backbone\r\n\r\n* Add testing, fix output attentions and hidden states, add cache for anchors generation\r\n\r\n* Fix copies, style, quality\r\n\r\n* Add documentation, conver key_padding_mask to attention_mask\r\n\r\n* revert changes to backbone_utils\r\n\r\n* Fic docstrings rst\r\n\r\n* Fix unused argument in config\r\n\r\n* Fix image link documentation\r\n\r\n* Reorder config and cleanup\r\n\r\n* Add tokenizer_init_kwargs in merge_kwargs of the processor\r\n\r\n* Change AutoTokenizer to CLIPTokenizer in convert\r\n\r\n* Fix init_weights\r\n\r\n* Add ProcessorMixin tests, Fix convert while waiting on uniform kwargs\r\n\r\n* change processor kwargs and make task input optional\r\n\r\n* Fix omdet docs\r\n\r\n* Remove unnecessary tests for processor kwargs\r\n\r\n* Replace nested BatchEncoding output of the processor by a flattened BatchFeature\r\n\r\n* Make modifications from Pavel review\r\n\r\n* Add changes Amy review\r\n\r\n* Remove unused param\r\n\r\n* Remove normalize_before param, Modify processor call docstring\r\n\r\n* Remove redundant decoder class, add gradient checkpointing for decoder\r\n\r\n* Remove commented out code\r\n\r\n* Fix inference in fp16 and add fp16 integrated test\r\n\r\n* update omdet md doc\r\n\r\n* Add OmdetTurboModel\r\n\r\n* fix caching and nit\r\n\r\n* add OmDetTurboModel to tests\r\n\r\n* nit change repeated key test\r\n\r\n* Improve inference speed in eager mode\r\n\r\n* fix copies\r\n\r\n* Fix nit\r\n\r\n* remove OmdetTurboModel\r\n\r\n* [run-slow] omdet_turbo\r\n\r\n* [run-slow] omdet_turbo\r\n\r\n* skip dataparallel test\r\n\r\n* [run-slow] omdet_turbo\r\n\r\n* update weights to new path\r\n\r\n* remove unnecessary config in class\r\n\r\n---------\r\n\r\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-91-248.ec2.internal>",
    "sha": "94f18cf23c128055a984ffbe9c57df133c1f6cc7",
    "files": [
        {
            "sha": "c454c35e5dd3b328a8b988334d14718594fbe88e",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7",
            "patch": "@@ -862,6 +862,8 @@\n         title: MGP-STR\n       - local: model_doc/nougat\n         title: Nougat\n+      - local: model_doc/omdet-turbo\n+        title: OmDet-Turbo\n       - local: model_doc/oneformer\n         title: OneFormer\n       - local: model_doc/owlvit"
        },
        {
            "sha": "41cc901ef878e15fb033a36f9eb8de8f9b844284",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7",
            "patch": "@@ -237,6 +237,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                 [Nyströmformer](model_doc/nystromformer)                 |       ✅        |         ❌         |      ❌      |\n |                          [OLMo](model_doc/olmo)                          |       ✅        |         ❌         |      ❌      |\n |                         [OLMoE](model_doc/olmoe)                         |       ✅        |         ❌         |      ❌      |\n+|                   [OmDet-Turbo](model_doc/omdet-turbo)                   |       ✅        |         ❌         |      ❌      |\n |                     [OneFormer](model_doc/oneformer)                     |       ✅        |         ❌         |      ❌      |\n |                    [OpenAI GPT](model_doc/openai-gpt)                    |       ✅        |         ✅         |      ❌      |\n |                      [OpenAI GPT-2](model_doc/gpt2)                      |       ✅        |         ✅         |      ✅      |"
        },
        {
            "sha": "190ac3e31eeaae61b62b5f669e025fcf26b9c5da",
            "filename": "docs/source/en/model_doc/omdet-turbo.md",
            "status": "added",
            "additions": 164,
            "deletions": 0,
            "changes": 164,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/docs%2Fsource%2Fen%2Fmodel_doc%2Fomdet-turbo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/docs%2Fsource%2Fen%2Fmodel_doc%2Fomdet-turbo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fomdet-turbo.md?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7",
            "patch": "@@ -0,0 +1,164 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# OmDet-Turbo\n+\n+## Overview\n+\n+The OmDet-Turbo model was proposed in [Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head](https://arxiv.org/abs/2403.06892) by Tiancheng Zhao, Peng Liu, Xuan He, Lu Zhang, Kyusong Lee. OmDet-Turbo incorporates components from RT-DETR and introduces a swift multimodal fusion module to achieve real-time open-vocabulary object detection capabilities while maintaining high accuracy. The base model achieves performance of up to 100.2 FPS and 53.4 AP on COCO zero-shot.\n+\n+The abstract from the paper is the following:\n+\n+*End-to-end transformer-based detectors (DETRs) have shown exceptional performance in both closed-set and open-vocabulary object detection (OVD) tasks through the integration of language modalities. However, their demanding computational requirements have hindered their practical application in real-time object detection (OD) scenarios. In this paper, we scrutinize the limitations of two leading models in the OVDEval benchmark, OmDet and Grounding-DINO, and introduce OmDet-Turbo. This novel transformer-based real-time OVD model features an innovative Efficient Fusion Head (EFH) module designed to alleviate the bottlenecks observed in OmDet and Grounding-DINO. Notably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with TensorRT and language cache techniques applied. Notably, in zero-shot scenarios on COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on par with current state-of-the-art supervised models. Furthermore, it establishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an AP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of OmDet-Turbo in industrial applications is underscored by its exceptional performance on benchmark datasets and superior inference speed, positioning it as a compelling choice for real-time object detection tasks.*\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/omdet_turbo_architecture.jpeg\" alt=\"drawing\" width=\"600\"/>\n+\n+<small> OmDet-Turbo architecture overview. Taken from the <a href=\"https://arxiv.org/abs/2403.06892\">original paper</a>. </small>\n+\n+This model was contributed by [yonigozlan](https://huggingface.co/yonigozlan).\n+The original code can be found [here](https://github.com/om-ai-lab/OmDet).\n+\n+## Usage tips\n+\n+One unique property of OmDet-Turbo compared to other zero-shot object detection models, such as [Grounding DINO](grounding-dino), is the decoupled classes and prompt embedding structure that allows caching of text embeddings. This means that the model needs both classes and task as inputs, where classes is a list of objects we want to detect and task is the grounded text used to guide open-vocabulary detection. This approach limits the scope of the open-vocabulary detection and makes the decoding process faster.\n+\n+[`OmDetTurboProcessor`] is used to prepare the classes, task and image triplet. The task input is optional, and when not provided, it will default to `\"Detect [class1], [class2], [class3], ...\"`. To process the results from the model, one can use `post_process_grounded_object_detection` from [`OmDetTurboProcessor`]. Notably, this function takes in the input classes, as unlike other zero-shot object detection models, the decoupling of classes and task embeddings means that no decoding of the predicted class embeddings is needed in the post-processing step, and the predicted classes can be matched to the inputted ones directly.\n+\n+## Usage example\n+\n+### Single image inference\n+\n+Here's how to load the model and prepare the inputs to perform zero-shot object detection on a single image:\n+\n+```python\n+import requests\n+from PIL import Image\n+\n+from transformers import AutoProcessor, OmDetTurboForObjectDetection\n+\n+processor = AutoProcessor.from_pretrained(\"omlab/omdet-turbo-tiny\")\n+model = OmDetTurboForObjectDetection.from_pretrained(\"omlab/omdet-turbo-tiny\")\n+\n+url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+classes = [\"cat\", \"remote\"]\n+inputs = processor(image, text=classes, return_tensors=\"pt\")\n+\n+outputs = model(**inputs)\n+\n+# convert outputs (bounding boxes and class logits)\n+results = processor.post_process_grounded_object_detection(\n+    outputs,\n+    classes=classes,\n+    target_sizes=[image.size[::-1]],\n+    score_threshold=0.3,\n+    nms_threshold=0.3,\n+)[0]\n+for score, class_name, box in zip(\n+    results[\"scores\"], results[\"classes\"], results[\"boxes\"]\n+):\n+    box = [round(i, 1) for i in box.tolist()]\n+    print(\n+        f\"Detected {class_name} with confidence \"\n+        f\"{round(score.item(), 2)} at location {box}\"\n+    )\n+```\n+\n+### Multi image inference\n+\n+OmDet-Turbo can perform batched multi-image inference, with support for different text prompts and classes in the same batch:\n+\n+```python\n+>>> import torch\n+>>> import requests\n+>>> from io import BytesIO\n+>>> from PIL import Image\n+>>> from transformers import AutoProcessor, OmDetTurboForObjectDetection\n+\n+>>> processor = AutoProcessor.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\")\n+>>> model = OmDetTurboForObjectDetection.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\")\n+\n+>>> url1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+>>> image1 = Image.open(BytesIO(requests.get(url1).content)).convert(\"RGB\")\n+>>> classes1 = [\"cat\", \"remote\"]\n+>>> task1 = \"Detect {}.\".format(\", \".join(classes1))\n+\n+>>> url2 = \"http://images.cocodataset.org/train2017/000000257813.jpg\"\n+>>> image2 = Image.open(BytesIO(requests.get(url2).content)).convert(\"RGB\")\n+>>> classes2 = [\"boat\"]\n+>>> task2 = \"Detect everything that looks like a boat.\"\n+\n+>>> url3 = \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n+>>> image3 = Image.open(BytesIO(requests.get(url3).content)).convert(\"RGB\")\n+>>> classes3 = [\"statue\", \"trees\"]\n+>>> task3 = \"Focus on the foreground, detect statue and trees.\"\n+\n+>>> inputs = processor(\n+...     images=[image1, image2, image3],\n+...     text=[classes1, classes2, classes3],\n+...     task=[task1, task2, task3],\n+...     return_tensors=\"pt\",\n+... )\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> # convert outputs (bounding boxes and class logits)\n+>>> results = processor.post_process_grounded_object_detection(\n+...     outputs,\n+...     classes=[classes1, classes2, classes3],\n+...     target_sizes=[image1.size[::-1], image2.size[::-1], image3.size[::-1]],\n+...     score_threshold=0.2,\n+...     nms_threshold=0.3,\n+... )\n+\n+>>> for i, result in enumerate(results):\n+...     for score, class_name, box in zip(\n+...         result[\"scores\"], result[\"classes\"], result[\"boxes\"]\n+...     ):\n+...         box = [round(i, 1) for i in box.tolist()]\n+...         print(\n+...             f\"Detected {class_name} with confidence \"\n+...             f\"{round(score.item(), 2)} at location {box} in image {i}\"\n+...         )\n+Detected remote with confidence 0.77 at location [39.9, 70.4, 176.7, 118.0] in image 0\n+Detected cat with confidence 0.72 at location [11.6, 54.2, 314.8, 474.0] in image 0\n+Detected remote with confidence 0.56 at location [333.4, 75.8, 370.7, 187.0] in image 0\n+Detected cat with confidence 0.55 at location [345.2, 24.0, 639.8, 371.7] in image 0\n+Detected boat with confidence 0.32 at location [146.9, 219.8, 209.6, 250.7] in image 1\n+Detected boat with confidence 0.3 at location [319.1, 223.2, 403.2, 238.4] in image 1\n+Detected boat with confidence 0.27 at location [37.7, 220.3, 84.0, 235.9] in image 1\n+Detected boat with confidence 0.22 at location [407.9, 207.0, 441.7, 220.2] in image 1\n+Detected statue with confidence 0.73 at location [544.7, 210.2, 651.9, 502.8] in image 2\n+Detected trees with confidence 0.25 at location [3.9, 584.3, 391.4, 785.6] in image 2\n+Detected trees with confidence 0.25 at location [1.4, 621.2, 118.2, 787.8] in image 2\n+Detected statue with confidence 0.2 at location [428.1, 205.5, 767.3, 759.5] in image 2\n+\n+```\n+\n+## OmDetTurboConfig\n+\n+[[autodoc]] OmDetTurboConfig\n+\n+## OmDetTurboProcessor\n+\n+[[autodoc]] OmDetTurboProcessor\n+    - post_process_grounded_object_detection\n+\n+## OmDetTurboForObjectDetection\n+\n+[[autodoc]] OmDetTurboForObjectDetection\n+    - forward"
        },
        {
            "sha": "1b281075eea9cf86a7cec2ee49534184e5dce497",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7",
            "patch": "@@ -609,6 +609,10 @@\n     \"models.nystromformer\": [\"NystromformerConfig\"],\n     \"models.olmo\": [\"OlmoConfig\"],\n     \"models.olmoe\": [\"OlmoeConfig\"],\n+    \"models.omdet_turbo\": [\n+        \"OmDetTurboConfig\",\n+        \"OmDetTurboProcessor\",\n+    ],\n     \"models.oneformer\": [\n         \"OneFormerConfig\",\n         \"OneFormerProcessor\",\n@@ -2861,6 +2865,12 @@\n             \"OlmoePreTrainedModel\",\n         ]\n     )\n+    _import_structure[\"models.omdet_turbo\"].extend(\n+        [\n+            \"OmDetTurboForObjectDetection\",\n+            \"OmDetTurboPreTrainedModel\",\n+        ]\n+    )\n     _import_structure[\"models.oneformer\"].extend(\n         [\n             \"OneFormerForUniversalSegmentation\",\n@@ -5407,6 +5417,10 @@\n     )\n     from .models.olmo import OlmoConfig\n     from .models.olmoe import OlmoeConfig\n+    from .models.omdet_turbo import (\n+        OmDetTurboConfig,\n+        OmDetTurboProcessor,\n+    )\n     from .models.oneformer import (\n         OneFormerConfig,\n         OneFormerProcessor,\n@@ -7383,6 +7397,10 @@\n             OlmoeModel,\n             OlmoePreTrainedModel,\n         )\n+        from .models.omdet_turbo import (\n+            OmDetTurboForObjectDetection,\n+            OmDetTurboPreTrainedModel,\n+        )\n         from .models.oneformer import (\n             OneFormerForUniversalSegmentation,\n             OneFormerModel,"
        },
        {
            "sha": "5bbaa45c0f9fd37d010a098eb76b193985a326c3",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7",
            "patch": "@@ -173,6 +173,7 @@\n     nystromformer,\n     olmo,\n     olmoe,\n+    omdet_turbo,\n     oneformer,\n     openai,\n     opt,"
        },
        {
            "sha": "0c648b6f3df39404293301276b370d3971afe8c0",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7",
            "patch": "@@ -60,6 +60,7 @@\n         (\"chinese_clip_vision_model\", \"ChineseCLIPVisionConfig\"),\n         (\"clap\", \"ClapConfig\"),\n         (\"clip\", \"CLIPConfig\"),\n+        (\"clip_text_model\", \"CLIPTextConfig\"),\n         (\"clip_vision_model\", \"CLIPVisionConfig\"),\n         (\"clipseg\", \"CLIPSegConfig\"),\n         (\"clvp\", \"ClvpConfig\"),\n@@ -191,6 +192,7 @@\n         (\"nystromformer\", \"NystromformerConfig\"),\n         (\"olmo\", \"OlmoConfig\"),\n         (\"olmoe\", \"OlmoeConfig\"),\n+        (\"omdet-turbo\", \"OmDetTurboConfig\"),\n         (\"oneformer\", \"OneFormerConfig\"),\n         (\"open-llama\", \"OpenLlamaConfig\"),\n         (\"openai-gpt\", \"OpenAIGPTConfig\"),\n@@ -348,6 +350,7 @@\n         (\"chinese_clip_vision_model\", \"ChineseCLIPVisionModel\"),\n         (\"clap\", \"CLAP\"),\n         (\"clip\", \"CLIP\"),\n+        (\"clip_text_model\", \"CLIPTextModel\"),\n         (\"clip_vision_model\", \"CLIPVisionModel\"),\n         (\"clipseg\", \"CLIPSeg\"),\n         (\"clvp\", \"CLVP\"),\n@@ -497,6 +500,7 @@\n         (\"nystromformer\", \"Nyströmformer\"),\n         (\"olmo\", \"OLMo\"),\n         (\"olmoe\", \"OLMoE\"),\n+        (\"omdet-turbo\", \"OmDet-Turbo\"),\n         (\"oneformer\", \"OneFormer\"),\n         (\"open-llama\", \"OpenLlama\"),\n         (\"openai-gpt\", \"OpenAI GPT\"),\n@@ -665,6 +669,7 @@\n         (\"xclip\", \"x_clip\"),\n         (\"clip_vision_model\", \"clip\"),\n         (\"qwen2_audio_encoder\", \"qwen2_audio\"),\n+        (\"clip_text_model\", \"clip\"),\n         (\"siglip_vision_model\", \"siglip\"),\n         (\"chinese_clip_vision_model\", \"chinese_clip\"),\n         (\"rt_detr_resnet\", \"rt_detr\"),"
        },
        {
            "sha": "54a572030283de6e33dc27940c0c555e36c002bc",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7",
            "patch": "@@ -60,6 +60,7 @@\n         (\"chinese_clip_vision_model\", \"ChineseCLIPVisionModel\"),\n         (\"clap\", \"ClapModel\"),\n         (\"clip\", \"CLIPModel\"),\n+        (\"clip_text_model\", \"CLIPTextModel\"),\n         (\"clip_vision_model\", \"CLIPVisionModel\"),\n         (\"clipseg\", \"CLIPSegModel\"),\n         (\"clvp\", \"ClvpModelForConditionalGeneration\"),\n@@ -181,6 +182,7 @@\n         (\"nystromformer\", \"NystromformerModel\"),\n         (\"olmo\", \"OlmoModel\"),\n         (\"olmoe\", \"OlmoeModel\"),\n+        (\"omdet-turbo\", \"OmDetTurboForObjectDetection\"),\n         (\"oneformer\", \"OneFormerModel\"),\n         (\"open-llama\", \"OpenLlamaModel\"),\n         (\"openai-gpt\", \"OpenAIGPTModel\"),\n@@ -812,6 +814,7 @@\n     [\n         # Model for Zero Shot Object Detection mapping\n         (\"grounding-dino\", \"GroundingDinoForObjectDetection\"),\n+        (\"omdet-turbo\", \"OmDetTurboForObjectDetection\"),\n         (\"owlv2\", \"Owlv2ForObjectDetection\"),\n         (\"owlvit\", \"OwlViTForObjectDetection\"),\n     ]\n@@ -1326,6 +1329,7 @@\n         (\"albert\", \"AlbertModel\"),\n         (\"bert\", \"BertModel\"),\n         (\"big_bird\", \"BigBirdModel\"),\n+        (\"clip_text_model\", \"CLIPTextModel\"),\n         (\"data2vec-text\", \"Data2VecTextModel\"),\n         (\"deberta\", \"DebertaModel\"),\n         (\"deberta-v2\", \"DebertaV2Model\"),"
        },
        {
            "sha": "8a7b8c2330d3ce9c052ae363432a6f0540457059",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7",
            "patch": "@@ -344,6 +344,10 @@\n             ),\n             (\"olmo\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"olmoe\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n+            (\n+                \"omdet-turbo\",\n+                (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n+            ),\n             (\"oneformer\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n             (\n                 \"openai-gpt\","
        },
        {
            "sha": "34eb6386298fb882671cdc077eefd57cab9dc0d7",
            "filename": "src/transformers/models/omdet_turbo/__init__.py",
            "status": "added",
            "additions": 56,
            "deletions": 0,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2F__init__.py?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7",
            "patch": "@@ -0,0 +1,56 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n+\n+\n+_import_structure = {\n+    \"configuration_omdet_turbo\": [\"OmDetTurboConfig\"],\n+    \"processing_omdet_turbo\": [\"OmDetTurboProcessor\"],\n+}\n+\n+try:\n+    if not is_torch_available():\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    pass\n+else:\n+    _import_structure[\"modeling_omdet_turbo\"] = [\n+        \"OmDetTurboForObjectDetection\",\n+        \"OmDetTurboPreTrainedModel\",\n+    ]\n+\n+if TYPE_CHECKING:\n+    from .configuration_omdet_turbo import (\n+        OmDetTurboConfig,\n+    )\n+    from .processing_omdet_turbo import OmDetTurboProcessor\n+\n+    try:\n+        if not is_torch_available():\n+            raise OptionalDependencyNotAvailable()\n+    except OptionalDependencyNotAvailable:\n+        pass\n+    else:\n+        from .modeling_omdet_turbo import (\n+            OmDetTurboForObjectDetection,\n+            OmDetTurboPreTrainedModel,\n+        )\n+\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)"
        },
        {
            "sha": "cb5e69db5f901a5e390dac1c621db965376c1de5",
            "filename": "src/transformers/models/omdet_turbo/configuration_omdet_turbo.py",
            "status": "added",
            "additions": 290,
            "deletions": 0,
            "changes": 290,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconfiguration_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconfiguration_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconfiguration_omdet_turbo.py?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7",
            "patch": "@@ -0,0 +1,290 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"OmDet-Turbo model configuration\"\"\"\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ...utils.backbone_utils import verify_backbone_config_arguments\n+from ..auto import CONFIG_MAPPING\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class OmDetTurboConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`OmDetTurboForObjectDetection`].\n+    It is used to instantiate a OmDet-Turbo model according to the specified arguments, defining the model architecture\n+    Instantiating a configuration with the defaults will yield a similar configuration to that of the OmDet-Turbo\n+    [omlab/omdet-turbo-swin-tiny-hf](https://huggingface.co/omlab/omdet-turbo-swin-tiny-hf) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`PretrainedConfig`, *optional*):\n+            The configuration of the text backbone.\n+        backbone_config (`PretrainedConfig`, *optional*):\n+            The configuration of the vision backbone.\n+        use_timm_backbone (`bool`, *optional*, defaults to `True`):\n+            Whether to use the timm for the vision backbone.\n+        backbone (`str`, *optional*, defaults to `\"swin_tiny_patch4_window7_224\"`):\n+            The name of the pretrained vision backbone to use. If `use_pretrained_backbone=False` a randomly initialized\n+            backbone with the same architecture `backbone` is used.\n+        backbone_kwargs (`dict`, *optional*):\n+            Additional kwargs for the vision backbone.\n+        use_pretrained_backbone (`bool`, *optional*, defaults to `False`):\n+            Whether to use a pretrained vision backbone.\n+        apply_layernorm_after_vision_backbone (`bool`, *optional*, defaults to `True`):\n+            Whether to apply layer normalization on the feature maps of the vision backbone output.\n+        image_size (`int`, *optional*, defaults to 640):\n+            The size (resolution) of each image.\n+        disable_custom_kernels (`bool`, *optional*, defaults to `False`):\n+            Whether to disable custom kernels.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon value for layer normalization.\n+        batch_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon value for batch normalization.\n+        init_std (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        text_projection_in_dim (`int`, *optional*, defaults to 512):\n+            The input dimension for the text projection.\n+        text_projection_out_dim (`int`, *optional*, defaults to 512):\n+            The output dimension for the text projection.\n+        task_encoder_hidden_dim (`int`, *optional*, defaults to 1024):\n+            The feedforward dimension for the task encoder.\n+        class_embed_dim (`int`, *optional*, defaults to 512):\n+            The dimension of the classes embeddings.\n+        class_distance_type (`str`, *optional*, defaults to `\"cosine\"`):\n+            The type of of distance to compare predicted classes to projected classes embeddings.\n+            Can be `\"cosine\"` or `\"dot\"`.\n+        num_queries (`int`, *optional*, defaults to 900):\n+            The number of queries.\n+        csp_activation (`str`, *optional*, defaults to `\"silu\"`):\n+            The activation function of the Cross Stage Partial (CSP) networks of the encoder.\n+        conv_norm_activation (`str`, *optional*, defaults to `\"gelu\"`):\n+            The activation function of the ConvNormLayer layers of the encoder.\n+        encoder_feedforward_activation (`str`, *optional*, defaults to `\"relu\"`):\n+            The activation function for the feedforward network of the encoder.\n+        encoder_feedforward_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout rate following the activation of the encoder feedforward network.\n+        encoder_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout rate of the encoder multi-head attention module.\n+        hidden_expansion (`int`, *optional*, defaults to 1):\n+            The hidden expansion of the CSP networks in the encoder.\n+        vision_features_channels (`tuple(int)`, *optional*, defaults to `[256, 256, 256]`):\n+            The projected vision features channels used as inputs for the decoder.\n+        encoder_hidden_dim (`int`, *optional*, defaults to 256):\n+            The hidden dimension of the encoder.\n+        encoder_in_channels (`List(int)`, *optional*, defaults to `[192, 384, 768]`):\n+            The input channels for the encoder.\n+        encoder_projection_indices (`List(int)`, *optional*, defaults to `[2]`):\n+            The indices of the input features projected by each layers.\n+        encoder_attention_heads (`int`, *optional*, defaults to 8):\n+            The number of attention heads for the encoder.\n+        encoder_dim_feedforward (`int`, *optional*, defaults to 2048):\n+            The feedforward dimension for the encoder.\n+        encoder_layers (`int`, *optional*, defaults to 1):\n+            The number of layers in the encoder.\n+        positional_encoding_temperature (`int`, *optional*, defaults to 10000):\n+            The positional encoding temperature in the encoder.\n+        num_feature_levels (`int`, *optional*, defaults to 3):\n+            The number of feature levels for the multi-scale deformable attention module of the decoder.\n+        decoder_hidden_dim (`int`, *optional*, defaults to 256):\n+            The hidden dimension of the decoder.\n+        decoder_num_heads (`int`, *optional*, defaults to 8):\n+            The number of heads for the decoder.\n+        decoder_num_layers (`int`, *optional*, defaults to 6):\n+            The number of layers for the decoder.\n+        decoder_activation (`str`, *optional*, defaults to `\"relu\"`):\n+            The activation function for the decoder.\n+        decoder_dim_feedforward (`int`, *optional*, defaults to 2048):\n+            The feedforward dimension for the decoder.\n+        decoder_num_points (`int`, *optional*, defaults to 4):\n+            The number of points sampled in the decoder multi-scale deformable attention module.\n+        decoder_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout rate for the decoder.\n+        eval_size (`Tuple[int, int]`, *optional*):\n+            Height and width used to computes the effective height and width of the position embeddings after taking\n+            into account the stride (see RTDetr).\n+        learn_initial_query (`bool`, *optional*, defaults to `False`):\n+            Whether to learn the initial query.\n+        cache_size (`int`, *optional*, defaults to 100):\n+            The cache size for the classes and prompts caches.\n+        is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n+            Whether the model is used as an encoder-decoder model or not.\n+        kwargs (`Dict[str, Any]`, *optional*):\n+            Additional parameters from the architecture. The values in kwargs will be saved as part of the configuration\n+            and can be used to control the model outputs.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from transformers import OmDetTurboConfig, OmDetTurboForObjectDetection\n+\n+    >>> # Initializing a OmDet-Turbo omlab/omdet-turbo-tiny style configuration\n+    >>> configuration = OmDetTurboConfig()\n+\n+    >>> # Initializing a model (with random weights) from the omlab/omdet-turbo-tiny style configuration\n+    >>> model = OmDetTurboForObjectDetection(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"omdet-turbo\"\n+    attribute_map = {\n+        \"encoder_hidden_dim\": \"d_model\",\n+        \"num_attention_heads\": \"encoder_attention_heads\",\n+    }\n+\n+    def __init__(\n+        self,\n+        text_config=None,\n+        backbone_config=None,\n+        use_timm_backbone=True,\n+        backbone=\"swin_tiny_patch4_window7_224\",\n+        backbone_kwargs=None,\n+        use_pretrained_backbone=False,\n+        apply_layernorm_after_vision_backbone=True,\n+        image_size=640,\n+        disable_custom_kernels=False,\n+        layer_norm_eps=1e-5,\n+        batch_norm_eps=1e-5,\n+        init_std=0.02,\n+        text_projection_in_dim=512,\n+        text_projection_out_dim=512,\n+        task_encoder_hidden_dim=1024,\n+        class_embed_dim=512,\n+        class_distance_type=\"cosine\",\n+        num_queries=900,\n+        csp_activation=\"silu\",\n+        conv_norm_activation=\"gelu\",\n+        encoder_feedforward_activation=\"relu\",\n+        encoder_feedforward_dropout=0.0,\n+        encoder_dropout=0.0,\n+        hidden_expansion=1,\n+        vision_features_channels=[256, 256, 256],\n+        encoder_hidden_dim=256,\n+        encoder_in_channels=[192, 384, 768],\n+        encoder_projection_indices=[2],\n+        encoder_attention_heads=8,\n+        encoder_dim_feedforward=2048,\n+        encoder_layers=1,\n+        positional_encoding_temperature=10000,\n+        num_feature_levels=3,\n+        decoder_hidden_dim=256,\n+        decoder_num_heads=8,\n+        decoder_num_layers=6,\n+        decoder_activation=\"relu\",\n+        decoder_dim_feedforward=2048,\n+        decoder_num_points=4,\n+        decoder_dropout=0.0,\n+        eval_size=None,\n+        learn_initial_query=False,\n+        cache_size=100,\n+        is_encoder_decoder=True,\n+        **kwargs,\n+    ):\n+        if use_timm_backbone:\n+            if backbone_config is None:\n+                backbone_kwargs = {\n+                    \"out_indices\": [1, 2, 3],\n+                    \"img_size\": image_size,\n+                    \"always_partition\": True,\n+                }\n+        elif backbone_config is None:\n+            logger.info(\"`backbone_config` is `None`. Initializing the config with the default `swin` vision config.\")\n+            backbone_config = CONFIG_MAPPING[\"swin\"](\n+                window_size=7,\n+                image_size=image_size,\n+                embed_dim=96,\n+                depths=[2, 2, 6, 2],\n+                num_heads=[3, 6, 12, 24],\n+                out_indices=[2, 3, 4],\n+            )\n+        elif isinstance(backbone_config, dict):\n+            backbone_model_type = backbone_config.get(\"model_type\")\n+            config_class = CONFIG_MAPPING[backbone_model_type]\n+            backbone_config = config_class.from_dict(backbone_config)\n+\n+        verify_backbone_config_arguments(\n+            use_timm_backbone=use_timm_backbone,\n+            use_pretrained_backbone=use_pretrained_backbone,\n+            backbone=backbone,\n+            backbone_config=backbone_config,\n+            backbone_kwargs=backbone_kwargs,\n+        )\n+\n+        if text_config is None:\n+            logger.info(\n+                \"`text_config` is `None`. Initializing the config with the default `clip_text_model` text config.\"\n+            )\n+            text_config = CONFIG_MAPPING[\"clip_text_model\"]()\n+        elif isinstance(text_config, dict):\n+            text_model_type = text_config.get(\"model_type\")\n+            text_config = CONFIG_MAPPING[text_model_type](**text_config)\n+\n+        if class_distance_type not in [\"cosine\", \"dot\"]:\n+            raise ValueError(\n+                f\"Invalid `class_distance_type`. It should be either `cosine` or `dot`, but got {class_distance_type}.\"\n+            )\n+\n+        self.text_config = text_config\n+        self.backbone_config = backbone_config\n+        self.use_timm_backbone = use_timm_backbone\n+        self.backbone = backbone\n+        self.backbone_kwargs = backbone_kwargs\n+        self.use_pretrained_backbone = use_pretrained_backbone\n+        self.apply_layernorm_after_vision_backbone = apply_layernorm_after_vision_backbone\n+        self.image_size = image_size\n+        self.disable_custom_kernels = disable_custom_kernels\n+        self.layer_norm_eps = layer_norm_eps\n+        self.batch_norm_eps = batch_norm_eps\n+        self.init_std = init_std\n+        self.text_projection_in_dim = text_projection_in_dim\n+        self.text_projection_out_dim = text_projection_out_dim\n+        self.task_encoder_hidden_dim = task_encoder_hidden_dim\n+        self.class_embed_dim = class_embed_dim\n+        self.class_distance_type = class_distance_type\n+        self.num_queries = num_queries\n+        self.csp_activation = csp_activation\n+        self.conv_norm_activation = conv_norm_activation\n+        self.encoder_feedforward_activation = encoder_feedforward_activation\n+        self.encoder_feedforward_dropout = encoder_feedforward_dropout\n+        self.encoder_dropout = encoder_dropout\n+        self.hidden_expansion = hidden_expansion\n+        self.vision_features_channels = vision_features_channels\n+        self.encoder_hidden_dim = encoder_hidden_dim\n+        self.encoder_in_channels = encoder_in_channels\n+        self.encoder_projection_indices = encoder_projection_indices\n+        self.encoder_attention_heads = encoder_attention_heads\n+        self.encoder_dim_feedforward = encoder_dim_feedforward\n+        self.encoder_layers = encoder_layers\n+        self.positional_encoding_temperature = positional_encoding_temperature\n+        self.num_feature_levels = num_feature_levels\n+        self.decoder_hidden_dim = decoder_hidden_dim\n+        self.decoder_num_heads = decoder_num_heads\n+        self.decoder_num_layers = decoder_num_layers\n+        self.decoder_activation = decoder_activation\n+        self.decoder_dim_feedforward = decoder_dim_feedforward\n+        self.decoder_num_points = decoder_num_points\n+        self.decoder_dropout = decoder_dropout\n+        self.eval_size = eval_size\n+        self.learn_initial_query = learn_initial_query\n+        self.cache_size = cache_size\n+        self.is_encoder_decoder = is_encoder_decoder\n+\n+        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)"
        },
        {
            "sha": "2e515e983408b887a4827c6849a8a7f601361614",
            "filename": "src/transformers/models/omdet_turbo/convert_omdet_turbo_to_hf.py",
            "status": "added",
            "additions": 349,
            "deletions": 0,
            "changes": 349,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7",
            "patch": "@@ -0,0 +1,349 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Convert OmDet-Turbo checkpoints from the original repository.\n+\n+URL: https://github.com/om-ai-lab/OmDet\"\"\"\n+\n+import argparse\n+\n+import requests\n+import torch\n+from PIL import Image\n+\n+from transformers import (\n+    CLIPTokenizer,\n+    DetrImageProcessor,\n+    OmDetTurboConfig,\n+    OmDetTurboForObjectDetection,\n+    OmDetTurboProcessor,\n+)\n+\n+\n+IMAGE_MEAN = [123.675, 116.28, 103.53]\n+IMAGE_STD = [58.395, 57.12, 57.375]\n+\n+\n+def get_omdet_turbo_config(model_name, use_timm_backbone):\n+    if \"tiny\" in model_name:\n+        window_size = 7\n+        embed_dim = 96\n+        depths = (2, 2, 6, 2)\n+        num_heads = (3, 6, 12, 24)\n+        image_size = 640\n+    else:\n+        raise ValueError(\"Model not supported, only supports tiny variant.\")\n+\n+    config = OmDetTurboConfig(\n+        backbone_window_size=window_size,\n+        backbone_image_size=image_size,\n+        backbone_embed_dim=embed_dim,\n+        backbone_depths=depths,\n+        backbone_num_heads=num_heads,\n+        backbone_out_indices=(1, 2, 3),\n+        text_config={\"model_type\": \"clip_text_model\"},\n+        use_timm_backbone=use_timm_backbone,\n+        backbone=\"swin_tiny_patch4_window7_224\" if use_timm_backbone else None,\n+        apply_layernorm_after_vision_backbone=True if use_timm_backbone else False,\n+        use_pretrained_backbone=False,\n+    )\n+\n+    return config\n+\n+\n+def create_rename_keys_vision(state_dict, config):\n+    rename_keys = []\n+    # fmt: off\n+    ########################################## VISION BACKBONE - START\n+    for layer_name in state_dict.keys():\n+        if layer_name.startswith(\"backbone\") and not layer_name.startswith(\"backbone.norm\"):\n+            if config.use_timm_backbone:\n+                layer_name_replace = layer_name.replace(\"backbone\", \"vision_backbone.vision_backbone._backbone\")\n+                layer_name_replace = layer_name_replace.replace(\".layers.\", \".layers_\")\n+                if \"downsample\" in layer_name:\n+                    # get layer number\n+                    layer_num = int(layer_name.split(\".\")[2])\n+                    layer_name_replace = layer_name_replace.replace(f\"{layer_num}.downsample\", f\"{layer_num+1}.downsample\")\n+            else:\n+                layer_name_replace = layer_name.replace(\"backbone\", \"vision_backbone.vision_backbone\")\n+                layer_name_replace = layer_name_replace.replace(\"patch_embed.proj\", \"embeddings.patch_embeddings.projection\")\n+                layer_name_replace = layer_name_replace.replace(\"patch_embed.norm\", \"embeddings.norm\")\n+                if layer_name.startswith(\"backbone.layers\"):\n+                    layer_name_replace = layer_name_replace.replace(\"norm1\", \"layernorm_before\")\n+                    layer_name_replace = layer_name_replace.replace(\"norm2\", \"layernorm_after\")\n+                    layer_name_replace = layer_name_replace.replace(\"attn.proj\", \"attention.output.dense\")\n+                    layer_name_replace = layer_name_replace.replace(\"mlp.fc1\", \"intermediate.dense\")\n+                    layer_name_replace = layer_name_replace.replace(\"mlp.fc2\", \"output.dense\")\n+                    layer_name_replace = layer_name_replace.replace(\".layers.\", \".encoder.layers.\")\n+                    layer_name_replace = layer_name_replace.replace(\".attn.\", \".attention.self.\")\n+        elif layer_name.startswith(\"backbone.norm\"):\n+            layer_num = int(layer_name.split(\"norm\")[1].split(\".\")[0])\n+            if config.use_timm_backbone:\n+                layer_name_replace = layer_name.replace(\"backbone\", \"vision_backbone\")\n+                layer_name_replace = layer_name_replace.replace(f\"norm{layer_num}\", f\"layer_norms.{layer_num-1}\")\n+            else:\n+                layer_name_replace = layer_name.replace(f\"backbone.norm{layer_num}\", f\"vision_backbone.vision_backbone.hidden_states_norms.stage{layer_num+1}\")\n+        else:\n+            continue\n+        rename_keys.append((layer_name, layer_name_replace))\n+    ########################################## VISION BACKBONE - END\n+\n+    ########################################## ENCODER - START\n+    for layer_name, params in state_dict.items():\n+        if \"neck\" in layer_name:\n+            layer_name_replace = layer_name.replace(\"neck\", \"encoder\")\n+            layer_name_replace = layer_name_replace.replace(\"input_proj\", \"channel_projection_layers\")\n+            if \"fpn_blocks\" in layer_name or \"pan_blocks\" in layer_name or \"lateral_convs\" in layer_name or \"downsample_convs\" in layer_name:\n+                layer_name_replace = layer_name_replace.replace(\".m.\", \".bottlenecks.\")\n+                layer_name_replace = layer_name_replace.replace(\".cv\", \".conv\")\n+                layer_name_replace = layer_name_replace.replace(\".bn\", \".norm\")\n+            if \"encoder_layer\" in layer_name:\n+                layer_name_replace = layer_name_replace.replace(\"encoder_layer\", \"encoder.0.layers.0\")\n+                layer_name_replace = layer_name_replace.replace(\".linear\", \".fc\")\n+                layer_name_replace = layer_name_replace.replace(\"norm1\", \"self_attn_layer_norm\")\n+                layer_name_replace = layer_name_replace.replace(\"norm2\", \"final_layer_norm\")\n+            rename_keys.append((layer_name, layer_name_replace))\n+    ########################################## ENCODER - END\n+\n+    ########################################## DECODER - START\n+    for layer_name, params in state_dict.items():\n+        if layer_name.startswith(\"decoder\"):\n+            layer_name_replace = layer_name.replace(\"decoder.decoder.layers\", \"decoder.layers\")\n+            layer_name_replace = layer_name_replace.replace(\"input_proj\", \"channel_projection_layers\")\n+            layer_name_replace = layer_name_replace.replace(\"query_pos_head\", \"query_position_head\")\n+            layer_name_replace = layer_name_replace.replace(\"enc_bbox_head\", \"encoder_bbox_head\")\n+            layer_name_replace = layer_name_replace.replace(\"enc_output\", \"encoder_vision_features\")\n+            layer_name_replace = layer_name_replace.replace(\"dec_score_head\", \"decoder_class_head\")\n+            layer_name_replace = layer_name_replace.replace(\"dec_bbox_head\", \"decoder_bbox_head\")\n+            layer_name_replace = layer_name_replace.replace(\"enc_score_head\", \"encoder_class_head\")\n+            rename_keys.append((layer_name, layer_name_replace))\n+    ########################################## DECODER - END\n+    # fmt: on\n+    return rename_keys\n+\n+\n+def create_rename_keys_language(state_dict):\n+    rename_keys = []\n+    # fmt: off\n+    for layer_name in state_dict.keys():\n+        if layer_name.startswith(\"language_backbone\") and not layer_name.startswith(\"language_backbone.text_projection\"):\n+            layer_name_replace = layer_name.replace(\"language_backbone\", \"language_backbone.model.text_model\")\n+            layer_name_replace = layer_name_replace.replace(\"transformer.resblocks\", \"encoder.layers\")\n+            layer_name_replace = layer_name_replace.replace(\"token_embedding\", \"embeddings.token_embedding\")\n+            layer_name_replace = layer_name_replace.replace(\"positional_embedding\", \"embeddings.position_embedding.weight\")\n+            layer_name_replace = layer_name_replace.replace(\".attn\", \".self_attn\")\n+            layer_name_replace = layer_name_replace.replace(\".mlp.c_fc\", \".mlp.fc1\")\n+            layer_name_replace = layer_name_replace.replace(\".mlp.c_proj\", \".mlp.fc2\")\n+            layer_name_replace = layer_name_replace.replace(\"ln_final\", \"final_layer_norm\")\n+            layer_name_replace = layer_name_replace.replace(\".ln_\", \".layer_norm\")\n+            rename_keys.append((layer_name, layer_name_replace))\n+    # fmt: on\n+    return rename_keys\n+\n+\n+def rename_key(dct, old, new):\n+    val = dct.pop(old)\n+    dct[new] = val\n+\n+\n+# we split up the matrix of each encoder layer into queries, keys and values\n+def read_in_q_k_v_vision(state_dict, config):\n+    state_dict_keys = list(state_dict.keys())\n+    for layer_name_vision in state_dict_keys:\n+        if layer_name_vision.startswith(\"vision_backbone\") and \"qkv\" in layer_name_vision:\n+            layer_num = int(layer_name_vision.split(\".\")[4])\n+            hidden_size = config.backbone_config.embed_dim * 2**layer_num\n+            if \"weight\" in layer_name_vision:\n+                in_proj_weight = state_dict.pop(layer_name_vision)\n+                state_dict[layer_name_vision.replace(\"qkv.weight\", \"key.weight\")] = in_proj_weight[:hidden_size, :]\n+                state_dict[layer_name_vision.replace(\"qkv.weight\", \"query.weight\")] = in_proj_weight[\n+                    hidden_size : hidden_size * 2, :\n+                ]\n+                state_dict[layer_name_vision.replace(\"qkv.weight\", \"value.weight\")] = in_proj_weight[-hidden_size:, :]\n+            elif \"bias\" in layer_name_vision:\n+                in_proj_bias = state_dict.pop(layer_name_vision)\n+                state_dict[layer_name_vision.replace(\"qkv.bias\", \"key.bias\")] = in_proj_bias[:hidden_size]\n+                state_dict[layer_name_vision.replace(\"qkv.bias\", \"query.bias\")] = in_proj_bias[\n+                    hidden_size : hidden_size * 2\n+                ]\n+                state_dict[layer_name_vision.replace(\"qkv.bias\", \"value.bias\")] = in_proj_bias[-hidden_size:]\n+\n+\n+def read_in_q_k_v_text(state_dict, config):\n+    state_dict_keys = list(state_dict.keys())\n+    hidden_size = config.text_config.projection_dim\n+    for layer_name_text in state_dict_keys:\n+        if layer_name_text.startswith(\"language_backbone\") and \"in_proj\" in layer_name_text:\n+            if \"weight\" in layer_name_text:\n+                in_proj_weight = state_dict.pop(layer_name_text)\n+                state_dict[layer_name_text.replace(\"in_proj_weight\", \"q_proj.weight\")] = in_proj_weight[\n+                    :hidden_size, :\n+                ]\n+                state_dict[layer_name_text.replace(\"in_proj_weight\", \"k_proj.weight\")] = in_proj_weight[\n+                    hidden_size : hidden_size * 2, :\n+                ]\n+                state_dict[layer_name_text.replace(\"in_proj_weight\", \"v_proj.weight\")] = in_proj_weight[\n+                    -hidden_size:, :\n+                ]\n+            elif \"bias\" in layer_name_text:\n+                in_proj_bias = state_dict.pop(layer_name_text)\n+                state_dict[layer_name_text.replace(\"in_proj_bias\", \"q_proj.bias\")] = in_proj_bias[:hidden_size]\n+                state_dict[layer_name_text.replace(\"in_proj_bias\", \"k_proj.bias\")] = in_proj_bias[\n+                    hidden_size : hidden_size * 2\n+                ]\n+                state_dict[layer_name_text.replace(\"in_proj_bias\", \"v_proj.bias\")] = in_proj_bias[-hidden_size:]\n+\n+\n+def read_in_q_k_v_encoder(state_dict, config):\n+    embed_dim = config.encoder_hidden_dim\n+    # read in weights + bias of input projection layer (in original implementation, this is a single matrix + bias)\n+    in_proj_weight = state_dict.pop(\"encoder.encoder.0.layers.0.self_attn.in_proj_weight\")\n+    in_proj_bias = state_dict.pop(\"encoder.encoder.0.layers.0.self_attn.in_proj_bias\")\n+    # next, add query, keys and values (in that order) to the state dict\n+    state_dict[\"encoder.encoder.0.layers.0.self_attn.query.weight\"] = in_proj_weight[:embed_dim, :]\n+    state_dict[\"encoder.encoder.0.layers.0.self_attn.query.bias\"] = in_proj_bias[:embed_dim]\n+    state_dict[\"encoder.encoder.0.layers.0.self_attn.key.weight\"] = in_proj_weight[embed_dim : embed_dim * 2, :]\n+    state_dict[\"encoder.encoder.0.layers.0.self_attn.key.bias\"] = in_proj_bias[embed_dim : embed_dim * 2]\n+    state_dict[\"encoder.encoder.0.layers.0.self_attn.value.weight\"] = in_proj_weight[-embed_dim:, :]\n+    state_dict[\"encoder.encoder.0.layers.0.self_attn.value.bias\"] = in_proj_bias[-embed_dim:]\n+\n+\n+def read_in_q_k_v_decoder(state_dict, config):\n+    for layer_num in range(config.decoder_num_layers):\n+        embed_dim = config.decoder_hidden_dim\n+        # read in weights + bias of input projection layer (in original implementation, this is a single matrix + bias)\n+        in_proj_weight = state_dict.pop(f\"decoder.layers.{layer_num}.self_attn.in_proj_weight\")\n+        in_proj_bias = state_dict.pop(f\"decoder.layers.{layer_num}.self_attn.in_proj_bias\")\n+        # next, add query, keys and values (in that order) to the state dict\n+        state_dict[f\"decoder.layers.{layer_num}.self_attn.query.weight\"] = in_proj_weight[:embed_dim, :]\n+        state_dict[f\"decoder.layers.{layer_num}.self_attn.query.bias\"] = in_proj_bias[:embed_dim]\n+        state_dict[f\"decoder.layers.{layer_num}.self_attn.key.weight\"] = in_proj_weight[embed_dim : embed_dim * 2, :]\n+        state_dict[f\"decoder.layers.{layer_num}.self_attn.key.bias\"] = in_proj_bias[embed_dim : embed_dim * 2]\n+        state_dict[f\"decoder.layers.{layer_num}.self_attn.value.weight\"] = in_proj_weight[-embed_dim:, :]\n+        state_dict[f\"decoder.layers.{layer_num}.self_attn.value.bias\"] = in_proj_bias[-embed_dim:]\n+\n+\n+def run_test(model, processor):\n+    # We will verify our results on an image of cute cats\n+    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n+\n+    classes = [\"cat\", \"remote\"]\n+    task = \"Detect {}.\".format(\", \".join(classes))\n+    inputs = processor(image, text=classes, task=task, return_tensors=\"pt\")\n+\n+    # Running forward\n+    with torch.no_grad():\n+        outputs = model(**inputs)\n+\n+    predicted_slice = outputs[1][0, :3, :3]\n+    print(predicted_slice)\n+    expected_slice = torch.tensor([[0.9427, -2.5958], [0.2105, -3.4569], [-2.6364, -4.1610]])\n+\n+    assert torch.allclose(predicted_slice, expected_slice, atol=1e-4)\n+    print(\"Looks ok!\")\n+\n+\n+@torch.no_grad()\n+def convert_omdet_turbo_checkpoint(args):\n+    model_name = args.model_name\n+    pytorch_dump_folder_path = args.pytorch_dump_folder_path\n+    push_to_hub = args.push_to_hub\n+    use_timm_backbone = args.use_timm_backbone\n+\n+    checkpoint_mapping = {\n+        \"omdet-turbo-tiny\": [\n+            \"https://huggingface.co/omlab/OmDet-Turbo_tiny_SWIN_T/resolve/main/OmDet-Turbo_tiny_SWIN_T.pth\",\n+            \"https://huggingface.co/omlab/OmDet-Turbo_tiny_SWIN_T/resolve/main/ViT-B-16.pt\",\n+        ],\n+    }\n+    # Define default OmDetTurbo configuation\n+    config = get_omdet_turbo_config(model_name, use_timm_backbone)\n+\n+    # Load original checkpoint\n+    checkpoint_url = checkpoint_mapping[model_name]\n+    original_state_dict_vision = torch.hub.load_state_dict_from_url(checkpoint_url[0], map_location=\"cpu\")[\"model\"]\n+    original_state_dict_vision = {k.replace(\"module.\", \"\"): v for k, v in original_state_dict_vision.items()}\n+\n+    # Rename keys\n+    new_state_dict = original_state_dict_vision.copy()\n+    rename_keys_vision = create_rename_keys_vision(new_state_dict, config)\n+\n+    rename_keys_language = create_rename_keys_language(new_state_dict)\n+\n+    for src, dest in rename_keys_vision:\n+        rename_key(new_state_dict, src, dest)\n+\n+    for src, dest in rename_keys_language:\n+        rename_key(new_state_dict, src, dest)\n+\n+    if not use_timm_backbone:\n+        read_in_q_k_v_vision(new_state_dict, config)\n+    read_in_q_k_v_text(new_state_dict, config)\n+    read_in_q_k_v_encoder(new_state_dict, config)\n+    read_in_q_k_v_decoder(new_state_dict, config)\n+    # add \"model\" prefix to all keys\n+    new_state_dict = {f\"model.{k}\": v for k, v in new_state_dict.items()}\n+\n+    # Load HF model\n+    model = OmDetTurboForObjectDetection(config)\n+    model.eval()\n+    missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)\n+    print(\"Missing keys:\", missing_keys)\n+    print(\"Unexpected keys:\", unexpected_keys)\n+\n+    image_processor = DetrImageProcessor(\n+        size={\"height\": config.backbone_image_size, \"width\": config.backbone_image_size},\n+        do_rescale=False,\n+        image_mean=IMAGE_MEAN,\n+        image_std=IMAGE_STD,\n+        do_pad=False,\n+    )\n+    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n+    processor = OmDetTurboProcessor(image_processor=image_processor, tokenizer=tokenizer)\n+\n+    # end-to-end consistency test\n+    run_test(model, processor)\n+\n+    if pytorch_dump_folder_path is not None:\n+        model.save_pretrained(pytorch_dump_folder_path)\n+        processor.save_pretrained(pytorch_dump_folder_path)\n+\n+    if push_to_hub:\n+        model.push_to_hub(f\"omlab/{model_name}\")\n+        processor.push_to_hub(f\"omlab/{model_name}\")\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    # Required parameters\n+    parser.add_argument(\n+        \"--model_name\",\n+        default=\"omdet-turbo-tiny\",\n+        type=str,\n+        choices=[\"omdet-turbo-tiny\"],\n+        help=\"Name of the OmDetTurbo model you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the 🤗 hub.\"\n+    )\n+    parser.add_argument(\n+        \"--use_timm_backbone\", action=\"store_true\", help=\"Whether or not to use timm backbone for vision backbone.\"\n+    )\n+\n+    args = parser.parse_args()\n+    convert_omdet_turbo_checkpoint(args)"
        },
        {
            "sha": "bb6c8838ff8c658581f492f74bace3b85a99f09e",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "added",
            "additions": 1810,
            "deletions": 0,
            "changes": 1810,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7",
            "patch": "@@ -0,0 +1,1810 @@\n+# coding=utf-8\n+# Copyright 2024 Om Research Lab and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch OmDet-Turbo model.\"\"\"\n+\n+import math\n+import os\n+import warnings\n+from collections import OrderedDict\n+from dataclasses import dataclass\n+from functools import lru_cache\n+from pathlib import Path\n+from typing import Optional, Tuple, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import Tensor, nn\n+from torch.autograd import Function\n+from torch.autograd.function import once_differentiable\n+\n+from ...activations import ACT2CLS, ACT2FN\n+from ...file_utils import (\n+    ModelOutput,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_torch_cuda_available,\n+    replace_return_docstrings,\n+)\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import is_ninja_available, logging\n+from ...utils.backbone_utils import load_backbone\n+from ..auto import AutoModel\n+from .configuration_omdet_turbo import OmDetTurboConfig\n+\n+\n+MultiScaleDeformableAttention = None\n+\n+logger = logging.get_logger(__name__)\n+_CONFIG_FOR_DOC = \"OmDetTurboConfig\"\n+\n+\n+@dataclass\n+class OmDetTurboEncoderOutput(ModelOutput):\n+    \"\"\"\n+    Base class for outputs of the OmDetTurboHybridEncoder.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor`):\n+            Last hidden states of the encoder.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        extracted_states (`Tuple[torch.FloatTensor]`):\n+            The extracted states from the Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) of the encoder.\n+    \"\"\"\n+\n+    last_hidden_state: torch.FloatTensor = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    extracted_states: Tuple[torch.FloatTensor] = None\n+\n+\n+@dataclass\n+class OmDetTurboDecoderOutput(ModelOutput):\n+    \"\"\"\n+    Base class for outputs of the OmDetTurboDecoder.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the decoder.\n+        decoder_coords (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+            The predicted coordinates of the objects.\n+        decoder_classes (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes)`):\n+            The predicted classes of the objects.\n+        encoder_coord_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+            The predicted coordinates of the objects from the encoder.\n+        encoder_class_logits (`Tuple[torch.FloatTensor]`) of shape `(batch_size, num_queries, num_classes)`:\n+            The predicted class of the objects from the encoder.\n+        init_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+            The initial reference points.\n+        intermediate_reference_points (`Tuple[Tuple[torch.FloatTensor]]`):\n+            The intermediate reference points.\n+        hidden_states (`Optional[Tuple[torch.FloatTensor]]`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of shape\n+            `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n+            plus the initial embedding outputs.\n+        attentions (`Optional[Tuple[Tuple[torch.FloatTensor]]]`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,\n+            sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the\n+            weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.\n+    \"\"\"\n+\n+    last_hidden_state: torch.FloatTensor = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n+    decoder_coords: torch.FloatTensor = None\n+    decoder_classes: torch.FloatTensor = None\n+    encoder_coord_logits: torch.FloatTensor = None\n+    encoder_class_logits: Tuple[torch.FloatTensor] = None\n+    init_reference_points: torch.FloatTensor = None\n+    intermediate_reference_points: Tuple[Tuple[torch.FloatTensor]] = None\n+\n+\n+@dataclass\n+class OmDetTurboObjectDetectionOutput(ModelOutput):\n+    \"\"\"\n+    Output type of [`OmDetTurboObjectDetectionOutput`].\n+\n+    Args:\n+        loss (`torch.FloatTensor`):\n+            The loss value.\n+        decoder_coord_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+            The predicted coordinates logits of the objects.\n+        decoder_class_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes)`):\n+            The predicted class of the objects.\n+        init_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+            The initial reference points.\n+        intermediate_reference_points (`Tuple[Tuple[torch.FloatTensor]]`):\n+            The intermediate reference points.\n+        encoder_coord_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+            The predicted coordinates of the objects from the encoder.\n+        encoder_class_logits (`Tuple[torch.FloatTensor]`):\n+            The predicted class of the objects from the encoder.\n+        encoder_extracted_states (`torch.FloatTensor`):\n+            The extracted states from the Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) of the encoder.\n+        decoder_hidden_states (`Optional[Tuple[torch.FloatTensor]]`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of shape\n+            `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n+            plus the initial embedding outputs.\n+        decoder_attentions (`Optional[Tuple[Tuple[torch.FloatTensor]]]`):\n+            Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,\n+            sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the\n+            weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.\n+        encoder_hidden_states (`Optional[Tuple[torch.FloatTensor]]`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of shape\n+            `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n+            plus the initial embedding outputs.\n+        encoder_attentions (`Optional[Tuple[Tuple[torch.FloatTensor]]]`):\n+            Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,\n+            sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the\n+            weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.\n+    \"\"\"\n+\n+    loss: torch.FloatTensor = None\n+    decoder_coord_logits: torch.FloatTensor = None\n+    decoder_class_logits: torch.FloatTensor = None\n+    init_reference_points: torch.FloatTensor = None\n+    intermediate_reference_points: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n+    encoder_coord_logits: torch.FloatTensor = None\n+    encoder_class_logits: Tuple[torch.FloatTensor] = None\n+    encoder_extracted_states: torch.FloatTensor = None\n+    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n+    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    encoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n+\n+\n+# Copied from models.deformable_detr.load_cuda_kernels\n+def load_cuda_kernels():\n+    from torch.utils.cpp_extension import load\n+\n+    global MultiScaleDeformableAttention\n+\n+    root = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"deformable_detr\"\n+    src_files = [\n+        root / filename\n+        for filename in [\n+            \"vision.cpp\",\n+            os.path.join(\"cpu\", \"ms_deform_attn_cpu.cpp\"),\n+            os.path.join(\"cuda\", \"ms_deform_attn_cuda.cu\"),\n+        ]\n+    ]\n+\n+    MultiScaleDeformableAttention = load(\n+        \"MultiScaleDeformableAttention\",\n+        src_files,\n+        with_cuda=True,\n+        extra_include_paths=[str(root)],\n+        extra_cflags=[\"-DWITH_CUDA=1\"],\n+        extra_cuda_cflags=[\n+            \"-DCUDA_HAS_FP16=1\",\n+            \"-D__CUDA_NO_HALF_OPERATORS__\",\n+            \"-D__CUDA_NO_HALF_CONVERSIONS__\",\n+            \"-D__CUDA_NO_HALF2_OPERATORS__\",\n+        ],\n+    )\n+\n+\n+# Copied from transformers.models.deformable_detr.modeling_deformable_detr.multi_scale_deformable_attention\n+def multi_scale_deformable_attention(\n+    value: Tensor, value_spatial_shapes: Tensor, sampling_locations: Tensor, attention_weights: Tensor\n+) -> Tensor:\n+    batch_size, _, num_heads, hidden_dim = value.shape\n+    _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n+    # Ignore copy\n+    value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n+    sampling_grids = 2 * sampling_locations - 1\n+    sampling_value_list = []\n+    for level_id, (height, width) in enumerate(value_spatial_shapes):\n+        # batch_size, height*width, num_heads, hidden_dim\n+        # -> batch_size, height*width, num_heads*hidden_dim\n+        # -> batch_size, num_heads*hidden_dim, height*width\n+        # -> batch_size*num_heads, hidden_dim, height, width\n+        value_l_ = (\n+            value_list[level_id].flatten(2).transpose(1, 2).reshape(batch_size * num_heads, hidden_dim, height, width)\n+        )\n+        # batch_size, num_queries, num_heads, num_points, 2\n+        # -> batch_size, num_heads, num_queries, num_points, 2\n+        # -> batch_size*num_heads, num_queries, num_points, 2\n+        sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n+        # batch_size*num_heads, hidden_dim, num_queries, num_points\n+        sampling_value_l_ = nn.functional.grid_sample(\n+            value_l_, sampling_grid_l_, mode=\"bilinear\", padding_mode=\"zeros\", align_corners=False\n+        )\n+        sampling_value_list.append(sampling_value_l_)\n+    # (batch_size, num_queries, num_heads, num_levels, num_points)\n+    # -> (batch_size, num_heads, num_queries, num_levels, num_points)\n+    # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)\n+    attention_weights = attention_weights.transpose(1, 2).reshape(\n+        batch_size * num_heads, 1, num_queries, num_levels * num_points\n+    )\n+    output = (\n+        (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)\n+        .sum(-1)\n+        .view(batch_size, num_heads * hidden_dim, num_queries)\n+    )\n+    return output.transpose(1, 2).contiguous()\n+\n+\n+class OmDetTurboLRUCache:\n+    def __init__(self, capacity: int):\n+        self.cache = OrderedDict()\n+        self.capacity = capacity\n+        self.current_load = 0\n+\n+    def has(self, key) -> bool:\n+        return key in self.cache\n+\n+    def get(self, key):\n+        \"\"\"\n+        Get the value of the key if the key exists in the cache, otherwise return None.\n+        Move the key to the end of the cache to show that it was recently used.\n+        \"\"\"\n+        if key not in self.cache:\n+            return None\n+        self.cache.move_to_end(key)\n+        return self.cache[key]\n+\n+    def put(self, key, value) -> None:\n+        \"\"\"\n+        Add the key-value pair to the cache.\n+        Move the key to the end of the cache to show that it was recently used.\n+        If the cache is full, remove the first key (least recently used).\n+        \"\"\"\n+        if key not in self.cache:\n+            self.current_load += 1\n+            if self.current_load > self.capacity:\n+                self.cache.popitem(last=False)\n+                self.current_load -= 1\n+\n+        self.cache[key] = value\n+        self.cache.move_to_end(key)\n+\n+\n+class OmDetTurboLanguageBackbone(nn.Module):\n+    def __init__(self, config: OmDetTurboConfig):\n+        super().__init__()\n+        self.model = AutoModel.from_config(config.text_config, attn_implementation=config._attn_implementation)\n+        self.text_projection = nn.Parameter(torch.zeros(config.text_projection_in_dim, config.text_projection_out_dim))\n+\n+    def forward(self, hidden_states, mask=None, encode_type=\"task\"):\n+        text_outputs = self.model(hidden_states)\n+        pooled_output = text_outputs[0]\n+        if encode_type == \"task\":\n+            if mask is None:\n+                raise ValueError(\"mask is required for task encoding\")\n+            max_len = (mask != 0).sum(1).max().item()\n+            truncated_mask = mask[:, :max_len]\n+            truncated_output = pooled_output[:, :max_len, :]\n+            return truncated_output.transpose(0, 1), truncated_mask\n+        elif encode_type == \"class\":\n+            max_pooled_output = pooled_output[torch.arange(pooled_output.shape[0]), hidden_states.argmax(dim=-1)]\n+            projected_output = max_pooled_output @ self.text_projection\n+            return projected_output\n+        else:\n+            raise ValueError(f\"encode_type {encode_type} is not supported\")\n+\n+\n+class OmDetTurboVisionBackbone(nn.Module):\n+    def __init__(self, config: OmDetTurboConfig):\n+        super().__init__()\n+        self.apply_layernorm_after_vision_backbone = config.apply_layernorm_after_vision_backbone\n+        self.vision_backbone = load_backbone(config)\n+        self.layer_norms = nn.ModuleList(\n+            [nn.LayerNorm(in_channel_dim, eps=config.layer_norm_eps) for in_channel_dim in config.encoder_in_channels]\n+        )\n+\n+    def forward(self, pixel_values):\n+        outputs = self.vision_backbone(pixel_values).feature_maps\n+        if self.apply_layernorm_after_vision_backbone:\n+            outputs = [\n+                layer_norm(output).permute(0, 3, 1, 2).contiguous()\n+                for layer_norm, output in zip(self.layer_norms, outputs)\n+            ]\n+\n+        return outputs\n+\n+\n+# Copied from transformers.models.deformable_detr.modeling_deformable_detr.MultiScaleDeformableAttentionFunction\n+class MultiScaleDeformableAttentionFunction(Function):\n+    @staticmethod\n+    def forward(\n+        context,\n+        value,\n+        value_spatial_shapes,\n+        value_level_start_index,\n+        sampling_locations,\n+        attention_weights,\n+        im2col_step,\n+    ):\n+        context.im2col_step = im2col_step\n+        output = MultiScaleDeformableAttention.ms_deform_attn_forward(\n+            value,\n+            value_spatial_shapes,\n+            value_level_start_index,\n+            sampling_locations,\n+            attention_weights,\n+            context.im2col_step,\n+        )\n+        context.save_for_backward(\n+            value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights\n+        )\n+        return output\n+\n+    @staticmethod\n+    @once_differentiable\n+    def backward(context, grad_output):\n+        (\n+            value,\n+            value_spatial_shapes,\n+            value_level_start_index,\n+            sampling_locations,\n+            attention_weights,\n+        ) = context.saved_tensors\n+        grad_value, grad_sampling_loc, grad_attn_weight = MultiScaleDeformableAttention.ms_deform_attn_backward(\n+            value,\n+            value_spatial_shapes,\n+            value_level_start_index,\n+            sampling_locations,\n+            attention_weights,\n+            grad_output,\n+            context.im2col_step,\n+        )\n+\n+        return grad_value, None, None, grad_sampling_loc, grad_attn_weight, None\n+\n+\n+# Copied from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrMultiscaleDeformableAttention with DeformableDetr->OmDetTurbo, Deformable DETR->OmDet-Turbo\n+class OmDetTurboMultiscaleDeformableAttention(nn.Module):\n+    \"\"\"\n+    Multiscale deformable attention as proposed in Deformable DETR.\n+    \"\"\"\n+\n+    def __init__(self, config: OmDetTurboConfig, num_heads: int, n_points: int):\n+        super().__init__()\n+\n+        kernel_loaded = MultiScaleDeformableAttention is not None\n+        if is_torch_cuda_available() and is_ninja_available() and not kernel_loaded:\n+            try:\n+                load_cuda_kernels()\n+            except Exception as e:\n+                logger.warning(f\"Could not load the custom kernel for multi-scale deformable attention: {e}\")\n+\n+        if config.d_model % num_heads != 0:\n+            raise ValueError(\n+                f\"embed_dim (d_model) must be divisible by num_heads, but got {config.d_model} and {num_heads}\"\n+            )\n+        dim_per_head = config.d_model // num_heads\n+        # check if dim_per_head is power of 2\n+        if not ((dim_per_head & (dim_per_head - 1) == 0) and dim_per_head != 0):\n+            warnings.warn(\n+                \"You'd better set embed_dim (d_model) in OmDetTurboMultiscaleDeformableAttention to make the\"\n+                \" dimension of each attention head a power of 2 which is more efficient in the authors' CUDA\"\n+                \" implementation.\"\n+            )\n+\n+        self.im2col_step = 64\n+\n+        self.d_model = config.d_model\n+        self.n_levels = config.num_feature_levels\n+        self.n_heads = num_heads\n+        self.n_points = n_points\n+\n+        self.sampling_offsets = nn.Linear(config.d_model, num_heads * self.n_levels * n_points * 2)\n+        self.attention_weights = nn.Linear(config.d_model, num_heads * self.n_levels * n_points)\n+        self.value_proj = nn.Linear(config.d_model, config.d_model)\n+        self.output_proj = nn.Linear(config.d_model, config.d_model)\n+\n+        self.disable_custom_kernels = config.disable_custom_kernels\n+\n+        self._reset_parameters()\n+\n+    def _reset_parameters(self):\n+        nn.init.constant_(self.sampling_offsets.weight.data, 0.0)\n+        default_dtype = torch.get_default_dtype()\n+        thetas = torch.arange(self.n_heads, dtype=torch.int64).to(default_dtype) * (2.0 * math.pi / self.n_heads)\n+        grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n+        grid_init = (\n+            (grid_init / grid_init.abs().max(-1, keepdim=True)[0])\n+            .view(self.n_heads, 1, 1, 2)\n+            .repeat(1, self.n_levels, self.n_points, 1)\n+        )\n+        for i in range(self.n_points):\n+            grid_init[:, :, i, :] *= i + 1\n+        with torch.no_grad():\n+            self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n+        nn.init.constant_(self.attention_weights.weight.data, 0.0)\n+        nn.init.constant_(self.attention_weights.bias.data, 0.0)\n+        nn.init.xavier_uniform_(self.value_proj.weight.data)\n+        nn.init.constant_(self.value_proj.bias.data, 0.0)\n+        nn.init.xavier_uniform_(self.output_proj.weight.data)\n+        nn.init.constant_(self.output_proj.bias.data, 0.0)\n+\n+    def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[Tensor]):\n+        return tensor if position_embeddings is None else tensor + position_embeddings\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        encoder_hidden_states=None,\n+        encoder_attention_mask=None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n+        reference_points=None,\n+        spatial_shapes=None,\n+        spatial_shapes_list=None,\n+        level_start_index=None,\n+        output_attentions: bool = False,\n+    ):\n+        # add position embeddings to the hidden states before projecting to queries and keys\n+        if position_embeddings is not None:\n+            hidden_states = self.with_pos_embed(hidden_states, position_embeddings)\n+\n+        batch_size, num_queries, _ = hidden_states.shape\n+        batch_size, sequence_length, _ = encoder_hidden_states.shape\n+        # Ignore copy\n+        total_elements = sum([shape[0] * shape[1] for shape in spatial_shapes_list])\n+        if total_elements != sequence_length:\n+            raise ValueError(\n+                \"Make sure to align the spatial shapes with the sequence length of the encoder hidden states\"\n+            )\n+\n+        value = self.value_proj(encoder_hidden_states)\n+        if attention_mask is not None:\n+            # we invert the attention_mask\n+            value = value.masked_fill(~attention_mask[..., None], float(0))\n+        value = value.view(batch_size, sequence_length, self.n_heads, self.d_model // self.n_heads)\n+        sampling_offsets = self.sampling_offsets(hidden_states).view(\n+            batch_size, num_queries, self.n_heads, self.n_levels, self.n_points, 2\n+        )\n+        attention_weights = self.attention_weights(hidden_states).view(\n+            batch_size, num_queries, self.n_heads, self.n_levels * self.n_points\n+        )\n+        attention_weights = F.softmax(attention_weights, -1).view(\n+            batch_size, num_queries, self.n_heads, self.n_levels, self.n_points\n+        )\n+        # batch_size, num_queries, n_heads, n_levels, n_points, 2\n+        num_coordinates = reference_points.shape[-1]\n+        if num_coordinates == 2:\n+            offset_normalizer = torch.stack([spatial_shapes[..., 1], spatial_shapes[..., 0]], -1)\n+            sampling_locations = (\n+                reference_points[:, :, None, :, None, :]\n+                + sampling_offsets / offset_normalizer[None, None, None, :, None, :]\n+            )\n+        elif num_coordinates == 4:\n+            sampling_locations = (\n+                reference_points[:, :, None, :, None, :2]\n+                + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5\n+            )\n+        else:\n+            raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n+\n+        if self.disable_custom_kernels:\n+            # PyTorch implementation\n+            output = multi_scale_deformable_attention(\n+                value, spatial_shapes_list, sampling_locations, attention_weights\n+            )\n+        else:\n+            try:\n+                # custom kernel\n+                output = MultiScaleDeformableAttentionFunction.apply(\n+                    value,\n+                    spatial_shapes,\n+                    level_start_index,\n+                    sampling_locations,\n+                    attention_weights,\n+                    self.im2col_step,\n+                )\n+            except Exception:\n+                # PyTorch implementation\n+                output = multi_scale_deformable_attention(\n+                    value, spatial_shapes_list, sampling_locations, attention_weights\n+                )\n+        output = self.output_proj(output)\n+\n+        return output, attention_weights\n+\n+\n+# Copied from transformers.models.rt_detr.modeling_rt_detr.RTDetrConvNormLayer with RTDetr->OmDetTurbo\n+class OmDetTurboConvNormLayer(nn.Module):\n+    def __init__(self, config, in_channels, out_channels, kernel_size, stride, padding=None, activation=None):\n+        super().__init__()\n+        self.conv = nn.Conv2d(\n+            in_channels,\n+            out_channels,\n+            kernel_size,\n+            stride,\n+            padding=(kernel_size - 1) // 2 if padding is None else padding,\n+            bias=False,\n+        )\n+        self.norm = nn.BatchNorm2d(out_channels, config.batch_norm_eps)\n+        self.activation = nn.Identity() if activation is None else ACT2CLS[activation]()\n+\n+    def forward(self, hidden_state):\n+        hidden_state = self.conv(hidden_state)\n+        hidden_state = self.norm(hidden_state)\n+        hidden_state = self.activation(hidden_state)\n+        return hidden_state\n+\n+\n+# Copied from transformers.models.rt_detr.modeling_rt_detr.RTDetrRepVggBlock with RTDetr->OmDetTurbo, activation_function->csp_activation\n+class OmDetTurboRepVggBlock(nn.Module):\n+    \"\"\"\n+    RepVGG architecture block introduced by the work \"RepVGG: Making VGG-style ConvNets Great Again\".\n+    \"\"\"\n+\n+    def __init__(self, config: OmDetTurboConfig):\n+        super().__init__()\n+\n+        activation = config.csp_activation\n+        hidden_channels = int(config.encoder_hidden_dim * config.hidden_expansion)\n+        self.conv1 = OmDetTurboConvNormLayer(config, hidden_channels, hidden_channels, 3, 1, padding=1)\n+        self.conv2 = OmDetTurboConvNormLayer(config, hidden_channels, hidden_channels, 1, 1, padding=0)\n+        self.activation = nn.Identity() if activation is None else ACT2CLS[activation]()\n+\n+    def forward(self, x):\n+        y = self.conv1(x) + self.conv2(x)\n+        return self.activation(y)\n+\n+\n+# Copied from transformers.models.rt_detr.modeling_rt_detr.RTDetrCSPRepLayer with RTDetr->OmDetTurbo, activation_function->csp_activation\n+class OmDetTurboCSPRepLayer(nn.Module):\n+    \"\"\"\n+    Cross Stage Partial (CSP) network layer with RepVGG blocks.\n+    \"\"\"\n+\n+    def __init__(self, config: OmDetTurboConfig):\n+        super().__init__()\n+\n+        in_channels = config.encoder_hidden_dim * 2\n+        out_channels = config.encoder_hidden_dim\n+        num_blocks = 3\n+        activation = config.csp_activation\n+\n+        hidden_channels = int(out_channels * config.hidden_expansion)\n+        self.conv1 = OmDetTurboConvNormLayer(config, in_channels, hidden_channels, 1, 1, activation=activation)\n+        self.conv2 = OmDetTurboConvNormLayer(config, in_channels, hidden_channels, 1, 1, activation=activation)\n+        self.bottlenecks = nn.Sequential(*[OmDetTurboRepVggBlock(config) for _ in range(num_blocks)])\n+        if hidden_channels != out_channels:\n+            self.conv3 = OmDetTurboConvNormLayer(config, hidden_channels, out_channels, 1, 1, activation=activation)\n+        else:\n+            self.conv3 = nn.Identity()\n+\n+    def forward(self, hidden_state):\n+        device = hidden_state.device\n+        hidden_state_1 = self.conv1(hidden_state)\n+        hidden_state_1 = self.bottlenecks(hidden_state_1).to(device)\n+        hidden_state_2 = self.conv2(hidden_state).to(device)\n+        return self.conv3(hidden_state_1 + hidden_state_2)\n+\n+\n+class OmDetTurboMultiheadAttention(nn.Module):\n+    \"\"\"Equivalent implementation of nn.MultiheadAttention with `batch_first=True`.\"\"\"\n+\n+    def __init__(self, config, hidden_size, num_attention_heads, dropout):\n+        super().__init__()\n+        if hidden_size % num_attention_heads != 0:\n+            raise ValueError(\n+                f\"The hidden size ({hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({num_attention_heads})\"\n+            )\n+        self.num_attention_heads = num_attention_heads\n+        self.attention_head_size = int(hidden_size / num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.query = nn.Linear(hidden_size, self.all_head_size)\n+        self.key = nn.Linear(hidden_size, self.all_head_size)\n+        self.value = nn.Linear(hidden_size, self.all_head_size)\n+        self.out_proj = nn.Linear(hidden_size, hidden_size)\n+        self.dropout = nn.Dropout(dropout)\n+\n+    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n+        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n+        x = x.view(new_x_shape)\n+        return x.permute(0, 2, 1, 3)\n+\n+    def forward(\n+        self,\n+        queries: torch.Tensor,\n+        keys: torch.Tensor,\n+        values: torch.Tensor,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor]:\n+        query_layer = self.transpose_for_scores(self.query(queries))\n+        key_layer = self.transpose_for_scores(self.key(keys))\n+        value_layer = self.transpose_for_scores(self.value(values))\n+\n+        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n+        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n+\n+        if attention_mask is not None:\n+            attention_scores = attention_scores + attention_mask\n+\n+        # Normalize the attention scores to probabilities.\n+        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+\n+        # This is actually dropping out entire tokens to attend to, which might\n+        # seem a bit unusual, but is taken from the original Transformer paper.\n+        attention_probs = self.dropout(attention_probs)\n+\n+        context_layer = torch.matmul(attention_probs, value_layer)\n+\n+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.view(new_context_layer_shape)\n+\n+        context_layer = self.out_proj(context_layer)\n+\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+\n+        return outputs\n+\n+\n+class OmDetTurboEncoderLayer(nn.Module):\n+    def __init__(self, config: OmDetTurboConfig):\n+        super().__init__()\n+        self.self_attn = OmDetTurboMultiheadAttention(\n+            config,\n+            hidden_size=config.encoder_hidden_dim,\n+            num_attention_heads=config.num_attention_heads,\n+            dropout=config.encoder_dropout,\n+        )\n+        self.self_attn_layer_norm = nn.LayerNorm(config.encoder_hidden_dim, eps=config.layer_norm_eps)\n+        self.dropout = nn.Dropout(config.encoder_dropout)\n+        self.activation_fn = ACT2FN[config.encoder_feedforward_activation]\n+        self.encoder_feedforward_dropout = nn.Dropout(config.encoder_feedforward_dropout)\n+        self.fc1 = nn.Linear(config.encoder_hidden_dim, config.encoder_dim_feedforward)\n+        self.fc2 = nn.Linear(config.encoder_dim_feedforward, config.encoder_hidden_dim)\n+        self.final_layer_norm = nn.LayerNorm(config.encoder_hidden_dim, eps=config.layer_norm_eps)\n+\n+    @staticmethod\n+    def with_pos_embed(tensor, pos_embed):\n+        return tensor if pos_embed is None else tensor + pos_embed\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        position_embeddings: torch.Tensor = None,\n+        output_attentions: bool = False,\n+    ):\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`): attention mask of size\n+                `(batch, 1, target_len, source_len)` where padding elements are indicated by very large negative\n+                values.\n+            position_embeddings (`torch.FloatTensor`, *optional*):\n+                Object queries (also called content embeddings), to be added to the hidden states.\n+            output_attentions (`bool`, *optional*, defaults to `False`):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+        \"\"\"\n+        residual = hidden_states\n+        query = key = self.with_pos_embed(hidden_states, position_embeddings)\n+\n+        hidden_states = self.self_attn(\n+            queries=query,\n+            keys=key,\n+            values=hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states, attentions = hidden_states if output_attentions else (hidden_states[0], None)\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = residual + hidden_states\n+        hidden_states = self.self_attn_layer_norm(hidden_states)\n+        residual = hidden_states\n+        hidden_states = self.activation_fn(self.fc1(hidden_states))\n+        hidden_states = self.encoder_feedforward_dropout(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = residual + hidden_states\n+        hidden_states = self.final_layer_norm(hidden_states)\n+        if self.training:\n+            if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n+                clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n+                hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n+\n+        if output_attentions:\n+            return hidden_states, attentions\n+\n+        return (hidden_states,)\n+\n+\n+class OmDetTurboEncoder(nn.Module):\n+    def __init__(self, config: OmDetTurboConfig):\n+        super().__init__()\n+\n+        self.layers = nn.ModuleList([OmDetTurboEncoderLayer(config) for _ in range(config.encoder_layers)])\n+\n+    def forward(\n+        self, src, src_mask=None, pos_embed=None, output_attentions: bool = False\n+    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]]]:\n+        hidden_states = src\n+        attention = () if output_attentions else None\n+        for layer in self.layers:\n+            hidden_states = layer(\n+                hidden_states,\n+                attention_mask=src_mask,\n+                position_embeddings=pos_embed,\n+                output_attentions=output_attentions,\n+            )\n+            if output_attentions:\n+                attention = attention + (hidden_states[1],)\n+            hidden_states = hidden_states[0]\n+\n+        return hidden_states, attention\n+\n+\n+class OmDetTurboHybridEncoder(nn.Module):\n+    \"\"\"\n+    Encoder consisting of channel projection layers, a set of `OmDetTurboEncoder`, a top-down Feature Pyramid Network\n+    (FPN) and a bottom-up Path Aggregation Network (PAN). More details on the paper: https://arxiv.org/abs/2304.08069\n+\n+    Args:\n+        config: OmDetTurboConfig\n+    \"\"\"\n+\n+    def __init__(self, config: OmDetTurboConfig):\n+        super().__init__()\n+        self.config = config\n+        self.in_channels = config.encoder_in_channels\n+        self.encoder_hidden_dim = config.encoder_hidden_dim\n+        self.encoder_projection_indices = config.encoder_projection_indices\n+        self.positional_encoding_temperature = config.positional_encoding_temperature\n+        self.eval_size = config.eval_size\n+        self.out_channels = [self.encoder_hidden_dim for _ in self.in_channels]\n+\n+        self.channel_projection_layers = nn.ModuleList()\n+        for in_channel in self.in_channels:\n+            self.channel_projection_layers.append(\n+                nn.Sequential(\n+                    nn.Conv2d(in_channel, self.encoder_hidden_dim, kernel_size=(1, 1), bias=False),\n+                    nn.BatchNorm2d(self.encoder_hidden_dim),\n+                )\n+            )\n+\n+        # encoder transformer\n+        self.encoder = nn.ModuleList([OmDetTurboEncoder(config) for _ in range(len(self.encoder_projection_indices))])\n+        # top-down fpn\n+        self.lateral_convs = nn.ModuleList()\n+        self.fpn_blocks = nn.ModuleList()\n+        for _ in range(len(self.in_channels) - 1, 0, -1):\n+            self.lateral_convs.append(\n+                OmDetTurboConvNormLayer(\n+                    config,\n+                    in_channels=self.encoder_hidden_dim,\n+                    out_channels=self.encoder_hidden_dim,\n+                    kernel_size=1,\n+                    stride=1,\n+                    activation=config.conv_norm_activation,\n+                )\n+            )\n+            self.fpn_blocks.append(OmDetTurboCSPRepLayer(config))\n+\n+        # bottom-up pan\n+        self.downsample_convs = nn.ModuleList()\n+        self.pan_blocks = nn.ModuleList()\n+        for _ in range(len(self.in_channels) - 1):\n+            self.downsample_convs.append(\n+                OmDetTurboConvNormLayer(\n+                    config,\n+                    in_channels=self.encoder_hidden_dim,\n+                    out_channels=self.encoder_hidden_dim,\n+                    kernel_size=3,\n+                    stride=2,\n+                    activation=config.conv_norm_activation,\n+                )\n+            )\n+            self.pan_blocks.append(OmDetTurboCSPRepLayer(config))\n+\n+    @staticmethod\n+    def build_2d_sincos_position_embedding(\n+        width, height, embed_dim=256, temperature=10000.0, device=\"cpu\", dtype=torch.float32\n+    ):\n+        grid_w = torch.arange(int(width), dtype=dtype, device=device)\n+        grid_h = torch.arange(int(height), dtype=dtype, device=device)\n+        grid_w, grid_h = torch.meshgrid(grid_w, grid_h, indexing=\"ij\")\n+        if embed_dim % 4 != 0:\n+            raise ValueError(\"Embed dimension must be divisible by 4 for 2D sin-cos position embedding\")\n+        pos_dim = embed_dim // 4\n+        omega = torch.arange(pos_dim, dtype=dtype, device=device) / pos_dim\n+        omega = 1.0 / (temperature**omega)\n+\n+        out_w = grid_w.flatten()[..., None] @ omega[None]\n+        out_h = grid_h.flatten()[..., None] @ omega[None]\n+\n+        return torch.concat([out_w.sin(), out_w.cos(), out_h.sin(), out_h.cos()], dim=1)[None, :, :]\n+\n+    def forward(\n+        self,\n+        inputs_embeddings=None,\n+        output_attentions=None,\n+        output_hidden_states=None,\n+        return_dict=None,\n+    ):\n+        r\"\"\"\n+        Args:\n+            inputs_embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Flattened feature map (output of the backbone + projection layers) that is passed to the encoder.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        hidden_states = inputs_embeddings\n+\n+        encoder_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+        # get projection features\n+        projected_features = [self.channel_projection_layers[i](feature) for i, feature in enumerate(hidden_states)]\n+        # encoder\n+        for encoder_layer_index, feature_to_project_index in enumerate(self.encoder_projection_indices):\n+            if output_hidden_states:\n+                encoder_states = encoder_states + (projected_features[feature_to_project_index],)\n+            height, width = projected_features[feature_to_project_index].shape[2:]\n+            # flatten [batch, channel, height, width] to [batch, height*width, channel]\n+            src_flatten = projected_features[feature_to_project_index].flatten(2).permute(0, 2, 1)\n+            if self.training or self.eval_size is None:\n+                pos_embed = self.build_2d_sincos_position_embedding(\n+                    width,\n+                    height,\n+                    self.encoder_hidden_dim,\n+                    self.positional_encoding_temperature,\n+                    device=src_flatten.device,\n+                    dtype=src_flatten.dtype,\n+                ).to(src_flatten.device, src_flatten.dtype)\n+            else:\n+                pos_embed = None\n+            layer_outputs = self.encoder[encoder_layer_index](\n+                src_flatten,\n+                pos_embed=pos_embed,\n+                output_attentions=output_attentions,\n+            )\n+            projected_features[feature_to_project_index] = (\n+                layer_outputs[0].permute(0, 2, 1).reshape(-1, self.encoder_hidden_dim, height, width).contiguous()\n+            )\n+\n+            if output_attentions:\n+                all_attentions = all_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            encoder_states = encoder_states + (projected_features[feature_to_project_index],)\n+\n+        # Feature Pyramid Network (FPN)\n+        fpn_feature_maps = [projected_features[-1]]\n+        for idx in range(len(self.in_channels) - 1, 0, -1):\n+            feat_high = fpn_feature_maps[0]\n+            feat_low = projected_features[idx - 1]\n+            feat_high = self.lateral_convs[len(self.in_channels) - 1 - idx](feat_high)\n+            fpn_feature_maps[0] = feat_high\n+            upsample_feat = F.interpolate(feat_high, scale_factor=2.0, mode=\"nearest\")\n+            fps_map = self.fpn_blocks[len(self.in_channels) - 1 - idx](torch.concat([upsample_feat, feat_low], dim=1))\n+            fpn_feature_maps.insert(0, fps_map)\n+\n+        # Path Aggregation Network (PAN)\n+        fpn_states = [fpn_feature_maps[0]]\n+        for idx in range(len(self.in_channels) - 1):\n+            feat_low = fpn_states[-1]\n+            feat_high = fpn_feature_maps[idx + 1]\n+            downsample_feat = self.downsample_convs[idx](feat_low)\n+            hidden_states = self.pan_blocks[idx](\n+                torch.concat([downsample_feat, feat_high.to(downsample_feat.device)], dim=1)\n+            )\n+            fpn_states.append(hidden_states)\n+        if not return_dict:\n+            return (fpn_states[-1], encoder_states, all_attentions, fpn_states)\n+        return OmDetTurboEncoderOutput(\n+            last_hidden_state=fpn_states[-1],\n+            hidden_states=encoder_states,\n+            attentions=all_attentions,\n+            extracted_states=fpn_states,\n+        )\n+\n+\n+class OmDetTurboMLPWithDropout(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.linear1 = nn.Linear(config.class_embed_dim, config.task_encoder_hidden_dim)\n+        self.activation = ACT2FN[config.decoder_activation]\n+        self.dropout = nn.Dropout(config.decoder_dropout)\n+        self.linear2 = nn.Linear(config.task_encoder_hidden_dim, config.class_embed_dim)\n+\n+    def forward(self, x):\n+        return self.linear2(self.dropout(self.activation(self.linear1(x))))\n+\n+\n+class OmDetTurboMLP(nn.Module):\n+    \"\"\"Very simple multi-layer perceptron (also called FFN)\"\"\"\n+\n+    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n+        super().__init__()\n+        self.num_layers = num_layers\n+        hidden_layers_dims = [hidden_dim] * (num_layers - 1)\n+        layers_dims = [input_dim] + hidden_layers_dims + [output_dim]\n+        self.layers = nn.ModuleList(\n+            [nn.Linear(in_dim, out_dim) for in_dim, out_dim in zip(layers_dims[:-1], layers_dims[1:])]\n+        )\n+\n+    def forward(self, x):\n+        for i, layer in enumerate(self.layers):\n+            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n+        return x\n+\n+\n+class OmDetTurboResidualLayer(nn.Module):\n+    \"\"\"\n+    A residual connection followed by a layer norm.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.norm1 = nn.LayerNorm(config.class_embed_dim, eps=config.layer_norm_eps)\n+        self.dropout = nn.Dropout(config.decoder_dropout)\n+\n+    def forward(self, x, y):\n+        return self.norm1(x + self.dropout(y))\n+\n+\n+class OmDetTurboTaskEncoder(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.mlp = OmDetTurboMLPWithDropout(config)\n+        self.res1 = OmDetTurboResidualLayer(config)\n+\n+    def forward(self, x):\n+        mlp_out = self.mlp(x)\n+        x = self.res1(x, mlp_out)\n+        return x\n+\n+\n+class OmDetTurboDeformableTransformerDecoderLayer(nn.Module):\n+    \"\"\"\n+    A single layer of the Deformable Transformer Decoder.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        # self attention\n+        self.self_attn = OmDetTurboMultiheadAttention(\n+            config,\n+            hidden_size=config.decoder_hidden_dim,\n+            num_attention_heads=config.decoder_num_heads,\n+            dropout=config.decoder_dropout,\n+        )\n+        self.dropout1 = nn.Dropout(config.decoder_dropout)\n+        self.norm1 = nn.LayerNorm(config.decoder_hidden_dim, eps=config.layer_norm_eps)\n+\n+        # cross attention\n+        self.cross_attn = OmDetTurboMultiscaleDeformableAttention(\n+            config, num_heads=config.decoder_num_heads, n_points=config.decoder_num_points\n+        )\n+        self.dropout2 = nn.Dropout(config.decoder_dropout)\n+        self.norm2 = nn.LayerNorm(config.decoder_hidden_dim, eps=config.layer_norm_eps)\n+\n+        # feed forward network\n+        self.linear1 = nn.Linear(config.decoder_hidden_dim, config.decoder_dim_feedforward)\n+        self.act = ACT2FN[config.decoder_activation]\n+        self.dropout3 = nn.Dropout(config.decoder_dropout)\n+        self.linear2 = nn.Linear(config.decoder_dim_feedforward, config.decoder_hidden_dim)\n+        self.dropout4 = nn.Dropout(config.decoder_dropout)\n+        self.norm3 = nn.LayerNorm(config.decoder_hidden_dim, eps=config.layer_norm_eps)\n+\n+        self.output_attentions = config.output_attentions\n+        self.output_hidden_states = config.output_hidden_states\n+\n+    @staticmethod\n+    def with_pos_embed(tensor, pos):\n+        return tensor if pos is None else tensor + pos\n+\n+    def forward(\n+        self,\n+        decoder_embeddings,\n+        task_features,\n+        reference_points,\n+        vision_features,\n+        vision_shapes,\n+        vision_shapes_list,\n+        level_start_index=None,\n+        attention_mask=None,\n+        padding_mask=None,\n+        query_position=None,\n+        output_attentions=None,\n+        output_hidden_states=None,\n+    ):\n+        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n+        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n+\n+        origin_embedding_len = decoder_embeddings.shape[1]\n+\n+        # self attention\n+        query = key = self.with_pos_embed(decoder_embeddings, query_position)\n+        # combine task_features with query, key, value\n+        task_features = task_features.transpose(0, 1)\n+        query = torch.cat((query, task_features), dim=1)\n+        key = torch.cat((key, task_features), dim=1)\n+        decoder_embeddings = torch.cat((decoder_embeddings, task_features), dim=1)\n+\n+        outputs = self.self_attn(\n+            query,\n+            key,\n+            decoder_embeddings,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        context, self_attention = outputs if output_attentions else (outputs[0], None)\n+        decoder_embeddings = decoder_embeddings + self.dropout1(context)\n+        decoder_embeddings = self.norm1(decoder_embeddings)\n+\n+        task_features = decoder_embeddings[:, origin_embedding_len:, :].transpose(0, 1)\n+        decoder_embeddings = decoder_embeddings[:, :origin_embedding_len, :]\n+\n+        # cross attention\n+        hidden_states = self.with_pos_embed(decoder_embeddings, query_position)\n+        reference_points = reference_points.unsqueeze(2)\n+        outputs, cross_attention = self.cross_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=padding_mask,\n+            encoder_hidden_states=vision_features,\n+            reference_points=reference_points,\n+            spatial_shapes=vision_shapes,\n+            spatial_shapes_list=vision_shapes_list,\n+            level_start_index=level_start_index,\n+        )\n+        decoder_embeddings = decoder_embeddings + self.dropout2(outputs)\n+        residual = self.norm2(decoder_embeddings)\n+\n+        # feed forward network\n+        decoder_embeddings = self.linear2(self.dropout3(self.act(self.linear1(residual))))\n+        decoder_embeddings = residual + self.dropout4(decoder_embeddings)\n+        decoder_embeddings = self.norm3(decoder_embeddings)\n+\n+        return (\n+            decoder_embeddings,\n+            task_features,\n+            self_attention if output_attentions else None,\n+            cross_attention if output_attentions else None,\n+        )\n+\n+\n+class OmDetTurboPreTrainedModel(PreTrainedModel):\n+    config_class = OmDetTurboConfig\n+    base_model_prefix = \"model\"\n+    main_input_name = \"pixel_values\"\n+\n+    def _init_weights(self, module):\n+        def linear_init_(module_to_init):\n+            bound = 1 / math.sqrt(module_to_init.weight.shape[0])\n+            nn.init.uniform_(module_to_init.weight, -bound, bound)\n+            if hasattr(module_to_init, \"bias\") and module_to_init.bias is not None:\n+                nn.init.uniform_(module_to_init.bias, -bound, bound)\n+\n+        if isinstance(module, OmDetTurboEncoderLayer):\n+            linear_init_(module.fc1)\n+            linear_init_(module.fc2)\n+        elif isinstance(module, OmDetTurboDecoder):\n+            nn.init.constant_(module.encoder_bbox_head.layers[-1].weight, 0.0)\n+            nn.init.constant_(module.encoder_bbox_head.layers[-1].bias, 0.0)\n+            for mlp in module.decoder_bbox_head:\n+                nn.init.constant_(mlp.layers[-1].weight, 0.0)\n+                nn.init.constant_(mlp.layers[-1].bias, 0.0)\n+            linear_init_(module.encoder_vision_features[0])\n+            nn.init.xavier_uniform_(module.encoder_vision_features[0].weight)\n+            if module.learn_initial_query:\n+                nn.init.xavier_uniform_(module.tgt_embed.weight)\n+            nn.init.xavier_uniform_(module.query_position_head.layers[0].weight)\n+            nn.init.xavier_uniform_(module.query_position_head.layers[1].weight)\n+            for layer in module.channel_projection_layers:\n+                nn.init.xavier_uniform_(layer[0].weight)\n+        elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n+            module.weight.data.normal_(mean=0.0, std=self.config.init_std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+\n+    def _set_gradient_checkpointing(self, module, value=False):\n+        if isinstance(module, OmDetTurboDecoder):\n+            module.gradient_checkpointing = value\n+\n+    @staticmethod\n+    def _get_cache_key_at_index(input_ids, attention_mask, index):\n+        input_ids = input_ids[index]\n+        input_mask = attention_mask[index]\n+        cache_key = tuple(input_ids[input_mask != 0].tolist())\n+        return cache_key\n+\n+    def get_cached_class_embeddings(self, classes_input_ids, classes_attention_mask):\n+        not_cached_index = []\n+        not_cached_classes = []\n+        total_embeddings = []\n+        for idx, _ in enumerate(classes_input_ids):\n+            cache_key = self._get_cache_key_at_index(classes_input_ids, classes_attention_mask, idx)\n+            if self.language_cache_class.has(cache_key):\n+                total_embeddings.append(self.language_cache_class.get(cache_key))\n+            else:\n+                total_embeddings.append(None)\n+                not_cached_index.append(idx)\n+                not_cached_classes.append(cache_key)\n+\n+        if not_cached_classes:\n+            not_cached_classes_ids = torch.stack([classes_input_ids[idx] for idx in not_cached_index])\n+            embeddings = self.language_backbone(not_cached_classes_ids, encode_type=\"class\")\n+            for idx, emb in enumerate(embeddings):\n+                idx_to_put = not_cached_index[idx]\n+                total_embeddings[idx_to_put] = emb\n+                self.language_cache_class.put(not_cached_classes[idx], emb)\n+\n+        total_class_embs = torch.stack(total_embeddings).to(self.device)\n+        return total_class_embs\n+\n+    def get_cached_task_embeddings(self, tasks_input_ids, tasks_attention_mask):\n+        not_cached_index = []\n+        not_cached_tasks = []\n+        total_task_features = []\n+        total_task_masks = []\n+        for idx, _ in enumerate(tasks_input_ids):\n+            cache_key = self._get_cache_key_at_index(tasks_input_ids, tasks_attention_mask, idx)\n+            if self.language_cache_prompt.has(cache_key):\n+                task_feature, task_mask = self.language_cache_prompt.get(cache_key)\n+                total_task_features.append(task_feature)\n+                total_task_masks.append(task_mask)\n+            else:\n+                total_task_features.append(None)\n+                total_task_masks.append(None)\n+                not_cached_index.append(idx)\n+                not_cached_tasks.append(cache_key)\n+\n+        if not_cached_tasks:\n+            not_cached_index_ids = torch.stack([tasks_input_ids[idx] for idx in not_cached_index])\n+            not_cached_mask = torch.stack([tasks_attention_mask[idx] for idx in not_cached_index])\n+            embeddings, masks = self.language_backbone(not_cached_index_ids, mask=not_cached_mask, encode_type=\"task\")\n+\n+            for idx in range(embeddings.shape[1]):\n+                emb = embeddings[:, [idx], :]\n+                idx_to_put = not_cached_index[idx]\n+                cur_mask = torch.unsqueeze(masks[idx], dim=0).to(self.device)\n+                total_task_features[idx_to_put] = emb\n+                total_task_masks[idx_to_put] = cur_mask\n+                self.language_cache_prompt.put(not_cached_tasks[idx], (emb, cur_mask))\n+\n+        # pad before concat if needed\n+        max_len = max([task.shape[0] for task in total_task_features])\n+        for idx, task in enumerate(total_task_features):\n+            if task.shape[0] < max_len:\n+                pad_size = max_len - task.shape[0]\n+                total_task_features[idx] = F.pad(task, (0, 0, 0, 0, 0, pad_size))\n+                total_task_masks[idx] = F.pad(total_task_masks[idx], (0, pad_size))\n+\n+        total_task_features = torch.cat(total_task_features, dim=1).to(self.device)\n+        total_task_masks = torch.cat(total_task_masks, dim=0).to(self.device)\n+\n+        return total_task_features, total_task_masks\n+\n+    def get_language_embedding(\n+        self,\n+        classes_input_ids,\n+        classes_attention_mask,\n+        tasks_input_ids,\n+        tasks_attention_mask,\n+        classes_structure,\n+    ):\n+        batched_classes_embeddings = self.get_cached_class_embeddings(classes_input_ids, classes_attention_mask)\n+        # regroup class embeddings using saved structure\n+        max_class_size = torch.max(classes_structure)\n+        class_embeddings_regrouped = []\n+        start = 0\n+        for size in classes_structure:\n+            pad_size = max_class_size - size\n+            class_embeddings_regrouped.append(\n+                F.pad(batched_classes_embeddings[start : start + size], (0, 0, 0, pad_size)).unsqueeze(1)\n+            )\n+            start += size\n+        class_embeddings = torch.cat(class_embeddings_regrouped, dim=1)\n+\n+        task_embeddings, task_mask = self.get_cached_task_embeddings(tasks_input_ids, tasks_attention_mask)\n+\n+        return class_embeddings, task_embeddings, task_mask\n+\n+\n+OMDET_TURBO_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`OmDetTurboConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+OMDET_TURBO_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Padding will be ignored by default should you provide it.\n+\n+            Pixel values can be obtained using [`AutoImageProcessor`]. See [`DetrImageProcessor.__call__`] for\n+            details.\n+\n+        classes_input_ids (`torch.LongTensor` of shape `(total_classes (>= batch_size), sequence_length)`):\n+            Indices of input classes sequence tokens in the vocabulary of the language model.\n+            Several classes can be provided for each tasks, thus the tokenized classes are flattened\n+            and the structure of the classes is provided in the `classes_structure` argument.\n+\n+            Indices can be obtained using [`OmDetTurboProcessor`]. See [`OmDetTurboProcessor.__call__`] for\n+            details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+\n+        classes_attention_mask (`torch.BoolTensor` of shape `(total_classes (>= batch_size), num_classes, sequence_length)`):\n+            Attention mask for the classes. This is a binary mask that indicates which tokens should be attended to,\n+            and which should not.\n+\n+        tasks_input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input tasks sequence tokens in the vocabulary of the language model.\n+\n+            Indices can be obtained using [`OmDetTurboProcessor`]. See [`OmDetTurboProcessor.__call__`] for\n+            details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+\n+        tasks_attention_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)`):\n+            Attention mask for the tasks. This is a binary mask that indicates which tokens should be attended to,\n+            and which should not.\n+\n+        classes_structure (torch.LongTensor of shape `(batch_size)`):\n+            Structure of the classes. This tensor indicates the number of classes for each task.\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\n+        \"\"\"\n+\n+\n+def _cosine_similarity_scaled(a, b, logit_scale):\n+    a = a / a.norm(dim=2, keepdim=True).clamp_min(1e-12)\n+    b = b / b.norm(dim=1, keepdim=True).clamp_min(1e-12)\n+    logit_scale = logit_scale.exp()\n+    logits_per_image = logit_scale * torch.bmm(a, b)\n+    return logits_per_image\n+\n+\n+def get_class_similarity(class_distance_type, cls_feature, class_proj):\n+    logit_scale = torch.tensor(1 / 0.07).log()\n+    if class_distance_type == \"cosine\":\n+        class_logits = _cosine_similarity_scaled(cls_feature, class_proj, logit_scale)\n+    elif class_distance_type == \"dot\":\n+        class_logits = torch.bmm(cls_feature, class_proj)\n+    else:\n+        raise Exception(\"Unknown class_distance_type {}\".format(class_distance_type))\n+    return class_logits\n+\n+\n+def _inverse_sigmoid(x, eps=1e-5):\n+    x = x.clamp(min=0, max=1)\n+    x1 = x.clamp(min=eps)\n+    x2 = (1 - x).clamp(min=eps)\n+    return torch.log(x1 / x2)\n+\n+\n+class OmDetTurboDecoder(OmDetTurboPreTrainedModel):\n+    def __init__(self, config: OmDetTurboConfig):\n+        self.config = config\n+        super().__init__(config)\n+        self.gradient_checkpointing = False\n+\n+        hidden_dim = config.decoder_hidden_dim\n+        self.num_queries = config.num_queries\n+        self.class_distance_type = config.class_distance_type\n+        self.learn_initial_query = config.learn_initial_query\n+\n+        # backbone feature projection\n+        self.channel_projection_layers = nn.ModuleList(\n+            nn.Sequential(nn.Conv2d(x, hidden_dim, 1, bias=False), nn.BatchNorm2d(hidden_dim))\n+            for x in config.vision_features_channels\n+        )\n+        self.task_encoder = OmDetTurboTaskEncoder(config)\n+        if config.class_embed_dim != hidden_dim:\n+            self.task_project = nn.Linear(config.class_embed_dim, hidden_dim)\n+\n+        # Transformer module\n+        self.layers = nn.ModuleList(\n+            [OmDetTurboDeformableTransformerDecoderLayer(config) for _ in range(config.decoder_num_layers)]\n+        )\n+        self.decoder_num_layers = config.decoder_num_layers\n+        # decoder embedding\n+        if self.learn_initial_query:\n+            self.tgt_embed = nn.Embedding(self.num_queries, hidden_dim)\n+        self.query_position_head = OmDetTurboMLP(\n+            input_dim=4, hidden_dim=2 * hidden_dim, output_dim=hidden_dim, num_layers=2\n+        )\n+\n+        # encoder head\n+        self.encoder_vision_features = nn.Sequential(\n+            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim, eps=config.layer_norm_eps)\n+        )\n+        self.encoder_class_head = nn.Linear(config.class_embed_dim, hidden_dim)\n+        self.encoder_bbox_head = OmDetTurboMLP(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=4, num_layers=3)\n+\n+        # decoder head\n+        self.decoder_class_head = nn.ModuleList(\n+            [nn.Linear(config.class_embed_dim, hidden_dim) for _ in range(config.decoder_num_layers)]\n+        )\n+        self.decoder_bbox_head = nn.ModuleList(\n+            [OmDetTurboMLP(hidden_dim, hidden_dim, 4, num_layers=3) for _ in range(config.decoder_num_layers)]\n+        )\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @lru_cache(maxsize=32)\n+    def generate_anchors(self, spatial_shapes=None, grid_size=0.05, device=\"cpu\", dtype=torch.float32):\n+        # We always generate anchors in float32 to preserve equivalence between\n+        # dynamic and static anchor inference\n+        # Ignore copy\n+        if spatial_shapes is None:\n+            raise ValueError(\"spatial_shapes must be provided\")\n+\n+        anchors = []\n+        for level, (height, width) in enumerate(spatial_shapes):\n+            grid_y, grid_x = torch.meshgrid(\n+                torch.arange(end=height, dtype=dtype, device=device),\n+                torch.arange(end=width, dtype=dtype, device=device),\n+                indexing=\"ij\",\n+            )\n+            grid_xy = torch.stack([grid_x, grid_y], -1)\n+            valid_wh = torch.tensor([width, height], dtype=dtype, device=device)\n+            grid_xy = (grid_xy.unsqueeze(0) + 0.5) / valid_wh\n+            wh = torch.ones_like(grid_xy, dtype=dtype, device=device) * grid_size * (2.0**level)\n+            anchors.append(torch.concat([grid_xy, wh], -1).reshape(-1, height * width, 4))\n+        # define the valid range for anchor coordinates\n+        eps = 1e-2\n+        anchors = torch.concat(anchors, 1)\n+        valid_mask = ((anchors > eps) * (anchors < 1 - eps)).all(-1, keepdim=True)\n+        anchors = torch.log(anchors / (1 - anchors))\n+        anchors = torch.where(valid_mask, anchors, torch.inf)\n+\n+        return anchors, valid_mask\n+\n+    def _get_encoder_input(self, vision_features):\n+        # get projection features\n+        vision_features = [self.channel_projection_layers[i](feat) for i, feat in enumerate(vision_features)]\n+        # get encoder inputs\n+        new_vision_features = []\n+        new_vision_shapes_list = []\n+        for feat in vision_features:\n+            height, width = feat.shape[2:]\n+            # [batch_size, channels, height, width] -> [batch_size, height*width, channels]\n+            new_vision_features.append(feat.flatten(2).permute(0, 2, 1))\n+            # [num_feature_levels, 2]\n+            new_vision_shapes_list.append((height, width))\n+\n+        # [batch_size, height*width, channels]\n+        new_vision_features = torch.cat(new_vision_features, 1)\n+        new_vision_shapes = torch.tensor(new_vision_shapes_list, dtype=torch.int64).to(vision_features[0].device)\n+        level_start_index = torch.cat((new_vision_shapes.new_zeros((1,)), new_vision_shapes.prod(1).cumsum(0)[:-1]))\n+\n+        return new_vision_features, new_vision_shapes, new_vision_shapes_list, level_start_index\n+\n+    def _get_decoder_input(\n+        self, vision_features, vision_shapes, class_features, denoise_embeddings=None, denoise_bboxes=None\n+    ):\n+        batch_size = len(vision_features)\n+        # prepare input for decoder\n+        anchors, valid_mask = self.generate_anchors(\n+            vision_shapes, device=vision_features.device, dtype=vision_features.dtype\n+        )\n+        predicted_class_features = self.encoder_vision_features(\n+            torch.where(\n+                valid_mask, vision_features, torch.tensor(0.0, dtype=vision_features.dtype).to(vision_features.device)\n+            )\n+        )\n+\n+        original_class_projected = self.encoder_class_head(class_features).permute(1, 2, 0)\n+        encoder_class_similarity = get_class_similarity(\n+            self.class_distance_type, predicted_class_features, original_class_projected\n+        )\n+\n+        # dynamic anchors + static content\n+        # (batch_size, height*width, 4)\n+        encoder_outputs_bboxes = self.encoder_bbox_head(predicted_class_features) + anchors\n+\n+        # query selection\n+        # (batch_size, num_queries)\n+        topk_ind = torch.topk(encoder_class_similarity.max(-1).values, self.num_queries, dim=1).indices.view(-1)\n+        # (batch_size, num_queries)\n+        batch_ind = (\n+            torch.arange(end=batch_size, dtype=topk_ind.dtype, device=topk_ind.device)\n+            .unsqueeze(-1)\n+            .repeat(1, self.num_queries)\n+            .view(-1)\n+        )\n+\n+        reference_points = encoder_outputs_bboxes[batch_ind, topk_ind].view(batch_size, self.num_queries, -1)\n+        encoder_bboxes = reference_points.sigmoid()\n+        if denoise_bboxes is not None:\n+            reference_points = torch.cat([denoise_bboxes, reference_points], 1)\n+        if self.training:\n+            reference_points = reference_points.detach()\n+        encoder_class_similarity = encoder_class_similarity[batch_ind, topk_ind].view(batch_size, self.num_queries, -1)\n+\n+        if self.learn_initial_query:\n+            embeddings = self.tgt_embed.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n+        else:\n+            embeddings = predicted_class_features[batch_ind, topk_ind].view(batch_size, self.num_queries, -1)\n+            if self.training:\n+                embeddings = embeddings.detach()\n+        if denoise_embeddings is not None:\n+            embeddings = torch.cat([denoise_embeddings, embeddings], 1)\n+\n+        return embeddings, reference_points, encoder_bboxes, encoder_class_similarity, anchors\n+\n+    def forward(\n+        self,\n+        vision_features,\n+        class_features,\n+        task_features,\n+        task_mask,\n+        output_attentions=None,\n+        output_hidden_states=None,\n+        return_dict=None,\n+    ):\n+        \"\"\"\n+        Args:\n+            vision_features (`torch.FloatTensor`): The sequence of vision features. shape depends on the vision\n+                backbone.\n+            class_features (`torch.FloatTensor`): The sequence of class features of shape\n+                `(class_sequence_length, batch_size, class_embed_dim)`.\n+            task_features (`torch.FloatTensor`): The sequence of task features of shape\n+                `(task_sequence_length, batch_size, decoder_hidden_dim)`.\n+            task_mask (`torch.LongTensor`): The mask for the task features of shape `(batch_size, task_sequence_length)`.\n+            output_attentions (`bool`, *optional*): Whether or not to return the attentions tensors of all attention\n+                layers. See `attentions` under returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*): Whether or not to return the hidden states of all layers. See\n+                `hidden_states` under returned tensors for more detail.\n+            return_dict (`bool`, *optional*): Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain\n+                tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        vision_features, vision_shapes, vision_shapes_list, level_start_index = self._get_encoder_input(\n+            vision_features\n+        )\n+\n+        # todo add denoising for training\n+        denoise_embeddings, denoise_bboxes, key_padding_mask = None, None, None\n+        batch_size = task_mask.shape[0]\n+\n+        # compose attn_mask for vision_emb and task_emb fusion\n+        task_features = self.task_encoder(task_features)\n+        if self.task_project is not None:\n+            task_features = self.task_project(task_features)\n+        src_key_mask = (task_mask == 0).detach()\n+        attn_mask_len = self.num_queries\n+        fusion_size = attn_mask_len + task_features.shape[0]\n+        key_padding_mask = torch.zeros([batch_size, fusion_size], dtype=torch.bool).to(task_features.device)\n+        key_padding_mask[:, attn_mask_len:] = src_key_mask\n+        attention_mask = _prepare_4d_attention_mask(~key_padding_mask, dtype=vision_features.dtype)\n+        decoder_embeddings, reference_points, encoder_bboxes, encoder_class_similarity, init_reference_points = (\n+            self._get_decoder_input(\n+                vision_features, tuple(vision_shapes_list), class_features, denoise_embeddings, denoise_bboxes\n+            )\n+        )\n+\n+        all_hidden_states = () if output_hidden_states else None\n+        all_attns = () if output_attentions else None\n+        all_self_attns = () if output_attentions else None\n+        all_cross_attns = () if output_attentions else None\n+        predicted_class_features = decoder_embeddings\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (predicted_class_features,)\n+        decoder_bboxes = []\n+        decoder_classes = []\n+        last_refined_bbox = None\n+        reference_points = reference_points.sigmoid()\n+        for i, layer in enumerate(self.layers):\n+            if self.gradient_checkpointing and self.training:\n+                predicted_class_features, task_features, self_attention, cross_attention = (\n+                    self._gradient_checkpointing_func(\n+                        layer.__call__,\n+                        predicted_class_features,\n+                        task_features,\n+                        reference_points,\n+                        vision_features,\n+                        vision_shapes,\n+                        vision_shapes_list,\n+                        level_start_index=level_start_index,\n+                        attention_mask=attention_mask,\n+                        query_position=self.query_position_head(reference_points),\n+                        output_attentions=output_attentions,\n+                        output_hidden_states=output_hidden_states,\n+                    )\n+                )\n+            else:\n+                predicted_class_features, task_features, self_attention, cross_attention = layer(\n+                    predicted_class_features,\n+                    task_features,\n+                    reference_points,\n+                    vision_features,\n+                    vision_shapes,\n+                    vision_shapes_list,\n+                    level_start_index=level_start_index,\n+                    attention_mask=attention_mask,\n+                    query_position=self.query_position_head(reference_points),\n+                    output_attentions=output_attentions,\n+                    output_hidden_states=output_hidden_states,\n+                )\n+            if output_attentions:\n+                all_self_attns = all_self_attns + (self_attention,)\n+                all_cross_attns = all_cross_attns + (cross_attention,)\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (predicted_class_features,)\n+\n+            refined_bbox = torch.sigmoid(\n+                self.decoder_bbox_head[i](predicted_class_features) + _inverse_sigmoid(reference_points)\n+            )\n+            original_class_projected = self.decoder_class_head[i](class_features).permute(1, 2, 0)\n+            if self.training:\n+                decoder_classes.append(\n+                    get_class_similarity(\n+                        class_distance_type=self.class_distance_type,\n+                        cls_feature=predicted_class_features,\n+                        class_proj=original_class_projected,\n+                    )\n+                )\n+                if i == 0:\n+                    decoder_bboxes.append(refined_bbox)\n+                else:\n+                    decoder_bboxes.append(\n+                        torch.sigmoid(\n+                            self.decoder_bbox_head[i](predicted_class_features) + _inverse_sigmoid(last_refined_bbox)\n+                        )\n+                    )\n+            elif i == self.decoder_num_layers - 1:\n+                decoder_classes.append(\n+                    get_class_similarity(self.class_distance_type, predicted_class_features, original_class_projected)\n+                )\n+                decoder_bboxes.append(refined_bbox)\n+                break\n+            last_refined_bbox = refined_bbox\n+            reference_points = refined_bbox.detach() if self.training else refined_bbox\n+        if output_attentions:\n+            all_attns += (all_self_attns, all_cross_attns)\n+\n+        last_hidden_state = predicted_class_features\n+        decoder_bboxes = torch.stack(decoder_bboxes)\n+        decoder_classes = torch.stack(decoder_classes)\n+\n+        if not return_dict:\n+            return (\n+                last_hidden_state,\n+                all_hidden_states,\n+                all_attns,\n+                decoder_bboxes,\n+                decoder_classes,\n+                encoder_bboxes,\n+                encoder_class_similarity,\n+                init_reference_points,\n+                reference_points,\n+            )\n+\n+        return OmDetTurboDecoderOutput(\n+            last_hidden_state=last_hidden_state,\n+            hidden_states=all_hidden_states,\n+            attentions=all_attns,\n+            decoder_coords=decoder_bboxes,\n+            decoder_classes=decoder_classes,\n+            encoder_coord_logits=encoder_bboxes,\n+            encoder_class_logits=encoder_class_similarity,\n+            init_reference_points=init_reference_points,\n+            intermediate_reference_points=reference_points,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    OmDetTurbo Model (consisting of a vision and a text backbone, and encoder-decoder architecture) outputting\n+    bounding boxes and classes scores for tasks such as COCO detection.\n+    \"\"\",\n+    OMDET_TURBO_START_DOCSTRING,\n+)\n+class OmDetTurboForObjectDetection(OmDetTurboPreTrainedModel):\n+    def __init__(self, config: OmDetTurboConfig):\n+        super().__init__(config)\n+        self.vision_backbone = OmDetTurboVisionBackbone(config)\n+        self.language_backbone = OmDetTurboLanguageBackbone(config)\n+        self.encoder = OmDetTurboHybridEncoder(config)\n+        self.decoder = OmDetTurboDecoder(config)\n+        self.num_queries = config.num_queries\n+\n+        self.language_cache_class = OmDetTurboLRUCache(config.cache_size)\n+        self.language_cache_prompt = OmDetTurboLRUCache(config.cache_size)\n+        self.vocab_size = config.text_config.vocab_size\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_backbone.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_backbone.model.set_input_embeddings(value)\n+\n+    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None) -> nn.Embedding:\n+        model_embeds = self.language_backbone.model.resize_token_embeddings(\n+            new_num_tokens=new_num_tokens, pad_to_multiple_of=pad_to_multiple_of\n+        )\n+        self.config.text_config.vocab_size = model_embeds.num_embeddings\n+        self.vocab_size = model_embeds.num_embeddings\n+        return model_embeds\n+\n+    @add_start_docstrings_to_model_forward(OMDET_TURBO_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=OmDetTurboObjectDetectionOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        pixel_values: Tensor,\n+        classes_input_ids: Tensor,\n+        classes_attention_mask: Tensor,\n+        tasks_input_ids: Tensor,\n+        tasks_attention_mask: Tensor,\n+        classes_structure: Tensor,\n+        labels: Optional[Tensor] = None,\n+        output_attentions=None,\n+        output_hidden_states=None,\n+        return_dict=None,\n+    ) -> Union[Tuple[torch.FloatTensor], OmDetTurboObjectDetectionOutput]:\n+        r\"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> import requests\n+        >>> from PIL import Image\n+\n+        >>> from transformers import AutoProcessor, OmDetTurboForObjectDetection\n+\n+        >>> processor = AutoProcessor.from_pretrained(\"omlab/omdet-turbo-tiny\")\n+        >>> model = OmDetTurboForObjectDetection.from_pretrained(\"omlab/omdet-turbo-tiny\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> classes = [\"cat\", \"remote\"]\n+        >>> task = \"Detect {}.\".format(\", \".join(classes))\n+        >>> inputs = processor(image, text=classes, task=task, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+\n+        >>> # convert outputs (bounding boxes and class logits)\n+        >>> results = processor.post_process_grounded_object_detection(\n+        ...     outputs,\n+        ...     classes=classes,\n+        ...     target_sizes=[image.size[::-1]],\n+        ...     score_threshold=0.3,\n+        ...     nms_threshold=0.3,\n+        >>> )[0]\n+        >>> for score, class_name, box in zip(results[\"scores\"], results[\"classes\"], results[\"boxes\"]):\n+        ...     box = [round(i, 1) for i in box.tolist()]\n+        ...     print(\n+        ...         f\"Detected {class_name} with confidence \"\n+        ...         f\"{round(score.item(), 2)} at location {box}\"\n+        ...     )\n+        Detected remote with confidence 0.76 at location [39.9, 71.3, 176.5, 117.9]\n+        Detected cat with confidence 0.72 at location [345.1, 22.5, 639.7, 371.9]\n+        Detected cat with confidence 0.65 at location [12.7, 53.8, 315.5, 475.3]\n+        Detected remote with confidence 0.57 at location [333.4, 75.6, 370.7, 187.0]\n+        ```\"\"\"\n+        if labels is not None:\n+            raise NotImplementedError(\"Training is not implemented yet\")\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        loss = None\n+        image_features = self.vision_backbone(pixel_values)\n+        encoder_outputs = self.encoder(\n+            image_features,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        class_features, task_features, task_mask = self.get_language_embedding(\n+            classes_input_ids,\n+            classes_attention_mask,\n+            tasks_input_ids,\n+            tasks_attention_mask,\n+            classes_structure,\n+        )\n+        encoder_extracted_states = encoder_outputs.extracted_states if return_dict else encoder_outputs[-1]\n+        decoder_outputs = self.decoder(\n+            encoder_extracted_states,\n+            class_features,\n+            task_features,\n+            task_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        if not return_dict:\n+            return tuple(\n+                output\n+                for output in [\n+                    loss,\n+                    decoder_outputs[3][-1],\n+                    decoder_outputs[4][-1],\n+                    decoder_outputs[7],\n+                    decoder_outputs[8],\n+                    decoder_outputs[5],\n+                    decoder_outputs[6],\n+                    encoder_outputs[-1],\n+                    decoder_outputs[1],\n+                    decoder_outputs[2],\n+                    encoder_outputs[1],\n+                    encoder_outputs[2],\n+                ]\n+                if output is not None\n+            )\n+\n+        return OmDetTurboObjectDetectionOutput(\n+            loss=loss,\n+            decoder_coord_logits=decoder_outputs.decoder_coords[-1],\n+            decoder_class_logits=decoder_outputs.decoder_classes[-1],\n+            init_reference_points=decoder_outputs.init_reference_points,\n+            intermediate_reference_points=decoder_outputs.intermediate_reference_points,\n+            encoder_coord_logits=decoder_outputs.encoder_coord_logits,\n+            encoder_class_logits=decoder_outputs.encoder_class_logits,\n+            encoder_extracted_states=encoder_outputs.extracted_states,\n+            decoder_hidden_states=decoder_outputs.hidden_states,\n+            decoder_attentions=decoder_outputs.attentions,\n+            encoder_hidden_states=encoder_outputs.hidden_states,\n+            encoder_attentions=encoder_outputs.attentions,\n+        )"
        },
        {
            "sha": "909281b0c6867a02da39acff89647b7ce34670be",
            "filename": "src/transformers/models/omdet_turbo/processing_omdet_turbo.py",
            "status": "added",
            "additions": 362,
            "deletions": 0,
            "changes": 362,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7",
            "patch": "@@ -0,0 +1,362 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"\n+Processor class for OmDet-Turbo.\n+\"\"\"\n+\n+import sys\n+from typing import List, Optional, Tuple, Union\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_transforms import center_to_corners_format\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, TextKwargs\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import (\n+    TensorType,\n+    is_torch_available,\n+    is_torchvision_available,\n+)\n+\n+\n+if sys.version_info >= (3, 11):\n+    from typing import Unpack\n+else:\n+    from typing_extensions import Unpack\n+\n+\n+class OmDetTurboTextKwargs(TextKwargs, total=False):\n+    task: Optional[Union[str, List[str], TextInput, PreTokenizedInput]]\n+\n+\n+class OmDetTurboProcessorKwargs(ProcessingKwargs, total=False):\n+    text_kwargs: OmDetTurboTextKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"add_special_tokens\": True,\n+            \"padding\": \"max_length\",\n+            \"truncation\": True,\n+            \"max_length\": 77,\n+            \"stride\": 0,\n+            \"return_overflowing_tokens\": False,\n+            \"return_special_tokens_mask\": False,\n+            \"return_offsets_mapping\": False,\n+            \"return_token_type_ids\": False,\n+            \"return_length\": False,\n+            \"verbose\": True,\n+            \"task\": None,\n+        },\n+        \"images_kwargs\": {},\n+    }\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    from torchvision.ops.boxes import batched_nms\n+\n+\n+def clip_boxes(box, box_size: Tuple[int, int]):\n+    \"\"\"\n+    Clip the boxes by limiting x coordinates to the range [0, width]\n+    and y coordinates to the range [0, height].\n+\n+    Args:\n+        box (Tensor): The box to be clipped.\n+        box_size (height, width): The clipping box's size.\n+    \"\"\"\n+    assert torch.isfinite(box).all(), \"Box tensor contains infinite or NaN!\"\n+    height, width = box_size\n+    x1 = box[:, 0].clamp(min=0, max=width)\n+    y1 = box[:, 1].clamp(min=0, max=height)\n+    x2 = box[:, 2].clamp(min=0, max=width)\n+    y2 = box[:, 3].clamp(min=0, max=height)\n+    box = torch.stack((x1, y1, x2, y2), dim=-1)\n+\n+    return box\n+\n+\n+def compute_score(boxes):\n+    \"\"\"\n+    Compute logit scores per class for each box (proposal) and an array of class indices\n+    corresponding to each proposal, flattened across the proposal_num.\n+    The indices in `classes` will later be used to filter and match the predicted classes\n+    with the input class names.\n+    \"\"\"\n+    num_classes = boxes.shape[2]\n+    proposal_num = boxes.shape[1]\n+    scores = torch.sigmoid(boxes)\n+    classes = torch.arange(num_classes, device=boxes.device).unsqueeze(0).repeat(proposal_num, 1).flatten(0, 1)\n+    return scores, classes\n+\n+\n+def _post_process_boxes_for_image(\n+    boxes: TensorType,\n+    scores: TensorType,\n+    predicted_classes: TensorType,\n+    classes: List[str],\n+    image_size: Tuple[int, int],\n+    num_classes: int,\n+    score_threshold: float,\n+    nms_threshold: float,\n+    max_num_det: int = None,\n+) -> dict:\n+    \"\"\"\n+    Filter predicted results using given thresholds and NMS.\n+    Args:\n+        boxes (torch.Tensor): A Tensor of predicted class-specific or class-agnostic\n+            boxes for the image. Shape : (num_queries, max_num_classes_in_batch * 4) if doing\n+            class-specific regression, or (num_queries, 4) if doing class-agnostic\n+            regression.\n+        scores (torch.Tensor): A Tensor of predicted class scores for the image.\n+            Shape : (num_queries, max_num_classes_in_batch + 1)\n+        predicted_classes (torch.Tensor): A Tensor of predicted classes for the image.\n+            Shape : (num_queries * (max_num_classes_in_batch + 1),)\n+        classes (List[str]): The input classes names.\n+        image_size (tuple): A tuple of (height, width) for the image.\n+        num_classes (int): The number of classes given for this image.\n+        score_threshold (float): Only return detections with a confidence score exceeding this\n+            threshold.\n+        nms_threshold (float):  The threshold to use for box non-maximum suppression. Value in [0, 1].\n+        max_num_det (int, optional): The maximum number of detections to return. Default is None.\n+    Returns:\n+        dict: A dictionary the following keys:\n+            \"boxes\" (Tensor): A tensor of shape (num_filtered_objects, 4), containing the predicted boxes in (x1, y1, x2, y2) format.\n+            \"scores\" (Tensor): A tensor of shape (num_filtered_objects,), containing the predicted confidence scores for each detection.\n+            \"classes\" (List[str]): A list of strings, where each string is the predicted class for the\n+                corresponding detection\n+    \"\"\"\n+    proposal_num = len(boxes) if max_num_det is None else max_num_det\n+    scores_per_image, topk_indices = scores.flatten(0, 1).topk(proposal_num, sorted=False)\n+    classes_per_image = predicted_classes[topk_indices]\n+    box_pred_per_image = boxes.view(-1, 1, 4).repeat(1, num_classes, 1).view(-1, 4)\n+    box_pred_per_image = box_pred_per_image[topk_indices]\n+\n+    # Score filtering\n+    box_pred_per_image = center_to_corners_format(box_pred_per_image)\n+    box_pred_per_image = box_pred_per_image * torch.tensor(image_size[::-1]).repeat(2).to(box_pred_per_image.device)\n+    filter_mask = scores_per_image > score_threshold  # R x K\n+    score_keep = filter_mask.nonzero(as_tuple=False).view(-1)\n+    box_pred_per_image = box_pred_per_image[score_keep]\n+    scores_per_image = scores_per_image[score_keep]\n+    classes_per_image = classes_per_image[score_keep]\n+\n+    filter_classes_mask = classes_per_image < len(classes)\n+    classes_keep = filter_classes_mask.nonzero(as_tuple=False).view(-1)\n+    box_pred_per_image = box_pred_per_image[classes_keep]\n+    scores_per_image = scores_per_image[classes_keep]\n+    classes_per_image = classes_per_image[classes_keep]\n+\n+    # NMS\n+    keep = batched_nms(box_pred_per_image, scores_per_image, classes_per_image, nms_threshold)\n+    box_pred_per_image = box_pred_per_image[keep]\n+    scores_per_image = scores_per_image[keep]\n+    classes_per_image = classes_per_image[keep]\n+    classes_per_image = [classes[i] for i in classes_per_image]\n+\n+    # create an instance\n+    result = {}\n+    result[\"boxes\"] = clip_boxes(box_pred_per_image, image_size)\n+    result[\"scores\"] = scores_per_image\n+    result[\"classes\"] = classes_per_image\n+\n+    return result\n+\n+\n+class OmDetTurboProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a OmDet-Turbo processor which wraps a Deformable DETR image processor and an AutoTokenizer into a\n+    single processor.\n+\n+    [`OmDetTurboProcessor`] offers all the functionalities of [`DetrImageProcessor`] and\n+    [`AutoTokenizer`]. See the docstring of [`~OmDetTurboProcessor.__call__`] and [`~OmDetTurboProcessor.decode`]\n+    for more information.\n+\n+    Args:\n+        image_processor (`DetrImageProcessor`):\n+            An instance of [`DetrImageProcessor`]. The image processor is a required input.\n+        tokenizer (`AutoTokenizer`):\n+            An instance of ['PreTrainedTokenizer`]. The tokenizer is a required input.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    image_processor_class = \"DetrImageProcessor\"\n+    tokenizer_class = \"AutoTokenizer\"\n+\n+    def __init__(self, image_processor, tokenizer):\n+        super().__init__(image_processor, tokenizer)\n+\n+    def __call__(\n+        self,\n+        images: ImageInput = None,\n+        text: Union[List[str], List[List[str]]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[OmDetTurboProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        This method uses [*DetrImageProcessor.__call__] method to prepare image(s) for the model, and\n+        [CLIPTokenizerFast.__call__] to prepare text for the model.\n+\n+        Please refer to the docstring of the above two methods for more information.\n+\n+        Args:\n+            images (`ImageInput`):\n+               Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255.\n+            text (`Union[str, List[str], List[List[str]]]`):\n+                The classes used to limit the scope of the open vocabulary detection. Expects a list of strings or a list\n+                of list of strings. Batched classes can be of different lengths.\n+                Examples: [\"cat\", \"dog\", \"bird\"], [[\"cat\", \"dog\", \"bird\"], [\"hat\", \"person\"], [\"car\"]]\n+        Kwargs:\n+            task (`Union[str, List[str], TextInput, PreTokenizedInput]`):\n+                The grounded text used to guide open vocabulary detection. Expects a single string or a list of strings.\n+                Examples: \"Detect a cat, a dog, and a bird.\",[ \"Detect everything.\", \"Detect trees and flowers.\"]\n+                When not provided, the default task is \"Detect [class1], [class2], [class3]\" etc.\n+            ...\n+        \"\"\"\n+        if images is None or text is None:\n+            raise ValueError(\"You have to specify both `images` and `text`\")\n+\n+        output_kwargs = self._merge_kwargs(\n+            OmDetTurboProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        if isinstance(text, str):\n+            text = text.strip(\" \").split(\",\")\n+\n+        if not (len(text) and isinstance(text[0], (list, tuple))):\n+            text = [text]\n+\n+        task = output_kwargs[\"text_kwargs\"].pop(\"task\", None)\n+        if task is None:\n+            task = [\"Detect {}.\".format(\", \".join(text_single)) for text_single in text]\n+        elif not isinstance(task, (list, tuple)):\n+            task = [task]\n+\n+        encoding_image_processor = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+        tasks_encoding = self.tokenizer(text=task, **output_kwargs[\"text_kwargs\"])\n+\n+        classes = text\n+\n+        classes_structure = torch.tensor([len(class_single) for class_single in classes], dtype=torch.long)\n+        classes_flattened = [class_single for class_batch in classes for class_single in class_batch]\n+        classes_encoding = self.tokenizer(text=classes_flattened, **output_kwargs[\"text_kwargs\"])\n+\n+        encoding = BatchFeature()\n+        encoding.update({f\"tasks_{key}\": value for key, value in tasks_encoding.items()})\n+        encoding.update({f\"classes_{key}\": value for key, value in classes_encoding.items()})\n+        encoding.update({\"classes_structure\": classes_structure})\n+        encoding.update(encoding_image_processor)\n+\n+        return encoding\n+\n+    # Copied from transformers.models.blip.processing_blip.BlipProcessor.batch_decode with BertTokenizerFast->PreTrainedTokenizer\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    # Copied from transformers.models.blip.processing_blip.BlipProcessor.decode with BertTokenizerFast->PreTrainedTokenizer\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    def post_process_grounded_object_detection(\n+        self,\n+        outputs,\n+        classes: Union[List[str], List[List[str]]],\n+        score_threshold: float = 0.3,\n+        nms_threshold: float = 0.5,\n+        target_sizes: Optional[Union[TensorType, List[Tuple]]] = None,\n+        max_num_det: Optional[int] = None,\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`OmDetTurboForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n+        bottom_right_x, bottom_right_y) format and get the associated text class.\n+\n+        Args:\n+            outputs ([`OmDetTurboObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            classes (Union[List[str], List[List[str]]]): The input classes names.\n+            score_threshold (float, defaults to 0.3): Only return detections with a confidence score exceeding this\n+                threshold.\n+            nms_threshold (float, defaults to 0.5):  The threshold to use for box non-maximum suppression. Value in [0, 1].\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*, defaults to None):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                `(height, width)` of each image in the batch. If unset, predictions will not be resized.\n+            max_num_det (int, *optional*, defaults to None): The maximum number of detections to return.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, classes and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        if isinstance(classes[0], str):\n+            classes = [classes]\n+\n+        boxes_logits = outputs.decoder_coord_logits\n+        scores_logits = outputs.decoder_class_logits\n+\n+        # Inputs consistency check\n+        if target_sizes is None:\n+            height = (\n+                self.image_processor.size[\"height\"]\n+                if \"height\" in self.image_processor.size\n+                else self.image_processor.size[\"shortest_edge\"]\n+            )\n+            width = (\n+                self.image_processor.size[\"width\"]\n+                if \"width\" in self.image_processor.size\n+                else self.image_processor.size[\"longest_edge\"]\n+            )\n+            target_sizes = ((height, width),) * len(boxes_logits)\n+        elif len(target_sizes[0]) != 2:\n+            raise ValueError(\n+                \"Each element of target_sizes must contain the size (height, width) of each image of the batch\"\n+            )\n+        if len(target_sizes) != len(boxes_logits):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as output sequences\")\n+        if len(classes) != len(boxes_logits):\n+            raise ValueError(\"Make sure that you pass in as many classes group as output sequences\")\n+\n+        # Convert target_sizes to list for easier handling\n+        if isinstance(target_sizes, torch.Tensor):\n+            target_sizes = target_sizes.tolist()\n+\n+        scores, predicted_classes = compute_score(scores_logits)\n+        num_classes = scores_logits.shape[2]\n+        results = []\n+        for scores_img, box_per_img, image_size, class_names in zip(scores, boxes_logits, target_sizes, classes):\n+            results.append(\n+                _post_process_boxes_for_image(\n+                    box_per_img,\n+                    scores_img,\n+                    predicted_classes,\n+                    class_names,\n+                    image_size,\n+                    num_classes,\n+                    score_threshold=score_threshold,\n+                    nms_threshold=nms_threshold,\n+                    max_num_det=max_num_det,\n+                )\n+            )\n+\n+        return results"
        },
        {
            "sha": "aadcc6f511ccef6a34651e25db5eafa109e459e0",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7",
            "patch": "@@ -6587,6 +6587,20 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class OmDetTurboForObjectDetection(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class OmDetTurboPreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class OneFormerForUniversalSegmentation(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/omdet_turbo/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/tests%2Fmodels%2Fomdet_turbo%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/tests%2Fmodels%2Fomdet_turbo%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fomdet_turbo%2F__init__.py?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7"
        },
        {
            "sha": "ed85c4c0007829b0c2fcbe698d1015c21ed408d9",
            "filename": "tests/models/omdet_turbo/test_modeling_omdet_turbo.py",
            "status": "added",
            "additions": 904,
            "deletions": 0,
            "changes": 904,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7",
            "patch": "@@ -0,0 +1,904 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch OmDet-Turbo model.\"\"\"\n+\n+import copy\n+import unittest\n+from io import BytesIO\n+\n+import requests\n+\n+from transformers import OmDetTurboConfig, is_torch_available, is_vision_available\n+from transformers.feature_extraction_utils import BatchFeature\n+from transformers.file_utils import cached_property\n+from transformers.testing_utils import (\n+    require_timm,\n+    require_torch,\n+    require_torch_gpu,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+    import torch.nn.functional as F\n+\n+    from transformers import OmDetTurboForObjectDetection\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import AutoProcessor\n+\n+\n+class OmDetTurboModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=6,\n+        is_training=False,\n+        num_channels=3,\n+        max_text_len=7,\n+        num_classes=3,\n+        use_timm_backbone=False,\n+        backbone=None,\n+        apply_layernorm_after_vision_backbone=False,\n+        image_size=224,\n+        text_projection_in_dim=16,\n+        text_projection_out_dim=16,\n+        class_embed_dim=16,\n+        hidden_size=8,\n+        num_hidden_layers=2,\n+        num_attention_heads=2,\n+        num_queries=20,\n+        encoder_in_channels=(16, 32, 64),\n+        encoder_dim_feedforward=32,\n+        num_projection_layers=1,\n+        decoder_n_points=4,\n+        num_feature_levels=3,\n+    ):\n+        super().__init__()\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.is_training = is_training\n+        self.num_channels = num_channels\n+        self.max_text_len = max_text_len\n+        self.num_classes = num_classes\n+        self.use_timm_backbone = use_timm_backbone\n+        self.backbone = backbone\n+        self.apply_layernorm_after_vision_backbone = apply_layernorm_after_vision_backbone\n+        self.image_size = image_size\n+        self.text_projection_in_dim = text_projection_in_dim\n+        self.text_projection_out_dim = text_projection_out_dim\n+        self.class_embed_dim = class_embed_dim\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_queries = num_queries\n+        self.encoder_in_channels = encoder_in_channels\n+        self.encoder_dim_feedforward = encoder_dim_feedforward\n+        self.num_projection_layers = num_projection_layers\n+        self.decoder_n_points = decoder_n_points\n+        self.num_feature_levels = num_feature_levels\n+\n+        self.encoder_seq_length_vision = self.image_size // 32\n+        self.decoder_seq_length = self.num_queries\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+\n+        input_ids_tasks = ids_tensor([self.batch_size, self.max_text_len], self.num_classes)\n+        input_ids_tasks = input_ids_tasks.to(torch_device)\n+        input_ids_classes = torch.cat(\n+            [ids_tensor([self.num_classes, self.max_text_len], self.num_classes) for _ in range(self.batch_size)]\n+        )\n+        input_ids_classes = input_ids_classes.to(torch_device)\n+        attention_mask_tasks = torch.ones_like(input_ids_tasks, device=torch_device)\n+        attention_mask_classes = torch.ones_like(input_ids_classes, device=torch_device)\n+        classes_structure = torch.ones(self.batch_size, dtype=torch.long, device=torch_device) * self.num_classes\n+        encoding = BatchFeature()\n+        encoding.update(\n+            {\n+                \"pixel_values\": pixel_values,\n+                \"classes_input_ids\": input_ids_classes,\n+                \"classes_attention_mask\": attention_mask_classes,\n+                \"tasks_input_ids\": input_ids_tasks,\n+                \"tasks_attention_mask\": attention_mask_tasks,\n+                \"classes_structure\": classes_structure,\n+            }\n+        )\n+        config = self.get_config()\n+        return config, encoding\n+\n+    def get_config(self):\n+        text_backbone = {\n+            \"hidden_size\": 16,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 2,\n+            \"intermediate_size\": 16,\n+            \"max_position_embeddings\": 8,\n+            \"model_type\": \"clip_text_model\",\n+        }\n+        backbone_config = {\n+            \"embed_dim\": self.hidden_size,\n+            \"depths\": (1, 1, 1, 1),\n+            \"num_heads\": (1, 1, 1, 1),\n+            \"window_size\": 7,\n+            \"image_size\": self.image_size,\n+            \"out_indices\": (2, 3, 4),\n+            \"model_type\": \"swin\",\n+        }\n+\n+        return OmDetTurboConfig(\n+            text_config=text_backbone,\n+            backbone_config=backbone_config,\n+            use_timm_backbone=self.use_timm_backbone,\n+            backbone=self.backbone,\n+            apply_layernorm_after_vision_backbone=self.apply_layernorm_after_vision_backbone,\n+            decoder_num_layers=self.num_hidden_layers,\n+            image_size=self.image_size,\n+            encoder_in_channels=self.encoder_in_channels,\n+            num_queries=self.num_queries,\n+            encoder_layers=self.num_hidden_layers,\n+            encoder_projection_indices=[2] * self.num_projection_layers,\n+            encoder_attention_heads=self.num_attention_heads,\n+            decoder_num_heads=self.num_attention_heads,\n+            decoder_num_points=self.decoder_n_points,\n+            num_feature_levels=self.num_feature_levels,\n+            encoder_dim_feedforward=self.encoder_dim_feedforward,\n+            task_encoder_hidden_dim=self.encoder_dim_feedforward,\n+            decoder_dim_feedforward=self.encoder_dim_feedforward,\n+            class_embed_dim=self.class_embed_dim,\n+            text_projection_in_dim=self.text_projection_in_dim,\n+            text_projection_out_dim=self.text_projection_out_dim,\n+            encoder_hidden_dim=self.hidden_size,\n+            decoder_hidden_dim=self.hidden_size,\n+            vision_features_channels=[self.hidden_size, self.hidden_size, self.hidden_size],\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config, inputs_dict = self.prepare_config_and_inputs()\n+        return config, inputs_dict\n+\n+    def create_and_check_object_detection_head_model(self, config, inputs_dict):\n+        model = OmDetTurboForObjectDetection(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        result = model(**inputs_dict)\n+\n+        self.parent.assertEqual(result.decoder_coord_logits.shape, (self.batch_size, self.num_queries, 4))\n+        self.parent.assertEqual(\n+            result.decoder_class_logits.shape, (self.batch_size, self.num_queries, self.num_classes)\n+        )\n+\n+\n+@require_torch\n+class OmDetTurboModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (OmDetTurboForObjectDetection,) if is_torch_available() else ()\n+    is_encoder_decoder = True\n+    test_pruning = False\n+    test_head_masking = False\n+    pipeline_model_mapping = (\n+        {\"zero-shot-object-detection\": OmDetTurboForObjectDetection} if is_torch_available() else {}\n+    )\n+\n+    # special case for head models\n+    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n+        inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n+\n+        return inputs_dict\n+\n+    def setUp(self):\n+        self.model_tester = OmDetTurboModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=OmDetTurboConfig,\n+            has_text_modality=False,\n+            common_properties=[\"d_model\", \"encoder_attention_heads\", \"decoder_num_heads\"],\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_object_detection_head_model(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_object_detection_head_model(config, inputs_dict)\n+\n+    @unittest.skip(\n+        reason=\"Unsupported as classes_input_ids are classes input are flattened by the processor: https://github.com/huggingface/transformers/issues/33669\"\n+    )\n+    def test_multi_gpu_data_parallel_forward(self):\n+        pass\n+\n+    @unittest.skip(reason=\"OmDet-Turbo does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"OmDet-Turbo does not have 'input_ids' and 'attention_mask'\")\n+    def test_torchscript_output_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"OmDet-Turbo does not have 'input_ids' and 'attention_mask'\")\n+    def test_torchscript_output_hidden_states(self):\n+        pass\n+\n+    @unittest.skip(reason=\"OmDet-Turbo does not have 'input_ids' and 'attention_mask'\")\n+    def test_torchscript_simple(self):\n+        pass\n+\n+    @unittest.skip(reason=\"OmDet-Turbo does not have 'input_ids' and 'attention_mask'\")\n+    def test_torchscript_output_hidden_state(self):\n+        pass\n+\n+    def test_resize_tokens_embeddings(self):\n+        # rewrite as OmDet-Turbo does not have \"input_ids\" and \"decoder_input_ids\"\n+        (\n+            original_config,\n+            inputs_dict,\n+        ) = self.model_tester.prepare_config_and_inputs_for_common()\n+        if not self.test_resize_embeddings:\n+            self.skipTest(reason=\"test_resize_embeddings is set to `False`\")\n+\n+        for model_class in self.all_model_classes:\n+            config = copy.deepcopy(original_config)\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model_embed_pre_resize = model.get_input_embeddings()\n+            type_model_embed_pre_resize = type(model_embed_pre_resize)\n+\n+            if self.model_tester.is_training is False:\n+                model.eval()\n+\n+            model_vocab_size = config.text_config.vocab_size if hasattr(config, \"text_config\") else config.vocab_size\n+            # Retrieve the embeddings and clone theme\n+            model_embed = model.resize_token_embeddings(model_vocab_size)\n+            cloned_embeddings = model_embed.weight.clone()\n+\n+            # Check that resizing the token embeddings with a larger vocab size increases the model's vocab size\n+            model_embed = model.resize_token_embeddings(model_vocab_size + 10)\n+            new_model_vocab_size = (\n+                model.config.text_config.vocab_size\n+                if hasattr(model.config, \"text_config\")\n+                else model.config.vocab_size\n+            )\n+            self.assertEqual(new_model_vocab_size, model_vocab_size + 10)\n+            # Check that it actually resizes the embeddings matrix\n+            self.assertEqual(model_embed.weight.shape[0], cloned_embeddings.shape[0] + 10)\n+            # Check to make sure the type of embeddings returned post resizing is same as type of input\n+            type_model_embed_post_resize = type(model_embed)\n+            self.assertEqual(type_model_embed_pre_resize, type_model_embed_post_resize)\n+            # Check that the model can still do a forward pass successfully (every parameter should be resized)\n+            model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            # Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size\n+            model_embed = model.resize_token_embeddings(model_vocab_size - 15)\n+            new_model_vocab_size = (\n+                model.config.text_config.vocab_size\n+                if hasattr(model.config, \"text_config\")\n+                else model.config.vocab_size\n+            )\n+            self.assertEqual(new_model_vocab_size, model_vocab_size - 15)\n+            # Check that it actually resizes the embeddings matrix\n+            self.assertEqual(model_embed.weight.shape[0], cloned_embeddings.shape[0] - 15)\n+\n+            # Check that the model can still do a forward pass successfully (every parameter should be resized)\n+            # Input ids should be clamped to the maximum size of the vocabulary\n+            inputs_dict[\"tasks_input_ids\"].clamp_(max=model_vocab_size - 15 - 1)\n+\n+            # make sure that classes_input_ids are resized as well\n+            if \"classes_input_ids\" in inputs_dict:\n+                inputs_dict[\"classes_input_ids\"].clamp_(max=model_vocab_size - 15 - 1)\n+            model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            # Check that adding and removing tokens has not modified the first part of the embedding matrix.\n+            models_equal = True\n+            for p1, p2 in zip(cloned_embeddings, model_embed.weight):\n+                if p1.data.ne(p2.data).sum() > 0:\n+                    models_equal = False\n+\n+            self.assertTrue(models_equal)\n+\n+            config = copy.deepcopy(original_config)\n+            model = model_class(config)\n+            model.to(torch_device)\n+\n+            model_vocab_size = config.text_config.vocab_size if hasattr(config, \"text_config\") else config.vocab_size\n+            model.resize_token_embeddings(model_vocab_size + 10, pad_to_multiple_of=1)\n+            new_model_vocab_size = (\n+                model.config.text_config.vocab_size\n+                if hasattr(model.config, \"text_config\")\n+                else model.config.vocab_size\n+            )\n+            self.assertTrue(new_model_vocab_size + 10, model_vocab_size)\n+\n+            model_embed = model.resize_token_embeddings(model_vocab_size, pad_to_multiple_of=64)\n+            new_model_vocab_size = (\n+                model.config.text_config.vocab_size\n+                if hasattr(model.config, \"text_config\")\n+                else model.config.vocab_size\n+            )\n+            self.assertTrue(model_embed.weight.shape[0] // 64, 0)\n+\n+            self.assertTrue(model_embed.weight.shape[0], new_model_vocab_size)\n+            self.assertTrue(new_model_vocab_size, model.vocab_size)\n+\n+            model_embed = model.resize_token_embeddings(model_vocab_size + 13, pad_to_multiple_of=64)\n+            self.assertTrue(model_embed.weight.shape[0] // 64, 0)\n+\n+            # Check that resizing a model to a multiple of pad_to_multiple leads to a model of exactly that size\n+            target_dimension = 128\n+            model_embed = model.resize_token_embeddings(target_dimension, pad_to_multiple_of=64)\n+            self.assertTrue(model_embed.weight.shape[0], target_dimension)\n+\n+            with self.assertRaisesRegex(\n+                ValueError,\n+                \"Asking to pad the embedding matrix to a multiple of `1.3`, which is not and integer. Please make sure to pass an integer\",\n+            ):\n+                model.resize_token_embeddings(model_vocab_size, pad_to_multiple_of=1.3)\n+\n+    # Overwrite as `init_reference_points` is not batch dependent and contains `inf` values\n+    def test_batching_equivalence(self):\n+        \"\"\"\n+        Tests that the model supports batching and that the output is nearly the same for the same input in\n+        different batch sizes.\n+        (Why \"nearly the same\" not \"exactly the same\"? Batching uses different matmul shapes, which often leads to\n+        different results: https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535)\n+        \"\"\"\n+\n+        def get_tensor_equivalence_function(batched_input):\n+            # models operating on continuous spaces have higher abs difference than LMs\n+            # instead, we can rely on cos distance for image/speech models, similar to `diffusers`\n+            if \"input_ids\" not in batched_input:\n+                return lambda tensor1, tensor2: (\n+                    1.0 - F.cosine_similarity(tensor1.float().flatten(), tensor2.float().flatten(), dim=0, eps=1e-38)\n+                )\n+            return lambda tensor1, tensor2: torch.max(torch.abs(tensor1 - tensor2))\n+\n+        def recursive_check(batched_object, single_row_object, model_name, key):\n+            if isinstance(batched_object, (list, tuple)):\n+                for batched_object_value, single_row_object_value in zip(batched_object, single_row_object):\n+                    recursive_check(batched_object_value, single_row_object_value, model_name, key)\n+            elif isinstance(batched_object, dict):\n+                for batched_object_value, single_row_object_value in zip(\n+                    batched_object.values(), single_row_object.values()\n+                ):\n+                    recursive_check(batched_object_value, single_row_object_value, model_name, key)\n+            # do not compare returned loss (0-dim tensor) / codebook ids (int) / caching objects\n+            elif batched_object is None or not isinstance(batched_object, torch.Tensor):\n+                return\n+            elif batched_object.dim() == 0:\n+                return\n+            elif key != \"init_reference_points\":\n+                # init\n+                # indexing the first element does not always work\n+                # e.g. models that output similarity scores of size (N, M) would need to index [0, 0]\n+                slice_ids = [slice(0, index) for index in single_row_object.shape]\n+                batched_row = batched_object[slice_ids]\n+                self.assertFalse(\n+                    torch.isnan(batched_row).any(), f\"Batched output has `nan` in {model_name} for key={key}\"\n+                )\n+                self.assertFalse(\n+                    torch.isinf(batched_row).any(), f\"Batched output has `inf` in {model_name} for key={key}\"\n+                )\n+                self.assertFalse(\n+                    torch.isnan(single_row_object).any(), f\"Single row output has `nan` in {model_name} for key={key}\"\n+                )\n+                self.assertFalse(\n+                    torch.isinf(single_row_object).any(),\n+                    f\"Single row output has `inf` in {model_name} for key={key}\",\n+                )\n+                self.assertTrue(\n+                    (equivalence(batched_row, single_row_object)) <= 1e-03,\n+                    msg=(\n+                        f\"Batched and Single row outputs are not equal in {model_name} for key={key}. \"\n+                        f\"Difference={equivalence(batched_row, single_row_object)}.\"\n+                    ),\n+                )\n+\n+        config, batched_input = self.model_tester.prepare_config_and_inputs_for_common()\n+        equivalence = get_tensor_equivalence_function(batched_input)\n+\n+        for model_class in self.all_model_classes:\n+            config.output_hidden_states = True\n+\n+            model_name = model_class.__name__\n+            if hasattr(self.model_tester, \"prepare_config_and_inputs_for_model_class\"):\n+                config, batched_input = self.model_tester.prepare_config_and_inputs_for_model_class(model_class)\n+            batched_input_prepared = self._prepare_for_class(batched_input, model_class)\n+            model = model_class(config).to(torch_device).eval()\n+            batch_size = self.model_tester.batch_size\n+            single_row_input = {}\n+            for key, value in batched_input_prepared.items():\n+                single_batch_shape = value.shape[0] // batch_size\n+                single_row_input[key] = value[:single_batch_shape]\n+\n+            with torch.no_grad():\n+                model_batched_output = model(**batched_input_prepared)\n+                model_row_output = model(**single_row_input)\n+\n+            if isinstance(model_batched_output, torch.Tensor):\n+                model_batched_output = {\"model_output\": model_batched_output}\n+                model_row_output = {\"model_output\": model_row_output}\n+\n+            for key in model_batched_output:\n+                # DETR starts from zero-init queries to decoder, leading to cos_similarity = `nan`\n+                if hasattr(self, \"zero_init_hidden_state\") and \"decoder_hidden_states\" in key:\n+                    model_batched_output[key] = model_batched_output[key][1:]\n+                    model_row_output[key] = model_row_output[key][1:]\n+                if key in (\"decoder_class_logits\", \"decoder_classes\", \"encoder_class_logits\"):\n+                    # check if all elements are close to 0, if so skip the test as the test strugles with comparing\n+                    # tensors with all elements close to 0\n+                    if torch.allclose(\n+                        model_batched_output[key], torch.zeros_like(model_batched_output[key]), atol=1e-6\n+                    ) and torch.allclose(model_row_output[key], torch.zeros_like(model_row_output[key]), atol=1e-6):\n+                        continue\n+\n+                recursive_check(model_batched_output[key], model_row_output[key], model_name, key)\n+\n+    def test_attention_outputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.encoder_attentions[-1]\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.encoder_attentions[-1]\n+            self.assertEqual(\n+                len(attentions), self.model_tester.num_hidden_layers * self.model_tester.num_projection_layers\n+            )\n+            # Rest of the shape seems to depend on backbone output shapes and image size\n+            self.assertListEqual(\n+                list(attentions[0].shape[-3:]),\n+                [\n+                    self.model_tester.num_attention_heads,\n+                    self.model_tester.encoder_seq_length_vision**2,\n+                    self.model_tester.encoder_seq_length_vision**2,\n+                ],\n+            )\n+            # decoder attentions\n+            decoder_attentions = outputs.decoder_attentions[0]\n+            self.assertIsInstance(decoder_attentions, (list, tuple))\n+            self.assertEqual(len(decoder_attentions), self.model_tester.num_hidden_layers)\n+            self.assertListEqual(\n+                list(decoder_attentions[0].shape[-3:]),\n+                [\n+                    self.model_tester.num_attention_heads,\n+                    self.model_tester.num_queries + self.model_tester.max_text_len,\n+                    self.model_tester.num_queries + self.model_tester.max_text_len,\n+                ],\n+            )\n+\n+            # cross attentions\n+            cross_attentions = outputs.decoder_attentions[-1]\n+            self.assertIsInstance(cross_attentions, (list, tuple))\n+            self.assertEqual(len(cross_attentions), self.model_tester.num_hidden_layers)\n+            self.assertListEqual(\n+                list(cross_attentions[0].shape[-3:]),\n+                [\n+                    self.model_tester.num_attention_heads,\n+                    self.model_tester.num_feature_levels,\n+                    self.model_tester.decoder_n_points,\n+                ],\n+            )\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            self_attentions = outputs.encoder_attentions[-1]\n+\n+            self.assertEqual(\n+                len(self_attentions), self.model_tester.num_hidden_layers * self.model_tester.num_projection_layers\n+            )\n+            self.assertListEqual(\n+                list(attentions[0].shape[-3:]),\n+                [\n+                    self.model_tester.num_attention_heads,\n+                    self.model_tester.encoder_seq_length_vision**2,\n+                    self.model_tester.encoder_seq_length_vision**2,\n+                ],\n+            )\n+\n+    # overwrite since encoder_hidden_states are 3-dim and not 2-dim\n+    def test_hidden_states_output(self):\n+        def check_hidden_states_output(inputs_dict, config, model_class):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            hidden_states = outputs.encoder_hidden_states\n+            expected_num_layers = getattr(\n+                self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_projection_layers + 1\n+            )\n+            self.assertEqual(len(hidden_states), expected_num_layers)\n+\n+            seq_len = self.model_tester.encoder_seq_length_vision\n+\n+            self.assertListEqual(list(hidden_states[0].shape[-3:]), [self.model_tester.hidden_size, seq_len, seq_len])\n+\n+            hidden_states = outputs.decoder_hidden_states\n+            expected_num_layers = getattr(\n+                self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n+            )\n+            self.assertIsInstance(hidden_states, (list, tuple))\n+            self.assertEqual(len(hidden_states), expected_num_layers)\n+            self.assertListEqual(\n+                list(hidden_states[0].shape[-2:]),\n+                [self.model_tester.decoder_seq_length, self.model_tester.hidden_size],\n+            )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_hidden_states\"] = True\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+            # check that output_hidden_states also work using config\n+            del inputs_dict[\"output_hidden_states\"]\n+            config.output_hidden_states = True\n+\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+    # removed retain_grad and grad on decoder_hidden_states, as queries don't require grad\n+    def test_retain_grad_hidden_states_attentions(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.output_hidden_states = True\n+        config.output_attentions = True\n+\n+        # no need to test all models as different heads yield the same functionality\n+        model_class = self.all_model_classes[0]\n+        model = model_class(config)\n+        model.to(torch_device)\n+\n+        inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+        outputs = model(**inputs)\n+\n+        output = outputs[0]\n+\n+        encoder_hidden_states = outputs.encoder_hidden_states[0]\n+        encoder_attentions = outputs.encoder_attentions[0][0]\n+        encoder_hidden_states.retain_grad()\n+        encoder_attentions.retain_grad()\n+\n+        cross_attentions = outputs.decoder_attentions[-1][0]\n+        cross_attentions.retain_grad()\n+\n+        output.flatten()[0].backward(retain_graph=True)\n+\n+        self.assertIsNotNone(encoder_hidden_states.grad)\n+        self.assertIsNotNone(encoder_attentions.grad)\n+        self.assertIsNotNone(cross_attentions.grad)\n+\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                if param.requires_grad:\n+                    if (\n+                        \"embeddings\" in name\n+                        or \".fc\" in name\n+                        or \"decoder.channel_projection_layers\" in name\n+                        or \"query_position_head\" in name\n+                        or \"decoder.encoder_vision_features\" in name\n+                    ):\n+                        continue\n+                    self.assertIn(\n+                        ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                        [0.0, 1.0],\n+                        msg=f\"Parameter {name} seems not properly initialized\",\n+                    )\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n+    return image\n+\n+\n+def prepare_text():\n+    classes = [\"cat\", \"remote\"]\n+    task = \"Detect {}.\".format(\", \".join(classes))\n+    return classes, task\n+\n+\n+def prepare_img_batched():\n+    url1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    url2 = \"http://images.cocodataset.org/train2017/000000257813.jpg\"\n+    url3 = \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n+\n+    return [Image.open(BytesIO(requests.get(url).content)).convert(\"RGB\") for url in [url1, url2, url3]]\n+\n+\n+def prepare_text_batched():\n+    classes1 = [\"cat\", \"remote\"]\n+    classes2 = [\"boat\"]\n+    classes3 = [\"statue\", \"trees\", \"torch\"]\n+\n+    task1 = \"Detect {}.\".format(\", \".join(classes1))\n+    task2 = \"Detect all the boat in the image.\"\n+    task3 = \"Focus on the foreground, detect statue, torch and trees.\"\n+    return [classes1, classes2, classes3], [task1, task2, task3]\n+\n+\n+@require_timm\n+@require_vision\n+@slow\n+class OmDetTurboModelIntegrationTests(unittest.TestCase):\n+    @cached_property\n+    def default_processor(self):\n+        return AutoProcessor.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\") if is_vision_available() else None\n+\n+    def test_inference_object_detection_head(self):\n+        model = OmDetTurboForObjectDetection.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\").to(torch_device)\n+\n+        processor = self.default_processor\n+        image = prepare_img()\n+        classes, task = prepare_text()\n+        encoding = processor(images=image, text=classes, task=task, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**encoding)\n+\n+        expected_shape_coord_logits = torch.Size((1, model.config.num_queries, 4))\n+        expected_shape_class_logits = torch.Size((1, model.config.num_queries, 2))\n+        self.assertEqual(outputs.decoder_coord_logits.shape, expected_shape_coord_logits)\n+        self.assertEqual(outputs.decoder_class_logits.shape, expected_shape_class_logits)\n+\n+        expected_class_logits = torch.tensor([[[0.9427, -2.5958], [0.2105, -3.4569], [-2.6364, -4.1610]]]).to(\n+            torch_device\n+        )\n+        expected_coord_logits = torch.tensor(\n+            [[[0.2550, 0.5501, 0.4738, 0.8745], [0.7695, 0.4121, 0.4603, 0.7244], [0.7691, 0.4117, 0.4603, 0.7214]]]\n+        ).to(torch_device)\n+\n+        self.assertTrue(torch.allclose(outputs.decoder_class_logits[:3, :3], expected_class_logits, atol=1e-1))\n+        self.assertTrue(torch.allclose(outputs.decoder_coord_logits[:3, :3], expected_coord_logits, atol=1e-3))\n+\n+        # verify grounded postprocessing\n+        results = processor.post_process_grounded_object_detection(\n+            outputs, classes=[classes], target_sizes=[image.size[::-1]]\n+        )[0]\n+        expected_scores = torch.tensor([0.7675, 0.7196, 0.5634, 0.5524]).to(torch_device)\n+        expected_slice_boxes = torch.tensor([39.8870, 70.3522, 176.7424, 118.0354]).to(torch_device)\n+\n+        self.assertEqual(len(results[\"scores\"]), 4)\n+        self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-2))\n+        self.assertTrue(torch.allclose(results[\"boxes\"][0, :], expected_slice_boxes, atol=1e-2))\n+\n+        expected_classes = [\"remote\", \"cat\", \"remote\", \"cat\"]\n+        self.assertListEqual(results[\"classes\"], expected_classes)\n+\n+    def test_inference_object_detection_head_fp16(self):\n+        model = OmDetTurboForObjectDetection.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\").to(\n+            torch_device, dtype=torch.float16\n+        )\n+\n+        processor = self.default_processor\n+        image = prepare_img()\n+        classes, task = prepare_text()\n+        encoding = processor(images=image, text=classes, task=task, return_tensors=\"pt\").to(\n+            torch_device, dtype=torch.float16\n+        )\n+\n+        with torch.no_grad():\n+            outputs = model(**encoding)\n+\n+        expected_shape_coord_logits = torch.Size((1, model.config.num_queries, 4))\n+        expected_shape_class_logits = torch.Size((1, model.config.num_queries, 2))\n+        self.assertEqual(outputs.decoder_coord_logits.shape, expected_shape_coord_logits)\n+        self.assertEqual(outputs.decoder_class_logits.shape, expected_shape_class_logits)\n+\n+        expected_class_logits = torch.tensor([[[0.9427, -2.5958], [0.2105, -3.4569], [-2.6364, -4.1610]]]).to(\n+            torch_device, dtype=torch.float16\n+        )\n+        expected_coord_logits = torch.tensor(\n+            [[[0.2550, 0.5501, 0.4738, 0.8745], [0.7695, 0.4121, 0.4603, 0.7244], [0.7691, 0.4117, 0.4603, 0.7214]]]\n+        ).to(torch_device, dtype=torch.float16)\n+\n+        self.assertTrue(torch.allclose(outputs.decoder_class_logits[:3, :3], expected_class_logits, atol=1e-1))\n+        self.assertTrue(torch.allclose(outputs.decoder_coord_logits[:3, :3], expected_coord_logits, atol=1e-3))\n+\n+        # verify grounded postprocessing\n+        results = processor.post_process_grounded_object_detection(\n+            outputs, classes=[classes], target_sizes=[image.size[::-1]]\n+        )[0]\n+        expected_scores = torch.tensor([0.7675, 0.7196, 0.5634, 0.5524]).to(torch_device, dtype=torch.float16)\n+        expected_slice_boxes = torch.tensor([39.8870, 70.3522, 176.7424, 118.0354]).to(\n+            torch_device, dtype=torch.float16\n+        )\n+\n+        self.assertEqual(len(results[\"scores\"]), 4)\n+        self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-2))\n+        self.assertTrue(torch.allclose(results[\"boxes\"][0, :], expected_slice_boxes, atol=1e-1))\n+\n+        expected_classes = [\"remote\", \"cat\", \"remote\", \"cat\"]\n+        self.assertListEqual(results[\"classes\"], expected_classes)\n+\n+    def test_inference_object_detection_head_no_task(self):\n+        model = OmDetTurboForObjectDetection.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\").to(torch_device)\n+\n+        processor = self.default_processor\n+        image = prepare_img()\n+        classes, _ = prepare_text()\n+        encoding = processor(images=image, text=classes, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**encoding)\n+\n+        expected_shape_coord_logits = torch.Size((1, model.config.num_queries, 4))\n+        expected_shape_class_logits = torch.Size((1, model.config.num_queries, 2))\n+        self.assertEqual(outputs.decoder_coord_logits.shape, expected_shape_coord_logits)\n+        self.assertEqual(outputs.decoder_class_logits.shape, expected_shape_class_logits)\n+\n+        expected_class_logits = torch.tensor([[[0.9427, -2.5958], [0.2105, -3.4569], [-2.6364, -4.1610]]]).to(\n+            torch_device\n+        )\n+        expected_coord_logits = torch.tensor(\n+            [[[0.2550, 0.5501, 0.4738, 0.8745], [0.7695, 0.4121, 0.4603, 0.7244], [0.7691, 0.4117, 0.4603, 0.7214]]]\n+        ).to(torch_device)\n+\n+        self.assertTrue(torch.allclose(outputs.decoder_class_logits[:3, :3], expected_class_logits, atol=1e-1))\n+        self.assertTrue(torch.allclose(outputs.decoder_coord_logits[:3, :3], expected_coord_logits, atol=1e-3))\n+\n+        # verify grounded postprocessing\n+        results = processor.post_process_grounded_object_detection(\n+            outputs, classes=[classes], target_sizes=[image.size[::-1]]\n+        )[0]\n+        expected_scores = torch.tensor([0.7675, 0.7196, 0.5634, 0.5524]).to(torch_device)\n+        expected_slice_boxes = torch.tensor([39.8870, 70.3522, 176.7424, 118.0354]).to(torch_device)\n+\n+        self.assertEqual(len(results[\"scores\"]), 4)\n+        self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-2))\n+        self.assertTrue(torch.allclose(results[\"boxes\"][0, :], expected_slice_boxes, atol=1e-2))\n+\n+        expected_classes = [\"remote\", \"cat\", \"remote\", \"cat\"]\n+        self.assertListEqual(results[\"classes\"], expected_classes)\n+\n+    def test_inference_object_detection_head_batched(self):\n+        torch_device = \"cpu\"\n+        model = OmDetTurboForObjectDetection.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\").to(torch_device)\n+\n+        processor = self.default_processor\n+        images_batched = prepare_img_batched()\n+        classes_batched, tasks_batched = prepare_text_batched()\n+        encoding = processor(images=images_batched, text=classes_batched, task=tasks_batched, return_tensors=\"pt\").to(\n+            torch_device\n+        )\n+\n+        with torch.no_grad():\n+            outputs = model(**encoding)\n+\n+        expected_shape_coord_logits = torch.Size((len(images_batched), model.config.num_queries, 4))\n+        expected_shape_class_logits = torch.Size((len(images_batched), model.config.num_queries, 3))\n+        self.assertEqual(outputs.decoder_coord_logits.shape, expected_shape_coord_logits)\n+        self.assertEqual(outputs.decoder_class_logits.shape, expected_shape_class_logits)\n+\n+        expected_class_logits = torch.tensor(\n+            [[[0.9427, -2.5958, -7.7601]], [[-2.3408, -9.3516, -9.3516]], [[1.0740, -2.3315, -1.1885]]]\n+        ).to(torch_device)\n+\n+        expected_coord_logits = torch.tensor(\n+            [[[0.2550, 0.5501, 0.4738]], [[0.2535, 0.6006, 0.0353]], [[0.3742, 0.3337, 0.0666]]]\n+        ).to(torch_device)\n+\n+        self.assertTrue(torch.allclose(outputs.decoder_class_logits[:, :1, :3], expected_class_logits, atol=1e-1))\n+        self.assertTrue(torch.allclose(outputs.decoder_coord_logits[:, :1, :3], expected_coord_logits, atol=1e-3))\n+\n+        # verify grounded postprocessing\n+        results = processor.post_process_grounded_object_detection(\n+            outputs,\n+            classes=classes_batched,\n+            target_sizes=[image.size[::-1] for image in images_batched],\n+            score_threshold=0.2,\n+        )\n+        expected_scores = torch.tensor([0.7675, 0.3016, 0.7454]).to(torch_device)\n+        expected_slice_boxes = torch.tensor(\n+            [\n+                [39.8870, 70.3522, 176.7424, 118.0354],\n+                [146.5446, 219.7132, 209.6983, 251.0456],\n+                [545.3470, 209.9055, 651.9860, 502.1882],\n+            ]\n+        ).to(torch_device)\n+\n+        self.assertListEqual([len(result[\"scores\"]) for result in results], [4, 4, 6])\n+        self.assertTrue(\n+            torch.allclose(torch.stack([result[\"scores\"][0] for result in results]), expected_scores, atol=1e-2)\n+        )\n+        self.assertTrue(\n+            torch.allclose(torch.stack([result[\"boxes\"][0, :] for result in results]), expected_slice_boxes, atol=1e-2)\n+        )\n+\n+        expected_classes = [\n+            [\"remote\", \"cat\", \"remote\", \"cat\"],\n+            [\"boat\", \"boat\", \"boat\", \"boat\"],\n+            [\"statue\", \"trees\", \"trees\", \"torch\", \"statue\", \"statue\"],\n+        ]\n+        self.assertListEqual([result[\"classes\"] for result in results], expected_classes)\n+\n+    @require_torch_gpu\n+    def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n+        processor = self.default_processor\n+        image = prepare_img()\n+        classes, task = prepare_text()\n+        encoding = processor(images=image, text=classes, task=task, return_tensors=\"pt\")\n+        # 1. run model on CPU\n+        model = OmDetTurboForObjectDetection.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\")\n+\n+        with torch.no_grad():\n+            cpu_outputs = model(**encoding)\n+\n+        # 2. run model on GPU\n+        model.to(\"cuda\")\n+        encoding = encoding.to(\"cuda\")\n+        with torch.no_grad():\n+            gpu_outputs = model(**encoding)\n+\n+        # 3. assert equivalence\n+        expected_class_logits = torch.tensor([[[0.9427, -2.5958], [0.2105, -3.4569], [-2.6364, -4.1610]]])\n+        expected_coord_logits = torch.tensor(\n+            [[[0.2550, 0.5501, 0.4738, 0.8745], [0.7695, 0.4121, 0.4603, 0.7244], [0.7691, 0.4117, 0.4603, 0.7214]]]\n+        )\n+\n+        self.assertTrue(torch.allclose(cpu_outputs.decoder_class_logits[:3, :3], expected_class_logits, atol=1e-1))\n+        self.assertTrue(torch.allclose(cpu_outputs.decoder_coord_logits[:3, :3], expected_coord_logits, atol=1e-3))\n+\n+        # verify grounded postprocessing\n+        results_cpu = processor.post_process_grounded_object_detection(\n+            cpu_outputs, classes=[classes], target_sizes=[image.size[::-1]]\n+        )[0]\n+        result_gpu = processor.post_process_grounded_object_detection(\n+            gpu_outputs, classes=[classes], target_sizes=[image.size[::-1]]\n+        )[0]\n+\n+        self.assertTrue(torch.allclose(results_cpu[\"scores\"], result_gpu[\"scores\"].cpu(), atol=1e-2))\n+        self.assertTrue(torch.allclose(results_cpu[\"boxes\"][0, :], result_gpu[\"boxes\"][0, :].cpu(), atol=1e-2))"
        },
        {
            "sha": "e6e2a1f50c52cd5afd2923f258c76ee69d7ae7e0",
            "filename": "tests/models/omdet_turbo/test_processor_omdet_turbo.py",
            "status": "added",
            "additions": 363,
            "deletions": 0,
            "changes": 363,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/tests%2Fmodels%2Fomdet_turbo%2Ftest_processor_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/tests%2Fmodels%2Fomdet_turbo%2Ftest_processor_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fomdet_turbo%2Ftest_processor_omdet_turbo.py?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7",
            "patch": "@@ -0,0 +1,363 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import shutil\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+import pytest\n+\n+from transformers import AutoProcessor, CLIPTokenizerFast, OmDetTurboProcessor\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+IMAGE_MEAN = [123.675, 116.28, 103.53]\n+IMAGE_STD = [58.395, 57.12, 57.375]\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers.models.omdet_turbo.modeling_omdet_turbo import OmDetTurboObjectDetectionOutput\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import DetrImageProcessor\n+\n+\n+@require_torch\n+@require_vision\n+class OmDetTurboProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = OmDetTurboProcessor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        image_processor = DetrImageProcessor()\n+        tokenizer = CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch32\")\n+\n+        processor = OmDetTurboProcessor(image_processor, tokenizer)\n+        processor.save_pretrained(self.tmpdirname)\n+\n+        self.input_keys = [\n+            \"tasks_input_ids\",\n+            \"tasks_attention_mask\",\n+            \"classes_input_ids\",\n+            \"classes_attention_mask\",\n+            \"classes_structure\",\n+            \"pixel_values\",\n+            \"pixel_mask\",\n+        ]\n+\n+        self.batch_size = 5\n+        self.num_queries = 5\n+        self.embed_dim = 3\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    def prepare_image_inputs(self):\n+        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n+        or a list of PyTorch tensors if one specifies torchify=True.\n+        \"\"\"\n+\n+        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n+\n+        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n+\n+        return image_inputs\n+\n+    def get_fake_omdet_turbo_output(self):\n+        torch.manual_seed(42)\n+        return OmDetTurboObjectDetectionOutput(\n+            decoder_coord_logits=torch.rand(self.batch_size, self.num_queries, 4),\n+            decoder_class_logits=torch.rand(self.batch_size, self.num_queries, self.embed_dim),\n+        )\n+\n+    def get_fake_omdet_turbo_classes(self):\n+        return [[f\"class{i}_{j}\" for i in range(self.num_queries)] for j in range(self.batch_size)]\n+\n+    def test_post_process_grounded_object_detection(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+\n+        processor = OmDetTurboProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+\n+        omdet_turbo_output = self.get_fake_omdet_turbo_output()\n+        omdet_turbo_classes = self.get_fake_omdet_turbo_classes()\n+\n+        post_processed = processor.post_process_grounded_object_detection(\n+            omdet_turbo_output, omdet_turbo_classes, target_sizes=[(400, 30) for _ in range(self.batch_size)]\n+        )\n+\n+        self.assertEqual(len(post_processed), self.batch_size)\n+        self.assertEqual(list(post_processed[0].keys()), [\"boxes\", \"scores\", \"classes\"])\n+        self.assertEqual(post_processed[0][\"boxes\"].shape, (self.num_queries, 4))\n+        self.assertEqual(post_processed[0][\"scores\"].shape, (self.num_queries,))\n+        expected_scores = torch.tensor([0.7310, 0.6579, 0.6513, 0.6444, 0.6252])\n+        self.assertTrue(torch.allclose(post_processed[0][\"scores\"], expected_scores, atol=1e-4))\n+\n+        expected_box_slice = torch.tensor([14.9657, 141.2052, 30.0000, 312.9670])\n+        self.assertTrue(torch.allclose(post_processed[0][\"boxes\"][0], expected_box_slice, atol=1e-4))\n+\n+    def test_save_load_pretrained_additional_features(self):\n+        processor = OmDetTurboProcessor(tokenizer=self.get_tokenizer(), image_processor=self.get_image_processor())\n+        processor.save_pretrained(self.tmpdirname)\n+\n+        tokenizer_add_kwargs = self.get_tokenizer(bos_token=\"(BOS)\", eos_token=\"(EOS)\")\n+        image_processor_add_kwargs = self.get_image_processor(do_normalize=False, padding_value=1.0)\n+\n+        processor = OmDetTurboProcessor.from_pretrained(\n+            self.tmpdirname, bos_token=\"(BOS)\", eos_token=\"(EOS)\", do_normalize=False, padding_value=1.0\n+        )\n+\n+        self.assertEqual(processor.tokenizer.get_vocab(), tokenizer_add_kwargs.get_vocab())\n+        self.assertIsInstance(processor.tokenizer, CLIPTokenizerFast)\n+\n+        self.assertEqual(processor.image_processor.to_json_string(), image_processor_add_kwargs.to_json_string())\n+        self.assertIsInstance(processor.image_processor, DetrImageProcessor)\n+\n+    def test_image_processor(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+\n+        processor = OmDetTurboProcessor(tokenizer=tokenizer, image_processor=image_processor).image_processor\n+\n+        image_input = self.prepare_image_inputs()\n+\n+        input_image_proc = image_processor(image_input, return_tensors=\"np\")\n+        input_processor = processor(images=image_input, return_tensors=\"np\")\n+\n+        for key in input_image_proc.keys():\n+            self.assertAlmostEqual(input_image_proc[key].sum(), input_processor[key].sum(), delta=1e-2)\n+\n+    def test_tokenizer(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+\n+        processor = OmDetTurboProcessor(tokenizer=tokenizer, image_processor=image_processor).tokenizer\n+\n+        input_str = \"lower newer\"\n+\n+        encoded_processor = processor(text=input_str, padding=\"max_length\", truncation=True, max_length=77)\n+\n+        encoded_tok = tokenizer(input_str, padding=\"max_length\", truncation=True, max_length=77)\n+\n+        for key in encoded_tok.keys():\n+            self.assertListEqual(encoded_tok[key], encoded_processor[key])\n+\n+    def test_processor(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+\n+        processor = OmDetTurboProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+\n+        input_tasks = \"task\"\n+        input_classes = [\"class1\", \"class2\"]\n+        image_input = self.prepare_image_inputs()\n+\n+        input_processor = processor(images=image_input, text=input_classes, task=input_tasks, return_tensors=\"pt\")\n+\n+        for key in self.input_keys:\n+            assert torch.is_tensor(input_processor[key])\n+        # test if it raises when no input is passed\n+        with pytest.raises(ValueError):\n+            processor()\n+\n+    def test_tokenizer_decode(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+\n+        processor = OmDetTurboProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+\n+        predicted_ids = [[1, 4, 5, 8, 1, 0, 8], [3, 4, 3, 1, 1, 8, 9]]\n+\n+        decoded_processor = processor.batch_decode(predicted_ids)\n+        decoded_tok = tokenizer.batch_decode(predicted_ids)\n+\n+        self.assertListEqual(decoded_tok, decoded_processor)\n+\n+    def test_model_input_names(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+\n+        processor = OmDetTurboProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+\n+        input_tasks = \"task\"\n+        input_classes = [\"class1\", \"class2\"]\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(images=image_input, text=input_classes, task=input_tasks, return_tensors=\"pt\")\n+\n+        self.assertListEqual(list(inputs.keys()), self.input_keys)\n+\n+    @require_vision\n+    @require_torch\n+    def test_tokenizer_defaults_preserved_by_kwargs(self):\n+        # Rewrite as OmDet-Turbo processor outputs \"input_ids\" for both tasks and classes.\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(images=image_input, text=[input_str], task=input_str, return_tensors=\"pt\")\n+\n+        self.assertEqual(len(inputs[\"tasks_input_ids\"][0]), 117)\n+        self.assertEqual(len(inputs[\"classes_input_ids\"][0]), 117)\n+\n+    @require_vision\n+    @require_torch\n+    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n+        # Rewrite as OmDet-Turbo processor outputs \"input_ids\" for both tasks and classes.\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(images=image_input, text=[input_str], task=input_str, return_tensors=\"pt\", max_length=112)\n+\n+        self.assertEqual(len(inputs[\"tasks_input_ids\"][0]), 112)\n+        self.assertEqual(len(inputs[\"classes_input_ids\"][0]), 112)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs(self):\n+        # Rewrite as OmDet-Turbo processor outputs \"input_ids\" for both tasks and classes.\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(\n+            images=image_input,\n+            text=[input_str],\n+            task=input_str,\n+            return_tensors=\"pt\",\n+            size={\"height\": 214, \"width\": 214},\n+            padding=\"max_length\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+        self.assertEqual(len(inputs[\"tasks_input_ids\"][0]), 76)\n+        self.assertEqual(len(inputs[\"classes_input_ids\"][0]), 76)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched(self):\n+        # Rewrite as OmDet-Turbo processor outputs \"input_ids\" for both tasks and classes.\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\", \"upper older longer string\"]\n+        image_input = self.prepare_image_inputs() * 2\n+        inputs = processor(\n+            images=image_input,\n+            text=[input_str],\n+            task=input_str,\n+            return_tensors=\"pt\",\n+            size={\"height\": 214, \"width\": 214},\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"tasks_input_ids\"][0]), 6)\n+        self.assertEqual(len(inputs[\"classes_input_ids\"][0]), 6)\n+\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested(self):\n+        # Rewrite as OmDet-Turbo processor outputs \"input_ids\" for both tasks and classes.\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76, \"task\": input_str},\n+        }\n+\n+        inputs = processor(images=image_input, text=[input_str], **all_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"tasks_input_ids\"][0]), 76)\n+        self.assertEqual(len(inputs[\"classes_input_ids\"][0]), 76)\n+\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested_from_dict(self):\n+        # Rewrite as OmDet-Turbo processor outputs \"input_ids\" for both tasks and classes.\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76, \"task\": input_str},\n+        }\n+\n+        inputs = processor(images=image_input, text=[input_str], **all_kwargs)\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"tasks_input_ids\"][0]), 76)\n+        self.assertEqual(len(inputs[\"classes_input_ids\"][0]), 76)"
        },
        {
            "sha": "5876818449558edad8aadd0577ee354c4c437bce",
            "filename": "utils/check_table.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f18cf23c128055a984ffbe9c57df133c1f6cc7/utils%2Fcheck_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f18cf23c128055a984ffbe9c57df133c1f6cc7/utils%2Fcheck_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_table.py?ref=94f18cf23c128055a984ffbe9c57df133c1f6cc7",
            "patch": "@@ -173,7 +173,13 @@ def _center_text(text: str, width: int) -> str:\n     \"XLS-R\": \"Wav2Vec2\",\n     \"XLSR-Wav2Vec2\": \"Wav2Vec2\",\n }\n-MODEL_NAMES_TO_IGNORE = [\"CLIPVisionModel\", \"SiglipVisionModel\", \"ChineseCLIPVisionModel\", \"Qwen2AudioEncoder\"]\n+MODEL_NAMES_TO_IGNORE = [\n+    \"ChineseCLIPVisionModel\",\n+    \"CLIPTextModel\",\n+    \"CLIPVisionModel\",\n+    \"Qwen2AudioEncoder\",\n+    \"SiglipVisionModel\",\n+]\n \n \n def get_model_table_from_auto_modules() -> str:"
        }
    ],
    "stats": {
        "total": 4355,
        "additions": 4354,
        "deletions": 1
    }
}