{
    "author": "cyyever",
    "message": "Fix the error where a keyword argument appearing before *args (#41099)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "071c7b142321ae31b621ccd8b4446d4a39fa3a58",
    "files": [
        {
            "sha": "fce524d4a6c0b219568229ea9b5ff387352a4485",
            "filename": "src/transformers/commands/add_new_model_like.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/071c7b142321ae31b621ccd8b4446d4a39fa3a58/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071c7b142321ae31b621ccd8b4446d4a39fa3a58/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py?ref=071c7b142321ae31b621ccd8b4446d4a39fa3a58",
            "patch": "@@ -755,7 +755,7 @@ def register_subcommand(parser: ArgumentParser):\n         )\n         add_new_model_like_parser.set_defaults(func=add_new_model_like_command_factory)\n \n-    def __init__(self, path_to_repo=None, *args):\n+    def __init__(self, path_to_repo=None, **kwargs):\n         (\n             self.old_model_infos,\n             self.new_lowercase_name,"
        },
        {
            "sha": "77c013d8926320c79f368246223c2e77438ff606",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/071c7b142321ae31b621ccd8b4446d4a39fa3a58/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071c7b142321ae31b621ccd8b4446d4a39fa3a58/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=071c7b142321ae31b621ccd8b4446d4a39fa3a58",
            "patch": "@@ -1454,7 +1454,7 @@ def pre_tokenizer(self, replacement, add_prefix_space):\n class HeliumConverter(SpmConverter):\n     handle_byte_fallback = True\n \n-    def __init__(self, vocab_file=None, *args):\n+    def __init__(self, vocab_file=None, **kwargs):\n         requires_backends(self, \"protobuf\")\n \n         Converter.__init__(self, vocab_file)\n@@ -1576,10 +1576,8 @@ def __init__(\n         pattern=r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\",\n         add_prefix_space=False,\n         additional_special_tokens=None,\n-        *args,\n         **kwargs,\n     ):\n-        super().__init__(*args)\n         self.vocab_file = vocab_file\n         self.pattern = pattern\n         self.add_prefix_space = add_prefix_space"
        },
        {
            "sha": "cdf237645fc102ae5273358eda10354a0a8b80a2",
            "filename": "src/transformers/integrations/mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/071c7b142321ae31b621ccd8b4446d4a39fa3a58/src%2Ftransformers%2Fintegrations%2Fmistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071c7b142321ae31b621ccd8b4446d4a39fa3a58/src%2Ftransformers%2Fintegrations%2Fmistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmistral.py?ref=071c7b142321ae31b621ccd8b4446d4a39fa3a58",
            "patch": "@@ -16,10 +16,8 @@ def __init__(\n         pattern=r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\",\n         add_prefix_space=False,\n         additional_special_tokens=None,\n-        *args,\n         **kwargs,\n     ):\n-        super().__init__(*args)\n         self.vocab = vocab\n         self.pattern = pattern\n         self.add_prefix_space = add_prefix_space"
        },
        {
            "sha": "96bcc863cbe791dde19497d876df754d4fdaf683",
            "filename": "src/transformers/pipelines/table_question_answering.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/071c7b142321ae31b621ccd8b4446d4a39fa3a58/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071c7b142321ae31b621ccd8b4446d4a39fa3a58/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py?ref=071c7b142321ae31b621ccd8b4446d4a39fa3a58",
            "patch": "@@ -122,8 +122,8 @@ class TableQuestionAnsweringPipeline(Pipeline):\n         max_new_tokens=256,\n     )\n \n-    def __init__(self, args_parser=TableQuestionAnsweringArgumentHandler(), *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n+    def __init__(self, args_parser=TableQuestionAnsweringArgumentHandler(), **kwargs):\n+        super().__init__(**kwargs)\n         self._args_parser = args_parser\n \n         mapping = MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING_NAMES.copy()"
        },
        {
            "sha": "12de5c19e2ca4818421d9d361b7ca1932c674602",
            "filename": "src/transformers/pipelines/token_classification.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/071c7b142321ae31b621ccd8b4446d4a39fa3a58/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071c7b142321ae31b621ccd8b4446d4a39fa3a58/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py?ref=071c7b142321ae31b621ccd8b4446d4a39fa3a58",
            "patch": "@@ -136,8 +136,8 @@ class TokenClassificationPipeline(ChunkPipeline):\n     _load_feature_extractor = False\n     _load_tokenizer = True\n \n-    def __init__(self, args_parser=TokenClassificationArgumentHandler(), *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n+    def __init__(self, args_parser=TokenClassificationArgumentHandler(), **kwargs):\n+        super().__init__(**kwargs)\n \n         self.check_model_type(MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES)\n "
        },
        {
            "sha": "917f4d753f8f03bcc9f233f313170a0d25c14ac3",
            "filename": "src/transformers/pipelines/zero_shot_classification.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/071c7b142321ae31b621ccd8b4446d4a39fa3a58/src%2Ftransformers%2Fpipelines%2Fzero_shot_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071c7b142321ae31b621ccd8b4446d4a39fa3a58/src%2Ftransformers%2Fpipelines%2Fzero_shot_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fzero_shot_classification.py?ref=071c7b142321ae31b621ccd8b4446d4a39fa3a58",
            "patch": "@@ -87,9 +87,9 @@ class ZeroShotClassificationPipeline(ChunkPipeline):\n     _load_feature_extractor = False\n     _load_tokenizer = True\n \n-    def __init__(self, args_parser=ZeroShotClassificationArgumentHandler(), *args, **kwargs):\n+    def __init__(self, args_parser=ZeroShotClassificationArgumentHandler(), **kwargs):\n         self._args_parser = args_parser\n-        super().__init__(*args, **kwargs)\n+        super().__init__(**kwargs)\n         if self.entailment_id == -1:\n             logger.warning(\n                 \"Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to \""
        },
        {
            "sha": "15ca6cec5b3146f1903bb6374341a47f1551cfde",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/071c7b142321ae31b621ccd8b4446d4a39fa3a58/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071c7b142321ae31b621ccd8b4446d4a39fa3a58/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=071c7b142321ae31b621ccd8b4446d4a39fa3a58",
            "patch": "@@ -1349,7 +1349,7 @@ class LayerWiseDummyOptimizer(torch.optim.Optimizer):\n     https://github.com/hiyouga/LLaMA-Factory/commit/8664262cde3919e10eaecbd66e8c5d356856362e#diff-ebe08ab14496dfb9e06075f0fdd36799ef6d1535cc4dd4715b74c4e3e06fe3ba\n     \"\"\"\n \n-    def __init__(self, optimizer_dict=None, *args, **kwargs):\n+    def __init__(self, optimizer_dict=None, **kwargs):\n         dummy_tensor = torch.randn(1, 1)\n         self.optimizer_dict = optimizer_dict\n         super().__init__([dummy_tensor], {\"lr\": kwargs.get(\"lr\", 1e-03)})"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 9,
        "deletions": 13
    }
}