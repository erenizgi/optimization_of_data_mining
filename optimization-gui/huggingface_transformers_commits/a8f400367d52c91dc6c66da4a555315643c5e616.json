{
    "author": "cyyever",
    "message": "Avoid attention_mask copy in qwen2.5 (#40658)\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "a8f400367d52c91dc6c66da4a555315643c5e616",
    "files": [
        {
            "sha": "e8419bcf40c43e704172e50b676850f40560a443",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a8f400367d52c91dc6c66da4a555315643c5e616/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a8f400367d52c91dc6c66da4a555315643c5e616/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=a8f400367d52c91dc6c66da4a555315643c5e616",
            "patch": "@@ -265,8 +265,8 @@ def get_rope_index(\n         mrope_position_deltas = []\n         if input_ids is not None and (image_grid_thw is not None or video_grid_thw is not None):\n             total_input_ids = input_ids\n-            if attention_mask is None:\n-                attention_mask = torch.ones_like(total_input_ids)\n+            if attention_mask is not None:\n+                attention_mask = attention_mask == 1\n             position_ids = torch.ones(\n                 3,\n                 input_ids.shape[0],\n@@ -275,9 +275,9 @@ def get_rope_index(\n                 device=input_ids.device,\n             )\n             image_idx, video_idx, audio_idx = 0, 0, 0\n-            attention_mask = attention_mask.to(total_input_ids.device)\n             for i, input_ids in enumerate(total_input_ids):\n-                input_ids = input_ids[attention_mask[i] == 1]\n+                if attention_mask is not None:\n+                    input_ids = input_ids[attention_mask[i]]\n                 image_nums, video_nums, audio_nums = 0, 0, 0\n                 vision_start_indices = torch.argwhere(input_ids == vision_start_token_id).squeeze(1)\n                 vision_tokens = input_ids[vision_start_indices + 1]\n@@ -458,9 +458,12 @@ def get_rope_index(\n \n                 llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)\n \n-                position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(position_ids.device)\n+                if attention_mask is not None:\n+                    position_ids[..., i, attention_mask[i]] = llm_positions.to(position_ids.device)\n+                else:\n+                    position_ids[..., i, :] = llm_positions.to(position_ids.device)\n                 mrope_position_deltas.append(llm_positions.max() + 1 - len(input_ids))\n-            mrope_position_deltas = torch.tensor(mrope_position_deltas, device=input_ids.device).unsqueeze(1)\n+            mrope_position_deltas = torch.tensor(mrope_position_deltas).unsqueeze(1).to(device=input_ids.device)\n \n             return position_ids, mrope_position_deltas\n         else:"
        },
        {
            "sha": "eb5679194b9033b01e99e87a15c17f406eb90ef7",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a8f400367d52c91dc6c66da4a555315643c5e616/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a8f400367d52c91dc6c66da4a555315643c5e616/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=a8f400367d52c91dc6c66da4a555315643c5e616",
            "patch": "@@ -1320,8 +1320,8 @@ def get_rope_index(\n         mrope_position_deltas = []\n         if input_ids is not None and (image_grid_thw is not None or video_grid_thw is not None):\n             total_input_ids = input_ids\n-            if attention_mask is None:\n-                attention_mask = torch.ones_like(total_input_ids)\n+            if attention_mask is not None:\n+                attention_mask = attention_mask == 1\n             position_ids = torch.ones(\n                 3,\n                 input_ids.shape[0],\n@@ -1330,9 +1330,9 @@ def get_rope_index(\n                 device=input_ids.device,\n             )\n             image_idx, video_idx, audio_idx = 0, 0, 0\n-            attention_mask = attention_mask.to(total_input_ids.device)\n             for i, input_ids in enumerate(total_input_ids):\n-                input_ids = input_ids[attention_mask[i] == 1]\n+                if attention_mask is not None:\n+                    input_ids = input_ids[attention_mask[i]]\n                 image_nums, video_nums, audio_nums = 0, 0, 0\n                 vision_start_indices = torch.argwhere(input_ids == vision_start_token_id).squeeze(1)\n                 vision_tokens = input_ids[vision_start_indices + 1]\n@@ -1513,9 +1513,12 @@ def get_rope_index(\n \n                 llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)\n \n-                position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(position_ids.device)\n+                if attention_mask is not None:\n+                    position_ids[..., i, attention_mask[i]] = llm_positions.to(position_ids.device)\n+                else:\n+                    position_ids[..., i, :] = llm_positions.to(position_ids.device)\n                 mrope_position_deltas.append(llm_positions.max() + 1 - len(input_ids))\n-            mrope_position_deltas = torch.tensor(mrope_position_deltas, device=input_ids.device).unsqueeze(1)\n+            mrope_position_deltas = torch.tensor(mrope_position_deltas).unsqueeze(1).to(device=input_ids.device)\n \n             return position_ids, mrope_position_deltas\n         else:"
        },
        {
            "sha": "fd67771f542ed6691edc987d3c49858e64c02049",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a8f400367d52c91dc6c66da4a555315643c5e616/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a8f400367d52c91dc6c66da4a555315643c5e616/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=a8f400367d52c91dc6c66da4a555315643c5e616",
            "patch": "@@ -1033,8 +1033,8 @@ def get_rope_index(\n         mrope_position_deltas = []\n         if input_ids is not None and (image_grid_thw is not None or video_grid_thw is not None):\n             total_input_ids = input_ids\n-            if attention_mask is None:\n-                attention_mask = torch.ones_like(total_input_ids)\n+            if attention_mask is not None:\n+                attention_mask = attention_mask == 1\n             position_ids = torch.ones(\n                 3,\n                 input_ids.shape[0],\n@@ -1043,9 +1043,9 @@ def get_rope_index(\n                 device=input_ids.device,\n             )\n             image_index, video_index = 0, 0\n-            attention_mask = attention_mask.to(total_input_ids.device)\n             for i, input_ids in enumerate(total_input_ids):\n-                input_ids = input_ids[attention_mask[i] == 1]\n+                if attention_mask is not None:\n+                    input_ids = input_ids[attention_mask[i]]\n                 image_nums, video_nums = 0, 0\n                 vision_start_indices = torch.argwhere(input_ids == vision_start_token_id).squeeze(1)\n                 vision_tokens = input_ids[vision_start_indices + 1]\n@@ -1122,9 +1122,12 @@ def get_rope_index(\n                     llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)\n \n                 llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)\n-                position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(position_ids.device)\n+                if attention_mask is not None:\n+                    position_ids[..., i, attention_mask[i]] = llm_positions.to(position_ids.device)\n+                else:\n+                    position_ids[..., i, :] = llm_positions.to(position_ids.device)\n                 mrope_position_deltas.append(llm_positions.max() + 1 - len(total_input_ids[i]))\n-            mrope_position_deltas = torch.tensor(mrope_position_deltas, device=input_ids.device).unsqueeze(1)\n+            mrope_position_deltas = torch.tensor(mrope_position_deltas).unsqueeze(1).to(device=input_ids.device)\n             return position_ids, mrope_position_deltas\n         else:\n             if attention_mask is not None:"
        },
        {
            "sha": "55f77c3cfbbaec7b2d60a0638bd26030aec1542f",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a8f400367d52c91dc6c66da4a555315643c5e616/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a8f400367d52c91dc6c66da4a555315643c5e616/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=a8f400367d52c91dc6c66da4a555315643c5e616",
            "patch": "@@ -421,8 +421,8 @@ def get_rope_index(\n         mrope_position_deltas = []\n         if input_ids is not None and (image_grid_thw is not None or video_grid_thw is not None):\n             total_input_ids = input_ids\n-            if attention_mask is None:\n-                attention_mask = torch.ones_like(total_input_ids)\n+            if attention_mask is not None:\n+                attention_mask = attention_mask == 1\n             position_ids = torch.ones(\n                 3,\n                 input_ids.shape[0],\n@@ -431,9 +431,9 @@ def get_rope_index(\n                 device=input_ids.device,\n             )\n             image_index, video_index = 0, 0\n-            attention_mask = attention_mask.to(total_input_ids.device)\n             for i, input_ids in enumerate(total_input_ids):\n-                input_ids = input_ids[attention_mask[i] == 1]\n+                if attention_mask is not None:\n+                    input_ids = input_ids[attention_mask[i]]\n                 image_nums, video_nums = 0, 0\n                 vision_start_indices = torch.argwhere(input_ids == vision_start_token_id).squeeze(1)\n                 vision_tokens = input_ids[vision_start_indices + 1]\n@@ -510,9 +510,12 @@ def get_rope_index(\n                     llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)\n \n                 llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)\n-                position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(position_ids.device)\n+                if attention_mask is not None:\n+                    position_ids[..., i, attention_mask[i]] = llm_positions.to(position_ids.device)\n+                else:\n+                    position_ids[..., i, :] = llm_positions.to(position_ids.device)\n                 mrope_position_deltas.append(llm_positions.max() + 1 - len(total_input_ids[i]))\n-            mrope_position_deltas = torch.tensor(mrope_position_deltas, device=input_ids.device).unsqueeze(1)\n+            mrope_position_deltas = torch.tensor(mrope_position_deltas).unsqueeze(1).to(device=input_ids.device)\n             return position_ids, mrope_position_deltas\n         else:\n             if attention_mask is not None:"
        }
    ],
    "stats": {
        "total": 60,
        "additions": 36,
        "deletions": 24
    }
}