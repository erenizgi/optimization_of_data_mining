{
    "author": "SunMarc",
    "message": "More trainer cleaning  (#41489)\n\nclean",
    "sha": "bc529a3368e9942bcf42ebf02bdd5546ef39f9be",
    "files": [
        {
            "sha": "2e97168ded4bc790a413c0ed369329ad002dbab6",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc529a3368e9942bcf42ebf02bdd5546ef39f9be/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc529a3368e9942bcf42ebf02bdd5546ef39f9be/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=bc529a3368e9942bcf42ebf02bdd5546ef39f9be",
            "patch": "@@ -1272,12 +1272,6 @@ class TrainingArguments:\n             \"help\": \"Whether to recursively concat inputs/losses/labels/predictions across batches. If `False`, will instead store them as lists, with each batch kept separate.\"\n         },\n     )\n-    _n_gpu: int = field(init=False, repr=False, default=-1)\n-    mp_parameters: str = field(\n-        default=\"\",\n-        metadata={\"help\": \"Used by the SageMaker launcher to send mp-specific args. Ignored in Trainer\"},\n-    )\n-\n     auto_find_batch_size: bool = field(\n         default=False,\n         metadata={"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 0,
        "deletions": 6
    }
}