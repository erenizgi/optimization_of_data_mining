{
    "author": "gante",
    "message": "Generate: move `prepare_inputs_for_generation` in encoder-decoder llms (#34048)",
    "sha": "37ac07853539216baef9abadfda5782e7b20dc87",
    "files": [
        {
            "sha": "2225b033aa0a9ef515616c2326a65291854d1bf1",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -387,13 +387,14 @@ def prepare_inputs_for_generation(\n                 input_ids = input_ids[:, cache_position]\n \n         # 3. Prepare base model inputs\n+        input_ids_key = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs[\"input_ids\"] = None\n+        if inputs_embeds is not None and not self.config.is_encoder_decoder and cache_position[0] == 0:\n+            model_inputs[input_ids_key] = None\n             model_inputs[\"inputs_embeds\"] = inputs_embeds\n         else:\n             # `clone` calls in this function ensure a consistent stride. See #32227\n-            model_inputs[\"input_ids\"] = input_ids.clone(memory_format=torch.contiguous_format)\n+            model_inputs[input_ids_key] = input_ids.clone(memory_format=torch.contiguous_format)\n             model_inputs[\"inputs_embeds\"] = None\n \n         # 4. Create missing `position_ids` on the fly\n@@ -421,8 +422,8 @@ def prepare_inputs_for_generation(\n                 batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n                 device = model_inputs[\"inputs_embeds\"].device\n             else:\n-                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n-                device = model_inputs[\"input_ids\"].device\n+                batch_size, sequence_length = model_inputs[input_ids_key].shape\n+                device = model_inputs[input_ids_key].device\n \n             # Create the causal mask with fixed shape in advance, to reduce recompilations. If the function to create\n             # the 4D causal mask exists, it should be present in the base model (XXXModel class).\n@@ -455,6 +456,8 @@ def prepare_inputs_for_generation(\n             if key not in model_inputs:\n                 model_inputs[key] = value\n \n+        # 8. Remove unexpected `generate` inputs (TODO @joao: fix trainer and examples)\n+        model_inputs.pop(\"labels\", None)\n         return model_inputs\n \n     def _prepare_model_inputs("
        },
        {
            "sha": "07c1fa622ea3b6895deed91d0d6a98859d4d7013",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 39,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -1682,45 +1682,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        decoder_attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past_key_values is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if decoder_input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = decoder_input_ids.shape[1] - 1\n-\n-            decoder_input_ids = decoder_input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"decoder_attention_mask\": decoder_attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n-        }\n-\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n "
        },
        {
            "sha": "19540a7498f5bdb21fee24091a8fa6b5dfecb931",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 39,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -2561,45 +2561,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        decoder_attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past_key_values is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if decoder_input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = decoder_input_ids.shape[1] - 1\n-\n-            decoder_input_ids = decoder_input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"decoder_attention_mask\": decoder_attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n-        }\n-\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n "
        },
        {
            "sha": "5c4fdfb472c37e707d85ebadad0203425935ab64",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -1333,43 +1333,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if decoder_input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = decoder_input_ids.shape[1] - 1\n-\n-            decoder_input_ids = decoder_input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n-        }\n-\n     @staticmethod\n     def _reorder_cache(past_key_values, beam_idx):\n         reordered_past = ()"
        },
        {
            "sha": "6f79d2a7d005cc84db95c37a80dc227fa65813f3",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -1285,43 +1285,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if decoder_input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = decoder_input_ids.shape[1] - 1\n-\n-            decoder_input_ids = decoder_input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n-        }\n-\n     @staticmethod\n     def _reorder_cache(past_key_values, beam_idx):\n         reordered_past = ()"
        },
        {
            "sha": "5ee7ae21f9d549a35e5f76942c074674efb3f8da",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -915,6 +915,8 @@ def forward(\n         )\n \n     def prepare_inputs_for_generation(self, input_ids, past_key_values=None, attention_mask=None, **model_kwargs):\n+        # Overwrite -- hardcoded key return (`is_decoder=True`)\n+\n         input_shape = input_ids.shape\n         # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n         if attention_mask is None:"
        },
        {
            "sha": "3f865c037c01b614555e372b187eaa48ede69f92",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 25,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -1191,7 +1191,7 @@ def __init__(self, config: FSMTConfig):\n     @add_end_docstrings(FSMT_GENERATION_EXAMPLE)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n@@ -1263,30 +1263,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n-        }\n-\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id)\n "
        },
        {
            "sha": "ee1ad90bfceaa289a3f5f4a89579a642c87285bd",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -2437,36 +2437,6 @@ def forward(\n             encoder_global_attentions=outputs.encoder_global_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        global_attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            decoder_input_ids = decoder_input_ids[:, -1:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"global_attention_mask\": global_attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n-        }\n-\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n "
        },
        {
            "sha": "d351e798ac7f8838dda0fca29bfb737991e3bc06",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 36,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -2085,42 +2085,6 @@ def forward(\n             encoder_attentions=encoder_outputs.attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past_key_values is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = input_ids.shape[1] - 1\n-\n-            input_ids = input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"decoder_input_ids\": input_ids,\n-            \"past_key_values\": past_key_values,\n-            \"encoder_outputs\": encoder_outputs,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,\n-        }\n-\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return self._shift_right(labels)\n "
        },
        {
            "sha": "cc35a3504255bfa79e743b5aca5dcb90c4384e6d",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -1621,43 +1621,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if decoder_input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = decoder_input_ids.shape[1] - 1\n-\n-            decoder_input_ids = decoder_input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n-        }\n-\n     @staticmethod\n     def _reorder_cache(past_key_values, beam_idx):\n         reordered_past = ()"
        },
        {
            "sha": "2d7c7d85daed64b957db52526ffa0edf9df68147",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 38,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -16,7 +16,7 @@\n \n import copy\n import math\n-from typing import Dict, List, Optional, Tuple, Union\n+from typing import List, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -1438,43 +1438,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids: torch.LongTensor,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n-        encoder_outputs: Optional[Union[Tuple[torch.Tensor], BaseModelOutput]] = None,\n-        **kwargs,\n-    ) -> Dict:\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if decoder_input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = decoder_input_ids.shape[1] - 1\n-\n-            decoder_input_ids = decoder_input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n-        }\n-\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n "
        },
        {
            "sha": "95cd7c65ef32c21da38282b6b1ff82e902a4430f",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -1647,43 +1647,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if decoder_input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = decoder_input_ids.shape[1] - 1\n-\n-            decoder_input_ids = decoder_input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n-        }\n-\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id)\n "
        },
        {
            "sha": "9051414d7414fa85f785e2e8d268b9f166d83e11",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 39,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -1820,45 +1820,6 @@ def forward(\n             encoder_attentions=encoder_outputs.attentions,\n         )\n \n-    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.prepare_inputs_for_generation\n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        decoder_attention_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past_key_values is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = input_ids.shape[1] - 1\n-\n-            input_ids = input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"decoder_input_ids\": input_ids,\n-            \"past_key_values\": past_key_values,\n-            \"encoder_outputs\": encoder_outputs,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"decoder_attention_mask\": decoder_attention_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,\n-        }\n-\n     # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.prepare_decoder_input_ids_from_labels\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return self._shift_right(labels)"
        },
        {
            "sha": "f68a4bb76b3e71d0826604406c9e42580cdac9e9",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -1475,43 +1475,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if decoder_input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = decoder_input_ids.shape[1] - 1\n-\n-            decoder_input_ids = decoder_input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n-        }\n-\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n "
        },
        {
            "sha": "9c095be16506e86114983eb29a57569e9a21e7d6",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -1762,44 +1762,6 @@ def _unpack_router_logits(self, router_outputs):\n         total_expert_indexes = torch.stack(total_expert_indexes, dim=1) if len(total_expert_indexes) > 0 else None\n         return total_router_logits, total_expert_indexes\n \n-    # Copied from transfomers.models.switch_transformers.SwitchTransformersForConditionalGeneration.prepare_inputs_for_generation\n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if decoder_input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = decoder_input_ids.shape[1] - 1\n-\n-            decoder_input_ids = decoder_input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n-        }\n-\n     @staticmethod\n     def _reorder_cache(past_key_values, beam_idx):\n         reordered_past = ()"
        },
        {
            "sha": "a737ef14d647cf04ca3b420ddfe8ec79250c3963",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -1390,43 +1390,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if decoder_input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = decoder_input_ids.shape[1] - 1\n-\n-            decoder_input_ids = decoder_input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n-        }\n-\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n "
        },
        {
            "sha": "f90a8d2deb26510ab3a99568bd578f33d7d5ac14",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -1588,37 +1588,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if decoder_input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = decoder_input_ids.shape[1] - 1\n-\n-            decoder_input_ids = decoder_input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n-        }\n-\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n "
        },
        {
            "sha": "490fefc686a524cb5fb145dfce01668c365b1aad",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 38,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -16,7 +16,7 @@\n \n import copy\n import math\n-from typing import Any, Dict, List, Optional, Tuple, Union\n+from typing import List, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -1372,43 +1372,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids: torch.LongTensor,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        decoder_head_mask: Optional[torch.Tensor] = None,\n-        cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n-        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n-        **kwargs,  # TODO: Check if this is needed. It is unused?\n-    ) -> Dict[str, Any]:\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if decoder_input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = decoder_input_ids.shape[1] - 1\n-\n-            decoder_input_ids = decoder_input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n-        }\n-\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id)\n "
        },
        {
            "sha": "137bd5ad828df5f1df2daa7862ec07cc7e7ad745",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -2018,35 +2018,6 @@ def _compute_loss(self, logits, labels, ignore_index=-100):\n \n         return loss\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        assert encoder_outputs is not None, \"`encoder_outputs` have to be passed for generation.\"\n-\n-        if past_key_values:\n-            decoder_input_ids = decoder_input_ids[:, -1:]\n-        # first step, decoder_cached_states are empty\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,\n-        }\n-\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return self._shift_right(labels)\n "
        },
        {
            "sha": "5e6f13ca478f323279ac1430e1b9a636f99b865a",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -1172,6 +1172,8 @@ def prepare_inputs_for_generation(\n         n_docs=None,\n         **kwargs,\n     ):\n+        # Overwritten -- `do_marginalize` is explicitly set in the output\n+\n         if past_key_values is not None:\n             # if past is defined use only last decoder_input_ids\n             decoder_input_ids = decoder_input_ids[:, -1:]"
        },
        {
            "sha": "c39e85bacdd3d14c66e5ceca7c5e1eb6f9a7a4ce",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 39,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -1702,45 +1702,6 @@ def _unpack_router_logits(self, router_outputs):\n                 total_expert_indexes.append(expert_indexes)\n         return torch.cat(total_router_logits, dim=1), torch.cat(total_expert_indexes, dim=1)\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past_key_values is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = input_ids.shape[1] - 1\n-\n-            input_ids = input_ids[:, remove_prefix_length:]\n-\n-        output_router_logits = kwargs.get(\"output_router_logits\", True)\n-\n-        return {\n-            \"decoder_input_ids\": input_ids,\n-            \"past_key_values\": past_key_values,\n-            \"encoder_outputs\": encoder_outputs,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,\n-            \"output_router_logits\": output_router_logits,\n-        }\n-\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return self._shift_right(labels)\n "
        },
        {
            "sha": "91596f013ab4f59bd91b6b72ca9d7a30259d38a6",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -1791,44 +1791,6 @@ def forward(\n             encoder_attentions=encoder_outputs.attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        decoder_attention_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past_key_values is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = input_ids.shape[1] - 1\n-\n-            input_ids = input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"decoder_input_ids\": input_ids,\n-            \"past_key_values\": past_key_values,\n-            \"encoder_outputs\": encoder_outputs,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"decoder_attention_mask\": decoder_attention_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,\n-        }\n-\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return self._shift_right(labels)\n "
        },
        {
            "sha": "bd621fc2fb3ac2579440943133ef7208660d5dac",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 39,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -1302,45 +1302,6 @@ def forward(\n             encoder_attentions=encoder_outputs.attentions,\n         )\n \n-    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.prepare_inputs_for_generation\n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        decoder_attention_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past_key_values is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = input_ids.shape[1] - 1\n-\n-            input_ids = input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"decoder_input_ids\": input_ids,\n-            \"past_key_values\": past_key_values,\n-            \"encoder_outputs\": encoder_outputs,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"decoder_attention_mask\": decoder_attention_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,\n-        }\n-\n     # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.prepare_decoder_input_ids_from_labels\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return self._shift_right(labels)"
        },
        {
            "sha": "81326c07d6cce9c05751b6d0317a8ed49606e50f",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -1519,6 +1519,8 @@ def prepare_inputs_for_generation(\n         use_cache=True,\n         **kwargs,\n     ):\n+        # Overwitten -- has a unique cache type, `HybridMambaAttentionDynamicCache`\n+\n         empty_past_kv = past_key_values is None\n \n         # Omit tokens covered by past_key_values"
        },
        {
            "sha": "02f4f1b6127ab5a4eba124525749b4410148ed89",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ac07853539216baef9abadfda5782e7b20dc87/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ac07853539216baef9abadfda5782e7b20dc87/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=37ac07853539216baef9abadfda5782e7b20dc87",
            "patch": "@@ -3841,6 +3841,38 @@ def test_prepare_inputs_for_generation_decoder_llm(self):\n         self.assertTrue(model_inputs[\"input_ids\"] is not None)\n         self.assertTrue(model_inputs[\"inputs_embeds\"] is None)\n \n+    def test_prepare_inputs_for_generation_encoder_decoder_llm(self):\n+        \"\"\"\n+        Same as `test_prepare_inputs_for_generation_decoder_llm` but for encoder-decoder models. Main difference: we\n+        should look for `decoder_input_ids`, instead of `input_ids`.\n+        \"\"\"\n+        model = AutoModelForSeq2SeqLM.from_pretrained(\"hf-internal-testing/tiny-random-t5\")\n+        model = model.to(torch_device)\n+\n+        # 1. Sanity check: the model's `prepare_inputs_for_generation` comes from `GenerationMixin`\n+        self.assertTrue(\"GenerationMixin\" in str(model.prepare_inputs_for_generation))\n+\n+        # 2. If we pass input ids by themselves, we should get back the same input ids -- with the encoder-decoder key\n+        decoder_input_ids = torch.tensor([[1, 2, 3], [4, 5, 6]]).to(torch_device)\n+        model_inputs = model.prepare_inputs_for_generation(decoder_input_ids)\n+        self.assertTrue(torch.all(model_inputs[\"decoder_input_ids\"] == decoder_input_ids))\n+\n+        # 3. If we pass the attention mask too, we will get back the attention mask. Encoder-decoder models usually\n+        # don't use `position_ids`\n+        decoder_attention_mask = torch.tensor([[1, 1, 1], [1, 1, 1]]).to(torch_device)\n+        model_inputs = model.prepare_inputs_for_generation(\n+            decoder_input_ids, decoder_attention_mask=decoder_attention_mask\n+        )\n+        self.assertTrue(torch.all(model_inputs[\"decoder_attention_mask\"] == decoder_attention_mask))\n+        self.assertTrue(\"position_ids\" not in model_inputs)\n+\n+        # 4. `use_cache` (and other kwargs, like the encoder outputs) are forwarded\n+        self.assertFalse(\"use_cache\" in model_inputs)  # From the previous input, there is no `use_cache`\n+        model_inputs = model.prepare_inputs_for_generation(decoder_input_ids, use_cache=True, encoder_outputs=\"foo\")\n+        self.assertTrue(model_inputs[\"use_cache\"] is True)\n+        self.assertTrue(model_inputs[\"encoder_outputs\"] == \"foo\")\n+        # See the decoder-only test for more corner cases. The code is the same, so we don't repeat it here.\n+\n     def test_generate_compile_fullgraph_tiny(self):\n         \"\"\"\n         Tests that we can call end-to-end generation with a tiny model (i.e. doesn't crash)"
        }
    ],
    "stats": {
        "total": 774,
        "additions": 49,
        "deletions": 725
    }
}