{
    "author": "matthewdouglas",
    "message": "Fix bnb training test failure (#34414)\n\n* Fix bnb training test: compatibility with OPTSdpaAttention",
    "sha": "e447185b1f19df3032b11b586506225bfdf6d111",
    "files": [
        {
            "sha": "3eae429abb206a22ae094411655e3c6fa67a7276",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e447185b1f19df3032b11b586506225bfdf6d111/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e447185b1f19df3032b11b586506225bfdf6d111/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=e447185b1f19df3032b11b586506225bfdf6d111",
            "patch": "@@ -29,6 +29,7 @@\n     BitsAndBytesConfig,\n     pipeline,\n )\n+from transformers.models.opt.modeling_opt import OPTAttention\n from transformers.testing_utils import (\n     apply_skip_if_not_implemented,\n     is_bitsandbytes_available,\n@@ -565,7 +566,7 @@ def test_training(self):\n \n         # Step 2: add adapters\n         for _, module in model.named_modules():\n-            if \"OPTAttention\" in repr(type(module)):\n+            if isinstance(module, OPTAttention):\n                 module.q_proj = LoRALayer(module.q_proj, rank=16)\n                 module.k_proj = LoRALayer(module.k_proj, rank=16)\n                 module.v_proj = LoRALayer(module.v_proj, rank=16)"
        },
        {
            "sha": "567aa956271b70e534f8c12e2c93484042743670",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e447185b1f19df3032b11b586506225bfdf6d111/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e447185b1f19df3032b11b586506225bfdf6d111/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=e447185b1f19df3032b11b586506225bfdf6d111",
            "patch": "@@ -29,6 +29,7 @@\n     BitsAndBytesConfig,\n     pipeline,\n )\n+from transformers.models.opt.modeling_opt import OPTAttention\n from transformers.testing_utils import (\n     apply_skip_if_not_implemented,\n     is_accelerate_available,\n@@ -868,7 +869,7 @@ def test_training(self):\n \n         # Step 2: add adapters\n         for _, module in model.named_modules():\n-            if \"OPTAttention\" in repr(type(module)):\n+            if isinstance(module, OPTAttention):\n                 module.q_proj = LoRALayer(module.q_proj, rank=16)\n                 module.k_proj = LoRALayer(module.k_proj, rank=16)\n                 module.v_proj = LoRALayer(module.v_proj, rank=16)"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 4,
        "deletions": 2
    }
}