{
    "author": "guangy10",
    "message": "MobileBERT is ExecuTorch compatible (#34473)\n\nCo-authored-by: Guang Yang <guangyang@fb.com>",
    "sha": "34620e8f0a974761debf52093968107c14f41315",
    "files": [
        {
            "sha": "d2bc11d09f17974f8bc56b5437b83b8795a3f9dd",
            "filename": "tests/models/mobilebert/test_modeling_mobilebert.py",
            "status": "modified",
            "additions": 42,
            "deletions": 1,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/34620e8f0a974761debf52093968107c14f41315/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34620e8f0a974761debf52093968107c14f41315/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py?ref=34620e8f0a974761debf52093968107c14f41315",
            "patch": "@@ -16,7 +16,9 @@\n \n import unittest\n \n-from transformers import MobileBertConfig, is_torch_available\n+from packaging import version\n+\n+from transformers import AutoTokenizer, MobileBertConfig, MobileBertForMaskedLM, is_torch_available\n from transformers.models.auto import get_values\n from transformers.testing_utils import require_sentencepiece, require_tokenizers, require_torch, slow, torch_device\n \n@@ -384,3 +386,42 @@ def test_inference_no_head(self):\n         upper_bound = torch.all((expected_slice / output[..., :3, :3]) <= 1 + TOLERANCE)\n \n         self.assertTrue(lower_bound and upper_bound)\n+\n+    @slow\n+    def test_export(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+\n+        mobilebert_model = \"google/mobilebert-uncased\"\n+        device = \"cpu\"\n+        attn_implementation = \"eager\"\n+        max_length = 512\n+\n+        tokenizer = AutoTokenizer.from_pretrained(mobilebert_model)\n+        inputs = tokenizer(\n+            f\"the man worked as a {tokenizer.mask_token}.\",\n+            return_tensors=\"pt\",\n+            padding=\"max_length\",\n+            max_length=max_length,\n+        )\n+\n+        model = MobileBertForMaskedLM.from_pretrained(\n+            mobilebert_model,\n+            device_map=device,\n+            attn_implementation=attn_implementation,\n+        )\n+\n+        logits = model(**inputs).logits\n+        eg_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n+        self.assertEqual(eg_predicted_mask.split(), [\"carpenter\", \"waiter\", \"mechanic\", \"teacher\", \"clerk\"])\n+\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n+            strict=True,\n+        )\n+\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n+        ep_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n+        self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
        }
    ],
    "stats": {
        "total": 43,
        "additions": 42,
        "deletions": 1
    }
}