{
    "author": "albertvillanova",
    "message": "Fix some models cache initialization (#42586)\n\n* Create cache when training in case generate needs being called\n\n* Align modular\n\n* fixes\n\n* cohere\n\n* fix modular\n\n* fix\n\n* review\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "a48d68c6b06172938948eef457ebb7bcd5dcb77c",
    "files": [
        {
            "sha": "74907217c85401f84dc6bb44e5acd61fdff702ea",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=a48d68c6b06172938948eef457ebb7bcd5dcb77c",
            "patch": "@@ -29,7 +29,6 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n@@ -233,7 +232,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n@@ -304,7 +303,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -398,7 +397,7 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        if use_cache and past_key_values is None and not self.training:\n+        if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:"
        },
        {
            "sha": "070040a2e42615391eaae0383320f866dd0468fd",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=a48d68c6b06172938948eef457ebb7bcd5dcb77c",
            "patch": "@@ -22,7 +22,6 @@\n from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import (\n     RopeParameters,\n@@ -271,7 +270,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n@@ -322,7 +321,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n@@ -367,7 +366,7 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        if use_cache and past_key_values is None and not self.training:\n+        if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:"
        },
        {
            "sha": "f1b79acff8c3e2e8f917d4651a4304679acf01dc",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 10,
            "deletions": 80,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=a48d68c6b06172938948eef457ebb7bcd5dcb77c",
            "patch": "@@ -33,7 +33,6 @@\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_masks_for_generate, create_sliding_window_causal_mask\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n@@ -347,7 +346,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n@@ -409,23 +408,19 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -438,12 +433,7 @@ def forward(\n         hidden_states = self.post_feedforward_layernorm(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -527,30 +517,16 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPast:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        if use_cache and past_key_values is None and not self.training:\n+        if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n@@ -591,41 +567,22 @@ def forward(\n         for layer_type in self.config.layer_types:\n             position_embeddings[layer_type] = self.rotary_emb(hidden_states, position_ids, layer_type)\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            layer_outputs = decoder_layer(\n+            hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_embeddings=position_embeddings[decoder_layer.attention_type],\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n         hidden_states = self.norm(hidden_states)\n \n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n         )\n \n \n@@ -918,10 +875,7 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        **lm_kwargs,\n+        **lm_kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Gemma3ModelOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -953,12 +907,6 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         # Replace image id with PAD if the image token if OOV, to avoid index-errors\n         if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n             special_image_mask = input_ids == self.config.image_token_id\n@@ -1005,16 +953,14 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n             **lm_kwargs,\n         )\n \n         return Gemma3ModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n-            past_key_values=outputs.past_key_values if use_cache else None,\n+            past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n             image_hidden_states=image_features if pixel_values is not None else None,\n@@ -1053,6 +999,7 @@ def set_input_embeddings(self, value):\n     def get_image_features(self, pixel_values):\n         return self.model.get_image_features(pixel_values)\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1066,11 +1013,8 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **lm_kwargs,\n+        **lm_kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Gemma3CausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1116,13 +1060,6 @@ def forward(\n         \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhere is the cat standing?\\nmodel\\nBased on the image, the cat is standing in a snowy area, likely outdoors. It appears to\"\n         ```\n         \"\"\"\n-\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,\n@@ -1133,9 +1070,6 @@ def forward(\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             labels=labels,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **lm_kwargs,\n         )\n@@ -1167,10 +1101,6 @@ def forward(\n             flat_labels = shift_labels.view(-1).to(shift_logits.device)\n             loss = loss_fct(flat_logits, flat_labels)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return Gemma3CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "f17945d909319956c156cf674f69970961066ff2",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 10,
            "deletions": 80,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=a48d68c6b06172938948eef457ebb7bcd5dcb77c",
            "patch": "@@ -23,7 +23,6 @@\n from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...masking_utils import create_causal_mask, create_masks_for_generate, create_sliding_window_causal_mask\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_rope_utils import (\n@@ -465,7 +464,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n@@ -527,23 +526,19 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -556,12 +551,7 @@ def forward(\n         hidden_states = self.post_feedforward_layernorm(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n GEMMA3_START_DOCSTRING = None\n@@ -620,30 +610,16 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPast:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        if use_cache and past_key_values is None and not self.training:\n+        if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n@@ -684,41 +660,22 @@ def forward(\n         for layer_type in self.config.layer_types:\n             position_embeddings[layer_type] = self.rotary_emb(hidden_states, position_ids, layer_type)\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            layer_outputs = decoder_layer(\n+            hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_embeddings=position_embeddings[decoder_layer.attention_type],\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n         hidden_states = self.norm(hidden_states)\n \n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n         )\n \n \n@@ -853,20 +810,11 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        **lm_kwargs,\n+        **lm_kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Gemma3ModelOutputWithPast]:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         # Replace image id with PAD if the image token if OOV, to avoid index-errors\n         if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n             special_image_mask = input_ids == self.config.image_token_id\n@@ -913,16 +861,14 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n             **lm_kwargs,\n         )\n \n         return Gemma3ModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n-            past_key_values=outputs.past_key_values if use_cache else None,\n+            past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n             image_hidden_states=image_features if pixel_values is not None else None,\n@@ -934,6 +880,7 @@ class Gemma3ForConditionalGeneration(PaliGemmaForConditionalGeneration):\n     # Fix: https://github.com/huggingface/transformers/issues/40564\n     accepts_loss_kwargs = False\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -947,11 +894,8 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **lm_kwargs,\n+        **lm_kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Gemma3CausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -997,13 +941,6 @@ def forward(\n         \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhere is the cat standing?\\nmodel\\nBased on the image, the cat is standing in a snowy area, likely outdoors. It appears to\"\n         ```\n         \"\"\"\n-\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,\n@@ -1014,9 +951,6 @@ def forward(\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             labels=labels,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **lm_kwargs,\n         )\n@@ -1048,10 +982,6 @@ def forward(\n             flat_labels = shift_labels.view(-1).to(shift_logits.device)\n             loss = loss_fct(flat_logits, flat_labels)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return Gemma3CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "d105f114de092bbcec2390f919dea264b6d50acd",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 14,
            "deletions": 206,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=a48d68c6b06172938948eef457ebb7bcd5dcb77c",
            "patch": "@@ -33,20 +33,17 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import check_model_inputs\n from ..auto import AutoModel\n from .configuration_gemma3n import Gemma3nAudioConfig, Gemma3nConfig, Gemma3nTextConfig, Gemma3nVisionConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n @dataclass\n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -1283,7 +1280,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.config.head_dim)\n@@ -1379,25 +1376,21 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         predictions = self.altup.predict(hidden_states)\n         active_prediction = predictions[self.config.altup_active_idx]\n \n         active_prediction_normed = self.input_layernorm(active_prediction)\n         laurel_output = self.laurel(active_prediction_normed)\n \n-        attn, self_attn_weights = self.self_attn(\n+        attn, _ = self.self_attn(\n             hidden_states=active_prediction_normed,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             position_embeddings=position_embeddings,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -1426,154 +1419,7 @@ def forward(\n         first_prediction = self.post_per_layer_input_norm(first_prediction)\n         corrected_predictions[1:] += first_prediction\n \n-        outputs = (corrected_predictions,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        return outputs\n-\n-\n-class Gemma3nMLP(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.config = config\n-        self.hidden_size = config.hidden_size\n-        self.intermediate_size = config.intermediate_size\n-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n-        self.act_fn = ACT2FN[config.hidden_activation]\n-\n-    def forward(self, x):\n-        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n-        return down_proj\n-\n-\n-class Gemma3nAttention(nn.Module):\n-    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n-\n-    def __init__(self, config: Gemma3nConfig, layer_idx: int):\n-        super().__init__()\n-        self.layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n-        self.config = config\n-        self.layer_idx = layer_idx\n-        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n-        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n-        self.scaling = config.query_pre_attn_scalar**-0.5\n-        self.attention_dropout = self.config.attention_dropout\n-        self.is_causal = not getattr(config, \"use_bidirectional_attention\", False)\n-\n-        self.q_proj = nn.Linear(\n-            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n-        )\n-        self.k_proj = nn.Linear(\n-            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n-        )\n-        self.v_proj = nn.Linear(\n-            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n-        )\n-        self.o_proj = nn.Linear(\n-            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n-        )\n-        self.rotary_fn = apply_rotary_pos_emb\n-        self.attn_logit_softcapping = self.config.attn_logit_softcapping\n-        self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        input_shape = hidden_states.shape[:-1]\n-        hidden_shape = (*input_shape, -1, self.head_dim)\n-\n-        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n-        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n-        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_values is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        attention_interface: Callable = eager_attention_forward\n-        if self.config._attn_implementation != \"eager\":\n-            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n-\n-        attn_output, attn_weights = attention_interface(\n-            self,\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask,\n-            dropout=self.attention_dropout if self.training else 0.0,\n-            scaling=self.scaling,\n-            sliding_window=self.sliding_window,\n-            softcap=self.attn_logit_softcapping,\n-            **kwargs,\n-        )\n-\n-        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n-        attn_output = self.o_proj(attn_output)\n-        return attn_output, attn_weights\n-\n-\n-class Gemma3nDecoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: Gemma3nConfig, layer_idx: int):\n-        super().__init__()\n-        self.hidden_size = config.hidden_size\n-        self.config = config\n-        self.attention_type = config.layer_types[layer_idx]\n-        self.self_attn = Gemma3nAttention(config=config, layer_idx=layer_idx)\n-        self.mlp = Gemma3nMLP(config)\n-        self.input_layernorm = Gemma3nRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.post_attention_layernorm = Gemma3nRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n-        self.pre_feedforward_layernorm = Gemma3nRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.post_feedforward_layernorm = Gemma3nRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        residual = hidden_states\n-\n-        hidden_states = self.input_layernorm(hidden_states)\n-\n-        # Self Attention\n-        hidden_states, _ = self.self_attn(\n-            hidden_states=hidden_states,\n-            position_embeddings=position_embeddings,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            cache_position=cache_position,\n-            **kwargs,\n-        )\n-        hidden_states = self.post_attention_layernorm(hidden_states)\n-        hidden_states = residual + hidden_states\n-\n-        residual = hidden_states\n-        hidden_states = self.pre_feedforward_layernorm(hidden_states)\n-        hidden_states = self.mlp(hidden_states)\n-        hidden_states = self.post_feedforward_layernorm(hidden_states)\n-        hidden_states = residual + hidden_states\n-\n-        return hidden_states\n+        return corrected_predictions\n \n \n @auto_docstring\n@@ -1590,8 +1436,8 @@ class Gemma3nPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n-        \"hidden_states\": Gemma3nDecoderLayer,\n-        \"attentions\": Gemma3nAttention,\n+        \"hidden_states\": Gemma3nTextDecoderLayer,\n+        \"attentions\": Gemma3nTextAttention,\n     }\n     input_modalities = (\"image\", \"text\", \"audio\")\n \n@@ -1741,7 +1587,7 @@ def __init__(self, config: Gemma3nTextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n@@ -1752,46 +1598,28 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPast:\n         r\"\"\"\n         per_layer_inputs (torch.Tensor, *optional*, defaults to None):\n             Pre-computed per-layer embeddings. If None, they are derived from input_ids if provided.\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n-\n         if input_ids is not None:\n             inputs_embeds = self.embed_tokens(input_ids)\n             per_layer_inputs = self.get_per_layer_inputs(input_ids)\n \n         per_layer_inputs = self.project_per_layer_inputs(inputs_embeds, per_layer_inputs)\n \n-        if use_cache and past_key_values is None and not self.training:\n+        if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position = torch.arange(\n-                past_seen_tokens,\n-                past_seen_tokens + inputs_embeds.shape[1],\n-                device=inputs_embeds.device,\n-            )\n+            cache_position = torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n \n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n@@ -1835,39 +1663,21 @@ def forward(\n         for layer_type in self.config.layer_types:\n             position_embeddings[layer_type] = self.rotary_emb(hidden_states, position_ids, layer_type)\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n             causal_mask = causal_mask_mapping[decoder_layer.attention_type]\n             per_layer_input = per_layer_inputs[:, :, decoder_layer.layer_idx, :]\n \n-            layer_outputs = decoder_layer(\n+            hidden_states = decoder_layer(\n                 hidden_states,\n                 position_embeddings[decoder_layer.attention_type],\n                 per_layer_input,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n-        # add hidden states from the last decoder layer (but before reprojecting to stay consistent with layer output)\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n         # Per-layer inputs to single output\n         target_magnitude = torch.mean(hidden_states[0] ** 2, dim=-1, keepdim=True) ** 0.5\n         temp_hidden_states = [hidden_states[0]]\n@@ -1887,8 +1697,6 @@ def forward(\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n         )\n \n     def get_per_layer_inputs(self, input_ids: torch.LongTensor) -> torch.Tensor:\n@@ -2175,7 +1983,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        **lm_kwargs,\n+        **lm_kwargs: Unpack[TransformersKwargs],\n     ) -> Gemma3nCausalLMOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -2363,7 +2171,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **lm_kwargs,\n+        **lm_kwargs: Unpack[TransformersKwargs],\n     ) -> Gemma3nCausalLMOutputWithPast:\n         r\"\"\"\n         input_features_mask (torch.Tensor, *optional*, defaults to None):"
        },
        {
            "sha": "83c604680ca4df59d64e9f69abde57eeec0ed1af",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 16,
            "deletions": 58,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=a48d68c6b06172938948eef457ebb7bcd5dcb77c",
            "patch": "@@ -26,12 +26,12 @@\n from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import RopeParameters\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.generic import check_model_inputs\n from ..auto import AutoModel\n from ..gemma2.configuration_gemma2 import Gemma2Config\n from ..gemma2.modeling_gemma2 import (\n@@ -1742,7 +1742,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.config.head_dim)\n@@ -1830,25 +1830,21 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         predictions = self.altup.predict(hidden_states)\n         active_prediction = predictions[self.config.altup_active_idx]\n \n         active_prediction_normed = self.input_layernorm(active_prediction)\n         laurel_output = self.laurel(active_prediction_normed)\n \n-        attn, self_attn_weights = self.self_attn(\n+        attn, _ = self.self_attn(\n             hidden_states=active_prediction_normed,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             position_embeddings=position_embeddings,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -1877,18 +1873,17 @@ def forward(\n         first_prediction = self.post_per_layer_input_norm(first_prediction)\n         corrected_predictions[1:] += first_prediction\n \n-        outputs = (corrected_predictions,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        return outputs\n+        return corrected_predictions\n \n \n class Gemma3nPreTrainedModel(Gemma2PreTrainedModel):\n     config: Gemma3nConfig\n     input_modalities = (\"image\", \"text\", \"audio\")\n     _no_split_modules = [\"Gemma3nTextDecoderLayer\"]\n+    _can_record_outputs = {\n+        \"hidden_states\": Gemma3nTextDecoderLayer,\n+        \"attentions\": Gemma3nTextAttention,\n+    }\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n@@ -1976,7 +1971,8 @@ def project_per_layer_inputs(\n             dtype=inputs_embeds.dtype, device=per_layer_projection.device\n         )\n \n-    @can_return_tuple\n+    # Last hidden states should be before reprojecting, to stay consistent with the other layer outputs\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n@@ -1987,46 +1983,28 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPast:\n         r\"\"\"\n         per_layer_inputs (torch.Tensor, *optional*, defaults to None):\n             Pre-computed per-layer embeddings. If None, they are derived from input_ids if provided.\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n-\n         if input_ids is not None:\n             inputs_embeds = self.embed_tokens(input_ids)\n             per_layer_inputs = self.get_per_layer_inputs(input_ids)\n \n         per_layer_inputs = self.project_per_layer_inputs(inputs_embeds, per_layer_inputs)\n \n-        if use_cache and past_key_values is None and not self.training:\n+        if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position = torch.arange(\n-                past_seen_tokens,\n-                past_seen_tokens + inputs_embeds.shape[1],\n-                device=inputs_embeds.device,\n-            )\n+            cache_position = torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n \n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n@@ -2070,39 +2048,21 @@ def forward(\n         for layer_type in self.config.layer_types:\n             position_embeddings[layer_type] = self.rotary_emb(hidden_states, position_ids, layer_type)\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n             causal_mask = causal_mask_mapping[decoder_layer.attention_type]\n             per_layer_input = per_layer_inputs[:, :, decoder_layer.layer_idx, :]\n \n-            layer_outputs = decoder_layer(\n+            hidden_states = decoder_layer(\n                 hidden_states,\n                 position_embeddings[decoder_layer.attention_type],\n                 per_layer_input,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n-        # add hidden states from the last decoder layer (but before reprojecting to stay consistent with layer output)\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n         # Per-layer inputs to single output\n         target_magnitude = torch.mean(hidden_states[0] ** 2, dim=-1, keepdim=True) ** 0.5\n         temp_hidden_states = [hidden_states[0]]\n@@ -2122,8 +2082,6 @@ def forward(\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n         )\n \n \n@@ -2284,7 +2242,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        **lm_kwargs,\n+        **lm_kwargs: Unpack[TransformersKwargs],\n     ) -> Gemma3nCausalLMOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -2456,7 +2414,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **lm_kwargs,\n+        **lm_kwargs: Unpack[TransformersKwargs],\n     ) -> Gemma3nCausalLMOutputWithPast:\n         r\"\"\"\n         input_features_mask (torch.Tensor, *optional*, defaults to None):"
        },
        {
            "sha": "ec9cc6fdd58abf6e301b3fb74873e7e0c61d12e6",
            "filename": "src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py?ref=a48d68c6b06172938948eef457ebb7bcd5dcb77c",
            "patch": "@@ -342,7 +342,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n         hidden_states = self.attn_norm(hidden_states)\n@@ -477,7 +477,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor, ...], BaseModelOutputWithPast]:\n         if (input_ids is None) == (inputs_embeds is None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -489,7 +489,7 @@ def forward(\n             batch_size, seq_length = inputs_embeds.shape[:2]\n \n         # Handle past_key_values and cache setup\n-        if use_cache and past_key_values is None and not self.training:\n+        if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n@@ -527,13 +527,12 @@ def forward(\n         for layer_type in self.config.layer_types:\n             position_embeddings[layer_type] = self.rotary_emb(hidden_states, position_ids, layer_type)\n \n-        for idx, decoder_layer in enumerate(self.layers):\n+        for decoder_layer in self.layers:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_embeddings=position_embeddings[decoder_layer.attention_type],\n                 past_key_values=past_key_values,\n-                use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_ids=position_ids,\n                 **kwargs,\n@@ -583,7 +582,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -686,7 +685,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "7f932c668f11571745f8f7612b84746758ece3a5",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=a48d68c6b06172938948eef457ebb7bcd5dcb77c",
            "patch": "@@ -394,7 +394,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n         hidden_states = self.attn_norm(hidden_states)\n@@ -525,7 +525,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor, ...], BaseModelOutputWithPast]:\n         if (input_ids is None) == (inputs_embeds is None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -537,7 +537,7 @@ def forward(\n             batch_size, seq_length = inputs_embeds.shape[:2]\n \n         # Handle past_key_values and cache setup\n-        if use_cache and past_key_values is None and not self.training:\n+        if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n@@ -575,13 +575,12 @@ def forward(\n         for layer_type in self.config.layer_types:\n             position_embeddings[layer_type] = self.rotary_emb(hidden_states, position_ids, layer_type)\n \n-        for idx, decoder_layer in enumerate(self.layers):\n+        for decoder_layer in self.layers:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_embeddings=position_embeddings[decoder_layer.attention_type],\n                 past_key_values=past_key_values,\n-                use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_ids=position_ids,\n                 **kwargs,\n@@ -631,7 +630,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -734,7 +733,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "5fab373c81d43b4db4c243d5f379882924a2de6d",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=a48d68c6b06172938948eef457ebb7bcd5dcb77c",
            "patch": "@@ -957,7 +957,7 @@ def forward(\n             )\n             use_cache = False\n \n-        if use_cache and past_key_values is None and not self.training:\n+        if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n \n         past_seen_tokens = 0 if past_key_values is None else past_key_values.get_seq_length()"
        },
        {
            "sha": "e7d1ff811fdbd20be7aa1f14e2aaf77f8998170d",
            "filename": "src/transformers/models/t5gemma2/modeling_t5gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a48d68c6b06172938948eef457ebb7bcd5dcb77c/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py?ref=a48d68c6b06172938948eef457ebb7bcd5dcb77c",
            "patch": "@@ -294,7 +294,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)"
        }
    ],
    "stats": {
        "total": 518,
        "additions": 70,
        "deletions": 448
    }
}