{
    "author": "SunMarc",
    "message": "remove to restriction for 4-bit model (#33122)\n\n* remove to restiction for 4-bit model\r\n\r\n* Update src/transformers/modeling_utils.py\r\n\r\nCo-authored-by: Matthew Douglas <38992547+matthewdouglas@users.noreply.github.com>\r\n\r\n* bitsandbytes: prevent dtype casting while allowing device movement with .to or .cuda\r\n\r\n* quality fix\r\n\r\n* Improve warning message for .to() and .cuda() on bnb quantized models\r\n\r\n---------\r\n\r\nCo-authored-by: Matthew Douglas <38992547+matthewdouglas@users.noreply.github.com>",
    "sha": "9ea1eacd11b10acedd489c9a44c4ae45a358508d",
    "files": [
        {
            "sha": "f931a6af3eb21dd3ca63556bed6d58f6c670094d",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 38,
            "deletions": 22,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ea1eacd11b10acedd489c9a44c4ae45a358508d/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ea1eacd11b10acedd489c9a44c4ae45a358508d/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=9ea1eacd11b10acedd489c9a44c4ae45a358508d",
            "patch": "@@ -2861,38 +2861,54 @@ def get_memory_footprint(self, return_buffers=True):\n     def cuda(self, *args, **kwargs):\n         if getattr(self, \"quantization_method\", None) == QuantizationMethod.HQQ:\n             raise ValueError(\"`.cuda` is not supported for HQQ-quantized models.\")\n-        # Checks if the model has been loaded in 8-bit\n+        # Checks if the model has been loaded in 4-bit or 8-bit with BNB\n         if getattr(self, \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES:\n-            raise ValueError(\n-                \"Calling `cuda()` is not supported for `4-bit` or `8-bit` quantized models. Please use the model as it is, since the\"\n-                \" model has already been set to the correct devices and casted to the correct `dtype`.\"\n-            )\n+            if getattr(self, \"is_loaded_in_8bit\", False):\n+                raise ValueError(\n+                    \"Calling `cuda()` is not supported for `8-bit` quantized models. \"\n+                    \" Please use the model as it is, since the model has already been set to the correct devices.\"\n+                )\n+            elif version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.43.2\"):\n+                raise ValueError(\n+                    \"Calling `cuda()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \"\n+                    f\"The current device is `{self.device}`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\"\n+                )\n         else:\n             return super().cuda(*args, **kwargs)\n \n     @wraps(torch.nn.Module.to)\n     def to(self, *args, **kwargs):\n+        # For BNB/GPTQ models, we prevent users from casting the model to another dytpe to restrict unwanted behaviours.\n+        # the correct API should be to load the model with the desired dtype directly through `from_pretrained`.\n+        dtype_present_in_args = \"dtype\" in kwargs\n+\n+        if not dtype_present_in_args:\n+            for arg in args:\n+                if isinstance(arg, torch.dtype):\n+                    dtype_present_in_args = True\n+                    break\n+\n         if getattr(self, \"quantization_method\", None) == QuantizationMethod.HQQ:\n             raise ValueError(\"`.to` is not supported for HQQ-quantized models.\")\n-        # Checks if the model has been loaded in 8-bit\n+        # Checks if the model has been loaded in 4-bit or 8-bit with BNB\n         if getattr(self, \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES:\n-            raise ValueError(\n-                \"`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\"\n-                \" model has already been set to the correct devices and casted to the correct `dtype`.\"\n-            )\n-        elif getattr(self, \"quantization_method\", None) == QuantizationMethod.GPTQ:\n-            # For GPTQ models, we prevent users from casting the model to another dytpe to restrict unwanted behaviours.\n-            # the correct API should be to load the model with the desired dtype directly through `from_pretrained`.\n-            dtype_present_in_args = False\n-\n-            if \"dtype\" not in kwargs:\n-                for arg in args:\n-                    if isinstance(arg, torch.dtype):\n-                        dtype_present_in_args = True\n-                        break\n-            else:\n-                dtype_present_in_args = True\n+            if dtype_present_in_args:\n+                raise ValueError(\n+                    \"You cannot cast a bitsandbytes model in a new `dtype`. Make sure to load the model using `from_pretrained` using the\"\n+                    \" desired `dtype` by passing the correct `torch_dtype` argument.\"\n+                )\n \n+            if getattr(self, \"is_loaded_in_8bit\", False):\n+                raise ValueError(\n+                    \"`.to` is not supported for `8-bit` bitsandbytes models. Please use the model as it is, since the\"\n+                    \" model has already been set to the correct devices and casted to the correct `dtype`.\"\n+                )\n+            elif version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.43.2\"):\n+                raise ValueError(\n+                    \"Calling `to()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \"\n+                    f\"The current device is `{self.device}`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\"\n+                )\n+        elif getattr(self, \"quantization_method\", None) == QuantizationMethod.GPTQ:\n             if dtype_present_in_args:\n                 raise ValueError(\n                     \"You cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\""
        },
        {
            "sha": "785402b3f798eedeff9e8f2d138efbc849e06ff2",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 39,
            "deletions": 9,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ea1eacd11b10acedd489c9a44c4ae45a358508d/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ea1eacd11b10acedd489c9a44c4ae45a358508d/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=9ea1eacd11b10acedd489c9a44c4ae45a358508d",
            "patch": "@@ -256,29 +256,56 @@ def test_generate_quality_dequantize(self):\n \n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n \n+    def test_device_assignment(self):\n+        if version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.43.2\"):\n+            self.skipTest(reason=\"This test requires bitsandbytes >= 0.43.2\")\n+\n+        mem_before = self.model_4bit.get_memory_footprint()\n+\n+        # Move to CPU\n+        self.model_4bit.to(\"cpu\")\n+        self.assertEqual(self.model_4bit.device.type, \"cpu\")\n+        self.assertAlmostEqual(self.model_4bit.get_memory_footprint(), mem_before)\n+\n+        # Move back to CUDA device\n+        self.model_4bit.to(0)\n+        self.assertEqual(self.model_4bit.device, torch.device(0))\n+        self.assertAlmostEqual(self.model_4bit.get_memory_footprint(), mem_before)\n+\n     def test_device_and_dtype_assignment(self):\n         r\"\"\"\n-        Test whether trying to cast (or assigning a device to) a model after converting it in 8-bit will throw an error.\n+        Test whether trying to cast (or assigning a device to) a model after converting it in 4-bit will throw an error.\n         Checks also if other models are casted correctly.\n         \"\"\"\n-        with self.assertRaises(ValueError):\n-            # Tries with `str`\n-            self.model_4bit.to(\"cpu\")\n+\n+        # Moving with `to` or `cuda` is not supported with versions < 0.43.2.\n+        if version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.43.2\"):\n+            with self.assertRaises(ValueError):\n+                # Tries with `str`\n+                self.model_4bit.to(\"cpu\")\n+\n+            with self.assertRaises(ValueError):\n+                # Tries with a `device`\n+                self.model_4bit.to(torch.device(\"cuda:0\"))\n+\n+            with self.assertRaises(ValueError):\n+                # Tries with `cuda`\n+                self.model_4bit.cuda()\n \n         with self.assertRaises(ValueError):\n-            # Tries with a `dtype``\n+            # Tries with a `dtype`\n             self.model_4bit.to(torch.float16)\n \n         with self.assertRaises(ValueError):\n-            # Tries with a `device`\n-            self.model_4bit.to(torch.device(\"cuda:0\"))\n+            # Tries with a `dtype` and `device`\n+            self.model_4bit.to(device=\"cuda:0\", dtype=torch.float16)\n \n         with self.assertRaises(ValueError):\n-            # Tries with a `device`\n+            # Tries with a cast\n             self.model_4bit.float()\n \n         with self.assertRaises(ValueError):\n-            # Tries with a `device`\n+            # Tries with a cast\n             self.model_4bit.half()\n \n         # Test if we did not break anything\n@@ -287,6 +314,9 @@ def test_device_and_dtype_assignment(self):\n         self.model_fp16 = self.model_fp16.to(torch.float32)\n         _ = self.model_fp16.generate(input_ids=encoded_input[\"input_ids\"].to(0), max_new_tokens=10)\n \n+        # Check that this does not throw an error\n+        _ = self.model_fp16.cuda()\n+\n         # Check this does not throw an error\n         _ = self.model_fp16.to(\"cpu\")\n "
        }
    ],
    "stats": {
        "total": 108,
        "additions": 77,
        "deletions": 31
    }
}