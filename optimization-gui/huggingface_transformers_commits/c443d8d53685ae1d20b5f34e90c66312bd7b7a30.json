{
    "author": "fpgaminer",
    "message": "Bug Fix for issue #34294 (#34295)\n\nUpdate SiglipVisionEmbeddings.forward to cast input to correct dtype before embedding it.",
    "sha": "c443d8d53685ae1d20b5f34e90c66312bd7b7a30",
    "files": [
        {
            "sha": "a42bcd0e17461edd06626f29d704592113318a6a",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c443d8d53685ae1d20b5f34e90c66312bd7b7a30/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c443d8d53685ae1d20b5f34e90c66312bd7b7a30/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=c443d8d53685ae1d20b5f34e90c66312bd7b7a30",
            "patch": "@@ -308,7 +308,8 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n \n     def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=False) -> torch.Tensor:\n         _, _, height, width = pixel_values.shape\n-        patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, width, grid, grid]\n+        target_dtype = self.patch_embedding.weight.dtype\n+        patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\n         embeddings = patch_embeds.flatten(2).transpose(1, 2)\n \n         if interpolate_pos_encoding:"
        }
    ],
    "stats": {
        "total": 3,
        "additions": 2,
        "deletions": 1
    }
}