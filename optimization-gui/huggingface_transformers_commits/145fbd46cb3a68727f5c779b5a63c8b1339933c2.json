{
    "author": "zucchini-nlp",
    "message": "LLaVA OV: fix unpadding precision (#34779)\n\n* fix\r\n\r\n* propagate\r\n\r\n* type check",
    "sha": "145fbd46cb3a68727f5c779b5a63c8b1339933c2",
    "files": [
        {
            "sha": "db9d1276832b397be68b19b6ab38e31894a54ef4",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/145fbd46cb3a68727f5c779b5a63c8b1339933c2/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/145fbd46cb3a68727f5c779b5a63c8b1339933c2/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=145fbd46cb3a68727f5c779b5a63c8b1339933c2",
            "patch": "@@ -163,7 +163,9 @@ def __call__(\n                 for sample in text:\n                     while self.image_token in sample:\n                         image_size = next(image_sizes)\n-                        orig_height, orig_width = image_size\n+                        if not isinstance(image_size, (list, tuple)):\n+                            # cast to list to avoid numerical precision errors when calculating unpadding\n+                            orig_height, orig_width = image_size.tolist()\n                         num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n                         if self.vision_feature_select_strategy == \"default\":\n                             num_image_tokens -= self.num_additional_image_tokens"
        },
        {
            "sha": "5805782f779e5f243b8ec5dae04805540892e005",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/145fbd46cb3a68727f5c779b5a63c8b1339933c2/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/145fbd46cb3a68727f5c779b5a63c8b1339933c2/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=145fbd46cb3a68727f5c779b5a63c8b1339933c2",
            "patch": "@@ -190,7 +190,9 @@ def __call__(\n                 for sample in text:\n                     while self.image_token in sample:\n                         image_size = next(image_sizes)\n-                        orig_height, orig_width = image_size\n+                        if not isinstance(image_size, (list, tuple)):\n+                            # cast to list to avoid numerical precision errors when calculating unpadding\n+                            orig_height, orig_width = image_size.tolist()\n                         num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n                         if self.vision_feature_select_strategy == \"default\":\n                             num_image_tokens -= self.num_additional_image_tokens"
        },
        {
            "sha": "4f67f9e4c030e86d21f9edbf440a7b8cdcd07400",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/145fbd46cb3a68727f5c779b5a63c8b1339933c2/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/145fbd46cb3a68727f5c779b5a63c8b1339933c2/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=145fbd46cb3a68727f5c779b5a63c8b1339933c2",
            "patch": "@@ -188,7 +188,10 @@ def _expand_image_tokens(\n         for sample in text:\n             while special_token in sample:\n                 image_size_list = next(image_sizes)\n-                orig_height, orig_width = image_size_list[0] if num_frames != 1 else image_size_list\n+                original_size = image_size_list[0] if num_frames != 1 else image_size_list\n+                if not isinstance(original_size, (list, tuple)):\n+                    # cast to list to avoid numerical precision errors when calculating unpadding\n+                    orig_height, orig_width = original_size.tolist()\n                 num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n                 if self.vision_feature_select_strategy == \"default\":\n                     num_image_tokens -= 1"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 10,
        "deletions": 3
    }
}