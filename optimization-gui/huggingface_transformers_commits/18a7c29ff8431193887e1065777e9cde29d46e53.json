{
    "author": "Cyrilvallez",
    "message": "More robust tied weight test (#39681)\n\n* Update test_modeling_common.py\n\n* remove old ones\n\n* Update test_modeling_common.py\n\n* Update test_modeling_common.py\n\n* add\n\n* Update test_modeling_musicgen_melody.py",
    "sha": "18a7c29ff8431193887e1065777e9cde29d46e53",
    "files": [
        {
            "sha": "7679ab55f6d64385de668ad01751c756c910fb28",
            "filename": "tests/models/csm/test_modeling_csm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 35,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py?ref=18a7c29ff8431193887e1065777e9cde29d46e53",
            "patch": "@@ -14,9 +14,7 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch ConversationalSpeechModel model.\"\"\"\n \n-import collections\n import copy\n-import re\n import unittest\n \n import pytest\n@@ -52,8 +50,6 @@\n if is_torch_available():\n     import torch\n \n-    from transformers.pytorch_utils import id_tensor_storage\n-\n \n class CsmModelTester:\n     def __init__(\n@@ -344,38 +340,9 @@ def test_generate_from_inputs_embeds_1_beam_search(self, _, num_beams):\n     def test_model_parallel_beam_search(self):\n         pass\n \n+    @unittest.skip(reason=\"CSM has special embeddings that can never be tied\")\n     def test_tied_weights_keys(self):\n-        \"\"\"\n-        Overrides [ModelTesterMixin.test_tied_weights_keys] to not test for text config (not applicable to CSM).\n-        \"\"\"\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        for model_class in self.all_model_classes:\n-            model_tied = model_class(config)\n-\n-            ptrs = collections.defaultdict(list)\n-            for name, tensor in model_tied.state_dict().items():\n-                ptrs[id_tensor_storage(tensor)].append(name)\n-\n-            # These are all the pointers of shared tensors.\n-            tied_params = [names for _, names in ptrs.items() if len(names) > 1]\n-\n-            tied_weight_keys = model_tied._tied_weights_keys if model_tied._tied_weights_keys is not None else []\n-            # Detect we get a hit for each key\n-            for key in tied_weight_keys:\n-                is_tied_key = any(re.search(key, p) for group in tied_params for p in group)\n-                self.assertTrue(is_tied_key, f\"{key} is not a tied weight key for {model_class}.\")\n-\n-            # Removed tied weights found from tied params -> there should only be one left after\n-            for key in tied_weight_keys:\n-                for i in range(len(tied_params)):\n-                    tied_params[i] = [p for p in tied_params[i] if re.search(key, p) is None]\n-\n-            tied_params = [group for group in tied_params if len(group) > 1]\n-            self.assertListEqual(\n-                tied_params,\n-                [],\n-                f\"Missing `_tied_weights_keys` for {model_class}: add all of {tied_params} except one.\",\n-            )\n+        pass\n \n     def _get_custom_4d_mask_test_data(self):\n         \"\"\""
        },
        {
            "sha": "b8b7360fa7442d96d51c528e9b835233ea3ee88d",
            "filename": "tests/models/dbrx/test_modeling_dbrx.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py?ref=18a7c29ff8431193887e1065777e9cde29d46e53",
            "patch": "@@ -108,10 +108,6 @@ def test_model_from_pretrained(self):\n         model = DbrxModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    @unittest.skip(reason=\"Dbrx models have weight tying disabled.\")\n-    def test_tied_weights_keys(self):\n-        pass\n-\n     # Offload does not work with Dbrx models because of the forward of DbrxExperts where we chunk the experts.\n     # The issue is that the offloaded weights of the mlp layer are still on meta device (w1_chunked, v1_chunked, w2_chunked)\n     @unittest.skip(reason=\"Dbrx models do not work with offload\")"
        },
        {
            "sha": "85047afb4ca96cbe2c7173d4acb1ed5926206e61",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=18a7c29ff8431193887e1065777e9cde29d46e53",
            "patch": "@@ -309,10 +309,6 @@ def test_initialization(self):\n                             msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                         )\n \n-    @unittest.skip(reason=\"Mamba 2 weights are not tied\")\n-    def test_tied_weights_keys(self):\n-        pass\n-\n     @unittest.skip(reason=\"A large mamba2 would be necessary (and costly) for that\")\n     def test_multi_gpu_data_parallel_forward(self):\n         pass"
        },
        {
            "sha": "8a41c47d6fad3f5573a21b000c4776777d2aa7c1",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=18a7c29ff8431193887e1065777e9cde29d46e53",
            "patch": "@@ -781,11 +781,7 @@ def test_gradient_checkpointing_backward_compatibility(self):\n     def test_tie_model_weights(self):\n         pass\n \n-    @unittest.skip(reason=\"MusicGen has multiple inputs embeds and lm heads that should not be tied.\")\n-    def test_tied_model_weights_key_ignore(self):\n-        pass\n-\n-    @unittest.skip(reason=\"MusicGen has multiple inputs embeds and lm heads that should not be tied.\")\n+    @unittest.skip(reason=\"MusicGen has multiple inputs embeds and lm heads that should not be tied\")\n     def test_tied_weights_keys(self):\n         pass\n "
        },
        {
            "sha": "72b20f345be9e0ac3620393b4e038a8c7c93b794",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=18a7c29ff8431193887e1065777e9cde29d46e53",
            "patch": "@@ -782,11 +782,7 @@ def test_gradient_checkpointing_backward_compatibility(self):\n     def test_tie_model_weights(self):\n         pass\n \n-    @unittest.skip(reason=\"MusicGen has multiple inputs embeds and lm heads that should not be tied.\")\n-    def test_tied_model_weights_key_ignore(self):\n-        pass\n-\n-    @unittest.skip(reason=\"MusicGen has multiple inputs embeds and lm heads that should not be tied.\")\n+    @unittest.skip(reason=\"MusicGen has multiple inputs embeds and lm heads that should not be tied\")\n     def test_tied_weights_keys(self):\n         pass\n "
        },
        {
            "sha": "2b67ec239737d28b53437d6ff27af8960e100448",
            "filename": "tests/models/pix2struct/test_modeling_pix2struct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py?ref=18a7c29ff8431193887e1065777e9cde29d46e53",
            "patch": "@@ -656,10 +656,6 @@ def test_resize_embeddings_untied(self):\n             # Check that the model can still do a forward pass successfully (every parameter should be resized)\n             model(**self._prepare_for_class(inputs_dict, model_class))\n \n-    @unittest.skip(reason=\"Pix2Struct doesn't use tied weights\")\n-    def test_tied_model_weights_key_ignore(self):\n-        pass\n-\n     def _create_and_check_torchscript(self, config, inputs_dict):\n         if not self.test_torchscript:\n             self.skipTest(reason=\"test_torchscript is set to False\")"
        },
        {
            "sha": "0bf79a6131698f02174e77812d7ee00f2ca0e82b",
            "filename": "tests/models/timm_backbone/test_modeling_timm_backbone.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py?ref=18a7c29ff8431193887e1065777e9cde29d46e53",
            "patch": "@@ -176,10 +176,6 @@ def test_can_load_with_meta_device_context_manager(self):\n     def test_tie_model_weights(self):\n         pass\n \n-    @unittest.skip(reason=\"model weights aren't tied in TimmBackbone.\")\n-    def test_tied_model_weights_key_ignore(self):\n-        pass\n-\n     @unittest.skip(reason=\"Only checkpoints on timm can be loaded into TimmBackbone\")\n     def test_load_save_without_tied_weights(self):\n         pass"
        },
        {
            "sha": "0e10d0999d986868eee659240f3c539849ed9506",
            "filename": "tests/models/xlstm/test_modeling_xlstm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py?ref=18a7c29ff8431193887e1065777e9cde29d46e53",
            "patch": "@@ -184,10 +184,6 @@ def test_initialization(self):\n                         # check if it's a ones like\n                         self.assertTrue(torch.allclose(param.data, torch.ones_like(param.data), atol=1e-5, rtol=1e-5))\n \n-    @unittest.skip(reason=\"xLSTM has no tied weights\")\n-    def test_tied_weights_keys(self):\n-        pass\n-\n     @unittest.skip(reason=\"xLSTM cache slicing test case is an edge case\")\n     def test_generate_without_input_ids(self):\n         pass"
        },
        {
            "sha": "38c581992b31d26111da83142873fbc3373137a3",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18a7c29ff8431193887e1065777e9cde29d46e53/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=18a7c29ff8431193887e1065777e9cde29d46e53",
            "patch": "@@ -2465,9 +2465,7 @@ def test_correct_missing_keys(self):\n                         extra_params.pop(key, None)\n \n                 if not extra_params:\n-                    # In that case, we *are* on a head model, but every\n-                    # single key is not actual parameters and this is\n-                    # tested in `test_tied_model_weights_key_ignore` test.\n+                    # In that case, we *are* on a head model, but every single key is not actual parameters\n                     continue\n \n                 with tempfile.TemporaryDirectory() as temp_dir_name:\n@@ -2564,9 +2562,17 @@ def test_load_save_without_tied_weights(self):\n                 self.assertEqual(infos[\"missing_keys\"], [])\n \n     def test_tied_weights_keys(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        original_config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n-            model_tied = model_class(copy.deepcopy(config))\n+            copied_config = copy.deepcopy(original_config)\n+            copied_config.get_text_config().tie_word_embeddings = True\n+            model_tied = model_class(copied_config)\n+\n+            tied_weight_keys = model_tied._tied_weights_keys if model_tied._tied_weights_keys is not None else []\n+            # If we don't find any tied weights keys, and by default we don't tie the embeddings, it's because the model\n+            # does not tie them\n+            if len(tied_weight_keys) == 0 and not original_config.tie_word_embeddings:\n+                continue\n \n             ptrs = collections.defaultdict(list)\n             for name, tensor in model_tied.state_dict().items():\n@@ -2575,7 +2581,6 @@ def test_tied_weights_keys(self):\n             # These are all the pointers of shared tensors.\n             tied_params = [names for _, names in ptrs.items() if len(names) > 1]\n \n-            tied_weight_keys = model_tied._tied_weights_keys if model_tied._tied_weights_keys is not None else []\n             # Detect we get a hit for each key\n             for key in tied_weight_keys:\n                 is_tied_key = any(re.search(key, p) for group in tied_params for p in group)"
        }
    ],
    "stats": {
        "total": 86,
        "additions": 15,
        "deletions": 71
    }
}