{
    "author": "alpertunga-bile",
    "message": "[fix] fix token healing tests and usage errors (#33931)\n\n* auto-gptq requirement is removed & model is changed & tokenizer pad token is assigned\r\n\r\n* values func is changed with extensions & sequence key value bug is fixed\r\n\r\n* map key value check is added in ExtensionsTree\r\n\r\n* empty trimmed_ids bug is fixed\r\n\r\n* tail_id IndexError is fixed\r\n\r\n* empty trimmed_ids bug fix is updated for failed test\r\n\r\n* too much specific case for specific tokenizer is removed\r\n\r\n* input_ids check is updated\r\n\r\n* require auto-gptq import is removed\r\n\r\n* key error check is changed with empty list check\r\n\r\n* empty input_ids check is added\r\n\r\n* empty trimmed_ids fix is checked with numel function\r\n\r\n* usage change comments are added\r\n\r\n* test changes are commented\r\n\r\n* comment style and quality bugs are fixed\r\n\r\n* test comment style and quality bug is fixed",
    "sha": "98bad9c6d6c9ac98b42164fd882f94d4b5bfa4d7",
    "files": [
        {
            "sha": "86ea702dd9f2fefbd1731086fffd4fde62e2cc26",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 29,
            "deletions": 6,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/98bad9c6d6c9ac98b42164fd882f94d4b5bfa4d7/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98bad9c6d6c9ac98b42164fd882f94d4b5bfa4d7/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=98bad9c6d6c9ac98b42164fd882f94d4b5bfa4d7",
            "patch": "@@ -1419,7 +1419,7 @@ def _prepare_generated_length(\n         input_ids_length,\n         inputs_tensor,\n     ):\n-        \"\"\"Prepared max and min length in generaion configs to avoid clashes between similar attributes\"\"\"\n+        \"\"\"Prepared max and min length in generation configs to avoid clashes between similar attributes\"\"\"\n \n         if generation_config.max_new_tokens is not None:\n             if not has_default_max_length and generation_config.max_length is not None:\n@@ -1662,7 +1662,7 @@ def _prepare_cache_for_generation(\n         device: torch.device,\n     ) -> bool:\n         \"\"\"\n-        Prepares the cache for generation (if applicable), given `generate`'s paramaterization. If a cache is\n+        Prepares the cache for generation (if applicable), given `generate`'s parameterization. If a cache is\n         instantiated, writes it to `model_kwargs`, under the name expected by the model.\n         \"\"\"\n \n@@ -1925,7 +1925,7 @@ def generate(\n                 deadlocking if one GPU finishes generating before other GPUs. Otherwise, defaults to `False`.\n             assistant_model (`PreTrainedModel`, *optional*):\n                 An assistant model that can be used to accelerate generation. The assistant model must have the exact\n-                same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistent model\n+                same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistant model\n                 is much faster than running generation with the model you're calling generate from. As such, the\n                 assistant model should be much smaller.\n             streamer (`BaseStreamer`, *optional*):\n@@ -2442,7 +2442,15 @@ def heal_tokens(\n         # replace bos with pad to not condition healing on it\n         input_ids = torch.where(input_ids == bos_token_id, pad_token_id, input_ids)\n \n+        \"\"\"\n+        the latter code assumes the input_ids is not empty,\n+        input_id has to be checked if contains elements\n+\t\t\"\"\"\n+        if input_ids.numel() == 0:\n+            return input_ids\n+\n         tail_ids = input_ids[:, -1].tolist()\n+\n         space_tok = tokenizer.convert_ids_to_tokens(tokenizer.convert_tokens_to_ids(\" \"))[0]\n         # tail tokens are used for a prefix search, thus, whitespaces are replaced with\n         # their tokenization (e.g. 'Ä ') to enable search for tokens prefixed with a whitespace\n@@ -2454,7 +2462,14 @@ def heal_tokens(\n                 continue  # skip empty sequences (all pad ids)\n \n             # apply bias for alternatives (extensions) to the tail token\n-            seq_bias = {(alt_tok,): 10.0 for alt_tok in vocab_trie.values(prefix=tail_tok)}\n+            \"\"\"\n+            seq_bias key has to be tuple with int so have to use\n+            tokenizer function to convert str to int\n+\t\t\t\"\"\"\n+            seq_bias = {\n+                (tokenizer.convert_tokens_to_ids(alt_tok),): 10.0 for alt_tok in vocab_trie.extensions(prefix=tail_tok)\n+            }\n+\n             if len(seq_bias) == 1:\n                 continue  # skip if there are no token alternatives to heal with\n \n@@ -2463,6 +2478,14 @@ def heal_tokens(\n             generation_config.update(sequence_bias=seq_bias)\n \n             trimmed_ids = batch_ids[:-1]\n+\n+            \"\"\"\n+            the latter code assumes trimmed_ids is not empty\n+            so have to check the its element count\n+\t\t\t\"\"\"\n+            if trimmed_ids.numel() == 0:\n+                continue\n+\n             # if the prompt is a single (non-pad) token, regenerate from bos\n             if len(batch_ids[batch_ids != pad_token_id]) == 1:\n                 trimmed_ids[-1] = bos_token_id\n@@ -2915,7 +2938,7 @@ def _contrastive_search(\n                     output_attentions=output_attentions,\n                 )\n \n-            # This is essential to avoid having a last reference to the big past K-V and double the necesary memory\n+            # This is essential to avoid having a last reference to the big past K-V and double the necessary memory\n             # in the next loop\n             del next_model_inputs\n \n@@ -3658,7 +3681,7 @@ def _group_beam_search(\n             )\n \n         # initialise score of first beam of each group with 0 and the rest with -1e9. This ensures that the beams in\n-        # the same group don't produce same tokens everytime.\n+        # the same group don't produce same tokens every time.\n         beam_scores = torch.full((batch_size, num_beams), -1e9, dtype=torch.float, device=device)\n         beam_scores[:, ::num_sub_beams] = 0\n         beam_scores = beam_scores.view((batch_size * num_beams,))"
        },
        {
            "sha": "d2433868cf189730ac5b03838ff2fc6b373f02da",
            "filename": "src/transformers/tokenization_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/98bad9c6d6c9ac98b42164fd882f94d4b5bfa4d7/src%2Ftransformers%2Ftokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98bad9c6d6c9ac98b42164fd882f94d4b5bfa4d7/src%2Ftransformers%2Ftokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils.py?ref=98bad9c6d6c9ac98b42164fd882f94d4b5bfa4d7",
            "patch": "@@ -316,6 +316,9 @@ def _get_node(self, token: str) -> dict:\n         \"\"\"\n         node = self.data\n         for char in token:\n+            if char not in node:\n+                break\n+\n             node = node[char]\n         return node\n "
        },
        {
            "sha": "6766fa22b9b8a0f47b60fff42334081efcdd434c",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/98bad9c6d6c9ac98b42164fd882f94d4b5bfa4d7/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98bad9c6d6c9ac98b42164fd882f94d4b5bfa4d7/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=98bad9c6d6c9ac98b42164fd882f94d4b5bfa4d7",
            "patch": "@@ -28,7 +28,6 @@\n from transformers.testing_utils import (\n     is_flaky,\n     require_accelerate,\n-    require_auto_gptq,\n     require_optimum_quanto,\n     require_torch,\n     require_torch_gpu,\n@@ -3912,11 +3911,6 @@ def test_generate_compile_fullgraph_tiny(self):\n class TokenHealingTestCase(unittest.TestCase):\n     @parameterized.expand(\n         [\n-            (\n-                \"square_bracket\",\n-                'An example [\"like this\"] and another example [',\n-                'An example [\"like this\"] and another example [\"',\n-            ),\n             (\"url\", 'The link is <a href=\"http:', 'The link is <a href=\"http://'),\n             # aggressive_healing: \"http\" shouldn't be replaced with \"https\"\n             (\"aggressive_healing\", 'The link is <a href=\"http', 'The link is <a href=\"http'),\n@@ -3926,9 +3920,8 @@ class TokenHealingTestCase(unittest.TestCase):\n             (\"empty_prompt\", \"\", \"\"),\n         ]\n     )\n-    @require_auto_gptq\n     def test_prompts(self, name, input, expected):\n-        model_name_or_path = \"TheBloke/deepseek-llm-7B-base-GPTQ\"\n+        model_name_or_path = \"distilbert/distilgpt2\"\n         tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n         completion_model = AutoModelForCausalLM.from_pretrained(\n             model_name_or_path,\n@@ -3937,9 +3930,16 @@ def test_prompts(self, name, input, expected):\n             revision=\"main\",\n             use_cache=True,\n         )\n+\n+        \"\"\"\n+        tokenizer.pad_token value can be empty but it is required in the latter codes\n+        so assigned it here with eos_token\n+\t\t\"\"\"\n+        tokenizer.pad_token = tokenizer.eos_token\n+\n         input_ids = tokenizer(input, return_tensors=\"pt\").input_ids.to(completion_model.device)\n \n-        healed_ids = completion_model.heal_tokens(input_ids)\n+        healed_ids = completion_model.heal_tokens(input_ids, tokenizer=tokenizer)\n         predicted = tokenizer.decode(healed_ids[0], skip_special_tokens=True)\n \n         self.assertEqual(predicted, expected)"
        },
        {
            "sha": "3600be91a7c7e343f84d4f4215e326b79f68456b",
            "filename": "tests/utils/test_tokenization_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/98bad9c6d6c9ac98b42164fd882f94d4b5bfa4d7/tests%2Futils%2Ftest_tokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98bad9c6d6c9ac98b42164fd882f94d4b5bfa4d7/tests%2Futils%2Ftest_tokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_tokenization_utils.py?ref=98bad9c6d6c9ac98b42164fd882f94d4b5bfa4d7",
            "patch": "@@ -325,8 +325,9 @@ def test_empty_prefix(self):\n     def test_no_extension_match(self):\n         trie = ExtensionsTrie()\n         # Test searching for a prefix that doesn't match any key\n-        with self.assertRaises(KeyError):\n-            trie.extensions(\"unknown\")\n+        values = trie.extensions(\"unknown\")\n+\n+        self.assertEqual(len(values), 0)\n \n     def test_update_value(self):\n         trie = ExtensionsTrie()"
        }
    ],
    "stats": {
        "total": 61,
        "additions": 44,
        "deletions": 17
    }
}