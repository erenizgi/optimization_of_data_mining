{
    "author": "gante",
    "message": "[Cache] Don't initialize the cache on `meta` device (#36543)",
    "sha": "c4161238bd8f67a2d80715de7d4ce45541955693",
    "files": [
        {
            "sha": "11c25b28278c24ca3be791ca039489ab3c08b443",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 52,
            "deletions": 106,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4161238bd8f67a2d80715de7d4ce45541955693/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4161238bd8f67a2d80715de7d4ce45541955693/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=c4161238bd8f67a2d80715de7d4ce45541955693",
            "patch": "@@ -10,7 +10,6 @@\n \n from .configuration_utils import PretrainedConfig\n from .utils import is_hqq_available, is_optimum_quanto_available, logging\n-from .utils.deprecation import deprecate_kwarg\n \n \n if is_hqq_available():\n@@ -1064,18 +1063,19 @@ class StaticCache(Cache):\n             The configuration file defining the shape-related attributes required to initialize the static cache.\n         batch_size (`int`):\n             The batch size with which the model will be used. Note that a new instance must be instantiated if a\n-            smaller batch size is used. If you are manually setting the batch size, make sure to take into account the number of beams if you are running beam search\n+            smaller batch size is used. If you are manually setting the batch size, make sure to take into account the\n+            number of beams if you are running beam search\n         max_cache_len (`int`):\n             The maximum sequence length with which the model will be used.\n         device (`torch.device` or `str`):\n-            The device on which the cache should be initialized. Should be the same as the layer.\n-            The recommended way however is not not indicate any `device`, in that case cache will be initialized on `meta`\n-            device by default, and then moved to input device when updating.\n+            The device on which the cache should be initialized. If you're using more than 1 computation device, you\n+            should pass the `layer_device_map` argument instead.\n         dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n         layer_device_map(`Dict[int, Union[str, torch.device, int]]]`, `optional`):\n-            Mapping between the layers and its device. This is required when you are manually initializing the cache and the model is splitted between different gpus.\n-            You can know which layers mapped to which device by checking the associated device_map: `model.hf_device_map`.\n+            Mapping between the layers and its device. This is required when you are manually initializing the cache\n+            and the model is splitted between differents gpus. You can know which layers mapped to which device by\n+            checking the associated device_map: `model.hf_device_map`.\n \n \n     Example:\n@@ -1101,7 +1101,6 @@ class StaticCache(Cache):\n     is_compileable = True\n \n     # TODO (joao): remove `=None` in non-optional arguments in v4.46. Remove from `OBJECTS_TO_IGNORE` as well.\n-    @deprecate_kwarg(\"layer_device_map\", version=\"4.52.0\")\n     def __init__(\n         self,\n         config: PretrainedConfig,\n@@ -1128,7 +1127,6 @@ def __init__(\n         )\n \n         self.dtype = dtype\n-        self.device = torch.device(device) if device is not None else torch.device(\"meta\")\n         self.num_key_value_heads = (\n             config.num_attention_heads\n             if getattr(config, \"num_key_value_heads\", None) is None\n@@ -1139,11 +1137,12 @@ def __init__(\n         self.value_cache: List[torch.Tensor] = []\n         # Note: There will be significant perf decrease if switching to use 5D tensors instead.\n         cache_shape = (self.max_batch_size, self.num_key_value_heads, self.max_cache_len, self.head_dim)\n+        device = torch.device(device) if device is not None else None\n         for idx in range(config.num_hidden_layers):\n             if layer_device_map is not None:\n                 layer_device = layer_device_map[idx]\n             else:\n-                layer_device = self.device\n+                layer_device = device\n             new_layer_key_cache = torch.zeros(cache_shape, dtype=self.dtype, device=layer_device)\n             new_layer_value_cache = torch.zeros(cache_shape, dtype=self.dtype, device=layer_device)\n             # Note: `mark_static_address` is used to tag the cache as a fixed data pointer,\n@@ -1178,12 +1177,7 @@ def update(\n         Return:\n             A tuple containing the updated key and value states.\n         \"\"\"\n-\n         cache_position = cache_kwargs.get(\"cache_position\")\n-        if self.key_cache[layer_idx].device.type == \"meta\":\n-            self.key_cache[layer_idx] = torch.zeros_like(self.key_cache[layer_idx], device=key_states.device)\n-            self.value_cache[layer_idx] = torch.zeros_like(self.value_cache[layer_idx], device=value_states.device)\n-\n         k_out = self.key_cache[layer_idx]\n         v_out = self.value_cache[layer_idx]\n         key_states = key_states.to(k_out.dtype)\n@@ -1211,8 +1205,6 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         # Occupied cache == any slot in the 3rd dim (sequence length) holds a non-zero value. To save on compute, let's\n         # limit the check to the first batch member and head dimension.\n         # TODO: deprecate this function in favor of `cache_position`\n-        if self.key_cache[layer_idx].device.type == \"meta\":\n-            return 0\n         return (self.key_cache[layer_idx][0, 0].any(dim=-1)).sum()\n \n     def get_max_cache_shape(self) -> Optional[int]:\n@@ -1221,10 +1213,9 @@ def get_max_cache_shape(self) -> Optional[int]:\n     def reset(self):\n         \"\"\"Resets the cache values while preserving the objects\"\"\"\n         for layer_idx in range(len(self.key_cache)):\n-            if self.key_cache[layer_idx].device.type != \"meta\":\n-                # In-place ops prevent breaking the static address\n-                self.key_cache[layer_idx].zero_()\n-                self.value_cache[layer_idx].zero_()\n+            # In-place ops prevent breaking the static address\n+            self.key_cache[layer_idx].zero_()\n+            self.value_cache[layer_idx].zero_()\n \n     @property\n     def batch_size(self):\n@@ -1261,14 +1252,14 @@ class SlidingWindowCache(StaticCache):\n         max_cache_len (`int`):\n             The maximum sequence length with which the model will be used.\n         device (`torch.device` or `str`):\n-            The device on which the cache should be initialized. Should be the same as the layer.\n-            The recommended way however is not not indicate any `device`, in that case cache will be initialized on `meta`\n-            device by default, and then moved to input device when updating.\n+            The device on which the cache should be initialized. If you're using more than 1 computation device, you\n+            should pass the `layer_device_map` argument instead.\n         dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n         layer_device_map(`Dict[int, Union[str, torch.device, int]]]`, `optional`):\n-            Mapping between the layers and its device. This is required when you are manually initializing the cache and the model is splitted between different gpus.\n-            You can know which layers mapped to which device by checking the associated device_map: `model.hf_device_map`.\n+            Mapping between the layers and its device. This is required when you are manually initializing the cache\n+            and the model is splitted between differents gpus. You can know which layers mapped to which device by\n+            checking the associated device_map: `model.hf_device_map`.\n \n     Example:\n \n@@ -1329,11 +1320,6 @@ def update(\n         cache_kwargs: Optional[Dict[str, Any]] = None,\n     ) -> Tuple[torch.Tensor]:\n         cache_position = cache_kwargs.get(\"cache_position\")\n-\n-        if self.key_cache[layer_idx].device.type == \"meta\":\n-            self.key_cache[layer_idx] = torch.zeros_like(self.key_cache[layer_idx], device=key_states.device)\n-            self.value_cache[layer_idx] = torch.zeros_like(self.value_cache[layer_idx], device=value_states.device)\n-\n         k_out = self.key_cache[layer_idx]\n         v_out = self.value_cache[layer_idx]\n         key_states = key_states.to(k_out.dtype)\n@@ -1380,10 +1366,9 @@ def get_max_cache_shape(self) -> Optional[int]:\n \n     def reset(self):\n         for layer_idx in range(len(self.key_cache)):\n-            if self.key_cache[layer_idx].device.type != \"meta\":\n-                # In-place ops prevent breaking the static address\n-                self.key_cache[layer_idx].zero_()\n-                self.value_cache[layer_idx].zero_()\n+            # In-place ops prevent breaking the static address\n+            self.key_cache[layer_idx].zero_()\n+            self.value_cache[layer_idx].zero_()\n \n \n class EncoderDecoderCache(Cache):\n@@ -1573,14 +1558,14 @@ class HybridCache(Cache):\n         max_cache_len (`int`):\n             The maximum sequence length with which the model will be used.\n         device (`torch.device` or `str`, *optional*):\n-            The device on which the cache should be initialized. Should be the same as the layer.\n-            The recommended way however is not not indicate any `device`, in that case cache will be initialized on `meta`\n-            device by default, and then moved to input device when updating.\n+            The device on which the cache should be initialized. If you're using more than 1 computation device, you\n+            should pass the `layer_device_map` argument instead.\n         dtype (torch.dtype, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n         layer_device_map(`Dict[int, Union[str, torch.device, int]]]`, `optional`):\n-            Mapping between the layers and its device. This is required when you are manually initializing the cache and the model is splitted between different gpus.\n-            You can know which layers mapped to which device by checking the associated device_map: `model.hf_device_map`.\n+            Mapping between the layers and its device. This is required when you are manually initializing the cache\n+            and the model is splitted between differents gpus. You can know which layers mapped to which device by\n+            checking the associated device_map: `model.hf_device_map`.\n \n     Example:\n \n@@ -1607,7 +1592,6 @@ class HybridCache(Cache):\n     # is_compileable = True\n \n     # TODO (joao): remove `=None` in non-optional arguments in v4.46. Remove from `OBJECTS_TO_IGNORE` as well.\n-    @deprecate_kwarg(\"layer_device_map\", version=\"4.52.0\")\n     def __init__(\n         self,\n         config: PretrainedConfig,\n@@ -1642,7 +1626,6 @@ def __init__(\n             config.num_attention_heads if config.num_key_value_heads is None else config.num_key_value_heads\n         )\n \n-        self.device = torch.device(device) if device is not None else torch.device(\"meta\")\n         layer_switch = config.sliding_window_pattern if hasattr(config, \"sliding_window_pattern\") else 2  # 2 is for BC\n         self.is_sliding = torch.tensor(\n             [bool((i + 1) % layer_switch) for i in range(config.num_hidden_layers)], dtype=torch.bool\n@@ -1656,11 +1639,12 @@ def __init__(\n             min(config.sliding_window, max_cache_len),\n             self.head_dim,\n         )\n+        device = torch.device(device) if device is not None else None\n         for i in range(config.num_hidden_layers):\n             if layer_device_map is not None:\n                 layer_device = layer_device_map[i]\n             else:\n-                layer_device = self.device\n+                layer_device = device\n             # Note: `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\n             # breaks when updating the cache.\n             cache_shape = global_cache_shape if not self.is_sliding[i] else sliding_cache_shape\n@@ -1717,9 +1701,12 @@ def update(\n         cache_position = cache_kwargs.get(\"cache_position\")\n         sliding_window = cache_kwargs.get(\"sliding_window\")\n \n-        if self.key_cache[layer_idx].device.type == \"meta\":\n-            self.key_cache[layer_idx] = torch.zeros_like(self.key_cache[layer_idx], device=key_states.device)\n-            self.value_cache[layer_idx] = torch.zeros_like(self.value_cache[layer_idx], device=value_states.device)\n+        # These two `if` blocks are only reached in multigpu and if `layer_device_map` is not passed. They are used\n+        # when the cache is initialized in the forward pass (e.g. Gemma2)\n+        if self.key_cache[layer_idx].device != key_states.device:\n+            self.key_cache[layer_idx] = self.key_cache[layer_idx].to(key_states.device)\n+        if self.value_cache[layer_idx].device != value_states.device:\n+            self.value_cache[layer_idx] = self.value_cache[layer_idx].to(value_states.device)\n \n         k_out = self.key_cache[layer_idx]\n         v_out = self.value_cache[layer_idx]\n@@ -1753,18 +1740,14 @@ def get_seq_length(self, layer_idx: Optional[int] = 0):\n                 \"`get_seq_length` on `HybridCache` may get inconsistent results depending on the layer index. \"\n                 \"Using the `layer_idx` argument is not supported.\"\n             )\n-\n-        if self.key_cache[layer_idx].device.type == \"meta\":\n-            return 0\n         return (self.key_cache[layer_idx][0, 0].any(dim=-1)).sum()\n \n     def reset(self):\n         \"\"\"Resets the cache values while preserving the objects\"\"\"\n         for layer_idx in range(len(self.key_cache)):\n-            if self.key_cache[layer_idx].device.type != \"meta\":\n-                # In-place ops prevent breaking the static address\n-                self.key_cache[layer_idx].zero_()\n-                self.value_cache[layer_idx].zero_()\n+            # In-place ops prevent breaking the static address\n+            self.key_cache[layer_idx].zero_()\n+            self.value_cache[layer_idx].zero_()\n \n     @property\n     def batch_size(self):\n@@ -1789,24 +1772,6 @@ class MambaCache:\n             The default `dtype` to use when initializing the layer.\n         device (`torch.device` or `str`, *optional*):\n             The device on which the cache should be initialized. Should be the same as the layer.\n-            The recommended way however is not not indicate any `device`, in that case cache will be initialized on `meta`\n-            device by default, and then moved to input device when updating.\n-\n-    Attributes:\n-        dtype: (`torch.dtype`):\n-            The default `dtype` used to initializing the cache.\n-        device (`torch.device`):\n-            The default device on which the cache was initialized.\n-        intermediate_size: (`int`):\n-            Model's intermediate_size taken from config.\n-        ssm_state_size: (`int`):\n-            Model's state_size taken from config.\n-        conv_kernel_size: (`int`):\n-            Model's convolution kernel size taken from config\n-        conv_states: (`torch.Tensor`):\n-            A tensor of shape `[layer_idx, batch_size, intermediate_size, conv_kernel_size]` that holds convolutional states.\n-        ssm_states: (`torch.Tensor`):\n-            A tensor of shape `[layer_idx, batch_size, intermediate_size, ssm_state_size]` that holds ssm states\n \n     Example:\n \n@@ -1829,6 +1794,7 @@ class MambaCache:\n     is_compileable = True\n \n     # TODO (joao): remove `=None` in non-optional arguments in v4.46. Remove from `OBJECTS_TO_IGNORE` as well.\n+    # TODO (joao): add layer_device_map arg and update code in `generate` accordingly\n     def __init__(\n         self,\n         config: PretrainedConfig,\n@@ -1847,23 +1813,23 @@ def __init__(\n         self.intermediate_size = config.intermediate_size\n         self.ssm_state_size = config.state_size\n         self.conv_kernel_size = config.conv_kernel\n-        self.device = torch.device(device) if device is not None else torch.device(\"meta\")\n \n         self.conv_states: List[torch.Tensor] = []\n         self.ssm_states: List[torch.Tensor] = []\n+        device = torch.device(device) if device is not None else None\n         for _ in range(config.num_hidden_layers):\n             conv_state: torch.Tensor = torch.zeros(\n                 self.max_batch_size,\n                 self.intermediate_size,\n                 self.conv_kernel_size,\n-                device=self.device,\n+                device=device,\n                 dtype=dtype,\n             )\n             ssm_state: torch.Tensor = torch.zeros(\n                 self.max_batch_size,\n                 self.intermediate_size,\n                 self.ssm_state_size,\n-                device=self.device,\n+                device=device,\n                 dtype=dtype,\n             )\n \n@@ -1875,11 +1841,10 @@ def __init__(\n     def update_conv_state(\n         self, layer_idx: int, new_conv_state: torch.Tensor, cache_position: torch.LongTensor\n     ) -> torch.Tensor:\n-        if self.conv_states[layer_idx].device.type == \"meta\":\n-            self.conv_states[layer_idx] = torch.zeros_like(\n-                self.conv_states[layer_idx],\n-                device=new_conv_state.device,\n-            )\n+        # This `if` blocks is only reached in multigpu and if `layer_device_map` is not passed. It is used\n+        # when the cache is initialized in the forward pass (e.g. Mamba)\n+        if self.conv_states[layer_idx].device != new_conv_state.device:\n+            self.conv_states[layer_idx] = self.conv_states[layer_idx].to(new_conv_state.device)\n \n         conv_state = self.conv_states[layer_idx]\n         cache_position = cache_position.clamp(0, self.conv_kernel_size - 1)\n@@ -1896,10 +1861,9 @@ def update_ssm_state(self, layer_idx: int, new_ssm_state: torch.Tensor):\n \n     def reset(self):\n         for layer_idx in range(len(self.conv_states)):\n-            if self.conv_states[layer_idx].device.type != \"meta\":\n-                # In-place ops prevent breaking the static address\n-                self.conv_states[layer_idx].zero_()\n-                self.ssm_states[layer_idx].zero_()\n+            # In-place ops prevent breaking the static address\n+            self.conv_states[layer_idx].zero_()\n+            self.ssm_states[layer_idx].zero_()\n \n     @property\n     def batch_size(self):\n@@ -1924,33 +1888,16 @@ class OffloadedStaticCache(StaticCache):\n         max_cache_len (`int`):\n             The maximum sequence length with which the model will be used.\n         device (`Union[str, torch.device]`):\n-            The device on which the cache should be initialized. Should be the same as the\n-            layer device.\n+            The device on which the cache should be initialized. If you're using more than 1 computation device, you\n+            should pass the `layer_device_map` argument instead.\n         dtype (`torch.dtype`, *optional*):\n             The default `dtype` to use when initializing the cache.\n         offload_device (`Union[str, torch.device]`, *optional*, defaults to `cpu`):\n             The device to offload to. Defaults to CPU.\n         layer_device_map (`Dict[int, Union[str, torch.device, int]]`, *optional*):\n-            Mapping between the layers and its device. This is required when you are manually initializing the cache and the model is splitted between different gpus.\n-            You can know which layers mapped to which device by checking the associated device_map: `model.hf_device_map`.\n-\n-    Attributes:\n-        key_cache (`List[torch.Tensor]`):\n-            Off-loaded key cache tensors. First one will be on device, where-as the others are\n-            off-loaded.\n-        value_cache (`List[torch.Tensor]`):\n-            Off-loaded value cache tensors. First one will be on device, where-as the others are\n-            off-loaded.\n-        max_batch_size (`int`):\n-            The maximum batch size with which this cache can be used.\n-        max_cache_len (`int`):\n-            The maximum sequence length with which this cache can be used.\n-        device (`torch.device`):\n-            The device on which the cache is used.\n-        offload_device (`torch.device`):\n-            The device used to offload to.\n-        dtype (`torch.dtype`):\n-            The `dtype` used to initializing the cache.\n+            Mapping between the layers and its device. This is required when you are manually initializing the cache\n+            and the model is splitted between differents gpus. You can know which layers mapped to which device by\n+            checking the associated device_map: `model.hf_device_map`.\n \n     Example:\n \n@@ -1973,7 +1920,6 @@ class OffloadedStaticCache(StaticCache):\n \n     is_compileable = True\n \n-    @deprecate_kwarg(\"layer_device_map\", version=\"4.52.0\")\n     def __init__(\n         self,\n         config: PretrainedConfig,"
        },
        {
            "sha": "6ee48ab3f1a5219c52ab437e2a2d787dd7615c69",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4161238bd8f67a2d80715de7d4ce45541955693/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4161238bd8f67a2d80715de7d4ce45541955693/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=c4161238bd8f67a2d80715de7d4ce45541955693",
            "patch": "@@ -483,7 +483,7 @@ def __init__(self, **kwargs):\n         self.assistant_lookbehind = kwargs.pop(\"assistant_lookbehind\", 10)\n         self.target_lookbehind = kwargs.pop(\"target_lookbehind\", 10)\n \n-        # Performances\n+        # Performance\n         self.compile_config = kwargs.pop(\"compile_config\", CompileConfig())\n         self.disable_compile = kwargs.pop(\"disable_compile\", False)\n         # Wild card"
        },
        {
            "sha": "8d5be7d7a05e73c9266651da2c5cbaa22b80507d",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 37,
            "deletions": 1,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4161238bd8f67a2d80715de7d4ce45541955693/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4161238bd8f67a2d80715de7d4ce45541955693/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=c4161238bd8f67a2d80715de7d4ce45541955693",
            "patch": "@@ -1618,6 +1618,40 @@ def _get_initial_cache_position(self, input_ids, model_kwargs):\n         model_kwargs[\"cache_position\"] = cache_position\n         return model_kwargs\n \n+    def _get_layer_device_map_for_cache_init(self):\n+        \"\"\"\n+        Taken from `dispatch_model` from accelerate.\n+        This is needed here if we don't want to make changes in accelerate in order to save execution_device\n+        For offloaded case, we need to get the execution device, not just the device where it is offloaded\n+        \"\"\"\n+        execution_device_map = None\n+\n+        if hasattr(self, \"hf_device_map\"):\n+            if set(self.hf_device_map.values()) == {\"cpu\"} or set(self.hf_device_map.values()) == {\"cpu\", \"disk\"}:\n+                main_device = \"cpu\"\n+            else:\n+                main_device = [d for d in self.hf_device_map.values() if d not in [\"cpu\", \"disk\"]][0]\n+            execution_device_map = {\n+                name: main_device if device in [\"cpu\", \"disk\"] else device\n+                for name, device in self.hf_device_map.items()\n+            }\n+\n+        num_hidden_layers = self.config.get_text_config().num_hidden_layers\n+        if execution_device_map is None:\n+            return None\n+        elif len(execution_device_map) == 1 and \"\" in execution_device_map:\n+            return {idx: execution_device_map[\"\"] for idx in range(num_hidden_layers)}\n+        layer_device_map = {}\n+        for layer in execution_device_map:\n+            for idx in range(num_hidden_layers):\n+                if f\".{idx}.\" in f\"{layer}.\":\n+                    layer_device_map[idx] = execution_device_map[layer]\n+                    break\n+        for idx in range(num_hidden_layers):\n+            if idx not in layer_device_map:\n+                raise RuntimeError(f\"layer {idx} has not been mapped to a device.\")\n+        return layer_device_map\n+\n     def _get_cache(\n         self, cache_implementation: str, batch_size: int, max_cache_len: int, device: torch.device, model_kwargs\n     ) -> Cache:\n@@ -1664,12 +1698,14 @@ def _get_cache(\n                     # models. May cause trobles with non-text modalities.\n                     cache_dtype = self.get_output_embeddings().weight.dtype\n \n+            layer_device_map = self._get_layer_device_map_for_cache_init()\n             cache_kwargs = {\n                 \"config\": self.config.get_text_config(),\n                 \"max_batch_size\": batch_size,\n                 \"max_cache_len\": max_cache_len,\n                 \"dtype\": cache_dtype,\n-                \"device\": device if cache_implementation == \"offloaded_static\" else None,\n+                \"device\": device,\n+                \"layer_device_map\": layer_device_map,\n             }\n             self._cache = cache_cls(**cache_kwargs)\n             if requires_cross_attention_cache:"
        },
        {
            "sha": "1e64518299103ab458dd360c36c4cf054beab792",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4161238bd8f67a2d80715de7d4ce45541955693/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4161238bd8f67a2d80715de7d4ce45541955693/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=c4161238bd8f67a2d80715de7d4ce45541955693",
            "patch": "@@ -597,11 +597,13 @@ def forward(\n \n         if use_cache and past_key_values is None and not self.training:\n             batch_size, seq_len, _ = inputs_embeds.shape\n+            # NOTE: ideally, `HybridCache` should be initialized outside the model with `layer_device_map`\n             past_key_values = HybridCache(\n                 self.config,\n                 max_batch_size=batch_size,\n                 max_cache_len=seq_len,\n                 dtype=inputs_embeds.dtype,\n+                device=self.device,\n             )\n \n         if cache_position is None:"
        },
        {
            "sha": "2a2b1c88c3bfcda1cb76e43c17dc00d20bfb2d97",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4161238bd8f67a2d80715de7d4ce45541955693/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4161238bd8f67a2d80715de7d4ce45541955693/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=c4161238bd8f67a2d80715de7d4ce45541955693",
            "patch": "@@ -488,11 +488,13 @@ def forward(\n \n         if use_cache and past_key_values is None and not self.training:\n             batch_size, seq_len, _ = inputs_embeds.shape\n+            # NOTE: ideally, `HybridCache` should be initialized outside the model with `layer_device_map`\n             past_key_values = HybridCache(\n                 self.config,\n                 max_batch_size=batch_size,\n                 max_cache_len=seq_len,\n                 dtype=inputs_embeds.dtype,\n+                device=self.device,\n             )\n \n         if cache_position is None:"
        },
        {
            "sha": "ba16f52c354745eb13264c2a09e27c2dcec6e257",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4161238bd8f67a2d80715de7d4ce45541955693/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4161238bd8f67a2d80715de7d4ce45541955693/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=c4161238bd8f67a2d80715de7d4ce45541955693",
            "patch": "@@ -599,11 +599,13 @@ def forward(\n \n         if use_cache and past_key_values is None and not self.training:\n             batch_size, seq_len, _ = inputs_embeds.shape\n+            # NOTE: ideally, `HybridCache` should be initialized outside the model with `layer_device_map`\n             past_key_values = HybridCache(\n                 self.config,\n                 max_batch_size=batch_size,\n                 max_cache_len=seq_len,\n                 dtype=inputs_embeds.dtype,\n+                device=self.device,\n             )\n \n         if cache_position is None:"
        },
        {
            "sha": "ab567c61d0d401241fd4ecfb692c249faa5bcc1f",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4161238bd8f67a2d80715de7d4ce45541955693/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4161238bd8f67a2d80715de7d4ce45541955693/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=c4161238bd8f67a2d80715de7d4ce45541955693",
            "patch": "@@ -437,11 +437,13 @@ def forward(\n \n         if use_cache and past_key_values is None and not self.training:\n             batch_size, seq_len, _ = inputs_embeds.shape\n+            # NOTE: ideally, `HybridCache` should be initialized outside the model with `layer_device_map`\n             past_key_values = HybridCache(\n                 self.config,\n                 max_batch_size=batch_size,\n                 max_cache_len=seq_len,\n                 dtype=inputs_embeds.dtype,\n+                device=self.device,\n             )\n \n         if cache_position is None:"
        },
        {
            "sha": "df58c7fc5c845ec10985dcc45c6c4fde09b99d60",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 39,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4161238bd8f67a2d80715de7d4ce45541955693/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4161238bd8f67a2d80715de7d4ce45541955693/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=c4161238bd8f67a2d80715de7d4ce45541955693",
            "patch": "@@ -2304,45 +2304,6 @@ def test_generate_methods_with_logits_to_keep(self):\n             without_all_logits = model.generate(**inputs_dict, **generation_kwargs)\n             self.assertEqual(with_all_logits.tolist(), without_all_logits.tolist())\n \n-    @pytest.mark.generate\n-    @is_flaky\n-    def test_assisted_decoding_with_logits_to_keep(self):\n-        for model_class in self.all_generative_model_classes:\n-            if \"logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):\n-                self.skipTest(reason=\"This model does not support `logits_to_keep` argument.\")\n-            if model_class._is_stateful:\n-                self.skipTest(reason=\"Stateful models don't support assisted generation\")\n-\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n-            # NOTE: assisted generation only works with cache on at the moment.\n-            if not hasattr(config.get_text_config(), \"use_cache\"):\n-                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n-            config.use_cache = True\n-            config.is_decoder = True\n-\n-            model = model_class(config).to(torch_device).eval()\n-            assistant_model = model\n-            # All generation methods (except assisted decoding) rely on always extracting the last token logits of the\n-            # full logits matrix, so testing out only greedy search and assisted decoding is enough (if it works,\n-            # other methods will work as well)\n-            generation_kwargs = {\n-                \"max_new_tokens\": 10,\n-                \"do_sample\": False,\n-                \"assistant_model\": assistant_model,\n-                \"return_dict_in_generate\": True,\n-                \"output_scores\": True,\n-            }\n-            logits_processor_kwargs = self._get_logits_processor_kwargs(config=model.config)\n-\n-            # Setting logits_to_keep at 0 keeps all logits (old behavior)\n-            with_all_logits = model.generate(\n-                **generation_kwargs, **inputs_dict, **logits_processor_kwargs, logits_to_keep=0\n-            )\n-            # By default, logits_to_keep is automatically set to 1 if not provided (new behavior)\n-            without_all_logits = model.generate(**inputs_dict, **generation_kwargs, **logits_processor_kwargs)\n-\n-            self._check_similar_generate_outputs(with_all_logits, without_all_logits)\n-\n     @pytest.mark.generate\n     def test_inherits_generation_mixin(self):\n         \"\"\""
        },
        {
            "sha": "fc7617e6493908987763b91140bcab2c4fe02c48",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4161238bd8f67a2d80715de7d4ce45541955693/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4161238bd8f67a2d80715de7d4ce45541955693/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=c4161238bd8f67a2d80715de7d4ce45541955693",
            "patch": "@@ -20,6 +20,7 @@\n \n from transformers import set_seed\n from transformers.testing_utils import (\n+    CaptureStderr,\n     get_gpu_count,\n     is_torch_available,\n     require_gptq,\n@@ -654,3 +655,42 @@ def test_data_parallel_dynamic_cache(self):\n                 torch.testing.assert_close(\n                     actual=parallelism_cache[layer_idx][kv_idx], expected=no_parallelism_cache[layer_idx][kv_idx]\n                 )\n+\n+    @require_torch_gpu\n+    def test_static_cache_no_cuda_graph_skips(self):\n+        \"\"\"\n+        Tests generating with static cache and compilation doesn't skip cuda graphs. Regression test for #36543.\n+\n+        (? We set `fullgraph=True`, which according to torch docs means it should raise an exception. Instead,\n+        messages are being thrown to stderr?)\n+        \"\"\"\n+        model_repo = \"hf-internal-testing/tiny-random-MistralForCausalLM\"\n+        model = AutoModelForCausalLM.from_pretrained(model_repo).to(torch_device)\n+        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n+        inputs = tokenizer([\"foo bar\"], return_tensors=\"pt\").to(torch_device)\n+\n+        # on `main`, prior to #36543, this would send stderr messages about cuda graphs being skipped.\n+        with CaptureStderr() as cap:\n+            model.generate(**inputs, max_new_tokens=2, cache_implementation=\"static\")\n+        self.assertEqual(cap.err, \"\")\n+\n+    @require_torch_multi_gpu\n+    def test_static_cache_multi_gpu(self):\n+        \"\"\"Regression test for #35164: static cache with multi-gpu\"\"\"\n+\n+        model_id = \"google/gemma-2-2b-it\"\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+        device_map = {\"model.embed_tokens\": 0, \"model.norm\": 1, \"model.rotary_emb\": 1, \"lm_head\": 0}\n+        num_hidden_layers = 26\n+        for i in range(num_hidden_layers):\n+            device_map[f\"model.layers.{i}\"] = 0 if i < 13 else 1\n+\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id,\n+            torch_dtype=\"bfloat16\",\n+            device_map=device_map,\n+        )\n+        inputs = tokenizer(\"Today is a beautiful day!\", return_tensors=\"pt\").to(0)\n+        _ = model(**inputs)\n+        _ = model.generate(**inputs, max_new_tokens=2, cache_implementation=\"hybrid\")"
        }
    ],
    "stats": {
        "total": 285,
        "additions": 138,
        "deletions": 147
    }
}