{
    "author": "Abdennacer-Badaoui",
    "message": "Fix validation checks order in benchmark_v2 (#42280)\n\nfix validation checks order",
    "sha": "ea30d827697cfe548dd6691e8fc191b010e2433f",
    "files": [
        {
            "sha": "34f08f525893ca7e386da7ebc734d9296995857d",
            "filename": "benchmark_v2/framework/benchmark_config.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea30d827697cfe548dd6691e8fc191b010e2433f/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea30d827697cfe548dd6691e8fc191b010e2433f/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_config.py?ref=ea30d827697cfe548dd6691e8fc191b010e2433f",
            "patch": "@@ -203,6 +203,8 @@ def adapt_configs(\n             config[\"sequence_length\"] = seqlen\n             config[\"num_tokens_to_generate\"] = ntok\n             config[\"gpu_monitoring\"] = monitor\n+            # Remove the old name so it gets re-inferred with the updated values\n+            config.pop(\"name\", None)\n             adapted_configs.append(BenchmarkConfig.from_dict(config))\n     return adapted_configs\n "
        },
        {
            "sha": "e4373812c715403124104293278306eb230ebcde",
            "filename": "benchmark_v2/run_benchmarks.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea30d827697cfe548dd6691e8fc191b010e2433f/benchmark_v2%2Frun_benchmarks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea30d827697cfe548dd6691e8fc191b010e2433f/benchmark_v2%2Frun_benchmarks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Frun_benchmarks.py?ref=ea30d827697cfe548dd6691e8fc191b010e2433f",
            "patch": "@@ -80,16 +80,16 @@\n     logger.info(f\"Benchmark run UUID: {benchmark_run_uuid}\")\n     logger.info(f\"Output directory: {args.output_dir}\")\n \n-    # We cannot compute ITL if we don't have at least two measurements\n-    if any(n <= 1 for n in args.num_tokens_to_generate):\n-        raise ValueError(\"--num_tokens_to_generate arguments should be larger than 1\")\n-\n     # Error out if one of the arguments is not provided\n-    if len(args.batch_size) * len(args.sequence_length) * len(args.num_tokens_to_generate) == 0:\n+    if any(arg is None for arg in [args.batch_size, args.sequence_length, args.num_tokens_to_generate]):\n         raise ValueError(\n-            \"At least one of the arguments --batch-size, --sequence-length, or --num-tokens-to-generate is required\"\n+            \"All of the arguments --batch-size, --sequence-length, and --num-tokens-to-generate are required\"\n         )\n \n+    # We cannot compute ITL if we don't have at least two measurements\n+    if any(n <= 1 for n in args.num_tokens_to_generate):\n+        raise ValueError(\"--num_tokens_to_generate arguments should be larger than 1\")\n+\n     # Get the configs for the given coverage level\n     configs = get_config_by_level(args.level)\n     # Adapt the configs to the given arguments"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 8,
        "deletions": 6
    }
}