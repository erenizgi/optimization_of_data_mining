{
    "author": "FremyCompany",
    "message": "Do not erase a cache_position passed explicitly to generate(), if there is one (#37986)\n\nDo not erase a cache_position initialization passed explicitly to generate(), if there is one.\n\nBut: Let initialization replace cache_position if it's set to None. I assume that if the value is explicitly passed but None, we should initialize anyway.",
    "sha": "774dc274ac966f4bccbcd90d55bba23f6cca37ae",
    "files": [
        {
            "sha": "ebea3af1b1f47c5daabdc3e4c47a675ddad1be91",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/774dc274ac966f4bccbcd90d55bba23f6cca37ae/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/774dc274ac966f4bccbcd90d55bba23f6cca37ae/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=774dc274ac966f4bccbcd90d55bba23f6cca37ae",
            "patch": "@@ -1711,6 +1711,8 @@ def _prepare_generation_config(\n     def _get_initial_cache_position(self, seq_length, device, model_kwargs):\n         \"\"\"Calculates `cache_position` for the pre-fill stage based on `input_ids` and optionally past length\"\"\"\n         # `torch.compile`-friendly `torch.arange` from a shape -- the lines below are equivalent to `torch.arange`\n+        if \"cache_position\" in model_kwargs and model_kwargs[\"cache_position\"]:\n+            return model_kwargs\n         if \"inputs_embeds\" in model_kwargs and not self.config.is_encoder_decoder:\n             cache_position = torch.ones_like(model_kwargs[\"inputs_embeds\"][0, :, 0], dtype=torch.int64).cumsum(0) - 1\n         elif \"decoder_inputs_embeds\" in model_kwargs and self.config.is_encoder_decoder:"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 2,
        "deletions": 0
    }
}