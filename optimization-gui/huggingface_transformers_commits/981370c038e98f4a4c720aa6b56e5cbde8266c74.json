{
    "author": "cyyever",
    "message": "Format MarkDown documentation and tiny fixes (#41638)\n\n* Fix MarkDown syntax\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "981370c038e98f4a4c720aa6b56e5cbde8266c74",
    "files": [
        {
            "sha": "8b2e315706ad5d6947b22024ae58afbc6158fab5",
            "filename": "docs/source/en/accelerator_selection.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Faccelerator_selection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Faccelerator_selection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Faccelerator_selection.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -55,6 +55,7 @@ deepspeed --num_gpus 2 trainer-program.py ...\n </hfoptions>\n \n ## Order of accelerators\n+\n To select specific accelerators to use and their order, use the environment variable appropriate for your hardware. This is often set on the command line for each run, but can also be added to your `~/.bashrc` or other startup config file.\n \n For example, if there are 4 accelerators (0, 1, 2, 3) and you only want to run accelerators 0 and 2:"
        },
        {
            "sha": "d08caaa347bdae45e84627c3c21f54544d85dff4",
            "filename": "docs/source/en/community.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fcommunity.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fcommunity.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcommunity.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -6,13 +6,13 @@ rendered properly in your Markdown viewer.\n \n This page regroups resources around ü§ó Transformers developed by the community.\n \n-## Community resources:\n+## Community resources\n \n | Resource     |      Description      |      Author      |\n |:----------|:-------------|------:|\n | [Hugging Face Transformers Glossary Flashcards](https://www.darigovresearch.com/huggingface-transformers-glossary-flashcards) | A set of flashcards based on the [Transformers Docs Glossary](glossary) that has been put into a form which can be easily learned/revised using [Anki](https://apps.ankiweb.net/) an open source, cross platform app specifically designed for long term knowledge retention. See this [Introductory video on how to use the flashcards](https://www.youtube.com/watch?v=Dji_h7PILrw). | [Darigov Research](https://www.darigovresearch.com/) |\n \n-## Community notebooks:\n+## Community notebooks\n \n | Notebook     |      Description      |      Author      |      |\n |:----------|:-------------|:-------------|------:|"
        },
        {
            "sha": "561ed655c3368ef78c047ae331d2afe0958f2593",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -208,7 +208,7 @@ Some models have a unique way of storing past kv pairs or states that is not com\n \n Mamba models, such as [Mamba](./model_doc/mamba), require a specific cache because the model doesn't have an attention mechanism or kv states. Thus, they are not compatible with the above [`Cache`] classes.\n \n-# Iterative generation\n+## Iterative generation\n \n A cache can also work in iterative generation settings where there is back-and-forth interaction with a model (chatbots). Like regular generation, iterative generation with a cache allows a model to efficiently handle ongoing conversations without recomputing the entire context at each step.\n "
        },
        {
            "sha": "2bba4423fced491680d3a9a4ec9203a73084e67b",
            "filename": "docs/source/en/model_doc/bart.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -23,6 +23,7 @@ rendered properly in your Markdown viewer.\n </div>\n \n # BART\n+\n [BART](https://huggingface.co/papers/1910.13461) is a sequence-to-sequence model that combines the pretraining objectives from BERT and GPT. It's pretrained by corrupting text in different ways like deleting words, shuffling sentences, or masking tokens and learning how to fix it. The encoder encodes the corrupted document and the corrupted text is fixed by the decoder. As it learns to recover the original text, BART gets really good at both understanding and generating language.\n \n You can find all the original BART checkpoints under the [AI at Meta](https://huggingface.co/facebook?search_models=bart) organization."
        },
        {
            "sha": "2b5312658701a4a421aa5d654ab2988fa5228fc5",
            "filename": "docs/source/en/model_doc/blt.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fblt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fblt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblt.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -38,7 +38,7 @@ The abstract from the paper is the following:\n efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating\n more compute and model capacity where increased data complexity demands it. We present the first flop controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.*\n \n-## Usage Tips:\n+## Usage Tips\n \n - **Dual Model Architecture**: BLT consists of two separate trained models:\n   - **Patcher (Entropy Model)**: A smaller transformer model that predicts byte-level entropy to determine patch boundaries and segment input."
        },
        {
            "sha": "65c0114fc7fb9e4dbd7aa7de794cd854c452b977",
            "filename": "docs/source/en/model_doc/chameleon.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -25,8 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Chameleon model was proposed in [Chameleon: Mixed-Modal Early-Fusion Foundation Models\n-](https://huggingface.co/papers/2405.09818) by META AI Chameleon Team. Chameleon is a Vision-Language Model that use vector quantization to tokenize images which enables the model to generate multimodal output. The model takes images and texts as input, including an interleaved format, and generates textual response. Image generation module is not released yet.\n+The Chameleon model was proposed in [Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://huggingface.co/papers/2405.09818) by META AI Chameleon Team. Chameleon is a Vision-Language Model that use vector quantization to tokenize images which enables the model to generate multimodal output. The model takes images and texts as input, including an interleaved format, and generates textual response. Image generation module is not released yet.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "400fdc70a7e6efc32dbc3dda04a40da509861ef3",
            "filename": "docs/source/en/model_doc/clvp.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fclvp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fclvp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclvp.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -39,7 +39,7 @@ The original code can be found [here](https://github.com/neonbjb/tortoise-tts).\n 3. The use of the [`ClvpModelForConditionalGeneration.generate()`] method is strongly recommended for tortoise usage.\n 4. Note that the CLVP model expects the audio to be sampled at 22.05 kHz contrary to other audio models which expects 16 kHz.\n \n-## Brief Explanation:\n+## Brief Explanation\n \n - The [`ClvpTokenizer`] tokenizes the text input, and the [`ClvpFeatureExtractor`] extracts the log mel-spectrogram from the desired audio.\n - [`ClvpConditioningEncoder`] takes those text tokens and audio representations and converts them into embeddings conditioned on the text and audio."
        },
        {
            "sha": "18b384fbdc12a09df354a5ddcbfe7c465e61ef6e",
            "filename": "docs/source/en/model_doc/deepseek_v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v2.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -28,6 +28,7 @@ This model was contributed by [VladOS95-cyber](https://github.com/VladOS95-cyber\n The original code can be found [here](https://huggingface.co/deepseek-ai/DeepSeek-V2).\n \n ### Usage tips\n+\n The model uses Multi-head Latent Attention (MLA) and DeepSeekMoE architectures for efficient inference and cost-effective training. It employs an auxiliary-loss-free strategy for load balancing and multi-token prediction training objective. The model can be used for various language tasks after being pre-trained on 14.8 trillion tokens and going through Supervised Fine-Tuning and Reinforcement Learning stages.\n \n ## DeepseekV2Config"
        },
        {
            "sha": "b902481afee32545e01c66c1e6ab681279d22bf6",
            "filename": "docs/source/en/model_doc/deepseek_v3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -34,6 +34,7 @@ We are super happy to make this code community-powered, and would love to see ho\n - static cache is not supported (this should be just a generation config issue / config shape issues)\n \n ### Usage tips\n+\n The model uses Multi-head Latent Attention (MLA) and DeepSeekMoE architectures for efficient inference and cost-effective training. It employs an auxiliary-loss-free strategy for load balancing and multi-token prediction training objective. The model can be used for various language tasks after being pre-trained on 14.8 trillion tokens and going through Supervised Fine-Tuning and Reinforcement Learning stages.\n \n You can run the model in `FP8` automatically, using 2 nodes of 8 H100 should be more than enough!"
        },
        {
            "sha": "792857b23f6cb04345842b6cb26fa1a4cd5363bb",
            "filename": "docs/source/en/model_doc/detr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -142,7 +142,7 @@ As a summary, consider the following table:\n |------|------------------|-----------------------|-----------------------|\n | **Description** | Predicting bounding boxes and class labels around objects in an image | Predicting masks around objects (i.e. instances) in an image | Predicting masks around both objects (i.e. instances) as well as \"stuff\" (i.e. background things like trees and roads) in an image |\n | **Model** | [`~transformers.DetrForObjectDetection`] | [`~transformers.DetrForSegmentation`] | [`~transformers.DetrForSegmentation`] |\n-| **Example dataset** | COCO detection | COCO detection, COCO panoptic | COCO panoptic  |                                                                        |\n+| **Example dataset** | COCO detection | COCO detection, COCO panoptic | COCO panoptic                           |\n | **Format of annotations to provide to**  [`~transformers.DetrImageProcessor`] | {'image_id': `int`, 'annotations': `list[Dict]`} each Dict being a COCO object annotation  | {'image_id': `int`, 'annotations': `list[Dict]`}  (in case of COCO detection) or {'file_name': `str`, 'image_id': `int`, 'segments_info': `list[Dict]`} (in case of COCO panoptic) | {'file_name': `str`, 'image_id': `int`, 'segments_info': `list[Dict]`} and masks_path (path to directory containing PNG files of the masks) |\n | **Postprocessing** (i.e. converting the output of the model to Pascal VOC format) | [`~transformers.DetrImageProcessor.post_process`] | [`~transformers.DetrImageProcessor.post_process_segmentation`] | [`~transformers.DetrImageProcessor.post_process_segmentation`], [`~transformers.DetrImageProcessor.post_process_panoptic`] |\n | **evaluators** | `CocoEvaluator` with `iou_types=\"bbox\"` | `CocoEvaluator` with `iou_types=\"bbox\"` or `\"segm\"` | `CocoEvaluator` with `iou_tupes=\"bbox\"` or `\"segm\"`, `PanopticEvaluator` |"
        },
        {
            "sha": "b6015c160bb5bc800fb5961c72116bd66f727bf4",
            "filename": "docs/source/en/model_doc/diffllama.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fdiffllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fdiffllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdiffllama.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -33,6 +33,7 @@ The abstract from the paper is the following:\n *Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.*\n \n ### Usage tips\n+\n The hyperparameters of this model is the same as Llama model.\n \n ## DiffLlamaConfig"
        },
        {
            "sha": "8f58374e445357b5e3f3b4ead0519940905bff3b",
            "filename": "docs/source/en/model_doc/dinat.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinat.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -47,7 +47,7 @@ Our large model is faster and ahead of its Swin counterpart by 1.5% box AP in CO\n Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.2 PQ)\n and ADE20K (48.5 PQ), and instance segmentation model on Cityscapes (44.5 AP) and ADE20K (35.4 AP) (no extra data).\n It also matches the state of the art specialized semantic segmentation models on ADE20K (58.2 mIoU),\n-and ranks second on Cityscapes (84.5 mIoU) (no extra data). *\n+and ranks second on Cityscapes (84.5 mIoU) (no extra data).*\n \n <img\n src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/dilated-neighborhood-attention-pattern.jpg\""
        },
        {
            "sha": "c06ed32ceaf8d5a386f4e50ec45fa058ae278e7a",
            "filename": "docs/source/en/model_doc/dinov3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -182,4 +182,4 @@ print(\"Pooled output shape:\", pooled_output.shape)\n ## DINOv3ConvNextBackbone\n \n [[autodoc]] DINOv3ConvNextBackbone\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "f99c715bb69d6715b423af96853905dc8e42bc72",
            "filename": "docs/source/en/model_doc/fastspeech2_conformer.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Ffastspeech2_conformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Ffastspeech2_conformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffastspeech2_conformer.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -28,15 +28,19 @@ The abstract from the original FastSpeech2 paper is the following:\n This model was contributed by [Connor Henderson](https://huggingface.co/connor-henderson). The original code can be found [here](https://github.com/espnet/espnet/blob/master/espnet2/tts/fastspeech2/fastspeech2.py).\n \n ## ü§ó Model Architecture\n+\n FastSpeech2's general structure with a Mel-spectrogram decoder was implemented, and the traditional transformer blocks were replaced with conformer blocks as done in the ESPnet library.\n \n #### FastSpeech2 Model Architecture\n+\n ![FastSpeech2 Model Architecture](https://www.microsoft.com/en-us/research/uploads/prod/2021/04/fastspeech2-1.png)\n \n #### Conformer Blocks\n+\n ![Conformer Blocks](https://www.researchgate.net/profile/Hirofumi-Inaguma-2/publication/344911155/figure/fig2/AS:951455406108673@1603856054097/An-overview-of-Conformer-block.png)\n \n #### Convolution Module\n+\n ![Convolution Module](https://d3i71xaburhd42.cloudfront.net/8809d0732f6147d4ad9218c8f9b20227c837a746/2-Figure1-1.png)\n \n ## ü§ó Transformers Usage"
        },
        {
            "sha": "b777fdd32014a66076bf19e1439a255a67092c9c",
            "filename": "docs/source/en/model_doc/gpt_neox.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -101,6 +101,7 @@ Below is an expected speedup diagram that compares pure inference time between t\n </div>\n \n ## Using Scaled Dot Product Attention (SDPA)\n+\n PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n [official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n@@ -123,6 +124,7 @@ On a local benchmark (rtx3080ti-16GB, PyTorch 2.2.1, OS Ubuntu 22.04) using `flo\n following speedups during training and inference.\n \n ### Training\n+\n | Batch size |    Seq len | Time per batch (Eager - s) |    Time per batch (SDPA - s) | Speedup (%) | Eager peak mem (MB) | SDPA peak mem (MB) |    Mem saving (%) |\n |-----------:|-----------:|---------------------------:|-----------------------------:|------------:|--------------------:|-------------------:|------------------:|\n |          1 |        128 |                      0.024 |                        0.019 |      28.945 |             1789.95 |            1789.95 |                 0 |\n@@ -142,6 +144,7 @@ following speedups during training and inference.\n |          4 |       2048 |                        OOM |                        0.731 |           / |                 OOM |            12705.1 | SDPA does not OOM |\n \n ### Inference\n+\n |    Batch size |      Seq len |    Per token latency Eager (ms) |    Per token latency SDPA (ms) |    Speedup (%) |    Mem Eager (MB) |   Mem SDPA (MB) |    Mem saved (%) |\n |--------------:|-------------:|--------------------------------:|-------------------------------:|---------------:|------------------:|----------------:|-----------------:|\n |             1 |          128 |                           6.569 |                          5.858 |          12.14 |           974.831 |         974.826 |                0 |"
        },
        {
            "sha": "1cd54daef6e9d1820f9117d78a072eb5b9c15dd4",
            "filename": "docs/source/en/model_doc/gpt_neox_japanese.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -41,7 +41,7 @@ The example below demonstrates how to generate text with [`Pipeline`] or the [`A\n <hfoptions id=\"usage\">\n <hfoption id=\"Pipeline\">\n \n-```py\n+```python\n import torch\n from transformers import pipeline\n pipeline = pipeline(task=\"text-generation\", \n@@ -52,7 +52,7 @@ pipeline(\"‰∫∫„Å®AI„ÅåÂçîË™ø„Åô„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅ\")\n </hfoption>\n <hfoption id=\"AutoModel\">\n \n-```py\n+```python\n import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n@@ -112,6 +112,7 @@ visualizer(\"<img>What is shown in this image?\")\n </div>\n \n ## Resources\n+\n Refer to the [Training a better GPT model: Learnings from PaLM](https://medium.com/ml-abeja/training-a-better-gpt-2-93b157662ae4) blog post for more details about how ABEJA trained GPT-NeoX-Japanese.\n \n ## GPTNeoXJapaneseConfig"
        },
        {
            "sha": "420665d3e7e8edcf119ffdacd83d06045b6d9f8c",
            "filename": "docs/source/en/model_doc/gptsan-japanese.md",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -79,6 +79,8 @@ When token_type_ids=None or all zero, it is equivalent to regular causal mask\n for example:\n \n >>> x_token = tokenizer(\"ÔΩ±ÔΩ≤ÔΩ≥ÔΩ¥\")\n+\n+```text\n input_ids:      | SOT | SEG | ÔΩ± | ÔΩ≤ | ÔΩ≥ | ÔΩ¥ |\n token_type_ids: | 1   | 0   | 0 | 0 | 0 | 0 |\n prefix_lm_mask:\n@@ -88,8 +90,11 @@ SEG | 1 1 0 0 0 0 |\n ÔΩ≤   | 1 1 1 1 0 0 |\n ÔΩ≥   | 1 1 1 1 1 0 |\n ÔΩ¥   | 1 1 1 1 1 1 |\n+```\n \n >>> x_token = tokenizer(\"\", prefix_text=\"ÔΩ±ÔΩ≤ÔΩ≥ÔΩ¥\")\n+\n+```text\n input_ids:      | SOT | ÔΩ± | ÔΩ≤ | ÔΩ≥ | ÔΩ¥ | SEG |\n token_type_ids: | 1   | 1 | 1 | 1 | 1 | 0  |\n prefix_lm_mask:\n@@ -99,8 +104,11 @@ SOT | 1 1 1 1 1 0 |\n ÔΩ≥   | 1 1 1 1 1 0 |\n ÔΩ¥   | 1 1 1 1 1 0 |\n SEG | 1 1 1 1 1 1 |\n+```\n \n >>> x_token = tokenizer(\"ÔΩ≥ÔΩ¥\", prefix_text=\"ÔΩ±ÔΩ≤\")\n+\n+```text\n input_ids:      | SOT | ÔΩ± | ÔΩ≤ | SEG | ÔΩ≥ | ÔΩ¥ |\n token_type_ids: | 1   | 1 | 1 | 0   | 0 | 0 |\n prefix_lm_mask:\n@@ -110,6 +118,7 @@ SOT | 1 1 1 0 0 0 |\n SEG | 1 1 1 1 0 0 |\n ÔΩ≥   | 1 1 1 1 1 0 |\n ÔΩ¥   | 1 1 1 1 1 1 |\n+```\n \n ### Spout Vector\n "
        },
        {
            "sha": "5001cadff7f2af9144722cac0b817f2ee490772b",
            "filename": "docs/source/en/model_doc/granite_speech.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -22,6 +22,7 @@ rendered properly in your Markdown viewer.\n </div>\n \n ## Overview\n+\n The [Granite Speech](https://huggingface.co/papers/2505.08699) model ([blog post](https://www.ibm.com/new/announcements/ibm-granite-3-3-speech-recognition-refined-reasoning-rag-loras)) is a multimodal language model, consisting of a speech encoder, speech projector, large language model, and LoRA adapter(s). More details regarding each component for the current (Granite 3.2 Speech) model architecture may be found below.\n \n 1. Speech Encoder: A [Conformer](https://huggingface.co/papers/2005.08100) encoder trained with Connectionist Temporal Classification (CTC) on character-level targets on ASR corpora. The encoder uses block-attention and self-conditioned CTC from the middle layer."
        },
        {
            "sha": "38bd9755b7839cc5ee9dcdd5cc91621484efeb8d",
            "filename": "docs/source/en/model_doc/helium.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fhelium.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fhelium.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhelium.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -39,14 +39,14 @@ It supports the following languages: English, French, German, Italian, Portugues\n \n <!-- This section describes the evaluation protocols and provides the results. -->\n \n-#### Testing Data\n+### Testing Data\n \n <!-- This should link to a Dataset Card if possible. -->\n \n The model was evaluated on MMLU, TriviaQA, NaturalQuestions, ARC Easy & Challenge, Open Book QA, Common Sense QA,\n Physical Interaction QA, Social Interaction QA, HellaSwag, WinoGrande, Multilingual Knowledge QA, FLORES 200.\n \n-#### Metrics\n+### Metrics\n \n <!-- These are the evaluation metrics being used, ideally with a description of why. -->\n "
        },
        {
            "sha": "48c9153a019ddb8fe65390c77aeb4887bcc63b90",
            "filename": "docs/source/en/model_doc/idefics.md",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -24,9 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The IDEFICS model was proposed in [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents\n-](https://huggingface.co/papers/2306.16527\n-) by Hugo Lauren√ßon, Lucile Saulnier, L√©o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, Victor Sanh\n+The IDEFICS model was proposed in [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents](https://huggingface.co/papers/2306.16527) by Hugo Lauren√ßon, Lucile Saulnier, L√©o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, Victor Sanh\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "858f5de9b5b83cc0dfea24910694db62d484911f",
            "filename": "docs/source/en/model_doc/idefics2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -215,13 +215,16 @@ A list of official Hugging Face and community (indicated by üåé) resources to h\n     - forward\n \n ## Idefics2ImageProcessor\n+\n [[autodoc]] Idefics2ImageProcessor\n     - preprocess\n \n ## Idefics2ImageProcessorFast\n+\n [[autodoc]] Idefics2ImageProcessorFast\n     - preprocess\n \n ## Idefics2Processor\n+\n [[autodoc]] Idefics2Processor\n     - __call__"
        },
        {
            "sha": "2a4d9eb242f8168151d7834363055160c573656e",
            "filename": "docs/source/en/model_doc/idefics3.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics3.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -77,13 +77,16 @@ This model was contributed by [amyeroberts](https://huggingface.co/amyeroberts)\n     - forward\n \n ## Idefics3ImageProcessor\n+\n [[autodoc]] Idefics3ImageProcessor\n     - preprocess\n \n ## Idefics3ImageProcessorFast\n+\n [[autodoc]] Idefics3ImageProcessorFast\n     - preprocess\n \n ## Idefics3Processor\n+\n [[autodoc]] Idefics3Processor\n     - __call__"
        },
        {
            "sha": "7af6fe3b995c586d978c0ab25c8f7dee2b94cd1a",
            "filename": "docs/source/en/model_doc/instructblipvideo.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -79,6 +79,7 @@ The attributes can be obtained from model config, as `model.config.num_query_tok\n     - forward\n \n ## InstructBlipVideoModel\n+\n [[autodoc]] InstructBlipVideoModel\n     - forward\n "
        },
        {
            "sha": "96809b45ce56af8345b73f083c386c65a5003bce",
            "filename": "docs/source/en/model_doc/internvl.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -105,6 +105,7 @@ This example demonstrates how to perform inference on a single image with the In\n ```\n \n ### Text-only generation\n+\n This example shows how to generate text using the InternVL model without providing any image input.\n \n ```python\n@@ -134,6 +135,7 @@ This example shows how to generate text using the InternVL model without providi\n ```\n \n ### Batched image and text inputs\n+\n InternVL models also support batched image and text inputs.\n \n ```python\n@@ -177,6 +179,7 @@ InternVL models also support batched image and text inputs.\n ```\n \n ### Batched multi-image input\n+\n This implementation of the InternVL models supports batched text-images inputs with different number of images for each text.\n \n ```python\n@@ -220,6 +223,7 @@ This implementation of the InternVL models supports batched text-images inputs w\n ```\n \n ### Video input\n+\n InternVL models can also handle video inputs. Here is an example of how to perform inference on a video input using chat templates.\n \n ```python\n@@ -259,6 +263,7 @@ InternVL models can also handle video inputs. Here is an example of how to perfo\n ```\n \n ### Interleaved image and video inputs\n+\n This example showcases how to handle a batch of chat conversations with interleaved image and video inputs using chat template.\n \n ```python"
        },
        {
            "sha": "11acadb547d45ff9a42565f27a7ad39786d8a549",
            "filename": "docs/source/en/model_doc/jukebox.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fjukebox.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fjukebox.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjukebox.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -14,6 +14,7 @@ rendered properly in your Markdown viewer.\n \n -->\n *This model was released on 2020-04-30 and added to Hugging Face Transformers on 2023-06-20.*\n+\n # Jukebox\n \n <div class=\"flex flex-wrap space-x-1\">"
        },
        {
            "sha": "4205ecaa1dd67b4c10a44f761bebe672a44fe537",
            "filename": "docs/source/en/model_doc/kyutai_speech_to_text.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -16,6 +16,7 @@ rendered properly in your Markdown viewer.\n *This model was released on 2025-06-17 and added to Hugging Face Transformers on 2025-06-25.*\n \n # Kyutai Speech-To-Text\n+\n ## Overview\n \n [Kyutai STT](https://kyutai.org/next/stt) is a speech-to-text model architecture based on the [Mimi codec](https://huggingface.co/docs/transformers/en/model_doc/mimi), which encodes audio into discrete tokens in a streaming fashion, and a [Moshi-like](https://huggingface.co/docs/transformers/en/model_doc/moshi) autoregressive decoder. Kyutai's lab has released two model checkpoints:"
        },
        {
            "sha": "4def9c4c4a51dbd3ffc8a00202a4822bd16dfab1",
            "filename": "docs/source/en/model_doc/levit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Flevit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Flevit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flevit.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -36,7 +36,7 @@ in vision transformers. As a result, we propose LeVIT: a hybrid neural network f\n We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of\n application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable\n to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect\n-to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. *\n+to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU.*\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/levit_architecture.png\"\n alt=\"drawing\" width=\"600\"/>"
        },
        {
            "sha": "09e2af47b9656412901820bd4c3f414578ef5158",
            "filename": "docs/source/en/model_doc/llama4.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -436,11 +436,6 @@ model = Llama4ForConditionalGeneration.from_pretrained(\n [[autodoc]] Llama4TextModel\n     - forward\n \n-## Llama4ForCausalLM\n-\n-[[autodoc]] Llama4ForCausalLM\n-    - forward\n-\n ## Llama4VisionModel\n \n [[autodoc]] Llama4VisionModel"
        },
        {
            "sha": "b5d40cf4d4545c85506616da33a5eba01671f51c",
            "filename": "docs/source/en/model_doc/llava_next_video.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -25,8 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The LLaVa-NeXT-Video model was proposed in [LLaVA-NeXT: A Strong Zero-shot Video Understanding Model\n-](https://llava-vl.github.io/blog/2024-04-30-llava-next-video/) by Yuanhan Zhang, Bo Li, Haotian Liu, Yong Jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, Chunyuan Li. LLaVa-NeXT-Video improves upon [LLaVa-NeXT](llava_next) by fine-tuning on a mix if video and image dataset thus increasing the model's performance on videos.\n+The LLaVa-NeXT-Video model was proposed in [LLaVA-NeXT: A Strong Zero-shot Video Understanding Model](https://llava-vl.github.io/blog/2024-04-30-llava-next-video/) by Yuanhan Zhang, Bo Li, Haotian Liu, Yong Jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, Chunyuan Li. LLaVa-NeXT-Video improves upon [LLaVa-NeXT](llava_next) by fine-tuning on a mix if video and image dataset thus increasing the model's performance on videos.\n \n [LLaVA-NeXT](llava_next) surprisingly has strong performance in understanding video content in zero-shot fashion with the AnyRes technique that it uses. The AnyRes technique naturally represents a high-resolution image into multiple images. This technique is naturally generalizable to represent videos because videos can be considered as a set of frames (similar to a set of images in LLaVa-NeXT). The current version of LLaVA-NeXT makes use of AnyRes and trains with supervised fine-tuning (SFT) on top of LLaVA-Next on video data to achieves better video understanding capabilities.The model is a current SOTA among open-source models on [VideoMME bench](https://huggingface.co/papers/2405.21075).\n "
        },
        {
            "sha": "23f4a49784daec697993c72d48e3642ed02ec2a9",
            "filename": "docs/source/en/model_doc/m2m_100.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -171,6 +171,7 @@ Below is an expected speedup diagram that compares pure inference time between t\n </div>\n \n ## Using Scaled Dot Product Attention (SDPA)\n+\n PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n [official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)"
        },
        {
            "sha": "4f1b5e1befb9e0634c1cc7b003365b3e41602f8c",
            "filename": "docs/source/en/model_doc/mega.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fmega.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fmega.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmega.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -39,7 +39,7 @@ attractive option for long-document NLP tasks.\n \n The abstract from the paper is the following:\n \n- *The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models. *\n+ *The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.*\n \n This model was contributed by [mnaylor](https://huggingface.co/mnaylor).\n The original code can be found [here](https://github.com/facebookresearch/mega)."
        },
        {
            "sha": "bd98761bfed193f693101a3f1352ade68cad3e46",
            "filename": "docs/source/en/model_doc/minimax.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -186,5 +186,6 @@ A list of official Hugging Face and community (indicated by üåé) resources to h\n     - forward\n \n ## MiniMaxForQuestionAnswering\n+\n [[autodoc]] MiniMaxForQuestionAnswering\n     - forward"
        },
        {
            "sha": "f247a20f656fb6285bc4196943b72d45e97c9196",
            "filename": "docs/source/en/model_doc/mixtral.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -223,5 +223,6 @@ A list of official Hugging Face and community (indicated by üåé) resources to h\n     - forward\n \n ## MixtralForQuestionAnswering\n+\n [[autodoc]] MixtralForQuestionAnswering\n     - forward"
        },
        {
            "sha": "745ae4c33aff8fcdbfdcfd05be09c54bd00dcf84",
            "filename": "docs/source/en/model_doc/mllama.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -136,11 +136,6 @@ print(processor.decode(output[0], skip_special_tokens=True))\n \n [[autodoc]] MllamaModel\n \n-## MllamaForCausalLM\n-\n-[[autodoc]] MllamaForCausalLM\n-    - forward\n-\n ## MllamaVisionModel\n \n [[autodoc]] MllamaVisionModel"
        },
        {
            "sha": "8716ed3ba31adc00aeec4b97b1020887fe1bfa48",
            "filename": "docs/source/en/model_doc/mms.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fmms.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fmms.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmms.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -316,6 +316,7 @@ with torch.no_grad():\n Different LID models are available based on the number of languages they can recognize - [126](https://huggingface.co/facebook/mms-lid-126), [256](https://huggingface.co/facebook/mms-lid-256), [512](https://huggingface.co/facebook/mms-lid-512), [1024](https://huggingface.co/facebook/mms-lid-1024), [2048](https://huggingface.co/facebook/mms-lid-2048), [4017](https://huggingface.co/facebook/mms-lid-4017).\n \n #### Inference\n+\n First, we install transformers and some other libraries\n \n ```bash"
        },
        {
            "sha": "b0956120a49997f503c3c5e873a091c16cd1d727",
            "filename": "docs/source/en/model_doc/moshi.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoshi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoshi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoshi.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -64,11 +64,11 @@ Note that each timestamp - i.e each codebook - gets its own set of Linear Layers\n \n It's the audio encoder from Kyutai, that has recently been integrated to transformers, which is used to \"tokenize\" audio. It has the same use that [`~EncodecModel`] has in [`~MusicgenModel`].\n \n-## Tips:\n+## Tips\n \n The original checkpoints can be converted using the conversion script `src/transformers/models/moshi/convert_moshi_transformers.py`\n \n-### How to use the model:\n+### How to use the model\n \n This implementation has two main aims:\n \n@@ -152,7 +152,7 @@ Once it's done, you can simply forward `text_labels` and `audio_labels` to [`Mos\n \n A training guide will come soon, but user contributions are welcomed!\n \n-### How does the model forward the inputs / generate:\n+### How does the model forward the inputs / generate\n \n 1. The input streams are embedded and combined into `inputs_embeds`.\n "
        },
        {
            "sha": "cd0c669cf577060362284e43c7096e7e8a9934b1",
            "filename": "docs/source/en/model_doc/musicgen_melody.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -50,7 +50,7 @@ MusicGen Melody is compatible with two generation modes: greedy and sampling. In\n \n Transformers supports both mono (1-channel) and stereo (2-channel) variants of MusicGen Melody. The mono channel versions generate a single set of codebooks. The stereo versions generate 2 sets of codebooks, 1 for each channel (left/right), and each set of codebooks is decoded independently through the audio compression model. The audio streams for each channel are combined to give the final stereo output.\n \n-#### Audio Conditional Generation\n+### Audio Conditional Generation\n \n The model can generate an audio sample conditioned on a text and an audio prompt through use of the [`MusicgenMelodyProcessor`] to pre-process the inputs.\n "
        },
        {
            "sha": "27588223d00199c25f745eab19d937745df3cce0",
            "filename": "docs/source/en/model_doc/myt5.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fmyt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fmyt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmyt5.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -40,7 +40,3 @@ The original code can be found [here](https://github.com/tomlimi/MYTE).\n     - get_special_tokens_mask\n     - create_token_type_ids_from_sequences\n     - save_vocabulary\n-\n-## MyT5Tokenizer\n-\n-[[autodoc]] MyT5Tokenizer"
        },
        {
            "sha": "d87633d4346eb6f0119dc0bbe51216c03ce62f74",
            "filename": "docs/source/en/model_doc/nat.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fnat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fnat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnat.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -47,7 +47,7 @@ with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster tha\n memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA\n that boosts image classification and downstream vision performance. Experimental results on NAT are competitive;\n NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on MS-COCO and 48.4% mIoU on ADE20K, which is 1.9%\n-ImageNet accuracy, 1.0% COCO mAP, and 2.6% ADE20K mIoU improvement over a Swin model with similar size. *\n+ImageNet accuracy, 1.0% COCO mAP, and 2.6% ADE20K mIoU improvement over a Swin model with similar size.*\n \n <img\n src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/neighborhood-attention-pattern.jpg\""
        },
        {
            "sha": "5ccc4ff02e82ea2fc2666449fcb3c2e04129a28b",
            "filename": "docs/source/en/model_doc/nemotron.md",
            "status": "modified",
            "additions": 6,
            "deletions": 9,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -21,21 +21,22 @@ specific language governing permissions and limitations under the License.\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n-### License\n+## License\n \n+Minitron is released under the [NVIDIA Open Model License Agreement](https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf).\n The use of this model is governed by the [NVIDIA AI Foundation Models Community License Agreement](https://developer.nvidia.com/downloads/nv-ai-foundation-models-license).\n \n-### Description\n+## Description\n \n Nemotron-4 is a family of enterprise ready generative text models compatible with [NVIDIA NeMo Framework](https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/).\n \n NVIDIA NeMo is an end-to-end, cloud-native platform to build, customize, and deploy generative AI models anywhere. It includes training and inferencing frameworks, guardrailing toolkits, data curation tools, and pretrained models, offering enterprises an easy, cost-effective, and fast way to adopt generative AI. To get access to NeMo Framework, please sign up at [this link](https://developer.nvidia.com/nemo-framework/join).\n \n-### References\n+## References\n \n [Announcement Blog](https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/)\n \n-### Model Architecture\n+## Model Architecture\n \n **Architecture Type:** Transformer\n \n@@ -80,10 +81,6 @@ output_text = tokenizer.decode(outputs[0])\n print(output_text)\n ```\n \n-### License\n-\n-Minitron is released under the [NVIDIA Open Model License Agreement](https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf).\n-\n ### Evaluation Results\n \n *5-shot performance.* Language Understanding evaluated using [Massive Multitask Language Understanding](https://huggingface.co/papers/2009.03300):\n@@ -96,7 +93,7 @@ Minitron is released under the [NVIDIA Open Model License Agreement](https://dev\n \n | HellaSwag | Winogrande | GSM8K| ARC-C | XLSum |\n | :------------- | :------------- | :------------- | :------------- | :------------- |\n-| 75.0 | 74.0 | 24.1  | 50.9 | 29.5\n+| 75.0 | 74.0 | 24.1  | 50.9 | 29.5 |\n \n *Code generation performance*. Evaluated using [HumanEval](https://github.com/openai/human-eval):\n "
        },
        {
            "sha": "6567556b9206b5e051a7aa273845f8222db1d78d",
            "filename": "docs/source/en/model_doc/olmo.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -25,6 +25,7 @@ rendered properly in your Markdown viewer.\n </div>\n \n # OLMo\n+\n [OLMo](https://huggingface.co/papers/2402.00838) is a 7B-parameter dense language model. It uses SwiGLU activations, non-parametric layer normalization, rotary positional embeddings, and a BPE tokenizer that masks personally identifiable information. It is pretrained on [Dolma](https://huggingface.co/datasets/allenai/dolma), a 3T-token dataset. OLMo was released to provide complete transparency of not just the model weights but the training data, training code, and evaluation code to enable more research on language models.\n \n You can find all the original OLMo checkpoints under the [OLMo](https://huggingface.co/collections/allenai/olmo-suite-65aeaae8fe5b6b2122b46778) collection."
        },
        {
            "sha": "0e60b4d7cafe4bb609e0ce3e003ef1fe56610c8f",
            "filename": "docs/source/en/model_doc/olmo2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -24,6 +24,7 @@ rendered properly in your Markdown viewer.\n </div>\n \n # OLMo2\n+\n [OLMo2](https://huggingface.co/papers/2501.00656) improves on [OLMo](./olmo) by changing the architecture and training recipes of the original models. This includes excluding all biases to improve training stability, non-parametric layer norm, SwiGLU activation function, rotary positional embeddings, and a modified BPE-based tokenizer that masks personal identifiable information. It is pretrained on [Dolma](https://huggingface.co/datasets/allenai/dolma), a dataset of 3T tokens.\n \n You can find all the original OLMo2 checkpoints under the [OLMo2](https://huggingface.co/collections/allenai/olmo-2-674117b93ab84e98afc72edc) collection."
        },
        {
            "sha": "ee8cae88af87916d5d980a8a6dd08b0cdc1b172e",
            "filename": "docs/source/en/model_doc/olmo3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo3.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n </div>\n \n # OLMo3\n+\n Olmo3 is an improvement on [OLMo2](./olmo2). More details will be released on *soon*.\n \n > [!TIP]"
        },
        {
            "sha": "3c8a9a84e204d28a0f424a69566a3070b5b35455",
            "filename": "docs/source/en/model_doc/pvt_v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt_v2.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -32,7 +32,7 @@ Another powerful feature of the PVTv2 is the complexity reduction in the self-at\n \n SRA was introduced in PVT, and is the default attention complexity reduction method used in PVTv2. However, PVTv2 also introduced the option of using a self-attention mechanism with linear complexity related to image size, which they called \"Linear SRA\". This method uses average pooling to reduce the hidden states to a fixed size that is invariant to their original resolution (although this is inherently more lossy than regular SRA). This option can be enabled by setting `linear_attention` to `True` in the PVTv2Config.\n \n-### Abstract from the paper:\n+### Abstract from the paper\n \n *Transformer recently has presented encouraging progress in computer vision. In this work, we present new baselines by improving the original Pyramid Vision Transformer (PVT v1) by adding three designs, including (1) linear complexity attention layer, (2) overlapping patch embedding, and (3) convolutional feed-forward network. With these modifications, PVT v2 reduces the computational complexity of PVT v1 to linear and achieves significant improvements on fundamental vision tasks such as classification, detection, and segmentation. Notably, the proposed PVT v2 achieves comparable or better performances than recent works such as Swin Transformer. We hope this work will facilitate state-of-the-art Transformer researches in computer vision. Code is available at https://github.com/whai362/PVT.*\n "
        },
        {
            "sha": "ea52a19b39db395cf293b2474438fadce618f38f",
            "filename": "docs/source/en/model_doc/qwen2_audio.md",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -34,7 +34,7 @@ It was proposed in [Qwen2-Audio Technical Report](https://huggingface.co/papers/\n \n The abstract from the paper is the following:\n \n-*We introduce the latest progress of Qwen-Audio, a large-scale audio-language model called Qwen2-Audio, which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions. In contrast to complex hierarchical tags, we have simplified the pre-training process by utilizing natural language prompts for different data and tasks, and have further expanded the data volume. We have boosted the instruction-following capability of Qwen2-Audio and implemented two distinct audio interaction modes for voice chat and audio analysis. In the voice chat mode, users can freely engage in voice interactions with Qwen2-Audio without text input. In the audio analysis mode, users could provide audio and text instructions for analysis during the interaction. Note that we do not use any system prompts to switch between voice chat and audio analysis modes. Qwen2-Audio is capable of intelligently comprehending the content within audio and following voice commands to respond appropriately. For instance, in an audio segment that simultaneously contains sounds, multi-speaker conversations, and a voice command, Qwen2-Audio can directly understand the command and provide an interpretation and response to the audio. Additionally, DPO has optimized the model's performance in terms of factuality and adherence to desired behavior. According to the evaluation results from AIR-Bench, Qwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests focused on audio-centric instruction-following capabilities. Qwen2-Audio is open-sourced with the aim of fostering the advancement of the multi-modal language community. *\n+*We introduce the latest progress of Qwen-Audio, a large-scale audio-language model called Qwen2-Audio, which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions. In contrast to complex hierarchical tags, we have simplified the pre-training process by utilizing natural language prompts for different data and tasks, and have further expanded the data volume. We have boosted the instruction-following capability of Qwen2-Audio and implemented two distinct audio interaction modes for voice chat and audio analysis. In the voice chat mode, users can freely engage in voice interactions with Qwen2-Audio without text input. In the audio analysis mode, users could provide audio and text instructions for analysis during the interaction. Note that we do not use any system prompts to switch between voice chat and audio analysis modes. Qwen2-Audio is capable of intelligently comprehending the content within audio and following voice commands to respond appropriately. For instance, in an audio segment that simultaneously contains sounds, multi-speaker conversations, and a voice command, Qwen2-Audio can directly understand the command and provide an interpretation and response to the audio. Additionally, DPO has optimized the model's performance in terms of factuality and adherence to desired behavior. According to the evaluation results from AIR-Bench, Qwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests focused on audio-centric instruction-following capabilities. Qwen2-Audio is open-sourced with the aim of fostering the advancement of the multi-modal language community.*\n \n ## Usage tips\n \n@@ -74,6 +74,7 @@ response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_\n In the following, we demonstrate how to use `Qwen2-Audio-7B-Instruct` for the inference, supporting both voice chat and audio analysis modes. Note that we have used the ChatML format for dialog, in this demo we show how to leverage `apply_chat_template` for this purpose.\n \n ### Voice Chat Inference\n+\n In the voice chat mode, users can freely engage in voice interactions with Qwen2-Audio without text input:\n \n ```python\n@@ -115,6 +116,7 @@ response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_\n ```\n \n ### Audio Analysis Inference\n+\n In the audio analysis, users could provide both audio and text instructions for analysis:\n \n ```python\n@@ -164,6 +166,7 @@ response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_\n ```\n \n ### Batch Inference\n+\n We also support batch inference:\n \n ```python"
        },
        {
            "sha": "b6ef27d88740a67954c45b0206ddda6b71bec5df",
            "filename": "docs/source/en/model_doc/smolvlm.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -24,6 +24,7 @@ rendered properly in your Markdown viewer.\n </div>\n \n ## Overview\n+\n [SmolVLM2](https://huggingface.co/papers/2504.05299) ([blog post](https://huggingface.co/blog/smolvlm2)) is an adaptation of the Idefics3 model with two main differences:\n \n - It uses SmolLM2 for the text model.\n@@ -193,17 +194,21 @@ print(generated_texts[0])\n     - forward\n \n ## SmolVLMImageProcessor\n+\n [[autodoc]] SmolVLMImageProcessor\n     - preprocess\n \n ## SmolVLMImageProcessorFast\n+\n [[autodoc]] SmolVLMImageProcessorFast\n     - preprocess\n \n ## SmolVLMVideoProcessor\n+\n [[autodoc]] SmolVLMVideoProcessor\n     - preprocess\n \n ## SmolVLMProcessor\n+\n [[autodoc]] SmolVLMProcessor\n     - __call__"
        },
        {
            "sha": "d9e5b4fb88f6ca972a4350da2717a2a5b95ed0ec",
            "filename": "docs/source/en/model_doc/speech-encoder-decoder.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech-encoder-decoder.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech-encoder-decoder.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech-encoder-decoder.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -33,7 +33,7 @@ Alexis Conneau.\n \n An example of how to use a [`SpeechEncoderDecoderModel`] for inference can be seen in [Speech2Text2](speech_to_text_2).\n \n-## Randomly initializing `SpeechEncoderDecoderModel` from model configurations.\n+## Randomly initializing `SpeechEncoderDecoderModel` from model configurations\n \n [`SpeechEncoderDecoderModel`] can be randomly initialized from an encoder and a decoder config. In the following example, we show how to do this using the default [`Wav2Vec2Model`] configuration for the encoder\n and the default [`BertForCausalLM`] configuration for the decoder.\n@@ -48,7 +48,7 @@ and the default [`BertForCausalLM`] configuration for the decoder.\n >>> model = SpeechEncoderDecoderModel(config=config)\n ```\n \n-## Initialising `SpeechEncoderDecoderModel` from a pretrained encoder and a pretrained decoder.\n+## Initialising `SpeechEncoderDecoderModel` from a pretrained encoder and a pretrained decoder\n \n [`SpeechEncoderDecoderModel`] can be initialized from a pretrained encoder checkpoint and a pretrained decoder checkpoint. Note that any pretrained Transformer-based speech model, *e.g.* [Wav2Vec2](wav2vec2), [Hubert](hubert) can serve as the encoder and both pretrained auto-encoding models, *e.g.* BERT, pretrained causal language models, *e.g.* GPT2, as well as the pretrained decoder part of sequence-to-sequence models, *e.g.* decoder of BART, can be used as the decoder.\n Depending on which architecture you choose as the decoder, the cross-attention layers might be randomly initialized.\n@@ -63,7 +63,7 @@ To do so, the `SpeechEncoderDecoderModel` class provides a [`SpeechEncoderDecode\n ... )\n ```\n \n-## Loading an existing `SpeechEncoderDecoderModel` checkpoint and perform inference.\n+## Loading an existing `SpeechEncoderDecoderModel` checkpoint and perform inference\n \n To load fine-tuned checkpoints of the `SpeechEncoderDecoderModel` class, [`SpeechEncoderDecoderModel`] provides the `from_pretrained(...)` method just like any other model architecture in Transformers.\n "
        },
        {
            "sha": "0ca00752d797d685996ca89ab51f2fedac41c5ec",
            "filename": "docs/source/en/model_doc/starcoder2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fstarcoder2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fstarcoder2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fstarcoder2.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -31,6 +31,7 @@ StarCoder2 is a family of open LLMs for code and comes in 3 different sizes with\n The abstract of the paper is the following:\n \n > The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.\n+>\n ## License\n \n The models are licensed under the [BigCode OpenRAIL-M v1 license agreement](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement)."
        },
        {
            "sha": "0813224a1fea29eb912a93e3b6fafda887581cdb",
            "filename": "docs/source/en/model_doc/tapas.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapas.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapas.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapas.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -335,29 +335,36 @@ In case of a conversational set-up, then each table-question pair must be provid\n - [Masked language modeling task guide](../tasks/masked_language_modeling)\n \n ## TAPAS specific outputs\n+\n [[autodoc]] models.tapas.modeling_tapas.TableQuestionAnsweringOutput\n \n ## TapasConfig\n+\n [[autodoc]] TapasConfig\n \n ## TapasTokenizer\n+\n [[autodoc]] TapasTokenizer\n     - __call__\n     - convert_logits_to_predictions\n     - save_vocabulary\n \n ## TapasModel\n+\n [[autodoc]] TapasModel\n     - forward\n \n ## TapasForMaskedLM\n+\n [[autodoc]] TapasForMaskedLM\n     - forward\n \n ## TapasForSequenceClassification\n+\n [[autodoc]] TapasForSequenceClassification\n     - forward\n \n ## TapasForQuestionAnswering\n+\n [[autodoc]] TapasForQuestionAnswering\n     - forward"
        },
        {
            "sha": "c6e508b9ffe1fc0258062d3e12fd0980834af349",
            "filename": "docs/source/en/model_doc/timm_wrapper.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimm_wrapper.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimm_wrapper.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimm_wrapper.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -51,7 +51,7 @@ Helper class to enable loading timm models to be used with the transformers libr\n >>> top5_probabilities, top5_class_indices = torch.topk(logits.softmax(dim=1) * 100, k=5)\n ```\n \n-## Resources:\n+## Resources\n \n A list of official Hugging Face and community (indicated by üåé) resources to help you get started with TimmWrapper.\n "
        },
        {
            "sha": "ab94ef0bda2a89f3e8d9621fb0819d28e770d01d",
            "filename": "docs/source/en/model_doc/umt5.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fumt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fumt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fumt5.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -47,6 +47,7 @@ Therefore, this model has to be fine-tuned before it is usable on a downstream t\n fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.\n \n ## Differences with mT5?\n+\n `UmT5` is based on mT5, with a non-shared relative positional bias that is computed for each layer. This means that the model set `has_relative_bias` for each layer.\n The conversion script is also different because the model was saved in t5x's latest checkpointing format.\n "
        },
        {
            "sha": "761eafb3a77dd171d76c350ce80078c09cd2c7e3",
            "filename": "docs/source/en/model_doc/xmod.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fxmod.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fxmod.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxmod.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -76,6 +76,7 @@ output = model(input_ids, lang_ids=lang_ids)\n ```\n \n ### Fine-tuning\n+\n The paper recommends that the embedding layer and the language adapters are frozen during fine-tuning. A method for doing this is provided:\n \n ```python\n@@ -84,6 +85,7 @@ model.freeze_embeddings_and_language_adapters()\n ```\n \n ### Cross-lingual transfer\n+\n After fine-tuning, zero-shot cross-lingual transfer can be tested by activating the language adapter of the target language:\n \n ```python"
        },
        {
            "sha": "f731b99e405ec3a2f3a525cb9808159c1f668481",
            "filename": "docs/source/en/model_doc/zamba.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -14,6 +14,7 @@ rendered properly in your Markdown viewer.\n \n -->\n *This model was released on 2024-04-16 and added to Hugging Face Transformers on 2024-10-04.*\n+\n # Zamba\n \n <div class=\"flex flex-wrap space-x-1\">\n@@ -73,6 +74,7 @@ The model cards can be found at:\n * [Zamba-7B](https://huggingface.co/Zyphra/Zamba-7B-v1)\n \n ## Issues\n+\n For issues with model output, or community discussion, please use the Hugging Face community [forum](https://huggingface.co/Zyphra/Zamba-7B-v1/discussions)\n \n ## License"
        },
        {
            "sha": "32a6b8982bf13c7f7a1e130b7d6ab67275d18509",
            "filename": "docs/source/en/model_doc/zamba2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba2.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -14,6 +14,7 @@ rendered properly in your Markdown viewer.\n \n -->\n *This model was released on 2024-11-22 and added to Hugging Face Transformers on 2025-01-27.*\n+\n # Zamba2\n \n <div class=\"flex flex-wrap space-x-1\">\n@@ -67,6 +68,7 @@ The model cards can be found at:\n * [Zamba2-7B](https://huggingface.co/Zyphra/Zamba2-7B)\n \n ## Issues\n+\n For issues with model output, or community discussion, please use the Hugging Face community [forum](https://huggingface.co/Zyphra/Zamba2-7B/discussions)\n \n ## License"
        },
        {
            "sha": "2be37eb68c661f29963e952ef9c4a4d8ac13661d",
            "filename": "docs/source/en/tiny_agents.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Ftiny_agents.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/981370c038e98f4a4c720aa6b56e5cbde8266c74/docs%2Fsource%2Fen%2Ftiny_agents.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftiny_agents.md?ref=981370c038e98f4a4c720aa6b56e5cbde8266c74",
            "patch": "@@ -1,4 +1,4 @@\n-### `tiny-agents` CLI and MCP Tools\n+# `tiny-agents` CLI and MCP Tools\n \n To showcase the use of MCP tools, let's see how to integrate the `transformers serve` server with the [`tiny-agents`](https://huggingface.co/blog/python-tiny-agents) CLI.\n "
        }
    ],
    "stats": {
        "total": 158,
        "additions": 102,
        "deletions": 56
    }
}