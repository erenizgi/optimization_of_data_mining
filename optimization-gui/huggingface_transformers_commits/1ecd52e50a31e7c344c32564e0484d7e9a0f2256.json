{
    "author": "lhoestq",
    "message": "Add torchcodec in docstrings/tests for `datasets` 4.0 (#39156)\n\n* fix dataset run_object_detection\n\n* bump version\n\n* keep same dataset actually\n\n* torchcodec in docstrings and testing utils\n\n* torchcodec in dockerfiles and requirements\n\n* remove duplicate\n\n* add torchocodec to all the remaining docker files\n\n* fix tests\n\n* support torchcodec in audio classification and ASR\n\n* [commit to revert] build ci-dev images\n\n* [commit to revert] trigger circleci\n\n* [commit to revert] build ci-dev images\n\n* fix\n\n* fix modeling_hubert\n\n* backward compatible run_object_detection\n\n* revert ci trigger commits\n\n* fix mono conversion and support torch tensor as input\n\n* revert map_to_array docs + fix it\n\n* revert mono\n\n* nit in docstring\n\n* style\n\n* fix modular\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
    "files": [
        {
            "sha": "2509c1f05b85f50277045b11df5ee2683317db0b",
            "filename": "docker/examples-torch.dockerfile",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docker%2Fexamples-torch.dockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docker%2Fexamples-torch.dockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Fexamples-torch.dockerfile?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -2,10 +2,10 @@ FROM python:3.9-slim\n ENV PYTHONDONTWRITEBYTECODE=1\n ARG REF=main\n USER root\n-RUN apt-get update &&  apt-get install -y --no-install-recommends libsndfile1-dev espeak-ng time git g++ cmake pkg-config openssh-client git\n+RUN apt-get update &&  apt-get install -y --no-install-recommends libsndfile1-dev espeak-ng time git g++ cmake pkg-config openssh-client git ffmpeg\n ENV UV_PYTHON=/usr/local/bin/python\n RUN pip --no-cache-dir install uv && uv venv && uv pip install --no-cache-dir -U pip setuptools\n-RUN uv pip install --no-cache-dir 'torch' 'torchaudio' 'torchvision' --index-url https://download.pytorch.org/whl/cpu\n+RUN uv pip install --no-cache-dir 'torch' 'torchaudio' 'torchvision' 'torchcodec' --index-url https://download.pytorch.org/whl/cpu\n RUN uv pip install --no-deps timm accelerate --extra-index-url https://download.pytorch.org/whl/cpu\n RUN uv pip install --no-cache-dir librosa \"git+https://github.com/huggingface/transformers.git@${REF}#egg=transformers[sklearn,sentencepiece,vision,testing]\" seqeval albumentations jiwer\n RUN uv pip uninstall transformers"
        },
        {
            "sha": "dd37683ecf15942fac585dfbf142a20d07eea162",
            "filename": "docker/pipeline-torch.dockerfile",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docker%2Fpipeline-torch.dockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docker%2Fpipeline-torch.dockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Fpipeline-torch.dockerfile?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -2,10 +2,10 @@ FROM python:3.9-slim\n ENV PYTHONDONTWRITEBYTECODE=1\n ARG REF=main\n USER root\n-RUN apt-get update &&  apt-get install -y --no-install-recommends libsndfile1-dev espeak-ng time git pkg-config openssh-client git\n+RUN apt-get update &&  apt-get install -y --no-install-recommends libsndfile1-dev espeak-ng time git pkg-config openssh-client git ffmpeg\n ENV UV_PYTHON=/usr/local/bin/python\n RUN pip --no-cache-dir install uv && uv venv && uv pip install --no-cache-dir -U pip setuptools\n-RUN uv pip install --no-cache-dir 'torch' 'torchaudio' 'torchvision' --index-url https://download.pytorch.org/whl/cpu\n+RUN uv pip install --no-cache-dir 'torch' 'torchaudio' 'torchvision' 'torchcodec' --index-url https://download.pytorch.org/whl/cpu\n RUN uv pip install --no-deps timm accelerate --extra-index-url https://download.pytorch.org/whl/cpu\n RUN uv pip install --no-cache-dir librosa \"git+https://github.com/huggingface/transformers.git@${REF}#egg=transformers[sklearn,sentencepiece,vision,testing]\"\n RUN uv pip uninstall transformers"
        },
        {
            "sha": "b4e6cdffb39a478875941dd05bd7220a94a7a64f",
            "filename": "docker/torch-light.dockerfile",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docker%2Ftorch-light.dockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docker%2Ftorch-light.dockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftorch-light.dockerfile?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -2,10 +2,10 @@ FROM python:3.9-slim\n ENV PYTHONDONTWRITEBYTECODE=1\n ARG REF=main\n USER root\n-RUN apt-get update &&  apt-get install -y --no-install-recommends libsndfile1-dev espeak-ng time git g++ cmake pkg-config openssh-client git git-lfs\n+RUN apt-get update &&  apt-get install -y --no-install-recommends libsndfile1-dev espeak-ng time git g++ cmake pkg-config openssh-client git git-lfs ffmpeg\n ENV UV_PYTHON=/usr/local/bin/python\n RUN pip --no-cache-dir install uv && uv venv && uv pip install --no-cache-dir -U pip setuptools\n-RUN uv pip install --no-cache-dir 'torch' 'torchaudio' 'torchvision' --index-url https://download.pytorch.org/whl/cpu\n+RUN uv pip install --no-cache-dir 'torch' 'torchaudio' 'torchvision' 'torchcodec' --index-url https://download.pytorch.org/whl/cpu\n RUN uv pip install --no-deps timm accelerate --extra-index-url https://download.pytorch.org/whl/cpu\n RUN uv pip install --no-cache-dir librosa \"git+https://github.com/huggingface/transformers.git@${REF}#egg=transformers[sklearn,sentencepiece,vision,testing,tiktoken,num2words,video]\"\n RUN uv pip uninstall transformers"
        },
        {
            "sha": "b6f9986b816dbe49f03e9d0e74b4bd35b9324a84",
            "filename": "docker/transformers-all-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docker%2Ftransformers-all-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docker%2Ftransformers-all-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-all-latest-gpu%2FDockerfile?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -26,7 +26,7 @@ RUN git clone https://github.com/huggingface/transformers && cd transformers &&\n # 1. Put several commands in a single `RUN` to avoid image/layer exporting issue. Could be revised in the future.\n # 2. Regarding `torch` part, We might need to specify proper versions for `torchvision` and `torchaudio`.\n #    Currently, let's not bother to specify their versions explicitly (so installed with their latest release versions).\n-RUN python3 -m pip install --no-cache-dir -e ./transformers[dev,onnxruntime] && [ ${#PYTORCH} -gt 0 -a \"$PYTORCH\" != \"pre\" ] && VERSION='torch=='$PYTORCH'.*' ||  VERSION='torch'; echo \"export VERSION='$VERSION'\" >> ~/.profile && echo torch=$VERSION && [ \"$PYTORCH\" != \"pre\" ] && python3 -m pip install --no-cache-dir -U $VERSION torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/$CUDA || python3 -m pip install --no-cache-dir -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/$CUDA && python3 -m pip uninstall -y tensorflow tensorflow_text tensorflow_probability\n+RUN python3 -m pip install --no-cache-dir -e ./transformers[dev,onnxruntime] && [ ${#PYTORCH} -gt 0 -a \"$PYTORCH\" != \"pre\" ] && VERSION='torch=='$PYTORCH'.*' ||  VERSION='torch'; echo \"export VERSION='$VERSION'\" >> ~/.profile && echo torch=$VERSION && [ \"$PYTORCH\" != \"pre\" ] && python3 -m pip install --no-cache-dir -U $VERSION torchvision torchaudio torchcodec --extra-index-url https://download.pytorch.org/whl/$CUDA || python3 -m pip install --no-cache-dir -U --pre torch torchvision torchaudio torchcodec --extra-index-url https://download.pytorch.org/whl/nightly/$CUDA && python3 -m pip uninstall -y tensorflow tensorflow_text tensorflow_probability\n \n RUN python3 -m pip uninstall -y flax jax\n "
        },
        {
            "sha": "b58435087d7817891372895254ff44c907859354",
            "filename": "docker/transformers-pytorch-deepspeed-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docker%2Ftransformers-pytorch-deepspeed-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docker%2Ftransformers-pytorch-deepspeed-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-pytorch-deepspeed-latest-gpu%2FDockerfile?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -21,7 +21,7 @@ RUN python3 -m pip install --no-cache-dir './transformers[deepspeed-testing]' 'p\n # Install latest release PyTorch\n # (PyTorch must be installed before pre-compiling any DeepSpeed c++/cuda ops.)\n # (https://www.deepspeed.ai/tutorials/advanced-install/#pre-install-deepspeed-ops)\n-RUN python3 -m pip uninstall -y torch torchvision torchaudio && python3 -m pip install --no-cache-dir -U torch==$PYTORCH torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/$CUDA\n+RUN python3 -m pip uninstall -y torch torchvision torchaudio && python3 -m pip install --no-cache-dir -U torch==$PYTORCH torchvision torchaudio torchcodec --extra-index-url https://download.pytorch.org/whl/$CUDA\n \n RUN python3 -m pip install --no-cache-dir git+https://github.com/huggingface/accelerate@main#egg=accelerate\n "
        },
        {
            "sha": "3a8ca977354f7561cb4e5aa3aeba418c78cbe98f",
            "filename": "docker/transformers-pytorch-deepspeed-nightly-gpu/Dockerfile",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docker%2Ftransformers-pytorch-deepspeed-nightly-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docker%2Ftransformers-pytorch-deepspeed-nightly-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-pytorch-deepspeed-nightly-gpu%2FDockerfile?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -19,7 +19,7 @@ RUN python3 -m pip uninstall -y torch torchvision torchaudio\n # Install **nightly** release PyTorch (flag `--pre`)\n # (PyTorch must be installed before pre-compiling any DeepSpeed c++/cuda ops.)\n # (https://www.deepspeed.ai/tutorials/advanced-install/#pre-install-deepspeed-ops)\n-RUN python3 -m pip install --no-cache-dir -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/$CUDA\n+RUN python3 -m pip install --no-cache-dir -U --pre torch torchvision torchaudio torchcodec --extra-index-url https://download.pytorch.org/whl/nightly/$CUDA\n \n # `datasets` requires pandas, pandas has some modules compiled with numpy=1.x causing errors\n RUN python3 -m pip install --no-cache-dir './transformers[deepspeed-testing]' 'pandas<2' 'numpy<2'"
        },
        {
            "sha": "cfc04780162eaef5f42cda38d9bfbdb7cd21137e",
            "filename": "docker/transformers-quantization-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -26,7 +26,7 @@ RUN [ ${#PYTORCH} -gt 0 ] && VERSION='torch=='$PYTORCH'.*' ||  VERSION='torch';\n RUN echo torch=$VERSION\n # `torchvision` and `torchaudio` should be installed along with `torch`, especially for nightly build.\n # Currently, let's just use their latest releases (when `torch` is installed with a release version)\n-RUN python3 -m pip install --no-cache-dir -U $VERSION torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/$CUDA\n+RUN python3 -m pip install --no-cache-dir -U $VERSION torchvision torchaudio torchcodec --extra-index-url https://download.pytorch.org/whl/$CUDA\n \n RUN python3 -m pip install --no-cache-dir git+https://github.com/huggingface/accelerate@main#egg=accelerate\n "
        },
        {
            "sha": "6d77e5ad39323927b4a32b4ad64fb6fd3419d011",
            "filename": "docs/source/en/model_doc/speech_to_text_2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text_2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text_2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text_2.md?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -61,19 +61,16 @@ predicted token ids.\n - Step-by-step Speech Translation\n \n ```python\n->>> import torch\n >>> from transformers import Speech2Text2Processor, SpeechEncoderDecoderModel\n >>> from datasets import load_dataset\n->>> import soundfile as sf\n \n >>> model = SpeechEncoderDecoderModel.from_pretrained(\"facebook/s2t-wav2vec2-large-en-de\")\n >>> processor = Speech2Text2Processor.from_pretrained(\"facebook/s2t-wav2vec2-large-en-de\")\n \n \n->>> def map_to_array(batch):\n-...     speech, _ = sf.read(batch[\"file\"])\n-...     batch[\"speech\"] = speech\n-...     return batch\n+>>> def map_to_array(example):\n+...     example[\"speech\"] = example[\"audio\"][\"array\"]\n+...     return example\n \n \n >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
        },
        {
            "sha": "340ac4b1938ef7d40ef1b9a1bf4bb35e3a4d2f2e",
            "filename": "docs/source/en/model_doc/wav2vec2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -172,9 +172,9 @@ Otherwise, [`~Wav2Vec2ProcessorWithLM.batch_decode`] performance will be slower\n >>> dataset = dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\n \n \n->>> def map_to_array(batch):\n-...     batch[\"speech\"] = batch[\"audio\"][\"array\"]\n-...     return batch\n+>>> def map_to_array(example):\n+...     example[\"speech\"] = example[\"audio\"][\"array\"]\n+...     return example\n \n \n >>> # prepare speech data for batch inference"
        },
        {
            "sha": "d66ba08f157253c69c06e67219a2acf46ed57db7",
            "filename": "examples/pytorch/_tests_requirements.txt",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/examples%2Fpytorch%2F_tests_requirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/examples%2Fpytorch%2F_tests_requirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2F_tests_requirements.txt?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -22,6 +22,7 @@ protobuf\n torch\n torchvision\n torchaudio\n+torchcodec\n jiwer\n librosa\n evaluate >= 0.2.0"
        },
        {
            "sha": "8a9ba6ddcd1669e55d9de0755c1689c5351d0849",
            "filename": "examples/pytorch/object-detection/requirements.txt",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/examples%2Fpytorch%2Fobject-detection%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/examples%2Fpytorch%2Fobject-detection%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fobject-detection%2Frequirements.txt?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -1,5 +1,5 @@\n albumentations >= 1.4.16\n timm\n-datasets\n+datasets>=4.0\n torchmetrics\n pycocotools"
        },
        {
            "sha": "f534d9a3236cabbd4b334f42fee51392b269bfa2",
            "filename": "examples/pytorch/object-detection/run_object_detection.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -399,7 +399,10 @@ def main():\n         dataset[\"validation\"] = split[\"test\"]\n \n     # Get dataset categories and prepare mappings for label_name <-> label_id\n-    categories = dataset[\"train\"].features[\"objects\"].feature[\"category\"].names\n+    if isinstance(dataset[\"train\"].features[\"objects\"], dict):\n+        categories = dataset[\"train\"].features[\"objects\"][\"category\"].feature.names\n+    else:  # (for old versions of `datasets` that used Sequence({...}) of the objects)\n+        categories = dataset[\"train\"].features[\"objects\"].feature[\"category\"].names\n     id2label = dict(enumerate(categories))\n     label2id = {v: k for k, v in id2label.items()}\n "
        },
        {
            "sha": "1133435d7be2e798e14035cb0dad7e477776c84f",
            "filename": "examples/pytorch/object-detection/run_object_detection_no_trainer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -460,7 +460,10 @@ def main():\n         dataset[\"validation\"] = split[\"test\"]\n \n     # Get dataset categories and prepare mappings for label_name <-> label_id\n-    categories = dataset[\"train\"].features[\"objects\"].feature[\"category\"].names\n+    if isinstance(dataset[\"train\"].features[\"objects\"], dict):\n+        categories = dataset[\"train\"].features[\"objects\"][\"category\"].feature.names\n+    else:  # (for old versions of `datasets` that used Sequence({...}) of the objects)\n+        categories = dataset[\"train\"].features[\"objects\"].feature[\"category\"].names\n     id2label = dict(enumerate(categories))\n     label2id = {v: k for k, v in id2label.items()}\n "
        },
        {
            "sha": "37215d4486d2e72714e492e76a54bc33979d23a5",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -435,10 +435,12 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, max_length, num_mel_bins)`):\n             Float values mel features extracted from the raw audio waveform. Raw audio waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via\n-            the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n-            [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n-            tensor of type `torch.FloatTensor`. See [`~ASTFeatureExtractor.__call__`]\n+            loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a\n+            `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or the soundfile library\n+            (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the\n+            mel features, padding and conversion into a tensor of type `torch.FloatTensor`.\n+            See [`~ASTFeatureExtractor.__call__`]\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -525,10 +527,11 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, max_length, num_mel_bins)`):\n             Float values mel features extracted from the raw audio waveform. Raw audio waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via\n-            the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n-            [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n-            tensor of type `torch.FloatTensor`. See [`~ASTFeatureExtractor.__call__`]\n+            loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via\n+            the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the\n+            mel features, padding and conversion into a tensor of type `torch.FloatTensor`.\n+            See [`~ASTFeatureExtractor.__call__`]\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the audio classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "a40c21932f83bea4f888a5d596783f2d5dbafe0f",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -1653,14 +1653,15 @@ def get_speech_features(\n         >>> text = \"This is an example text.\"\n         >>> ds = datasets.load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         >>> ds = ds.cast_column(\"audio\", datasets.Audio(sampling_rate=22050))\n-        >>> _, audio, sr = ds.sort(\"id\").select(range(1))[:1][\"audio\"][0].values()\n+        >>> audio = ds.sort(\"id\")[\"audio\"][0]\n+        >>> audio_sample, sr = audio[\"array\"], audio[\"sampling_rate\"]\n \n         >>> # Define processor and model\n         >>> processor = ClvpProcessor.from_pretrained(\"susnato/clvp_dev\")\n         >>> model = ClvpModelForConditionalGeneration.from_pretrained(\"susnato/clvp_dev\")\n \n         >>> # Generate processor output and model output\n-        >>> processor_output = processor(raw_speech=audio, sampling_rate=sr, text=text, return_tensors=\"pt\")\n+        >>> processor_output = processor(raw_speech=audio_sample, sampling_rate=sr, text=text, return_tensors=\"pt\")\n         >>> speech_embeds = model.get_speech_features(\n         ...     input_ids=processor_output[\"input_ids\"], input_features=processor_output[\"input_features\"]\n         ... )\n@@ -1732,14 +1733,15 @@ def forward(\n \n         >>> ds = datasets.load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         >>> ds = ds.cast_column(\"audio\", datasets.Audio(sampling_rate=22050))\n-        >>> _, audio, sr = ds.sort(\"id\").select(range(1))[:1][\"audio\"][0].values()\n+        >>> audio = ds.sort(\"id\")[\"audio\"][0]\n+        >>> audio_sample, sr = audio[\"array\"], audio[\"sampling_rate\"]\n \n         >>> # Define processor and model\n         >>> processor = ClvpProcessor.from_pretrained(\"susnato/clvp_dev\")\n         >>> model = ClvpModelForConditionalGeneration.from_pretrained(\"susnato/clvp_dev\")\n \n         >>> # processor outputs and model outputs\n-        >>> processor_output = processor(raw_speech=audio, sampling_rate=sr, text=text, return_tensors=\"pt\")\n+        >>> processor_output = processor(raw_speech=audio_sample, sampling_rate=sr, text=text, return_tensors=\"pt\")\n         >>> outputs = model(\n         ...     input_ids=processor_output[\"input_ids\"],\n         ...     input_features=processor_output[\"input_features\"],"
        },
        {
            "sha": "d140c407106f6a2ba1d94f8106d93448dfb58b6f",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -1022,9 +1022,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Data2VecAudioProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Data2VecAudioProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1136,9 +1137,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Data2VecAudioProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Data2VecAudioProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1318,9 +1320,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Data2VecAudioProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Data2VecAudioProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "4b36a575b8261aba9c297a30b04b6c0631a8751b",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -954,16 +954,14 @@ def forward(\n         ```python\n         >>> from transformers import AutoProcessor, HubertModel\n         >>> from datasets import load_dataset\n-        >>> import soundfile as sf\n \n         >>> processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n         >>> model = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n \n \n-        >>> def map_to_array(batch):\n-        ...     speech, _ = sf.read(batch[\"file\"])\n-        ...     batch[\"speech\"] = speech\n-        ...     return batch\n+        >>> def map_to_array(example):\n+        ...     example[\"speech\"] = example[\"audio\"][\"array\"]\n+        ...     return example\n \n \n         >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n@@ -1230,9 +1228,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`HubertProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`HubertProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "a0de497dc0067e87f5f22c5c7efd21575bf6547b",
            "filename": "src/transformers/models/hubert/modeling_tf_hubert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 10,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -1459,16 +1459,14 @@ def call(\n         ```python\n         >>> from transformers import AutoProcessor, TFHubertModel\n         >>> from datasets import load_dataset\n-        >>> import soundfile as sf\n \n         >>> processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n         >>> model = TFHubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n \n \n-        >>> def map_to_array(batch):\n-        ...     speech, _ = sf.read(batch[\"file\"])\n-        ...     batch[\"speech\"] = speech\n-        ...     return batch\n+        >>> def map_to_array(example):\n+        ...     example[\"speech\"] = example[\"audio\"][\"array\"]\n+        ...     return example\n \n \n         >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n@@ -1571,16 +1569,14 @@ def call(\n         >>> import tensorflow as tf\n         >>> from transformers import AutoProcessor, TFHubertForCTC\n         >>> from datasets import load_dataset\n-        >>> import soundfile as sf\n \n         >>> processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n         >>> model = TFHubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n \n \n-        >>> def map_to_array(batch):\n-        ...     speech, _ = sf.read(batch[\"file\"])\n-        ...     batch[\"speech\"] = speech\n-        ...     return batch\n+        >>> def map_to_array(example):\n+        ...     example[\"speech\"] = example[\"audio\"][\"array\"]\n+        ...     return example\n \n \n         >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
        },
        {
            "sha": "29f186f811fc218741e44d9496bbfdd6a57933ad",
            "filename": "src/transformers/models/hubert/modular_hubert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -239,16 +239,14 @@ def forward(\n         ```python\n         >>> from transformers import AutoProcessor, HubertModel\n         >>> from datasets import load_dataset\n-        >>> import soundfile as sf\n \n         >>> processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n         >>> model = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n \n \n-        >>> def map_to_array(batch):\n-        ...     speech, _ = sf.read(batch[\"file\"])\n-        ...     batch[\"speech\"] = speech\n-        ...     return batch\n+        >>> def map_to_array(example):\n+        ...     example[\"speech\"] = example[\"audio\"][\"array\"]\n+        ...     return example\n \n \n         >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
        },
        {
            "sha": "2605d0f032621d609ff1d442681584235c5bd74f",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -540,8 +540,9 @@ def forward(\n         Args:\n             input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n                 Float values of the raw speech waveform. Raw speech waveform can be\n-                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n-                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n+                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+                the soundfile library (`pip install soundfile`). To prepare the array into\n                 `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n                 and conversion into a tensor of type `torch.FloatTensor`.\n             attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -922,8 +923,9 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n             Float values of the raw speech waveform. Raw speech waveform can be\n-            obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n-            `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+            obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n+            `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+            the soundfile library (`pip install soundfile`). To prepare the array into\n             `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n             and conversion into a tensor of type `torch.FloatTensor`.\n         Example:\n@@ -1039,8 +1041,9 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n             Float values of the raw speech waveform. Raw speech waveform can be\n-            obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n-            `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+            obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n+            `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+            the soundfile library (`pip install soundfile`). To prepare the array into\n             `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n             and conversion into a tensor of type `torch.FloatTensor`.\n "
        },
        {
            "sha": "c8d0fe56b1035d0083f8920cef36d78c5bdcc13b",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -575,8 +575,9 @@ def forward(\n         Args:\n             input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n                 Float values of the raw speech waveform. Raw speech waveform can be\n-                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n-                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n+                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+                the soundfile library (`pip install soundfile`). To prepare the array into\n                 `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n                 and conversion into a tensor of type `torch.FloatTensor`.\n             attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -751,8 +752,9 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n             Float values of the raw speech waveform. Raw speech waveform can be\n-            obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n-            `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+            obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n+            `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+            the soundfile library (`pip install soundfile`). To prepare the array into\n             `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n             and conversion into a tensor of type `torch.FloatTensor`.\n         Example:\n@@ -852,8 +854,9 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n             Float values of the raw speech waveform. Raw speech waveform can be\n-            obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n-            `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+            obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n+            `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+            the soundfile library (`pip install soundfile`). To prepare the array into\n             `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n             and conversion into a tensor of type `torch.FloatTensor`.\n "
        },
        {
            "sha": "3a55c3a35132da19dc4c188377655b5440d4cd6a",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -810,8 +810,9 @@ def forward(\n         r\"\"\"\n         input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):\n             Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n-            obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n-            `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+            obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n+            `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+            the soundfile library (`pip install soundfile`). To prepare the array into\n             `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n             and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n         feature_lens (`torch.LongTensor` of shape `(batch_size,)`):\n@@ -1830,10 +1831,11 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, feature_sequence_length)`):\n             Float values mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via\n-            the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n-            [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n-            tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n+            loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via\n+            the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the\n+            mel features, padding and conversion into a tensor of type `torch.FloatTensor`.\n+            See [`~WhisperFeatureExtractor.__call__`]\n         pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size), *optional*):\n             The tensors corresponding to the input videos. Pixel values can be obtained using\n             [`AutoImageProcessor`]. See [`SiglipImageProcessor.__call__`] for details ([]`NewTaskModelProcessor`] uses"
        },
        {
            "sha": "5ee0f347dd434ecae0b3061068f47ec88fe6d188",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -1795,8 +1795,9 @@ def forward(\n         r\"\"\"\n         input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):\n             Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n-            obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n-            `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+            obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n+            `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+            the soundfile library (`pip install soundfile`). To prepare the array into\n             `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n             and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n         feature_lens (`torch.LongTensor` of shape `(batch_size,)`):\n@@ -2276,10 +2277,11 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, feature_sequence_length)`):\n             Float values mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via\n-            the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n-            [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n-            tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n+            loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via\n+            the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the\n+            mel features, padding and conversion into a tensor of type `torch.FloatTensor`.\n+            See [`~WhisperFeatureExtractor.__call__`]\n         pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size), *optional*):\n             The tensors corresponding to the input videos. Pixel values can be obtained using\n             [`AutoImageProcessor`]. See [`SiglipImageProcessor.__call__`] for details ([]`NewTaskModelProcessor`] uses"
        },
        {
            "sha": "e0652693208c9a4f8ba8c223c8b83d3b00af05d7",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -362,8 +362,9 @@ def forward(\n         Args:\n             input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):\n                 Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n-                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n-                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n+                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+                the soundfile library (`pip install soundfile`). To prepare the array into\n                 `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n                 and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n             attention_mask (`torch.Tensor`)`, *optional*):\n@@ -742,10 +743,11 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, feature_sequence_length)`):\n             Float values mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via\n-            the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n-            [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n-            tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n+            loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via\n+            the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the\n+            mel features, padding and conversion into a tensor of type `torch.FloatTensor`.\n+            See [`~WhisperFeatureExtractor.__call__`]\n         feature_attention_mask (`torch.Tensor` of shape `(batch_size, feature_sequence_length)`):\n             Mask to avoid performing attention on padding feature indices. Mask values selected in `[0, 1]`:\n "
        },
        {
            "sha": "c19571bda21265ebf0e0b4224731cd66ac2e67d7",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -1046,9 +1046,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`SEWProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`SEWProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "678cddde30cd47c1f67f32b3770e65f107397262",
            "filename": "src/transformers/models/sew_d/modeling_sew_d.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -1597,9 +1597,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`SEWDProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`SEWDProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "c809b804c85cc1773a7987f507586b3dceb37a79",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_flax_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_flax_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_flax_speech_encoder_decoder.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -86,8 +86,9 @@\n     Args:\n         inputs (`jnp.ndarray` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, feature_dim)`, *optional*):\n             Float values of input raw speech waveform or speech features. Values can be obtained by loading a `.flac`\n-            or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile\n-            library (`pip install soundfile`). To prepare the array into `inputs`, either the [`Wav2Vec2Processor`] or\n+            or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.*\n+            via the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `inputs`, either the [`Wav2Vec2Processor`] or\n             [`Speech2TextProcessor`] should be used for padding and conversion into a tensor of type\n             `torch.FloatTensor`.\n         attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -128,10 +129,10 @@\n     Args:\n         inputs (`jnp.ndarray` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, feature_dim)`, *optional*):\n             Float values of input raw speech waveform or speech features. Values can be obtained by loading a *.flac*\n-            or *.wav* audio file into an array of type *list[float]* or a *numpy.ndarray*, *e.g.* via the soundfile\n-            library (*pip install soundfile*). To prepare the array into *inputs*, either the [`Wav2Vec2Processor`] or\n-            [`Speech2TextProcessor`] should be used for padding and conversion into a tensor of type\n-            *torch.FloatTensor*.\n+            or *.wav* audio file into an array of type *list[float]* or a *numpy.ndarray*, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into *inputs*, either the [`Wav2Vec2Processor`] or [`Speech2TextProcessor`] should be used\n+            for padding and conversion into a tensor of type *torch.FloatTensor*.\n         attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n "
        },
        {
            "sha": "5894c035b717bb0f14b3ff6123fa8e1b7f2fe085",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -339,8 +339,9 @@ def forward(\n         r\"\"\"\n         inputs (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, feature_dim)`, *optional*):\n             Float values of input raw speech waveform or speech features. Values can be obtained by loading a `.flac`\n-            or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile\n-            library (`pip install soundfile`). To prepare the array into `inputs`, either the [`Wav2Vec2Processor`] or\n+            or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.*\n+            via the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `inputs`, either the [`Wav2Vec2Processor`] or\n             [`Speech2TextProcessor`] should be used for padding and conversion into a tensor of type\n             `torch.FloatTensor`.\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -369,15 +370,17 @@ def forward(\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Float values of input raw speech waveform. Values can be obtained by loading a *.flac* or *.wav* audio file\n-            into an array of type *list[float]* or a *numpy.ndarray*, *e.g.* via the soundfile library (*pip install\n-            soundfile*). To prepare the array into *input_values*, the [`Wav2Vec2Processor`] should be used for padding\n-            and conversion into a tensor of type *torch.FloatTensor*. See [`Wav2Vec2Processor.__call__`] for details.\n+            into an array of type *list[float]* or a *numpy.ndarray*, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into *input_values*, the [`Wav2Vec2Processor`] should be used for padding and conversion\n+            into a tensor of type *torch.FloatTensor*. See [`Wav2Vec2Processor.__call__`] for details.\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, feature_size)`, *optional*):\n             Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be obtained\n-            by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.*\n-            via the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n-            [`Speech2TextFeatureExtractor`] should be used for extracting the fbank features, padding and conversion\n-            into a tensor of type `torch.FloatTensor`. See [`~Speech2TextFeatureExtractor.__call__`]\n+            by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.*\n+            via the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`Speech2TextFeatureExtractor`] should be used for extracting\n+            the fbank features, padding and conversion into a tensor of type `torch.FloatTensor`.\n+            See [`~Speech2TextFeatureExtractor.__call__`]\n \n         Examples:\n "
        },
        {
            "sha": "29debaeaac0377e533df0a09710fb04ca1da7c80",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 15,
            "deletions": 10,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -619,8 +619,9 @@ def forward(\n         Args:\n             input_features (`torch.LongTensor` of shape `(batch_size, sequence_length, feature_size)`):\n                 Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be\n-                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n-                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n+                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+                the soundfile library (`pip install soundfile`). To prepare the array into\n                 `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the fbank features,\n                 padding and conversion into a tensor of type `torch.FloatTensor`. See\n                 [`~Speech2TextFeatureExtractor.__call__`]\n@@ -1096,10 +1097,12 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, feature_size)`):\n             Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be obtained\n-            by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.*\n-            via the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n-            [`AutoFeatureExtractor`] should be used for extracting the fbank features, padding and conversion into a\n-            tensor of type `torch.FloatTensor`. See [`~Speech2TextFeatureExtractor.__call__`]\n+            by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a\n+            `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or the soundfile library\n+            (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting\n+            the fbank features, padding and conversion into a tensor of type `torch.FloatTensor`.\n+            See [`~Speech2TextFeatureExtractor.__call__`]\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n \n@@ -1258,10 +1261,12 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, feature_size)`):\n             Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be obtained\n-            by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.*\n-            via the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n-            [`AutoFeatureExtractor`] should be used for extracting the fbank features, padding and conversion into a\n-            tensor of type `torch.FloatTensor`. See [`~Speech2TextFeatureExtractor.__call__`]\n+            by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a\n+            `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or the soundfile library\n+            (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting\n+            the fbank features, padding and conversion into a tensor of type `torch.FloatTensor`.\n+            See [`~Speech2TextFeatureExtractor.__call__`]\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n "
        },
        {
            "sha": "555fc5659b88a6faf4f87c600553959549141c91",
            "filename": "src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_tf_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_tf_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_tf_speech_to_text.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -691,10 +691,12 @@ def input_signature(self):\n     Args:\n         input_features (`tf.Tensor` of shape `(batch_size, sequence_length, feature_size)`):\n             Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be obtained\n-            by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.*\n-            via the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n-            [`AutoFeatureExtractor`] should be used for extracting the fbank features, padding and conversion into a\n-            tensor of floats. See [`~Speech2TextFeatureExtractor.__call__`]\n+            by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray or a\n+            `torch.Tensor``, *e.g.* via the torchcodec library (`pip install torchcodec`) or the soundfile library\n+            (`pip install soundfile`).\n+            To prepare the arrayinto `input_features`, the [`AutoFeatureExtractor`] should be used for extracting\n+            the fbank features, padding and conversion into a tensor of floats.\n+            See [`~Speech2TextFeatureExtractor.__call__`]\n         attention_mask (`tf.Tensor` of shape `({0})`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n@@ -847,8 +849,9 @@ def call(\n         Args:\n             input_features (`tf.Tensor` of shape `(batch_size, sequence_length, feature_size)`):\n                 Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be\n-                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n-                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n+                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+            the soundfile library (`pip install soundfile`). To prepare the array into\n                 `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the fbank features,\n                 padding and conversion into a tensor of floats. See [`~Speech2TextFeatureExtractor.__call__`]\n             attention_mask (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1469,18 +1472,16 @@ def call(\n         >>> import tensorflow as tf\n         >>> from transformers import Speech2TextProcessor, TFSpeech2TextForConditionalGeneration\n         >>> from datasets import load_dataset\n-        >>> import soundfile as sf\n \n         >>> model = TFSpeech2TextForConditionalGeneration.from_pretrained(\n         ...     \"facebook/s2t-small-librispeech-asr\", from_pt=True\n         ... )\n         >>> processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n \n \n-        >>> def map_to_array(batch):\n-        ...     speech, _ = sf.read(batch[\"file\"])\n-        ...     batch[\"speech\"] = speech\n-        ...     return batch\n+        >>> def map_to_array(example):\n+        ...     example[\"speech\"] = example[\"audio\"][\"array\"]\n+        ...     return example\n \n \n         >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
        },
        {
            "sha": "8f46fd00c8f894da030dd4659c8005c83c395e9c",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -2156,8 +2156,9 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a *.flac* or *.wav* audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (*pip install\n-            soundfile*). To prepare the array into `input_values`, the [`SpeechT5Processor`] should be used for padding\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`SpeechT5Processor`] should be used for padding\n             and conversion into a tensor of type `torch.FloatTensor`. See [`SpeechT5Processor.__call__`] for details.\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -2841,9 +2842,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a *.flac* or *.wav* audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (*pip install\n-            soundfile*). To prepare the array into `input_values`, the [`SpeechT5Processor`] should be used for padding\n-            and conversion into a tensor of type `torch.FloatTensor`. See [`SpeechT5Processor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`SpeechT5Processor`] should be used for padding and conversion into\n+            a tensor of type `torch.FloatTensor`. See [`SpeechT5Processor.__call__`] for details.\n         decoder_input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_mel_bins)`):\n             Float values of input mel spectrogram.\n \n@@ -2966,10 +2968,11 @@ def generate_speech(\n             input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n                 Float values of input raw speech waveform.\n \n-                Values can be obtained by loading a *.flac* or *.wav* audio file into an array of type `list[float]` or\n-                a `numpy.ndarray`, *e.g.* via the soundfile library (*pip install soundfile*). To prepare the array\n-                into `input_values`, the [`SpeechT5Processor`] should be used for padding and conversion into a tensor\n-                of type `torch.FloatTensor`. See [`SpeechT5Processor.__call__`] for details.\n+                Values can be obtained by loading a *.flac* or *.wav* audio file into an array of type `list[float]`,\n+                a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`)\n+                or the soundfile library (`pip install soundfile`).\n+                To prepare the array into `input_values`, the [`SpeechT5Processor`] should be used for padding and\n+                conversion into a tensor of type `torch.FloatTensor`. See [`SpeechT5Processor.__call__`] for details.\n             speaker_embeddings (`torch.FloatTensor` of shape `(batch_size, config.speaker_embedding_dim)`, *optional*):\n                 Tensor containing the speaker embeddings.\n             attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "e982240e264a77b8f45c35cee2aa54b64824cfdb",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -1455,9 +1455,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`UniSpeechProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`UniSpeechProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "18e9d88645451bb33744d01bbb5f7e93698e1ad0",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -1450,9 +1450,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`UniSpeechSatProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`UniSpeechSatProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1564,9 +1565,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`UniSpeechSatProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`UniSpeechSatProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1746,9 +1748,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`UniSpeechSatProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`UniSpeechSatProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "472600afd942d1bcc1fde7a5801c8c8eedbe2947",
            "filename": "src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 18,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_flax_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_flax_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_flax_wav2vec2.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -255,9 +255,10 @@ def _sample_negative_indices(features_shape: tuple, num_negatives: int, attentio\n     Args:\n         input_values (`jnp.ndarray` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `jnp.ndarray`. See [`Wav2Vec2Processor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.*  via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `jnp.ndarray`. See [`Wav2Vec2Processor.__call__`] for details.\n         attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,\n             1]`:\n@@ -1064,16 +1065,14 @@ class FlaxWav2Vec2Model(FlaxWav2Vec2PreTrainedModel):\n     ```python\n     >>> from transformers import AutoProcessor, FlaxWav2Vec2Model\n     >>> from datasets import load_dataset\n-    >>> import soundfile as sf\n \n     >>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-large-lv60\")\n     >>> model = FlaxWav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-lv60\")\n \n \n-    >>> def map_to_array(batch):\n-    ...     speech, _ = sf.read(batch[\"file\"])\n-    ...     batch[\"speech\"] = speech\n-    ...     return batch\n+    >>> def map_to_array(example):\n+    ...     example[\"speech\"] = example[\"audio\"][\"array\"]\n+    ...     return example\n \n \n     >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n@@ -1183,16 +1182,14 @@ class FlaxWav2Vec2ForCTC(FlaxWav2Vec2PreTrainedModel):\n     >>> import jax.numpy as jnp\n     >>> from transformers import AutoProcessor, FlaxWav2Vec2ForCTC\n     >>> from datasets import load_dataset\n-    >>> import soundfile as sf\n \n     >>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60\")\n     >>> model = FlaxWav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h-lv60\")\n \n \n-    >>> def map_to_array(batch):\n-    ...     speech, _ = sf.read(batch[\"file\"])\n-    ...     batch[\"speech\"] = speech\n-    ...     return batch\n+    >>> def map_to_array(example):\n+    ...     example[\"speech\"] = example[\"audio\"][\"array\"]\n+    ...     return example\n \n \n     >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n@@ -1384,16 +1381,14 @@ def __call__(\n     >>> from transformers import AutoFeatureExtractor, FlaxWav2Vec2ForPreTraining\n     >>> from transformers.models.wav2vec2.modeling_flax_wav2vec2 import _compute_mask_indices\n     >>> from datasets import load_dataset\n-    >>> import soundfile as sf\n \n     >>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-lv60\")\n     >>> model = FlaxWav2Vec2ForPreTraining.from_pretrained(\"facebook/wav2vec2-large-lv60\")\n \n \n-    >>> def map_to_array(batch):\n-    ...     speech, _ = sf.read(batch[\"file\"])\n-    ...     batch[\"speech\"] = speech\n-    ...     return batch\n+    >>> def map_to_array(example):\n+    ...     example[\"speech\"] = example[\"audio\"][\"array\"]\n+    ...     return example\n \n \n     >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
        },
        {
            "sha": "d4364c314782a43c76328d09e3bbf88aa0252197",
            "filename": "src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -1530,16 +1530,14 @@ def call(\n         ```python\n         >>> from transformers import AutoProcessor, TFWav2Vec2Model\n         >>> from datasets import load_dataset\n-        >>> import soundfile as sf\n \n         >>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n         >>> model = TFWav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n \n \n-        >>> def map_to_array(batch):\n-        ...     speech, _ = sf.read(batch[\"file\"])\n-        ...     batch[\"speech\"] = speech\n-        ...     return batch\n+        >>> def map_to_array(example):\n+        ...     example[\"speech\"] = example[\"audio\"][\"array\"]\n+        ...     return example\n \n \n         >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n@@ -1642,16 +1640,15 @@ def call(\n         >>> import tensorflow as tf\n         >>> from transformers import AutoProcessor, TFWav2Vec2ForCTC\n         >>> from datasets import load_dataset\n-        >>> import soundfile as sf\n+        >>> from torchcodec.decoders import AudioDecoder\n \n         >>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n         >>> model = TFWav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n \n \n-        >>> def map_to_array(batch):\n-        ...     speech, _ = sf.read(batch[\"file\"])\n-        ...     batch[\"speech\"] = speech\n-        ...     return batch\n+        >>> def map_to_array(example):\n+        ...     example[\"speech\"] = example[\"audio\"][\"array\"]\n+        ...     return example\n \n \n         >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
        },
        {
            "sha": "c12dc4093a62215ca57413e562556853a4d57aae",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -1981,9 +1981,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2Processor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2Processor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -2095,9 +2096,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2Processor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2Processor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -2277,9 +2279,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2Processor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2Processor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "a2be15d8cea6e9eb8525f2b8b7cfc1a8b9ceb09d",
            "filename": "src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 20,
            "deletions": 15,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -997,9 +997,10 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.*  via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Indices to mask extracted features for contrastive loss. When in training mode, model learns to predict\n             masked extracted features in *config.proj_codevector_dim* space.\n@@ -1094,9 +1095,10 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n             Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to\n             the sequence length of the output logits. Indices are selected in `[-100, 0, ..., config.vocab_size - 1]`.\n@@ -1205,9 +1207,10 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1300,9 +1303,10 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1463,9 +1467,10 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "64f4103195b7b30f162d53cfe08acec67e85d2d5",
            "filename": "src/transformers/models/wav2vec2_bert/modular_wav2vec2_bert.py",
            "status": "modified",
            "additions": 20,
            "deletions": 15,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -708,9 +708,10 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.*  via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Indices to mask extracted features for contrastive loss. When in training mode, model learns to predict\n             masked extracted features in *config.proj_codevector_dim* space.\n@@ -779,9 +780,10 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n             Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to\n             the sequence length of the output logits. Indices are selected in `[-100, 0, ..., config.vocab_size - 1]`.\n@@ -875,9 +877,10 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -950,9 +953,10 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1016,9 +1020,10 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "9cc47da5d1d0f9fe36d623477eacd0a57d2a7646",
            "filename": "src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -1579,9 +1579,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1681,9 +1682,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1851,9 +1853,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "867e15e923a8ff9dbea206fab1585b107b0e446a",
            "filename": "src/transformers/models/wavlm/modeling_wavlm.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -1328,9 +1328,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`WavLMProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`WavLMProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1442,9 +1443,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`WavLMProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`WavLMProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1624,9 +1626,10 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type `torch.FloatTensor`. See [`WavLMProcessor.__call__`] for details.\n+            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+            into a tensor of type `torch.FloatTensor`. See [`WavLMProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "1bed6ce27b463c6e3b50f43a1aa17971321b385f",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -429,10 +429,11 @@ def generate(\n         Parameters:\n             input_features (`torch.Tensor` of shape `(batch_size, feature_size, sequence_length)`, *optional*):\n                 Float values of log-mel features extracted from the raw speech waveform. The raw speech waveform can be obtained by\n-                loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via\n-                the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n-                [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n-                tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`] for details.\n+                loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`,\n+                *e.g.*  via the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+                To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel\n+                features, padding and conversion into a tensor of type `torch.FloatTensor`.\n+                See [`~WhisperFeatureExtractor.__call__`] for details.\n             generation_config ([`~generation.GenerationConfig`], *optional*):\n                 The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n                 passed to generate matching the attributes of `generation_config` will override them. If\n@@ -1598,7 +1599,7 @@ def detect_language(\n         Parameters:\n             input_features (`torch.Tensor` of shape `(batch_size, feature_size, sequence_length)`, *optional*):\n                 Float values of log-mel features extracted from the raw speech waveform. The raw speech waveform can be obtained by\n-                loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via\n+                loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via\n                 the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n                 [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n                 tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`] for details."
        },
        {
            "sha": "183fdd58f42cac2e8767c002193dcafd1bb3744c",
            "filename": "src/transformers/models/whisper/modeling_flax_whisper.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_flax_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_flax_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_flax_whisper.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -101,10 +101,12 @@ def sinusoidal_embedding_init(key, shape, dtype=jnp.float_) -> jax.Array:\n     Args:\n         input_features (`numpy.ndarray` of shape `(batch_size, feature_size, sequence_length)`):\n             Float values mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via\n-            the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n-            [`WhisperFeatureExtractor`] should be used for extracting the features, padding and conversion into a\n-            tensor of type `numpy.ndarray`. See [`~WhisperFeatureExtractor.__call__`]\n+            loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a\n+            `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or the soundfile library\n+            (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`WhisperFeatureExtractor`] should be used for extracting\n+            the features, padding and conversion into a tensor of type `numpy.ndarray`.\n+            See [`~WhisperFeatureExtractor.__call__`]\n         attention_mask (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Whisper does not support masking of the `input_features`, this argument is preserved for compatibility, but\n             is not used. By default the silence in the input log mel spectrogram are ignored.\n@@ -138,10 +140,11 @@ def sinusoidal_embedding_init(key, shape, dtype=jnp.float_) -> jax.Array:\n     Args:\n         input_features (`numpy.ndarray` of shape `(batch_size, feature_size, sequence_length)`):\n             Float values mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via\n-            the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n-            [`WhisperFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n-            tensor of type `numpy.ndarray`. See [`~WhisperFeatureExtractor.__call__`].\n+            loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via\n+            the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`WhisperFeatureExtractor`] should be used for extracting\n+            the mel features, padding and conversion into a tensor of type `numpy.ndarray`.\n+            See [`~WhisperFeatureExtractor.__call__`].\n         attention_mask (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Whisper does not support masking of the `input_features`, this argument is preserved for compatibility, but\n             is not used. By default the silence in the input log mel spectrogram are ignored."
        },
        {
            "sha": "32adc6b5af6eaa221f3c3efdf3d787d674c8b74a",
            "filename": "src/transformers/models/whisper/modeling_tf_whisper.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -600,10 +600,12 @@ def input_signature(self):\n     Args:\n         input_features (`tf.Tensor` of shape `(batch_size, feature_size, sequence_length)`):\n             Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be obtained\n-            by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.*\n-            via the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n-            [`AutoFeatureExtractor`] should be used for extracting the fbank features, padding and conversion into a\n-            tensor of type `tf.Tensor`. See [`~WhisperFeatureExtractor.__call__`]\n+            by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a\n+            `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or the soundfile library\n+            (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the\n+            fbank features, padding and conversion into a tensor of type `tf.Tensor`.\n+            See [`~WhisperFeatureExtractor.__call__`]\n         decoder_input_ids (`tf.Tensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n \n@@ -728,8 +730,9 @@ def call(\n         Args:\n             input_features (`tf.Tensor` of shape `(batch_size, feature_size, sequence_length)`):\n                 Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be\n-                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n-                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n+                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+                the soundfile library (`pip install soundfile`). To prepare the array into\n                 `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the fbank features,\n                 padding and conversion into a tensor of type `tf.Tensor`. See [`~WhisperFeatureExtractor.__call__`]\n             head_mask (`tf.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):"
        },
        {
            "sha": "d9bb321582aac60fe4e8d332a0c0fd8248e33522",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 18,
            "deletions": 14,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -650,8 +650,9 @@ def forward(\n         Args:\n             input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):\n                 Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n-                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n-                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n+                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n+                the soundfile library (`pip install soundfile`). To prepare the array into\n                 `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n                 and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n             attention_mask (`torch.Tensor`)`, *optional*):\n@@ -1096,10 +1097,11 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, sequence_length)`):\n             Float values mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via\n-            the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n-            [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n-            tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n+            loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via\n+            the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the\n+            mel features, padding and conversion into a tensor of type `torch.FloatTensor`.\n+            See [`~WhisperFeatureExtractor.__call__`]\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n \n@@ -1266,10 +1268,11 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, sequence_length)`):\n             Float values mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via\n-            the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n-            [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n-            tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n+            loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via\n+            the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the\n+            mel features, padding and conversion into a tensor of type `torch.FloatTensor`.\n+            See [`~WhisperFeatureExtractor.__call__`]\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n \n@@ -1600,10 +1603,11 @@ def forward(\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, sequence_length)`):\n             Float values mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via\n-            the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n-            [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n-            tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n+            loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via\n+            the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the\n+            mel features, padding and conversion into a tensor of type `torch.FloatTensor`.\n+            See [`~WhisperFeatureExtractor.__call__`]\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "5ce133def98b22babc7259a8e5c4492aa84a94b4",
            "filename": "src/transformers/pipelines/audio_classification.py",
            "status": "modified",
            "additions": 21,
            "deletions": 4,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fpipelines%2Faudio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fpipelines%2Faudio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Faudio_classification.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -17,7 +17,7 @@\n import numpy as np\n import requests\n \n-from ..utils import add_end_docstrings, is_torch_available, is_torchaudio_available, logging\n+from ..utils import add_end_docstrings, is_torch_available, is_torchaudio_available, is_torchcodec_available, logging\n from .base import Pipeline, build_pipeline_init_args\n \n \n@@ -174,14 +174,29 @@ def preprocess(self, inputs):\n         if isinstance(inputs, bytes):\n             inputs = ffmpeg_read(inputs, self.feature_extractor.sampling_rate)\n \n+        if is_torch_available():\n+            import torch\n+\n+            if isinstance(inputs, torch.Tensor):\n+                inputs = inputs.cpu().numpy()\n+\n+        if is_torchcodec_available():\n+            import torch\n+            import torchcodec\n+\n+            if isinstance(inputs, torchcodec.decoders.AudioDecoder):\n+                _audio_samples = inputs.get_all_samples()\n+                _array = _audio_samples.data\n+                inputs = {\"array\": _array, \"sampling_rate\": _audio_samples.sample_rate}\n+\n         if isinstance(inputs, dict):\n             inputs = inputs.copy()  # So we don't mutate the original dictionary outside the pipeline\n             # Accepting `\"array\"` which is the key defined in `datasets` for\n             # better integration\n             if not (\"sampling_rate\" in inputs and (\"raw\" in inputs or \"array\" in inputs)):\n                 raise ValueError(\n                     \"When passing a dictionary to AudioClassificationPipeline, the dict needs to contain a \"\n-                    '\"raw\" key containing the numpy array representing the audio and a \"sampling_rate\" key, '\n+                    '\"raw\" key containing the numpy array or torch tensor representing the audio and a \"sampling_rate\" key, '\n                     \"containing the sampling_rate associated with that array\"\n                 )\n \n@@ -204,11 +219,13 @@ def preprocess(self, inputs):\n                     )\n \n                 inputs = F.resample(\n-                    torch.from_numpy(inputs), in_sampling_rate, self.feature_extractor.sampling_rate\n+                    torch.from_numpy(inputs) if isinstance(inputs, np.ndarray) else inputs,\n+                    in_sampling_rate,\n+                    self.feature_extractor.sampling_rate,\n                 ).numpy()\n \n         if not isinstance(inputs, np.ndarray):\n-            raise TypeError(\"We expect a numpy ndarray as input\")\n+            raise TypeError(\"We expect a numpy ndarray or torch tensor as input\")\n         if len(inputs.shape) != 1:\n             raise ValueError(\"We expect a single channel audio input for AudioClassificationPipeline\")\n "
        },
        {
            "sha": "a950ab5ee6a390498e6430fa8a87816e1fa0b36f",
            "filename": "src/transformers/pipelines/automatic_speech_recognition.py",
            "status": "modified",
            "additions": 22,
            "deletions": 4,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -19,7 +19,7 @@\n \n from ..generation import GenerationConfig\n from ..tokenization_utils import PreTrainedTokenizer\n-from ..utils import is_torch_available, is_torchaudio_available, logging\n+from ..utils import is_torch_available, is_torchaudio_available, is_torchcodec_available, logging\n from .audio_utils import ffmpeg_read\n from .base import ChunkPipeline\n \n@@ -364,14 +364,29 @@ def preprocess(self, inputs, chunk_length_s=0, stride_length_s=None):\n \n         stride = None\n         extra = {}\n+\n+        if is_torch_available():\n+            import torch\n+\n+            if isinstance(inputs, torch.Tensor):\n+                inputs = inputs.cpu().numpy()\n+\n+        if is_torchcodec_available():\n+            import torchcodec\n+\n+            if isinstance(inputs, torchcodec.decoders.AudioDecoder):\n+                _audio_samples = inputs.get_all_samples()\n+                _array = _audio_samples.data\n+                inputs = {\"array\": _array, \"sampling_rate\": _audio_samples.sample_rate}\n+\n         if isinstance(inputs, dict):\n             stride = inputs.pop(\"stride\", None)\n             # Accepting `\"array\"` which is the key defined in `datasets` for\n             # better integration\n             if not (\"sampling_rate\" in inputs and (\"raw\" in inputs or \"array\" in inputs)):\n                 raise ValueError(\n                     \"When passing a dictionary to AutomaticSpeechRecognitionPipeline, the dict needs to contain a \"\n-                    '\"raw\" key containing the numpy array representing the audio and a \"sampling_rate\" key, '\n+                    '\"raw\" key containing the numpy array or torch tensor representing the audio and a \"sampling_rate\" key, '\n                     \"containing the sampling_rate associated with that array\"\n                 )\n \n@@ -393,7 +408,10 @@ def preprocess(self, inputs, chunk_length_s=0, stride_length_s=None):\n                     )\n \n                 inputs = F.resample(\n-                    torch.from_numpy(inputs), in_sampling_rate, self.feature_extractor.sampling_rate\n+                    torch.from_numpy(inputs) if isinstance(inputs, np.ndarray) else inputs,\n+                    in_sampling_rate,\n+                    in_sampling_rate,\n+                    self.feature_extractor.sampling_rate,\n                 ).numpy()\n                 ratio = self.feature_extractor.sampling_rate / in_sampling_rate\n             else:\n@@ -408,7 +426,7 @@ def preprocess(self, inputs, chunk_length_s=0, stride_length_s=None):\n                 # of the original length in the stride so we can cut properly.\n                 stride = (inputs.shape[0], int(round(stride[0] * ratio)), int(round(stride[1] * ratio)))\n         if not isinstance(inputs, np.ndarray):\n-            raise TypeError(f\"We expect a numpy ndarray as input, got `{type(inputs)}`\")\n+            raise TypeError(f\"We expect a numpy ndarray or torch tensor as input, got `{type(inputs)}`\")\n         if len(inputs.shape) != 1:\n             raise ValueError(\"We expect a single channel audio input for AutomaticSpeechRecognitionPipeline\")\n "
        },
        {
            "sha": "3dc29371baab3e12a71575ad53f891a6592b54a6",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -130,7 +130,6 @@\n     is_scipy_available,\n     is_sentencepiece_available,\n     is_seqio_available,\n-    is_soundfile_available,\n     is_spacy_available,\n     is_speech_available,\n     is_spqr_available,\n@@ -656,7 +655,7 @@ def require_torchcodec(test_case):\n     These tests are skipped when Torchcodec isn't installed.\n \n     \"\"\"\n-    return unittest.skipUnless(is_torchcodec_available(), \"test requires Torchvision\")(test_case)\n+    return unittest.skipUnless(is_torchcodec_available(), \"test requires Torchcodec\")(test_case)\n \n \n def require_torch_or_tf(test_case):\n@@ -1268,16 +1267,6 @@ def require_clearml(test_case):\n     return unittest.skipUnless(is_clearml_available(), \"test requires clearml\")(test_case)\n \n \n-def require_soundfile(test_case):\n-    \"\"\"\n-    Decorator marking a test that requires soundfile\n-\n-    These tests are skipped when soundfile isn't installed.\n-\n-    \"\"\"\n-    return unittest.skipUnless(is_soundfile_available(), \"test requires soundfile\")(test_case)\n-\n-\n def require_deepspeed(test_case):\n     \"\"\"\n     Decorator marking a test that requires deepspeed"
        },
        {
            "sha": "5028f4687a810ea8c8e7e3ec747015c6c4b4b33e",
            "filename": "src/transformers/utils/args_doc.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Futils%2Fargs_doc.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/src%2Ftransformers%2Futils%2Fargs_doc.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fargs_doc.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -248,9 +248,10 @@ class ModelArgs:\n     input_values = {\n         \"description\": \"\"\"\n     Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-    into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n-    soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n-    conversion into a tensor of type `torch.FloatTensor`. See [`{processor_class}.__call__`] for details.\n+    into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n+    (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n+    To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and conversion\n+    into a tensor of type `torch.FloatTensor`. See [`{processor_class}.__call__`] for details.\n     \"\"\",\n         \"shape\": \"of shape `(batch_size, sequence_length)`\",\n     }"
        },
        {
            "sha": "b0ee0066fb082f026d4f2ad96814b68e9b055e53",
            "filename": "tests/models/audio_spectrogram_transformer/test_feature_extraction_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_feature_extraction_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_feature_extraction_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_feature_extraction_audio_spectrogram_transformer.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -154,7 +154,7 @@ def _load_datasamples(self, num_samples):\n \n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         # automatic decoding with librispeech\n-        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n \n         return [x[\"array\"] for x in speech_samples]\n "
        },
        {
            "sha": "e349e081199f959f955051412dd6447a40195780",
            "filename": "tests/models/clap/test_feature_extraction_clap.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fclap%2Ftest_feature_extraction_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fclap%2Ftest_feature_extraction_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclap%2Ftest_feature_extraction_clap.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -165,7 +165,7 @@ def test_double_precision_pad(self):\n     def _load_datasamples(self, num_samples):\n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         # automatic decoding with librispeech\n-        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n \n         return [x[\"array\"] for x in speech_samples]\n "
        },
        {
            "sha": "409e51000588ac419e19cb532ed609d0aa90bfe4",
            "filename": "tests/models/clvp/test_feature_extraction_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fclvp%2Ftest_feature_extraction_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fclvp%2Ftest_feature_extraction_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_feature_extraction_clvp.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -215,7 +215,7 @@ def _load_datasamples(self, num_samples):\n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         ds = ds.cast_column(\"audio\", Audio(sampling_rate=22050))\n         # automatic decoding with librispeech\n-        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n \n         return [x[\"array\"] for x in speech_samples], [x[\"sampling_rate\"] for x in speech_samples]\n "
        },
        {
            "sha": "a33d787dc7ccd2d602f50b31180f999c4b96158c",
            "filename": "tests/models/clvp/test_modeling_clvp.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -373,10 +373,12 @@ def prepare_config_and_inputs(self):\n \n         ds = datasets.load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         ds = ds.cast_column(\"audio\", datasets.Audio(sampling_rate=22050))\n-        _, audio, sr = ds.sort(\"id\").select(range(1))[:1][\"audio\"][0].values()\n+        audio = ds.sort(\"id\")[0][\"audio\"]\n+        audio_sample = audio[\"array\"]\n+        sr = audio[\"sampling_rate\"]\n \n         feature_extractor = ClvpFeatureExtractor()\n-        input_features = feature_extractor(raw_speech=audio, sampling_rate=sr, return_tensors=\"pt\")[\n+        input_features = feature_extractor(raw_speech=audio_sample, sampling_rate=sr, return_tensors=\"pt\")[\n             \"input_features\"\n         ].to(torch_device)\n \n@@ -562,7 +564,8 @@ def setUp(self):\n         self.text = \"This is an example text.\"\n         ds = datasets.load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         ds = ds.cast_column(\"audio\", datasets.Audio(sampling_rate=22050))\n-        _, self.speech_samples, self.sr = ds.sort(\"id\").select(range(1))[:1][\"audio\"][0].values()\n+        audio = ds.sort(\"id\")[\"audio\"][0]\n+        self.speech_samples, self.sr = audio[\"array\"], audio[\"sampling_rate\"]\n \n         self.model = ClvpModelForConditionalGeneration.from_pretrained(\"susnato/clvp_dev\").to(torch_device)\n         self.model.eval()"
        },
        {
            "sha": "c995485d33114c7de441601864a5f343203e3705",
            "filename": "tests/models/dac/test_feature_extraction_dac.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fdac%2Ftest_feature_extraction_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fdac%2Ftest_feature_extraction_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdac%2Ftest_feature_extraction_dac.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -143,7 +143,7 @@ def _load_datasamples(self, num_samples):\n \n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         # automatic decoding with librispeech\n-        audio_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        audio_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n \n         return [x[\"array\"] for x in audio_samples]\n "
        },
        {
            "sha": "630f6238e76e949fdeb3c34e74168adb6591c251",
            "filename": "tests/models/data2vec/test_modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -21,7 +21,7 @@\n \n from tests.test_modeling_common import floats_tensor, ids_tensor, random_attention_mask\n from transformers import Data2VecAudioConfig, is_torch_available\n-from transformers.testing_utils import require_soundfile, require_torch, slow, torch_device\n+from transformers.testing_utils import require_torch, require_torchcodec, slow, torch_device\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, _config_zero_init\n@@ -656,7 +656,7 @@ def test_compute_mask_indices_short_audio(self):\n \n \n @require_torch\n-@require_soundfile\n+@require_torchcodec\n @slow\n class Data2VecAudioModelIntegrationTest(unittest.TestCase):\n     def _load_datasamples(self, num_samples):"
        },
        {
            "sha": "9a6f797d53465f4f7954162a1ea7b0c725babe53",
            "filename": "tests/models/dia/test_feature_extraction_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fdia%2Ftest_feature_extraction_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fdia%2Ftest_feature_extraction_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_feature_extraction_dia.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -145,7 +145,7 @@ def _load_datasamples(self, num_samples):\n \n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         # automatic decoding with librispeech\n-        audio_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        audio_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n \n         return [x[\"array\"] for x in audio_samples]\n "
        },
        {
            "sha": "447491f90102245ea867d5bf0030594a1e942ffd",
            "filename": "tests/models/dia/test_modeling_dia.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -665,8 +665,12 @@ def test_dia_model_integration_generate_tts(self):\n     @require_torch_accelerator\n     def test_dia_model_integration_generate_audio_context(self):\n         text = [\"[S1] Dia is an open weights text to dialogue model.\", \"This is a test\"]\n-        audio_sample_1 = torchaudio.load(self.audio_prompt_1_path, channels_first=True)[0].squeeze().numpy()\n-        audio_sample_2 = torchaudio.load(self.audio_prompt_2_path, channels_first=True)[0].squeeze().numpy()\n+        audio_sample_1 = (\n+            torchaudio.load(self.audio_prompt_1_path, channels_first=True, backend=\"soundfile\")[0].squeeze().numpy()\n+        )\n+        audio_sample_2 = (\n+            torchaudio.load(self.audio_prompt_2_path, channels_first=True, backend=\"soundfile\")[0].squeeze().numpy()\n+        )\n         audio = [audio_sample_1, audio_sample_2]\n \n         processor = DiaProcessor.from_pretrained(self.model_checkpoint)"
        },
        {
            "sha": "2823b00993724edf0f0b218b296ba6ae0c036bdb",
            "filename": "tests/models/encodec/test_feature_extraction_encodec.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fencodec%2Ftest_feature_extraction_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fencodec%2Ftest_feature_extraction_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencodec%2Ftest_feature_extraction_encodec.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -139,7 +139,7 @@ def _load_datasamples(self, num_samples):\n \n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         # automatic decoding with librispeech\n-        audio_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        audio_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n \n         return [x[\"array\"] for x in audio_samples]\n "
        },
        {
            "sha": "44a862400154a15ce7ce762106e9eff0eb5677b6",
            "filename": "tests/models/granite_speech/test_modeling_granite_speech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -340,7 +340,7 @@ def _get_prompt(self, tokenizer):\n     def _load_datasamples(self, num_samples):\n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         # automatic decoding with librispeech\n-        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n \n         return [x[\"array\"] for x in speech_samples]\n "
        },
        {
            "sha": "904a04e1f9a446464952787f00384474544c3c19",
            "filename": "tests/models/hubert/test_modeling_hubert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -22,7 +22,7 @@\n import pytest\n \n from transformers import HubertConfig, is_torch_available\n-from transformers.testing_utils import require_soundfile, require_torch, slow, torch_device\n+from transformers.testing_utils import require_torch, require_torchcodec, slow, torch_device\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n@@ -750,7 +750,7 @@ def test_compute_mask_indices_overlap(self):\n \n \n @require_torch\n-@require_soundfile\n+@require_torchcodec\n @slow\n class HubertModelIntegrationTest(unittest.TestCase):\n     def _load_datasamples(self, num_samples):"
        },
        {
            "sha": "ad516904ef347b49d365d736b9cd70f80c70af04",
            "filename": "tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -713,7 +713,7 @@ def _load_dataset(cls):\n     def _load_datasamples(self, num_samples):\n         self._load_dataset()\n         ds = self._dataset\n-        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n         return [x[\"array\"] for x in speech_samples]\n \n     @slow"
        },
        {
            "sha": "e0a617aeaf6fe1dae554a2944acc30f26ff6a400",
            "filename": "tests/models/moonshine/test_modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -443,7 +443,7 @@ def tearDown(self):\n     def _load_datasamples(self, num_samples):\n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         # automatic decoding with librispeech\n-        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n \n         return [x[\"array\"] for x in speech_samples]\n "
        },
        {
            "sha": "8d235b51990ba5c789e39b71c5acd341d74ae0d7",
            "filename": "tests/models/phi4_multimodal/test_feature_extractor_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fphi4_multimodal%2Ftest_feature_extractor_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fphi4_multimodal%2Ftest_feature_extractor_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2Ftest_feature_extractor_phi4_multimodal.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -207,7 +207,7 @@ def test_double_precision_pad(self):\n     def _load_datasamples(self, num_samples):\n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         # automatic decoding with librispeech\n-        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n \n         return [x[\"array\"] for x in speech_samples]\n "
        },
        {
            "sha": "07fd24577bf8d86f16f3f7e188a942b071b25a57",
            "filename": "tests/models/phi4_multimodal/test_modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 8,
            "deletions": 11,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import tempfile\n import unittest\n \n import requests\n@@ -33,13 +32,13 @@\n from transformers.testing_utils import (\n     Expectations,\n     cleanup,\n-    require_soundfile,\n     require_torch,\n     require_torch_large_accelerator,\n+    require_torchcodec,\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_soundfile_available\n+from transformers.utils import is_torchcodec_available\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -54,8 +53,8 @@\n     from PIL import Image\n \n \n-if is_soundfile_available():\n-    import soundfile\n+if is_torchcodec_available():\n+    import torchcodec\n \n \n class Phi4MultimodalModelTester:\n@@ -296,11 +295,9 @@ def setUp(self):\n         self.assistant_token = \"<|assistant|>\"\n         self.end_token = \"<|end|>\"\n         self.image = Image.open(requests.get(self.image_url, stream=True).raw)\n-        with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=\".wav\") as tmp:\n-            tmp.write(requests.get(self.audio_url, stream=True).raw.data)\n-            tmp.flush()\n-            tmp.seek(0)\n-            self.audio, self.sampling_rate = soundfile.read(tmp.name)\n+        audio_bytes = requests.get(self.audio_url, stream=True).raw.data\n+        samples = torchcodec.decoders.AudioDecoder(audio_bytes).get_all_samples()\n+        self.audio, self.sampling_rate = samples.data, samples.sample_rate\n \n         cleanup(torch_device, gc_collect=True)\n \n@@ -378,7 +375,7 @@ def test_multi_image_vision_text_generation(self):\n \n         self.assertEqual(response, EXPECTED_RESPONSE)\n \n-    @require_soundfile\n+    @require_torchcodec\n     def test_audio_text_generation(self):\n         model = AutoModelForCausalLM.from_pretrained(\n             self.checkpoint_path, revision=self.revision, torch_dtype=torch.float16, device_map=torch_device"
        },
        {
            "sha": "270f91bdf628ffa3a63b486d6233a01dd7e4e587",
            "filename": "tests/models/sew/test_modeling_sew.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -19,7 +19,7 @@\n import pytest\n \n from transformers import SEWConfig, is_torch_available\n-from transformers.testing_utils import require_soundfile, require_torch, slow, torch_device\n+from transformers.testing_utils import require_torch, require_torchcodec, slow, torch_device\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n@@ -453,7 +453,7 @@ def test_compute_mask_indices_overlap(self):\n \n \n @require_torch\n-@require_soundfile\n+@require_torchcodec\n @slow\n class SEWModelIntegrationTest(unittest.TestCase):\n     def _load_datasamples(self, num_samples):"
        },
        {
            "sha": "86064250b8f6cf2fe32933b9d7f5b64d820d0afb",
            "filename": "tests/models/sew_d/test_modeling_sew_d.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -19,7 +19,7 @@\n import pytest\n \n from transformers import SEWDConfig, is_torch_available\n-from transformers.testing_utils import require_soundfile, require_torch, slow, torch_device\n+from transformers.testing_utils import require_torch, require_torchcodec, slow, torch_device\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n@@ -464,7 +464,7 @@ def test_compute_mask_indices_overlap(self):\n \n \n @require_torch\n-@require_soundfile\n+@require_torchcodec\n @slow\n class SEWDModelIntegrationTest(unittest.TestCase):\n     def _load_datasamples(self, num_samples):"
        },
        {
            "sha": "15f0d89a3be73fa6af800186cebb3df569de4cec",
            "filename": "tests/models/speech_to_text/test_feature_extraction_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fspeech_to_text%2Ftest_feature_extraction_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fspeech_to_text%2Ftest_feature_extraction_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_feature_extraction_speech_to_text.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -294,7 +294,7 @@ def _load_datasamples(self, num_samples):\n \n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         # automatic decoding with librispeech\n-        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n \n         return [x[\"array\"] for x in speech_samples]\n "
        },
        {
            "sha": "9c1a3b524d96df6f35c510b36eafba43ed5d6bda",
            "filename": "tests/models/speecht5/test_feature_extraction_speecht5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fspeecht5%2Ftest_feature_extraction_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fspeecht5%2Ftest_feature_extraction_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_feature_extraction_speecht5.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -381,7 +381,7 @@ def _load_datasamples(self, num_samples):\n \n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         # automatic decoding with librispeech\n-        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n \n         return [x[\"array\"] for x in speech_samples]\n "
        },
        {
            "sha": "9c0fa0fa394f01db0e97df210a279e4a3dd0d437",
            "filename": "tests/models/speecht5/test_modeling_speecht5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -764,7 +764,7 @@ def _load_datasamples(self, num_samples):\n \n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         # automatic decoding with librispeech\n-        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n \n         return [x[\"array\"] for x in speech_samples]\n \n@@ -1792,7 +1792,7 @@ def _load_datasamples(self, num_samples):\n \n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         # automatic decoding with librispeech\n-        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n \n         return [x[\"array\"] for x in speech_samples]\n "
        },
        {
            "sha": "00614bca7c8452a0368c57cb545dab965c57cd0a",
            "filename": "tests/models/unispeech/test_modeling_unispeech.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Funispeech%2Ftest_modeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Funispeech%2Ftest_modeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funispeech%2Ftest_modeling_unispeech.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -21,7 +21,7 @@\n from datasets import load_dataset\n \n from transformers import UniSpeechConfig, is_torch_available\n-from transformers.testing_utils import is_flaky, require_soundfile, require_torch, slow, torch_device\n+from transformers.testing_utils import is_flaky, require_torch, require_torchcodec, slow, torch_device\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n@@ -553,7 +553,7 @@ def test_model_from_pretrained(self):\n \n \n @require_torch\n-@require_soundfile\n+@require_torchcodec\n @slow\n class UniSpeechModelIntegrationTest(unittest.TestCase):\n     def _load_datasamples(self, num_samples):"
        },
        {
            "sha": "2c5001fbbc58b628986d6a3ad993c4a17ab510c5",
            "filename": "tests/models/unispeech_sat/test_modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Funispeech_sat%2Ftest_modeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Funispeech_sat%2Ftest_modeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funispeech_sat%2Ftest_modeling_unispeech_sat.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -21,7 +21,7 @@\n from datasets import load_dataset\n \n from transformers import UniSpeechSatConfig, is_torch_available\n-from transformers.testing_utils import require_soundfile, require_torch, slow, torch_device\n+from transformers.testing_utils import require_torch, require_torchcodec, slow, torch_device\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n@@ -807,7 +807,7 @@ def test_model_from_pretrained(self):\n \n \n @require_torch\n-@require_soundfile\n+@require_torchcodec\n @slow\n class UniSpeechSatModelIntegrationTest(unittest.TestCase):\n     def _load_datasamples(self, num_samples):"
        },
        {
            "sha": "e57c40396e294c18e6c1b3c478cfed7ac3184257",
            "filename": "tests/models/univnet/test_feature_extraction_univnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Funivnet%2Ftest_feature_extraction_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Funivnet%2Ftest_feature_extraction_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funivnet%2Ftest_feature_extraction_univnet.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -330,7 +330,7 @@ def _load_datasamples(self, num_samples):\n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         ds = ds.cast_column(\"audio\", Audio(sampling_rate=self.feat_extract_tester.sampling_rate))\n         # automatic decoding with librispeech\n-        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n \n         return [x[\"array\"] for x in speech_samples], [x[\"sampling_rate\"] for x in speech_samples]\n "
        },
        {
            "sha": "00066d89bdb32cc3686b7388b72b6cea93b85ff1",
            "filename": "tests/models/univnet/test_modeling_univnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -216,7 +216,7 @@ def _load_datasamples(self, num_samples, sampling_rate=24000):\n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         ds = ds.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n         # automatic decoding with librispeech\n-        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n \n         return [x[\"array\"] for x in speech_samples], [x[\"sampling_rate\"] for x in speech_samples]\n "
        },
        {
            "sha": "560a8af6d9c3f945c9784f18d551510f2e03063f",
            "filename": "tests/models/wav2vec2/test_modeling_wav2vec2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -34,10 +34,10 @@\n     is_torchaudio_available,\n     require_flash_attn,\n     require_pyctcdecode,\n-    require_soundfile,\n     require_torch,\n     require_torch_gpu,\n     require_torchaudio,\n+    require_torchcodec,\n     run_test_in_subprocess,\n     slow,\n     torch_device,\n@@ -1444,7 +1444,7 @@ def test_sample_negatives_with_mask(self):\n \n \n @require_torch\n-@require_soundfile\n+@require_torchcodec\n @slow\n class Wav2Vec2ModelIntegrationTest(unittest.TestCase):\n     def tearDown(self):"
        },
        {
            "sha": "0834edb4e2ad5376a9537f298d4bda400543f1e3",
            "filename": "tests/models/whisper/test_feature_extraction_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fwhisper%2Ftest_feature_extraction_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fwhisper%2Ftest_feature_extraction_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_feature_extraction_whisper.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -254,7 +254,7 @@ def test_double_precision_pad(self):\n     def _load_datasamples(self, num_samples):\n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         # automatic decoding with librispeech\n-        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n \n         return [x[\"array\"] for x in speech_samples]\n "
        },
        {
            "sha": "df442342ee9f338dcf8ed8417a7a7b1ca3eb30c5",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -1460,7 +1460,7 @@ def _load_dataset(cls):\n     def _load_datasamples(self, num_samples):\n         self._load_dataset()\n         ds = self._dataset\n-        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n         return [x[\"array\"] for x in speech_samples]\n \n     @slow"
        },
        {
            "sha": "0e3f2246cc5713aeb6604b923f8beace390afcba",
            "filename": "tests/pipelines/test_pipelines_automatic_speech_recognition.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -1190,7 +1190,7 @@ def test_speculative_decoding_whisper_distil(self):\n             num_beams=1,\n         )\n \n-        transcription_non_ass = pipe(sample.copy(), generate_kwargs={\"assistant_model\": assistant_model})[\"text\"]\n+        transcription_non_ass = pipe(sample, generate_kwargs={\"assistant_model\": assistant_model})[\"text\"]\n         transcription_ass = pipe(sample)[\"text\"]\n \n         self.assertEqual(transcription_ass, transcription_non_ass)"
        },
        {
            "sha": "4d0459d9a8b0eba5b61b08f7b08f04cd8e0ccf8d",
            "filename": "tests/utils/test_audio_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Futils%2Ftest_audio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/tests%2Futils%2Ftest_audio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_audio_utils.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -278,7 +278,7 @@ def _load_datasamples(self, num_samples):\n \n         if self._dataset is None:\n             self._dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n-        speech_samples = self._dataset.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        speech_samples = self._dataset.sort(\"id\")[:num_samples][\"audio\"]\n         return [x[\"array\"] for x in speech_samples]\n \n     def test_spectrogram_impulse(self):"
        },
        {
            "sha": "ea2b87735542bf54e2aab772634ec7c42f8f9d26",
            "filename": "utils/print_env.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/utils%2Fprint_env.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ecd52e50a31e7c344c32564e0484d7e9a0f2256/utils%2Fprint_env.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fprint_env.py?ref=1ecd52e50a31e7c344c32564e0484d7e9a0f2256",
            "patch": "@@ -72,3 +72,14 @@\n     print(\"Number of TF GPUs available:\", len(tf.config.list_physical_devices(\"GPU\")))\n except ImportError:\n     print(\"TensorFlow version:\", None)\n+\n+\n+try:\n+    import torchcodec\n+\n+    versions = torchcodec._core.get_ffmpeg_library_versions()\n+    print(\"FFmpeg version:\", versions[\"ffmpeg_version\"])\n+except ImportError:\n+    print(\"FFmpeg version:\", None)\n+except (AttributeError, KeyError):\n+    print(\"Failed to get FFmpeg version\")"
        }
    ],
    "stats": {
        "total": 798,
        "additions": 448,
        "deletions": 350
    }
}