{
    "author": "TaskerJang",
    "message": "ğŸŒ [i18n-KO] Translated llama4.md to Korean (#40396)\n\n* docs: ko: llama4.md\n\n* feat: nmt draft\n\n* fix: manual edits\n\n* Update docs/source/ko/model_doc/llama4.md\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\n\n* Update docs/source/ko/model_doc/llama4.md\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\n\n* Update docs/source/ko/model_doc/llama4.md\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\n\n* Update docs/source/ko/model_doc/llama4.md\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\n\n---------\n\nCo-authored-by: TaskerJang <bymyself103@naver.com>\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>",
    "sha": "eefbf4ac8b1f34e30e8eaeb0130f75666a60e57e",
    "files": [
        {
            "sha": "a01564789a747612d2b5d763708df79e66aa5fc5",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eefbf4ac8b1f34e30e8eaeb0130f75666a60e57e/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/eefbf4ac8b1f34e30e8eaeb0130f75666a60e57e/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=eefbf4ac8b1f34e30e8eaeb0130f75666a60e57e",
            "patch": "@@ -1101,7 +1101,7 @@\n         title: LayoutXLM\n       - local: in_translation\n         title: LiLT\n-      - local: in_translation\n+      - local: model_doc/llama4\n         title: Llama4\n       - local: in_translation\n         title: Llava"
        },
        {
            "sha": "49642a135b74a42488482363dd0c0e13f6183183",
            "filename": "docs/source/ko/model_doc/llama4.md",
            "status": "added",
            "additions": 443,
            "deletions": 0,
            "changes": 443,
            "blob_url": "https://github.com/huggingface/transformers/blob/eefbf4ac8b1f34e30e8eaeb0130f75666a60e57e/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/eefbf4ac8b1f34e30e8eaeb0130f75666a60e57e/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama4.md?ref=eefbf4ac8b1f34e30e8eaeb0130f75666a60e57e",
            "patch": "@@ -0,0 +1,443 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+*MetaëŠ” ì´ ëª¨ë¸ì„ 2025-04-05ì— ì¶œì‹œí•˜ê³  ê°™ì€ ë‚  Hugging Face Transformersì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.*\n+\n+# Llama4[[llama4]]\n+\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n+    </div>\n+</div>\n+\n+Metaì—ì„œ ê°œë°œí•œ [Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)ëŠ” ìƒˆë¡œìš´ ìê¸°íšŒê·€ Mixture-of-Experts (MoE) ì•„í‚¤í…ì²˜ë¥¼ ë„ì…í•©ë‹ˆë‹¤.\n+ì´ ì„¸ëŒ€ëŠ” ë‘ ê°€ì§€ ëª¨ë¸ë¡œ ë‚˜ë‰©ë‹ˆë‹¤:\n+- 128ê°œì˜ ì „ë¬¸ê°€(expert)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ ì•½ 400B ë§¤ê°œë³€ìˆ˜ ì¤‘ 17B í™œì„± ë§¤ê°œë³€ìˆ˜ë¥¼ ê°–ëŠ” ê³ ì„±ëŠ¥ Llama 4 Maverick\n+- 16ê°œì˜ ì „ë¬¸ê°€ë§Œ ì‚¬ìš©í•˜ì—¬ ì´ ì•½ 109B ë§¤ê°œë³€ìˆ˜ ì¤‘ 17B í™œì„± ë§¤ê°œë³€ìˆ˜ë¥¼ ê°–ëŠ” ê²½ëŸ‰í™”ëœ Llama 4 Scout\n+-\n+ë‘ ëª¨ë¸ ëª¨ë‘ ë„¤ì´í‹°ë¸Œ ë©€í‹°ëª¨ë‹¬ì„ ìœ„í•œ ì´ˆê¸° ìœµí•©(early fusion)ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì…ë ¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+Maverickê³¼ Scout ëª¨ë‘ 200ê°œ ì–¸ì–´ë¥¼ í¬í•¨í•˜ëŠ” ë°ì´í„°ì—ì„œ ìµœëŒ€ 40ì¡°ê°œì˜ í† í°ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤.\n+(ì•„ëì–´, ìŠ¤í˜ì¸ì–´, ë…ì¼ì–´, íŒë””ì–´ë¥¼ í¬í•¨í•œ 12ê°œ ì–¸ì–´ì— ëŒ€í•œ íŠ¹ì • ë¯¸ì„¸ ì¡°ì • ì§€ì› í¬í•¨)\n+\n+MetaëŠ” Llama 4 Scoutì„ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. Scoutì€ 4ë¹„íŠ¸ ë˜ëŠ” 8ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ì ìš©í•˜ë©´ ë‹¨ì¼ ì„œë²„ê¸‰ GPUì—ì„œë„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´, ë” ëŒ€ê·œëª¨ì¸ Llama 4 Maverickì€ ê³ ì„±ëŠ¥ ì—°ì‚°ì„ ìœ„í•´ BF16ê³¼ FP8 í˜•ì‹ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.\n+ì´ ëª¨ë¸ë“¤ì€ ëª¨ë¸ ì €ì¥ì†Œì—ì„œ ì œê³µë˜ëŠ” ì‚¬ìš©ì ì§€ì • Llama 4 ì»¤ë®¤ë‹ˆí‹° ë¼ì´ì„ ìŠ¤ ê³„ì•½ì— ë”°ë¼ ì¶œì‹œë©ë‹ˆë‹¤.\n+\n+ëª¨ë“  ì›ë³¸ Llama ì²´í¬í¬ì¸íŠ¸ëŠ” hugging face [meta-llama](https://huggingface.co/meta-llama) í˜ì´ì§€ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+> [!TIP]\n+> Llama 4 ëª¨ë¸ íŒ¨ë°€ë¦¬ëŠ” ë‘ ê°€ì§€ í˜•íƒœë¡œ ì œê³µë©ë‹ˆë‹¤: 109Bì™€ 402B ë§¤ê°œë³€ìˆ˜ì…ë‹ˆë‹¤. ì´ ë‘ í˜•íƒœ ëª¨ë‘ ë§¤ìš° í° ëª¨ë¸ì´ë©°\n+> ì¼ë°˜ì ì¸ ê¸°ê¸°ì—ì„œëŠ” ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ì— ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ëŠ” ë°©ë²• ëª‡ ê°€ì§€ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.\n+>\n+> ë”ìš± ë¹ ë¥´ê³  ì•ˆì •ì ì¸ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•´ `hf_xet` ì¢…ì†ì„± ì„¤ì¹˜ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤:\n+> `pip install transformers[hf_xet]`\n+\n+ì•„ë˜ ì˜ˆì‹œë“¤ì€ [`Pipeline`] ë˜ëŠ” [`AutoModel`]ë¡œ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ ì¼ë¶€ Llama 4 ë³€í˜•ì´\n+ìµœëŒ€ 1ì²œë§Œ í† í°ì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ê°–ê¸° ë•Œë¬¸ì—, ë§¤ìš° ê¸´ ì»¨í…ìŠ¤íŠ¸ ìƒì„±ì„ í™œì„±í™”í•˜ê¸° ìœ„í•´ ì˜¬ë°”ë¥¸ ì†ì„±ì„ í† ê¸€í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì‹œë„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.\n+\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+from transformers import pipeline\n+import torch\n+\n+model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n+\n+messages = [\n+    {\"role\": \"user\", \"content\": \"ë§ˆìš”ë„¤ì¦ˆ ë ˆì‹œí”¼ê°€ ë¬´ì—‡ì¸ê°€ìš”?\"},\n+]\n+\n+pipe = pipeline(\n+    \"text-generation\",\n+    model=model_id,\n+    device_map=\"auto\",\n+    dtype=torch.bfloat16\n+)\n+\n+output = pipe(messages, do_sample=False, max_new_tokens=200)\n+print(output[0][\"generated_text\"][-1][\"content\"])\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel - Text only\">\n+\n+```py\n+from transformers import AutoTokenizer, Llama4ForConditionalGeneration\n+import torch\n+\n+model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+messages = [\n+    {\"role\": \"user\", \"content\": \"ë‹¹ì‹ ì€ ëˆ„êµ¬ì‹ ê°€ìš”?\"},\n+]\n+inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True)\n+\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    dtype=torch.bfloat16\n+)\n+\n+outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n+outputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])\n+print(outputs[0])\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel - Multimodal\">\n+\n+```py\n+from transformers import AutoProcessor, Llama4ForConditionalGeneration\n+import torch\n+\n+model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n+\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    dtype=torch.bfloat16,\n+)\n+\n+img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"url\": img_url},\n+            {\"type\": \"text\", \"text\": \"ì´ ì´ë¯¸ì§€ë¥¼ ë‘ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"},\n+        ]\n+    },\n+]\n+\n+inputs = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+).to(model.device)\n+\n+outputs = model.generate(\n+    **inputs,\n+    max_new_tokens=256,\n+)\n+\n+response = processor.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])[0]\n+print(response)\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel - Multimodal with multiple images\">\n+\n+```py\n+from transformers import AutoProcessor, Llama4ForConditionalGeneration\n+import torch\n+\n+model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n+\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    dtype=torch.bfloat16,\n+)\n+\n+url1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n+url2 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/cat_style_layout.png\"\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"url\": url1},\n+            {\"type\": \"image\", \"url\": url2},\n+            {\"type\": \"text\", \"text\": \"ì´ ë‘ ì´ë¯¸ì§€ê°€ ì–´ë–»ê²Œ ë¹„ìŠ·í•˜ê³ , ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ ì„¤ëª…í•´ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?\"},\n+        ]\n+    },\n+]\n+\n+inputs = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+).to(model.device)\n+\n+outputs = model.generate(\n+    **inputs,\n+    max_new_tokens=256,\n+)\n+\n+response = processor.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])[0]\n+print(response)\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel - Long context\">\n+\n+ì£¼ì˜: ì•„ë˜ ì˜ˆì‹œëŠ” `device_map=\"auto\"`ì™€ flex-attentionì„ ëª¨ë‘ ì‚¬ìš©í•©ë‹ˆë‹¤.\n+ì´ ì˜ˆì‹œë¥¼ í…ì„œ ë³‘ë ¬ ëª¨ë“œë¡œ ì‹¤í–‰í•˜ë ¤ë©´ `torchrun`ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n+\n+í–¥í›„ í…ì„œ ë³‘ë ¬ ì—†ì´ `device_map=\"auto\"`ì™€ flex-attentionì„ í•¨ê»˜ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡\n+ì‘ì—…í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n+\n+```py\n+from transformers import Llama4ForConditionalGeneration, AutoTokenizer, infer_device\n+import torch\n+import time\n+\n+file = \"very_long_context_prompt.txt\"\n+model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n+\n+with open(file, \"r\") as f:\n+    very_long_text = \"\\n\".join(f.readlines())\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    attn_implementation=\"flex_attention\",\n+    dtype=torch.bfloat16\n+)\n+\n+messages = [\n+    {\"role\": \"user\", \"content\": f\"ë‹¤ìŒ í…ìŠ¤íŠ¸ë“¤ì„ ë³´ì„¸ìš”: [{very_long_text}]\\n\\n\\n\\nì±…ë“¤ì€ ë¬´ì—‡ì´ë©°, ëˆ„ê°€ ì¼ë‚˜ìš”? ì¢‹ì€ ëª©ë¡ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\"},\n+]\n+input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n+\n+device = infer_device()\n+torch_device_module = getattr(torch, device, torch.cuda)\n+torch_device_module.synchronize()\n+start = time.time()\n+out = model.generate(\n+    input_ids.to(model.device),\n+    prefill_chunk_size=2048*8,\n+    max_new_tokens=300,\n+    cache_implementation=\"hybrid\",\n+)\n+print(time.time()-start)\n+print(tokenizer.batch_decode(out[:, input_ids.shape[-1]:]))\n+print(f\"{torch_device_module.max_memory_allocated(model.device) / 1024**3:.2f} GiB\")\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## íš¨ìœ¨ì„±; Llama 4ì˜ ìµœëŒ€ ì„±ëŠ¥ í™œìš©í•˜ê¸°[[efficiency-how-to-get-the-best-out-of-llama-4]]\n+\n+### ì–´í…ì…˜ ë°©ë²•[[the-attention-methods]]\n+\n+ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì£¼ì–´ì§€ëŠ” ì–´í…ì…˜ í•¨ìˆ˜ë¥¼ ë³€ê²½í•˜ë©´ ê³„ì‚° ì„±ëŠ¥ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¸í„°í˜ì´ìŠ¤ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ [ì–´í…ì…˜ ì¸í„°í˜ì´ìŠ¤](../attention_interface) ê°œìš”ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n+\n+Llama 4 ëª¨ë¸ì€ ì²˜ìŒ ê³µê°œë  ë•Œë¶€í„° ë‹¤ìŒ ì–´í…ì…˜ ë°©ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤: `eager`, `flex_attention`, `sdpa`. ìµœìƒì˜ ê²°ê³¼ë¥¼ ìœ„í•´ `flex_attention` ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n+ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì „í™˜ì€ ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ë•Œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤:\n+\n+\n+<hfoptions id=\"Attention\">\n+<hfoption id=\"Flex Attention\">\n+\n+Flex Attentionì€ ëª¨ë¸ì´ ê¸´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ë•Œ ìµœì ì˜ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.\n+\n+> [!TIP] ì£¼ì˜: ì•„ë˜ ì˜ˆì‹œëŠ” `device_map=\"auto\"`ì™€ flex-attentionì„ ëª¨ë‘ ì‚¬ìš©í•©ë‹ˆë‹¤.\n+> ì´ ì˜ˆì‹œë¥¼ í…ì„œ ë³‘ë ¬ ëª¨ë“œë¡œ ì‹¤í–‰í•˜ë ¤ë©´ `torchrun`ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n+>\n+> í–¥í›„ í…ì„œ ë³‘ë ¬ ì—†ì´ `device_map=\"auto\"`ì™€ flex-attentionì„ í•¨ê»˜ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡\n+> ì‘ì—…í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n+\n+```py\n+from transformers import Llama4ForConditionalGeneration\n+import torch\n+\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    attn_implementation=\"flex_attention\",\n+    device_map=\"auto\",\n+    dtype=torch.bfloat16,\n+)\n+```\n+</hfoption>\n+<hfoption id=\"SDPA\">\n+`sdpa` ì–´í…ì…˜ ë°©ë²•ì€ ì¼ë°˜ì ìœ¼ë¡œ `eager` ë°©ë²•ë³´ë‹¤ ê³„ì‚° íš¨ìœ¨ì ì…ë‹ˆë‹¤.\n+\n+```py\n+from transformers import Llama4ForConditionalGeneration\n+import torch\n+\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    attn_implementation=\"sdpa\",\n+    device_map=\"auto\",\n+    dtype=torch.bfloat16,\n+)\n+```\n+</hfoption>\n+<hfoption id=\"Eager\">\n+`eager` ì–´í…ì…˜ ë°©ë²•ì´ ê¸°ë³¸ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ëª¨ë¸ ë¡œë“œ ì‹œ ë‹¤ë¥¸ ì„¤ì •ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤:\n+\n+```py\n+from transformers import Llama4ForConditionalGeneration\n+import torch\n+\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    dtype=torch.bfloat16,\n+)\n+```\n+</hfoption>\n+</hfoptions>\n+\n+\n+### ì–‘ìí™”[[quantization]]\n+\n+ì–‘ìí™”ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë” ë‚®ì€ ì •ë°€ë„ë¡œ ë°”ê¿” ëŒ€í˜• ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ë¶€ë‹´ì„ ì¤„ì…ë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì–‘ìí™” ë°±ì—”ë“œì— ëŒ€í•´ì„œëŠ” [ì–‘ìí™”](../quantization/overview) ê°œìš”ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n+í˜„ì¬ëŠ” FBGEMMê³¼ LLM-Compressorë¥¼ ì§€ì›í•˜ë©°, ê³§ ë” ë§ì€ ë°©ì‹ì´ ì¶”ê°€ë  ì˜ˆì •ì…ë‹ˆë‹¤.\n+\n+ë‘ ê°€ì§€ ë°©ë²•ì„ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œë¥¼ ì•„ë˜ì—ì„œ í™•ì¸í•˜ì„¸ìš”:\n+\n+\n+\n+ë‹¤ìŒì€ FBGEMM ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•˜ì—¬ BF16 ëª¨ë¸ì„ FP8ë¡œ ë¡œë“œí•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤:\n+\n+<hfoptions id=\"Quantization\">\n+<hfoption id=\"FBGEMM\">\n+\n+```python\n+from transformers import AutoTokenizer, Llama4ForConditionalGeneration, FbgemmFp8Config\n+import torch\n+\n+model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+messages = [\n+    {\"role\": \"user\", \"content\": \"ë‹¹ì‹ ì€ ëˆ„êµ¬ì‹ ê°€ìš”?\"},\n+]\n+inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True)\n+\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    dtype=torch.bfloat16,\n+    quantization_config=FbgemmFp8Config()\n+)\n+\n+outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n+outputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])\n+print(outputs[0])\n+```\n+\n+</hfoption>\n+<hfoption id=\"LLM-Compressor\">\n+\n+LLLM-Compressorë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” í•¨ê»˜ ì œê³µë˜ëŠ” ì‚¬ì „ ì–‘ìí™”ëœ FP8 ì²´í¬í¬ì¸íŠ¸ë¥¼ ì“°ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤:\n+\n+```python\n+from transformers import AutoTokenizer, Llama4ForConditionalGeneration\n+import torch\n+\n+model_id = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+messages = [\n+    {\"role\": \"user\", \"content\": \"ë‹¹ì‹ ì€ ëˆ„êµ¬ì‹ ê°€ìš”?\"},\n+]\n+inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True)\n+\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    tp_plan=\"auto\",\n+    dtype=torch.bfloat16,\n+)\n+\n+outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n+outputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])\n+print(outputs[0])\n+```\n+</hfoption>\n+</hfoptions>\n+\n+### ì˜¤í”„ë¡œë”©[[offloading]]\n+\n+CPU ì˜¤í”„ë¡œë”©ì„ í™œì„±í™”í•˜ë©´, GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ë•Œ ëª¨ë¸ì´ êµ¬ì„± ìš”ì†Œë¥¼ CPUë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤.\n+ì¶”ë¡  ì‹œ ë‹¤ì–‘í•œ êµ¬ì„± ìš”ì†Œë“¤ì´ GPUì™€ CPU ê°„ì— ë™ì ìœ¼ë¡œ ë¡œë“œë˜ê³  ì–¸ë¡œë“œë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ CPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•œ í•œ ë” ì‘ì€ ë¨¸ì‹ ì—ì„œë„ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+ë‹¤ë§Œ í†µì‹  ì˜¤ë²„í—¤ë“œë¡œ ì¸í•´ ì¶”ë¡  ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+CPU ì˜¤í”„ë¡œë”©ì„ í™œì„±í™”í•˜ë ¤ë©´ ëª¨ë¸ ë¡œë“œ ì‹œ `device_map`ì„ `auto`ë¡œ ì§€ì •í•˜ë©´ ë©ë‹ˆë‹¤\n+\n+```py\n+from transformers import Llama4ForConditionalGeneration\n+import torch\n+\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    dtype=torch.bfloat16,\n+)\n+```\n+\n+## Llama4Config\n+\n+[[autodoc]] Llama4Config\n+\n+## Llama4TextConfig\n+\n+[[autodoc]] Llama4TextConfig\n+\n+## Llama4VisionConfig\n+\n+[[autodoc]] Llama4VisionConfig\n+\n+## Llama4Processor\n+\n+[[autodoc]] Llama4Processor\n+\n+## Llama4ImageProcessorFast\n+\n+[[autodoc]] Llama4ImageProcessorFast\n+\n+## Llama4ForConditionalGeneration\n+\n+[[autodoc]] Llama4ForConditionalGeneration\n+- forward\n+\n+## Llama4ForCausalLM\n+\n+[[autodoc]] Llama4ForCausalLM\n+- forward\n+\n+## Llama4TextModel\n+\n+[[autodoc]] Llama4TextModel\n+- forward\n+\n+## Llama4ForCausalLM\n+\n+[[autodoc]] Llama4ForCausalLM\n+- forward\n+\n+## Llama4VisionModel\n+\n+[[autodoc]] Llama4VisionModel\n+- forward\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 445,
        "additions": 444,
        "deletions": 1
    }
}