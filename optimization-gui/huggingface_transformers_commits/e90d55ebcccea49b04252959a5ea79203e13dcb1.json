{
    "author": "gante",
    "message": "[Tests] add `min_new_tokens` to prevent flaky length checks (#37175)",
    "sha": "e90d55ebcccea49b04252959a5ea79203e13dcb1",
    "files": [
        {
            "sha": "f61b26c26ec8932c0a0378be7e0d5afce16489af",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e90d55ebcccea49b04252959a5ea79203e13dcb1/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e90d55ebcccea49b04252959a5ea79203e13dcb1/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=e90d55ebcccea49b04252959a5ea79203e13dcb1",
            "patch": "@@ -281,6 +281,7 @@ def _greedy_generate(\n             do_sample=False,\n             num_beams=1,\n             max_new_tokens=self.max_new_tokens,\n+            min_new_tokens=self.max_new_tokens,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             output_scores=output_scores,\n@@ -311,6 +312,7 @@ def _sample_generate(\n             do_sample=True,\n             num_beams=1,\n             max_new_tokens=self.max_new_tokens,\n+            min_new_tokens=self.max_new_tokens,\n             num_return_sequences=num_return_sequences,\n             output_scores=output_scores,\n             output_logits=output_logits,\n@@ -340,6 +342,7 @@ def _beam_search_generate(\n         output_generate = model.generate(\n             do_sample=False,\n             max_new_tokens=self.max_new_tokens,\n+            min_new_tokens=self.max_new_tokens,\n             output_scores=output_scores,\n             output_logits=output_logits,\n             output_attentions=output_attentions,\n@@ -370,6 +373,7 @@ def _beam_sample_generate(\n         output_generate = model.generate(\n             do_sample=True,\n             max_new_tokens=self.max_new_tokens,\n+            min_new_tokens=self.max_new_tokens,\n             output_scores=output_scores,\n             output_logits=output_logits,\n             output_attentions=output_attentions,\n@@ -399,6 +403,7 @@ def _group_beam_search_generate(\n         output_generate = model.generate(\n             do_sample=False,\n             max_new_tokens=self.max_new_tokens,\n+            min_new_tokens=self.max_new_tokens,\n             output_scores=output_scores,\n             output_logits=output_logits,\n             output_attentions=output_attentions,\n@@ -429,6 +434,7 @@ def _constrained_beam_search_generate(\n         output_generate = model.generate(\n             do_sample=False,\n             max_new_tokens=self.max_new_tokens,\n+            min_new_tokens=self.max_new_tokens,\n             output_scores=output_scores,\n             output_logits=output_logits,\n             output_attentions=output_attentions,\n@@ -464,6 +470,7 @@ def _contrastive_generate(\n             do_sample=False,\n             num_beams=1,\n             max_new_tokens=self.max_new_tokens,\n+            min_new_tokens=self.max_new_tokens,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             output_scores=output_scores,"
        }
    ],
    "stats": {
        "total": 7,
        "additions": 7,
        "deletions": 0
    }
}