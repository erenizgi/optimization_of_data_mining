{
    "author": "zucchini-nlp",
    "message": "Remove head mask in generative models (#35786)\n\n* just squash into one commit\n\n* delete print",
    "sha": "955e61b0da09266fa2ada2e164b703081c8c9940",
    "files": [
        {
            "sha": "1e414735ed6629a26d8c680bfa417efdec5aa3f7",
            "filename": "docs/source/en/model_doc/albert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -57,6 +57,7 @@ This model was contributed by [lysandre](https://huggingface.co/lysandre). This\n - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters.\n - Layers are split in groups that share parameters (to save memory).\n Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A. The model must predict if they have been swapped or not.\n+- The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`  \n \n ### Using Scaled Dot Product Attention (SDPA)\n "
        },
        {
            "sha": "b24daa3e6e123cd083b14c2a260492c11d93f7b2",
            "filename": "docs/source/en/model_doc/bart.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -55,6 +55,7 @@ This model was contributed by [sshleifer](https://huggingface.co/sshleifer). The\n   * mask a span of k tokens with a single mask token (a span of 0 tokens is an insertion of a mask token)\n   * permute sentences\n   * rotate the document to make it start at a specific token\n+- The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`  \n \n ## Implementation Notes\n "
        },
        {
            "sha": "d7145993a89c5047486a6d817a3305d578c7f7f6",
            "filename": "docs/source/en/model_doc/biogpt.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -36,6 +36,7 @@ This model was contributed by [kamalkraj](https://huggingface.co/kamalkraj). The\n - BioGPT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than the left.\n - BioGPT was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows BioGPT to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n - The model can take the `past_key_values` (for PyTorch) as input, which is the previously computed key/value attention pairs. Using this (past_key_values or past) value prevents the model from re-computing pre-computed values in the context of text generation. For PyTorch, see past_key_values argument of the BioGptForCausalLM.forward() method for more information on its usage.\n+- The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`  \n \n ### Using Scaled Dot Product Attention (SDPA)\n "
        },
        {
            "sha": "083fa55d1c7b8811c287de559f3591b8340d49fa",
            "filename": "docs/source/en/model_doc/data2vec.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -53,6 +53,7 @@ The original code for vision can be found [here](https://github.com/facebookrese\n - For Data2VecAudio, preprocessing is identical to [`Wav2Vec2Model`], including feature extraction\n - For Data2VecText, preprocessing is identical to [`RobertaModel`], including tokenization.\n - For Data2VecVision, preprocessing is identical to [`BeitModel`], including feature extraction.\n+- The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`  \n \n ### Using Scaled Dot Product Attention (SDPA)\n "
        },
        {
            "sha": "3620281893098a3dcf5ded55f04fd8413ddb8263",
            "filename": "docs/source/en/model_doc/gpt_bigcode.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -46,8 +46,12 @@ The main differences compared to GPT2.\n - Merge the key and value caches into one (this changes the format of layer_past/ present, does it risk creating problems?)\n - Use the memory layout (self.num_heads, 3, self.head_dim) instead of `(3, self.num_heads, self.head_dim)` for the QKV tensor with MHA. (prevents an overhead with the merged key and values, but makes the checkpoints incompatible with the original openai-community/gpt2 model).\n \n+\n You can read more about the optimizations in the [original pull request](https://github.com/huggingface/transformers/pull/22575)\n \n+> [!NOTE]\n+> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n+\n ## Combining Starcoder and Flash Attention 2\n \n First, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature."
        },
        {
            "sha": "98f00867bdad5ea8a88168b6918ec11a3e5b100c",
            "filename": "docs/source/en/model_doc/hubert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -50,7 +50,7 @@ This model was contributed by [patrickvonplaten](https://huggingface.co/patrickv\n - Hubert is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\n - Hubert model was fine-tuned using connectionist temporal classification (CTC) so the model output has to be decoded\n   using [`Wav2Vec2CTCTokenizer`].\n-\n+- The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`  \n \n ## Using Flash Attention 2\n "
        },
        {
            "sha": "550916b5586e4a8e30d0f9766743102ec6c17dd5",
            "filename": "docs/source/en/model_doc/m2m_100.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -51,6 +51,9 @@ multilingual it expects the sequences in a certain format: A special language id\n source and target text. The source text format is `[lang_code] X [eos]`, where `lang_code` is source language\n id for source text and target language id for target text, with `X` being the source or target text.\n \n+> [!NOTE]\n+> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n+\n The [`M2M100Tokenizer`] depends on `sentencepiece` so be sure to install it before running the\n examples. To install `sentencepiece` run `pip install sentencepiece`.\n "
        },
        {
            "sha": "3bd3ca0bc6d9f814d7cd98854bf657d75c2114a8",
            "filename": "docs/source/en/model_doc/mbart.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fmbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fmbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmbart.md?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -35,6 +35,9 @@ You can find all the original mBART checkpoints under the [AI at Meta](https://h\n > [!TIP]\n > Click on the mBART models in the right sidebar for more examples of applying mBART to different language tasks.\n \n+> [!NOTE]\n+> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n+\n The example below demonstrates how to translate text with [`Pipeline`] or the [`AutoModel`] class.\n \n <hfoptions id=\"usage\">"
        },
        {
            "sha": "917347af102c9308333a29c09a35ed2086b18310",
            "filename": "docs/source/en/model_doc/musicgen.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -62,6 +62,9 @@ python src/transformers/models/musicgen/convert_musicgen_transformers.py \\\n     --checkpoint small --pytorch_dump_folder /output/path --safe_serialization \n ```\n \n+> [!NOTE]\n+> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n+\n ## Generation\n \n MusicGen is compatible with two generation modes: greedy and sampling. In practice, sampling leads to significantly"
        },
        {
            "sha": "eea1a184e866877f9118ef00c2b8552ca019d890",
            "filename": "docs/source/en/model_doc/musicgen_melody.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -44,6 +44,9 @@ There are two key differences with MusicGen:\n 1. The audio prompt is used here as a conditional signal for the generated audio sample, whereas it's used for audio continuation in [MusicGen](https://huggingface.co/docs/transformers/main/en/model_doc/musicgen).\n 2. Conditional text and audio signals are concatenated to the decoder's hidden states instead of being used as a cross-attention signal, as in MusicGen.\n \n+> [!NOTE]\n+> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n+\n ## Generation\n \n MusicGen Melody is compatible with two generation modes: greedy and sampling. In practice, sampling leads to significantly better results than greedy, thus we encourage sampling mode to be used where possible. Sampling is enabled by default, and can be explicitly specified by setting `do_sample=True` in the call to [`MusicgenMelodyForConditionalGeneration.generate`], or by overriding the model's generation config (see below)."
        },
        {
            "sha": "8d72403aba2fe4de7c19b4fe846fb8761a6473d7",
            "filename": "docs/source/en/model_doc/opt.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -41,6 +41,9 @@ Tips:\n - OPT has the same architecture as [`BartDecoder`].\n - Contrary to GPT2, OPT adds the EOS token `</s>` to the beginning of every prompt.\n \n+> [!NOTE]\n+> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n+\n ## Resources\n \n A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with OPT. If you're"
        },
        {
            "sha": "03a54f3893594349048f7916c9fc316534c0629f",
            "filename": "docs/source/en/model_doc/qwen2_audio.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -40,6 +40,9 @@ The abstract from the paper is the following:\n \n `Qwen2-Audio-7B` and `Qwen2-Audio-7B-Instruct` can be found on the [Huggingface Hub](https://huggingface.co/Qwen)\n \n+> [!NOTE]\n+> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n+\n ### Inference\n \n ```python"
        },
        {
            "sha": "660d8176c285b6dc19fdef1078e1354da5287a00",
            "filename": "docs/source/en/model_doc/sew.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew.md?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -46,6 +46,9 @@ This model was contributed by [anton-l](https://huggingface.co/anton-l).\n - SEWForCTC is fine-tuned using connectionist temporal classification (CTC) so the model output has to be decoded using\n   [`Wav2Vec2CTCTokenizer`].\n \n+> [!NOTE]\n+> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n+\n ## Resources\n \n - [Audio classification task guide](../tasks/audio_classification)"
        },
        {
            "sha": "c526bb434d3d301889f7a968c26ebabfb52ea38c",
            "filename": "docs/source/en/model_doc/unispeech-sat.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech-sat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech-sat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech-sat.md?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -54,6 +54,9 @@ found [here](https://github.com/microsoft/UniSpeech/tree/main/UniSpeech-SAT).\n   decoded using [`Wav2Vec2CTCTokenizer`].\n - UniSpeechSat performs especially well on speaker verification, speaker identification, and speaker diarization tasks.\n \n+> [!NOTE]\n+> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n+\n ## Resources\n \n - [Audio classification task guide](../tasks/audio_classification)"
        },
        {
            "sha": "9f23656b229a9e6805352e2053a2a3b9b0731f1c",
            "filename": "docs/source/en/model_doc/unispeech.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech.md?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -49,6 +49,9 @@ found [here](https://github.com/microsoft/UniSpeech/tree/main/UniSpeech).\n - UniSpeech model can be fine-tuned using connectionist temporal classification (CTC) so the model output has to be\n   decoded using [`Wav2Vec2CTCTokenizer`].\n \n+> [!NOTE]\n+> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n+\n ## Resources\n \n - [Audio classification task guide](../tasks/audio_classification)"
        },
        {
            "sha": "a5fedef0f722daa77b1cfba8ff5cc4a9629ced6d",
            "filename": "docs/source/en/model_doc/wav2vec2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -50,6 +50,9 @@ Note: Meta (FAIR) released a new version of [Wav2Vec2-BERT 2.0](https://huggingf\n - Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\n   using [`Wav2Vec2CTCTokenizer`].\n \n+> [!NOTE]\n+> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n+\n ## Using Flash Attention 2\n \n Flash Attention 2 is an faster, optimized version of the model."
        },
        {
            "sha": "3aa6e5c3018ec947bfe4e141f0fb88de5d6c0bcc",
            "filename": "docs/source/en/model_doc/whisper.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -32,6 +32,9 @@ rendered properly in your Markdown viewer.\n \n You can find all the original Whisper checkpoints under the [Whisper](https://huggingface.co/collections/openai/whisper-release-6501bba2cf999715fd953013) collection.\n \n+> [!NOTE]\n+> The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n+\n > [!TIP]\n > Click on the Whisper models in the right sidebar for more examples of how to apply Whisper to different audio tasks.\n "
        },
        {
            "sha": "11fd1f939ccd9ad81b223beca06dd6b20e8dad38",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -367,15 +367,15 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: bool = False,\n     ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n-        if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n+        if self.position_embedding_type != \"absolute\" or output_attentions:\n             logger.warning(\n                 \"AlbertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to \"\n+                \"non-absolute `position_embedding_type` or `output_attentions=True` . Falling back to \"\n                 \"the eager attention implementation, but specifying the eager implementation will be required from \"\n                 \"Transformers version v5.0.0 onwards. This warning can be removed using the argument \"\n                 '`attn_implementation=\"eager\"` when loading the model.'\n             )\n-            return super().forward(hidden_states, attention_mask, head_mask, output_attentions)\n+            return super().forward(hidden_states, attention_mask, output_attentions=output_attentions)\n \n         batch_size, seq_len, _ = hidden_states.size()\n         query_layer = self.transpose_for_scores(self.query(hidden_states))"
        },
        {
            "sha": "d81978f89c7ea16f17d2b7fae4a4d4750776c968",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -290,10 +290,6 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # BartFlashAttention2 attention does not support output_attentions\n-        if output_attentions:\n-            raise ValueError(\"BartFlashAttention2 attention does not support output_attentions\")\n-\n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n@@ -400,18 +396,17 @@ def forward(\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions or layer_head_mask is not None:\n+        if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"BartModel is using BartSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n+                \"BartModel is using BartSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n                 ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n                 hidden_states,\n                 key_value_states=key_value_states,\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n-                layer_head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n             )\n "
        },
        {
            "sha": "68de3b509e271a9a6227a74d59447d7c5e46a639",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -253,18 +253,17 @@ def forward(\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions or layer_head_mask is not None:\n+        if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"BioGptModel is using BioGptSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n+                \"BioGptModel is using BioGptSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n                 ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n                 hidden_states,\n                 key_value_states=key_value_states,\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n-                layer_head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n             )\n "
        },
        {
            "sha": "e1a822ea0377837fef8ebb5dc33ff876c07bff25",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -352,10 +352,6 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # Data2VecAudioFlashAttention2 attention does not support output_attentions\n-        if output_attentions:\n-            raise ValueError(\"Data2VecAudioFlashAttention2 attention does not support output_attentions\")\n-\n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n@@ -462,18 +458,17 @@ def forward(\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions or layer_head_mask is not None:\n+        if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"Data2VecAudioModel is using Data2VecAudioSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n+                \"Data2VecAudioModel is using Data2VecAudioSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n                 ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n                 hidden_states,\n                 key_value_states=key_value_states,\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n-                layer_head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n             )\n "
        },
        {
            "sha": "7f40aabefb5bd44350e16a7b6cb2386e41b31220",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 5,
            "deletions": 11,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -391,13 +391,7 @@ def forward(\n \n \n class GPTBigCodeSdpaAttention(GPTBigCodeAttention):\n-    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n-        if head_mask is not None:\n-            # The super dispatch is done in the forward.\n-            raise ValueError(\n-                \"PyTorch SDPA does not support head_mask. Please open an issue in Transformers repository.\"\n-            )\n-\n+    def _attn(self, query, key, value, attention_mask=None):\n         scale = None\n         if not self.scale_attn_weights:\n             scale = 1\n@@ -507,17 +501,17 @@ def forward(\n \n         key, value = key_value.split((self.head_dim, self.head_dim), dim=-1)\n \n-        if not output_attentions and head_mask is None:\n+        if not output_attentions:\n             # Difference with the original implementation: there is no need to transpose the key here,\n             # as SDPA expects seq_length to be at index -2 for the key as well\n-            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n+            attn_output, attn_weights = self._attn(query, key, value, attention_mask)\n         else:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"GPTBigCodeModel is using GPTBigCodeSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` and `head_mask` not None.\"\n+                \"GPTBigCodeModel is using GPTBigCodeSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`.\"\n                 ' Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n-            attn_output, attn_weights = super()._attn(query, key.transpose(-1, -2), value, attention_mask, head_mask)\n+            attn_output, attn_weights = super()._attn(query, key.transpose(-1, -2), value, attention_mask)\n \n         if not self.multi_query:\n             attn_output = attn_output.transpose(1, 2).reshape(hidden_states.shape)"
        },
        {
            "sha": "d920e998f97fd79c08b64e31ab0bb3f8ebcb5f38",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -409,10 +409,6 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # HubertFlashAttention2 attention does not support output_attentions\n-        if output_attentions:\n-            raise ValueError(\"HubertFlashAttention2 attention does not support output_attentions\")\n-\n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n@@ -519,18 +515,17 @@ def forward(\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions or layer_head_mask is not None:\n+        if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"HubertModel is using HubertSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n+                \"HubertModel is using HubertSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n                 ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n                 hidden_states,\n                 key_value_states=key_value_states,\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n-                layer_head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n             )\n "
        },
        {
            "sha": "39503193be7108b32f0ae638774309a9a6b57abf",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -360,10 +360,6 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # M2M100FlashAttention2 attention does not support output_attentions\n-        if output_attentions:\n-            raise ValueError(\"M2M100FlashAttention2 attention does not support output_attentions\")\n-\n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n@@ -471,18 +467,17 @@ def forward(\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions or layer_head_mask is not None:\n+        if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"M2M100Model is using M2M100SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n+                \"M2M100Model is using M2M100SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n                 ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n                 hidden_states,\n                 key_value_states=key_value_states,\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n-                layer_head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n             )\n \n@@ -1270,11 +1265,6 @@ def __init__(self, config: M2M100Config):\n         self.encoder = M2M100Encoder(config, self.shared)\n         self.decoder = M2M100Decoder(config, self.shared)\n \n-        if config._attn_implementation == \"flash_attention_2\":\n-            logger.warning_once(\n-                \"Attention with Flash Attention 2 does not support `layer_head_mask`. If you need this feature, please use standard attention.\"\n-            )\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n "
        },
        {
            "sha": "88c0d79af34cf22ed91a27835c76f1a3f3631c39",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -297,10 +297,6 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # MBartFlashAttention2 attention does not support output_attentions\n-        if output_attentions:\n-            raise ValueError(\"MBartFlashAttention2 attention does not support output_attentions\")\n-\n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n@@ -408,18 +404,17 @@ def forward(\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions or layer_head_mask is not None:\n+        if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"MBartModel is using MBartSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n+                \"MBartModel is using MBartSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n                 ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n                 hidden_states,\n                 key_value_states=key_value_states,\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n-                layer_head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n             )\n "
        },
        {
            "sha": "835756cb770371ffb6d7df3908b4af7edf2cc69a",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -333,10 +333,6 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # MusicgenFlashAttention2 attention does not support output_attentions\n-        if output_attentions:\n-            raise ValueError(\"MusicgenFlashAttention2 attention does not support output_attentions\")\n-\n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n@@ -443,18 +439,18 @@ def forward(\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions or layer_head_mask is not None:\n+\n+        if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"MusicgenModel is using MusicgenSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n+                \"MusicgenModel is using MusicgenSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention\"\n                 ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n                 hidden_states,\n                 key_value_states=key_value_states,\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n-                layer_head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n             )\n \n@@ -471,7 +467,6 @@ def forward(\n                 key_value_states=key_value_states,\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n-                layer_head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n             )\n "
        },
        {
            "sha": "84efc89288729df2ff056959b1d68716a70a33cf",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -346,10 +346,6 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # MusicgenMelodyFlashAttention2 attention does not support output_attentions\n-        if output_attentions:\n-            raise ValueError(\"MusicgenMelodyFlashAttention2 attention does not support output_attentions\")\n-\n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n@@ -457,18 +453,17 @@ def forward(\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions or layer_head_mask is not None:\n+        if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"MusicgenMelodyModel is using MusicgenMelodySdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n+                \"MusicgenMelodyModel is using MusicgenMelodySdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n                 ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n                 hidden_states,\n                 key_value_states=key_value_states,\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n-                layer_head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n             )\n "
        },
        {
            "sha": "5218a48e480b9221f7ac2f6eef3a1bd483b8e47d",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -298,16 +298,15 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions or layer_head_mask is not None:\n+        if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"Qwen2AudioModel is using Qwen2AudioSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n+                \"Qwen2AudioModel is using Qwen2AudioSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention\"\n                 ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n                 hidden_states,\n                 attention_mask=attention_mask,\n-                layer_head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n             )\n "
        },
        {
            "sha": "812f8fb4bc7430a9e23b227ef0575022e4a945bb",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -559,10 +559,6 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # SEWFlashAttention2 attention does not support output_attentions\n-        if output_attentions:\n-            raise ValueError(\"SEWFlashAttention2 attention does not support output_attentions\")\n-\n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n@@ -670,18 +666,17 @@ def forward(\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions or layer_head_mask is not None:\n+        if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"SEWModel is using SEWSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n+                \"SEWModel is using SEWSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n                 ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n                 hidden_states,\n                 key_value_states=key_value_states,\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n-                layer_head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n             )\n "
        },
        {
            "sha": "071db33fdcd430d328f865b337f5fe0a0684c8bc",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -448,10 +448,6 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # UniSpeechFlashAttention2 attention does not support output_attentions\n-        if output_attentions:\n-            raise ValueError(\"UniSpeechFlashAttention2 attention does not support output_attentions\")\n-\n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n@@ -558,18 +554,17 @@ def forward(\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions or layer_head_mask is not None:\n+        if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"UniSpeechModel is using UniSpeechSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n+                \"UniSpeechModel is using UniSpeechSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n                 ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n                 hidden_states,\n                 key_value_states=key_value_states,\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n-                layer_head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n             )\n "
        },
        {
            "sha": "8e67b41a77f33878f84029c579fb7f203d2b9ea0",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -451,10 +451,6 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # UniSpeechSatFlashAttention2 attention does not support output_attentions\n-        if output_attentions:\n-            raise ValueError(\"UniSpeechSatFlashAttention2 attention does not support output_attentions\")\n-\n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n@@ -561,18 +557,17 @@ def forward(\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions or layer_head_mask is not None:\n+        if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"UniSpeechSatModel is using UniSpeechSatSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n+                \"UniSpeechSatModel is using UniSpeechSatSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n                 ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n                 hidden_states,\n                 key_value_states=key_value_states,\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n-                layer_head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n             )\n "
        },
        {
            "sha": "4ad2e9144490b58a336c347afc0ed0cc157271f6",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -652,10 +652,6 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # Wav2Vec2FlashAttention2 attention does not support output_attentions\n-        if output_attentions:\n-            raise ValueError(\"Wav2Vec2FlashAttention2 attention does not support output_attentions\")\n-\n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n@@ -763,18 +759,17 @@ def forward(\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions or layer_head_mask is not None:\n+        if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"Wav2Vec2Model is using Wav2Vec2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n+                \"Wav2Vec2Model is using Wav2Vec2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` . Falling back to the manual attention\"\n                 ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n                 hidden_states,\n                 key_value_states=key_value_states,\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n-                layer_head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n             )\n "
        },
        {
            "sha": "8d354f4f92cd029326902a4463ecb1c7b29993c2",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -373,9 +373,6 @@ def forward(\n                 \"The `static` cache implementation is not compatible with `attn_implementation='flash_attention_2'`. \"\n                 \"Use `attn_implementation='sdpa'` in the meantime, and open an issue at https://github.com/huggingface/transformers\"\n             )\n-        # WhisperFlashAttention2 attention does not support output_attentions\n-        if output_attentions:\n-            raise ValueError(\"WhisperFlashAttention2 attention does not support output_attentions\")\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n@@ -477,18 +474,18 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions or layer_head_mask is not None:\n+\n+        if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"WhisperModel is using WhisperSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n+                \"WhisperModel is using WhisperSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention\"\n                 ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n                 hidden_states,\n                 key_value_states=key_value_states,\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n-                layer_head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )"
        },
        {
            "sha": "aafa003ee670ab25121f1159dec57bf4440b666a",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -1414,6 +1414,7 @@ def test_generate_with_head_masking(self):\n         attention_names = [\"encoder_attentions\", \"decoder_attentions\", \"cross_attentions\"]\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            config._attn_implementation = \"eager\"  # head mask works only in eager mode and will be removed soon\n             text_config = config.get_text_config()\n             if self.has_attentions:\n                 config._attn_implementation = \"eager\"  # can't output attentions otherwise"
        },
        {
            "sha": "6d80b84131089d47d5ac2bc0676fa84de647c888",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -58,28 +58,16 @@ def prepare_bart_inputs_dict(\n     decoder_input_ids=None,\n     attention_mask=None,\n     decoder_attention_mask=None,\n-    head_mask=None,\n-    decoder_head_mask=None,\n-    cross_attn_head_mask=None,\n ):\n     if attention_mask is None:\n         attention_mask = input_ids.ne(config.pad_token_id)\n     if decoder_attention_mask is None:\n         decoder_attention_mask = decoder_input_ids.ne(config.pad_token_id)\n-    if head_mask is None:\n-        head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads, device=torch_device)\n-    if decoder_head_mask is None:\n-        decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads, device=torch_device)\n-    if cross_attn_head_mask is None:\n-        cross_attn_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads, device=torch_device)\n     return {\n         \"input_ids\": input_ids,\n         \"decoder_input_ids\": decoder_input_ids,\n         \"attention_mask\": attention_mask,\n         \"decoder_attention_mask\": attention_mask,\n-        \"head_mask\": head_mask,\n-        \"decoder_head_mask\": decoder_head_mask,\n-        \"cross_attn_head_mask\": cross_attn_head_mask,\n     }\n \n \n@@ -167,10 +155,9 @@ def create_and_check_decoder_model_past_large_inputs(self, config, inputs_dict):\n         model = BartModel(config=config).get_decoder().to(torch_device).eval()\n         input_ids = inputs_dict[\"input_ids\"]\n         attention_mask = inputs_dict[\"attention_mask\"]\n-        head_mask = inputs_dict[\"head_mask\"]\n \n         # first forward pass\n-        outputs = model(input_ids, attention_mask=attention_mask, head_mask=head_mask, use_cache=True)\n+        outputs = model(input_ids, attention_mask=attention_mask, use_cache=True)\n \n         output, past_key_values = outputs.to_tuple()\n "
        },
        {
            "sha": "63b2e8bd6d8aa0eb869b2a14a255d24b34c48bb1",
            "filename": "tests/models/bart/test_modeling_tf_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fbart%2Ftest_modeling_tf_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fbart%2Ftest_modeling_tf_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_tf_bart.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -119,11 +119,10 @@ def check_decoder_model_past_large_inputs(self, config, inputs_dict):\n \n         input_ids = input_ids[:1, :]\n         attention_mask = inputs_dict[\"attention_mask\"][:1, :]\n-        head_mask = inputs_dict[\"head_mask\"]\n         self.batch_size = 1\n \n         # first forward pass\n-        outputs = model(input_ids, attention_mask=attention_mask, head_mask=head_mask, use_cache=True)\n+        outputs = model(input_ids, attention_mask=attention_mask, use_cache=True)\n \n         output, past_key_values = outputs.to_tuple()\n \n@@ -158,9 +157,6 @@ def prepare_bart_inputs_dict(\n     decoder_input_ids,\n     attention_mask=None,\n     decoder_attention_mask=None,\n-    head_mask=None,\n-    decoder_head_mask=None,\n-    cross_attn_head_mask=None,\n ):\n     if attention_mask is None:\n         attention_mask = tf.cast(tf.math.not_equal(input_ids, config.pad_token_id), tf.int8)\n@@ -172,20 +168,11 @@ def prepare_bart_inputs_dict(\n             ],\n             axis=-1,\n         )\n-    if head_mask is None:\n-        head_mask = tf.ones((config.encoder_layers, config.encoder_attention_heads))\n-    if decoder_head_mask is None:\n-        decoder_head_mask = tf.ones((config.decoder_layers, config.decoder_attention_heads))\n-    if cross_attn_head_mask is None:\n-        cross_attn_head_mask = tf.ones((config.decoder_layers, config.decoder_attention_heads))\n     return {\n         \"input_ids\": input_ids,\n         \"decoder_input_ids\": decoder_input_ids,\n         \"attention_mask\": attention_mask,\n         \"decoder_attention_mask\": decoder_attention_mask,\n-        \"head_mask\": head_mask,\n-        \"decoder_head_mask\": decoder_head_mask,\n-        \"cross_attn_head_mask\": cross_attn_head_mask,\n     }\n \n "
        },
        {
            "sha": "e359c1d6d75b75985ad64e0ad985adb60bea2025",
            "filename": "tests/models/biogpt/test_modeling_biogpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -135,9 +135,7 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_biogpt_model_attention_mask_past(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, *args\n-    ):\n+    def create_and_check_biogpt_model_attention_mask_past(self, config, input_ids, input_mask, token_type_ids, *args):\n         model = BioGptModel(config=config)\n         model.to(torch_device)\n         model.eval()\n@@ -177,9 +175,7 @@ def create_and_check_biogpt_model_attention_mask_past(\n         # test that outputs are equal for slice\n         self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n \n-    def create_and_check_biogpt_model_past_large_inputs(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, *args\n-    ):\n+    def create_and_check_biogpt_model_past_large_inputs(self, config, input_ids, input_mask, token_type_ids, *args):\n         model = BioGptModel(config=config).to(torch_device).eval()\n \n         attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n@@ -213,7 +209,7 @@ def create_and_check_biogpt_model_past_large_inputs(\n         self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n \n     def create_and_check_forward_and_backwards(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, *args, gradient_checkpointing=False\n+        self, config, input_ids, input_mask, token_type_ids, *args, gradient_checkpointing=False\n     ):\n         model = BioGptForCausalLM(config)\n         model.to(torch_device)\n@@ -233,9 +229,7 @@ def create_and_check_biogpt_weight_initialization(self, config, *args):\n                 self.parent.assertLessEqual(abs(torch.std(model.state_dict()[key]) - model_std), 0.001)\n                 self.parent.assertLessEqual(abs(torch.mean(model.state_dict()[key]) - 0.0), 0.01)\n \n-    def create_and_check_biogpt_for_token_classification(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, *args\n-    ):\n+    def create_and_check_biogpt_for_token_classification(self, config, input_ids, input_mask, token_type_ids, *args):\n         config.num_labels = self.num_labels\n         model = BioGptForTokenClassification(config)\n         model.to(torch_device)"
        },
        {
            "sha": "8eeb65de7700a7da1836b698ec190f59be9020d9",
            "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 9,
            "deletions": 14,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -128,13 +128,10 @@ def prepare_config_and_inputs(\n             reorder_and_upcast_attn=reorder_and_upcast_attn,\n         )\n \n-        head_mask = ids_tensor([self.num_hidden_layers, self.num_attention_heads], 2)\n-\n         return (\n             config,\n             input_ids,\n             input_mask,\n-            head_mask,\n             token_type_ids,\n             mc_token_ids,\n             sequence_labels,\n@@ -174,19 +171,19 @@ def get_pipeline_config(self):\n         config.vocab_size = 300\n         return config\n \n-    def create_and_check_gpt_bigcode_model(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n+    def create_and_check_gpt_bigcode_model(self, config, input_ids, input_mask, token_type_ids, *args):\n         model = GPTBigCodeModel(config=config)\n         model.to(torch_device)\n         model.eval()\n \n-        result = model(input_ids, token_type_ids=token_type_ids, head_mask=head_mask)\n+        result = model(input_ids, token_type_ids=token_type_ids)\n         result = model(input_ids, token_type_ids=token_type_ids)\n         result = model(input_ids)\n \n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n         self.parent.assertEqual(len(result.past_key_values), config.n_layer)\n \n-    def create_and_check_gpt_bigcode_model_past(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n+    def create_and_check_gpt_bigcode_model_past(self, config, input_ids, input_mask, token_type_ids, *args):\n         model = GPTBigCodeModel(config=config)\n         model.to(torch_device)\n         model.eval()\n@@ -223,7 +220,7 @@ def create_and_check_gpt_bigcode_model_past(self, config, input_ids, input_mask,\n         self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n \n     def create_and_check_gpt_bigcode_model_attention_mask_past(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, *args\n+        self, config, input_ids, input_mask, token_type_ids, *args\n     ):\n         model = GPTBigCodeModel(config=config)\n         model.to(torch_device)\n@@ -265,7 +262,7 @@ def create_and_check_gpt_bigcode_model_attention_mask_past(\n         self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n \n     def create_and_check_gpt_bigcode_model_past_large_inputs(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, *args\n+        self, config, input_ids, input_mask, token_type_ids, *args\n     ):\n         model = GPTBigCodeModel(config=config)\n         model.to(torch_device)\n@@ -302,7 +299,7 @@ def create_and_check_gpt_bigcode_model_past_large_inputs(\n         # test that outputs are equal for slice\n         self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n \n-    def create_and_check_lm_head_model(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n+    def create_and_check_lm_head_model(self, config, input_ids, input_mask, token_type_ids, *args):\n         model = GPTBigCodeForCausalLM(config)\n         model.to(torch_device)\n         model.eval()\n@@ -312,7 +309,7 @@ def create_and_check_lm_head_model(self, config, input_ids, input_mask, head_mas\n         self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n \n     def create_and_check_forward_and_backwards(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, *args, gradient_checkpointing=False\n+        self, config, input_ids, input_mask, token_type_ids, *args, gradient_checkpointing=False\n     ):\n         model = GPTBigCodeForCausalLM(config)\n         model.to(torch_device)\n@@ -325,7 +322,7 @@ def create_and_check_forward_and_backwards(\n         result.loss.backward()\n \n     def create_and_check_gpt_bigcode_for_sequence_classification(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, mc_token_ids, sequence_labels, *args\n+        self, config, input_ids, input_mask, token_type_ids, mc_token_ids, sequence_labels, *args\n     ):\n         config.num_labels = self.num_labels\n         model = GPTBigCodeForSequenceClassification(config)\n@@ -335,7 +332,7 @@ def create_and_check_gpt_bigcode_for_sequence_classification(\n         self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n \n     def create_and_check_gpt_bigcode_for_token_classification(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, mc_token_ids, sequence_labels, *args\n+        self, config, input_ids, input_mask, token_type_ids, mc_token_ids, sequence_labels, *args\n     ):\n         config.num_labels = self.num_labels\n         model = GPTBigCodeForTokenClassification(config)\n@@ -359,7 +356,6 @@ def prepare_config_and_inputs_for_common(self):\n             config,\n             input_ids,\n             input_mask,\n-            head_mask,\n             token_type_ids,\n             mc_token_ids,\n             sequence_labels,\n@@ -370,7 +366,6 @@ def prepare_config_and_inputs_for_common(self):\n         inputs_dict = {\n             \"input_ids\": input_ids,\n             \"token_type_ids\": token_type_ids,\n-            \"head_mask\": head_mask,\n         }\n \n         return config, inputs_dict"
        },
        {
            "sha": "91bda0d8b54797a0da16c413c3f5ba73824fdf29",
            "filename": "tests/models/m2m_100/test_modeling_m2m_100.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -51,28 +51,16 @@ def prepare_m2m_100_inputs_dict(\n     decoder_input_ids,\n     attention_mask=None,\n     decoder_attention_mask=None,\n-    head_mask=None,\n-    decoder_head_mask=None,\n-    cross_attn_head_mask=None,\n ):\n     if attention_mask is None:\n         attention_mask = input_ids.ne(config.pad_token_id)\n     if decoder_attention_mask is None:\n         decoder_attention_mask = decoder_input_ids.ne(config.pad_token_id)\n-    if head_mask is None:\n-        head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads, device=torch_device)\n-    if decoder_head_mask is None:\n-        decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads, device=torch_device)\n-    if cross_attn_head_mask is None:\n-        cross_attn_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads, device=torch_device)\n     return {\n         \"input_ids\": input_ids,\n         \"decoder_input_ids\": decoder_input_ids,\n         \"attention_mask\": attention_mask,\n         \"decoder_attention_mask\": attention_mask,\n-        \"head_mask\": head_mask,\n-        \"decoder_head_mask\": decoder_head_mask,\n-        \"cross_attn_head_mask\": cross_attn_head_mask,\n     }\n \n \n@@ -166,10 +154,9 @@ def create_and_check_decoder_model_past_large_inputs(self, config, inputs_dict):\n         model = M2M100Model(config=config).get_decoder().to(torch_device).eval()\n         input_ids = inputs_dict[\"input_ids\"]\n         attention_mask = inputs_dict[\"attention_mask\"]\n-        head_mask = inputs_dict[\"head_mask\"]\n \n         # first forward pass\n-        outputs = model(input_ids, attention_mask=attention_mask, head_mask=head_mask, use_cache=True)\n+        outputs = model(input_ids, attention_mask=attention_mask, use_cache=True)\n \n         output, past_key_values = outputs.to_tuple()\n "
        },
        {
            "sha": "9ec4d56a1a1c97f65ef5b4a360b3975f38855f5e",
            "filename": "tests/models/mbart/test_modeling_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -55,28 +55,16 @@ def prepare_mbart_inputs_dict(\n     decoder_input_ids,\n     attention_mask=None,\n     decoder_attention_mask=None,\n-    head_mask=None,\n-    decoder_head_mask=None,\n-    cross_attn_head_mask=None,\n ):\n     if attention_mask is None:\n         attention_mask = input_ids.ne(config.pad_token_id)\n     if decoder_attention_mask is None:\n         decoder_attention_mask = decoder_input_ids.ne(config.pad_token_id)\n-    if head_mask is None:\n-        head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads, device=torch_device)\n-    if decoder_head_mask is None:\n-        decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads, device=torch_device)\n-    if cross_attn_head_mask is None:\n-        cross_attn_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads, device=torch_device)\n     return {\n         \"input_ids\": input_ids,\n         \"decoder_input_ids\": decoder_input_ids,\n         \"attention_mask\": attention_mask,\n         \"decoder_attention_mask\": attention_mask,\n-        \"head_mask\": head_mask,\n-        \"decoder_head_mask\": decoder_head_mask,\n-        \"cross_attn_head_mask\": cross_attn_head_mask,\n     }\n \n \n@@ -158,10 +146,9 @@ def create_and_check_decoder_model_past_large_inputs(self, config, inputs_dict):\n         model = MBartModel(config=config).get_decoder().to(torch_device).eval()\n         input_ids = inputs_dict[\"input_ids\"]\n         attention_mask = inputs_dict[\"attention_mask\"]\n-        head_mask = inputs_dict[\"head_mask\"]\n \n         # first forward pass\n-        outputs = model(input_ids, attention_mask=attention_mask, head_mask=head_mask, use_cache=True)\n+        outputs = model(input_ids, attention_mask=attention_mask, use_cache=True)\n \n         output, past_key_values = outputs.to_tuple()\n "
        },
        {
            "sha": "1e2986f7b531ac8df3d3ba00aaa562d8be55789e",
            "filename": "tests/models/mbart/test_modeling_tf_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fmbart%2Ftest_modeling_tf_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fmbart%2Ftest_modeling_tf_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_tf_mbart.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -107,11 +107,10 @@ def check_decoder_model_past_large_inputs(self, config, inputs_dict):\n \n         input_ids = input_ids[:1, :]\n         attention_mask = inputs_dict[\"attention_mask\"][:1, :]\n-        head_mask = inputs_dict[\"head_mask\"]\n         self.batch_size = 1\n \n         # first forward pass\n-        outputs = model(input_ids, attention_mask=attention_mask, head_mask=head_mask, use_cache=True)\n+        outputs = model(input_ids, attention_mask=attention_mask, use_cache=True)\n \n         output, past_key_values = outputs.to_tuple()\n         past_key_values = past_key_values[1]\n@@ -123,9 +122,6 @@ def prepare_mbart_inputs_dict(\n     decoder_input_ids,\n     attention_mask=None,\n     decoder_attention_mask=None,\n-    head_mask=None,\n-    decoder_head_mask=None,\n-    cross_attn_head_mask=None,\n ):\n     if attention_mask is None:\n         attention_mask = tf.cast(tf.math.not_equal(input_ids, config.pad_token_id), tf.int8)\n@@ -137,20 +133,11 @@ def prepare_mbart_inputs_dict(\n             ],\n             axis=-1,\n         )\n-    if head_mask is None:\n-        head_mask = tf.ones((config.encoder_layers, config.encoder_attention_heads))\n-    if decoder_head_mask is None:\n-        decoder_head_mask = tf.ones((config.decoder_layers, config.decoder_attention_heads))\n-    if cross_attn_head_mask is None:\n-        cross_attn_head_mask = tf.ones((config.decoder_layers, config.decoder_attention_heads))\n     return {\n         \"input_ids\": input_ids,\n         \"decoder_input_ids\": decoder_input_ids,\n         \"attention_mask\": attention_mask,\n         \"decoder_attention_mask\": decoder_attention_mask,\n-        \"head_mask\": head_mask,\n-        \"decoder_head_mask\": decoder_head_mask,\n-        \"cross_attn_head_mask\": cross_attn_head_mask,\n     }\n \n "
        },
        {
            "sha": "b14df8de0e4c14d37e3ad28157accb8b97031276",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -76,27 +76,19 @@ def prepare_musicgen_decoder_inputs_dict(\n     config,\n     input_ids,\n     attention_mask=None,\n-    head_mask=None,\n     encoder_hidden_states=None,\n     encoder_attention_mask=None,\n-    cross_attn_head_mask=None,\n ):\n     if attention_mask is None:\n         attention_mask = input_ids.reshape(-1, config.num_codebooks, input_ids.shape[-1])[:, 0, :]\n         attention_mask = attention_mask.ne(config.pad_token_id)\n-    if head_mask is None:\n-        head_mask = torch.ones(config.num_hidden_layers, config.num_attention_heads, device=torch_device)\n     if encoder_attention_mask is None and encoder_hidden_states is not None:\n         encoder_attention_mask = torch.ones(encoder_hidden_states.shape[:2], device=torch_device)\n-    if cross_attn_head_mask is None:\n-        cross_attn_head_mask = torch.ones(config.num_hidden_layers, config.num_attention_heads, device=torch_device)\n     return {\n         \"input_ids\": input_ids,\n         \"attention_mask\": attention_mask,\n         \"encoder_hidden_states\": encoder_hidden_states,\n         \"encoder_attention_mask\": encoder_attention_mask,\n-        \"head_mask\": head_mask,\n-        \"cross_attn_head_mask\": cross_attn_head_mask,\n     }\n \n \n@@ -467,36 +459,18 @@ def prepare_musicgen_inputs_dict(\n     decoder_input_ids,\n     attention_mask=None,\n     decoder_attention_mask=None,\n-    head_mask=None,\n-    decoder_head_mask=None,\n-    cross_attn_head_mask=None,\n     labels=None,\n ):\n     if decoder_attention_mask is None:\n         decoder_attention_mask = decoder_input_ids.reshape(\n             -1, config.decoder.num_codebooks, decoder_input_ids.shape[-1]\n         )[:, 0, :]\n         decoder_attention_mask = decoder_attention_mask.ne(config.decoder.pad_token_id)\n-    if head_mask is None:\n-        head_mask = torch.ones(\n-            config.text_encoder.num_hidden_layers, config.text_encoder.num_attention_heads, device=torch_device\n-        )\n-    if decoder_head_mask is None:\n-        decoder_head_mask = torch.ones(\n-            config.decoder.num_hidden_layers, config.decoder.num_attention_heads, device=torch_device\n-        )\n-    if cross_attn_head_mask is None:\n-        cross_attn_head_mask = torch.ones(\n-            config.decoder.num_hidden_layers, config.decoder.num_attention_heads, device=torch_device\n-        )\n     return {\n         \"input_ids\": input_ids,\n         \"attention_mask\": attention_mask,\n         \"decoder_input_ids\": decoder_input_ids,\n         \"decoder_attention_mask\": decoder_attention_mask,\n-        \"head_mask\": head_mask,\n-        \"decoder_head_mask\": decoder_head_mask,\n-        \"cross_attn_head_mask\": cross_attn_head_mask,\n         \"labels\": labels,\n     }\n "
        },
        {
            "sha": "72cbb990c9ae04936a31004ec3c4b357e64c772f",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -80,23 +80,19 @@ def prepare_musicgen_melody_decoder_inputs_dict(\n     config,\n     input_ids,\n     attention_mask=None,\n-    head_mask=None,\n     encoder_hidden_states=None,\n     encoder_attention_mask=None,\n ):\n     if attention_mask is None:\n         attention_mask = input_ids.reshape(-1, config.num_codebooks, input_ids.shape[-1])[:, 0, :]\n         attention_mask = attention_mask.ne(config.pad_token_id)\n-    if head_mask is None:\n-        head_mask = torch.ones(config.num_hidden_layers, config.num_attention_heads, device=torch_device)\n     if encoder_attention_mask is None and encoder_hidden_states is not None:\n         encoder_attention_mask = torch.ones(encoder_hidden_states.shape[:2], device=torch_device)\n     return {\n         \"input_ids\": input_ids,\n         \"attention_mask\": attention_mask,\n         \"encoder_hidden_states\": encoder_hidden_states,\n         \"encoder_attention_mask\": encoder_attention_mask,\n-        \"head_mask\": head_mask,\n     }\n \n \n@@ -475,30 +471,18 @@ def prepare_musicgen_melody_inputs_dict(\n     decoder_input_ids,\n     attention_mask=None,\n     decoder_attention_mask=None,\n-    head_mask=None,\n-    decoder_head_mask=None,\n     labels=None,\n ):\n     if decoder_attention_mask is None:\n         decoder_attention_mask = decoder_input_ids.reshape(\n             -1, config.decoder.num_codebooks, decoder_input_ids.shape[-1]\n         )[:, 0, :]\n         decoder_attention_mask = decoder_attention_mask.ne(config.decoder.pad_token_id)\n-    if head_mask is None:\n-        head_mask = torch.ones(\n-            config.text_encoder.num_hidden_layers, config.text_encoder.num_attention_heads, device=torch_device\n-        )\n-    if decoder_head_mask is None:\n-        decoder_head_mask = torch.ones(\n-            config.decoder.num_hidden_layers, config.decoder.num_attention_heads, device=torch_device\n-        )\n     return {\n         \"input_ids\": input_ids,\n         \"attention_mask\": attention_mask,\n         \"decoder_input_ids\": decoder_input_ids,\n         \"decoder_attention_mask\": decoder_attention_mask,\n-        \"head_mask\": head_mask,\n-        \"decoder_head_mask\": decoder_head_mask,\n         \"labels\": labels,\n     }\n "
        },
        {
            "sha": "3ec267552ccec1ccc37334835f60074c30f50133",
            "filename": "tests/models/opt/test_modeling_opt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -52,15 +52,12 @@ def prepare_opt_inputs_dict(\n     decoder_input_ids=None,\n     attention_mask=None,\n     decoder_attention_mask=None,\n-    head_mask=None,\n-    decoder_head_mask=None,\n ):\n     if attention_mask is None:\n         attention_mask = input_ids.ne(config.pad_token_id)\n     return {\n         \"input_ids\": input_ids,\n         \"attention_mask\": attention_mask,\n-        \"head_mask\": head_mask,\n     }\n \n \n@@ -156,10 +153,9 @@ def create_and_check_decoder_model_past_large_inputs(self, config, inputs_dict):\n \n         input_ids = inputs_dict[\"input_ids\"]\n         attention_mask = inputs_dict[\"attention_mask\"]\n-        head_mask = inputs_dict[\"head_mask\"]\n \n         # first forward pass\n-        outputs = model(input_ids, attention_mask=attention_mask, head_mask=head_mask, use_cache=True)\n+        outputs = model(input_ids, attention_mask=attention_mask, use_cache=True)\n \n         output, past_key_values = outputs.to_tuple()\n \n@@ -187,7 +183,7 @@ def create_and_check_decoder_model_past_large_inputs(self, config, inputs_dict):\n         self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n \n         # test no attention_mask works\n-        outputs = model(input_ids, attention_mask=attention_mask, head_mask=head_mask, use_cache=True)\n+        outputs = model(input_ids, attention_mask=attention_mask, use_cache=True)\n         _, past_key_values = outputs.to_tuple()\n         output_from_no_past = model(next_input_ids)[\"last_hidden_state\"]\n "
        },
        {
            "sha": "e978fe2fe1009a366d5bd0a5c47bf52df94d3ad2",
            "filename": "tests/models/whisper/test_modeling_tf_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -62,25 +62,13 @@ def prepare_whisper_inputs_dict(\n     decoder_input_ids,\n     attention_mask=None,\n     decoder_attention_mask=None,\n-    head_mask=None,\n-    decoder_head_mask=None,\n-    cross_attn_head_mask=None,\n ):\n     if decoder_attention_mask is None:\n         decoder_attention_mask = tf.where(decoder_input_ids != config.pad_token_id, 1, 0)\n-    if head_mask is None:\n-        head_mask = tf.ones((config.encoder_layers, config.encoder_attention_heads))\n-    if decoder_head_mask is None:\n-        decoder_head_mask = tf.ones((config.decoder_layers, config.decoder_attention_heads))\n-    if cross_attn_head_mask is None:\n-        cross_attn_head_mask = tf.ones((config.decoder_layers, config.decoder_attention_heads))\n     return {\n         \"input_features\": input_features,\n         \"decoder_input_ids\": decoder_input_ids,\n         \"decoder_attention_mask\": decoder_attention_mask,\n-        \"head_mask\": head_mask,\n-        \"decoder_head_mask\": decoder_head_mask,\n-        \"cross_attn_head_mask\": cross_attn_head_mask,\n     }\n \n \n@@ -350,9 +338,6 @@ def test_inputs_embeds(self):\n     def test_training(self):\n         pass\n \n-    def test_generate_with_head_masking(self):\n-        pass\n-\n     @unittest.skip(\"fp16 is not yet supported for TF models\")\n     def test_generate_fp16(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs()"
        },
        {
            "sha": "6aa31e7206a4f521ab931faea73a646f100e2fe0",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 27,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -159,26 +159,14 @@ def prepare_whisper_inputs_dict(\n     decoder_input_ids,\n     attention_mask=None,\n     decoder_attention_mask=None,\n-    head_mask=None,\n-    decoder_head_mask=None,\n-    cross_attn_head_mask=None,\n ):\n     if decoder_attention_mask is None:\n         decoder_attention_mask = decoder_input_ids.ne(config.pad_token_id)\n-    if head_mask is None:\n-        head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads, device=torch_device)\n-    if decoder_head_mask is None:\n-        decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads, device=torch_device)\n-    if cross_attn_head_mask is None:\n-        cross_attn_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads, device=torch_device)\n     return {\n         # \"input_ids\": input_features,\n         \"input_features\": input_features,\n         \"decoder_input_ids\": decoder_input_ids,\n         \"decoder_attention_mask\": decoder_attention_mask,\n-        \"head_mask\": head_mask,\n-        \"decoder_head_mask\": decoder_head_mask,\n-        \"cross_attn_head_mask\": cross_attn_head_mask,\n     }\n \n \n@@ -3235,12 +3223,6 @@ def test_tiny_static_generation_long_form(self):\n         self.assertTrue((eager_generated_ids[permutation_idx, :] == static_generated_ids).all())\n \n \n-def prepare_whisper_encoder_inputs_dict(config, input_features, head_mask=None):\n-    if head_mask is None:\n-        head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads, device=torch_device)\n-    return {\"input_features\": input_features, \"head_mask\": head_mask}\n-\n-\n @require_torch\n class WhisperEncoderModelTester:\n     def __init__(\n@@ -3314,10 +3296,7 @@ def prepare_config_and_inputs(self):\n         input_features = floats_tensor([self.batch_size, self.num_mel_bins, self.seq_length])\n \n         config = self.get_config()\n-        inputs_dict = prepare_whisper_encoder_inputs_dict(\n-            config,\n-            input_features=input_features,\n-        )\n+        inputs_dict = {\"input_features\": input_features}\n         return config, inputs_dict\n \n     def prepare_config_and_inputs_for_common(self):\n@@ -3427,8 +3406,6 @@ def test_encoder_outputs(self):\n             encoder_inputs = {\"input_features\": inputs[\"input_features\"]}\n             del inputs[\"input_features\"]\n \n-            if \"head_mask\" in inputs:\n-                encoder_inputs[\"head_mask\"] = inputs[\"head_mask\"]\n             if \"attention_mask\" in inputs:\n                 encoder_inputs[\"attention_mask\"] = inputs[\"attention_mask\"]\n             if \"output_attentions\" in inputs:\n@@ -3523,9 +3500,6 @@ def prepare_config_and_inputs(self):\n         )\n \n         inputs_dict.pop(\"input_features\")\n-        inputs_dict.pop(\"head_mask\")\n-        inputs_dict.pop(\"decoder_head_mask\")\n-        inputs_dict.pop(\"cross_attn_head_mask\")\n \n         inputs_dict[\"attention_mask\"] = inputs_dict.pop(\"decoder_attention_mask\")\n         inputs_dict[\"input_ids\"] = inputs_dict.pop(\"decoder_input_ids\")"
        },
        {
            "sha": "1e5714d366312c7567e6e07c9d2ad9376e193967",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/955e61b0da09266fa2ada2e164b703081c8c9940/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=955e61b0da09266fa2ada2e164b703081c8c9940",
            "patch": "@@ -1444,6 +1444,7 @@ def test_headmasking(self):\n         inputs_dict[\"output_attentions\"] = True\n         config.output_hidden_states = True\n         configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n+        configs_no_init._attn_implementation = \"eager\"  # head mask works only in eager mode and will be removed soon\n         for model_class in self.all_model_classes:\n             model = model_class(config=configs_no_init)\n             model.to(torch_device)"
        }
    ],
    "stats": {
        "total": 397,
        "additions": 103,
        "deletions": 294
    }
}