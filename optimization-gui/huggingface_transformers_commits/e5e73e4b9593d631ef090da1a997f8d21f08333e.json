{
    "author": "gante",
    "message": "[docs] Add reference to HF-maintained `custom_generate` collections (#39894)\n\ndecoding -> generation; add collections",
    "sha": "e5e73e4b9593d631ef090da1a997f8d21f08333e",
    "files": [
        {
            "sha": "b32b26a281a05a308e381415f7946268a867d4a4",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 26,
            "deletions": 21,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5e73e4b9593d631ef090da1a997f8d21f08333e/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5e73e4b9593d631ef090da1a997f8d21f08333e/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=e5e73e4b9593d631ef090da1a997f8d21f08333e",
            "patch": "@@ -315,37 +315,37 @@ tokenizer.batch_decode(outputs, skip_special_tokens=True)\n ```\n \n \n-## Custom decoding methods\n+## Custom generation methods\n \n-Custom decoding methods enable specialized generation behavior such as the following:\n+Custom generation methods enable specialized behavior such as:\n - have the model continue thinking if it is uncertain;\n - roll back generation if the model gets stuck;\n - handle special tokens with custom logic;\n-- enhanced input preparation for advanced models;\n+- use specialized KV caches;\n \n-We enable custom decoding methods through model repositories, assuming a specific model tag and file structure (see subsection below). This feature is an extension of [custom modeling code](./models.md#custom-models) and, like such, requires setting `trust_remote_code=True`.\n+We enable custom generation methods through model repositories, assuming a specific model tag and file structure (see subsection below). This feature is an extension of [custom modeling code](./models.md#custom-models) and, like such, requires setting `trust_remote_code=True`.\n \n-If a model repository holds a custom decoding method, the easiest way to try it out is to load the model and generate with it:\n+If a model repository holds a custom generation method, the easiest way to try it out is to load the model and generate with it:\n \n ```py\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n # `transformers-community/custom_generate_example` holds a copy of `Qwen/Qwen2.5-0.5B-Instruct`, but\n-# with custom generation code -> calling `generate` uses the custom decoding method!\n+# with custom generation code -> calling `generate` uses the custom generation method!\n tokenizer = AutoTokenizer.from_pretrained(\"transformers-community/custom_generate_example\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"transformers-community/custom_generate_example\", device_map=\"auto\", trust_remote_code=True\n )\n \n inputs = tokenizer([\"The quick brown\"], return_tensors=\"pt\").to(model.device)\n-# The custom decoding method is a minimal greedy decoding implementation. It also prints a custom message at run time.\n+# The custom generation method is a minimal greedy decoding implementation. It also prints a custom message at run time.\n gen_out = model.generate(**inputs)\n # you should now see its custom message, \"✨ using a custom generation method ✨\"\n print(tokenizer.batch_decode(gen_out, skip_special_tokens=True))\n 'The quick brown fox jumps over a lazy dog, and the dog is a type of animal. Is'\n ```\n \n-Model repositories with custom decoding methods have a special property: their decoding method can be loaded from **any** model through [`~GenerationMixin.generate`]'s `custom_generate` argument. This means anyone can create and share their custom generation method to potentially work with any Transformers model, without requiring users to install additional Python packages.\n+Model repositories with custom generation methods have a special property: their generation method can be loaded from **any** model through [`~GenerationMixin.generate`]'s `custom_generate` argument. This means anyone can create and share their custom generation method to potentially work with any Transformers model, without requiring users to install additional Python packages.\n \n ```py\n from transformers import AutoModelForCausalLM, AutoTokenizer\n@@ -354,7 +354,7 @@ tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", device_map=\"auto\")\n \n inputs = tokenizer([\"The quick brown\"], return_tensors=\"pt\").to(model.device)\n-# `custom_generate` replaces the original `generate` by the custom decoding method defined in\n+# `custom_generate` replaces the original `generate` by the custom generation method defined in\n # `transformers-community/custom_generate_example`\n gen_out = model.generate(**inputs, custom_generate=\"transformers-community/custom_generate_example\", trust_remote_code=True)\n print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\n@@ -364,7 +364,7 @@ print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\n You should read the `README.md` file of the repository containing the custom generation strategy to see what the new arguments and output type differences are, if they exist. Otherwise, you can assume it works like the base [`~GenerationMixin.generate`] method.\n \n > [!TIP]\n-> You can find all custom decoding methods by [searching for their custom tag.](https://huggingface.co/models?other=custom_generate), `custom_generate`\n+> You can find all custom generation methods by [searching for their custom tag.](https://huggingface.co/models?other=custom_generate), `custom_generate`.\n \n Consider the Hub repository [transformers-community/custom_generate_example](https://huggingface.co/transformers-community/custom_generate_example) as an example. The `README.md` states that it has an additional input argument, `left_padding`, which adds a number of padding tokens before the prompt.\n \n@@ -387,11 +387,11 @@ torch>=99.0 (installed: 2.6.0)\n \n Updating your Python requirements accordingly will remove this error message.\n \n-### Creating a custom decoding method\n+### Creating a custom generation method\n \n-To create a new decoding method, you need to create a new [**Model**](https://huggingface.co/new) repository and push a few files into it.\n-1. The model you've designed your decoding method with.\n-2. `custom_generate/generate.py`, which contains all the logic for your custom decoding method.\n+To create a new generation method, you need to create a new [**Model**](https://huggingface.co/new) repository and push a few files into it.\n+1. The model you've designed your generation method with.\n+2. `custom_generate/generate.py`, which contains all the logic for your custom generation method.\n 3. `custom_generate/requirements.txt`, used to optionally add new Python requirements and/or lock specific versions to correctly use your method.\n 4. `README.md`, where you should add the `custom_generate` tag and document any new arguments or output type differences of your custom method here.\n \n@@ -409,7 +409,7 @@ your_repo/\n \n #### Adding the base model\n \n-The starting point for your custom decoding method is a model repository just like any other. The model to add to this repository should be the model you've designed your method with, and it is meant to be part of a working self-contained model-generate pair. When the model in this repository is loaded, your custom decoding method will override `generate`. Don't worry -- your decoding method can still be loaded with any other Transformers model, as explained in the section above.\n+The starting point for your custom generation method is a model repository just like any other. The model to add to this repository should be the model you've designed your method with, and it is meant to be part of a working self-contained model-generate pair. When the model in this repository is loaded, your custom generation method will override `generate`. Don't worry -- your generation method can still be loaded with any other Transformers model, as explained in the section above.\n \n If you simply want to copy an existing model, you can do\n \n@@ -418,13 +418,13 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"source/model_repo\")\n model = AutoModelForCausalLM.from_pretrained(\"source/model_repo\")\n-tokenizer.save_pretrained(\"your/decoding_method\", push_to_hub=True)\n-model.save_pretrained(\"your/decoding_method\", push_to_hub=True)\n+tokenizer.save_pretrained(\"your/generation_method\", push_to_hub=True)\n+model.save_pretrained(\"your/generation_method\", push_to_hub=True)\n ```\n \n #### generate.py\n \n-This is the core of your decoding method. It *must* contain a method named `generate`, and this method *must* contain a `model` argument as its first argument. `model` is the model instance, which means you have access to all attributes and methods in the model, including the ones defined in [`GenerationMixin`] (like the base `generate` method).\n+This is the core of your generation method. It *must* contain a method named `generate`, and this method *must* contain a `model` argument as its first argument. `model` is the model instance, which means you have access to all attributes and methods in the model, including the ones defined in [`GenerationMixin`] (like the base `generate` method).\n \n > [!WARNING]\n > `generate.py` must be placed in a folder named `custom_generate`, and not at the root level of the repository. The file paths for this feature are hardcoded.\n@@ -465,7 +465,7 @@ def generate(model, input_ids, generation_config=None, left_padding=None, **kwar\n     return input_ids\n ```\n \n-Follow the recommended practices below to ensure your custom decoding method works as expected.\n+Follow the recommended practices below to ensure your custom generation method works as expected.\n - Feel free to reuse the logic for validation and input preparation in the original [`~GenerationMixin.generate`].\n - Pin the `transformers` version in the requirements if you use any private method/attribute in `model`.\n - Consider adding model validation, input validation, or even a separate test file to help users sanity-check your code in their environment.\n@@ -476,7 +476,7 @@ Your custom `generate` method can relative import code from the `custom_generate\n from .utils import some_function\n ```\n \n-Only relative imports from the same-level `custom_generate` folder are supported. Parent/sibling folder imports are not valid. The `custom_generate` argument also works locally with any directory that contains a `custom_generate` structure. This is the recommended workflow for developing your custom decoding method.\n+Only relative imports from the same-level `custom_generate` folder are supported. Parent/sibling folder imports are not valid. The `custom_generate` argument also works locally with any directory that contains a `custom_generate` structure. This is the recommended workflow for developing your custom generation method.\n \n \n #### requirements.txt\n@@ -485,7 +485,7 @@ You can optionally specify additional Python requirements in a `requirements.txt\n \n #### README.md\n \n-The root level `README.md` in the model repository usually describes the model therein. However, since the focus of the repository is the custom decoding method, we highly recommend to shift its focus towards describing the custom decoding method. In addition to a description of the method, we recommend documenting any input and/or output differences to the original [`~GenerationMixin.generate`]. This way, users can focus on what's new, and rely on Transformers docs for generic implementation details.\n+The root level `README.md` in the model repository usually describes the model therein. However, since the focus of the repository is the custom generation method, we highly recommend to shift its focus towards describing the custom generation method. In addition to a description of the method, we recommend documenting any input and/or output differences to the original [`~GenerationMixin.generate`]. This way, users can focus on what's new, and rely on Transformers docs for generic implementation details.\n \n For discoverability, we highly recommend you to add the `custom_generate` tag to your repository. To do so, the top of your `README.md` file should look like the example below. After you push the file, you should see the tag in your repository!\n \n@@ -504,6 +504,11 @@ Recommended practices:\n - Add self-contained examples to enable quick experimentation.\n - Describe soft-requirements such as if the method only works well with a certain family of models.\n \n+### Finding custom generation methods\n+\n+You can find all custom generation methods by [searching for their custom tag.](https://huggingface.co/models?other=custom_generate), `custom_generate`. In addition to the tag, we curate two collections of `custom_generate` methods:\n+- [Custom generation methods - Community](https://huggingface.co/collections/transformers-community/custom-generation-methods-community-6888fb1da0efbc592d3a8ab6) -- a collection of powerful methods contributed by the community;\n+- [Custom generation methods - Tutorials](https://huggingface.co/collections/transformers-community/custom-generation-methods-tutorials-6823589657a94940ea02cfec) -- a collection of reference implementations for methods that previously were part of `transformers`, as well as tutorials for `custom_generate`.\n \n ## Resources\n "
        }
    ],
    "stats": {
        "total": 47,
        "additions": 26,
        "deletions": 21
    }
}