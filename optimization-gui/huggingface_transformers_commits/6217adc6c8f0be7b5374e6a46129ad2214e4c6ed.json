{
    "author": "ArthurZucker",
    "message": "Default auto (#42805)\n\n* default to `\"auto\"` dtype\n\n* the actual change?\n\n* up?\n\n* style\n\n* up?\n\n* only sam models were broken with this\n\n* fix sams\n\n* update\n\n* fix sam2 now\n\n* up\n\n* this?\n\n* proper fix\n\n* lol\n\n* fix\n\n* fixes\n\n* nit\n\n* fix\n\n* fix copies\n\n* fixes\n\n* fix bigbird\n\n* revert one bit",
    "sha": "6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
    "files": [
        {
            "sha": "e7f4edda61d66a46e400af2cf1f34d1f9aaf9403",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
            "patch": "@@ -885,7 +885,7 @@ def convert_and_load_state_dict_in_model(\n             elif dtype_plan != {} and dtype_policy_alt.search(renamed_key):\n                 matched_dtype_pattern = dtype_policy_alt.search(renamed_key)\n                 if matched_dtype_pattern is not None:\n-                    _dtype = dtype_plan[matched_dtype_pattern.group()]\n+                    _dtype = dtype_plan[dtype_policy_by_group_name[matched_dtype_pattern.lastgroup]]\n             elif empty_param is not None and empty_param.dtype != _dtype:\n                 _dtype = empty_param.dtype  # usually correct when initializing\n "
        },
        {
            "sha": "e1907a8fab8894a6e53446377b758ef590be74a6",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
            "patch": "@@ -350,6 +350,9 @@ def lazy_load_kernel(kernel_name: str, mapping: dict[str, ModuleType | None] = _\n             mapping[kernel_name] = kernel\n         except FileNotFoundError:\n             mapping[kernel_name] = None\n+        except AssertionError:\n+            # Happens when torch is built without an accelerator backend; fall back to slow path.\n+            mapping[kernel_name] = None\n \n     else:\n         # Try to import is_{kernel_name}_available from ..utils"
        },
        {
            "sha": "5cd365c514a08b2bb15febfd4ce40837cab9d263",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 20,
            "deletions": 3,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
            "patch": "@@ -277,7 +277,9 @@ def get_state_dict_dtype(state_dict):\n             return t.dtype\n \n     # if no floating dtype was found return whatever the first dtype is\n-    return next(state_dict.values()).dtype\n+    if len(state_dict) == 0:\n+        return torch.float32\n+    return next(iter(state_dict.values())).dtype\n \n \n str_to_torch_dtype = {\n@@ -769,7 +771,7 @@ def _get_dtype(\n         for key in config.sub_configs:\n             if (sub_config := getattr(config, key)) is not None:\n                 sub_config.dtype = default_dtype\n-\n+    dtype = dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)\n     return config, dtype, dtype_orig\n \n \n@@ -796,7 +798,11 @@ def dtype(self) -> torch.dtype:\n         \"\"\"\n         `torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\n         \"\"\"\n-        return next(param.dtype for param in self.parameters() if param.is_floating_point())\n+        dtype = self._dtype or next(param.dtype for param in self.parameters() if param.is_floating_point())\n+        if isinstance(dtype, str):\n+            if hasattr(torch, dtype):\n+                dtype = getattr(torch, dtype)\n+        return dtype\n \n     def invert_attention_mask(self, encoder_attention_mask: Tensor) -> Tensor:\n         \"\"\"\n@@ -1075,6 +1081,7 @@ class PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToH\n     _keep_in_fp32_modules_strict = None\n \n     dtype_plan: Optional[dict[str, torch.dtype]] = None\n+    _dtype: Optional[Union[str, torch.dtype]] = torch.get_default_dtype()\n \n     # a list of `re` patterns of `state_dict` keys that should be removed from the list of missing\n     # keys we find (keys inside the model but not in the checkpoint) and avoid unnecessary warnings.\n@@ -1219,6 +1226,8 @@ def __init__(self, config: PreTrainedConfig, *inputs, **kwargs):\n                 f\"`model = {self.__class__.__name__}.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n             )\n         self.config = config\n+        default_dtype = torch.get_default_dtype()\n+        self._dtype = default_dtype\n \n         # Check the attention implementation is supported, or set it if not yet set (on the internal attr, to avoid\n         # setting it recursively)\n@@ -1457,6 +1466,11 @@ def _set_default_dtype(cls, dtype: torch.dtype) -> torch.dtype:\n         Note `set_default_dtype` currently only works with floating-point types and asserts if for example,\n         `torch.int64` is passed. So if a non-float `dtype` is passed this functions will throw an exception.\n         \"\"\"\n+        if isinstance(dtype, str):\n+            if hasattr(torch, dtype):\n+                dtype = getattr(torch, dtype)\n+            else:\n+                raise ValueError(f\"Received an invalid string dtype: {dtype}\")\n         if not dtype.is_floating_point:\n             raise ValueError(\n                 f\"Can't instantiate {cls.__name__} model under dtype={dtype} since it is not a floating point dtype\"\n@@ -1465,6 +1479,7 @@ def _set_default_dtype(cls, dtype: torch.dtype) -> torch.dtype:\n         logger.info(f\"Instantiating {cls.__name__} model under default dtype {dtype}.\")\n         dtype_orig = torch.get_default_dtype()\n         torch.set_default_dtype(dtype)\n+        cls._dtype = dtype\n         return dtype_orig\n \n     @property\n@@ -3822,6 +3837,8 @@ def from_pretrained(\n         # For BC on torch_dtype argument\n         if torch_dtype is not None:\n             dtype = dtype if dtype is not None else torch_dtype\n+        if dtype is None:\n+            dtype = \"auto\"\n \n         if is_offline_mode() and not local_files_only:\n             local_files_only = True"
        },
        {
            "sha": "d0e73fd72274103891216b7544a5668a248ce2b0",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
            "patch": "@@ -216,7 +216,7 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n                 \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n             )\n \n-        embeddings = self.projection(pixel_values)\n+        embeddings = self.projection(pixel_values.to(self.projection.weight.dtype))\n         patch_height, patch_width = embeddings.shape[2], embeddings.shape[3]\n         embeddings = embeddings.flatten(2).transpose(1, 2)\n "
        },
        {
            "sha": "79604f41ed358fe175c1b69982a9d3440975f8ae",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
            "patch": "@@ -1154,7 +1154,6 @@ def forward(\n         return outputs\n \n \n-# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -1178,7 +1177,7 @@ def eager_attention_forward(\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = torch.matmul(attn_weights.to(value.dtype), value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n \n     return attn_output, attn_weights"
        },
        {
            "sha": "3ca2d0ed7dc6757a5cafc7dd9593f5079f96ef00",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
            "patch": "@@ -603,7 +603,7 @@ def forward(\n \n         # This is actually dropping out entire tokens to attend to, which might\n         # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs_dropped = self.dropout(attention_probs)\n+        attention_probs_dropped = self.dropout(attention_probs).to(value_layer.dtype)\n \n         context_layer = torch.matmul(attention_probs_dropped, value_layer)\n "
        },
        {
            "sha": "3a866ad2b1f8e730c09b0ad32652916965aadac0",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
            "patch": "@@ -216,7 +216,7 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n                 \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n             )\n \n-        embeddings = self.projection(pixel_values)\n+        embeddings = self.projection(pixel_values.to(self.projection.weight.dtype))\n         patch_height, patch_width = embeddings.shape[2], embeddings.shape[3]\n         embeddings = embeddings.flatten(2).transpose(1, 2)\n "
        },
        {
            "sha": "76fab30cac2183a061aa6c022975a0347ca13d4d",
            "filename": "src/transformers/models/edgetam/modeling_edgetam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py?ref=6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
            "patch": "@@ -393,7 +393,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[tuple[torch.Tensor, ...]\n         n = len(self.convs) - 1\n         for i in range(n, -1, -1):\n             lateral_features = hidden_states[i].permute(0, 3, 1, 2)\n-            lateral_features = self.convs[n - i](lateral_features)\n+            lateral_features = self.convs[n - i](lateral_features.to(self.convs[i].weight.dtype))\n             if i not in self.fpn_top_down_levels or i == n:\n                 prev_features = lateral_features\n             else:"
        },
        {
            "sha": "3ecf8db7ef068fe6b0f406e1822f24a80aaa9c5e",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
            "patch": "@@ -302,7 +302,7 @@ def eager_attention_forward(\n     combined_logits = combined_logits - combined_logits.max(dim=-1, keepdim=True).values\n     probs = F.softmax(combined_logits, dim=-1, dtype=combined_logits.dtype)\n     scores = probs[..., :-1]  # we drop the sink here\n-    attn_weights = nn.functional.dropout(scores, p=dropout, training=module.training)\n+    attn_weights = nn.functional.dropout(scores, p=dropout, training=module.training).to(value_states.dtype)\n     attn_output = torch.matmul(attn_weights, value_states)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n     return attn_output, attn_weights"
        },
        {
            "sha": "26587ba4e2a8b0450b260d3533ad7dcb3fccc105",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
            "patch": "@@ -239,7 +239,7 @@ def eager_attention_forward(\n     combined_logits = combined_logits - combined_logits.max(dim=-1, keepdim=True).values\n     probs = F.softmax(combined_logits, dim=-1, dtype=combined_logits.dtype)\n     scores = probs[..., :-1]  # we drop the sink here\n-    attn_weights = nn.functional.dropout(scores, p=dropout, training=module.training)\n+    attn_weights = nn.functional.dropout(scores, p=dropout, training=module.training).to(value_states.dtype)\n     attn_output = torch.matmul(attn_weights, value_states)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n     return attn_output, attn_weights"
        },
        {
            "sha": "e09c8aaf495a78beb1676283b6a6d33f268ed08b",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
            "patch": "@@ -208,7 +208,7 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n                 \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n             )\n \n-        embeddings = self.projection(pixel_values)\n+        embeddings = self.projection(pixel_values.to(self.projection.weight.dtype))\n         patch_height, patch_width = embeddings.shape[2], embeddings.shape[3]\n         embeddings = embeddings.flatten(2).transpose(1, 2)\n "
        },
        {
            "sha": "8ae4f52bcdc87e642bdca9abd02a07d672f853b2",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
            "patch": "@@ -176,7 +176,7 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n                 \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n             )\n \n-        embeddings = self.projection(pixel_values)\n+        embeddings = self.projection(pixel_values.to(self.projection.weight.dtype))\n         patch_height, patch_width = embeddings.shape[2], embeddings.shape[3]\n         embeddings = embeddings.flatten(2).transpose(1, 2)\n "
        },
        {
            "sha": "eee68ba59b62d32239f2eae36b08f2c4c33edadf",
            "filename": "src/transformers/models/sam2/modeling_sam2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py?ref=6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
            "patch": "@@ -143,7 +143,7 @@ def __init__(self, config: Sam2HieraDetConfig):\n \n     def forward(self, pixel_values):\n         _, num_channels, height, width = pixel_values.shape\n-        embeddings = self.projection(pixel_values).permute(0, 2, 3, 1)\n+        embeddings = self.projection(pixel_values.to(self.projection.weight.dtype)).permute(0, 2, 3, 1)\n         return embeddings\n \n \n@@ -221,7 +221,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[tuple[torch.Tensor, ...]\n         n = len(self.convs) - 1\n         for i in range(n, -1, -1):\n             lateral_features = hidden_states[i].permute(0, 3, 1, 2)\n-            lateral_features = self.convs[n - i](lateral_features)\n+            lateral_features = self.convs[n - i](lateral_features.to(self.convs[i].weight.dtype))\n             if i not in self.fpn_top_down_levels or i == n:\n                 prev_features = lateral_features\n             else:"
        },
        {
            "sha": "7844e700fd6414bff199a83fc53fbb8ac72d7d86",
            "filename": "src/transformers/models/sam2/modular_sam2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py?ref=6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
            "patch": "@@ -387,7 +387,7 @@ def __init__(self, config: Sam2HieraDetConfig):\n \n     def forward(self, pixel_values):\n         _, num_channels, height, width = pixel_values.shape\n-        embeddings = self.projection(pixel_values).permute(0, 2, 3, 1)\n+        embeddings = self.projection(pixel_values.to(self.projection.weight.dtype)).permute(0, 2, 3, 1)\n         return embeddings\n \n \n@@ -422,7 +422,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[tuple[torch.Tensor, ...]\n         n = len(self.convs) - 1\n         for i in range(n, -1, -1):\n             lateral_features = hidden_states[i].permute(0, 3, 1, 2)\n-            lateral_features = self.convs[n - i](lateral_features)\n+            lateral_features = self.convs[n - i](lateral_features.to(self.convs[i].weight.dtype))\n             if i not in self.fpn_top_down_levels or i == n:\n                 prev_features = lateral_features\n             else:"
        },
        {
            "sha": "e81e336ff4ec1ffb607ed966d1bf806c51c3cc88",
            "filename": "src/transformers/models/sam3/modeling_sam3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py?ref=6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
            "patch": "@@ -557,7 +557,7 @@ def __init__(self, config: Sam3ViTConfig):\n         self.projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size, bias=False)\n \n     def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n-        embeddings = self.projection(pixel_values).flatten(2).transpose(1, 2)\n+        embeddings = self.projection(pixel_values.to(self.projection.weight.dtype)).flatten(2).transpose(1, 2)\n         return embeddings\n \n \n@@ -953,6 +953,7 @@ def __init__(self, in_channels: int, fpn_dim: int, scale_factor: float):\n         self.proj2 = nn.Conv2d(in_channels=fpn_dim, out_channels=fpn_dim, kernel_size=3, padding=1)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = hidden_states.to(self.proj1.weight.dtype)\n         for layer in self.scale_layers:\n             hidden_states = layer(hidden_states)\n "
        },
        {
            "sha": "652ae76fec76c95ef46d4a743e8a037847e863a1",
            "filename": "src/transformers/models/timm_wrapper/modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py?ref=6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
            "patch": "@@ -225,7 +225,7 @@ def forward(\n                 \"different architecture or updating the timm package to a compatible version.\"\n             )\n \n-        pixel_values = pixel_values.to(self.device, self.dtype)\n+        pixel_values = pixel_values.to(self.device)\n \n         if self.features_only:\n             last_hidden_state = self.timm_model.forward(pixel_values, **kwargs)"
        },
        {
            "sha": "b2b726e725f2132c0f6a1ee652889b91d3b38654",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6217adc6c8f0be7b5374e6a46129ad2214e4c6ed/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=6217adc6c8f0be7b5374e6a46129ad2214e4c6ed",
            "patch": "@@ -4036,12 +4036,14 @@ def test_bc_torch_dtype(self):\n                 for dtype in [\"float16\", \"bfloat16\", \"float32\", \"auto\", torch.float16, torch.bfloat16, torch.float32]:\n                     model_torch_dtype = model_class.from_pretrained(tmpdirname, torch_dtype=dtype)\n                     model_dtype = model_class.from_pretrained(tmpdirname, dtype=dtype)\n+\n                     for (k1, v1), (k2, v2) in zip(\n                         model_torch_dtype.named_parameters(), model_dtype.named_parameters()\n                     ):\n-                        self.assertEqual(k1, k2)\n-                        self.assertEqual(v1.dtype, v2.dtype)\n-                    torch.testing.assert_close(v1, v2, msg=f\"{k1} and  {k2} do not match: {v1} != {v2}\")\n+                        with self.subTest(f\"{dtype} for {model_class.__name__}.{k1}\"):\n+                            self.assertEqual(k1, k2)\n+                            self.assertEqual(v1.dtype, v2.dtype)\n+                            torch.testing.assert_close(v1, v2, msg=f\"{k1} and  {k2} do not match: {v1} != {v2}\")\n \n     def test_tp_plan_matches_params(self):\n         \"\"\"Make sure that each entry of the tp plan matches at least one param (this avoid typos and/or edge cases"
        }
    ],
    "stats": {
        "total": 68,
        "additions": 45,
        "deletions": 23
    }
}