{
    "author": "mario-aws",
    "message": "Correctly drop tokens in SwitchTransformer (#37123)\n\nPreviously, the identity function was used for dropped tokens\nwith a weight from the expert that was not applied to the hidden states.\nThis was misleading, because dropping means, the expert weight is zero.\nInstead of trying to fix the weight, we take an easier approach by initializing with zeros.\n\nFixes issue https://github.com/huggingface/transformers/issues/37017",
    "sha": "bde41d69b47c37e0dc1704cb4cd1a2a4709a4136",
    "files": [
        {
            "sha": "d7a158e9e8d46c96bc7aed1acee29c829a57066b",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bde41d69b47c37e0dc1704cb4cd1a2a4709a4136/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bde41d69b47c37e0dc1704cb4cd1a2a4709a4136/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=bde41d69b47c37e0dc1704cb4cd1a2a4709a4136",
            "patch": "@@ -301,10 +301,8 @@ def forward(self, hidden_states):\n         router_mask, router_probs, router_logits = self.router(hidden_states)\n         expert_index = torch.argmax(router_mask, dim=-1)\n \n-        # The routers introduced might not always map all the tokens, to a router, which means that some hidden states\n-        # can be unchanged from one layer to another. That is why the hidden states are cloned before updating only the selected ones.\n-\n-        next_states = hidden_states.clone()\n+        # If a token gets dropped, we just set it to zero such that it does not get updated.\n+        next_states = torch.zeros(hidden_states.shape, device=hidden_states.device, dtype=hidden_states.dtype)\n \n         router_mask = router_mask.bool()\n         batch_size, seq_len, num_experts = router_mask.shape"
        },
        {
            "sha": "18475d9603422122c5f546a08ab7dc1487e0764a",
            "filename": "tests/models/switch_transformers/test_modeling_switch_transformers.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/bde41d69b47c37e0dc1704cb4cd1a2a4709a4136/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bde41d69b47c37e0dc1704cb4cd1a2a4709a4136/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py?ref=bde41d69b47c37e0dc1704cb4cd1a2a4709a4136",
            "patch": "@@ -42,6 +42,7 @@\n         SwitchTransformersEncoderModel,\n         SwitchTransformersForConditionalGeneration,\n         SwitchTransformersModel,\n+        SwitchTransformersSparseMLP,\n         SwitchTransformersTop1Router,\n     )\n     from transformers.models.switch_transformers.modeling_switch_transformers import (\n@@ -1133,3 +1134,16 @@ def test_small_batch_generate(self):\n \n         for i in range(0, BATCH_SIZE, 2):\n             self.assertEqual(batch_output[i], batch_output[i + 1])\n+\n+\n+@require_torch\n+class SwitchTransformersSparseMLPTests(unittest.TestCase):\n+    def test_token_dropping(self):\n+        r\"\"\"\n+        This test checks if the token dropping actually drops tokens.\n+        \"\"\"\n+        config = SwitchTransformersConfig(expert_capacity=0)  # we drop everything\n+        moe = SwitchTransformersSparseMLP(config)\n+        dropped_token_results = moe(torch.randn(2, 3, 768))[0]\n+\n+        assert (dropped_token_results == 0).all(), f\"Some tokens not dropped: {dropped_token_results}.\""
        }
    ],
    "stats": {
        "total": 20,
        "additions": 16,
        "deletions": 4
    }
}