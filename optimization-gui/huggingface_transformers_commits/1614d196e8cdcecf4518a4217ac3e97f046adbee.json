{
    "author": "blbadger",
    "message": "Mllama fsdp (#36000)\n\n* pixel input assignment revoked\n\n* double send\n\n* Update src/transformers/models/mllama/modeling_mllama.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n---------\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "1614d196e8cdcecf4518a4217ac3e97f046adbee",
    "files": [
        {
            "sha": "4a705083f3ba8e0227edf101670229d6c290ddbc",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1614d196e8cdcecf4518a4217ac3e97f046adbee/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1614d196e8cdcecf4518a4217ac3e97f046adbee/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=1614d196e8cdcecf4518a4217ac3e97f046adbee",
            "patch": "@@ -1541,7 +1541,9 @@ def forward(\n         aspect_ratio_ids = aspect_ratio_ids.reshape(batch_size * num_concurrent_media, -1)\n \n         # Patch embedding\n-        patch_embeds = self.patch_embedding(pixel_values.to(self.dtype).to(self.device))\n+        target_dtype = self.patch_embedding.weight.dtype\n+        target_device = self.patch_embedding.weight.device\n+        patch_embeds = self.patch_embedding(pixel_values.to(target_device, target_dtype))\n         hidden_state = patch_embeds.flatten(2).transpose(1, 2)\n \n         # Tile embeddings"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 3,
        "deletions": 1
    }
}