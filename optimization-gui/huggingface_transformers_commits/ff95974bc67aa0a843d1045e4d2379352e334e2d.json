{
    "author": "zucchini-nlp",
    "message": "[static cache] fix device map per layer in VLMs (#38488)\n\nreturn lm as decoder",
    "sha": "ff95974bc67aa0a843d1045e4d2379352e334e2d",
    "files": [
        {
            "sha": "faafd10f2c39ed863c13ed78e68a2d877a145dda",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -1056,6 +1056,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1220,10 +1226,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder\n \n     def get_image_features(\n         self,"
        },
        {
            "sha": "5b84cbf93ba3cd98ad6182323015245847285424",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -211,6 +211,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -389,10 +395,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder\n \n     def get_image_features(\n         self,"
        },
        {
            "sha": "7a3a177c4322835cff28f2bfa78627a144457d98",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -1451,6 +1451,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.text_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.text_model = decoder\n+\n+    def get_decoder(self):\n+        return self.text_model\n+\n     def get_image_tokens(self, pixel_values: torch.FloatTensor, image_sizes: torch.LongTensor):\n         \"\"\"\n         Tokenizes images into discrete tokens with VQGAN module. Converts\n@@ -1599,10 +1605,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder()\n \n     # Make modules available throught conditional class for BC\n     @property"
        },
        {
            "sha": "fbcafff16ceac670b8ae5780a7dc2303ce45620f",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -938,6 +938,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.text_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.text_model = decoder\n+\n+    def get_decoder(self):\n+        return self.text_model\n+\n     def get_image_tokens(self, pixel_values: torch.FloatTensor, image_sizes: torch.LongTensor):\n         \"\"\"\n         Tokenizes images into discrete tokens with VQGAN module. Converts\n@@ -1086,10 +1092,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder()\n \n     # Make modules available throught conditional class for BC\n     @property"
        },
        {
            "sha": "9c5d1a262eab55c6889d4fc33ae4a011aa71528b",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -86,6 +86,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     def gather_continuous_embeddings(\n         self,\n         word_embeddings: torch.Tensor,"
        },
        {
            "sha": "16d4673053edd4b15b5ecf13ce3b60e0fa5da4f1",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -829,6 +829,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n         Projects the last hidden state from the vision model into language model space.\n@@ -1014,10 +1020,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder()\n \n     def get_image_features(self, pixel_values):\n         return self.model.get_image_features(pixel_values)"
        },
        {
            "sha": "6527d3efbbc126c2ce599530b54eb93e3d660d33",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -637,6 +637,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -757,10 +763,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder\n \n     def get_image_features(\n         self,"
        },
        {
            "sha": "571104934ef71de79160f223745e36612c96bfc6",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -627,6 +627,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -878,10 +884,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder\n \n     def get_image_features(\n         self,"
        },
        {
            "sha": "0d0427fbb5319b96b38034414ce798d99cb6dbd7",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -181,6 +181,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -371,10 +377,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder\n \n     def get_image_features(\n         self,"
        },
        {
            "sha": "ceeba4f0785a9be399a711c8dd1ee2f33dd7f7c1",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -294,6 +294,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n         \"\"\"\n         Reshape, unpad and then pack each image_feature into a single image_features tensor containing all visual vectors.\n@@ -569,10 +575,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder()\n \n     def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n         return self.model.pack_image_features("
        },
        {
            "sha": "6e352cf7888860b4f375b42ae7d22a8334ae30bc",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -348,6 +348,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n         \"\"\"\n         Reshape, unpad and then pack each image_feature into a single image_features tensor containing all visual vectors.\n@@ -701,10 +707,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder()\n \n     def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n         return self.model.pack_image_features("
        },
        {
            "sha": "3f4f8c6de669950a4cda394531d19f38ee5938e9",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -350,6 +350,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     def pack_image_features(self, image_features, image_sizes, image_newline=None, vision_aspect_ratio=\"anyres_max_9\"):\n         \"\"\"\n         Reshape, unpad and then pack each image_feature into a single image_features tensor containing all visual vectors.\n@@ -742,10 +748,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder()\n \n     def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n         return self.model.pack_image_features("
        },
        {
            "sha": "426539ce5a39a78516759bed184cd1e1b1c8e2e4",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -248,6 +248,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -407,10 +413,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder\n \n     def get_image_features(\n         self,"
        },
        {
            "sha": "993c44f9207ce7a80362c264469b8cd03ebcb820",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -1641,6 +1641,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1792,10 +1798,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder()\n \n     # Make modules available throught conditional class for BC\n     @property"
        },
        {
            "sha": "12edf452d277d3c7aa0e4ebfdbe68e2e6b06b96f",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -173,6 +173,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     def _update_causal_mask(\n         self,\n         attention_mask,\n@@ -418,10 +424,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder()\n \n     def get_image_features(self, pixel_values):\n         return self.model.get_image_features(pixel_values)"
        },
        {
            "sha": "dd6b60306774bc39d0b0fab4b4574f47389f54f1",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -1847,6 +1847,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n     def get_video_features(\n         self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n     ):"
        },
        {
            "sha": "4412184cd3f904bf16d4659cb2d07a31349f80c2",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -2269,6 +2269,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n     def get_video_features(\n         self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n     ):"
        },
        {
            "sha": "c851244e6b754d9d82acc248ecdb84656651dcab",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -1067,6 +1067,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     def get_rope_index(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1498,10 +1504,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder()\n \n     def get_video_features(\n         self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None"
        },
        {
            "sha": "acf1e4025b9a67777d7f7dcc1a95097a794c7cfd",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -1033,6 +1033,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     def get_rope_index(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1382,10 +1388,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder()\n \n     def get_video_features(\n         self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None"
        },
        {
            "sha": "c619bf378c29948e432eefa30680c22585c32a72",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -202,6 +202,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     def get_image_features(\n         self,\n         pixel_values_images: torch.FloatTensor,\n@@ -444,10 +450,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder()\n \n     def get_image_features(\n         self,"
        },
        {
            "sha": "a2deef74b72eed0e76fd970b51c22550f3cacef7",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff95974bc67aa0a843d1045e4d2379352e334e2d/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=ff95974bc67aa0a843d1045e4d2379352e334e2d",
            "patch": "@@ -182,6 +182,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     def get_image_features(\n         self, pixel_values: torch.FloatTensor, vision_feature_layers: Optional[Union[int, list[int]]] = None\n     ):\n@@ -327,10 +333,10 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder\n \n     def get_image_features(\n         self, pixel_values: torch.FloatTensor, vision_feature_layers: Optional[Union[int, list[int]]] = None"
        }
    ],
    "stats": {
        "total": 198,
        "additions": 162,
        "deletions": 36
    }
}