{
    "author": "bakrianoo",
    "message": "Support QuestionAnswering Module for ModernBert based models. (#35566)\n\n* push ModernBertForQuestionAnswering\n\n* update ModernBertForQuestionAnswering\n\n* update __init__ loading\n\n* set imports for ModernBertForQuestionAnswering\n\n* update ModernBertForQuestionAnswering\n\n* remove debugging logs\n\n* update init_weights method\n\n* remove custom initialization for ModernBertForQuestionAnswering\n\n* apply make fix-copies\n\n* apply make style\n\n* apply make fix-copies\n\n* append ModernBertForQuestionAnswering to the pipeline supported models\n\n* remove unused file\n\n* remove invalid autoload value\n\n* update en/model_doc/modernbert.md\n\n* apply make fixup command\n\n* make fixup\n\n* Update dummies\n\n* update usage tips for ModernBertForQuestionAnswering\n\n* update usage tips for ModernBertForQuestionAnswering\n\n* add init\n\n* add lint\n\n* add consistency\n\n* update init test\n\n* change text to trigger stuck text\n\n* use self.loss_function instead of custom loss\r\n\r\nBy @Cyrilvallez\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\n\n* Update modeling_modernbert.py\n\nmake comparable commit to even it out\n\n* Match whitespace\n\n* whitespace\n\n---------\n\nCo-authored-by: Matt <rocketknight1@gmail.com>\nCo-authored-by: Orion Weller <wellerorion@gmail.com>\nCo-authored-by: Orion Weller <31665361+orionw@users.noreply.github.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "49b5ab6a27511de5168c72e83318164f1b4adc43",
    "files": [
        {
            "sha": "a54adb04c2f9e14381ee0bd6d3f718dde40b2bba",
            "filename": "docs/source/en/model_doc/modernbert.md",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/49b5ab6a27511de5168c72e83318164f1b4adc43/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/49b5ab6a27511de5168c72e83318164f1b4adc43/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md?ref=49b5ab6a27511de5168c72e83318164f1b4adc43",
            "patch": "@@ -60,6 +60,9 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n \n - [Masked language modeling task guide](../tasks/masked_language_modeling)\n \n+<PipelineTag pipeline=\"question-answering\"/>\n+\n+- [`ModernBertForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb).\n \n ## ModernBertConfig\n \n@@ -88,5 +91,15 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n [[autodoc]] ModernBertForTokenClassification\n     - forward\n \n+## ModernBertForQuestionAnswering\n+\n+[[autodoc]] ModernBertForQuestionAnswering\n+    - forward\n+\n+### Usage tips\n+\n+The ModernBert model can be fine-tuned using the HuggingFace Transformers library with its [official script](https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py) for question-answering tasks.\n+\n+\n </pt>\n </frameworkcontent>"
        },
        {
            "sha": "03ed4056280d454ee6802ad2d2b4c2a62bbe8325",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/49b5ab6a27511de5168c72e83318164f1b4adc43/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49b5ab6a27511de5168c72e83318164f1b4adc43/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=49b5ab6a27511de5168c72e83318164f1b4adc43",
            "patch": "@@ -3047,6 +3047,7 @@\n     _import_structure[\"models.modernbert\"].extend(\n         [\n             \"ModernBertForMaskedLM\",\n+            \"ModernBertForQuestionAnswering\",\n             \"ModernBertForSequenceClassification\",\n             \"ModernBertForTokenClassification\",\n             \"ModernBertModel\",\n@@ -7967,6 +7968,7 @@\n         )\n         from .models.modernbert import (\n             ModernBertForMaskedLM,\n+            ModernBertForQuestionAnswering,\n             ModernBertForSequenceClassification,\n             ModernBertForTokenClassification,\n             ModernBertModel,"
        },
        {
            "sha": "add0619aa8f86ffa9915cdade1c71dc26b8d6c3d",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/49b5ab6a27511de5168c72e83318164f1b4adc43/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49b5ab6a27511de5168c72e83318164f1b4adc43/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=49b5ab6a27511de5168c72e83318164f1b4adc43",
            "patch": "@@ -1138,6 +1138,7 @@\n         (\"mistral\", \"MistralForQuestionAnswering\"),\n         (\"mixtral\", \"MixtralForQuestionAnswering\"),\n         (\"mobilebert\", \"MobileBertForQuestionAnswering\"),\n+        (\"modernbert\", \"ModernBertForQuestionAnswering\"),\n         (\"mpnet\", \"MPNetForQuestionAnswering\"),\n         (\"mpt\", \"MptForQuestionAnswering\"),\n         (\"mra\", \"MraForQuestionAnswering\"),"
        },
        {
            "sha": "28ba79b830a7b6c778e05ee881eaf73fa1a56c39",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 99,
            "deletions": 2,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/49b5ab6a27511de5168c72e83318164f1b4adc43/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49b5ab6a27511de5168c72e83318164f1b4adc43/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=49b5ab6a27511de5168c72e83318164f1b4adc43",
            "patch": "@@ -30,7 +30,13 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n-from ...modeling_outputs import BaseModelOutput, MaskedLMOutput, SequenceClassifierOutput, TokenClassifierOutput\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+    MaskedLMOutput,\n+    QuestionAnsweringModelOutput,\n+    SequenceClassifierOutput,\n+    TokenClassifierOutput,\n+)\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -650,7 +656,10 @@ def init_weight(module: nn.Module, std: float):\n             init_weight(module.dense, stds[\"out\"])\n         elif isinstance(module, ModernBertForMaskedLM):\n             init_weight(module.decoder, stds[\"out\"])\n-        elif isinstance(module, (ModernBertForSequenceClassification, ModernBertForTokenClassification)):\n+        elif isinstance(\n+            module,\n+            (ModernBertForSequenceClassification, ModernBertForTokenClassification, ModernBertForQuestionAnswering),\n+        ):\n             init_weight(module.classifier, stds[\"final_out\"])\n \n     @classmethod\n@@ -1384,10 +1393,98 @@ def forward(\n         )\n \n \n+@add_start_docstrings(\n+    \"\"\"\n+    The ModernBert Model with a span classification head on top for extractive question-answering tasks like SQuAD\n+    (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n+    \"\"\",\n+    MODERNBERT_START_DOCSTRING,\n+)\n+class ModernBertForQuestionAnswering(ModernBertPreTrainedModel):\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+\n+        self.model = ModernBertModel(config)\n+        self.head = ModernBertPredictionHead(config)\n+        self.drop = torch.nn.Dropout(config.classifier_dropout)\n+        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n+\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(MODERNBERT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=QuestionAnsweringModelOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        sliding_window_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        start_positions: Optional[torch.Tensor] = None,\n+        end_positions: Optional[torch.Tensor] = None,\n+        indices: Optional[torch.Tensor] = None,\n+        cu_seqlens: Optional[torch.Tensor] = None,\n+        max_seqlen: Optional[int] = None,\n+        batch_size: Optional[int] = None,\n+        seq_len: Optional[int] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n+    ) -> Union[Tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        self._maybe_set_compile()\n+\n+        outputs = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            sliding_window_mask=sliding_window_mask,\n+            position_ids=position_ids,\n+            indices=indices,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            batch_size=batch_size,\n+            seq_len=seq_len,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        last_hidden_state = outputs[0]\n+\n+        last_hidden_state = self.head(last_hidden_state)\n+        last_hidden_state = self.drop(last_hidden_state)\n+        logits = self.classifier(last_hidden_state)\n+\n+        start_logits, end_logits = logits.split(1, dim=-1)\n+        start_logits = start_logits.squeeze(-1).contiguous()\n+        end_logits = end_logits.squeeze(-1).contiguous()\n+\n+        loss = None\n+        if start_positions is not None and end_positions is not None:\n+            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n+\n+        if not return_dict:\n+            output = (start_logits, end_logits) + outputs[1:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return QuestionAnsweringModelOutput(\n+            loss=loss,\n+            start_logits=start_logits,\n+            end_logits=end_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n __all__ = [\n     \"ModernBertModel\",\n     \"ModernBertPreTrainedModel\",\n     \"ModernBertForMaskedLM\",\n     \"ModernBertForSequenceClassification\",\n     \"ModernBertForTokenClassification\",\n+    \"ModernBertForQuestionAnswering\",\n ]"
        },
        {
            "sha": "0901662f66a7897306c5dc282fcade32d525cfb8",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 93,
            "deletions": 1,
            "changes": 94,
            "blob_url": "https://github.com/huggingface/transformers/blob/49b5ab6a27511de5168c72e83318164f1b4adc43/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49b5ab6a27511de5168c72e83318164f1b4adc43/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=49b5ab6a27511de5168c72e83318164f1b4adc43",
            "patch": "@@ -29,6 +29,7 @@\n from ...modeling_outputs import (\n     BaseModelOutput,\n     MaskedLMOutput,\n+    QuestionAnsweringModelOutput,\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n@@ -825,7 +826,10 @@ def init_weight(module: nn.Module, std: float):\n             init_weight(module.dense, stds[\"out\"])\n         elif isinstance(module, ModernBertForMaskedLM):\n             init_weight(module.decoder, stds[\"out\"])\n-        elif isinstance(module, (ModernBertForSequenceClassification, ModernBertForTokenClassification)):\n+        elif isinstance(\n+            module,\n+            (ModernBertForSequenceClassification, ModernBertForTokenClassification, ModernBertForQuestionAnswering),\n+        ):\n             init_weight(module.classifier, stds[\"final_out\"])\n \n     @classmethod\n@@ -1487,11 +1491,99 @@ def forward(\n         )\n \n \n+@add_start_docstrings(\n+    \"\"\"\n+    The ModernBert Model with a span classification head on top for extractive question-answering tasks like SQuAD\n+    (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n+    \"\"\",\n+    MODERNBERT_START_DOCSTRING,\n+)\n+class ModernBertForQuestionAnswering(ModernBertPreTrainedModel):\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+\n+        self.model = ModernBertModel(config)\n+        self.head = ModernBertPredictionHead(config)\n+        self.drop = torch.nn.Dropout(config.classifier_dropout)\n+        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n+\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(MODERNBERT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=QuestionAnsweringModelOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        sliding_window_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        start_positions: Optional[torch.Tensor] = None,\n+        end_positions: Optional[torch.Tensor] = None,\n+        indices: Optional[torch.Tensor] = None,\n+        cu_seqlens: Optional[torch.Tensor] = None,\n+        max_seqlen: Optional[int] = None,\n+        batch_size: Optional[int] = None,\n+        seq_len: Optional[int] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n+    ) -> Union[Tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        self._maybe_set_compile()\n+\n+        outputs = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            sliding_window_mask=sliding_window_mask,\n+            position_ids=position_ids,\n+            indices=indices,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            batch_size=batch_size,\n+            seq_len=seq_len,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        last_hidden_state = outputs[0]\n+\n+        last_hidden_state = self.head(last_hidden_state)\n+        last_hidden_state = self.drop(last_hidden_state)\n+        logits = self.classifier(last_hidden_state)\n+\n+        start_logits, end_logits = logits.split(1, dim=-1)\n+        start_logits = start_logits.squeeze(-1).contiguous()\n+        end_logits = end_logits.squeeze(-1).contiguous()\n+\n+        loss = None\n+        if start_positions is not None and end_positions is not None:\n+            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n+\n+        if not return_dict:\n+            output = (start_logits, end_logits) + outputs[1:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return QuestionAnsweringModelOutput(\n+            loss=loss,\n+            start_logits=start_logits,\n+            end_logits=end_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n __all__ = [\n     \"ModernBertConfig\",\n     \"ModernBertModel\",\n     \"ModernBertPreTrainedModel\",\n     \"ModernBertForMaskedLM\",\n     \"ModernBertForSequenceClassification\",\n     \"ModernBertForTokenClassification\",\n+    \"ModernBertForQuestionAnswering\",\n ]"
        },
        {
            "sha": "b52d2bed7856f35514cc86a74320ba4e24d64388",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/49b5ab6a27511de5168c72e83318164f1b4adc43/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49b5ab6a27511de5168c72e83318164f1b4adc43/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=49b5ab6a27511de5168c72e83318164f1b4adc43",
            "patch": "@@ -6692,6 +6692,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class ModernBertForQuestionAnswering(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class ModernBertForSequenceClassification(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "14882b0879c2628c51bccd963f3f25e223512b52",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/49b5ab6a27511de5168c72e83318164f1b4adc43/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49b5ab6a27511de5168c72e83318164f1b4adc43/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=49b5ab6a27511de5168c72e83318164f1b4adc43",
            "patch": "@@ -40,6 +40,7 @@\n     from transformers import (\n         MODEL_FOR_PRETRAINING_MAPPING,\n         ModernBertForMaskedLM,\n+        ModernBertForQuestionAnswering,\n         ModernBertForSequenceClassification,\n         ModernBertForTokenClassification,\n         ModernBertModel,\n@@ -224,6 +225,7 @@ class ModernBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n             ModernBertForMaskedLM,\n             ModernBertForSequenceClassification,\n             ModernBertForTokenClassification,\n+            ModernBertForQuestionAnswering,\n         )\n         if is_torch_available()\n         else ()\n@@ -235,6 +237,7 @@ class ModernBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n             \"text-classification\": ModernBertForSequenceClassification,\n             \"token-classification\": ModernBertForTokenClassification,\n             \"zero-shot\": ModernBertForSequenceClassification,\n+            \"question-answering\": ModernBertForQuestionAnswering,\n         }\n         if is_torch_available()\n         else {}\n@@ -289,7 +292,12 @@ def test_initialization(self):\n                 # are initialized without `initializer_range`, so they're not set to ~0 via the _config_zero_init\n                 if param.requires_grad and not (\n                     name == \"classifier.weight\"\n-                    and model_class in [ModernBertForSequenceClassification, ModernBertForTokenClassification]\n+                    and model_class\n+                    in [\n+                        ModernBertForSequenceClassification,\n+                        ModernBertForTokenClassification,\n+                        ModernBertForQuestionAnswering,\n+                    ]\n                 ):\n                     self.assertIn(\n                         ((param.data.mean() * 1e9).round() / 1e9).item(),"
        },
        {
            "sha": "76bbf2766fd9bc3b955f08c3400f1ff07bee21db",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/49b5ab6a27511de5168c72e83318164f1b4adc43/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49b5ab6a27511de5168c72e83318164f1b4adc43/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=49b5ab6a27511de5168c72e83318164f1b4adc43",
            "patch": "@@ -3086,6 +3086,7 @@ def test_mismatched_shapes_have_properly_initialized_weights(self):\n                 \"ModernBertForSequenceClassification\",\n                 \"ModernBertForTokenClassification\",\n                 \"TimmWrapperForImageClassification\",\n+                \"ModernBertForQuestionAnswering\",\n             ]\n             special_param_names = [\n                 r\"^bit\\.\","
        }
    ],
    "stats": {
        "total": 229,
        "additions": 225,
        "deletions": 4
    }
}