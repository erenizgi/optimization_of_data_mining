{
    "author": "Wauplin",
    "message": "Remove deprecated `use_auth_token` parameter (#41666)\n\n* Remove deprecated use_auth_token\n\n* code styl\n\n* fix test\n\n* Update examples/pytorch/speech-recognition/README.md",
    "sha": "252d7cd9524e561171bea3b7474f96cef9fd3465",
    "files": [
        {
            "sha": "ee1adfc4a9958f420288e00e33d979bb69ee1cc5",
            "filename": "docs/source/en/hpo_train.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/docs%2Fsource%2Fen%2Fhpo_train.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/docs%2Fsource%2Fen%2Fhpo_train.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fhpo_train.md?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -37,7 +37,6 @@ def model_init(trial):\n         config=config,\n         cache_dir=model_args.cache_dir,\n         revision=model_args.model_revision,\n-        token=True if model_args.use_auth_token else None,\n     )\n ```\n "
        },
        {
            "sha": "33234e6f3560b331d17127bab6322d007c954736",
            "filename": "docs/source/ja/hpo_train.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/docs%2Fsource%2Fja%2Fhpo_train.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/docs%2Fsource%2Fja%2Fhpo_train.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fhpo_train.md?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -91,7 +91,6 @@ Wandbについては、[object_parameter](https://docs.wandb.ai/guides/sweeps/co\n ...         config=config,\n ...         cache_dir=model_args.cache_dir,\n ...         revision=model_args.model_revision,\n-...         token=True if model_args.use_auth_token else None,\n ...     )\n ```\n "
        },
        {
            "sha": "1038d5081ed2781404045862003a2d888a677a21",
            "filename": "docs/source/ko/hpo_train.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/docs%2Fsource%2Fko%2Fhpo_train.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/docs%2Fsource%2Fko%2Fhpo_train.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fhpo_train.md?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -74,7 +74,6 @@ wandb의 경우, 해당 [object_parameter](https://docs.wandb.ai/guides/sweeps/c\n ...         config=config,\n ...         cache_dir=model_args.cache_dir,\n ...         revision=model_args.model_revision,\n-...         token=True if model_args.use_auth_token else None,\n ...     )\n ```\n "
        },
        {
            "sha": "a60f3c99691111016fa06b6d27427f03c46967a0",
            "filename": "docs/source/zh/hpo_train.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/docs%2Fsource%2Fzh%2Fhpo_train.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/docs%2Fsource%2Fzh%2Fhpo_train.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fhpo_train.md?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -87,7 +87,6 @@ Optuna提供了多目标HPO。您可以在`hyperparameter_search`中传递`direc\n ...         config=config,\n ...         cache_dir=model_args.cache_dir,\n ...         revision=model_args.model_revision,\n-...         use_auth_token=True if model_args.use_auth_token else None,\n ...     )\n ```\n "
        },
        {
            "sha": "695e9ba967b65dd9ffe5c14022a99735688351b3",
            "filename": "examples/pytorch/object-detection/run_object_detection_no_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -279,11 +279,6 @@ def parse_args():\n         type=str,\n         help=\"Path to a folder in which the model and dataset will be cached.\",\n     )\n-    parser.add_argument(\n-        \"--use_auth_token\",\n-        action=\"store_true\",\n-        help=\"Whether to use an authentication token to access the model repository.\",\n-    )\n     parser.add_argument(\n         \"--per_device_train_batch_size\",\n         type=int,"
        },
        {
            "sha": "2d1be6bc14eb59269d31a9b466cabba13561eaac",
            "filename": "examples/pytorch/semantic-segmentation/run_semantic_segmentation_no_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation_no_trainer.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -119,11 +119,6 @@ def parse_args():\n         type=str,\n         help=\"Path to a folder in which the model and dataset will be cached.\",\n     )\n-    parser.add_argument(\n-        \"--use_auth_token\",\n-        action=\"store_true\",\n-        help=\"Whether to use an authentication token to access the model repository.\",\n-    )\n     parser.add_argument(\n         \"--per_device_train_batch_size\",\n         type=int,"
        },
        {
            "sha": "d43d49fc90d17af1be5ba5d9548e0fb441ad9208",
            "filename": "examples/pytorch/speech-recognition/README.md",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/examples%2Fpytorch%2Fspeech-recognition%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/examples%2Fpytorch%2Fspeech-recognition%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2FREADME.md?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -389,8 +389,7 @@ python run_speech_recognition_seq2seq.py \\\n \t--fp16 \\\n \t--do_train \\\n \t--do_eval \\\n-\t--predict_with_generate \\\n-\t--use_auth_token\n+\t--predict_with_generate\n ```\n On a single V100, training should take approximately 8 hours, with a final cross-entropy loss of **1e-4** and word error rate of **32.6%**.\n \n@@ -429,8 +428,7 @@ torchrun \\\n \t--fp16 \\\n \t--do_train \\\n \t--do_eval \\\n-\t--predict_with_generate \\\n-\t--use_auth_token\n+\t--predict_with_generate\n ```\n On two V100s, training should take approximately 4 hours, with a final cross-entropy loss of **1e-4** and word error rate of **32.6%**.\n "
        },
        {
            "sha": "1167055a090c5ae9076358e27eeebfddafb0e558",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -437,8 +437,6 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n             kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n-        self._set_token_in_kwargs(kwargs)\n-\n         if os.path.isfile(save_directory):\n             raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n \n@@ -486,33 +484,6 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n                 token=kwargs.get(\"token\"),\n             )\n \n-    @staticmethod\n-    def _set_token_in_kwargs(kwargs, token=None):\n-        \"\"\"Temporary method to deal with `token` and `use_auth_token`.\n-\n-        This method is to avoid apply the same changes in all model config classes that overwrite `from_pretrained`.\n-\n-        Need to clean up `use_auth_token` in a follow PR.\n-        \"\"\"\n-        # Some model config classes like CLIP define their own `from_pretrained` without the new argument `token` yet.\n-        if token is None:\n-            token = kwargs.pop(\"token\", None)\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if token is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            token = use_auth_token\n-\n-        if token is not None:\n-            kwargs[\"token\"] = token\n-\n     @classmethod\n     def from_pretrained(\n         cls: type[SpecificPreTrainedConfigType],\n@@ -601,8 +572,6 @@ def from_pretrained(\n         kwargs[\"local_files_only\"] = local_files_only\n         kwargs[\"revision\"] = revision\n \n-        cls._set_token_in_kwargs(kwargs, token)\n-\n         config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n         if cls.base_config_key and cls.base_config_key in config_dict:\n             config_dict = config_dict[cls.base_config_key]\n@@ -639,8 +608,6 @@ def get_config_dict(\n             `tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the configuration object.\n \n         \"\"\"\n-        cls._set_token_in_kwargs(kwargs)\n-\n         original_kwargs = copy.deepcopy(kwargs)\n         # Get config dict associated with the base config file\n         config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)"
        },
        {
            "sha": "b70879120f73c6e87af399e0b4b117e728398229",
            "filename": "src/transformers/dynamic_module_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fdynamic_module_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fdynamic_module_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdynamic_module_utils.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -26,7 +26,6 @@\n import signal\n import sys\n import threading\n-import warnings\n from pathlib import Path\n from types import ModuleType\n from typing import Any, Optional, Union\n@@ -371,16 +370,6 @@ def get_cached_module_file(\n     Returns:\n         `str`: The path to the module inside the cache.\n     \"\"\"\n-    use_auth_token = deprecated_kwargs.pop(\"use_auth_token\", None)\n-    if use_auth_token is not None:\n-        warnings.warn(\n-            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-            FutureWarning,\n-        )\n-        if token is not None:\n-            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n-        token = use_auth_token\n-\n     if is_offline_mode() and not local_files_only:\n         logger.info(\"Offline mode: forcing local_files_only=True\")\n         local_files_only = True\n@@ -571,16 +560,6 @@ def get_class_from_dynamic_module(\n     # module.\n     cls = get_class_from_dynamic_module(\"sgugger/my-bert-model--modeling.MyBertModel\", \"sgugger/another-bert-model\")\n     ```\"\"\"\n-    use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-    if use_auth_token is not None:\n-        warnings.warn(\n-            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-            FutureWarning,\n-        )\n-        if token is not None:\n-            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n-        token = use_auth_token\n-\n     # Catch the name of the repo if it's specified in `class_reference`\n     if \"--\" in class_reference:\n         repo_id, class_reference = class_reference.split(\"--\")"
        },
        {
            "sha": "4c9db36b020b25e71c7f411f625a011a4c33b737",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -18,7 +18,6 @@\n import copy\n import json\n import os\n-import warnings\n from collections import UserDict\n from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union\n \n@@ -333,18 +332,6 @@ def from_pretrained(\n         kwargs[\"local_files_only\"] = local_files_only\n         kwargs[\"revision\"] = revision\n \n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if token is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            token = use_auth_token\n-\n         if token is not None:\n             kwargs[\"token\"] = token\n \n@@ -367,19 +354,6 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n             kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if kwargs.get(\"token\") is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            kwargs[\"token\"] = use_auth_token\n-\n         if os.path.isfile(save_directory):\n             raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n \n@@ -433,21 +407,9 @@ def get_feature_extractor_dict(\n         proxies = kwargs.pop(\"proxies\", None)\n         subfolder = kwargs.pop(\"subfolder\", None)\n         token = kwargs.pop(\"token\", None)\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n         local_files_only = kwargs.pop(\"local_files_only\", False)\n         revision = kwargs.pop(\"revision\", None)\n \n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if token is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            token = use_auth_token\n-\n         from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n         from_auto_class = kwargs.pop(\"_from_auto\", False)\n "
        },
        {
            "sha": "7be052a9a946d451f39871eb640180f970b9ff6d",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -17,7 +17,6 @@\n import copy\n import json\n import os\n-import warnings\n from abc import ABC, abstractmethod\n from collections.abc import Callable\n from dataclasses import dataclass, is_dataclass\n@@ -734,20 +733,6 @@ def save_pretrained(\n         except ValueError as exc:\n             raise ValueError(str(exc) + \"\\n\\nFix these issues to save the configuration.\")\n \n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. \"\n-                \"Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if kwargs.get(\"token\") is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            kwargs[\"token\"] = use_auth_token\n-\n         config_file_name = config_file_name if config_file_name is not None else GENERATION_CONFIG_NAME\n \n         if os.path.isfile(save_directory):\n@@ -870,23 +855,11 @@ def from_pretrained(\n         config_file_name = config_file_name if config_file_name is not None else GENERATION_CONFIG_NAME\n \n         proxies = kwargs.pop(\"proxies\", None)\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n         subfolder = kwargs.pop(\"subfolder\", \"\")\n         from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n         from_auto_class = kwargs.pop(\"_from_auto\", False)\n         commit_hash = kwargs.pop(\"_commit_hash\", None)\n \n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if token is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            token = use_auth_token\n-\n         user_agent = {\"file_type\": \"config\", \"from_auto_class\": from_auto_class}\n         if from_pipeline is not None:\n             user_agent[\"using_pipeline\"] = from_pipeline"
        },
        {
            "sha": "7960d1113d5510819555d5e360611beadbbab61d",
            "filename": "src/transformers/image_processing_base.py",
            "status": "modified",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fimage_processing_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fimage_processing_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_base.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -16,7 +16,6 @@\n import copy\n import json\n import os\n-import warnings\n from typing import Any, Optional, TypeVar, Union\n \n import numpy as np\n@@ -180,18 +179,6 @@ def from_pretrained(\n         kwargs[\"local_files_only\"] = local_files_only\n         kwargs[\"revision\"] = revision\n \n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if token is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            token = use_auth_token\n-\n         if token is not None:\n             kwargs[\"token\"] = token\n \n@@ -214,19 +201,6 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n             kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if kwargs.get(\"token\") is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            kwargs[\"token\"] = use_auth_token\n-\n         if os.path.isfile(save_directory):\n             raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n \n@@ -284,7 +258,6 @@ def get_image_processor_dict(\n         force_download = kwargs.pop(\"force_download\", False)\n         proxies = kwargs.pop(\"proxies\", None)\n         token = kwargs.pop(\"token\", None)\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n         local_files_only = kwargs.pop(\"local_files_only\", False)\n         revision = kwargs.pop(\"revision\", None)\n         subfolder = kwargs.pop(\"subfolder\", \"\")\n@@ -293,17 +266,6 @@ def get_image_processor_dict(\n         from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n         from_auto_class = kwargs.pop(\"_from_auto\", False)\n \n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if token is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            token = use_auth_token\n-\n         user_agent = {\"file_type\": \"image processor\", \"from_auto_class\": from_auto_class}\n         if from_pipeline is not None:\n             user_agent[\"using_pipeline\"] = from_pipeline"
        },
        {
            "sha": "6854b92fe946d3842d91fd1a9cd45cb4b28ea193",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -3518,20 +3518,8 @@ def save_pretrained(\n             kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n         ignore_metadata_errors = kwargs.pop(\"ignore_metadata_errors\", False)\n \n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if token is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            token = use_auth_token\n-\n         if token is not None:\n             kwargs[\"token\"] = token\n "
        },
        {
            "sha": "8c3283375acee6cadbe8bfae9a3db9e2675aab50",
            "filename": "src/transformers/models/auto/auto_factory.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -18,7 +18,6 @@\n import importlib\n import json\n import os\n-import warnings\n from collections import OrderedDict\n from collections.abc import Iterator\n from typing import Any, TypeVar, Union\n@@ -259,7 +258,6 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n             \"proxies\",\n             \"revision\",\n             \"subfolder\",\n-            \"use_auth_token\",\n             \"token\",\n         ]\n         hub_kwargs = {name: kwargs.pop(name) for name in hub_kwargs_names if name in kwargs}\n@@ -268,17 +266,6 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n         adapter_kwargs = kwargs.pop(\"adapter_kwargs\", None)\n \n         token = hub_kwargs.pop(\"token\", None)\n-        use_auth_token = hub_kwargs.pop(\"use_auth_token\", None)\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if token is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            token = use_auth_token\n \n         if token is not None:\n             hub_kwargs[\"token\"] = token"
        },
        {
            "sha": "7e2e84a445efdee86f8a3ba6c8733394d7589bec",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -17,7 +17,6 @@\n import importlib\n import os\n import re\n-import warnings\n from collections import OrderedDict\n from collections.abc import Callable, Iterator, KeysView, ValuesView\n from typing import Any, TypeVar, Union\n@@ -1315,18 +1314,6 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n         {'foo': False}\n         ```\n         \"\"\"\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if kwargs.get(\"token\") is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            kwargs[\"token\"] = use_auth_token\n-\n         kwargs[\"_from_auto\"] = True\n         kwargs[\"name_or_path\"] = pretrained_model_name_or_path\n         trust_remote_code = kwargs.pop(\"trust_remote_code\", None)"
        },
        {
            "sha": "746f14dd52fcb6841625f7c0eeb30f50df6490f3",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -17,7 +17,6 @@\n import importlib\n import json\n import os\n-import warnings\n from collections import OrderedDict\n from typing import Optional, Union\n \n@@ -168,16 +167,6 @@ def get_feature_extractor_config(\n     tokenizer.save_pretrained(\"tokenizer-test\")\n     tokenizer_config = get_tokenizer_config(\"tokenizer-test\")\n     ```\"\"\"\n-    use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-    if use_auth_token is not None:\n-        warnings.warn(\n-            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-            FutureWarning,\n-        )\n-        if token is not None:\n-            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n-        token = use_auth_token\n-\n     resolved_config_file = cached_file(\n         pretrained_model_name_or_path,\n         FEATURE_EXTRACTOR_NAME,\n@@ -285,18 +274,6 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n         >>> # If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)\n         >>> # feature_extractor = AutoFeatureExtractor.from_pretrained(\"./test/saved_model/\")\n         ```\"\"\"\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if kwargs.get(\"token\") is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            kwargs[\"token\"] = use_auth_token\n-\n         config = kwargs.pop(\"config\", None)\n         trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n         kwargs[\"_from_auto\"] = True"
        },
        {
            "sha": "46b99c13dc8d7e972a3aca925af15cd34b337fb1",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -306,16 +306,6 @@ def get_image_processor_config(\n     image_processor.save_pretrained(\"image-processor-test\")\n     image_processor_config = get_image_processor_config(\"image-processor-test\")\n     ```\"\"\"\n-    use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-    if use_auth_token is not None:\n-        warnings.warn(\n-            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-            FutureWarning,\n-        )\n-        if token is not None:\n-            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n-        token = use_auth_token\n-\n     resolved_config_file = cached_file(\n         pretrained_model_name_or_path,\n         IMAGE_PROCESSOR_NAME,\n@@ -437,18 +427,6 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n         >>> # If image processor files are in a directory (e.g. image processor was saved using *save_pretrained('./test/saved_model/')*)\n         >>> # image_processor = AutoImageProcessor.from_pretrained(\"./test/saved_model/\")\n         ```\"\"\"\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if kwargs.get(\"token\") is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            kwargs[\"token\"] = use_auth_token\n-\n         config = kwargs.pop(\"config\", None)\n         # TODO: @yoni, change in v4.48 (use_fast set to True by default)\n         use_fast = kwargs.pop(\"use_fast\", None)"
        },
        {
            "sha": "5186b78b07e0fec3c3785383497130e6e0f767a9",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -17,7 +17,6 @@\n import importlib\n import inspect\n import json\n-import warnings\n from collections import OrderedDict\n \n # Build the list of all feature extractors\n@@ -265,18 +264,6 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n         >>> # If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)\n         >>> # processor = AutoProcessor.from_pretrained(\"./test/saved_model/\")\n         ```\"\"\"\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if kwargs.get(\"token\") is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            kwargs[\"token\"] = use_auth_token\n-\n         config = kwargs.pop(\"config\", None)\n         trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n         kwargs[\"_from_auto\"] = True"
        },
        {
            "sha": "a861aee12c57f2fa0827cd0ca5f5076f471d6665",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -17,7 +17,6 @@\n import importlib\n import json\n import os\n-import warnings\n from collections import OrderedDict\n from typing import Any, Optional, Union\n \n@@ -894,16 +893,6 @@ def get_tokenizer_config(\n     tokenizer.save_pretrained(\"tokenizer-test\")\n     tokenizer_config = get_tokenizer_config(\"tokenizer-test\")\n     ```\"\"\"\n-    use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-    if use_auth_token is not None:\n-        warnings.warn(\n-            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-            FutureWarning,\n-        )\n-        if token is not None:\n-            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n-        token = use_auth_token\n-\n     commit_hash = kwargs.get(\"_commit_hash\")\n     resolved_config_file = cached_file(\n         pretrained_model_name_or_path,\n@@ -1021,18 +1010,6 @@ def from_pretrained(\n         >>> # Download vocabulary from huggingface.co and define model-specific arguments\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\", add_prefix_space=True)\n         ```\"\"\"\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if kwargs.get(\"token\") is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            kwargs[\"token\"] = use_auth_token\n-\n         config = kwargs.pop(\"config\", None)\n         kwargs[\"_from_auto\"] = True\n "
        },
        {
            "sha": "d9ac45af912f53ec6fd26aea5e3c493e9f283ac9",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -17,7 +17,6 @@\n import importlib\n import json\n import os\n-import warnings\n from collections import OrderedDict\n from typing import TYPE_CHECKING, Optional, Union\n \n@@ -168,16 +167,6 @@ def get_video_processor_config(\n     video_processor.save_pretrained(\"video-processor-test\")\n     video_processor = get_video_processor_config(\"video-processor-test\")\n     ```\"\"\"\n-    use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-    if use_auth_token is not None:\n-        warnings.warn(\n-            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-            FutureWarning,\n-        )\n-        if token is not None:\n-            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n-        token = use_auth_token\n-\n     resolved_config_file = cached_file(\n         pretrained_model_name_or_path,\n         VIDEO_PROCESSOR_NAME,\n@@ -283,18 +272,6 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n         >>> # If video processor files are in a directory (e.g. video processor was saved using *save_pretrained('./test/saved_model/')*)\n         >>> # video_processor = AutoVideoProcessor.from_pretrained(\"./test/saved_model/\")\n         ```\"\"\"\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if kwargs.get(\"token\") is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            kwargs[\"token\"] = use_auth_token\n-\n         config = kwargs.pop(\"config\", None)\n         trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n         kwargs[\"_from_auto\"] = True"
        },
        {
            "sha": "b14924f1eeeb5e32b5985b599f9e07f02813d8f1",
            "filename": "src/transformers/models/bark/processing_bark.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -85,7 +85,7 @@ def from_pretrained(\n                 Additional keyword arguments passed along to both\n                 [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`].\n         \"\"\"\n-\n+        token = kwargs.get(\"token\")\n         if speaker_embeddings_dict_path is not None:\n             speaker_embeddings_path = cached_file(\n                 pretrained_processor_name_or_path,\n@@ -95,7 +95,7 @@ def from_pretrained(\n                 force_download=kwargs.pop(\"force_download\", False),\n                 proxies=kwargs.pop(\"proxies\", None),\n                 local_files_only=kwargs.pop(\"local_files_only\", False),\n-                token=kwargs.pop(\"use_auth_token\", None),\n+                token=token,\n                 revision=kwargs.pop(\"revision\", None),\n                 _raise_exceptions_for_gated_repo=False,\n                 _raise_exceptions_for_missing_entries=False,\n@@ -181,6 +181,7 @@ def _load_voice_preset(self, voice_preset: Optional[str] = None, **kwargs):\n         voice_preset_paths = self.speaker_embeddings[voice_preset]\n \n         voice_preset_dict = {}\n+        token = kwargs.get(\"token\")\n         for key in [\"semantic_prompt\", \"coarse_prompt\", \"fine_prompt\"]:\n             if key not in voice_preset_paths:\n                 raise ValueError(\n@@ -195,7 +196,7 @@ def _load_voice_preset(self, voice_preset: Optional[str] = None, **kwargs):\n                 force_download=kwargs.pop(\"force_download\", False),\n                 proxies=kwargs.pop(\"proxies\", None),\n                 local_files_only=kwargs.pop(\"local_files_only\", False),\n-                token=kwargs.pop(\"use_auth_token\", None),\n+                token=token,\n                 revision=kwargs.pop(\"revision\", None),\n                 _raise_exceptions_for_gated_repo=False,\n                 _raise_exceptions_for_missing_entries=False,"
        },
        {
            "sha": "3953a3e5fd1f4e571a79d67f92a1a02ea63bb9a2",
            "filename": "src/transformers/models/clvp/configuration_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -132,8 +132,6 @@ def __init__(\n     def from_pretrained(\n         cls, pretrained_model_name_or_path: Union[str, os.PathLike], config_type: str = \"text_config\", **kwargs\n     ):\n-        cls._set_token_in_kwargs(kwargs)\n-\n         config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n \n         # make sure to have the config_type be either \"text_config\" or \"speech_config\""
        },
        {
            "sha": "57fae4d9d605fb23e313437b7cd737420e3bfb84",
            "filename": "src/transformers/models/deprecated/jukebox/configuration_jukebox.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -347,8 +347,6 @@ def __init__(\n \n     @classmethod\n     def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], level=0, **kwargs):\n-        cls._set_token_in_kwargs(kwargs)\n-\n         config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n \n         # get the prior config dict if we are loading from JukeboxConfig\n@@ -471,8 +469,6 @@ def __init__(\n \n     @classmethod\n     def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs):\n-        cls._set_token_in_kwargs(kwargs)\n-\n         config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n \n         # get the text config dict if we are loading from CLIPConfig"
        },
        {
            "sha": "8b10750d3966932968905670e0408d6ecbfd1b73",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -3780,7 +3780,7 @@ def from_pretrained(\n             force_download=kwargs.pop(\"force_download\", False),\n             proxies=kwargs.pop(\"proxies\", None),\n             local_files_only=kwargs.pop(\"local_files_only\", False),\n-            token=kwargs.pop(\"use_auth_token\", None),\n+            token=token,\n             revision=kwargs.pop(\"revision\", None),\n         )\n         if spk_path is None:"
        },
        {
            "sha": "80ba844522a221ea1c1f8dfa0d05a47594b3d4ff",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -4082,7 +4082,7 @@ def from_pretrained(\n             force_download=kwargs.pop(\"force_download\", False),\n             proxies=kwargs.pop(\"proxies\", None),\n             local_files_only=kwargs.pop(\"local_files_only\", False),\n-            token=kwargs.pop(\"use_auth_token\", None),\n+            token=token,\n             revision=kwargs.pop(\"revision\", None),\n         )\n         if spk_path is None:"
        },
        {
            "sha": "82399d0933dc692b8b7911022e32155fea28bd63",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -1163,21 +1163,9 @@ def load_adapter(self, target_lang: str, force_load=True, **kwargs):\n         proxies = kwargs.pop(\"proxies\", None)\n         local_files_only = kwargs.pop(\"local_files_only\", False)\n         token = kwargs.pop(\"token\", None)\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n         revision = kwargs.pop(\"revision\", None)\n         use_safetensors = kwargs.pop(\"use_safetensors\", None)\n \n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if token is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            token = use_auth_token\n-\n         model_path_or_id = self.config._name_or_path\n         state_dict = None\n "
        },
        {
            "sha": "87db1981cbc083eaceb63dd5a9bed363966be8af",
            "filename": "src/transformers/pipelines/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 23,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2F__init__.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -341,16 +341,6 @@ def get_supported_tasks() -> list[str]:\n \n \n def get_task(model: str, token: Optional[str] = None, **deprecated_kwargs) -> str:\n-    use_auth_token = deprecated_kwargs.pop(\"use_auth_token\", None)\n-    if use_auth_token is not None:\n-        warnings.warn(\n-            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-            FutureWarning,\n-        )\n-        if token is not None:\n-            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n-        token = use_auth_token\n-\n     if is_offline_mode():\n         raise RuntimeError(\"You cannot infer task automatically within `pipeline` when using offline mode\")\n     try:\n@@ -639,9 +629,9 @@ def pipeline(\n             artifacts on huggingface.co, so `revision` can be any identifier allowed by git.\n         use_fast (`bool`, *optional*, defaults to `True`):\n             Whether or not to use a Fast tokenizer if possible (a [`PreTrainedTokenizerFast`]).\n-        use_auth_token (`str` or *bool*, *optional*):\n+        token (`str` or *bool*, *optional*):\n             The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n-            when running `hf auth login` (stored in `~/.huggingface`).\n+            when running `hf auth login`.\n         device (`int` or `str` or `torch.device`):\n             Defines the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank like `1`) on which this\n             pipeline will be allocated.\n@@ -694,17 +684,6 @@ def pipeline(\n     ```\"\"\"\n     if model_kwargs is None:\n         model_kwargs = {}\n-    # Make sure we only pass use_auth_token once as a kwarg (it used to be possible to pass it in model_kwargs,\n-    # this is to keep BC).\n-    use_auth_token = model_kwargs.pop(\"use_auth_token\", None)\n-    if use_auth_token is not None:\n-        warnings.warn(\n-            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-            FutureWarning,\n-        )\n-        if token is not None:\n-            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n-        token = use_auth_token\n \n     code_revision = kwargs.pop(\"code_revision\", None)\n     commit_hash = kwargs.pop(\"_commit_hash\", None)"
        },
        {
            "sha": "ada2bae8f70b09470c71bc28bda582fafe3db6c4",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -22,7 +22,6 @@\n import sys\n import traceback\n import types\n-import warnings\n from abc import ABC, abstractmethod\n from collections import UserDict\n from contextlib import contextmanager\n@@ -974,19 +973,6 @@ def save_pretrained(\n             kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if kwargs.get(\"token\") is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            kwargs[\"token\"] = use_auth_token\n-\n         if os.path.isfile(save_directory):\n             logger.error(f\"Provided path ({save_directory}) should be a directory, not a file\")\n             return"
        },
        {
            "sha": "2a0fc63a0a66655be7efa7703a53d786fced6ce6",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -22,7 +22,6 @@\n import os\n import sys\n import typing\n-import warnings\n from dataclasses import dataclass\n from pathlib import Path\n from typing import Annotated, Any, Literal, Optional, TypedDict, TypeVar, Union\n@@ -773,20 +772,8 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n             kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n         save_jinja_files = kwargs.pop(\"save_jinja_files\", True)\n \n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if kwargs.get(\"token\") is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            kwargs[\"token\"] = use_auth_token\n-\n         os.makedirs(save_directory, exist_ok=True)\n \n         if push_to_hub:\n@@ -1401,18 +1388,6 @@ def from_pretrained(\n         kwargs[\"local_files_only\"] = local_files_only\n         kwargs[\"revision\"] = revision\n \n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if token is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            token = use_auth_token\n-\n         if token is not None:\n             kwargs[\"token\"] = token\n "
        },
        {
            "sha": "002464266eef2b79b82e6b54626043772d54dee7",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -1944,24 +1944,12 @@ def from_pretrained(\n         assert tokenizer.unk_token == \"<unk>\"\n         ```\"\"\"\n         proxies = kwargs.pop(\"proxies\", None)\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n         subfolder = kwargs.pop(\"subfolder\", None)\n         from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n         from_auto_class = kwargs.pop(\"_from_auto\", False)\n         commit_hash = kwargs.pop(\"_commit_hash\", None)\n         gguf_file = kwargs.get(\"gguf_file\")\n \n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if token is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            token = use_auth_token\n-\n         user_agent = {\"file_type\": \"tokenizer\", \"from_auto_class\": from_auto_class, \"is_fast\": \"Fast\" in cls.__name__}\n         if from_pipeline is not None:\n             user_agent[\"using_pipeline\"] = from_pipeline\n@@ -2511,20 +2499,8 @@ def save_pretrained(\n         Returns:\n             A tuple of `str`: The files saved.\n         \"\"\"\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n         save_jinja_files = kwargs.pop(\"save_jinja_files\", True)\n \n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if kwargs.get(\"token\") is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            kwargs[\"token\"] = use_auth_token\n-\n         if os.path.isfile(save_directory):\n             logger.error(f\"Provided path ({save_directory}) should be a directory, not a file\")\n             return"
        },
        {
            "sha": "8f6d3c16bbdc1ba446b269c4701dcc161ff254b7",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -407,16 +407,6 @@ def cached_files(\n     model_weights_file = cached_file(\"google-bert/bert-base-uncased\", \"pytorch_model.bin\")\n     ```\n     \"\"\"\n-    use_auth_token = deprecated_kwargs.pop(\"use_auth_token\", None)\n-    if use_auth_token is not None:\n-        warnings.warn(\n-            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-            FutureWarning,\n-        )\n-        if token is not None:\n-            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n-        token = use_auth_token\n-\n     if is_offline_mode() and not local_files_only:\n         logger.info(\"Offline mode: forcing local_files_only=True\")\n         local_files_only = True\n@@ -642,16 +632,6 @@ def has_file(\n \n     </Tip>\n     \"\"\"\n-    use_auth_token = deprecated_kwargs.pop(\"use_auth_token\", None)\n-    if use_auth_token is not None:\n-        warnings.warn(\n-            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-            FutureWarning,\n-        )\n-        if token is not None:\n-            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n-        token = use_auth_token\n-\n     # If path to local directory, check if the file exists\n     if os.path.isdir(path_or_repo):\n         return os.path.isfile(os.path.join(path_or_repo, filename))\n@@ -901,21 +881,10 @@ def push_to_hub(\n         {object}.push_to_hub(\"huggingface/my-finetuned-bert\")\n         ```\n         \"\"\"\n-        use_auth_token = deprecated_kwargs.pop(\"use_auth_token\", None)\n         ignore_metadata_errors = deprecated_kwargs.pop(\"ignore_metadata_errors\", False)\n         save_jinja_files = deprecated_kwargs.pop(\n             \"save_jinja_files\", None\n         )  # TODO: This is only used for testing and should be removed once save_jinja_files becomes the default\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if token is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            token = use_auth_token\n \n         repo_path_or_name = deprecated_kwargs.pop(\"repo_path_or_name\", None)\n         if repo_path_or_name is not None:\n@@ -1044,17 +1013,6 @@ def get_checkpoint_shard_files(\n     For the description of each arg, see [`PreTrainedModel.from_pretrained`]. `index_filename` is the full path to the\n     index (downloaded and cached if `pretrained_model_name_or_path` is a model ID on the Hub).\n     \"\"\"\n-\n-    use_auth_token = deprecated_kwargs.pop(\"use_auth_token\", None)\n-    if use_auth_token is not None:\n-        warnings.warn(\n-            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-            FutureWarning,\n-        )\n-        if token is not None:\n-            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n-        token = use_auth_token\n-\n     if not os.path.isfile(index_filename):\n         raise ValueError(f\"Can't find a checkpoint index ({index_filename}) in {pretrained_model_name_or_path}.\")\n "
        },
        {
            "sha": "08de802f22f5b91d177eb823b3b0b7d2c91b92ad",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -539,18 +539,6 @@ def from_pretrained(\n         kwargs[\"local_files_only\"] = local_files_only\n         kwargs[\"revision\"] = revision\n \n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if token is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            token = use_auth_token\n-\n         if token is not None:\n             kwargs[\"token\"] = token\n \n@@ -573,19 +561,6 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n             kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if kwargs.get(\"token\") is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            kwargs[\"token\"] = use_auth_token\n-\n         if os.path.isfile(save_directory):\n             raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n \n@@ -641,25 +616,13 @@ def get_video_processor_dict(\n         force_download = kwargs.pop(\"force_download\", False)\n         proxies = kwargs.pop(\"proxies\", None)\n         token = kwargs.pop(\"token\", None)\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n         local_files_only = kwargs.pop(\"local_files_only\", False)\n         revision = kwargs.pop(\"revision\", None)\n         subfolder = kwargs.pop(\"subfolder\", \"\")\n \n         from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n         from_auto_class = kwargs.pop(\"_from_auto\", False)\n \n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if token is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            token = use_auth_token\n-\n         user_agent = {\"file_type\": \"video processor\", \"from_auto_class\": from_auto_class}\n         if from_pipeline is not None:\n             user_agent[\"using_pipeline\"] = from_pipeline"
        },
        {
            "sha": "79e982bc483288a9801a837480919a5258a208d8",
            "filename": "tests/sagemaker/scripts/pytorch/run_glue_model_parallelism.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -172,15 +172,6 @@ class ModelArguments:\n         default=\"main\",\n         metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n     )\n-    use_auth_token: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Will use the token generated when running `hf auth login` (necessary to use this script \"\n-                \"with private models).\"\n-            )\n-        },\n-    )\n \n \n def main():\n@@ -294,22 +285,19 @@ def main():\n         finetuning_task=data_args.task_name,\n         cache_dir=model_args.cache_dir,\n         revision=model_args.model_revision,\n-        token=True if model_args.use_auth_token else None,\n     )\n     tokenizer = AutoTokenizer.from_pretrained(\n         model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n         cache_dir=model_args.cache_dir,\n         use_fast=model_args.use_fast_tokenizer,\n         revision=model_args.model_revision,\n-        token=True if model_args.use_auth_token else None,\n     )\n     model = AutoModelForSequenceClassification.from_pretrained(\n         model_args.model_name_or_path,\n         from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n         config=config,\n         cache_dir=model_args.cache_dir,\n         revision=model_args.model_revision,\n-        token=True if model_args.use_auth_token else None,\n     )\n \n     # Preprocessing the datasets"
        },
        {
            "sha": "ea25f367cc9e68bafc0207523ed1369db6c9999b",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/252d7cd9524e561171bea3b7474f96cef9fd3465/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252d7cd9524e561171bea3b7474f96cef9fd3465/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=252d7cd9524e561171bea3b7474f96cef9fd3465",
            "patch": "@@ -295,9 +295,7 @@ def test_offline(self):\n                     hub.TRANSFORMERS_CACHE = tmpdir\n                     # First offline load should fail\n                     try:\n-                        AutoModelForImageClassification.from_pretrained(\n-                            TINY_IMAGE_CLASSIF, revision=\"main\", use_auth_token=None\n-                        )\n+                        AutoModelForImageClassification.from_pretrained(TINY_IMAGE_CLASSIF, revision=\"main\")\n                     except OSError:\n                         LOG.info(\"Loading model %s in offline mode failed as expected\", TINY_IMAGE_CLASSIF)\n                     else:\n@@ -310,9 +308,7 @@ def test_offline(self):\n \n                     LOG.info(\"Model %s downloaded in %s\", TINY_IMAGE_CLASSIF, local_dir)\n \n-                    AutoModelForImageClassification.from_pretrained(\n-                        TINY_IMAGE_CLASSIF, revision=\"main\", use_auth_token=None\n-                    )\n+                    AutoModelForImageClassification.from_pretrained(TINY_IMAGE_CLASSIF, revision=\"main\")\n             finally:\n                 # Tear down: reset env as it was before calling this test\n                 hub._is_offline_mode = offlfine_env\n@@ -340,7 +336,7 @@ def test_local_files_only(self):\n                     hub.TRANSFORMERS_CACHE = tmpdir\n                     try:\n                         AutoModelForImageClassification.from_pretrained(\n-                            TINY_IMAGE_CLASSIF, revision=\"main\", use_auth_token=None, local_files_only=True\n+                            TINY_IMAGE_CLASSIF, revision=\"main\", local_files_only=True\n                         )\n                     except OSError:\n                         LOG.info(\"Loading model %s in offline mode failed as expected\", TINY_IMAGE_CLASSIF)\n@@ -354,7 +350,7 @@ def test_local_files_only(self):\n                     LOG.info(\"Model %s downloaded in %s\", TINY_IMAGE_CLASSIF, local_dir)\n \n                     AutoModelForImageClassification.from_pretrained(\n-                        TINY_IMAGE_CLASSIF, revision=\"main\", use_auth_token=None, local_files_only=True\n+                        TINY_IMAGE_CLASSIF, revision=\"main\", local_files_only=True\n                     )\n             finally:\n                 # Tear down: reset env as it was before calling this test\n@@ -2437,7 +2433,7 @@ def test_push_to_hub_with_description(self):\n ```\n \"\"\"\n             commit_details = model.push_to_hub(\n-                tmp_repo.repo_id, use_auth_token=self._token, create_pr=True, commit_description=COMMIT_DESCRIPTION\n+                tmp_repo.repo_id, create_pr=True, token=self._token, commit_description=COMMIT_DESCRIPTION\n             )\n             self.assertEqual(commit_details.commit_description, COMMIT_DESCRIPTION)\n "
        }
    ],
    "stats": {
        "total": 541,
        "additions": 15,
        "deletions": 526
    }
}