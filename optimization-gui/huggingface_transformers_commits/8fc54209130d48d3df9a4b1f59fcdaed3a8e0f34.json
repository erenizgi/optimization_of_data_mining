{
    "author": "RyanMullins",
    "message": "Gemma3 conversion script maintenance (#41704)\n\n* conversion: add include_vision_encoder flag (default true)\n\n* conversion: update for inverted model.language_model weight path\n\n* conversion: revert include_vision_encoder to True by default\n\n* conversion: add chat template path flag",
    "sha": "8fc54209130d48d3df9a4b1f59fcdaed3a8e0f34",
    "files": [
        {
            "sha": "c11176d38637734dfd5f1063b59df90a6874da3a",
            "filename": "src/transformers/models/gemma3/convert_gemma3_weights.py",
            "status": "modified",
            "additions": 63,
            "deletions": 27,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/8fc54209130d48d3df9a4b1f59fcdaed3a8e0f34/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8fc54209130d48d3df9a4b1f59fcdaed3a8e0f34/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights.py?ref=8fc54209130d48d3df9a4b1f59fcdaed3a8e0f34",
            "patch": "@@ -21,6 +21,7 @@\n     --tokenizer_path=\"$HOME/gemma3/tokenizer/gemma3_cleaned_262144_v2.spiece.model\" \\\n     --checkpoint_path=\"$HOME/gemma3/gemma3_4b_pt_orbax/\" \\\n     --output_path=\"$HOME/gemma3/gemma3_4b_pt_safetensors/\"\n+    --include_vision_encoder\n \"\"\"\n \n from collections.abc import Iterator, Sequence\n@@ -182,7 +183,7 @@\n     ),\n     _VARIANT_GEMMA_3_4B: Gemma3Config(\n         text_config=Gemma3TextConfig(\n-            vocab_size=262_208,\n+            vocab_size=262_144,\n             hidden_size=2560,\n             intermediate_size=2560 * 8 // 2,\n             num_attention_heads=8,\n@@ -200,7 +201,7 @@\n     ),\n     _VARIANT_GEMMA_3_12B: Gemma3Config(\n         text_config=Gemma3TextConfig(\n-            vocab_size=262_208,\n+            vocab_size=262_144,\n             hidden_size=30 * 128,\n             intermediate_size=30 * 128 * 8 // 2,\n             num_attention_heads=16,\n@@ -218,7 +219,7 @@\n     ),\n     _VARIANT_GEMMA_3_27B: Gemma3Config(\n         text_config=Gemma3TextConfig(\n-            vocab_size=262_208,\n+            vocab_size=262_144,\n             hidden_size=42 * 128,\n             intermediate_size=42 * 128 * 8 // 2,\n             num_attention_heads=32,\n@@ -236,10 +237,15 @@\n     ),\n }\n \n-_TEXT_ONLY_VARIANTS = (_VARIANT_EMBEDDINGGEMMA, _VARIANT_GEMMA_3_270M, _VARIANT_GEMMA_3_1B)\n-\n # ==== Flags ====\n \n+_CHAT_TEMPLATE_PATH = flags.DEFINE_string(\n+    name=\"chat_template_path\",\n+    default=None,\n+    help=\"Path to the chat template.\",\n+    required=False,\n+)\n+\n _CHECKPOINT_PATH = flags.DEFINE_string(\n     name=\"checkpoint_path\",\n     default=None,\n@@ -251,6 +257,15 @@\n     name=\"include_chat_template\", default=False, help=\"If true, will save the default chat template with the tokenizer\"\n )\n \n+_INCLUDE_VISION_ENCODER = flags.DEFINE_bool(\n+    name=\"include_vision_encoder\",\n+    default=True,\n+    help=(\n+        \"If true, the model will expect vision weights in the checkpoint at `checkpoint_path` an if not found loading\"\n+        \" the weights will throw a `RuntimeError`.\"\n+    ),\n+)\n+\n _OUTPUT_PATH = flags.DEFINE_string(\n     name=\"output_path\",\n     default=None,\n@@ -299,6 +314,17 @@\n )\n \n \n+def get_chat_template() -> Optional[str]:\n+    if not _INCLUDE_CHAT_TEMPLATE.value:\n+        return None\n+\n+    if _CHAT_TEMPLATE_PATH.value:\n+        with open(_CHAT_TEMPLATE_PATH.value, \"r\") as f:\n+            return f.read()\n+\n+    return _CHAT_TEMPLATE\n+\n+\n def convert_siglip_weight(\n     config: SiglipVisionConfig,\n     paths: Sequence[str],\n@@ -405,35 +431,36 @@ def convert_transformer_weights(\n     if path.endswith(_TRANSFORMER_EMBEDDER):\n         if prop == \"input_embedding\":\n             # Tied to language_model.lm_head.weight, assigned at the end.\n-            converted_paths = [\"language_model.model.embed_tokens.weight\"]\n+            converted_paths = [\"model.language_model.embed_tokens.weight\"]\n \n-            if _VARIANT.value not in _TEXT_ONLY_VARIANTS:\n+            if _INCLUDE_VISION_ENCODER.value:\n                 # Gemma3 model doesn't have image soft token in input and output embeddings, resize to avoid bugs we had with Mllama\n                 pre_expansion_embeddings = weights\n                 mu = np.mean(pre_expansion_embeddings, axis=0)\n                 sigma = np.cov(pre_expansion_embeddings, rowvar=False, bias=True)\n                 new_embeddings = np.random.multivariate_normal(mu, 1e-5 * sigma, size=64)\n                 weights = np.vstack([pre_expansion_embeddings, new_embeddings])\n+                config.vocab_size += 64\n \n             converted_weights = [weights]\n-        elif _VARIANT.value in _TEXT_ONLY_VARIANTS or prop in (\"mm_output_embedding\", \"mm_input_embedding_extra\"):\n+        elif not _INCLUDE_VISION_ENCODER.value or prop in (\"mm_output_embedding\", \"mm_input_embedding_extra\"):\n             return zip([], [])\n         else:\n             raise ValueError(f\"Unexpected member, {prop}, in Embedder.\")\n     elif path.startswith(f\"{_TRANSFORMER_EMBEDDER}/mm\"):\n-        if _VARIANT.value in _TEXT_ONLY_VARIANTS:\n+        if not _INCLUDE_VISION_ENCODER.value:\n             return zip([], [])\n \n         if path.endswith(\"/mm_input_projection\"):\n-            converted_paths = [\"multi_modal_projector.mm_input_projection_weight\"]\n+            converted_paths = [\"model.multi_modal_projector.mm_input_projection_weight\"]\n             converted_weights = [weights]\n         elif path.endswith(\"/mm_soft_embedding_norm\"):\n-            converted_paths = [\"multi_modal_projector.mm_soft_emb_norm.weight\"]\n+            converted_paths = [\"model.multi_modal_projector.mm_soft_emb_norm.weight\"]\n             converted_weights = [weights]\n         else:\n             raise ValueError(f\"Unexpected subpath, `{path}`, in Embedder.\")\n     elif path.endswith(_TRANSFORMER_FINAL_NORM):\n-        converted_paths = [\"language_model.model.norm.weight\"]\n+        converted_paths = [\"model.language_model.norm.weight\"]\n         converted_weights = [weights]\n     elif _TRANSFORMER_DECODER_BLOCK in path:\n         decoder_block_start = path.find(_TRANSFORMER_DECODER_BLOCK)\n@@ -443,7 +470,7 @@ def convert_transformer_weights(\n         layer_idx = decoder_block_path[:next_path_separator_idx]\n         decoder_block_path = decoder_block_path[next_path_separator_idx:]\n \n-        base_path = f\"language_model.model.layers.{layer_idx}\"\n+        base_path = f\"model.language_model.layers.{layer_idx}\"\n \n         if path.endswith(\"attn/attn_vec_einsum\"):\n             converted_paths = [f\"{base_path}.self_attn.o_proj.weight\"]\n@@ -522,26 +549,35 @@ def update_tree(path: str, weights: np.ndarray, target_dtype: torch.dtype) -> No\n \n     for paths, value in orbax_tree_flat:\n         if paths[0].startswith(\"SigLiPFromPatches_\"):\n-            if config.vision_config is None:\n+            if not _INCLUDE_VISION_ENCODER.value:\n                 continue\n \n             path, weights = convert_siglip_weight(config=config.vision_config, paths=paths, weights=value)\n             update_tree(path, weights, config.vision_config.dtype)\n         else:\n             for path, weights in convert_transformer_weights(config=config.text_config, paths=paths, weights=value):\n-                if variant in _TEXT_ONLY_VARIANTS:\n-                    path = path[len(\"language_model.\") :]\n+                if not _INCLUDE_VISION_ENCODER.value:\n+                    # Paths generated during weights conversion assume it is targeting a Gemma3ForConditionalGeneration\n+                    # model, which has a Gemma3TextModel at \"model.language_model\". If _INCLUDE_VISION_ENCODER.value is\n+                    # False, then this is targeting a Gemma3ForCausalLM, which has its Gemma3TextModel at \"model\", so\n+                    # the \"language_model.\" portion of the path needs to be removed prior to calling load_state_dict().\n+                    path = path.replace(\"language_model.\", \"\")\n+\n                 if variant == _VARIANT_EMBEDDINGGEMMA:\n+                    # EmbeddingGemma only the Gemma3TextModel instead of an LLM of VLM class for loading weights, and\n+                    # defers final model construction to SentenceTransformers, so the \"model.\" portion of the path\n+                    # needs to be removed prior to calling load_state_dict().\n                     path = path[len(\"model.\") :]\n \n                 update_tree(path, weights, config.text_config.dtype)\n \n     if variant == _VARIANT_EMBEDDINGGEMMA:\n         return hf_tree, [weight[1].T for weight in orbax_tree_flat[: _NUM_LINEAR_LAYERS.value]]\n-    elif config.vision_config is None:\n-        hf_tree[\"lm_head.weight\"] = hf_tree[\"model.embed_tokens.weight\"]\n+\n+    if _INCLUDE_VISION_ENCODER.value:\n+        hf_tree[\"lm_head.weight\"] = hf_tree[\"model.language_model.embed_tokens.weight\"]\n     else:\n-        hf_tree[\"language_model.lm_head.weight\"] = hf_tree[\"language_model.model.embed_tokens.weight\"]\n+        hf_tree[\"lm_head.weight\"] = hf_tree[\"model.embed_tokens.weight\"]\n \n     return hf_tree, None\n \n@@ -555,10 +591,10 @@ def main(*args):\n     config = _VARIANTS[variant]\n     config.text_config.dtype = getattr(torch, _TRANSFORMER_DTYPE.value)\n \n-    if variant in _TEXT_ONLY_VARIANTS:\n-        config.vision_config = None\n-    else:\n+    if _INCLUDE_VISION_ENCODER.value:\n         config.vision_config.dtype = getattr(torch, _VISION_DTYPE.value)\n+    else:\n+        config.vision_config = None\n \n     if _INCLUDE_CHAT_TEMPLATE.value:\n         # Chat template is included for instruction tuned models, which treat\n@@ -577,10 +613,10 @@ def main(*args):\n     with accelerate.init_empty_weights():\n         if variant == _VARIANT_EMBEDDINGGEMMA:\n             model = Gemma3TextModel(config=config.text_config)\n-        elif variant in _TEXT_ONLY_VARIANTS:\n-            model = Gemma3ForCausalLM(config=config.text_config)\n-        else:\n+        elif _INCLUDE_VISION_ENCODER.value:\n             model = Gemma3ForConditionalGeneration(config)\n+        else:\n+            model = Gemma3ForCausalLM(config=config.text_config)\n \n     model.load_state_dict(state_tree, assign=True, strict=True)\n     logging.info(\n@@ -608,12 +644,12 @@ def main(*args):\n             \"boi_token\": \"<start_of_image>\",  # Should be ID=255_999\n             \"eoi_token\": \"<end_of_image>\",  # Should be ID=256_000\n         },\n-        chat_template=_CHAT_TEMPLATE if _INCLUDE_CHAT_TEMPLATE.value else None,\n+        chat_template=get_chat_template(),\n     )\n     tokenizer.save_pretrained(output_path)\n     logging.info(\"Saved GemmaTokenizer for %s to %s\", variant, output_path)\n \n-    if variant not in _TEXT_ONLY_VARIANTS:\n+    if _INCLUDE_VISION_ENCODER.value:\n         image_processor = Gemma3ImageProcessor(\n             image_seq_length=256,\n             image_mean=(0.5,) * 3,"
        }
    ],
    "stats": {
        "total": 90,
        "additions": 63,
        "deletions": 27
    }
}