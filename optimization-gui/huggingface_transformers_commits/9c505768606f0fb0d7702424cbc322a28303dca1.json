{
    "author": "dxoigmn",
    "message": "[mllama] Allow `pixel_values` with `inputs_embeds` (#38334)\n\n* Allow pixel_values and inputs_embeds at the same time\n\n* remove unnecessary overwritten tests",
    "sha": "9c505768606f0fb0d7702424cbc322a28303dca1",
    "files": [
        {
            "sha": "c28671c81f86b20fe4ebf3d1c92befe5c6520b0f",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9c505768606f0fb0d7702424cbc322a28303dca1/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9c505768606f0fb0d7702424cbc322a28303dca1/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=9c505768606f0fb0d7702424cbc322a28303dca1",
            "patch": "@@ -1699,11 +1699,6 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n         if pixel_values is not None and cross_attention_states is not None:\n             raise ValueError(\"`pixel_values` and `cross_attention_states` cannot be provided simultaneously\")\n "
        },
        {
            "sha": "eed367f3aa3789594c8af17010de7454c2208584",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/9c505768606f0fb0d7702424cbc322a28303dca1/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9c505768606f0fb0d7702424cbc322a28303dca1/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=9c505768606f0fb0d7702424cbc322a28303dca1",
            "patch": "@@ -285,49 +285,6 @@ def setUp(self):\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    # while some other models require pixel_values to be present\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n     def test_resize_embeddings_results_in_successful_loss(self):\n         # resizing embeddings should result in successful loss computation\n         config, inputs = self.model_tester.prepare_config_and_inputs_for_common()"
        }
    ],
    "stats": {
        "total": 48,
        "additions": 0,
        "deletions": 48
    }
}