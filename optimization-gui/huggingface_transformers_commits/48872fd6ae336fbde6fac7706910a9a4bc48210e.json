{
    "author": "yonigozlan",
    "message": "Add Image Processor Fast RT-DETR (#34354)\n\n* add fast image processor rtdetr\r\n\r\n* add gpu/cpu test and fix docstring\r\n\r\n* remove prints\r\n\r\n* add to doc\r\n\r\n* nit docstring\r\n\r\n* avoid iterating over images/annotations several times\r\n\r\n* change torch typing\r\n\r\n* Add image processor fast documentation",
    "sha": "48872fd6ae336fbde6fac7706910a9a4bc48210e",
    "files": [
        {
            "sha": "320916f1ce94219feb0eb0c4dc22ec247e7c1fe9",
            "filename": "docs/source/en/main_classes/image_processor.md",
            "status": "modified",
            "additions": 43,
            "deletions": 0,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/48872fd6ae336fbde6fac7706910a9a4bc48210e/docs%2Fsource%2Fen%2Fmain_classes%2Fimage_processor.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/48872fd6ae336fbde6fac7706910a9a4bc48210e/docs%2Fsource%2Fen%2Fmain_classes%2Fimage_processor.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fimage_processor.md?ref=48872fd6ae336fbde6fac7706910a9a4bc48210e",
            "patch": "@@ -18,6 +18,49 @@ rendered properly in your Markdown viewer.\n \n An image processor is in charge of preparing input features for vision models and post processing their outputs. This includes transformations such as resizing, normalization, and conversion to PyTorch, TensorFlow, Flax and Numpy tensors. It may also include model specific post-processing such as converting logits to segmentation masks.\n \n+Fast image processors are available for a few models and more will be added in the future. They are based on the [torchvision](https://pytorch.org/vision/stable/index.html) library and provide a significant speed-up, especially when processing on GPU.\n+They have the same API as the base image processors and can be used as drop-in replacements.\n+To use a fast image processor, you need to install the `torchvision` library, and set the `use_fast` argument to `True` when instantiating the image processor:\n+\n+```python\n+from transformers import AutoImageProcessor\n+\n+processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", use_fast=True)\n+```\n+\n+When using a fast image processor, you can also set the `device` argument to specify the device on which the processing should be done. By default, the processing is done on the same device as the inputs if the inputs are tensors, or on the CPU otherwise.\n+\n+```python\n+from torchvision.io import read_image\n+from transformers import DetrImageProcessorFast\n+\n+images = read_image(\"image.jpg\")\n+processor = DetrImageProcessorFast.from_pretrained(\"facebook/detr-resnet-50\")\n+images_processed = processor(images, return_tensors=\"pt\", device=\"cuda\")\n+```\n+\n+Here are some speed comparisons between the base and fast image processors for the `DETR` and `RT-DETR` models, and how they impact overall inference time:\n+\n+<div class=\"flex\">\n+  <div>\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_detr_fast_padded.png\" />\n+  </div>\n+  <div>\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_detr_fast_batched_compiled.png\" />\n+  </div>\n+</div>\n+\n+<div class=\"flex\">\n+  <div>\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_rt_detr_fast_single.png\" />\n+  </div>\n+  <div>\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_rt_detr_fast_batched.png\" />\n+  </div>\n+</div>\n+\n+These benchmarks were run on an [AWS EC2 g5.2xlarge instance](https://aws.amazon.com/ec2/instance-types/g5/), utilizing an NVIDIA A10G Tensor Core GPU.\n+\n \n ## ImageProcessingMixin\n "
        },
        {
            "sha": "8ad220dc4bd113532899aaeb473b329311588283",
            "filename": "docs/source/en/model_doc/rt_detr.md",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/48872fd6ae336fbde6fac7706910a9a4bc48210e/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/48872fd6ae336fbde6fac7706910a9a4bc48210e/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md?ref=48872fd6ae336fbde6fac7706910a9a4bc48210e",
            "patch": "@@ -46,7 +46,7 @@ Initially, an image is processed using a pre-trained convolutional neural networ\n >>> from PIL import Image\n >>> from transformers import RTDetrForObjectDetection, RTDetrImageProcessor\n \n->>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg' \n+>>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n >>> image = Image.open(requests.get(url, stream=True).raw)\n \n >>> image_processor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r50vd\")\n@@ -95,6 +95,12 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n     - preprocess\n     - post_process_object_detection\n \n+## RTDetrImageProcessorFast\n+\n+[[autodoc]] RTDetrImageProcessorFast\n+    - preprocess\n+    - post_process_object_detection\n+\n ## RTDetrModel\n \n [[autodoc]] RTDetrModel"
        },
        {
            "sha": "e6789c77fb825abd5986dd6125405988a785bde6",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/48872fd6ae336fbde6fac7706910a9a4bc48210e/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48872fd6ae336fbde6fac7706910a9a4bc48210e/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=48872fd6ae336fbde6fac7706910a9a4bc48210e",
            "patch": "@@ -1228,7 +1228,7 @@\n     _import_structure[\"models.poolformer\"].extend([\"PoolFormerFeatureExtractor\", \"PoolFormerImageProcessor\"])\n     _import_structure[\"models.pvt\"].extend([\"PvtImageProcessor\"])\n     _import_structure[\"models.qwen2_vl\"].extend([\"Qwen2VLImageProcessor\"])\n-    _import_structure[\"models.rt_detr\"].extend([\"RTDetrImageProcessor\"])\n+    _import_structure[\"models.rt_detr\"].extend([\"RTDetrImageProcessor\", \"RTDetrImageProcessorFast\"])\n     _import_structure[\"models.sam\"].extend([\"SamImageProcessor\"])\n     _import_structure[\"models.segformer\"].extend([\"SegformerFeatureExtractor\", \"SegformerImageProcessor\"])\n     _import_structure[\"models.seggpt\"].extend([\"SegGptImageProcessor\"])\n@@ -6152,7 +6152,7 @@\n         )\n         from .models.pvt import PvtImageProcessor\n         from .models.qwen2_vl import Qwen2VLImageProcessor\n-        from .models.rt_detr import RTDetrImageProcessor\n+        from .models.rt_detr import RTDetrImageProcessor, RTDetrImageProcessorFast\n         from .models.sam import SamImageProcessor\n         from .models.segformer import SegformerFeatureExtractor, SegformerImageProcessor\n         from .models.seggpt import SegGptImageProcessor"
        },
        {
            "sha": "3c1be325b7eb304060e3ed8aa28981619c677129",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 66,
            "deletions": 1,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/48872fd6ae336fbde6fac7706910a9a4bc48210e/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48872fd6ae336fbde6fac7706910a9a4bc48210e/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=48872fd6ae336fbde6fac7706910a9a4bc48210e",
            "patch": "@@ -15,14 +15,18 @@\n \n import functools\n from dataclasses import dataclass\n+from typing import Any, Iterable, List, Optional, Tuple\n \n from .image_processing_utils import BaseImageProcessor\n-from .utils.import_utils import is_torchvision_available\n+from .utils.import_utils import is_torch_available, is_torchvision_available\n \n \n if is_torchvision_available():\n     from torchvision.transforms import Compose\n \n+if is_torch_available():\n+    import torch\n+\n \n @dataclass(frozen=True)\n class SizeDict:\n@@ -66,3 +70,64 @@ def to_dict(self):\n         encoder_dict = super().to_dict()\n         encoder_dict.pop(\"_transform_params\", None)\n         return encoder_dict\n+\n+\n+def get_image_size_for_max_height_width(\n+    image_size: Tuple[int, int],\n+    max_height: int,\n+    max_width: int,\n+) -> Tuple[int, int]:\n+    \"\"\"\n+    Computes the output image size given the input image and the maximum allowed height and width. Keep aspect ratio.\n+    Important, even if image_height < max_height and image_width < max_width, the image will be resized\n+    to at least one of the edges be equal to max_height or max_width.\n+\n+    For example:\n+        - input_size: (100, 200), max_height: 50, max_width: 50 -> output_size: (25, 50)\n+        - input_size: (100, 200), max_height: 200, max_width: 500 -> output_size: (200, 400)\n+\n+    Args:\n+        image_size (`Tuple[int, int]`):\n+            The image to resize.\n+        max_height (`int`):\n+            The maximum allowed height.\n+        max_width (`int`):\n+            The maximum allowed width.\n+    \"\"\"\n+    height, width = image_size\n+    height_scale = max_height / height\n+    width_scale = max_width / width\n+    min_scale = min(height_scale, width_scale)\n+    new_height = int(height * min_scale)\n+    new_width = int(width * min_scale)\n+    return new_height, new_width\n+\n+\n+def safe_squeeze(tensor: \"torch.Tensor\", axis: Optional[int] = None) -> \"torch.Tensor\":\n+    \"\"\"\n+    Squeezes a tensor, but only if the axis specified has dim 1.\n+    \"\"\"\n+    if axis is None:\n+        return tensor.squeeze()\n+\n+    try:\n+        return tensor.squeeze(axis=axis)\n+    except ValueError:\n+        return tensor\n+\n+\n+def max_across_indices(values: Iterable[Any]) -> List[Any]:\n+    \"\"\"\n+    Return the maximum value across all indices of an iterable of values.\n+    \"\"\"\n+    return [max(values_i) for values_i in zip(*values)]\n+\n+\n+def get_max_height_width(images: List[\"torch.Tensor\"]) -> Tuple[int]:\n+    \"\"\"\n+    Get the maximum height and width across all images in a batch.\n+    \"\"\"\n+\n+    _, max_height, max_width = max_across_indices([img.shape for img in images])\n+\n+    return (max_height, max_width)"
        },
        {
            "sha": "5698abe15c8029f732690ede9a9540a3c205fea1",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/48872fd6ae336fbde6fac7706910a9a4bc48210e/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48872fd6ae336fbde6fac7706910a9a4bc48210e/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=48872fd6ae336fbde6fac7706910a9a4bc48210e",
            "patch": "@@ -123,7 +123,7 @@\n             (\"qwen2_vl\", (\"Qwen2VLImageProcessor\",)),\n             (\"regnet\", (\"ConvNextImageProcessor\",)),\n             (\"resnet\", (\"ConvNextImageProcessor\",)),\n-            (\"rt_detr\", \"RTDetrImageProcessor\"),\n+            (\"rt_detr\", (\"RTDetrImageProcessor\", \"RTDetrImageProcessorFast\")),\n             (\"sam\", (\"SamImageProcessor\",)),\n             (\"segformer\", (\"SegformerImageProcessor\",)),\n             (\"seggpt\", (\"SegGptImageProcessor\",)),"
        },
        {
            "sha": "eadde59e55e4750283b538e0d399e1374b4d5165",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 78,
            "deletions": 130,
            "changes": 208,
            "blob_url": "https://github.com/huggingface/transformers/blob/48872fd6ae336fbde6fac7706910a9a4bc48210e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48872fd6ae336fbde6fac7706910a9a4bc48210e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=48872fd6ae336fbde6fac7706910a9a4bc48210e",
            "patch": "@@ -21,7 +21,13 @@\n from typing import Any, Dict, List, Optional, Set, Tuple, Union\n \n from ...image_processing_utils import BatchFeature, get_size_dict\n-from ...image_processing_utils_fast import BaseImageProcessorFast, SizeDict\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    SizeDict,\n+    get_image_size_for_max_height_width,\n+    get_max_height_width,\n+    safe_squeeze,\n+)\n from ...image_transforms import (\n     center_to_corners_format,\n     corners_to_center_format,\n@@ -55,7 +61,6 @@\n     compute_segments,\n     convert_segmentation_to_rle,\n     get_size_with_aspect_ratio,\n-    max_across_indices,\n     remove_low_and_no_objects,\n )\n \n@@ -85,60 +90,6 @@\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n-def get_image_size_for_max_height_width(\n-    image_size: Tuple[int, int],\n-    max_height: int,\n-    max_width: int,\n-) -> Tuple[int, int]:\n-    \"\"\"\n-    Computes the output image size given the input image and the maximum allowed height and width. Keep aspect ratio.\n-    Important, even if image_height < max_height and image_width < max_width, the image will be resized\n-    to at least one of the edges be equal to max_height or max_width.\n-\n-    For example:\n-        - input_size: (100, 200), max_height: 50, max_width: 50 -> output_size: (25, 50)\n-        - input_size: (100, 200), max_height: 200, max_width: 500 -> output_size: (200, 400)\n-\n-    Args:\n-        image_size (`Tuple[int, int]`):\n-            The image to resize.\n-        max_height (`int`):\n-            The maximum allowed height.\n-        max_width (`int`):\n-            The maximum allowed width.\n-    \"\"\"\n-    height, width = image_size\n-    height_scale = max_height / height\n-    width_scale = max_width / width\n-    min_scale = min(height_scale, width_scale)\n-    new_height = int(height * min_scale)\n-    new_width = int(width * min_scale)\n-    return new_height, new_width\n-\n-\n-def safe_squeeze(tensor: torch.Tensor, axis: Optional[int] = None) -> torch.Tensor:\n-    \"\"\"\n-    Squeezes a tensor, but only if the axis specified has dim 1.\n-    \"\"\"\n-    if axis is None:\n-        return tensor.squeeze()\n-\n-    try:\n-        return tensor.squeeze(axis=axis)\n-    except ValueError:\n-        return tensor\n-\n-\n-def get_max_height_width(images: List[torch.Tensor]) -> Tuple[int]:\n-    \"\"\"\n-    Get the maximum height and width across all images in a batch.\n-    \"\"\"\n-\n-    _, max_height, max_width = max_across_indices([img.shape for img in images])\n-\n-    return (max_height, max_width)\n-\n-\n # inspired by https://github.com/facebookresearch/detr/blob/master/datasets/coco.py#L33\n def convert_coco_poly_to_mask(segmentations, height: int, width: int, device: torch.device) -> torch.Tensor:\n     \"\"\"\n@@ -191,18 +142,21 @@ def prepare_coco_detection_annotation(\n \n     # Get all COCO annotations for the given image.\n     annotations = target[\"annotations\"]\n-    annotations = [obj for obj in annotations if \"iscrowd\" not in obj or obj[\"iscrowd\"] == 0]\n+    classes = []\n+    area = []\n+    boxes = []\n+    keypoints = []\n+    for obj in annotations:\n+        if \"iscrowd\" not in obj or obj[\"iscrowd\"] == 0:\n+            classes.append(obj[\"category_id\"])\n+            area.append(obj[\"area\"])\n+            boxes.append(obj[\"bbox\"])\n+            if \"keypoints\" in obj:\n+                keypoints.append(obj[\"keypoints\"])\n \n-    classes = [obj[\"category_id\"] for obj in annotations]\n     classes = torch.as_tensor(classes, dtype=torch.int64, device=image.device)\n-\n-    # for conversion to coco api\n-    area = torch.as_tensor([obj[\"area\"] for obj in annotations], dtype=torch.float32, device=image.device)\n-    iscrowd = torch.as_tensor(\n-        [obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in annotations], dtype=torch.int64, device=image.device\n-    )\n-\n-    boxes = [obj[\"bbox\"] for obj in annotations]\n+    area = torch.as_tensor(area, dtype=torch.float32, device=image.device)\n+    iscrowd = torch.zeros_like(classes, dtype=torch.int64, device=image.device)\n     # guard against no boxes via resizing\n     boxes = torch.as_tensor(boxes, dtype=torch.float32, device=image.device).reshape(-1, 4)\n     boxes[:, 2:] += boxes[:, :2]\n@@ -211,19 +165,16 @@ def prepare_coco_detection_annotation(\n \n     keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n \n-    new_target = {}\n-    new_target[\"image_id\"] = image_id\n-    new_target[\"class_labels\"] = classes[keep]\n-    new_target[\"boxes\"] = boxes[keep]\n-    new_target[\"area\"] = area[keep]\n-    new_target[\"iscrowd\"] = iscrowd[keep]\n-    new_target[\"orig_size\"] = torch.as_tensor(\n-        [int(image_height), int(image_width)], dtype=torch.int64, device=image.device\n-    )\n+    new_target = {\n+        \"image_id\": image_id,\n+        \"class_labels\": classes[keep],\n+        \"boxes\": boxes[keep],\n+        \"area\": area[keep],\n+        \"iscrowd\": iscrowd[keep],\n+        \"orig_size\": torch.as_tensor([int(image_height), int(image_width)], dtype=torch.int64, device=image.device),\n+    }\n \n-    if annotations and \"keypoints\" in annotations[0]:\n-        keypoints = [obj[\"keypoints\"] for obj in annotations]\n-        # Converting the filtered keypoints list to a numpy array\n+    if keypoints:\n         keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=image.device)\n         # Apply the keep mask here to filter the relevant annotations\n         keypoints = keypoints[keep]\n@@ -911,84 +862,81 @@ def preprocess(\n         if input_data_format == ChannelDimension.LAST:\n             images = [image.permute(2, 0, 1).contiguous() for image in images]\n \n-        # prepare (COCO annotations as a list of Dict -> DETR target as a single Dict per image)\n-        if annotations is not None:\n-            prepared_images = []\n-            prepared_annotations = []\n-            for image, target in zip(images, annotations):\n-                target = self.prepare_annotation(\n+        if do_rescale and do_normalize:\n+            # fused rescale and normalize\n+            new_mean = torch.tensor(image_mean, device=images[0].device) * (1.0 / rescale_factor)\n+            new_std = torch.tensor(image_std, device=images[0].device) * (1.0 / rescale_factor)\n+\n+        processed_images = []\n+        processed_annotations = []\n+        pixel_masks = []  # Initialize pixel_masks here\n+        for image, annotation in zip(images, annotations if annotations is not None else [None] * len(images)):\n+            # prepare (COCO annotations as a list of Dict -> DETR target as a single Dict per image)\n+            if annotations is not None:\n+                annotation = self.prepare_annotation(\n                     image,\n-                    target,\n+                    annotation,\n                     format,\n                     return_segmentation_masks=return_segmentation_masks,\n                     masks_path=masks_path,\n                     input_data_format=input_data_format,\n                 )\n-                prepared_images.append(image)\n-                prepared_annotations.append(target)\n-            images = prepared_images\n-            annotations = prepared_annotations\n-            del prepared_images, prepared_annotations\n-\n-        if do_resize:\n-            if isinstance(resample, (PILImageResampling, int)):\n-                interpolation = pil_torch_interpolation_mapping[resample]\n-            else:\n-                interpolation = resample\n-            resized_images = [self.resize(image, size=size, interpolation=interpolation) for image in images]\n-            if annotations is not None:\n-                for i, (image, target) in enumerate(zip(resized_images, annotations)):\n-                    annotations[i] = self.resize_annotation(\n-                        target,\n-                        orig_size=images[i].size()[-2:],\n-                        target_size=image.size()[-2:],\n+\n+            if do_resize:\n+                interpolation = (\n+                    pil_torch_interpolation_mapping[resample]\n+                    if isinstance(resample, (PILImageResampling, int))\n+                    else resample\n+                )\n+                resized_image = self.resize(image, size=size, interpolation=interpolation)\n+                if annotations is not None:\n+                    annotation = self.resize_annotation(\n+                        annotation,\n+                        orig_size=image.size()[-2:],\n+                        target_size=resized_image.size()[-2:],\n                     )\n-            images = resized_images\n-            del resized_images\n+                image = resized_image\n \n-        if do_rescale and do_normalize:\n-            # fused rescale and normalize\n-            new_mean = torch.tensor(image_mean, device=images[0].device) * (1.0 / rescale_factor)\n-            new_std = torch.tensor(image_std, device=images[0].device) * (1.0 / rescale_factor)\n-            images = [F.normalize(image.to(dtype=torch.float32), new_mean, new_std) for image in images]\n-        elif do_rescale:\n-            images = [image * rescale_factor for image in images]\n-        elif do_normalize:\n-            images = [F.normalize(image, image_mean, image_std) for image in images]\n-\n-        if do_convert_annotations and annotations is not None:\n-            annotations = [\n-                self.normalize_annotation(annotation, get_image_size(image, input_data_format))\n-                for annotation, image in zip(annotations, images)\n-            ]\n+            if do_rescale and do_normalize:\n+                # fused rescale and normalize\n+                image = F.normalize(image.to(dtype=torch.float32), new_mean, new_std)\n+            elif do_rescale:\n+                image = image * rescale_factor\n+            elif do_normalize:\n+                image = F.normalize(image, image_mean, image_std)\n+\n+            if do_convert_annotations and annotations is not None:\n+                annotation = self.normalize_annotation(annotation, get_image_size(image, input_data_format))\n+\n+            processed_images.append(image)\n+            processed_annotations.append(annotation)\n+        images = processed_images\n+        annotations = processed_annotations if annotations is not None else None\n \n         if do_pad:\n-            # Pads images and returns their mask: {'pixel_values': ..., 'pixel_mask': ...}\n+            # depends on all resized image shapes so we need another loop\n             if pad_size is not None:\n                 padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n             else:\n                 padded_size = get_max_height_width(images)\n \n-            annotation_list = annotations if annotations is not None else [None] * len(images)\n             padded_images = []\n-            pixel_masks = []\n             padded_annotations = []\n-            for image, annotation in zip(images, annotation_list):\n+            for image, annotation in zip(images, annotations if annotations is not None else [None] * len(images)):\n+                # Pads images and returns their mask: {'pixel_values': ..., 'pixel_mask': ...}\n                 if padded_size == image.size()[-2:]:\n                     padded_images.append(image)\n                     pixel_masks.append(torch.ones(padded_size, dtype=torch.int64, device=image.device))\n                     padded_annotations.append(annotation)\n                     continue\n-                padded_image, pixel_mask, padded_annotation = self.pad(\n+                image, pixel_mask, annotation = self.pad(\n                     image, padded_size, annotation=annotation, update_bboxes=do_convert_annotations\n                 )\n-                padded_images.append(padded_image)\n+                padded_images.append(image)\n+                padded_annotations.append(annotation)\n                 pixel_masks.append(pixel_mask)\n-                padded_annotations.append(padded_annotation)\n             images = padded_images\n-            if annotations is not None:\n-                annotations = padded_annotations\n-            del padded_images, padded_annotations\n+            annotations = padded_annotations if annotations is not None else None\n             data.update({\"pixel_mask\": torch.stack(pixel_masks, dim=0)})\n \n         data.update({\"pixel_values\": torch.stack(images, dim=0)})"
        },
        {
            "sha": "52453f38b2c4f411012ec6ec7a517d6c050c4dd8",
            "filename": "src/transformers/models/rt_detr/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/48872fd6ae336fbde6fac7706910a9a4bc48210e/src%2Ftransformers%2Fmodels%2Frt_detr%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48872fd6ae336fbde6fac7706910a9a4bc48210e/src%2Ftransformers%2Fmodels%2Frt_detr%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2F__init__.py?ref=48872fd6ae336fbde6fac7706910a9a4bc48210e",
            "patch": "@@ -26,6 +26,7 @@\n     pass\n else:\n     _import_structure[\"image_processing_rt_detr\"] = [\"RTDetrImageProcessor\"]\n+    _import_structure[\"image_processing_rt_detr_fast\"] = [\"RTDetrImageProcessorFast\"]\n \n try:\n     if not is_torch_available():\n@@ -55,6 +56,7 @@\n         pass\n     else:\n         from .image_processing_rt_detr import RTDetrImageProcessor\n+        from .image_processing_rt_detr_fast import RTDetrImageProcessorFast\n \n     try:\n         if not is_torch_available():"
        },
        {
            "sha": "eead5b18693d2f7bf0216c2af061c99f0c53ec28",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/48872fd6ae336fbde6fac7706910a9a4bc48210e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48872fd6ae336fbde6fac7706910a9a4bc48210e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py?ref=48872fd6ae336fbde6fac7706910a9a4bc48210e",
            "patch": "@@ -1062,10 +1062,8 @@ def post_process_object_detection(\n                 raise ValueError(\n                     \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n                 )\n-\n             if isinstance(target_sizes, List):\n-                img_h = torch.Tensor([i[0] for i in target_sizes])\n-                img_w = torch.Tensor([i[1] for i in target_sizes])\n+                img_h, img_w = torch.as_tensor(target_sizes).unbind(1)\n             else:\n                 img_h, img_w = target_sizes.unbind(1)\n             scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n@@ -1089,10 +1087,13 @@ def post_process_object_detection(\n                 boxes = torch.gather(boxes, dim=1, index=index.unsqueeze(-1).tile(1, 1, boxes.shape[-1]))\n \n         results = []\n-        for s, l, b in zip(scores, labels, boxes):\n-            score = s[s > threshold]\n-            label = l[s > threshold]\n-            box = b[s > threshold]\n-            results.append({\"scores\": score, \"labels\": label, \"boxes\": box})\n+        for score, label, box in zip(scores, labels, boxes):\n+            results.append(\n+                {\n+                    \"scores\": score[score > threshold],\n+                    \"labels\": label[score > threshold],\n+                    \"boxes\": box[score > threshold],\n+                }\n+            )\n \n         return results"
        },
        {
            "sha": "9f63b5b7ced46703b60c35c27220f5004ae51b0f",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "added",
            "additions": 798,
            "deletions": 0,
            "changes": 798,
            "blob_url": "https://github.com/huggingface/transformers/blob/48872fd6ae336fbde6fac7706910a9a4bc48210e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48872fd6ae336fbde6fac7706910a9a4bc48210e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=48872fd6ae336fbde6fac7706910a9a4bc48210e",
            "patch": "@@ -0,0 +1,798 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for RT-DETR.\"\"\"\n+\n+import functools\n+import pathlib\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    SizeDict,\n+    get_image_size_for_max_height_width,\n+    get_max_height_width,\n+    safe_squeeze,\n+)\n+from ...image_transforms import (\n+    center_to_corners_format,\n+    corners_to_center_format,\n+)\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    AnnotationFormat,\n+    AnnotationType,\n+    ChannelDimension,\n+    ImageInput,\n+    ImageType,\n+    PILImageResampling,\n+    get_image_size,\n+    get_image_type,\n+    infer_channel_dimension_format,\n+    make_list_of_images,\n+    pil_torch_interpolation_mapping,\n+    validate_annotations,\n+)\n+from ...utils import (\n+    TensorType,\n+    filter_out_non_signature_kwargs,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    logging,\n+    requires_backends,\n+)\n+from .image_processing_rt_detr import (\n+    get_size_with_aspect_ratio,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+if is_torchvision_available():\n+    from ...image_utils import pil_torch_interpolation_mapping\n+\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION,)\n+\n+\n+def prepare_coco_detection_annotation(\n+    image,\n+    target,\n+    return_segmentation_masks: bool = False,\n+    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+):\n+    \"\"\"\n+    Convert the target in COCO format into the format expected by RT-DETR.\n+    \"\"\"\n+    image_height, image_width = image.size()[-2:]\n+\n+    image_id = target[\"image_id\"]\n+    image_id = torch.as_tensor([image_id], dtype=torch.int64, device=image.device)\n+\n+    # Get all COCO annotations for the given image.\n+    annotations = target[\"annotations\"]\n+    classes = []\n+    area = []\n+    boxes = []\n+    keypoints = []\n+    for obj in annotations:\n+        if \"iscrowd\" not in obj or obj[\"iscrowd\"] == 0:\n+            classes.append(obj[\"category_id\"])\n+            area.append(obj[\"area\"])\n+            boxes.append(obj[\"bbox\"])\n+            if \"keypoints\" in obj:\n+                keypoints.append(obj[\"keypoints\"])\n+\n+    classes = torch.as_tensor(classes, dtype=torch.int64, device=image.device)\n+    area = torch.as_tensor(area, dtype=torch.float32, device=image.device)\n+    iscrowd = torch.zeros_like(classes, dtype=torch.int64, device=image.device)\n+    # guard against no boxes via resizing\n+    boxes = torch.as_tensor(boxes, dtype=torch.float32, device=image.device).reshape(-1, 4)\n+    boxes[:, 2:] += boxes[:, :2]\n+    boxes[:, 0::2] = boxes[:, 0::2].clip(min=0, max=image_width)\n+    boxes[:, 1::2] = boxes[:, 1::2].clip(min=0, max=image_height)\n+\n+    keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n+\n+    new_target = {\n+        \"image_id\": image_id,\n+        \"class_labels\": classes[keep],\n+        \"boxes\": boxes[keep],\n+        \"area\": area[keep],\n+        \"iscrowd\": iscrowd[keep],\n+        \"orig_size\": torch.as_tensor([int(image_height), int(image_width)], dtype=torch.int64, device=image.device),\n+    }\n+\n+    if keypoints:\n+        keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=image.device)\n+        # Apply the keep mask here to filter the relevant annotations\n+        keypoints = keypoints[keep]\n+        num_keypoints = keypoints.shape[0]\n+        keypoints = keypoints.reshape((-1, 3)) if num_keypoints else keypoints\n+        new_target[\"keypoints\"] = keypoints\n+\n+    return new_target\n+\n+\n+class RTDetrImageProcessorFast(BaseImageProcessorFast):\n+    r\"\"\"\n+    Constructs a fast RT-DETR DETR image processor.\n+\n+    Args:\n+        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be\n+            overridden by the `do_resize` parameter in the `preprocess` method.\n+        size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 800, \"longest_edge\": 1333}`):\n+            Size of the image's `(height, width)` dimensions after resizing. Can be overridden by the `size` parameter\n+            in the `preprocess` method. Available options are:\n+                - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n+                    Do NOT keep the aspect ratio.\n+                - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n+                    the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n+                    less or equal to `longest_edge`.\n+                - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n+                    aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n+                    `max_width`.\n+        resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`):\n+            Resampling filter to use if resizing the image.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Controls whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the\n+            `do_rescale` parameter in the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overridden by the `rescale_factor` parameter in the\n+            `preprocess` method.\n+        do_normalize (`bool`, *optional*, defaults to `False`):\n+            Controls whether to normalize the image. Can be overridden by the `do_normalize` parameter in the\n+            `preprocess` method.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_MEAN`):\n+            Mean values to use when normalizing the image. Can be a single value or a list of values, one for each\n+            channel. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_STD`):\n+            Standard deviation values to use when normalizing the image. Can be a single value or a list of values, one\n+            for each channel. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+            Controls whether to convert the annotations to the format expected by the DETR model. Converts the\n+            bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+            Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+        do_pad (`bool`, *optional*, defaults to `False`):\n+            Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n+            method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n+            If `pad_size` is provided, the image will be padded to the specified dimensions.\n+            Otherwise, the image will be padded to the maximum height and width of the batch.\n+        pad_size (`Dict[str, int]`, *optional*):\n+            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+            height and width in the batch.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+\n+    def __init__(\n+        self,\n+        format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n+        do_resize: bool = True,\n+        size: Dict[str, int] = None,\n+        resample: Union[PILImageResampling, F.InterpolationMode] = PILImageResampling.BILINEAR,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = False,\n+        image_mean: Union[float, List[float]] = None,\n+        image_std: Union[float, List[float]] = None,\n+        do_convert_annotations: bool = True,\n+        do_pad: bool = False,\n+        pad_size: Optional[Dict[str, int]] = None,\n+        **kwargs,\n+    ) -> None:\n+        size = size if size is not None else {\"height\": 640, \"width\": 640}\n+        size = get_size_dict(size, default_to_square=False)\n+\n+        if do_convert_annotations is None:\n+            do_convert_annotations = do_normalize\n+\n+        super().__init__(**kwargs)\n+        self.format = format\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.do_convert_annotations = do_convert_annotations\n+        self.image_mean = image_mean if image_mean is not None else IMAGENET_DEFAULT_MEAN\n+        self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n+        self.do_pad = do_pad\n+        self.pad_size = pad_size\n+\n+    def prepare_annotation(\n+        self,\n+        image: torch.Tensor,\n+        target: Dict,\n+        format: Optional[AnnotationFormat] = None,\n+        return_segmentation_masks: bool = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> Dict:\n+        \"\"\"\n+        Prepare an annotation for feeding into RTDETR model.\n+        \"\"\"\n+        format = format if format is not None else self.format\n+\n+        if format == AnnotationFormat.COCO_DETECTION:\n+            return_segmentation_masks = False if return_segmentation_masks is None else return_segmentation_masks\n+            target = prepare_coco_detection_annotation(\n+                image, target, return_segmentation_masks, input_data_format=input_data_format\n+            )\n+        else:\n+            raise ValueError(f\"Format {format} is not supported.\")\n+        return target\n+\n+    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.resize\n+    def resize(\n+        self,\n+        image: torch.Tensor,\n+        size: SizeDict,\n+        interpolation: F.InterpolationMode = F.InterpolationMode.BILINEAR,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Resize the image to the given size. Size can be `min_size` (scalar) or `(height, width)` tuple. If size is an\n+        int, smaller edge of the image will be matched to this number.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Size of the image's `(height, width)` dimensions after resizing. Available options are:\n+                    - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n+                        Do NOT keep the aspect ratio.\n+                    - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n+                        the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n+                        less or equal to `longest_edge`.\n+                    - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n+                        aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n+                        `max_width`.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                Resampling filter to use if resizing the image.\n+        \"\"\"\n+        if size.shortest_edge and size.longest_edge:\n+            # Resize the image so that the shortest edge or the longest edge is of the given size\n+            # while maintaining the aspect ratio of the original image.\n+            new_size = get_size_with_aspect_ratio(\n+                image.size()[-2:],\n+                size[\"shortest_edge\"],\n+                size[\"longest_edge\"],\n+            )\n+        elif size.max_height and size.max_width:\n+            new_size = get_image_size_for_max_height_width(image.size()[-2:], size[\"max_height\"], size[\"max_width\"])\n+        elif size.height and size.width:\n+            new_size = (size[\"height\"], size[\"width\"])\n+        else:\n+            raise ValueError(\n+                \"Size must contain 'height' and 'width' keys or 'shortest_edge' and 'longest_edge' keys. Got\"\n+                f\" {size.keys()}.\"\n+            )\n+\n+        image = F.resize(\n+            image,\n+            size=new_size,\n+            interpolation=interpolation,\n+            **kwargs,\n+        )\n+        return image\n+\n+    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.resize_annotation\n+    def resize_annotation(\n+        self,\n+        annotation: Dict[str, Any],\n+        orig_size: Tuple[int, int],\n+        target_size: Tuple[int, int],\n+        threshold: float = 0.5,\n+        interpolation: F.InterpolationMode = F.InterpolationMode.NEAREST,\n+    ):\n+        \"\"\"\n+        Resizes an annotation to a target size.\n+\n+        Args:\n+            annotation (`Dict[str, Any]`):\n+                The annotation dictionary.\n+            orig_size (`Tuple[int, int]`):\n+                The original size of the input image.\n+            target_size (`Tuple[int, int]`):\n+                The target size of the image, as returned by the preprocessing `resize` step.\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The threshold used to binarize the segmentation masks.\n+            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST`):\n+                The resampling filter to use when resizing the masks.\n+        \"\"\"\n+        ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n+\n+        new_annotation = {}\n+        new_annotation[\"size\"] = target_size\n+\n+        for key, value in annotation.items():\n+            if key == \"boxes\":\n+                boxes = value\n+                scaled_boxes = boxes * torch.as_tensor(\n+                    [ratio_width, ratio_height, ratio_width, ratio_height], dtype=torch.float32, device=boxes.device\n+                )\n+                new_annotation[\"boxes\"] = scaled_boxes\n+            elif key == \"area\":\n+                area = value\n+                scaled_area = area * (ratio_width * ratio_height)\n+                new_annotation[\"area\"] = scaled_area\n+            elif key == \"masks\":\n+                masks = value[:, None]\n+                masks = [F.resize(mask, target_size, interpolation=interpolation) for mask in masks]\n+                masks = torch.stack(masks).to(torch.float32)\n+                masks = masks[:, 0] > threshold\n+                new_annotation[\"masks\"] = masks\n+            elif key == \"size\":\n+                new_annotation[\"size\"] = target_size\n+            else:\n+                new_annotation[key] = value\n+\n+        return new_annotation\n+\n+    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.normalize_annotation\n+    def normalize_annotation(self, annotation: Dict, image_size: Tuple[int, int]) -> Dict:\n+        image_height, image_width = image_size\n+        norm_annotation = {}\n+        for key, value in annotation.items():\n+            if key == \"boxes\":\n+                boxes = value\n+                boxes = corners_to_center_format(boxes)\n+                boxes /= torch.as_tensor(\n+                    [image_width, image_height, image_width, image_height], dtype=torch.float32, device=boxes.device\n+                )\n+                norm_annotation[key] = boxes\n+            else:\n+                norm_annotation[key] = value\n+        return norm_annotation\n+\n+    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast._update_annotation_for_padded_image\n+    def _update_annotation_for_padded_image(\n+        self,\n+        annotation: Dict,\n+        input_image_size: Tuple[int, int],\n+        output_image_size: Tuple[int, int],\n+        padding,\n+        update_bboxes,\n+    ) -> Dict:\n+        \"\"\"\n+        Update the annotation for a padded image.\n+        \"\"\"\n+        new_annotation = {}\n+        new_annotation[\"size\"] = output_image_size\n+        ratio_height, ratio_width = (input / output for output, input in zip(output_image_size, input_image_size))\n+\n+        for key, value in annotation.items():\n+            if key == \"masks\":\n+                masks = value\n+                masks = F.pad(\n+                    masks,\n+                    padding,\n+                    fill=0,\n+                )\n+                masks = safe_squeeze(masks, 1)\n+                new_annotation[\"masks\"] = masks\n+            elif key == \"boxes\" and update_bboxes:\n+                boxes = value\n+                boxes *= torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height], device=boxes.device)\n+                new_annotation[\"boxes\"] = boxes\n+            elif key == \"size\":\n+                new_annotation[\"size\"] = output_image_size\n+            else:\n+                new_annotation[key] = value\n+        return new_annotation\n+\n+    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.pad\n+    def pad(\n+        self,\n+        image: torch.Tensor,\n+        padded_size: Tuple[int, int],\n+        annotation: Optional[Dict[str, Any]] = None,\n+        update_bboxes: bool = True,\n+        fill: int = 0,\n+    ):\n+        original_size = image.size()[-2:]\n+        padding_bottom = padded_size[0] - original_size[0]\n+        padding_right = padded_size[1] - original_size[1]\n+        if padding_bottom < 0 or padding_right < 0:\n+            raise ValueError(\n+                f\"Padding dimensions are negative. Please make sure that the padded size is larger than the \"\n+                f\"original size. Got padded size: {padded_size}, original size: {original_size}.\"\n+            )\n+        if original_size != padded_size:\n+            padding = [0, 0, padding_right, padding_bottom]\n+            image = F.pad(image, padding, fill=fill)\n+            if annotation is not None:\n+                annotation = self._update_annotation_for_padded_image(\n+                    annotation, original_size, padded_size, padding, update_bboxes\n+                )\n+\n+        # Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n+        pixel_mask = torch.zeros(padded_size, dtype=torch.int64, device=image.device)\n+        pixel_mask[: original_size[0], : original_size[1]] = 1\n+\n+        return image, pixel_mask, annotation\n+\n+    @functools.lru_cache(maxsize=1)\n+    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast._validate_input_arguments\n+    def _validate_input_arguments(\n+        self,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Union[float, List[float]],\n+        image_std: Union[float, List[float]],\n+        do_resize: bool,\n+        size: Dict[str, int],\n+        resample: \"PILImageResampling\",\n+        data_format: Union[str, ChannelDimension],\n+        return_tensors: Union[TensorType, str],\n+    ):\n+        if return_tensors != \"pt\":\n+            raise ValueError(\"Only returning PyTorch tensors is currently supported.\")\n+\n+        if data_format != ChannelDimension.FIRST:\n+            raise ValueError(\"Only channel first data format is currently supported.\")\n+\n+        if do_resize and None in (size, resample):\n+            raise ValueError(\"Size and resample must be specified if do_resize is True.\")\n+\n+        if do_rescale and rescale_factor is None:\n+            raise ValueError(\"Rescale factor must be specified if do_rescale is True.\")\n+\n+        if do_normalize and None in (image_mean, image_std):\n+            raise ValueError(\"Image mean and standard deviation must be specified if do_normalize is True.\")\n+\n+    @filter_out_non_signature_kwargs(extra=[\"device\"])\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n+        return_segmentation_masks: bool = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[Dict[str, int]] = None,\n+        resample: Optional[Union[PILImageResampling, F.InterpolationMode]] = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[Union[int, float]] = None,\n+        do_normalize: Optional[bool] = None,\n+        do_convert_annotations: Optional[bool] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_pad: Optional[bool] = None,\n+        format: Optional[Union[str, AnnotationFormat]] = None,\n+        return_tensors: Optional[Union[TensorType, str]] = None,\n+        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        pad_size: Optional[Dict[str, int]] = None,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess an image or a batch of images so that it can be used by the model.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image or batch of images to preprocess. Expects a single or batch of images with pixel values ranging\n+                from 0 to 255. If passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            annotations (`AnnotationType` or `List[AnnotationType]`, *optional*):\n+                List of annotations associated with the image or batch of images. If annotation is for object\n+                detection, the annotations should be a dictionary with the following keys:\n+                - \"image_id\" (`int`): The image id.\n+                - \"annotations\" (`List[Dict]`): List of annotations for an image. Each annotation should be a\n+                  dictionary. An image can have no annotations, in which case the list should be empty.\n+                If annotation is for segmentation, the annotations should be a dictionary with the following keys:\n+                - \"image_id\" (`int`): The image id.\n+                - \"segments_info\" (`List[Dict]`): List of segments for an image. Each segment should be a dictionary.\n+                  An image can have no segments, in which case the list should be empty.\n+                - \"file_name\" (`str`): The file name of the image.\n+            return_segmentation_masks (`bool`, *optional*, defaults to self.return_segmentation_masks):\n+                Whether to return segmentation masks.\n+            masks_path (`str` or `pathlib.Path`, *optional*):\n+                Path to the directory containing the segmentation masks.\n+            do_resize (`bool`, *optional*, defaults to self.do_resize):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to self.size):\n+                Size of the image's `(height, width)` dimensions after resizing. Available options are:\n+                    - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n+                        Do NOT keep the aspect ratio.\n+                    - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n+                        the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n+                        less or equal to `longest_edge`.\n+                    - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n+                        aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n+                        `max_width`.\n+            resample (`PILImageResampling` or `InterpolationMode`, *optional*, defaults to self.resample):\n+                Resampling filter to use when resizing the image.\n+            do_rescale (`bool`, *optional*, defaults to self.do_rescale):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to self.rescale_factor):\n+                Rescale factor to use when rescaling the image.\n+            do_normalize (`bool`, *optional*, defaults to self.do_normalize):\n+                Whether to normalize the image.\n+            do_convert_annotations (`bool`, *optional*, defaults to self.do_convert_annotations):\n+                Whether to convert the annotations to the format expected by the model. Converts the bounding\n+                boxes from the format `(top_left_x, top_left_y, width, height)` to `(center_x, center_y, width, height)`\n+                and in relative coordinates.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to self.image_mean):\n+                Mean to use when normalizing the image.\n+            image_std (`float` or `List[float]`, *optional*, defaults to self.image_std):\n+                Standard deviation to use when normalizing the image.\n+            do_pad (`bool`, *optional*, defaults to self.do_pad):\n+                Whether to pad the image. If `True`, padding will be applied to the bottom and right of\n+                the image with zeros. If `pad_size` is provided, the image will be padded to the specified\n+                dimensions. Otherwise, the image will be padded to the maximum height and width of the batch.\n+            format (`str` or `AnnotationFormat`, *optional*, defaults to self.format):\n+                Format of the annotations.\n+            return_tensors (`str` or `TensorType`, *optional*, defaults to self.return_tensors):\n+                Type of tensors to return. If `None`, will return the list of images.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+            pad_size (`Dict[str, int]`, *optional*):\n+                The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+                provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+                height and width in the batch.\n+        \"\"\"\n+        do_resize = self.do_resize if do_resize is None else do_resize\n+        size = self.size if size is None else size\n+        size = get_size_dict(size=size, default_to_square=True)\n+        resample = self.resample if resample is None else resample\n+        do_rescale = self.do_rescale if do_rescale is None else do_rescale\n+        rescale_factor = self.rescale_factor if rescale_factor is None else rescale_factor\n+        do_normalize = self.do_normalize if do_normalize is None else do_normalize\n+        image_mean = self.image_mean if image_mean is None else image_mean\n+        image_std = self.image_std if image_std is None else image_std\n+        do_convert_annotations = (\n+            self.do_convert_annotations if do_convert_annotations is None else do_convert_annotations\n+        )\n+        do_pad = self.do_pad if do_pad is None else do_pad\n+        pad_size = self.pad_size if pad_size is None else pad_size\n+        format = self.format if format is None else format\n+        return_tensors = \"pt\" if return_tensors is None else return_tensors\n+        device = kwargs.pop(\"device\", None)\n+\n+        # Make hashable for cache\n+        size = SizeDict(**size)\n+        image_mean = tuple(image_mean) if isinstance(image_mean, list) else image_mean\n+        image_std = tuple(image_std) if isinstance(image_std, list) else image_std\n+\n+        images = make_list_of_images(images)\n+        image_type = get_image_type(images[0])\n+\n+        if image_type not in [ImageType.PIL, ImageType.TORCH, ImageType.NUMPY]:\n+            raise ValueError(f\"Unsupported input image type {image_type}\")\n+\n+        self._validate_input_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+            return_tensors=return_tensors,\n+            data_format=data_format,\n+        )\n+\n+        if annotations is not None and isinstance(annotations, dict):\n+            annotations = [annotations]\n+\n+        if annotations is not None and len(images) != len(annotations):\n+            raise ValueError(\n+                f\"The number of images ({len(images)}) and annotations ({len(annotations)}) do not match.\"\n+            )\n+\n+        format = AnnotationFormat(format)\n+        if annotations is not None:\n+            validate_annotations(format, SUPPORTED_ANNOTATION_FORMATS, annotations)\n+\n+        data = {}\n+        if image_type == ImageType.PIL:\n+            images = [F.pil_to_tensor(image) for image in images]\n+        elif image_type == ImageType.NUMPY:\n+            # not using F.to_tensor as it doesn't handle (C, H, W) numpy arrays\n+            images = [torch.from_numpy(image).contiguous() for image in images]\n+\n+        if device is not None:\n+            images = [image.to(device) for image in images]\n+\n+        # We assume that all images have the same channel dimension format.\n+        if input_data_format is None:\n+            input_data_format = infer_channel_dimension_format(images[0])\n+        if input_data_format == ChannelDimension.LAST:\n+            images = [image.permute(2, 0, 1).contiguous() for image in images]\n+\n+        if do_rescale and do_normalize:\n+            # fused rescale and normalize\n+            new_mean = torch.tensor(image_mean, device=images[0].device) * (1.0 / rescale_factor)\n+            new_std = torch.tensor(image_std, device=images[0].device) * (1.0 / rescale_factor)\n+\n+        processed_images = []\n+        processed_annotations = []\n+        pixel_masks = []  # Initialize pixel_masks here\n+        for image, annotation in zip(images, annotations if annotations is not None else [None] * len(images)):\n+            # prepare (COCO annotations as a list of Dict -> DETR target as a single Dict per image)\n+            if annotations is not None:\n+                annotation = self.prepare_annotation(\n+                    image,\n+                    annotation,\n+                    format,\n+                    return_segmentation_masks=return_segmentation_masks,\n+                    masks_path=masks_path,\n+                    input_data_format=input_data_format,\n+                )\n+\n+            if do_resize:\n+                interpolation = (\n+                    pil_torch_interpolation_mapping[resample]\n+                    if isinstance(resample, (PILImageResampling, int))\n+                    else resample\n+                )\n+                resized_image = self.resize(image, size=size, interpolation=interpolation)\n+                if annotations is not None:\n+                    annotation = self.resize_annotation(\n+                        annotation,\n+                        orig_size=image.size()[-2:],\n+                        target_size=resized_image.size()[-2:],\n+                    )\n+                image = resized_image\n+\n+            if do_rescale and do_normalize:\n+                # fused rescale and normalize\n+                image = F.normalize(image.to(dtype=torch.float32), new_mean, new_std)\n+            elif do_rescale:\n+                image = image * rescale_factor\n+            elif do_normalize:\n+                image = F.normalize(image, image_mean, image_std)\n+\n+            if do_convert_annotations and annotations is not None:\n+                annotation = self.normalize_annotation(annotation, get_image_size(image, input_data_format))\n+\n+            processed_images.append(image)\n+            processed_annotations.append(annotation)\n+        images = processed_images\n+        annotations = processed_annotations if annotations is not None else None\n+\n+        if do_pad:\n+            # depends on all resized image shapes so we need another loop\n+            if pad_size is not None:\n+                padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+            else:\n+                padded_size = get_max_height_width(images)\n+\n+            padded_images = []\n+            padded_annotations = []\n+            for image, annotation in zip(images, annotations if annotations is not None else [None] * len(images)):\n+                # Pads images and returns their mask: {'pixel_values': ..., 'pixel_mask': ...}\n+                if padded_size == image.size()[-2:]:\n+                    padded_images.append(image)\n+                    pixel_masks.append(torch.ones(padded_size, dtype=torch.int64, device=image.device))\n+                    padded_annotations.append(annotation)\n+                    continue\n+                image, pixel_mask, annotation = self.pad(\n+                    image, padded_size, annotation=annotation, update_bboxes=do_convert_annotations\n+                )\n+                padded_images.append(image)\n+                padded_annotations.append(annotation)\n+                pixel_masks.append(pixel_mask)\n+            images = padded_images\n+            annotations = padded_annotations if annotations is not None else None\n+            data.update({\"pixel_mask\": torch.stack(pixel_masks, dim=0)})\n+\n+        data.update({\"pixel_values\": torch.stack(images, dim=0)})\n+        encoded_inputs = BatchFeature(data, tensor_type=return_tensors)\n+        if annotations is not None:\n+            encoded_inputs[\"labels\"] = [\n+                BatchFeature(annotation, tensor_type=return_tensors) for annotation in annotations\n+            ]\n+        return encoded_inputs\n+\n+    # Copied from transformers.models.rt_detr.image_processing_rt_detr.RTDetrImageProcessor.post_process_object_detection\n+    def post_process_object_detection(\n+        self,\n+        outputs,\n+        threshold: float = 0.5,\n+        target_sizes: Union[TensorType, List[Tuple]] = None,\n+        use_focal_loss: bool = True,\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`DetrForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n+        bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`DetrObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                `(height, width)` of each image in the batch. If unset, predictions will not be resized.\n+            use_focal_loss (`bool` defaults to `True`):\n+                Variable informing if the focal loss was used to predict the outputs. If `True`, a sigmoid is applied\n+                to compute the scores of each detection, otherwise, a softmax function is used.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        requires_backends(self, [\"torch\"])\n+        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n+        # convert from relative cxcywh to absolute xyxy\n+        boxes = center_to_corners_format(out_bbox)\n+        if target_sizes is not None:\n+            if len(out_logits) != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+            if isinstance(target_sizes, List):\n+                img_h, img_w = torch.as_tensor(target_sizes).unbind(1)\n+            else:\n+                img_h, img_w = target_sizes.unbind(1)\n+            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n+            boxes = boxes * scale_fct[:, None, :]\n+\n+        num_top_queries = out_logits.shape[1]\n+        num_classes = out_logits.shape[2]\n+\n+        if use_focal_loss:\n+            scores = torch.nn.functional.sigmoid(out_logits)\n+            scores, index = torch.topk(scores.flatten(1), num_top_queries, axis=-1)\n+            labels = index % num_classes\n+            index = index // num_classes\n+            boxes = boxes.gather(dim=1, index=index.unsqueeze(-1).repeat(1, 1, boxes.shape[-1]))\n+        else:\n+            scores = torch.nn.functional.softmax(out_logits)[:, :, :-1]\n+            scores, labels = scores.max(dim=-1)\n+            if scores.shape[1] > num_top_queries:\n+                scores, index = torch.topk(scores, num_top_queries, dim=-1)\n+                labels = torch.gather(labels, dim=1, index=index)\n+                boxes = torch.gather(boxes, dim=1, index=index.unsqueeze(-1).tile(1, 1, boxes.shape[-1]))\n+\n+        results = []\n+        for score, label, box in zip(scores, labels, boxes):\n+            results.append(\n+                {\n+                    \"scores\": score[score > threshold],\n+                    \"labels\": label[score > threshold],\n+                    \"boxes\": box[score > threshold],\n+                }\n+            )\n+\n+        return results"
        },
        {
            "sha": "19cf02a4e8582687b5858e61e6adadb255b9f858",
            "filename": "src/transformers/utils/dummy_vision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/48872fd6ae336fbde6fac7706910a9a4bc48210e/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48872fd6ae336fbde6fac7706910a9a4bc48210e/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py?ref=48872fd6ae336fbde6fac7706910a9a4bc48210e",
            "patch": "@@ -569,6 +569,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\"])\n \n \n+class RTDetrImageProcessorFast(metaclass=DummyObject):\n+    _backends = [\"vision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\"])\n+\n+\n class SamImageProcessor(metaclass=DummyObject):\n     _backends = [\"vision\"]\n "
        },
        {
            "sha": "f91c520873668fa9a55b0320cf3b95ac71fafb02",
            "filename": "tests/models/detr/test_image_processing_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/48872fd6ae336fbde6fac7706910a9a4bc48210e/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48872fd6ae336fbde6fac7706910a9a4bc48210e/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py?ref=48872fd6ae336fbde6fac7706910a9a4bc48210e",
            "patch": "@@ -677,7 +677,7 @@ def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n \n         target = {\"image_id\": 39769, \"annotations\": target}\n \n-        processor = self.image_processor_list[1].from_pretrained(\"facebook/detr-resnet-50\")\n+        processor = self.image_processor_list[1]()\n         # 1. run processor on CPU\n         encoding_cpu = processor(images=image, annotations=target, return_tensors=\"pt\", device=\"cpu\")\n         # 2. run processor on GPU\n@@ -734,7 +734,7 @@ def test_fast_processor_equivalence_cpu_gpu_coco_panoptic_annotations(self):\n \n         masks_path = pathlib.Path(\"./tests/fixtures/tests_samples/COCO/coco_panoptic\")\n \n-        processor = self.image_processor_list[1].from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n+        processor = self.image_processor_list[1](format=\"coco_panoptic\")\n         # 1. run processor on CPU\n         encoding_cpu = processor(\n             images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\", device=\"cpu\""
        },
        {
            "sha": "e7bfbae3f9c27aed2f20407f3be608912a866dbb",
            "filename": "tests/models/rt_detr/test_image_processing_rt_detr.py",
            "status": "modified",
            "additions": 244,
            "deletions": 180,
            "changes": 424,
            "blob_url": "https://github.com/huggingface/transformers/blob/48872fd6ae336fbde6fac7706910a9a4bc48210e/tests%2Fmodels%2Frt_detr%2Ftest_image_processing_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48872fd6ae336fbde6fac7706910a9a4bc48210e/tests%2Fmodels%2Frt_detr%2Ftest_image_processing_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr%2Ftest_image_processing_rt_detr.py?ref=48872fd6ae336fbde6fac7706910a9a4bc48210e",
            "patch": "@@ -16,16 +16,16 @@\n \n import requests\n \n-from transformers.testing_utils import require_torch, require_vision, slow\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.testing_utils import require_torch, require_torch_gpu, require_vision, slow\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n if is_vision_available():\n     from PIL import Image\n \n-    from transformers import RTDetrImageProcessor\n+    from transformers import RTDetrImageProcessor, RTDetrImageProcessorFast\n \n if is_torch_available():\n     import torch\n@@ -91,6 +91,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class RtDetrImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = RTDetrImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = RTDetrImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -101,17 +102,19 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"resample\"))\n-        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n-        self.assertTrue(hasattr(image_processing, \"return_tensors\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"resample\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"return_tensors\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 640, \"width\": 640})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 640, \"width\": 640})\n \n     def test_valid_coco_detection_annotations(self):\n         # prepare image and target\n@@ -121,32 +124,33 @@ def test_valid_coco_detection_annotations(self):\n \n         params = {\"image_id\": 39769, \"annotations\": target}\n \n-        # encode them\n-        image_processing = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r50vd\")\n+        for image_processing_class in self.image_processor_list:\n+            # encode them\n+            image_processing = image_processing_class.from_pretrained(\"PekingU/rtdetr_r50vd\")\n \n-        # legal encodings (single image)\n-        _ = image_processing(images=image, annotations=params, return_tensors=\"pt\")\n-        _ = image_processing(images=image, annotations=[params], return_tensors=\"pt\")\n+            # legal encodings (single image)\n+            _ = image_processing(images=image, annotations=params, return_tensors=\"pt\")\n+            _ = image_processing(images=image, annotations=[params], return_tensors=\"pt\")\n \n-        # legal encodings (batch of one image)\n-        _ = image_processing(images=[image], annotations=params, return_tensors=\"pt\")\n-        _ = image_processing(images=[image], annotations=[params], return_tensors=\"pt\")\n+            # legal encodings (batch of one image)\n+            _ = image_processing(images=[image], annotations=params, return_tensors=\"pt\")\n+            _ = image_processing(images=[image], annotations=[params], return_tensors=\"pt\")\n \n-        # legal encoding (batch of more than one image)\n-        n = 5\n-        _ = image_processing(images=[image] * n, annotations=[params] * n, return_tensors=\"pt\")\n+            # legal encoding (batch of more than one image)\n+            n = 5\n+            _ = image_processing(images=[image] * n, annotations=[params] * n, return_tensors=\"pt\")\n \n-        # example of an illegal encoding (missing the 'image_id' key)\n-        with self.assertRaises(ValueError) as e:\n-            image_processing(images=image, annotations={\"annotations\": target}, return_tensors=\"pt\")\n+            # example of an illegal encoding (missing the 'image_id' key)\n+            with self.assertRaises(ValueError) as e:\n+                image_processing(images=image, annotations={\"annotations\": target}, return_tensors=\"pt\")\n \n-        self.assertTrue(str(e.exception).startswith(\"Invalid COCO detection annotations\"))\n+            self.assertTrue(str(e.exception).startswith(\"Invalid COCO detection annotations\"))\n \n-        # example of an illegal encoding (unequal lengths of images and annotations)\n-        with self.assertRaises(ValueError) as e:\n-            image_processing(images=[image] * n, annotations=[params] * (n - 1), return_tensors=\"pt\")\n+            # example of an illegal encoding (unequal lengths of images and annotations)\n+            with self.assertRaises(ValueError) as e:\n+                image_processing(images=[image] * n, annotations=[params] * (n - 1), return_tensors=\"pt\")\n \n-        self.assertTrue(str(e.exception) == \"The number of images (5) and annotations (4) do not match.\")\n+            self.assertTrue(str(e.exception) == \"The number of images (5) and annotations (4) do not match.\")\n \n     @slow\n     def test_call_pytorch_with_coco_detection_annotations(self):\n@@ -157,55 +161,57 @@ def test_call_pytorch_with_coco_detection_annotations(self):\n \n         target = {\"image_id\": 39769, \"annotations\": target}\n \n-        # encode them\n-        image_processing = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r50vd\")\n-        encoding = image_processing(images=image, annotations=target, return_tensors=\"pt\")\n-\n-        # verify pixel values\n-        expected_shape = torch.Size([1, 3, 640, 640])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        expected_slice = torch.tensor([0.5490, 0.5647, 0.5725])\n-        self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n-\n-        # verify area\n-        expected_area = torch.tensor([2827.9883, 5403.4761, 235036.7344, 402070.2188, 71068.8281, 79601.2812])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n-        # verify boxes\n-        expected_boxes_shape = torch.Size([6, 4])\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n-        expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n-        # verify image_id\n-        expected_image_id = torch.tensor([39769])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n-        # verify is_crowd\n-        expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n-        # verify class_labels\n-        expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n-        # verify orig_size\n-        expected_orig_size = torch.tensor([480, 640])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n-        # verify size\n-        expected_size = torch.tensor([640, 640])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+        for image_processing_class in self.image_processor_list:\n+            # encode them\n+            image_processing = image_processing_class.from_pretrained(\"PekingU/rtdetr_r50vd\")\n+            encoding = image_processing(images=image, annotations=target, return_tensors=\"pt\")\n+\n+            # verify pixel values\n+            expected_shape = torch.Size([1, 3, 640, 640])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            expected_slice = torch.tensor([0.5490, 0.5647, 0.5725])\n+            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+\n+            # verify area\n+            expected_area = torch.tensor([2827.9883, 5403.4761, 235036.7344, 402070.2188, 71068.8281, 79601.2812])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+            # verify boxes\n+            expected_boxes_shape = torch.Size([6, 4])\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n+            expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+            # verify image_id\n+            expected_image_id = torch.tensor([39769])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+            # verify is_crowd\n+            expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+            # verify class_labels\n+            expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+            # verify orig_size\n+            expected_orig_size = torch.tensor([480, 640])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+            # verify size\n+            expected_size = torch.tensor([640, 640])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n \n     @slow\n     def test_image_processor_outputs(self):\n         image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n \n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        encoding = image_processing(images=image, return_tensors=\"pt\")\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            encoding = image_processing(images=image, return_tensors=\"pt\")\n \n-        # verify pixel values: shape\n-        expected_shape = torch.Size([1, 3, 640, 640])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+            # verify pixel values: shape\n+            expected_shape = torch.Size([1, 3, 640, 640])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n \n-        # verify pixel values: output values\n-        expected_slice = torch.tensor([0.5490196347236633, 0.5647059082984924, 0.572549045085907])\n-        self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-5))\n+            # verify pixel values: output values\n+            expected_slice = torch.tensor([0.5490196347236633, 0.5647059082984924, 0.572549045085907])\n+            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-5))\n \n     def test_multiple_images_processor_outputs(self):\n         images_urls = [\n@@ -224,31 +230,32 @@ def test_multiple_images_processor_outputs(self):\n             image = Image.open(requests.get(url, stream=True).raw)\n             images.append(image)\n \n-        # apply image processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        encoding = image_processing(images=images, return_tensors=\"pt\")\n-\n-        # verify if pixel_values is part of the encoding\n-        self.assertIn(\"pixel_values\", encoding)\n-\n-        # verify pixel values: shape\n-        expected_shape = torch.Size([8, 3, 640, 640])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        # verify pixel values: output values\n-        expected_slices = torch.tensor(\n-            [\n-                [0.5333333611488342, 0.5568627715110779, 0.5647059082984924],\n-                [0.5372549295425415, 0.4705882668495178, 0.4274510145187378],\n-                [0.3960784673690796, 0.35686275362968445, 0.3686274588108063],\n-                [0.20784315466880798, 0.1882353127002716, 0.15294118225574493],\n-                [0.364705890417099, 0.364705890417099, 0.3686274588108063],\n-                [0.8078432083129883, 0.8078432083129883, 0.8078432083129883],\n-                [0.4431372880935669, 0.4431372880935669, 0.4431372880935669],\n-                [0.19607844948768616, 0.21176472306251526, 0.3607843220233917],\n-            ]\n-        )\n-        self.assertTrue(torch.allclose(encoding[\"pixel_values\"][:, 1, 0, :3], expected_slices, atol=1e-5))\n+        for image_processing_class in self.image_processor_list:\n+            # apply image processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            encoding = image_processing(images=images, return_tensors=\"pt\")\n+\n+            # verify if pixel_values is part of the encoding\n+            self.assertIn(\"pixel_values\", encoding)\n+\n+            # verify pixel values: shape\n+            expected_shape = torch.Size([8, 3, 640, 640])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            # verify pixel values: output values\n+            expected_slices = torch.tensor(\n+                [\n+                    [0.5333333611488342, 0.5568627715110779, 0.5647059082984924],\n+                    [0.5372549295425415, 0.4705882668495178, 0.4274510145187378],\n+                    [0.3960784673690796, 0.35686275362968445, 0.3686274588108063],\n+                    [0.20784315466880798, 0.1882353127002716, 0.15294118225574493],\n+                    [0.364705890417099, 0.364705890417099, 0.3686274588108063],\n+                    [0.8078432083129883, 0.8078432083129883, 0.8078432083129883],\n+                    [0.4431372880935669, 0.4431372880935669, 0.4431372880935669],\n+                    [0.19607844948768616, 0.21176472306251526, 0.3607843220233917],\n+                ]\n+            )\n+            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][:, 1, 0, :3], expected_slices, atol=1e-5))\n \n     @slow\n     def test_batched_coco_detection_annotations(self):\n@@ -277,89 +284,146 @@ def test_batched_coco_detection_annotations(self):\n         images = [image_0, image_1]\n         annotations = [annotations_0, annotations_1]\n \n-        image_processing = RTDetrImageProcessor()\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            return_segmentation_masks=True,\n-            return_tensors=\"pt\",  # do_convert_annotations=True\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class()\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                return_segmentation_masks=True,\n+                return_tensors=\"pt\",  # do_convert_annotations=True\n+            )\n+\n+            # Check the pixel values have been padded\n+            postprocessed_height, postprocessed_width = 640, 640\n+            expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            # Check the bounding boxes have been adjusted for padded images\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            expected_boxes_0 = torch.tensor(\n+                [\n+                    [0.6879, 0.4609, 0.0755, 0.3691],\n+                    [0.2118, 0.3359, 0.2601, 0.1566],\n+                    [0.5011, 0.5000, 0.9979, 1.0000],\n+                    [0.5010, 0.5020, 0.9979, 0.9959],\n+                    [0.3284, 0.5944, 0.5884, 0.8112],\n+                    [0.8394, 0.5445, 0.3213, 0.9110],\n+                ]\n+            )\n+            expected_boxes_1 = torch.tensor(\n+                [\n+                    [0.5503, 0.2765, 0.0604, 0.2215],\n+                    [0.1695, 0.2016, 0.2080, 0.0940],\n+                    [0.5006, 0.4933, 0.9977, 0.9865],\n+                    [0.5008, 0.5002, 0.9983, 0.9955],\n+                    [0.2627, 0.5456, 0.4707, 0.8646],\n+                    [0.7715, 0.4115, 0.4570, 0.7161],\n+                ]\n+            )\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+\n+            # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n+            # format and not in the range [0, 1]\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                return_segmentation_masks=True,\n+                do_convert_annotations=False,\n+                return_tensors=\"pt\",\n+            )\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            # Convert to absolute coordinates\n+            unnormalized_boxes_0 = torch.vstack(\n+                [\n+                    expected_boxes_0[:, 0] * postprocessed_width,\n+                    expected_boxes_0[:, 1] * postprocessed_height,\n+                    expected_boxes_0[:, 2] * postprocessed_width,\n+                    expected_boxes_0[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            unnormalized_boxes_1 = torch.vstack(\n+                [\n+                    expected_boxes_1[:, 0] * postprocessed_width,\n+                    expected_boxes_1[:, 1] * postprocessed_height,\n+                    expected_boxes_1[:, 2] * postprocessed_width,\n+                    expected_boxes_1[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n+            expected_boxes_0 = torch.vstack(\n+                [\n+                    unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n+                    unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n+                ]\n+            ).T\n+            expected_boxes_1 = torch.vstack(\n+                [\n+                    unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n+                    unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n+                ]\n+            ).T\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n \n-        # Check the pixel values have been padded\n-        postprocessed_height, postprocessed_width = 640, 640\n-        expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        # Check the bounding boxes have been adjusted for padded images\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        expected_boxes_0 = torch.tensor(\n-            [\n-                [0.6879, 0.4609, 0.0755, 0.3691],\n-                [0.2118, 0.3359, 0.2601, 0.1566],\n-                [0.5011, 0.5000, 0.9979, 1.0000],\n-                [0.5010, 0.5020, 0.9979, 0.9959],\n-                [0.3284, 0.5944, 0.5884, 0.8112],\n-                [0.8394, 0.5445, 0.3213, 0.9110],\n-            ]\n+    @slow\n+    @require_torch_gpu\n+    # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations\n+    def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n+        # prepare image and target\n+        image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+        with open(\"./tests/fixtures/tests_samples/COCO/coco_annotations.txt\", \"r\") as f:\n+            target = json.loads(f.read())\n+\n+        target = {\"image_id\": 39769, \"annotations\": target}\n+\n+        processor = self.image_processor_list[1]()\n+        # 1. run processor on CPU\n+        encoding_cpu = processor(images=image, annotations=target, return_tensors=\"pt\", device=\"cpu\")\n+        # 2. run processor on GPU\n+        encoding_gpu = processor(images=image, annotations=target, return_tensors=\"pt\", device=\"cuda\")\n+\n+        # verify pixel values\n+        self.assertEqual(encoding_cpu[\"pixel_values\"].shape, encoding_gpu[\"pixel_values\"].shape)\n+        self.assertTrue(\n+            torch.allclose(\n+                encoding_cpu[\"pixel_values\"][0, 0, 0, :3],\n+                encoding_gpu[\"pixel_values\"][0, 0, 0, :3].to(\"cpu\"),\n+                atol=1e-4,\n+            )\n         )\n-        expected_boxes_1 = torch.tensor(\n-            [\n-                [0.5503, 0.2765, 0.0604, 0.2215],\n-                [0.1695, 0.2016, 0.2080, 0.0940],\n-                [0.5006, 0.4933, 0.9977, 0.9865],\n-                [0.5008, 0.5002, 0.9983, 0.9955],\n-                [0.2627, 0.5456, 0.4707, 0.8646],\n-                [0.7715, 0.4115, 0.4570, 0.7161],\n-            ]\n+        # verify area\n+        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"area\"], encoding_gpu[\"labels\"][0][\"area\"].to(\"cpu\")))\n+        # verify boxes\n+        self.assertEqual(encoding_cpu[\"labels\"][0][\"boxes\"].shape, encoding_gpu[\"labels\"][0][\"boxes\"].shape)\n+        self.assertTrue(\n+            torch.allclose(\n+                encoding_cpu[\"labels\"][0][\"boxes\"][0], encoding_gpu[\"labels\"][0][\"boxes\"][0].to(\"cpu\"), atol=1e-3\n+            )\n         )\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n-\n-        # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n-        # format and not in the range [0, 1]\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            return_segmentation_masks=True,\n-            do_convert_annotations=False,\n-            return_tensors=\"pt\",\n+        # verify image_id\n+        self.assertTrue(\n+            torch.allclose(encoding_cpu[\"labels\"][0][\"image_id\"], encoding_gpu[\"labels\"][0][\"image_id\"].to(\"cpu\"))\n         )\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        # Convert to absolute coordinates\n-        unnormalized_boxes_0 = torch.vstack(\n-            [\n-                expected_boxes_0[:, 0] * postprocessed_width,\n-                expected_boxes_0[:, 1] * postprocessed_height,\n-                expected_boxes_0[:, 2] * postprocessed_width,\n-                expected_boxes_0[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        unnormalized_boxes_1 = torch.vstack(\n-            [\n-                expected_boxes_1[:, 0] * postprocessed_width,\n-                expected_boxes_1[:, 1] * postprocessed_height,\n-                expected_boxes_1[:, 2] * postprocessed_width,\n-                expected_boxes_1[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n-        expected_boxes_0 = torch.vstack(\n-            [\n-                unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n-                unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n-            ]\n-        ).T\n-        expected_boxes_1 = torch.vstack(\n-            [\n-                unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n-                unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n-            ]\n-        ).T\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+        # verify is_crowd\n+        self.assertTrue(\n+            torch.allclose(encoding_cpu[\"labels\"][0][\"iscrowd\"], encoding_gpu[\"labels\"][0][\"iscrowd\"].to(\"cpu\"))\n+        )\n+        # verify class_labels\n+        self.assertTrue(\n+            torch.allclose(\n+                encoding_cpu[\"labels\"][0][\"class_labels\"], encoding_gpu[\"labels\"][0][\"class_labels\"].to(\"cpu\")\n+            )\n+        )\n+        # verify orig_size\n+        self.assertTrue(\n+            torch.allclose(encoding_cpu[\"labels\"][0][\"orig_size\"], encoding_gpu[\"labels\"][0][\"orig_size\"].to(\"cpu\"))\n+        )\n+        # verify size\n+        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"size\"], encoding_gpu[\"labels\"][0][\"size\"].to(\"cpu\")))"
        }
    ],
    "stats": {
        "total": 1584,
        "additions": 1259,
        "deletions": 325
    }
}