{
    "author": "yao-matrix",
    "message": "fix transformers_cli import relative path issue (#36989)\n\n* fix transformers_cli relative import path issue\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "5b08db884443fe9446138dd835cb98b0b4ba5c54",
    "files": [
        {
            "sha": "4721f1ccf6603629cfa8199505e33ebc2d2a0d8d",
            "filename": "src/transformers/commands/env.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b08db884443fe9446138dd835cb98b0b4ba5c54/src%2Ftransformers%2Fcommands%2Fenv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b08db884443fe9446138dd835cb98b0b4ba5c54/src%2Ftransformers%2Fcommands%2Fenv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fenv.py?ref=5b08db884443fe9446138dd835cb98b0b4ba5c54",
            "patch": "@@ -94,6 +94,7 @@ def run(self):\n \n             pt_version = torch.__version__\n             pt_cuda_available = torch.cuda.is_available()\n+            pt_xpu_available = torch.xpu.is_available()\n             pt_npu_available = is_torch_npu_available()\n             pt_hpu_available = is_torch_hpu_available()\n \n@@ -151,6 +152,9 @@ def run(self):\n             if pt_cuda_available:\n                 info[\"Using GPU in script?\"] = \"<fill in>\"\n                 info[\"GPU type\"] = torch.cuda.get_device_name()\n+            elif pt_xpu_available:\n+                info[\"Using XPU in script?\"] = \"<fill in>\"\n+                info[\"XPU type\"] = torch.xpu.get_device_name()\n             elif pt_hpu_available:\n                 info[\"Using HPU in script?\"] = \"<fill in>\"\n                 info[\"HPU type\"] = torch.hpu.get_device_name()"
        },
        {
            "sha": "a066a165d9b79f517cefc2f08e0bf4576bd5f792",
            "filename": "src/transformers/commands/transformers_cli.py",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b08db884443fe9446138dd835cb98b0b4ba5c54/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b08db884443fe9446138dd835cb98b0b4ba5c54/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py?ref=5b08db884443fe9446138dd835cb98b0b4ba5c54",
            "patch": "@@ -14,15 +14,14 @@\n # limitations under the License.\n \n from transformers import HfArgumentParser\n-\n-from .add_fast_image_processor import AddFastImageProcessorCommand\n-from .add_new_model_like import AddNewModelLikeCommand\n-from .chat import ChatCommand\n-from .convert import ConvertCommand\n-from .download import DownloadCommand\n-from .env import EnvironmentCommand\n-from .run import RunCommand\n-from .serving import ServeCommand\n+from transformers.commands.add_fast_image_processor import AddFastImageProcessorCommand\n+from transformers.commands.add_new_model_like import AddNewModelLikeCommand\n+from transformers.commands.chat import ChatCommand\n+from transformers.commands.convert import ConvertCommand\n+from transformers.commands.download import DownloadCommand\n+from transformers.commands.env import EnvironmentCommand\n+from transformers.commands.run import RunCommand\n+from transformers.commands.serving import ServeCommand\n \n \n def main():"
        }
    ],
    "stats": {
        "total": 21,
        "additions": 12,
        "deletions": 9
    }
}