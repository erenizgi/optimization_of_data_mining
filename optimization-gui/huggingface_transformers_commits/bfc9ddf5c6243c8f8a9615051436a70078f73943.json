{
    "author": "SunMarc",
    "message": "Add StableAdamW Optimizer  (#39446)\n\n* Added StableAdamW as an optimizer option for Trainer. Also wrote tests to verify its behaviour.\n\n* Fixed issue with\n\n* Added docs for StableAdamW. Also fixed a typo in schedule free optimizers\n\n---------\n\nCo-authored-by: Gautham Krithiwas <gauthamkrithiwas2003@gmail.com>",
    "sha": "bfc9ddf5c6243c8f8a9615051436a70078f73943",
    "files": [
        {
            "sha": "20bb956ad658508cca0e5ff1147dc8655accaf4a",
            "filename": "docs/source/en/optimizers.md",
            "status": "modified",
            "additions": 27,
            "deletions": 1,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfc9ddf5c6243c8f8a9615051436a70078f73943/docs%2Fsource%2Fen%2Foptimizers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfc9ddf5c6243c8f8a9615051436a70078f73943/docs%2Fsource%2Fen%2Foptimizers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Foptimizers.md?ref=bfc9ddf5c6243c8f8a9615051436a70078f73943",
            "patch": "@@ -164,7 +164,7 @@ args = TrainingArguments(\n     output_dir=\"./test-schedulefree\",\n     max_steps=1000,\n     per_device_train_batch_size=4,\n-+   optim=\"schedule_free_radamw,\n++   optim=\"schedule_free_radamw\",\n +   lr_scheduler_type=\"constant\",\n     gradient_checkpointing=True,\n     logging_strategy=\"steps\",\n@@ -174,3 +174,29 @@ args = TrainingArguments(\n     run_name=\"sfo\",\n )\n ```\n+\n+## StableAdamW\n+\n+```bash\n+pip install torch-optimi\n+```\n+\n+[StableAdamW](https://arxiv.org/pdf/2304.13013) is a hybrid between AdamW and AdaFactor. It ports AdaFactor's update clipping into AdamW, which removes the need for gradient clipping. Otherwise, it behaves as a drop-in replacement for AdamW.\n+\n+> [!TIP]\n+> If training on large batch sizes or still observing training loss spikes, consider reducing beta_2 between [0.95, 0.99].\n+\n+```diff\n+args = TrainingArguments(\n+    output_dir=\"./test-stable-adamw\",\n+    max_steps=1000,\n+    per_device_train_batch_size=4,\n++   optim=\"stable_adamw\",\n+    gradient_checkpointing=True,\n+    logging_strategy=\"steps\",\n+    logging_steps=1,\n+    learning_rate=2e-6,\n+    save_strategy=\"no\",\n+    run_name=\"stable-adamw\",\n+)\n+```\n\\ No newline at end of file"
        },
        {
            "sha": "c582f0e4fb94e7563d0c51b08c07a069cb8e8a5c",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfc9ddf5c6243c8f8a9615051436a70078f73943/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfc9ddf5c6243c8f8a9615051436a70078f73943/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=bfc9ddf5c6243c8f8a9615051436a70078f73943",
            "patch": "@@ -152,6 +152,7 @@\n     is_torch_mlu_available,\n     is_torch_neuroncore_available,\n     is_torch_npu_available,\n+    is_torch_optimi_available,\n     is_torch_sdpa_available,\n     is_torch_tensorrt_fx_available,\n     is_torch_tf32_available,\n@@ -379,6 +380,14 @@ def require_apollo_torch(test_case):\n     return unittest.skipUnless(is_apollo_torch_available(), \"test requires APOLLO\")(test_case)\n \n \n+def require_torch_optimi(test_case):\n+    \"\"\"\n+    Decorator marking a test that requires torch-optimi. These tests are skipped when torch-optimi isn't installed.\n+    https://github.com/jxnl/torch-optimi\n+    \"\"\"\n+    return unittest.skipUnless(is_torch_optimi_available(), \"test requires torch-optimi\")(test_case)\n+\n+\n def require_lomo(test_case):\n     \"\"\"\n     Decorator marking a test that requires LOMO. These tests are skipped when LOMO-optim isn't installed."
        },
        {
            "sha": "3db5940a8d89f946cdeac27b929fa6b5a8bc28c0",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfc9ddf5c6243c8f8a9615051436a70078f73943/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfc9ddf5c6243c8f8a9615051436a70078f73943/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=bfc9ddf5c6243c8f8a9615051436a70078f73943",
            "patch": "@@ -169,6 +169,7 @@\n     is_torch_musa_available,\n     is_torch_neuroncore_available,\n     is_torch_npu_available,\n+    is_torch_optimi_available,\n     is_torch_xla_available,\n     is_torch_xpu_available,\n     is_torchao_available,\n@@ -1723,6 +1724,31 @@ def optimizer_hook(param):\n                 }\n             )\n             optimizer_kwargs.update(additional_optim_kwargs)\n+        elif args.optim == OptimizerNames.STABLE_ADAMW:\n+            if not is_torch_optimi_available():\n+                raise ImportError(\n+                    \"You need to install `torch-optimi` in order to use stable_adamw optimizers. \"\n+                    \"Install it with `pip install torch-optimi`.\"\n+                )\n+            from optimi import StableAdamW\n+\n+            max_lr = optim_args.pop(\"max_lr\", None)\n+            if max_lr is not None:\n+                max_lr = float(max_lr)\n+\n+            kahan_sum = optim_args.pop(\"kahan_sum\", None)\n+            if kahan_sum is not None:\n+                kahan_sum = bool(kahan_sum)\n+\n+            stable_adamw_kwargs = {\n+                \"decouple_lr\": bool(optim_args.pop(\"decouple_lr\", False)),\n+                \"max_lr\": max_lr,\n+                \"kahan_sum\": kahan_sum,\n+            }\n+\n+            optimizer_cls = StableAdamW\n+            optimizer_kwargs.update(adam_kwargs)\n+            optimizer_kwargs.update(stable_adamw_kwargs)\n         else:\n             raise ValueError(f\"Trainer cannot instantiate unsupported optimizer: {args.optim}\")\n         return optimizer_cls, optimizer_kwargs"
        },
        {
            "sha": "e7cf36c68d2463840fda36c84641c63126f6dd3f",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfc9ddf5c6243c8f8a9615051436a70078f73943/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfc9ddf5c6243c8f8a9615051436a70078f73943/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=bfc9ddf5c6243c8f8a9615051436a70078f73943",
            "patch": "@@ -186,6 +186,7 @@ class OptimizerNames(ExplicitEnum):\n     SCHEDULE_FREE_SGD = \"schedule_free_sgd\"\n     APOLLO_ADAMW = \"apollo_adamw\"\n     APOLLO_ADAMW_LAYERWISE = \"apollo_adamw_layerwise\"\n+    STABLE_ADAMW = \"stable_adamw\"\n \n \n def _convert_str_dict(passed_value: dict):"
        },
        {
            "sha": "9c1132ec5c3ef776f4d8d7ef101cda53f46adf8b",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfc9ddf5c6243c8f8a9615051436a70078f73943/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfc9ddf5c6243c8f8a9615051436a70078f73943/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=bfc9ddf5c6243c8f8a9615051436a70078f73943",
            "patch": "@@ -249,6 +249,7 @@\n     is_torch_musa_available,\n     is_torch_neuroncore_available,\n     is_torch_npu_available,\n+    is_torch_optimi_available,\n     is_torch_sdpa_available,\n     is_torch_tensorrt_fx_available,\n     is_torch_tf32_available,"
        },
        {
            "sha": "c20d3d36f5957328bbccde5194fcbde7f581add6",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfc9ddf5c6243c8f8a9615051436a70078f73943/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfc9ddf5c6243c8f8a9615051436a70078f73943/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=bfc9ddf5c6243c8f8a9615051436a70078f73943",
            "patch": "@@ -127,6 +127,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _lomo_available = _is_package_available(\"lomo_optim\")\n _grokadamw_available = _is_package_available(\"grokadamw\")\n _schedulefree_available, _schedulefree_version = _is_package_available(\"schedulefree\", return_version=True)\n+_torch_optimi_available = importlib.util.find_spec(\"optimi\") is not None\n # `importlib.metadata.version` doesn't work with `bs4` but `beautifulsoup4`. For `importlib.util.find_spec`, reversed.\n _bs4_available = importlib.util.find_spec(\"bs4\") is not None\n _coloredlogs_available = _is_package_available(\"coloredlogs\")\n@@ -474,6 +475,10 @@ def is_apollo_torch_available():\n     return _apollo_torch_available\n \n \n+def is_torch_optimi_available():\n+    return _torch_optimi_available\n+\n+\n def is_lomo_available():\n     return _lomo_available\n "
        },
        {
            "sha": "9ad624cd083b48408454dd3f7c4c17067336f390",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 118,
            "deletions": 0,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfc9ddf5c6243c8f8a9615051436a70078f73943/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfc9ddf5c6243c8f8a9615051436a70078f73943/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=bfc9ddf5c6243c8f8a9615051436a70078f73943",
            "patch": "@@ -99,6 +99,7 @@\n     require_torch_multi_accelerator,\n     require_torch_non_multi_accelerator,\n     require_torch_non_multi_gpu,\n+    require_torch_optimi,\n     require_torch_tensorrt_fx,\n     require_torch_tf32,\n     require_torch_up_to_2_accelerators,\n@@ -2518,6 +2519,123 @@ def test_apollo_lr_display_with_scheduler(self):\n         # warm up steps << total steps\n         self.assertTrue(len(decreasing_lrs) > len(increasing_lrs))\n \n+    @require_torch_optimi\n+    @require_torch_gpu\n+    def test_stable_adamw(self):\n+        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+        tiny_llama = LlamaForCausalLM(config)\n+        x = torch.randint(0, 100, (128,))\n+        train_dataset = RepeatDataset(x)\n+\n+        # Trainer without inf/nan filter\n+        args = TrainingArguments(\n+            self.get_auto_remove_tmp_dir(),\n+            learning_rate=1e-9,\n+            logging_steps=5,\n+            optim=\"stable_adamw\",\n+            optim_target_modules=[r\".*attn.*\", r\".*mlp.*\"],\n+        )\n+        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+        _ = trainer.train()\n+\n+    @require_torch_optimi\n+    @require_torch_gpu\n+    def test_stable_adamw_extra_args(self):\n+        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+        tiny_llama = LlamaForCausalLM(config)\n+        x = torch.randint(0, 100, (128,))\n+        train_dataset = RepeatDataset(x)\n+\n+        # Trainer without inf/nan filter\n+        args = TrainingArguments(\n+            self.get_auto_remove_tmp_dir(),\n+            learning_rate=1e-9,\n+            logging_steps=5,\n+            optim=\"stable_adamw\",\n+            optim_args=\"decouple_lr=True,max_lr=1e-3,kahan_sum=True\",\n+            optim_target_modules=[r\".*attn.*\", r\".*mlp.*\"],\n+        )\n+        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+\n+        # Check this works\n+        _ = trainer.train()\n+\n+    @require_torch_optimi\n+    @require_torch_gpu\n+    def test_stable_adamw_lr_display_without_scheduler(self):\n+        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+        tiny_llama = LlamaForCausalLM(config)\n+        x = torch.randint(0, 100, (128,))\n+        train_dataset = RepeatDataset(x)\n+\n+        learning_rate = 1e-9\n+        num_steps = 10\n+\n+        # Trainer without inf/nan filter\n+        args = TrainingArguments(\n+            self.get_auto_remove_tmp_dir(),\n+            learning_rate=learning_rate,\n+            logging_steps=5,\n+            optim=\"stable_adamw\",\n+            optim_target_modules=[r\".*attn.*\", r\".*mlp.*\"],\n+        )\n+        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+        trainer.create_optimizer_and_scheduler(num_training_steps=num_steps)\n+\n+        # reflects displayed lr in trainer\n+        self.assertEqual(trainer.get_learning_rates(), [learning_rate, learning_rate])\n+\n+    @require_torch_optimi\n+    @require_torch_gpu\n+    def test_stable_adamw_lr_display_with_scheduler(self):\n+        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+        tiny_llama = LlamaForCausalLM(config)\n+        x = torch.randint(0, 100, (128,))\n+        train_dataset = RepeatDataset(x)\n+\n+        learning_rate = 2e-4\n+        num_train_epochs = 10\n+        num_warmup_steps = 5\n+\n+        # Trainer without inf/nan filter\n+        args = TrainingArguments(\n+            self.get_auto_remove_tmp_dir(),\n+            num_train_epochs=num_train_epochs,\n+            learning_rate=learning_rate,\n+            warmup_steps=num_warmup_steps,\n+            lr_scheduler_type=\"cosine\",\n+            logging_steps=1,\n+            optim=\"stable_adamw\",\n+            optim_target_modules=[r\".*attn.*\", r\".*mlp.*\"],\n+        )\n+        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+\n+        # creating log history of trainer, results don't matter\n+        trainer.train()\n+        logs = trainer.state.log_history[1:][:-1]\n+\n+        # reach given learning rate peak and end with 0 lr\n+        self.assertTrue(logs[num_warmup_steps - 2][\"learning_rate\"] == learning_rate)\n+        self.assertTrue(logs[-1][\"learning_rate\"] == 0)\n+\n+        # increasing and decreasing pattern of lrs\n+        increasing_lrs = [\n+            logs[i][\"learning_rate\"] < logs[i + 1][\"learning_rate\"]\n+            for i in range(len(logs))\n+            if i < num_warmup_steps - 2\n+        ]\n+        decreasing_lrs = [\n+            logs[i][\"learning_rate\"] > logs[i + 1][\"learning_rate\"]\n+            for i in range(len(logs) - 1)\n+            if i >= num_warmup_steps - 2\n+        ]\n+\n+        self.assertTrue(all(increasing_lrs))\n+        self.assertTrue(all(decreasing_lrs))\n+\n+        # warm up steps << total steps\n+        self.assertTrue(len(decreasing_lrs) > len(increasing_lrs))\n+\n     @require_torch_multi_accelerator\n     def test_data_is_not_parallelized_when_model_is_parallel(self):\n         model = RegressionModel()"
        }
    ],
    "stats": {
        "total": 188,
        "additions": 187,
        "deletions": 1
    }
}