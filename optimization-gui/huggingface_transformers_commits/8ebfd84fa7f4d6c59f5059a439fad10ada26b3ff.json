{
    "author": "Qubitium",
    "message": "Fully deprecate AutoGPTQ and AutoAWQ for GPT-QModel (#41567)\n\n* fully deprecate autogptq\n\n* remove use_cuda and use_exllama toggles are fully deprecated in gptqmodel\n\n* format\n\n* add `act_group_aware` property\n\n* fix QUANT_TYPE assert\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* format\n\n* mod awq import\n\n* remove autoawq fuse support\n\n* remove remove autoawq.config fuse\n\n* cleanup\n\n* remove awq fuse test\n\n* fix import\n\n* use gptqmodel\n\n* cleanup\n\n* remove get_modules_to_fuse\n\n* mod require_auto_awq -> require_gptqmodel\n\n* convert vertion to checkpoint_format\n\n* check is_gptqmodel_available\n\n* revert modules_to_not_convert\n\n* pass bits, sym, desc_act\n\n* fix awqconfig init\n\n* fix wrong args\n\n* fix ipex\n\n* mod ipex version check\n\n* cleanup\n\n* fix awq_linear\n\n* remove self.exllama_config = exllama_config\n\n* cleanuo\n\n* Revert \"cleanuo\"\n\nThis reverts commit 90019c6fc4f7a617ed9db482a42ecd1cd07f9108.\n\n* update is_trainable\n\n* cleanup\n\n* remove fused\n\n* call hf_select_quant_linear_v2()\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* Remove the \"version\" field from AwqConfig\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* Add torch_fused inferencefix test_gptq test\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* fix test_awq\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* fix test_awq\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* fix AwqConfig\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* call hf_select_quant_linear_v2()\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* remove auto_awq\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* fix typo\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* Compatible with legacy field: checkpoint_format\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* Compatible with legacy field: checkpoint_format\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* format\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* CLEANUP\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* update test_awq\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* fix get_modules_to_not_convert()\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* fix test_awq.py::AwqTest::test_quantized_model_exllama\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* Apply style fixes\n\n* test_awq.py added EXPECTED_OUTPUT\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* update test_gptq.py\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* fix test_awq.py::AwqTest::test_save_pretrained\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* use assertEqual() instead of assertTrue()\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* fix test_quantized_layers_class()\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* remove ExllamaV1 Test\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* format\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* fix get_modules_to_not_convert()\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* added EXPECTED_OUTPUT\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* remove ExllamaV1 Test\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* add AwqBackend.AUTO_TRAINABLE\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n* Update docs/source/zh/llm_tutorial.md\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* revert temporarily fix\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\n\n---------\n\nSigned-off-by: ZX-ModelCloud <zx@modelcloud.ai>\nCo-authored-by: ZX-ModelCloud <zx@modelcloud.ai>\nCo-authored-by: LRL2-ModelCloud <lrl2@modelcloud.ai>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>\nCo-authored-by: ZX-ModelCloud <165115237+ZX-ModelCloud@users.noreply.github.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
    "files": [
        {
            "sha": "f09d48cb6c05d776c2afb9188c5b1f7abbc8b016",
            "filename": "docker/transformers-intel-cpu/Dockerfile",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docker%2Ftransformers-intel-cpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docker%2Ftransformers-intel-cpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-intel-cpu%2FDockerfile?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -48,7 +48,6 @@ RUN pip install --upgrade pip wheel\n RUN pip install torch torchvision torchaudio torchcodec --index-url https://download.pytorch.org/whl/cpu --no-cache-dir\n RUN pip install av pyctcdecode pytesseract decord galore-torch fire scipy scikit-learn sentencepiece sentence_transformers sacremoses nltk rouge_score librosa soundfile mpi4py pytorch_msssim\n RUN pip install onnx optimum onnxruntime\n-RUN pip install autoawq\n RUN pip install gptqmodel --no-build-isolation\n RUN pip install -U datasets timm transformers accelerate peft diffusers opencv-python kenlm evaluate\n RUN pip install -U intel-openmp"
        },
        {
            "sha": "f7b2c9d430ed974ee56566b56d27a8da8e7723bb",
            "filename": "docker/transformers-pytorch-xpu/Dockerfile",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docker%2Ftransformers-pytorch-xpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docker%2Ftransformers-pytorch-xpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-pytorch-xpu%2FDockerfile?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -74,7 +74,7 @@ RUN pip install torchcodec torchdata --no-cache-dir\n RUN pip install evaluate pyctcdecode pytesseract decord galore-torch fire scipy scikit-learn sentencepiece sacremoses nltk rouge_score librosa soundfile g2p_en mpi4py requests_mock\n RUN pip install pretty_midi essentia resampy Levenshtein av sacrebleu phonemizer invisible_watermark schedulefree setuptools\n RUN pip install gptqmodel --no-build-isolation\n-RUN pip install gguf hqq compressed_tensors autoawq deepspeed torchao onnx auto_round\n+RUN pip install gguf hqq compressed_tensors deepspeed torchao onnx auto_round\n RUN pip install hf_transfer huggingface-hub hf-doc-builder datasets optimum-quanto timm transformers accelerate optimum peft diffusers trl kernels\n \n # install liger-kernel"
        },
        {
            "sha": "c635137ca10c4c961b2ecd7575152bc547e9da3a",
            "filename": "docker/transformers-quantization-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -49,9 +49,6 @@ RUN python3 -m pip install --no-cache-dir hqq\n # For GGUF tests\n RUN python3 -m pip install --no-cache-dir gguf\n \n-# Add autoawq for quantization testing\n-RUN python3 -m pip install --no-cache-dir --no-build-isolation autoawq[kernels]\n-\n # Add quanto for quantization testing\n RUN python3 -m pip install --no-cache-dir optimum-quanto\n "
        },
        {
            "sha": "6d6cbfdf90201a8208e9ac2deaf603726b196a47",
            "filename": "docs/source/ar/llm_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Far%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Far%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fllm_tutorial.md?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -238,7 +238,7 @@ LLMs Ù‡ÙŠ [Ù…Ø¹Ù…Ø§Ø±ÙŠØ§Øª ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± ÙÙ‚Ø·](https://huggingface.co/l\n \n ### Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© ÙˆØ§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© ÙˆØ§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©\n 1. Ø¯Ù„ÙŠÙ„ ØªØ­Ø³ÙŠÙ† Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø©: Ø¯Ù„ÙŠÙ„ ØªØ­Ø³ÙŠÙ† Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©.\n-2. Ø§Ù„ØªÙƒÙ…ÙŠÙ… (Quantization): Ø¯Ù„ÙŠÙ„ Ø­ÙˆÙ„ ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØªÙƒÙ…ÙŠÙ… Ø§Ù„ØªÙƒÙ…ÙŠÙ… Ù…Ø«Ù„ ØªÙ‚Ù†ÙŠØªÙŠ bitsandbytes Ùˆ autogptqØŒ ÙˆØ§Ù„ØªÙŠ ØªÙˆØ¶Ø­ ÙƒÙŠÙÙŠØ© ØªÙ‚Ù„ÙŠÙ„ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ±.\n+2. Ø§Ù„ØªÙƒÙ…ÙŠÙ… (Quantization): Ø¯Ù„ÙŠÙ„ Ø­ÙˆÙ„ ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØªÙƒÙ…ÙŠÙ… Ø§Ù„ØªÙƒÙ…ÙŠÙ… Ù…Ø«Ù„ ØªÙ‚Ù†ÙŠØªÙŠ bitsandbytes Ùˆ GPT-QModelØŒ ÙˆØ§Ù„ØªÙŠ ØªÙˆØ¶Ø­ ÙƒÙŠÙÙŠØ© ØªÙ‚Ù„ÙŠÙ„ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ±.\n \n ### Ù…ÙƒØªØ¨Ø§Øª Ù…Ø±ØªØ¨Ø·Ø©\n 1. [`optimum`](https://github.com/huggingface/optimum), Ø§Ù…ØªØ¯Ø§Ø¯ Ù„Ù…ÙƒØªØ¨Ø© Transformers ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù„Ø£Ø¬Ù‡Ø²Ø© Ù…Ø¹ÙŠÙ†Ø©."
        },
        {
            "sha": "86e93ca2a99ca012cd62cb1433328dedfead171d",
            "filename": "docs/source/ar/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -273,7 +273,7 @@ flush()\n \n ÙŠØ³Ù…Ø­ ØªÙƒÙ…ÙŠÙ… 4 Ø¨Øª Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª Ù…Ø«Ù„ RTX3090 Ùˆ V100 Ùˆ T4 ÙˆØ§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„ÙŠÙ‡Ø§ Ø¨Ø³Ù‡ÙˆÙ„Ø© Ù„Ù…Ø¹Ø¸Ù… Ø§Ù„Ø£Ø´Ø®Ø§Øµ.\n \n-Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­ÙˆÙ„ Ø§Ù„ØªÙƒÙ…ÙŠÙ… ÙˆÙ„Ù…Ø¹Ø±ÙØ© ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªÙƒÙ…ÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ø·Ù„Ø¨ Ø°Ø§ÙƒØ±Ø© GPU VRAM Ø£Ù‚Ù„ Ø­ØªÙ‰ Ù…Ù† 4 Ø¨ØªØŒ Ù†ÙˆØµÙŠ Ø¨Ø§Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ ØªÙ†ÙÙŠØ° [`AutoGPTQ`](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#autogptq-integration%60).\n+Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­ÙˆÙ„ Ø§Ù„ØªÙƒÙ…ÙŠÙ… ÙˆÙ„Ù…Ø¹Ø±ÙØ© ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªÙƒÙ…ÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ø·Ù„Ø¨ Ø°Ø§ÙƒØ±Ø© GPU VRAM Ø£Ù‚Ù„ Ø­ØªÙ‰ Ù…Ù† 4 Ø¨ØªØŒ Ù†ÙˆØµÙŠ Ø¨Ø§Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ ØªÙ†ÙÙŠØ° [`GPT-QModel`](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#gptqmodel).\n \n > ÙƒØ§Ø³ØªÙ†ØªØ§Ø¬ØŒ Ù…Ù† Ø§Ù„Ù…Ù‡Ù… ØªØ°ÙƒØ± Ø£Ù† ØªÙƒÙ…ÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØªØ¯Ø§ÙˆÙ„ ÙƒÙØ§Ø¡Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø¯Ù‚Ø© ÙˆÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø§Ù„Ø§Øª ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„.\n "
        },
        {
            "sha": "2cb50ba2879a4d0b4a9c9323eb7e239645a47c59",
            "filename": "docs/source/en/llm_optims.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fen%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fen%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_optims.md?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -360,7 +360,7 @@ Quantization reduces the size of model weights by storing them in a lower precis\n If you aren't limited by your GPU, you don't necessarily need to quantize your model because it can increase latency slightly (except for AWQ and fused AWQ modules) due to the extra step required to quantize and dequantize the weights.\n \n > [!TIP]\n-> There are many quantization libraries (see the [Quantization](./quantization) guide for more details) available, such as Quanto, AQLM, VPTQ, AWQ, and AutoGPTQ. Feel free to try them out and see which one works best for your use case. We also recommend reading the [Overview of natively supported quantization schemes in ğŸ¤— Transformers](https://hf.co/blog/overview-quantization-transformers) blog post which compares AutoGPTQ and bitsandbytes.\n+> There are many quantization libraries (see the [Quantization](./quantization) guide for more details) available, such as Quanto, AQLM, VPTQ, AWQ, and GPT-QModel. Feel free to try them out and see which one works best for your use case. We also recommend reading the [Overview of natively supported quantization schemes in ğŸ¤— Transformers](https://hf.co/blog/overview-quantization-transformers) blog post for a comparison of different approaches.\n \n Use the Model Memory Calculator below to estimate and compare how much memory is required to load a model. For example, try estimating the memory required to load [Mistral-7B-v0.1](https://hf.co/mistralai/Mistral-7B-v0.1).\n "
        },
        {
            "sha": "243f7beb08586abd75fe8895e055886a9f5e7927",
            "filename": "docs/source/en/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -286,7 +286,7 @@ Overall, we saw that running OctoCoder in 8-bit precision reduced the required G\n \n 4-bit quantization allows the model to be run on GPUs such as RTX3090, V100, and T4 which are quite accessible for most people.\n \n-For more information on quantization and to see how one can quantize models to require even less GPU VRAM memory than 4-bit, we recommend looking into the [`AutoGPTQ`](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#autogptq-integration%60) implementation.\n+For more information on quantization and to see how one can quantize models to require even less GPU VRAM memory than 4-bit, we recommend looking into the [`GPT-QModel`](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#gptqmodel) implementation.\n \n > As a conclusion, it is important to remember that model quantization trades improved memory efficiency against accuracy and in some cases inference time.\n "
        },
        {
            "sha": "51ecfd825b123ae9a55a158ee404d1524a3666e7",
            "filename": "docs/source/en/quantization/gptq.md",
            "status": "modified",
            "additions": 9,
            "deletions": 58,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fen%2Fquantization%2Fgptq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fen%2Fquantization%2Fgptq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fgptq.md?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -16,36 +16,22 @@ rendered properly in your Markdown viewer.\n \n # GPTQ\n \n-The [GPTQModel](https://github.com/ModelCloud/GPTQModel) and [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) implements the GPTQ algorithm, a post-training quantization technique where each row of the weight matrix is quantized independently to find a version of the weights that minimizes the error. These weights are quantized to int4, but they're restored to fp16 on the fly during inference. This can save memory usage by 4x because the int4 weights are dequantized in a fused kernel rather than a GPU's global memory. Inference is also faster because a lower bitwidth takes less time to communicate.\n+The [GPT-QModel](https://github.com/ModelCloud/GPTQModel) project (Python package `gptqmodel`) implements the GPTQ algorithm, a post-training quantization technique where each row of the weight matrix is quantized independently to find a version of the weights that minimizes the error. These weights are quantized to int4, but they're restored to fp16 on the fly during inference. This can save memory usage by 4x because the int4 weights are dequantized in a fused kernel rather than a GPU's global memory. Inference is also faster because a lower bitwidth takes less time to communicate.\n \n-> [!WARNING]\n-> AutoGPTQ is likely to be deprecated in the future due to lack of continued support for new models and features. See the [GPTQModel](#gptqmodel) section for more details.\n+AutoGPTQ is no longer supported in Transformers. Install GPT-QModel] instead.\n \n Install Accelerate, Transformers and Optimum first.\n \n ```bash\n pip install --upgrade accelerate optimum transformers\n ```\n \n-Then run the command below to install a GPTQ library.\n-\n-<hfoptions id=\"install\">\n-<hfoption id=\"GPTQmodel\">\n+Then run the command below to install GPT-QModel].\n \n ```bash\n pip install gptqmodel --no-build-isolation\n ```\n \n-</hfoption>\n-<hfoption id=\"AutoGPTQ\">\n-\n-```bash\n-pip install auto-gptq --no-build-isolation\n-```\n-\n-</hfoption>\n-</hfoptions>\n-\n Create a [`GPTQConfig`] class and set the number of bits to quantize to, a dataset to calbrate the weights for quantization, and a tokenizer to prepare the dataset.\n \n ```py\n@@ -58,7 +44,7 @@ gptq_config = GPTQConfig(bits=4, dataset=\"c4\", tokenizer=tokenizer)\n You can pass your own dataset as a list of strings, but it is highly recommended to use the same dataset from the GPTQ paper.\n \n ```py\n-dataset = [\"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"]\n+dataset = [\"gptqmodel is an easy-to-use model quantization library with user-friendly apis, based on the GPTQ algorithm.\"]\n gptq_config = GPTQConfig(bits=4, dataset=dataset, tokenizer=tokenizer)\n ```\n \n@@ -121,51 +107,16 @@ from transformers import AutoModelForCausalLM, GPTQConfig\n model = AutoModelForCausalLM.from_pretrained(\"{your_username}/opt-125m-gptq\", device_map=\"auto\", quantization_config=GPTQConfig(bits=4, backend=\"marlin\"))\n ```\n \n-## ExLlama\n-\n-> [!WARNING]\n-> Only 4-bit models are supported, and we recommend deactivating the ExLlama kernels if you're finetuning a quantized model with PEFT.\n-\n-[ExLlama](https://github.com/turboderp/exllama) is a Python/C++/CUDA implementation of the [Llama](model_doc/llama) model that is designed for faster inference with 4-bit GPTQ weights (check out these [benchmarks](https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark)). The ExLlama kernel is activated by default when you create a [`GPTQConfig`] object.\n-\n-To boost inference speed even further, use the [ExLlamaV2](https://github.com/turboderp/exllamav2) kernels by configuring the `exllama_config` parameter in [`GPTQConfig`].\n-\n-```py\n-import torch\n-from transformers import AutoModelForCausalLM, GPTQConfig\n-\n-gptq_config = GPTQConfig(bits=4, exllama_config={\"version\":2})\n-model = AutoModelForCausalLM.from_pretrained(\n-    \"{your_username}/opt-125m-gptq\",\n-    device_map=\"auto\",\n-    quantization_config=gptq_config\n-)\n-```\n-\n-The ExLlama kernels are only supported when the entire model is on the GPU. If you're doing inference on a CPU with AutoGPTQ 0.4.2+, disable the ExLlama kernel in [`GPTQConfig`]. This overwrites the attributes related to the ExLlama kernels in the quantization config of the `config.json` file.\n-\n-```py\n-import torch\n-from transformers import AutoModelForCausalLM, GPTQConfig\n-\n-gptq_config = GPTQConfig(bits=4, use_exllama=False)\n-model = AutoModelForCausalLM.from_pretrained(\n-    \"{your_username}/opt-125m-gptq\",\n-    device_map=\"cpu\",\n-    quantization_config=gptq_config\n-)\n-```\n-\n-## GPTQModel\n+## GPT-QModel]\n \n-It is recommended to use GPTQModel, originally a maintained fork of AutoGPTQ, because it has since diverged from AutoGTPQ with some significant features. GPTQModel has faster quantization, lower memory usage, and more accurate default quantization.\n+GPT-QModel] is the actively maintained backend for GPTQ in Transformers. It was originally forked from AutoGPTQ, but has since diverged with significant improvements such as faster quantization, lower memory usage, and more accurate defaults.\n \n-GPTQModel provides asymmetric quantization which can potentially lower quantization errors compared to symmetric quantization. It is not backward compatible with AutoGPTQ, and not all kernels (Marlin) support asymmetric quantization.\n+GPT-QModel] provides asymmetric quantization which can potentially lower quantization errors compared to symmetric quantization. It is not backward compatible with legacy AutoGPTQ checkpoints, and not all kernels (Marlin) support asymmetric quantization.\n \n-GPTQModel also has broader support for the latest LLM models, multimodal models (Qwen2-VL and Ovis1.6-VL), platforms (Linux, macOS, Windows 11), and hardware (AMD ROCm, Apple Silicon, Intel/AMD CPUs, and Intel Datacenter Max/Arc GPUs, etc.).\n+GPT-QModel] also has broader support for the latest LLM models, multimodal models (Qwen2-VL and Ovis1.6-VL), platforms (Linux, macOS, Windows 11), and hardware (AMD ROCm, Apple Silicon, Intel/AMD CPUs, and Intel Datacenter Max/Arc GPUs, etc.).\n \n The Marlin kernels are also updated for A100 GPUs and other kernels are updated to include auto-padding for legacy models and models with non-uniform in/out-features.\n \n ## Resources\n \n-Run the GPTQ quantization with PEFT [notebook](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing) for a hands-on experience, and read [Making LLMs lighter with AutoGPTQ and transformers](https://huggingface.co/blog/gptq-integration) to learn more about the AutoGPTQ integration.\n+Run the GPTQ quantization with PEFT [notebook](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing) for a hands-on experience."
        },
        {
            "sha": "1f1c03d7393b9f6e21c2ef3ae5d22c520446c8ed",
            "filename": "docs/source/en/quantization/overview.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Foverview.md?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -32,8 +32,7 @@ Use the Space below to help you pick a quantization method depending on your har\n | [EETQ](./eetq)                            | ğŸŸ¢                   | ğŸ”´              | ğŸŸ¢        | ğŸ”´        | ğŸ”´                                 | ğŸ”´              | ?               | 8            | ğŸŸ¢               | ğŸŸ¢                          | ğŸŸ¢                      | https://github.com/NetEase-FuXi/EETQ        |\n | [FP-Quant](./fp_quant)                          | ğŸŸ¢                   | ğŸ”´              | ğŸŸ¢        | ğŸ”´        | ğŸ”´                                 | ğŸ”´              | ğŸŸ¢              | 4           | ğŸ”´               | ğŸŸ¢                          | ğŸŸ¢                      | https://github.com/IST-DASLab/FP-Quant      |\n | [GGUF / GGML (llama.cpp)](../gguf)        | ğŸŸ¢                   | ğŸŸ¢              | ğŸŸ¢        | ğŸ”´        | ğŸŸ¢                                 | ğŸŸ¢              | ğŸ”´              | 1/8          | ğŸ”´               | [See Notes](../gguf)     | [See Notes](../gguf) | https://github.com/ggerganov/llama.cpp      |\n-| [GPTQModel](./gptq)                       | ğŸ”´                   | ğŸŸ¢ | ğŸŸ¢        | ğŸŸ¢        | ğŸŸ¢                                 | ğŸŸ¢ | ğŸ”´              | 2/3/4/8      | ğŸŸ¢               | ğŸŸ¢                          | ğŸŸ¢                      | https://github.com/ModelCloud/GPTQModel        |\n-| [AutoGPTQ](./gptq)                        | ğŸ”´                   | ğŸ”´              | ğŸŸ¢        | ğŸŸ¢        | ğŸ”´                                 | ğŸ”´              | ğŸ”´              | 2/3/4/8      | ğŸŸ¢               | ğŸŸ¢                          | ğŸŸ¢                      | https://github.com/AutoGPTQ/AutoGPTQ        |\n+| [GPT-QModel](./gptq)                     | ğŸ”´                   | ğŸŸ¢ | ğŸŸ¢        | ğŸŸ¢        | ğŸŸ¢                                 | ğŸŸ¢ | ğŸ”´              | 2/3/4/8      | ğŸŸ¢               | ğŸŸ¢                          | ğŸŸ¢                      | https://github.com/ModelCloud/GPTQModel        |\n | [HIGGS](./higgs)                          | ğŸŸ¢                   | ğŸ”´              | ğŸŸ¢        | ğŸ”´        | ğŸ”´                                 | ğŸ”´              | ğŸŸ¢              | 2/4          | ğŸ”´               | ğŸŸ¢                          | ğŸŸ¢                      | https://github.com/HanGuo97/flute           |\n | [HQQ](./hqq)                              | ğŸŸ¢                   | ğŸŸ¢              | ğŸŸ¢        | ğŸ”´        | ğŸ”´                                 | ğŸŸ¢              | ğŸŸ¢              | 1/8          | ğŸŸ¢               | ğŸ”´                          | ğŸŸ¢                      | https://github.com/mobiusml/hqq/            |\n | [optimum-quanto](./quanto)                | ğŸŸ¢                   | ğŸŸ¢              | ğŸŸ¢        | ğŸ”´        | ğŸŸ¢                                 | ğŸŸ¢              | ğŸŸ¢              | 2/4/8        | ğŸ”´               | ğŸ”´                          | ğŸŸ¢                      | https://github.com/huggingface/optimum-quanto       |"
        },
        {
            "sha": "d7f2776d5e524e68f32971a034f745549e8f0bdd",
            "filename": "docs/source/ja/main_classes/quantization.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fja%2Fmain_classes%2Fquantization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fja%2Fmain_classes%2Fquantization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fquantization.md?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -16,22 +16,22 @@ rendered properly in your Markdown viewer.\n \n # Quantize ğŸ¤— Transformers models\n \n-## `AutoGPTQ` Integration\n+## GPT-QModel Integration\n \n \n ğŸ¤— Transformers ã«ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã§ GPTQ é‡å­åŒ–ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã® `optimum` API ãŒçµ±åˆã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¤§å¹…ã«ä½ä¸‹ã•ã›ã‚‹ã“ã¨ãªãã€æ¨è«–é€Ÿåº¦ã‚’é«˜é€ŸåŒ–ã™ã‚‹ã“ã¨ãªãã€ãƒ¢ãƒ‡ãƒ«ã‚’ 8ã€4ã€3ã€ã•ã‚‰ã«ã¯ 2 ãƒ“ãƒƒãƒˆã§ãƒ­ãƒ¼ãƒ‰ãŠã‚ˆã³é‡å­åŒ–ã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€ã»ã¨ã‚“ã©ã® GPU ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n \n é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n - [GPTQ](https://huggingface.co/papers/2210.17323) è«–æ–‡\n - GPTQ é‡å­åŒ–ã«é–¢ã™ã‚‹ `optimum` [ã‚¬ã‚¤ãƒ‰](https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization)\n-- ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ [`AutoGPTQ`](https://github.com/PanQiWei/AutoGPTQ) ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n+- ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ `GPT-QModel` (https://github.com/ModelCloud/GPTQModel) ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n \n ### Requirements\n \n ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®è¦ä»¶ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š \n \n-- æœ€æ–°ã® `AutoGPTQ` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚\n-`pip install auto-gptq` ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚\n+- æœ€æ–°ã® `GPT-QModel` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚\n+`pip install gptqmodel --no-build-isolation` ã‚’å®Ÿè¡Œã™ã‚‹ã€‚\n \n - æœ€æ–°ã® `optimum` ã‚’ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚\n `git+https://github.com/huggingface/optimum.git` ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚\n@@ -63,7 +63,7 @@ gptq_config = GPTQConfig(bits=4, dataset = \"c4\", tokenizer=tokenizer)\n ç‹¬è‡ªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã¨ã—ã¦æ¸¡ã™ã“ã¨ãŒã§ãã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ãŸã ã—ã€GPTQ è«–æ–‡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’å¼·ããŠå‹§ã‚ã—ã¾ã™ã€‚\n \n ```python\n-dataset = [\"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"]\n+dataset = [\"gptqmodel is an easy-to-use model quantization library with user-friendly apis, based on the GPTQ algorithm.\"]\n quantization = GPTQConfig(bits=4, dataset = dataset, tokenizer=tokenizer)\n ```\n "
        },
        {
            "sha": "b2031bf3776b4d85ef73eb93916f05eb1b99b8f7",
            "filename": "docs/source/ko/llm_optims.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fko%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fko%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fllm_optims.md?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -372,7 +372,7 @@ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable\n ì–‘ìí™”ëŠ” LLM ê°€ì¤‘ì¹˜ë¥¼ ë” ë‚®ì€ ì •ë°€ë„ë¡œ ì €ì¥í•˜ì—¬ í¬ê¸°ë¥¼ ì¤„ì…ë‹ˆë‹¤. ì´ëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ë©° GPU ë©”ëª¨ë¦¬ì— ì œì•½ì´ ìˆëŠ” ê²½ìš° ì¶”ë¡ ì„ ìœ„í•´ LLMì„ ë¡œë“œí•˜ëŠ” ê²ƒì„ ë” ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤. GPUê°€ ì¶©ë¶„í•˜ë‹¤ë©´, ëª¨ë¸ì„ ì–‘ìí™”í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤. ì¶”ê°€ì ì¸ ì–‘ìí™” ë° ì–‘ìí™” í•´ì œ ë‹¨ê³„ë¡œ ì¸í•´ ì•½ê°„ì˜ ì§€ì—°ì´ ë°œìƒí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤(AWQ ë° ìœµí•© AWQ ëª¨ë“ˆ ì œì™¸).\n \n > [!TIP]\n-> ë‹¤ì–‘í•œ ì–‘ìí™” ë¼ì´ë¸ŒëŸ¬ë¦¬(ìì„¸í•œ ë‚´ìš©ì€ [Quantization](./quantization) ê°€ì´ë“œë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤)ê°€ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” Quanto, AQLM, VPTQ, AWQ ë° AutoGPTQê°€ í¬í•¨ë©ë‹ˆë‹¤. ì‚¬ìš© ì‚¬ë¡€ì— ê°€ì¥ ì˜ ë§ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ ë³´ì‹­ì‹œì˜¤. ë˜í•œ AutoGPTQì™€ bitsandbytesë¥¼ ë¹„êµí•˜ëŠ” [Overview of natively supported quantization schemes in ğŸ¤— Transformers](https://hf.co/blog/overview-quantization-transformers) ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ì„ ì½ì–´ë³´ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.\n+> ë‹¤ì–‘í•œ ì–‘ìí™” ë¼ì´ë¸ŒëŸ¬ë¦¬(ìì„¸í•œ ë‚´ìš©ì€ [Quantization](./quantization) ê°€ì´ë“œë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤)ê°€ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” Quanto, AQLM, VPTQ, AWQ ë° GPT-QModelì´ í¬í•¨ë©ë‹ˆë‹¤. ì‚¬ìš© ì‚¬ë¡€ì— ê°€ì¥ ì˜ ë§ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ ë³´ì‹­ì‹œì˜¤. ë˜í•œ gptqmodelê³¼ bitsandbytesë¥¼ ë¹„êµí•˜ëŠ” [Overview of natively supported quantization schemes in ğŸ¤— Transformers](https://hf.co/blog/overview-quantization-transformers) ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ì„ ì½ì–´ë³´ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.\n \n ì•„ë˜ì˜ ëª¨ë¸ ë©”ëª¨ë¦¬ ê³„ì‚°ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ë° í•„ìš”í•œ ë©”ëª¨ë¦¬ë¥¼ ì¶”ì •í•˜ê³  ë¹„êµí•´ ë³´ì‹­ì‹œì˜¤. ì˜ˆë¥¼ ë“¤ì–´ [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)ë¥¼ ë¡œë“œí•˜ëŠ” ë° í•„ìš”í•œ ë©”ëª¨ë¦¬ë¥¼ ì¶”ì •í•´ ë³´ì‹­ì‹œì˜¤.\n "
        },
        {
            "sha": "9e838a9ae2b1c21e3262c06fbfcc8d24a46d57ba",
            "filename": "docs/source/ko/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -269,7 +269,7 @@ flush()\n \n 4ë¹„íŠ¸ ì–‘ìí™”ëŠ” RTX3090, V100, T4ì™€ ê°™ì€ GPUì—ì„œ ëª¨ë¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•´ì£¼ë©°, ì´ëŠ” ëŒ€ë¶€ë¶„ì˜ ì‚¬ëŒë“¤ì´ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” GPUì…ë‹ˆë‹¤.\n \n-ì–‘ìí™”ì— ëŒ€í•œ ë” ë§ì€ ì •ë³´ë¥¼ í™•ì¸í•˜ê³  4ë¹„íŠ¸ë³´ë‹¤ ë” ì ì€ GPU VRAM ë©”ëª¨ë¦¬ë¡œ ëª¨ë¸ì„ ì–‘ìí™”í•˜ê±°ë‚˜, ë” ë§ì€ ì–‘ìí™” ê´€ë ¨ ì •ë³´ë¥¼ ë³´ë ¤ë©´ [`AutoGPTQ`](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#autogptq-integration%60) êµ¬í˜„ì„ ì°¸ì¡°í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.\n+ì–‘ìí™”ì— ëŒ€í•œ ë” ë§ì€ ì •ë³´ë¥¼ í™•ì¸í•˜ê³  4ë¹„íŠ¸ë³´ë‹¤ ë” ì ì€ GPU VRAM ë©”ëª¨ë¦¬ë¡œ ëª¨ë¸ì„ ì–‘ìí™”í•˜ê±°ë‚˜, ë” ë§ì€ ì–‘ìí™” ê´€ë ¨ ì •ë³´ë¥¼ ë³´ë ¤ë©´ [`GPT-QModel`](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#gptqmodel) êµ¬í˜„ì„ ì°¸ì¡°í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.\n \n > ê²°ë¡ ì ìœ¼ë¡œ, ëª¨ë¸ ì–‘ìí™”ëŠ” í–¥ìƒëœ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ëª¨ë¸ ì •í™•ì„± ê°„ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ê²ƒì´ë©°, ê²½ìš°ì— ë”°ë¼ ì¶”ë¡  ì‹œê°„ì—ë„ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n "
        },
        {
            "sha": "cea3df4f7014d752f328fa5c31aed82ddde75d54",
            "filename": "docs/source/ko/model_doc/llama2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama2.md?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -82,7 +82,7 @@ LLaMA2ë¥¼ ì‹œì‘í•˜ëŠ” ë° ë„ì›€ì´ ë  Hugging Faceì˜ ê³µì‹ ë° ì»¤ë®¤ë‹ˆí‹°\n - ê°œì¸ ì»´í“¨í„°ì—ì„œ QLoRAì™€ TRLì„ ì‚¬ìš©í•˜ì—¬ Llama 2 ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ [ë…¸íŠ¸ë¶](https://colab.research.google.com/drive/1SYpgFpcmtIUzdE7pxqknrM4ArCASfkFQ?usp=sharing)ì…ë‹ˆë‹¤. ğŸŒ\n \n âš¡ï¸ ì¶”ë¡ \n-- AutoGPTQ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ GPTQë¥¼ ì‚¬ìš©í•˜ì—¬ Llama 2 ëª¨ë¸ì„ ì–‘ìí™”í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ [ë…¸íŠ¸ë¶](https://colab.research.google.com/drive/1TC56ArKerXUpbgRy5vM3woRsbTEVNq7h?usp=sharing)ì…ë‹ˆë‹¤. ğŸŒ\n+- GPT-QModel ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ GPTQë¥¼ ì‚¬ìš©í•˜ì—¬ Llama 2 ëª¨ë¸ì„ ì–‘ìí™”í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ [ë…¸íŠ¸ë¶](https://colab.research.google.com/drive/1TC56ArKerXUpbgRy5vM3woRsbTEVNq7h?usp=sharing)ì…ë‹ˆë‹¤. ğŸŒ\n - ë¡œì»¬ ì»´í“¨í„°ë‚˜ Google Colabì—ì„œ 4-bit ì–‘ìí™”ë¡œ Llama 2 ì±„íŒ… ëª¨ë¸ì„ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ [ë…¸íŠ¸ë¶](https://colab.research.google.com/drive/1X1z9Q6domMKl2CnEM0QGHNwidLfR4dW2?usp=sharing)ì…ë‹ˆë‹¤. ğŸŒ\n \n ğŸš€ ë°°í¬"
        },
        {
            "sha": "8c18b6cf2215e1d476e6bb42efeee9735ea2514b",
            "filename": "docs/source/ko/quantization/gptq.md",
            "status": "modified",
            "additions": 3,
            "deletions": 30,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fko%2Fquantization%2Fgptq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fko%2Fquantization%2Fgptq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fquantization%2Fgptq.md?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -22,12 +22,12 @@ PEFTë¥¼ í™œìš©í•œ GPTQ ì–‘ìí™”ë¥¼ ì‚¬ìš©í•´ë³´ì‹œë ¤ë©´ ì´ [ë…¸íŠ¸ë¶](https:\n \n </Tip>\n \n-[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” GPTQ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ì´ëŠ” í›ˆë ¨ í›„ ì–‘ìí™” ê¸°ë²•ìœ¼ë¡œ, ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ ê° í–‰ì„ ë…ë¦½ì ìœ¼ë¡œ ì–‘ìí™”í•˜ì—¬ ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê°€ì¤‘ì¹˜ ë²„ì „ì„ ì°¾ìŠµë‹ˆë‹¤. ì´ ê°€ì¤‘ì¹˜ëŠ” int4ë¡œ ì–‘ìí™”ë˜ì§€ë§Œ, ì¶”ë¡  ì¤‘ì—ëŠ” ì‹¤ì‹œê°„ìœ¼ë¡œ fp16ìœ¼ë¡œ ë³µì›ë©ë‹ˆë‹¤. ì´ëŠ” int4 ê°€ì¤‘ì¹˜ê°€ GPUì˜ ì „ì—­ ë©”ëª¨ë¦¬ ëŒ€ì‹  ê²°í•©ëœ ì»¤ë„ì—ì„œ ì—­ì–‘ìí™”ë˜ê¸° ë•Œë¬¸ì— ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ 4ë°° ì ˆì•½í•  ìˆ˜ ìˆìœ¼ë©°, ë” ë‚®ì€ ë¹„íŠ¸ ë„ˆë¹„ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ í†µì‹  ì‹œê°„ì´ ì¤„ì–´ë“¤ì–´ ì¶”ë¡  ì†ë„ê°€ ë¹¨ë¼ì§ˆ ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+[GPT-QModel](https://github.com/ModelCloud/GPTQModel) ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” GPTQ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ì´ëŠ” í›ˆë ¨ í›„ ì–‘ìí™” ê¸°ë²•ìœ¼ë¡œ, ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ ê° í–‰ì„ ë…ë¦½ì ìœ¼ë¡œ ì–‘ìí™”í•˜ì—¬ ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê°€ì¤‘ì¹˜ ë²„ì „ì„ ì°¾ìŠµë‹ˆë‹¤. ì´ ê°€ì¤‘ì¹˜ëŠ” int4ë¡œ ì–‘ìí™”ë˜ì§€ë§Œ, ì¶”ë¡  ì¤‘ì—ëŠ” ì‹¤ì‹œê°„ìœ¼ë¡œ fp16ìœ¼ë¡œ ë³µì›ë©ë‹ˆë‹¤. ì´ëŠ” int4 ê°€ì¤‘ì¹˜ê°€ GPUì˜ ì „ì—­ ë©”ëª¨ë¦¬ ëŒ€ì‹  ê²°í•©ëœ ì»¤ë„ì—ì„œ ì—­ì–‘ìí™”ë˜ê¸° ë•Œë¬¸ì— ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ 4ë°° ì ˆì•½í•  ìˆ˜ ìˆìœ¼ë©°, ë” ë‚®ì€ ë¹„íŠ¸ ë„ˆë¹„ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ í†µì‹  ì‹œê°„ì´ ì¤„ì–´ë“¤ì–´ ì¶”ë¡  ì†ë„ê°€ ë¹¨ë¼ì§ˆ ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n \n ì‹œì‘í•˜ê¸° ì „ì— ë‹¤ìŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:\n \n ```bash\n-pip install auto-gptq\n+pip install gptqmodel --no-build-isolation\n pip install --upgrade accelerate optimum transformers\n ```\n \n@@ -44,7 +44,7 @@ gptq_config = GPTQConfig(bits=4, dataset=\"c4\", tokenizer=tokenizer)\n ìì‹ ì˜ ë°ì´í„°ì…‹ì„ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì „ë‹¬í•  ìˆ˜ë„ ìˆì§€ë§Œ, GPTQ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ ë™ì¼í•œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê°•ë ¥íˆ ê¶Œì¥í•©ë‹ˆë‹¤.\n \n ```py\n-dataset = [\"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"]\n+dataset = [\"gptqmodel is an easy-to-use model quantization library with user-friendly apis, based on the GPTQ algorithm.\"]\n gptq_config = GPTQConfig(bits=4, dataset=dataset, tokenizer=tokenizer)\n ```\n \n@@ -91,30 +91,3 @@ from transformers import AutoModelForCausalLM\n \n model = AutoModelForCausalLM.from_pretrained(\"{your_username}/opt-125m-gptq\", device_map=\"auto\")\n ```\n-\n-## ExLlama [[exllama]]\n-\n-[ExLlama](https://github.com/turboderp/exllama)ì€ [Llama](model_doc/llama) ëª¨ë¸ì˜ Python/C++/CUDA êµ¬í˜„ì²´ë¡œ, 4ë¹„íŠ¸ GPTQ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ë¹ ë¥¸ ì¶”ë¡ ì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤(ì´ [ë²¤ì¹˜ë§ˆí¬](https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”). ['GPTQConfig'] ê°ì²´ë¥¼ ìƒì„±í•  ë•Œ ExLlama ì»¤ë„ì´ ê¸°ë³¸ì ìœ¼ë¡œ í™œì„±í™”ë©ë‹ˆë‹¤. ì¶”ë¡  ì†ë„ë¥¼ ë”ìš± ë†’ì´ê¸° ìœ„í•´, `exllama_config` ë§¤ê°œë³€ìˆ˜ë¥¼ êµ¬ì„±í•˜ì—¬ [ExLlamaV2](https://github.com/turboderp/exllamav2) ì»¤ë„ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n-\n-```py\n-import torch\n-from transformers import AutoModelForCausalLM, GPTQConfig\n-\n-gptq_config = GPTQConfig(bits=4, exllama_config={\"version\":2})\n-model = AutoModelForCausalLM.from_pretrained(\"{your_username}/opt-125m-gptq\", device_map=\"auto\", quantization_config=gptq_config)\n-```\n-\n-<Tip warning={true}>\n-\n-4ë¹„íŠ¸ ëª¨ë¸ë§Œ ì§€ì›ë˜ë©°, ì–‘ìí™”ëœ ëª¨ë¸ì„ PEFTë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²½ìš° ExLlama ì»¤ë„ì„ ë¹„í™œì„±í™”í•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n-\n-</Tip>\n-\n-ExLlama ì»¤ë„ì€ ì „ì²´ ëª¨ë¸ì´ GPUì— ìˆì„ ë•Œë§Œ ì§€ì›ë©ë‹ˆë‹¤. AutoGPTQ(ë²„ì „ 0.4.2 ì´ìƒ)ë¡œ CPUì—ì„œ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” ê²½ìš° ExLlama ì»¤ë„ì„ ë¹„í™œì„±í™”í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ config.json íŒŒì¼ì˜ ì–‘ìí™” ì„¤ì •ì—ì„œ ExLlama ì»¤ë„ê³¼ ê´€ë ¨ëœ ì†ì„±ì„ ë®ì–´ì¨ì•¼ í•©ë‹ˆë‹¤.\n-\n-```py\n-import torch\n-from transformers import AutoModelForCausalLM, GPTQConfig\n-gptq_config = GPTQConfig(bits=4, use_exllama=False)\n-model = AutoModelForCausalLM.from_pretrained(\"{your_username}/opt-125m-gptq\", device_map=\"cpu\", quantization_config=gptq_config)\n-```\n\\ No newline at end of file"
        },
        {
            "sha": "02e219155afea0a5cd9be92265b8d693a2869bda",
            "filename": "docs/source/zh/llm_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fzh%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fzh%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fllm_tutorial.md?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -261,7 +261,7 @@ LLMsæ˜¯[ä»…è§£ç å™¨](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)\n ### å»¶è¿Ÿã€ååé‡å’Œå†…å­˜åˆ©ç”¨ç‡\n \n 1. [æŒ‡å—](llm_tutorial_optimization),å¦‚ä½•ä¼˜åŒ–LLMsä»¥æé«˜é€Ÿåº¦å’Œå†…å­˜åˆ©ç”¨ï¼›\n-2. [æŒ‡å—](main_classes/quantization), å…³äº`quantization`ï¼Œå¦‚bitsandbyteså’Œautogptqçš„æŒ‡å—ï¼Œæ•™æ‚¨å¦‚ä½•å¤§å¹…é™ä½å†…å­˜éœ€æ±‚ã€‚\n+2. [æŒ‡å—](main_classes/quantization), å…³äº`quantization`ï¼Œå¦‚bitsandbyteså’ŒGPT-QModelçš„æŒ‡å—ï¼Œæ•™æ‚¨å¦‚ä½•å¤§å¹…é™ä½å†…å­˜éœ€æ±‚ã€‚\n \n ### ç›¸å…³åº“\n "
        },
        {
            "sha": "e0122e3a9bdd4a3912bca289e4d61623e14d442c",
            "filename": "docs/source/zh/main_classes/quantization.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fzh%2Fmain_classes%2Fquantization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/docs%2Fsource%2Fzh%2Fmain_classes%2Fquantization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fquantization.md?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -113,22 +113,22 @@ model = AutoModelForCausalLM.from_pretrained(\"TheBloke/zephyr-7B-alpha-AWQ\", att\n \n [[autodoc]] AwqConfig\n \n-## `AutoGPTQ` é›†æˆ\n+## GPT-QModel é›†æˆ\n \n ğŸ¤— Transformerså·²ç»æ•´åˆäº†`optimum` APIï¼Œç”¨äºå¯¹è¯­è¨€æ¨¡å‹æ‰§è¡ŒGPTQé‡åŒ–ã€‚æ‚¨å¯ä»¥ä»¥8ã€4ã€3ç”šè‡³2ä½åŠ è½½å’Œé‡åŒ–æ‚¨çš„æ¨¡å‹ï¼Œè€Œæ€§èƒ½æ— æ˜æ˜¾ä¸‹é™ï¼Œå¹¶ä¸”æ¨ç†é€Ÿåº¦æ›´å¿«ï¼è¿™å—åˆ°å¤§å¤šæ•°GPUç¡¬ä»¶çš„æ”¯æŒã€‚\n \n è¦äº†è§£æ›´å¤šå…³äºé‡åŒ–æ¨¡å‹çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ï¼š\n - [GPTQ](https://huggingface.co/papers/2210.17323)è®ºæ–‡\n - `optimum`å…³äºGPTQé‡åŒ–çš„[æŒ‡å—](https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization)\n-- ç”¨ä½œåç«¯çš„[`AutoGPTQ`](https://github.com/PanQiWei/AutoGPTQ)åº“\n+- ç”¨ä½œåç«¯çš„`GPT-QModel` (https://github.com/ModelCloud/GPTQModel)åº“\n \n \n ### è¦æ±‚\n \n ä¸ºäº†è¿è¡Œä¸‹é¢çš„ä»£ç ï¼Œæ‚¨éœ€è¦å®‰è£…ï¼š\n \n-- å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ `AutoGPTQ` åº“\n-`pip install auto-gptq`\n+- å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ `GPT-QModel` åº“\n+`pip install gptqmodel --no-build-isolation`\n \n - ä»æºä»£ç å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„`optimum`\n `pip install git+https://github.com/huggingface/optimum.git`\n@@ -162,7 +162,7 @@ gptq_config = GPTQConfig(bits=4, dataset = \"c4\", tokenizer=tokenizer)\n \n \n ```python\n-dataset = [\"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"]\n+dataset = [\"gptqmodel is an easy-to-use model quantization library with user-friendly apis, based on the GPTQ algorithm.\"]\n quantization = GPTQConfig(bits=4, dataset = dataset, tokenizer=tokenizer)\n ```\n "
        },
        {
            "sha": "29fab55080e2364e376474b10b094e19295d639a",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -19,7 +19,6 @@\n _import_structure = {\n     \"aqlm\": [\"replace_with_aqlm_linear\"],\n     \"awq\": [\n-        \"fuse_awq_modules\",\n         \"post_init_awq_exllama_modules\",\n         \"post_init_awq_ipex_modules\",\n         \"replace_quantization_scales\",\n@@ -167,7 +166,6 @@\n if TYPE_CHECKING:\n     from .aqlm import replace_with_aqlm_linear\n     from .awq import (\n-        fuse_awq_modules,\n         post_init_awq_exllama_modules,\n         post_init_awq_ipex_modules,\n         replace_quantization_scales,"
        },
        {
            "sha": "5c9374af09d2be0b87c97045e3c8e0fa0f48e2a8",
            "filename": "src/transformers/integrations/awq.py",
            "status": "modified",
            "additions": 46,
            "deletions": 456,
            "changes": 502,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fawq.py?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -13,19 +13,11 @@\n # limitations under the License.\n \"AWQ (Activation aware Weight Quantization) integration file\"\n \n-import importlib\n+from typing import Optional, Union\n \n-from packaging import version\n-\n-from ..activations import ACT2FN\n-from ..modeling_rope_utils import ROPE_INIT_FUNCTIONS\n-from ..modeling_utils import PreTrainedModel\n-from ..utils import is_auto_awq_available, is_ipex_available, is_torch_available, logging\n+from ..utils import is_gptqmodel_available, is_llm_awq_available, is_torch_available, logging\n from ..utils.quantization_config import (\n-    AwqBackendPackingMethod,\n-    AwqConfig,\n-    AWQLinearVersion,\n-    ExllamaVersion,\n+    AwqBackend,\n )\n \n \n@@ -35,44 +27,6 @@\n \n logger = logging.get_logger(__name__)\n \n-AWQ_FUSED_MAPPINGS = {\n-    \"mistral\": {\n-        \"attention\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n-        \"mlp\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n-        \"layernorm\": [\"input_layernorm\", \"post_attention_layernorm\", \"norm\"],\n-        \"use_alibi\": False,\n-    },\n-    \"mixtral\": {\n-        \"attention\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n-        \"mlp\": [\"w1\", \"w3\", \"w2\"],\n-        \"layernorm\": [\"input_layernorm\", \"post_attention_layernorm\", \"norm\"],\n-        \"use_alibi\": False,\n-    },\n-    \"llama\": {\n-        \"attention\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n-        \"mlp\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n-        \"layernorm\": [\"input_layernorm\", \"post_attention_layernorm\", \"norm\"],\n-        \"use_alibi\": False,\n-    },\n-    \"llava\": {\n-        \"attention\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n-        \"mlp\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n-        \"layernorm\": [\"input_layernorm\", \"post_attention_layernorm\", \"norm\"],\n-        \"use_alibi\": False,\n-    },\n-    \"qwen2\": {\n-        \"attention\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n-        \"mlp\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n-        \"layernorm\": [\"input_layernorm\", \"post_attention_layernorm\", \"norm\"],\n-        \"use_alibi\": False,\n-    },\n-    \"qwen3\": {\n-        \"attention\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"q_norm\", \"k_norm\"],\n-        \"mlp\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n-        \"layernorm\": [\"input_layernorm\", \"post_attention_layernorm\", \"norm\"],\n-        \"use_alibi\": False,\n-    },\n-}\n \n AWQ_SCALES_MAPPINGS = {\n     \"starcoder2\": {\"act\": \"act\", \"layer_before_act\": \"c_fc\"},\n@@ -86,55 +40,8 @@\n }\n \n \n-if is_auto_awq_available():\n-    from awq.modules.fused.attn import RoPE\n-\n-    class AWQRoPE(RoPE):\n-        \"\"\"\n-        AWQRoPE module for hacking rope implementation in AWQ fused attention modules to support more models.\n-\n-        Args:\n-            rope_type (`str`):\n-                The rope type to use.\n-            head_dim (`int`):\n-                The head dimension.\n-            max_seq_len (`int`):\n-                The maximum sequence length.\n-            config (`PreTrainedConfig`):\n-                The model config object.\n-            device (`torch.device`):\n-                The device to put the module on.\n-        \"\"\"\n-\n-        def __init__(self, rope_type, head_dim, max_seq_len, config, device):\n-            rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]\n-            self.inv_freq, self.attention_scaling = rope_init_fn(config, device)\n-            # Use fake rope_theta to initialize the parent class\n-            super().__init__(head_dim=head_dim, max_seq_len=max_seq_len, device=device, rope_theta=-1)\n-\n-        def precompute_freqs_cis(self, dim: int, end: int, theta=-1):\n-            t = torch.arange(end, device=self.inv_freq.device)\n-            freqs = torch.outer(t, self.inv_freq).float()\n-            freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n-            del self.inv_freq  # free the memory\n-            return freqs_cis\n-\n-        def forward(\n-            self,\n-            xq: torch.Tensor,\n-            xk: torch.Tensor,\n-            start_pos: int,\n-            seqlen: int,\n-            partial: bool = False,\n-        ):\n-            xq_out, xk_out = super().forward(xq, xk, start_pos, seqlen, partial)\n-            xq_out = (xq_out * self.attention_scaling).type_as(xq)\n-            xk_out = (xk_out * self.attention_scaling).type_as(xk)\n-            return xq_out, xk_out\n-\n-\n def replace_quantization_scales(model, model_type):\n-    from awq.modules.act import ScaledActivation\n+    from gptqmodel.quantization.awq.modules.act import ScaledActivation\n \n     if model_type not in AWQ_SCALES_MAPPINGS:\n         return model\n@@ -156,6 +63,7 @@ def replace_with_awq_linear(\n     quantization_config=None,\n     current_key_name=None,\n     has_been_replaced=False,\n+    device_map: Optional[Union[str, dict]] = None,\n ) -> bool:\n     \"\"\"\n     Public method that recursively replaces the Linear layers of the given model with AWQ quantized layers.\n@@ -183,37 +91,27 @@ def replace_with_awq_linear(\n \n     backend = quantization_config.backend\n \n-    if not is_auto_awq_available():\n+    if not is_gptqmodel_available() and not is_llm_awq_available():\n         raise ValueError(\n-            \"AWQ (either `autoawq` or `llmawq`) is not available. Please install it with `pip install autoawq` or check out the installation guide in https://github.com/mit-han-lab/llm-awq\"\n+            \"AWQ (either `llmawq`) is not available. Please install it with `pip install gptqmodel` or check out the installation guide in https://github.com/mit-han-lab/llm-awq\"\n         )\n \n-    if backend == AwqBackendPackingMethod.AUTOAWQ:\n-        if quantization_config.version == AWQLinearVersion.GEMM:\n-            from awq.modules.linear.gemm import WQLinear_GEMM\n-\n-            target_cls = WQLinear_GEMM\n-        elif quantization_config.version == AWQLinearVersion.GEMV:\n-            from awq.modules.linear.gemv import WQLinear_GEMV\n-\n-            target_cls = WQLinear_GEMV\n-        elif quantization_config.version == AWQLinearVersion.EXLLAMA:\n-            if quantization_config.exllama_config[\"version\"] == ExllamaVersion.ONE:\n-                from awq.modules.linear.exllama import WQLinear_Exllama\n-\n-                target_cls = WQLinear_Exllama\n-            elif quantization_config.exllama_config[\"version\"] == ExllamaVersion.TWO:\n-                from awq.modules.linear.exllamav2 import WQLinear_ExllamaV2\n-\n-                target_cls = WQLinear_ExllamaV2\n-            else:\n-                raise ValueError(f\"Unrecognized Exllama version: {quantization_config.exllama_config['version']}\")\n-        elif quantization_config.version == AWQLinearVersion.IPEX:\n-            from awq.modules.linear.gemm_ipex import WQLinear_IPEX\n-\n-            target_cls = WQLinear_IPEX\n-        else:\n-            raise ValueError(f\"Unrecognized AWQ version: {quantization_config.version}\")\n+    if backend != AwqBackend.LLMAWQ:\n+        from gptqmodel.quantization import METHOD\n+        from gptqmodel.utils.importer import hf_select_quant_linear_v2\n+\n+        target_cls = hf_select_quant_linear_v2(\n+            bits=quantization_config.bits,\n+            group_size=quantization_config.group_size,\n+            desc_act=False,\n+            sym=False,\n+            format=quantization_config.format,\n+            backend=quantization_config.backend,\n+            device_map=device_map,\n+            quant_method=METHOD.AWQ,\n+            zero_point=quantization_config.zero_point,\n+            pack=False,\n+        )\n     else:\n         from awq.quantize.qmodule import WQLinear\n \n@@ -230,14 +128,27 @@ def replace_with_awq_linear(\n                 in_features = module.in_features\n                 out_features = module.out_features\n \n-                model._modules[name] = target_cls(\n-                    w_bit=quantization_config.bits,\n-                    group_size=quantization_config.group_size,\n-                    in_features=in_features,\n-                    out_features=out_features,\n-                    bias=module.bias is not None,\n-                    dev=module.weight.device,\n-                )\n+                if backend != AwqBackend.LLMAWQ:\n+                    model._modules[name] = target_cls(\n+                        bits=quantization_config.bits,\n+                        sym=quantization_config.sym,\n+                        desc_act=quantization_config.desc_act,\n+                        group_size=quantization_config.group_size,\n+                        in_features=in_features,\n+                        out_features=out_features,\n+                        bias=module.bias is not None,\n+                        dev=module.weight.device,\n+                        register_buffers=True,\n+                    )\n+                else:\n+                    model._modules[name] = target_cls(\n+                        w_bit=quantization_config.bits,\n+                        group_size=quantization_config.group_size,\n+                        in_features=in_features,\n+                        out_features=out_features,\n+                        bias=module.bias is not None,\n+                        dev=module.weight.device,\n+                    )\n                 has_been_replaced = True\n \n                 # Force requires grad to False to avoid unexpected errors\n@@ -249,341 +160,20 @@ def replace_with_awq_linear(\n                 current_key_name=current_key_name,\n                 quantization_config=quantization_config,\n                 has_been_replaced=has_been_replaced,\n+                device_map=device_map,\n             )\n         # Remove the last key for recursion\n         current_key_name.pop(-1)\n     return model, has_been_replaced\n \n \n-def get_modules_to_fuse(model, quantization_config):\n-    \"\"\"\n-    Returns the fusing mapping given the quantization config and the model\n-\n-    Args:\n-        model (`~PreTrainedModel`):\n-            The model to fuse - note this model should have been converted into AWQ format beforehand.\n-        quantization_config (`~transformers.quantization_config.AWQConfig`):\n-            The quantization configuration to use.\n-    \"\"\"\n-    if not isinstance(model, PreTrainedModel):\n-        raise TypeError(f\"The model should be an instance of `PreTrainedModel`, got {model.__class__.__name__}\")\n-\n-    # Always default to `quantization_config.modules_to_fuse`\n-    if quantization_config.modules_to_fuse is not None:\n-        current_fused_mapping = quantization_config.modules_to_fuse\n-        current_fused_mapping[\"max_seq_len\"] = quantization_config.fuse_max_seq_len\n-    elif model.config.model_type in AWQ_FUSED_MAPPINGS:\n-        current_fused_mapping = AWQ_FUSED_MAPPINGS[model.config.model_type]\n-\n-        # Properly deal with the case where we have a multi-modal model as well (e.g. Llava)\n-        config = model.config.get_text_config(decoder=True)\n-\n-        # Handle hidden_size, num_attention_heads, num_key_value_heads, rope_parameters on our own.\n-        hidden_size = config.hidden_size\n-        num_attention_heads = config.num_attention_heads\n-        num_key_value_heads = getattr(config, \"num_key_value_heads\", num_attention_heads)\n-        rope_parameters = config.rope_parameters\n-\n-        # Fill `current_fused_mapping` with the expected values\n-        current_fused_mapping[\"hidden_size\"] = hidden_size\n-        current_fused_mapping[\"num_attention_heads\"] = num_attention_heads\n-        current_fused_mapping[\"num_key_value_heads\"] = num_key_value_heads\n-        current_fused_mapping[\"rope_parameters\"] = rope_parameters\n-        current_fused_mapping[\"max_seq_len\"] = quantization_config.fuse_max_seq_len\n-    else:\n-        raise ValueError(\n-            \"Fusing mapping not found either on the quantization config or the supported `AWQ_FUSED_MAPPINGS`. Please pass a `fused_mapping` argument\"\n-            \" in the `quantization_config` or raise an issue on transformers https://github.com/huggingface/transformers to add its support.\"\n-        )\n-    return current_fused_mapping\n-\n-\n-def fuse_awq_modules(model, quantization_config):\n-    \"\"\"\n-    Optionally fuse some modules in the model to speedup inference.\n-\n-    Args:\n-        model (`~PreTrainedModel`):\n-            The model to fuse - note this model should have been converted into AWQ format beforehand.\n-        quantization_config (`Union[AwqConfig, dict]`):\n-            The quantization configuration to use.\n-    \"\"\"\n-    # We need to convert it from dict in order to get an AwqConfig object\n-    # otherwise the fields `backend` etc. will not be available\n-    # https://github.com/huggingface/transformers/pull/27411#discussion_r1414044495\n-    if isinstance(quantization_config, dict):\n-        quantization_config = AwqConfig.from_dict(quantization_config)\n-    backend = quantization_config.backend\n-\n-    modules_to_fuse = get_modules_to_fuse(model, quantization_config)\n-    modules_to_not_convert = getattr(quantization_config, \"modules_to_not_convert\", None)\n-\n-    if backend == AwqBackendPackingMethod.AUTOAWQ:\n-        from awq.modules.fused.attn import QuantAttentionFused\n-        from awq.modules.fused.mlp import QuantFusedMLP\n-        from awq.modules.fused.norm import FasterTransformerRMSNorm\n-\n-        # Hack QuantAttentionFused to modify the return value of forward function to avoid returning past_key_value\n-        old_quant_attention_fused_forward = QuantAttentionFused.forward\n-\n-        def new_quant_attention_fused_forward(self, *args, **kwargs):\n-            attn_output, attention_weight, _ = old_quant_attention_fused_forward(self, *args, **kwargs)\n-            return attn_output, attention_weight\n-\n-        QuantAttentionFused.forward = new_quant_attention_fused_forward\n-    else:\n-        raise ValueError(\"Fusing is only supported for the AutoAWQ backend\")\n-\n-    fused_attention_modules = []\n-\n-    for name, module in model.named_modules():\n-        if modules_to_not_convert is not None:\n-            if any(module_name_to_not_convert in name for module_name_to_not_convert in modules_to_not_convert):\n-                continue\n-\n-        # Replace layer norms\n-        _fuse_awq_layernorm(modules_to_fuse[\"layernorm\"], module, FasterTransformerRMSNorm)\n-\n-        # Replace MLP layers if awq version is not ipex.\n-        if quantization_config.version != \"ipex\":\n-            _fuse_awq_mlp(model, name, modules_to_fuse[\"mlp\"], module, QuantFusedMLP)\n-        else:\n-            logger.info(\"The IPEX version AWQ does not support fuse mlp for now.\")\n-\n-        # Replace attention layers\n-        attention_has_been_fused = _fuse_awq_attention_layers(\n-            model, module, modules_to_fuse, name, QuantAttentionFused\n-        )\n-\n-        if attention_has_been_fused:\n-            fused_attention_modules.append(name.split(\".\")[0])\n-\n-    # For AWQ fused + Llama we need to set `config._attn_implementation` = \"custom\" to avoid unexpected behavior and pass\n-    # `None` attention mask to the fused attention modules as now the attention mask is dropped by our models and dealt\n-    # by the `AttentionMaskConverter` module.\n-    if len(fused_attention_modules) > 0:\n-        for module_name, module in model.named_modules():\n-            if any(\n-                module_name in fused_attention_modules for fused_attention_parent_module in fused_attention_modules\n-            ):\n-                if hasattr(module, \"config\") and hasattr(module.config, \"_attn_implementation\"):\n-                    module.config._attn_implementation = \"custom\"\n-    return model\n-\n-\n-def _fuse_awq_layernorm(fuse_module_names, module, target_cls):\n-    \"\"\"\n-    Fuse the LayerNorm layers into a target class using autoawq\n-\n-    Args:\n-        fuse_module_names (`list[str]`):\n-            The list of module names to fuse\n-        module (`nn.Module`):\n-            The pytorch parent module that has layernorm modules to fuse\n-        target_cls (`~autoawq.FasterTransformerRMSNorm`):\n-            The `FasterTransformerRMSNorm` class as it only supports that class\n-            for now.\n-    \"\"\"\n-    for module_name in fuse_module_names:\n-        if hasattr(module, module_name):\n-            old_module = getattr(module, module_name)\n-            module._modules[module_name] = target_cls(\n-                old_module.weight,\n-                old_module.variance_epsilon,\n-            ).to(old_module.weight.device)\n-            del old_module\n-\n-\n-def _fuse_awq_mlp(model, current_module_name, fuse_module_names, module, target_cls):\n-    \"\"\"\n-    Fuse the MLP layers into a target class using autoawq\n-\n-    Args:\n-        model (`~PreTrainedModel`):\n-            The input pretrained model\n-        current_module_name (`str`):\n-            The current submodule name\n-        fuse_module_names (`list[str]`):\n-            The list of module names to fuse. For the MLP layers it has to be an array\n-            of length 3 that consists of the 3 MLP layers in the order (gate (dense layer post-attention) / up / down layers)\n-        module (`nn.Module`):\n-            The pytorch parent module that has layernorm modules to fuse\n-        target_cls (`~autoawq.QuantFusedMLP`):\n-            The `QuantFusedMLP` class as it only supports that class\n-            for now.\n-    \"\"\"\n-    if len(fuse_module_names) == 0:\n-        return\n-\n-    if hasattr(module, fuse_module_names[0]):\n-        gate_proj = getattr(module, fuse_module_names[0])\n-        up_proj = getattr(module, fuse_module_names[1])\n-        down_proj = getattr(module, fuse_module_names[2])\n-\n-        previous_device = gate_proj.qweight.device\n-\n-        # Deal also with the case model has `text_config` attribute\n-        config = model.config.get_text_config(decoder=True)\n-        hidden_act = config.hidden_act\n-        activation_fn = ACT2FN[hidden_act]\n-        new_module = target_cls(gate_proj, down_proj, up_proj, activation_fn)\n-\n-        parent_name, child_name = current_module_name.rsplit(\".\", 1)\n-        parent = model.get_submodule(parent_name)\n-        setattr(parent, child_name, new_module.to(previous_device))\n-\n-        del gate_proj, up_proj, down_proj\n-\n-\n-def _fuse_awq_attention_layers(model, module, modules_to_fuse, current_module_name, target_cls):\n-    \"\"\"\n-    Fuse the Attention layers into a target class using autoawq\n-\n-    Args:\n-        model (`~PreTrainedModel`):\n-            The input pretrained model\n-        module (`nn.Module`):\n-            The pytorch parent module that has layernorm modules to fuse\n-        modules_to_fuse (`list[str]`):\n-            The module fusing mapping. The dictionary has to contain a field `attention` with attention module names\n-            in the correct order: q, k, v, o layer, (q_norm, k_norm) optional\n-        current_module_name (`str`):\n-            The current submodule name\n-        target_cls (`~autoawq.QuantAttentionFused`):\n-            The `QuantAttentionFused` class as it only supports that class\n-            for now.\n-    \"\"\"\n-    from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV\n-\n-    module_has_been_fused = False\n-\n-    if len(modules_to_fuse[\"attention\"]) == 0:\n-        return module_has_been_fused\n-\n-    if hasattr(module, modules_to_fuse[\"attention\"][0]):\n-        # First, we pack the QKV layers together\n-        q_proj = getattr(module, modules_to_fuse[\"attention\"][0])\n-\n-        if isinstance(q_proj, WQLinear_GEMV):\n-            linear_target_cls = WQLinear_GEMV\n-            cat_dim = 0\n-        elif isinstance(q_proj, WQLinear_GEMM):\n-            linear_target_cls = WQLinear_GEMM\n-            cat_dim = 1\n-        elif is_ipex_available() and version.parse(importlib.metadata.version(\"autoawq\")) > version.parse(\"0.2.6\"):\n-            from awq.modules.linear import WQLinear_IPEX\n-\n-            if isinstance(q_proj, WQLinear_IPEX):\n-                linear_target_cls = WQLinear_IPEX\n-                cat_dim = 1\n-        else:\n-            raise ValueError(\"Unsupported q_proj type: {type(q_proj)}\")\n-\n-        previous_device = q_proj.qweight.device\n-\n-        k_proj = getattr(module, modules_to_fuse[\"attention\"][1])\n-        v_proj = getattr(module, modules_to_fuse[\"attention\"][2])\n-        o_proj = getattr(module, modules_to_fuse[\"attention\"][3])\n-\n-        # maybe there are q_norm and k_norm layers\n-        if len(modules_to_fuse[\"attention\"]) > 4:\n-            q_norm = getattr(module, modules_to_fuse[\"attention\"][4])\n-            k_norm = getattr(module, modules_to_fuse[\"attention\"][5])\n-        else:\n-            q_norm = None\n-            k_norm = None\n-\n-        bias = torch.cat([q_proj.bias, k_proj.bias, v_proj.bias], dim=0) if q_proj.bias is not None else None\n-\n-        qkv_layer = linear_target_cls(\n-            q_proj.w_bit,\n-            q_proj.group_size,\n-            q_proj.in_features,\n-            q_proj.out_features + k_proj.out_features + v_proj.out_features,\n-            q_proj.bias is not None,\n-            next(iter(module.state_dict().values())).device,\n-        )\n-\n-        qkv_layer.qweight = torch.cat([q_proj.qweight, k_proj.qweight, v_proj.qweight], dim=cat_dim)\n-        qkv_layer.qzeros = torch.cat([q_proj.qzeros, k_proj.qzeros, v_proj.qzeros], dim=cat_dim)\n-        qkv_layer.scales = torch.cat([q_proj.scales, k_proj.scales, v_proj.scales], dim=cat_dim)\n-\n-        if isinstance(qkv_layer, WQLinear_GEMV):\n-            qkv_layer.split_k_iters = q_proj.split_k_iters\n-\n-        qkv_layer.bias = bias\n-\n-        fused_attention_layer = target_cls(\n-            modules_to_fuse[\"hidden_size\"],\n-            modules_to_fuse[\"num_attention_heads\"],\n-            modules_to_fuse[\"num_key_value_heads\"],\n-            qkv_layer,\n-            o_proj,\n-            previous_device,\n-            modules_to_fuse[\"max_seq_len\"],\n-            use_alibi=modules_to_fuse[\"use_alibi\"],\n-            # The default value in autoawq is set to 10000.0\n-            rope_theta=modules_to_fuse[\"rope_parameters\"].get(\"rope_theta\", 10000.0),\n-            q_norm=q_norm,\n-            k_norm=k_norm,\n-        )\n-\n-        # Hack the rope module if not using alibi and rope_type is not default\n-        # As the default rope implementation in autoawq only supports the \"default\" rope type\n-        rope_type = modules_to_fuse[\"rope_parameters\"].get(\"rope_type\", \"default\")\n-        if not modules_to_fuse[\"use_alibi\"] and rope_type != \"default\":\n-            fused_attention_layer.rope = AWQRoPE(\n-                rope_type,\n-                modules_to_fuse[\"hidden_size\"] // modules_to_fuse[\"num_attention_heads\"],\n-                modules_to_fuse[\"max_seq_len\"],\n-                model.config.get_text_config(decoder=True),\n-                previous_device,\n-            )\n-\n-        fused_attention_layer.is_hf_transformers = True\n-\n-        parent_name, child_name = current_module_name.rsplit(\".\", 1)\n-        parent = model.get_submodule(parent_name)\n-        setattr(parent, child_name, fused_attention_layer.to(previous_device))\n-\n-        del q_proj, k_proj, v_proj, o_proj, q_norm, k_norm\n-        module_has_been_fused = True\n-\n-    return module_has_been_fused\n-\n-\n-def post_init_awq_exllama_modules(model, exllama_config):\n-    \"\"\"\n-    Runs post init for Exllama layers which performs:\n-        - Weights unpacking, reordering and repacking\n-        - Devices scratch space allocation\n-    \"\"\"\n-\n-    if exllama_config[\"version\"] == ExllamaVersion.ONE:\n-        from awq.modules.linear.exllama import exllama_post_init\n-\n-        model = exllama_post_init(model)\n-    elif exllama_config[\"version\"] == ExllamaVersion.TWO:\n-        from awq.modules.linear.exllamav2 import exllamav2_post_init\n-\n-        model = exllamav2_post_init(\n-            model,\n-            max_input_len=exllama_config[\"max_input_len\"],\n-            max_batch_size=exllama_config[\"max_batch_size\"],\n-        )\n-    else:\n-        raise ValueError(f\"Unrecognized Exllama version: {exllama_config['version']}\")\n-\n-    return model\n-\n-\n def post_init_awq_ipex_modules(model):\n     \"\"\"\n     Runs post init for IPEX layers which performs:\n         - Weights packing, reordering and repacking\n     \"\"\"\n \n-    from awq.modules.linear.gemm_ipex import ipex_post_init\n+    from gptqmodel.quantization.awq.modules.linear.gemm_ipex import ipex_post_init\n \n     model = ipex_post_init(model)\n "
        },
        {
            "sha": "9b2d951518e251cc1df190528b30f87e5e97ae4e",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -41,7 +41,7 @@ def _assign_original_dtype(module, original_dtype):\n         _assign_original_dtype(child, original_dtype)\n \n \n-def get_keys_to_not_convert(model):\n+def get_keys_to_not_convert(model) -> list:\n     r\"\"\"\n     Function to automatically detect keys to not convert for usage like quantization. For example for CausalLM modules\n     we may want to keep the lm_head in full precision for numerical stability reasons."
        },
        {
            "sha": "af6c6147298361d6696c0984b987b1ddc183507a",
            "filename": "src/transformers/quantizers/quantizer_awq.py",
            "status": "modified",
            "additions": 13,
            "deletions": 71,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -22,8 +22,8 @@\n if TYPE_CHECKING:\n     from ..modeling_utils import PreTrainedModel\n \n-from ..utils import is_accelerate_available, is_auto_awq_available, is_torch_available, logging\n-from ..utils.quantization_config import AWQLinearVersion\n+from ..utils import is_accelerate_available, is_gptqmodel_available, is_torch_available, logging\n+from ..utils.quantization_config import AwqBackend\n \n \n if is_torch_available():\n@@ -44,54 +44,12 @@ def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n \n     def validate_environment(self, device_map, **kwargs):\n-        if not is_auto_awq_available():\n-            raise ImportError(\"Loading an AWQ quantized model requires auto-awq library (`pip install autoawq`)\")\n+        if not is_gptqmodel_available():\n+            raise ImportError(\"Loading an AWQ quantized model requires gptqmodel library (`pip install gptqmodel`)\")\n \n         if not is_accelerate_available():\n             raise ImportError(\"Loading an AWQ quantized model requires accelerate (`pip install accelerate`)\")\n \n-        if (\n-            self.quantization_config.version == AWQLinearVersion.GEMM\n-            and not torch.cuda.is_available()\n-            and not torch.xpu.is_available()\n-        ):\n-            logger.warning_once(\"No CUDA or XPU found, consider switching to the IPEX version for CPU-only execution.\")\n-            self.quantization_config.version = AWQLinearVersion.IPEX\n-\n-        if self.quantization_config.version == AWQLinearVersion.IPEX:\n-            if version.parse(importlib.metadata.version(\"autoawq\")) < version.parse(\"0.2.6\"):\n-                raise RuntimeError(\n-                    \"To use IPEX backend, you need autoawq>0.2.6. Please install the latest version or from source.\"\n-                )\n-            if device_map is None:\n-                logger.warning_once(\n-                    \"You have loaded an AWQ model without setting device_map, please set 'cpu' or 'xpu' or 'auto'\"\n-                )\n-            elif isinstance(device_map, dict) and \"disk\" in device_map.values():\n-                raise ValueError(\n-                    \"You are attempting to load an IPEX version AWQ model with a device_map that contains disk device.\"\n-                    \" This is not supported. Please make sure only cpu and xpu in the device_map.\"\n-                )\n-        else:\n-            if not torch.cuda.is_available() and not torch.xpu.is_available():\n-                raise RuntimeError(\n-                    \"GPU is required to run AWQ quantized model. You can use IPEX version AWQ if you have an Intel CPU\"\n-                )\n-\n-            if device_map is None:\n-                logger.warning_once(\n-                    \"You have loaded an AWQ model on CPU and have a CUDA/XPU device available, make sure to set \"\n-                    \"your model on a GPU device in order to run your model.\"\n-                )\n-            elif device_map is not None:\n-                if isinstance(device_map, dict) and any(\n-                    forbidden in device_map.values() for forbidden in (\"cpu\", torch.device(\"cpu\"), \"disk\")\n-                ):\n-                    raise ValueError(\n-                        \"You are attempting to load an AWQ model with a device_map that contains a CPU or disk device.\"\n-                        \" This is not supported. Please remove the CPU or disk device from the device_map.\"\n-                    )\n-\n     def update_dtype(self, dtype):\n         if dtype is None:\n             dtype = torch.float16\n@@ -115,7 +73,10 @@ def _process_model_before_weight_loading(\n         )\n \n         model, has_been_replaced = replace_with_awq_linear(\n-            model, quantization_config=self.quantization_config, modules_to_not_convert=self.modules_to_not_convert\n+            model,\n+            quantization_config=self.quantization_config,\n+            modules_to_not_convert=self.modules_to_not_convert,\n+            device_map=kwargs.get(\"device_map\"),\n         )\n \n         model = replace_quantization_scales(model, model.config.model_type)\n@@ -127,36 +88,17 @@ def _process_model_before_weight_loading(\n             )\n \n     def _process_model_after_weight_loading(self, model, **kwargs):\n-        if self.quantization_config.do_fuse:\n-            from ..integrations import fuse_awq_modules\n-\n-            model = fuse_awq_modules(model, self.quantization_config)\n-            model._awq_is_fused = True  # TODO: consider storing this flag in model.config instead\n-\n-        if self.quantization_config.version == AWQLinearVersion.EXLLAMA:\n-            from ..integrations import post_init_awq_exllama_modules\n-\n-            model = post_init_awq_exllama_modules(model, self.quantization_config.exllama_config)\n+        from gptqmodel.utils.model import hf_gptqmodel_post_init\n \n-        if self.quantization_config.version == AWQLinearVersion.IPEX:\n-            from ..integrations import post_init_awq_ipex_modules\n-\n-            model = post_init_awq_ipex_modules(model)\n-\n-    def is_serializable(self):\n-        # AWQ through auto-awq has been always serializable, except if the model is fused.\n-        if self.quantization_config.do_fuse:\n-            logger.warning(\"You cannot save an AWQ model that uses fused modules!\")\n-            return False\n+        hf_gptqmodel_post_init(model, use_act_order=self.quantization_config.desc_act)\n \n-        if self.quantization_config.version == AWQLinearVersion.EXLLAMA:\n+    def is_serializable(self, safe_serialization=None):\n+        if self.quantization_config.backend in [AwqBackend.EXLLAMA_V1, AwqBackend.EXLLAMA_V2]:\n             logger.warning(\"You cannot save an AWQ model that uses Exllama backend!\")\n             return False\n \n         return True\n \n     @property\n     def is_trainable(self):\n-        # AWQ supports PEFT fine-tuning from version 0.2.0\n-        MIN_AWQ_VERSION_FOR_PEFT = \"0.2.0\"\n-        return version.parse(importlib.metadata.version(\"autoawq\")) >= version.parse(MIN_AWQ_VERSION_FOR_PEFT)\n+        return version.parse(importlib.metadata.version(\"gptqmodel\")) >= version.parse(\"5.0.0\")"
        },
        {
            "sha": "d28d02825065b85929ce6bdbdfff7184edf51cca",
            "filename": "src/transformers/quantizers/quantizer_gptq.py",
            "status": "modified",
            "additions": 6,
            "deletions": 21,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -22,7 +22,7 @@\n if TYPE_CHECKING:\n     from ..modeling_utils import PreTrainedModel\n \n-from ..utils import is_auto_gptq_available, is_gptqmodel_available, is_optimum_available, is_torch_available, logging\n+from ..utils import is_gptqmodel_available, is_optimum_available, is_torch_available, logging\n from ..utils.quantization_config import GPTQConfig, QuantizationConfigMixin\n \n \n@@ -35,7 +35,8 @@\n class GptqHfQuantizer(HfQuantizer):\n     \"\"\"\n     Quantizer of the GPTQ method - for GPTQ the quantizer support calibration of the model through\n-    `auto_gptq` or `gptqmodel` package. Quantization is done under the hood for users if they load a non-prequantized model.\n+    the GPT-QModel package (Python import name `gptqmodel`). Quantization is done under the hood for users if they\n+    load a non-prequantized model.\n     \"\"\"\n \n     requires_calibration = False\n@@ -52,25 +53,12 @@ def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n     def validate_environment(self, *args, **kwargs):\n         if not is_optimum_available():\n             raise ImportError(\"Loading a GPTQ quantized model requires optimum (`pip install optimum`)\")\n-        if is_auto_gptq_available() and is_gptqmodel_available():\n-            logger.warning(\"Detected gptqmodel and auto-gptq, will use gptqmodel\")\n \n-        gptq_supports_cpu = (\n-            is_auto_gptq_available()\n-            and version.parse(importlib.metadata.version(\"auto-gptq\")) > version.parse(\"0.4.2\")\n-        ) or is_gptqmodel_available()\n+        gptq_supports_cpu = is_gptqmodel_available()\n         if not gptq_supports_cpu and not torch.cuda.is_available():\n             raise RuntimeError(\"GPU is required to quantize or run quantize model.\")\n-        elif not (is_auto_gptq_available() or is_gptqmodel_available()):\n-            raise ImportError(\n-                \"Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. \"\n-            )\n-        elif is_auto_gptq_available() and version.parse(importlib.metadata.version(\"auto_gptq\")) < version.parse(\n-            \"0.4.2\"\n-        ):\n-            raise ImportError(\n-                \"You need a version of auto_gptq >= 0.4.2 to use GPTQ: `pip install --upgrade auto-gptq` or use gptqmodel by `pip install gptqmodel>=1.4.3`.\"\n-            )\n+        elif not is_gptqmodel_available():\n+            raise ImportError(\"Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) library.\")\n         elif is_gptqmodel_available() and (\n             version.parse(importlib.metadata.version(\"gptqmodel\")) < version.parse(\"1.4.3\")\n             or version.parse(importlib.metadata.version(\"optimum\")) < version.parse(\"1.23.99\")\n@@ -88,9 +76,6 @@ def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n     def update_device_map(self, device_map):\n         if device_map is None:\n             device_map = {\"\": torch.device(\"cpu\")}\n-        # Only with auto-gptq do not support CPU, we should move the model to cuda if available.\n-        if not is_gptqmodel_available() and device_map in (\"cpu\", {\"\": torch.device(\"cpu\")}):\n-            device_map = {\"\": 0}\n         return device_map\n \n     def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", **kwargs):"
        },
        {
            "sha": "84071495cf969fd38576a3966ab2d16828e43970",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 14,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -80,8 +80,6 @@\n     is_apex_available,\n     is_apollo_torch_available,\n     is_aqlm_available,\n-    is_auto_awq_available,\n-    is_auto_gptq_available,\n     is_auto_round_available,\n     is_av_available,\n     is_bitsandbytes_available,\n@@ -1297,13 +1295,11 @@ def require_tensorboard(test_case):\n     return unittest.skipUnless(is_tensorboard_available(), \"test requires tensorboard\")\n \n \n-def require_gptq(test_case):\n+def require_gptqmodel(test_case):\n     \"\"\"\n-    Decorator for auto_gptq dependency\n+    Decorator for gptqmodel dependency\n     \"\"\"\n-    return unittest.skipUnless(\n-        is_gptqmodel_available() or is_auto_gptq_available(), \"test requires gptqmodel or auto-gptq\"\n-    )(test_case)\n+    return unittest.skipUnless(is_gptqmodel_available(), \"test requires gptqmodel\")(test_case)\n \n \n def require_hqq(test_case):\n@@ -1313,13 +1309,6 @@ def require_hqq(test_case):\n     return unittest.skipUnless(is_hqq_available(), \"test requires hqq\")(test_case)\n \n \n-def require_auto_awq(test_case):\n-    \"\"\"\n-    Decorator for auto_awq dependency\n-    \"\"\"\n-    return unittest.skipUnless(is_auto_awq_available(), \"test requires autoawq\")(test_case)\n-\n-\n def require_auto_round(test_case):\n     \"\"\"\n     Decorator for auto_round dependency"
        },
        {
            "sha": "1110272dc73ccfbaa95df897b9f64d174036ac32",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -113,8 +113,6 @@\n     is_apex_available,\n     is_apollo_torch_available,\n     is_aqlm_available,\n-    is_auto_awq_available,\n-    is_auto_gptq_available,\n     is_auto_round_available,\n     is_av_available,\n     is_bitsandbytes_available,\n@@ -161,6 +159,7 @@\n     is_libcst_available,\n     is_librosa_available,\n     is_liger_kernel_available,\n+    is_llm_awq_available,\n     is_lomo_available,\n     is_matplotlib_available,\n     is_mistral_common_available,"
        },
        {
            "sha": "9f87ca810061ec31cee5a2f1231b5230d77be272",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -994,7 +994,7 @@ def is_optimum_available() -> bool:\n \n \n @lru_cache\n-def is_auto_awq_available() -> bool:\n+def is_llm_awq_available() -> bool:\n     return _is_package_available(\"awq\")\n \n \n@@ -1031,11 +1031,6 @@ def is_compressed_tensors_available() -> bool:\n     return _is_package_available(\"compressed_tensors\")\n \n \n-@lru_cache\n-def is_auto_gptq_available() -> bool:\n-    return _is_package_available(\"auto_gptq\")\n-\n-\n @lru_cache\n def is_gptqmodel_available() -> bool:\n     return _is_package_available(\"gptqmodel\")"
        },
        {
            "sha": "a4a4d5e87d0ae7650e46679dfcac421421f002e3",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 76,
            "deletions": 231,
            "changes": 307,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -28,16 +28,13 @@\n from packaging import version\n \n from ..utils import (\n-    is_auto_awq_available,\n     is_compressed_tensors_available,\n-    is_gptqmodel_available,\n     is_hqq_available,\n     is_quark_available,\n     is_torch_available,\n     is_torchao_available,\n     logging,\n )\n-from .import_utils import is_auto_gptq_available\n \n \n if is_torch_available():\n@@ -68,29 +65,26 @@ class QuantizationMethod(str, Enum):\n     MXFP4 = \"mxfp4\"\n \n \n-class AWQLinearVersion(str, Enum):\n+class AwqFormat(str, Enum):\n     GEMM = \"gemm\"\n     GEMV = \"gemv\"\n-    EXLLAMA = \"exllama\"\n-    IPEX = \"ipex\"\n+    GEMV_FAST = \"gemv_fast\"\n \n-    @staticmethod\n-    def from_str(version: str):\n-        version = version.lower()\n-        if version == \"gemm\":\n-            return AWQLinearVersion.GEMM\n-        elif version == \"gemv\":\n-            return AWQLinearVersion.GEMV\n-        elif version == \"exllama\":\n-            return AWQLinearVersion.EXLLAMA\n-        elif version == \"ipex\":\n-            return AWQLinearVersion.IPEX\n-        else:\n-            raise ValueError(f\"Unknown AWQLinearVersion {version}\")\n \n-\n-class AwqBackendPackingMethod(str, Enum):\n-    AUTOAWQ = \"autoawq\"\n+class AwqBackend(str, Enum):\n+    LEGACY_AWQ = \"autoawq\"\n+    AUTO = \"auto\"\n+    AUTO_TRAINABLE = \"auto_trainable\"\n+    MACHETE = \"machete\"\n+    MARLIN = \"marlin\"\n+    EXLLAMA_V2 = \"exllama_v2\"\n+    EXLLAMA_V1 = \"exllama_v1\"\n+    GEMM = \"gemm\"\n+    GEMM_TRITON = \"gemm_triton\"\n+    GEMV = \"gemv\"\n+    GEMV_FAST = \"gemv_fast\"\n+    TORCH_AWQ = \"torch_awq\"\n+    TORCH_FUSED_AWQ = \"torch_fused_awq\"\n     LLMAWQ = \"llm-awq\"\n \n \n@@ -620,7 +614,7 @@ class ExllamaVersion(int, Enum):\n class GPTQConfig(QuantizationConfigMixin):\n     \"\"\"\n     This is a wrapper class about all possible attributes and features that you can play with a model that has been\n-    loaded using `optimum` api for gptq quantization relying on auto_gptq backend.\n+    loaded using `optimum` api for GPTQ quantization relying on the gptqmodel backend.\n \n     Args:\n         bits (`int`):\n@@ -641,22 +635,23 @@ class GPTQConfig(QuantizationConfigMixin):\n         desc_act (`bool`, *optional*, defaults to `False`):\n             Whether to quantize columns in order of decreasing activation size. Setting it to False can significantly\n             speed up inference but the perplexity may become slightly worse. Also known as act-order.\n+        act_group_aware (`bool`, *optional*, defaults to `True`):\n+            Use GAR (group aware activation order) during quantization. Has measurable positive impact on quantization\n+            quality. Only applicable when `desc_act = False`. Will forced to be `False` when `desc_act = True`.\n         sym (`bool`, *optional*, defaults to `True`):\n             Whether to use symmetric quantization.\n         true_sequential (`bool`, *optional*, defaults to `True`):\n             Whether to perform sequential quantization even within a single Transformer block. Instead of quantizing\n             the entire block at once, we perform layer-wise quantization. As a result, each layer undergoes\n             quantization using inputs that have passed through the previously quantized layers.\n-        checkpoint_format (`str`, *optional*, defaults to `\"gptq\"`):\n-            GPTQ weight format. `gptq`(v1) is supported by both gptqmodel and auto-gptq. `gptq_v2` is gptqmodel only.\n+        format (`str`, *optional*, defaults to `\"gptq\"`):\n+            GPTQ weight format. `gptq` (v1) is supported by gptqmodel. `gptq_v2` is gptqmodel only.\n         meta (`dict[str, any]`, *optional*):\n             Properties, such as tooling:version, that do not directly contributes to quantization or quant inference are stored in meta.\n             i.e. `meta.quantizer`: [\"optimum:_version_\", \"gptqmodel:_version_\"]\n         backend (`str`, *optional*):\n-            Controls which gptq kernel to be used. Valid values for gptqmodel are `auto`, `auto_trainable` and more. For auto-gptq, only\n-            valid value is None and `auto_trainable`. Ref gptqmodel backends: https://github.com/ModelCloud/GPTQModel/blob/main/gptqmodel/utils/backend.py\n-        use_cuda_fp16 (`bool`, *optional*, defaults to `False`):\n-            Whether or not to use optimized cuda kernel for fp16 model. Need to have model in fp16. Auto-gptq only.\n+            Controls which kernel to use. Valid values for gptqmodel are `auto`, `auto_trainable` and more. Ref gptqmodel backends:\n+            https://github.com/ModelCloud/GPTQModel/blob/main/gptqmodel/utils/backend.py\n         model_seqlen (`int`, *optional*):\n             The maximum sequence length that the model can take.\n         block_name_to_quantize (`str`, *optional*):\n@@ -667,14 +662,9 @@ class GPTQConfig(QuantizationConfigMixin):\n             The batch size used when processing the dataset\n         pad_token_id (`int`, *optional*):\n             The pad token id. Needed to prepare the dataset when `batch_size` > 1.\n-        use_exllama (`bool`, *optional*):\n-            Whether to use exllama backend. Defaults to `True` if unset. Only works with `bits` = 4.\n         max_input_length (`int`, *optional*):\n             The maximum input length. This is needed to initialize a buffer that depends on the maximum expected input\n             length. It is specific to the exllama backend with act-order.\n-        exllama_config (`dict[str, Any]`, *optional*):\n-            The exllama config. You can specify the version of the exllama kernel through the `version` key. Defaults\n-            to `{\"version\": 1}` if unset.\n         cache_block_outputs (`bool`, *optional*, defaults to `True`):\n             Whether to cache block outputs to reuse as inputs for the succeeding block.\n         modules_in_block_to_quantize (`list[list[str]]`, *optional*):\n@@ -694,20 +684,18 @@ def __init__(\n         group_size: int = 128,\n         damp_percent: float = 0.1,\n         desc_act: bool = False,\n+        act_group_aware: bool = True,\n         sym: bool = True,\n         true_sequential: bool = True,\n-        checkpoint_format: str = \"gptq\",\n-        meta: dict[str, Any] | None = None,\n-        backend: str | None = None,\n-        use_cuda_fp16: bool = False,\n-        model_seqlen: int | None = None,\n-        block_name_to_quantize: str | None = None,\n-        module_name_preceding_first_block: list[str] | None = None,\n+        format: str = \"gptq\",\n+        meta: Optional[dict[str, Any]] = None,\n+        backend: Optional[str] = None,\n+        model_seqlen: Optional[int] = None,\n+        block_name_to_quantize: Optional[str] = None,\n+        module_name_preceding_first_block: Optional[list[str]] = None,\n         batch_size: int = 1,\n-        pad_token_id: int | None = None,\n-        use_exllama: bool | None = None,\n-        max_input_length: int | None = None,\n-        exllama_config: dict[str, Any] | None = None,\n+        pad_token_id: Optional[int] = None,\n+        max_input_length: Optional[int] = None,\n         cache_block_outputs: bool = True,\n         modules_in_block_to_quantize: list[list[str]] | None = None,\n         **kwargs,\n@@ -719,33 +707,28 @@ def __init__(\n         self.group_size = group_size\n         self.damp_percent = damp_percent\n         self.desc_act = desc_act\n+        self.act_group_aware = act_group_aware\n         self.sym = sym\n         self.true_sequential = true_sequential\n-        self.checkpoint_format = checkpoint_format.lower()\n+        self.format = format.lower()\n+        # Compatible with legacy field: checkpoint_format\n+        if kwargs.get(\"checkpoint_format\") is not None:\n+            self.format = kwargs.pop(\"checkpoint_format\").lower()\n         self.meta = meta\n         self.backend = backend.lower() if isinstance(backend, str) else backend\n-        self.use_cuda_fp16 = use_cuda_fp16\n         self.model_seqlen = model_seqlen\n         self.block_name_to_quantize = block_name_to_quantize\n         self.module_name_preceding_first_block = module_name_preceding_first_block\n         self.batch_size = batch_size\n         self.pad_token_id = pad_token_id\n-        self.use_exllama = use_exllama\n         self.max_input_length = max_input_length\n-        self.exllama_config = exllama_config\n         self.cache_block_outputs = cache_block_outputs\n         self.modules_in_block_to_quantize = modules_in_block_to_quantize\n         self.post_init()\n \n     def get_loading_attributes(self):\n         attributes_dict = copy.deepcopy(self.__dict__)\n-        loading_attributes = [\n-            \"use_exllama\",\n-            \"exllama_config\",\n-            \"use_cuda_fp16\",\n-            \"max_input_length\",\n-            \"backend\",\n-        ]\n+        loading_attributes = [\"max_input_length\", \"backend\"]\n         loading_attributes_dict = {i: j for i, j in attributes_dict.items() if i in loading_attributes}\n         return loading_attributes_dict\n \n@@ -772,46 +755,14 @@ def post_init(self):\n                     ['wikitext2','c4','c4-new'], but we found {self.dataset}\"\"\"\n                 )\n \n-        # make sure backend is back/forward compatible with both gptqmodel (full) and auto-gptq (partial)\n-        if is_gptqmodel_available():\n-            # convert auto-gptq control into gptqmodel backend\n-            if self.backend is None:\n-                self.backend = \"auto_trainable\" if self.use_exllama is not None and not self.use_exllama else \"auto\"\n-        else:\n-            # convert gptqmodel backend `auto_trainable` into auto-gptq control\n-            if self.backend == \"auto_trainable\":\n-                self.use_exllama = False\n-\n-        # auto-gptq specific kernel control logic\n-        if self.use_exllama is None:\n-            # New default behaviour\n-            self.use_exllama = True\n+        # act_group_order is only applicable when `desc_act = False`\n+        if self.desc_act and self.act_group_aware:\n+            self.act_group_aware = False\n+            logger.warning(\"`act_group_aware` has been auto-disabled as it is not compatible with `desc_act = True`.\")\n \n-        if self.exllama_config is None:\n-            self.exllama_config = {\"version\": ExllamaVersion.ONE}\n-        else:\n-            if \"version\" not in self.exllama_config:\n-                raise ValueError(\"`exllama_config` needs to have a `version` key.\")\n-            elif self.exllama_config[\"version\"] not in [ExllamaVersion.ONE, ExllamaVersion.TWO]:\n-                exllama_version = self.exllama_config[\"version\"]\n-                raise ValueError(\n-                    f\"Only supported versions are in [ExllamaVersion.ONE, ExllamaVersion.TWO] - not recognized version {exllama_version}\"\n-                )\n-\n-        if self.bits == 4 and self.use_exllama:\n-            if self.exllama_config[\"version\"] == ExllamaVersion.ONE:\n-                logger.info(\n-                    \"You have activated exllama backend. Note that you can get better inference \"\n-                    \"speed using exllamav2 kernel by setting `exllama_config`.\"\n-                )\n-            elif self.exllama_config[\"version\"] == ExllamaVersion.TWO:\n-                if is_auto_gptq_available():\n-                    optimum_version = version.parse(importlib.metadata.version(\"optimum\"))\n-                    autogptq_version = version.parse(importlib.metadata.version(\"auto_gptq\"))\n-                    if optimum_version <= version.parse(\"1.13.2\") or autogptq_version <= version.parse(\"0.4.2\"):\n-                        raise ValueError(\n-                            f\"You need optimum > 1.13.2 and auto-gptq > 0.4.2 . Make sure to have that version installed - detected version : optimum {optimum_version} and autogptq {autogptq_version}\"\n-                        )\n+        # make sure backend default stays consistent with gptqmodel expectations\n+        if self.backend is None:\n+            self.backend = \"auto\"\n         if self.modules_in_block_to_quantize is not None:\n             optimum_version = version.parse(importlib.metadata.version(\"optimum\"))\n             if optimum_version < version.parse(\"1.15.0\"):\n@@ -821,35 +772,28 @@ def post_init(self):\n \n     def to_dict(self) -> dict[str, Any]:\n         config_dict = super().to_dict()\n-        config_dict.pop(\"disable_exllama\", None)\n+        # Compatible with legacy field: checkpoint_format\n+        config_dict[\"checkpoint_format\"] = self.format\n         return config_dict\n \n     def to_dict_optimum(self):\n         \"\"\"\n         Get compatible dict for optimum gptq config\n         \"\"\"\n-        quant_dict = self.to_dict()\n-        # make it compatible with optimum config\n-        quant_dict[\"disable_exllama\"] = not self.use_exllama\n-        return quant_dict\n+        return self.to_dict()\n \n     @classmethod\n     def from_dict_optimum(cls, config_dict):\n         \"\"\"\n         Get compatible class with optimum gptq config dict\n         \"\"\"\n \n-        if \"disable_exllama\" in config_dict:\n-            config_dict[\"use_exllama\"] = not config_dict[\"disable_exllama\"]\n-            # switch to None to not trigger the warning\n-            config_dict.pop(\"disable_exllama\")\n-\n         config = cls(**config_dict)\n         return config\n \n \n @dataclass\n-class AwqConfig(QuantizationConfigMixin):\n+class AwqConfig(GPTQConfig):\n     \"\"\"\n     This is a wrapper class about all possible attributes and features that you can play with a model that has been\n     loaded using `auto-awq` library awq quantization relying on auto_awq backend.\n@@ -861,83 +805,54 @@ class AwqConfig(QuantizationConfigMixin):\n             The group size to use for quantization. Recommended value is 128 and -1 uses per-column quantization.\n         zero_point (`bool`, *optional*, defaults to `True`):\n             Whether to use zero point quantization.\n-        version (`AWQLinearVersion`, *optional*, defaults to `AWQLinearVersion.GEMM`):\n-            The version of the quantization algorithm to use. GEMM is better for big batch_size (e.g. >= 8) otherwise,\n-            GEMV is better (e.g. < 8 ). GEMM models are compatible with Exllama kernels.\n-        backend (`AwqBackendPackingMethod`, *optional*, defaults to `AwqBackendPackingMethod.AUTOAWQ`):\n+        backend (`AwqBackend`, *optional*, defaults to `AwqBackend.AUTO`):\n             The quantization backend. Some models might be quantized using `llm-awq` backend. This is useful for users\n             that quantize their own models using `llm-awq` library.\n-        do_fuse (`bool`, *optional*, defaults to `False`):\n-            Whether to fuse attention and mlp layers together for faster inference\n-        fuse_max_seq_len (`int`, *optional*):\n-            The Maximum sequence length to generate when using fusing.\n-        modules_to_fuse (`dict`, *optional*, default to `None`):\n-            Overwrite the natively supported fusing scheme with the one specified by the users.\n         modules_to_not_convert (`list`, *optional*, default to `None`):\n             The list of modules to not quantize, useful for quantizing models that explicitly require to have\n             some modules left in their original precision (e.g. Whisper encoder, Llava encoder, Mixtral gate layers).\n             Note you cannot quantize directly with transformers, please refer to `AutoAWQ` documentation for quantizing HF models.\n-        exllama_config (`dict[str, Any]`, *optional*):\n-            You can specify the version of the exllama kernel through the `version` key, the maximum sequence\n-            length through the `max_input_len` key, and the maximum batch size through the `max_batch_size` key.\n-            Defaults to `{\"version\": 2, \"max_input_len\": 2048, \"max_batch_size\": 8}` if unset.\n     \"\"\"\n \n     def __init__(\n         self,\n         bits: int = 4,\n         group_size: int = 128,\n         zero_point: bool = True,\n-        version: AWQLinearVersion = AWQLinearVersion.GEMM,\n-        backend: AwqBackendPackingMethod = AwqBackendPackingMethod.AUTOAWQ,\n-        do_fuse: bool | None = None,\n-        fuse_max_seq_len: int | None = None,\n-        modules_to_fuse: dict | None = None,\n+        backend: AwqBackend = AwqBackend.AUTO,\n         modules_to_not_convert: list | None = None,\n-        exllama_config: dict[str, int] | None = None,\n         **kwargs,\n     ):\n-        self.quant_method = QuantizationMethod.AWQ\n-\n-        self.bits = bits\n-        self.group_size = group_size\n+        format = kwargs.pop(\"format\", AwqFormat.GEMM)\n+        # Compatible with legacy field: version\n+        if kwargs.get(\"version\") is not None:\n+            format = kwargs.pop(\"version\").lower()\n+        # Compatible with legacy backend\n+        if backend == AwqBackend.LEGACY_AWQ:\n+            backend = AwqBackend.AUTO\n         self.zero_point = zero_point\n-        self.version = version\n-        self.backend = backend\n-        self.fuse_max_seq_len = fuse_max_seq_len\n         self.modules_to_not_convert = modules_to_not_convert\n-        self.exllama_config = exllama_config\n-\n-        self.modules_to_fuse = modules_to_fuse\n-        if do_fuse is None:\n-            self.do_fuse = modules_to_fuse is not None and len(modules_to_fuse) > 0\n-        else:\n-            self.do_fuse = do_fuse\n-        self.fuse_max_seq_len = fuse_max_seq_len\n \n-        self.post_init()\n+        super().__init__(bits=bits, group_size=group_size, backend=backend, format=format, **kwargs)\n+        self.quant_method = QuantizationMethod.AWQ\n \n     def post_init(self):\n         r\"\"\"\n         Safety checker that arguments are correct\n         \"\"\"\n-        if self.backend not in [AwqBackendPackingMethod.AUTOAWQ, AwqBackendPackingMethod.LLMAWQ]:\n-            raise ValueError(\n-                f\"Only supported quantization backends in {AwqBackendPackingMethod.AUTOAWQ} and {AwqBackendPackingMethod.LLMAWQ} - not recognized backend {self.backend}\"\n-            )\n-\n-        self.version = AWQLinearVersion.from_str(self.version)\n-        if self.version not in [\n-            AWQLinearVersion.GEMM,\n-            AWQLinearVersion.GEMV,\n-            AWQLinearVersion.EXLLAMA,\n-            AWQLinearVersion.IPEX,\n+        if self.format not in [\n+            AwqFormat.GEMM,\n+            AwqFormat.GEMV,\n+            AwqFormat.GEMV_FAST,\n         ]:\n             raise ValueError(\n-                f\"Only supported versions are in [AWQLinearVersion.GEMM, AWQLinearVersion.GEMV, AWQLinearVersion.EXLLAMA, AWQLinearVersion.IPEX] - not recognized version {self.version}\"\n+                f\"Only supported versions are in [AWQLinearVersion.GEMM, AWQLinearVersion.GEMV, AWQLinearVersion.GEMV_FAST] - not recognized version {self.format}\"\n             )\n \n-        if self.backend == AwqBackendPackingMethod.LLMAWQ:\n+        if self.backend not in AwqBackend.__members__.values():\n+            raise ValueError(f\"Invalid backend '{self.backend}'. Must be one of: {[b.value for b in AwqBackend]}\")\n+\n+        if self.backend == AwqBackend.LLMAWQ:\n             # Only cuda device can run this function\n             if not (torch.cuda.is_available() or torch.xpu.is_available()):\n                 raise ValueError(\"LLM-AWQ backend is only supported on CUDA and XPU\")\n@@ -947,82 +862,12 @@ def post_init(self):\n                 if major < 8:\n                     raise ValueError(\"LLM-AWQ backend is only supported on CUDA GPUs with compute capability >= 8.0\")\n \n-        if self.do_fuse and self.fuse_max_seq_len is None:\n-            raise ValueError(\n-                \"You cannot enable fused modules without specifying a `fuse_max_seq_len`, make sure to pass a valid `fuse_max_seq_len` for your usecase\"\n-            )\n-\n-        if self.do_fuse:\n-            awq_version_supports_fusing = False\n-            MIN_AWQ_VERSION = \"0.1.7\"\n-            if is_auto_awq_available():\n-                awq_version_supports_fusing = version.parse(importlib.metadata.version(\"autoawq\")) >= version.parse(\n-                    MIN_AWQ_VERSION\n-                )\n-\n-            if not awq_version_supports_fusing:\n-                raise ValueError(\n-                    f\"You current version of `autoawq` does not support module fusing, please upgrade `autoawq` package to at least {MIN_AWQ_VERSION}.\"\n-                )\n-\n-        if self.modules_to_not_convert is not None:\n-            awq_version_supports_non_conversion = False\n-            MIN_AWQ_VERSION = \"0.1.8\"\n-            if is_auto_awq_available():\n-                awq_version_supports_non_conversion = version.parse(\n-                    importlib.metadata.version(\"autoawq\")\n-                ) >= version.parse(MIN_AWQ_VERSION)\n-\n-            if not awq_version_supports_non_conversion:\n-                raise ValueError(\n-                    f\"You current version of `autoawq` does not support module quantization skipping, please upgrade `autoawq` package to at least {MIN_AWQ_VERSION}.\"\n-                )\n-\n-        if self.do_fuse and self.modules_to_fuse is not None:\n-            required_keys = [\n-                \"hidden_size\",\n-                \"num_attention_heads\",\n-                \"num_key_value_heads\",\n-                \"mlp\",\n-                \"attention\",\n-                \"layernorm\",\n-                \"use_alibi\",\n-            ]\n-            if not all(key in self.modules_to_fuse for key in required_keys):\n-                raise ValueError(\n-                    f\"Required fields are missing in the fusing mapping, required fields are {required_keys}\"\n-                )\n-\n-        if self.version == AWQLinearVersion.EXLLAMA:\n-            awq_version_supports_exllama = False\n-            MIN_AWQ_VERSION = \"0.2.0\"\n-            if is_auto_awq_available():\n-                awq_version_supports_exllama = version.parse(importlib.metadata.version(\"autoawq\")) >= version.parse(\n-                    MIN_AWQ_VERSION\n-                )\n-\n-            if not awq_version_supports_exllama:\n-                raise ValueError(\n-                    f\"You current version of `autoawq` does not support exllama backend, \"\n-                    f\"please upgrade `autoawq` package to at least {MIN_AWQ_VERSION}.\"\n-                )\n-\n-            if self.exllama_config is None:\n-                self.exllama_config = {\"version\": ExllamaVersion.TWO, \"max_input_len\": 2048, \"max_batch_size\": 8}\n-            else:\n-                if \"version\" not in self.exllama_config:\n-                    raise ValueError(\"`exllama_config` needs to have a `version` key.\")\n-                elif self.exllama_config[\"version\"] not in [ExllamaVersion.ONE, ExllamaVersion.TWO]:\n-                    exllama_version = self.exllama_config[\"version\"]\n-                    raise ValueError(\n-                        f\"Only supported versions are in [ExllamaVersion.ONE, ExllamaVersion.TWO] - not recognized version {exllama_version}\"\n-                    )\n-\n-    def get_loading_attributes(self):\n-        attributes_dict = copy.deepcopy(self.__dict__)\n-        loading_attributes = [\"version\", \"do_fuse\", \"modules_to_fuse\", \"fuse_max_seq_len\", \"exllama_config\"]\n-        loading_attributes_dict = {i: j for i, j in attributes_dict.items() if i in loading_attributes}\n-        return loading_attributes_dict\n+    def to_dict(self) -> dict[str, Any]:\n+        config_dict = super().to_dict()\n+        config_dict.pop(\"checkpoint_format\")\n+        # Compatible with legacy field: version\n+        config_dict[\"version\"] = self.format\n+        return config_dict\n \n \n @dataclass"
        },
        {
            "sha": "a43b565eb73a249f7e30b124660a8254e3712faf",
            "filename": "tests/quantization/autoawq/test_awq.py",
            "status": "modified",
            "additions": 34,
            "deletions": 279,
            "changes": 313,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoawq%2Ftest_awq.py?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -16,24 +16,20 @@\n import tempfile\n import unittest\n \n-import pytest\n-\n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AwqConfig, OPTForCausalLM\n from transformers.testing_utils import (\n     backend_empty_cache,\n     get_device_properties,\n     require_accelerate,\n-    require_auto_awq,\n-    require_flash_attn,\n-    require_intel_extension_for_pytorch,\n+    require_gptqmodel,\n     require_torch_accelerator,\n     require_torch_gpu,\n     require_torch_multi_accelerator,\n-    require_torch_multi_gpu,\n     slow,\n     torch_device,\n )\n from transformers.utils import is_accelerate_available, is_torch_available\n+from transformers.utils.quantization_config import AwqBackend\n \n \n if is_torch_available():\n@@ -86,13 +82,18 @@ def test_to_dict(self):\n         config_to_dict = quantization_config.to_dict()\n \n         for key in config_to_dict:\n-            self.assertEqual(getattr(quantization_config, key), config_to_dict[key])\n+            if key == \"version\":\n+                # \"version\" is legacy filed.\n+                # It will be written in to_dict() for compatibility, but AwqConfig will not have this field.\n+                self.assertFalse(hasattr(quantization_config, key))\n+            else:\n+                self.assertEqual(getattr(quantization_config, key), config_to_dict[key])\n \n     def test_from_dict(self):\n         \"\"\"\n         Simple test that checks if one uses a dict and converts it to a config object, the config object is the same as the dict\n         \"\"\"\n-        dict = {\"bits\": 2, \"zero_point\": False, \"backend\": \"autoawq\"}\n+        dict = {\"bits\": 2, \"zero_point\": False, \"backend\": \"auto\"}\n         quantization_config = AwqConfig.from_dict(dict)\n \n         self.assertEqual(dict[\"bits\"], quantization_config.bits)\n@@ -102,7 +103,7 @@ def test_from_dict(self):\n \n @slow\n @require_torch_accelerator\n-@require_auto_awq\n+@require_gptqmodel\n @require_accelerate\n class AwqTest(unittest.TestCase):\n     model_name = \"TheBloke/Mistral-7B-v0.1-AWQ\"\n@@ -121,6 +122,12 @@ class AwqTest(unittest.TestCase):\n     EXPECTED_OUTPUT.add(\n         \"Hello my name is Katie and I am a 20 year old student at the University of North Carolina at Chapel Hill. I am a junior and I am majoring in Exercise and Sport Science with a\"\n     )\n+    EXPECTED_OUTPUT.add(\n+        \"Hello my name is Katie and I am a 20 year old student from the UK. I am currently studying for a degree in English Literature and History at the University of York. I am a very out\"\n+    )\n+    EXPECTED_OUTPUT.add(\n+        \"Hello my name is Katie and I am a 20 year old student from the UK. I am currently studying for a degree in English Literature and History at the University of York. I am a very creative\"\n+    )\n \n     EXPECTED_OUTPUT_BF16 = [\n         \"Hello my name is Katie and I am a 20 year old student at the University of North Carolina at Chapel Hill. I am a junior and I am majoring in Journalism and minoring in Spanish\"\n@@ -129,6 +136,7 @@ class AwqTest(unittest.TestCase):\n     EXPECTED_OUTPUT_EXLLAMA = [\n         \"Hello my name is Katie and I am a 20 year old student from the UK. I am currently studying for a degree in English Literature and History at the University of York. I am a very out\",\n         \"Hello my name is Katie and I am a 20 year old student from the UK. I am currently studying for a degree in English Literature and History at the University of York. I am a very creative\",\n+        \"Hello my name is Katie and I am a 20 year old student at the University of North Carolina at Chapel Hill. I am a junior and I am majoring in Journalism and minoring in Spanish\",\n     ]\n     device_map = torch_device\n \n@@ -139,7 +147,11 @@ def setUpClass(cls):\n         Setup quantized model\n         \"\"\"\n         cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n-        cls.quantized_model = AutoModelForCausalLM.from_pretrained(cls.model_name, device_map=cls.device_map)\n+        # Use GEMM so that test_save_pretrained() writes out the quantized weights.\n+        quantization_config = AwqConfig(backend=AwqBackend.GEMM)\n+        cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n+            cls.model_name, device_map=cls.device_map, quantization_config=quantization_config\n+        )\n \n     def tearDown(self):\n         gc.collect()\n@@ -150,7 +162,8 @@ def test_quantized_model_conversion(self):\n         \"\"\"\n         Simple test that checks if the quantized model has been converted properly\n         \"\"\"\n-        from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV\n+        from gptqmodel.nn_modules.qlinear.gemm_awq import AwqGEMMQuantLinear\n+        from gptqmodel.nn_modules.qlinear.gemv_awq import AwqGEMVQuantLinear\n \n         from transformers.integrations.awq import replace_with_awq_linear\n \n@@ -169,7 +182,7 @@ def test_quantized_model_conversion(self):\n         model, _ = replace_with_awq_linear(model, quantization_config=quantization_config)\n         nb_awq_linear = 0\n         for module in model.modules():\n-            if isinstance(module, (WQLinear_GEMM, WQLinear_GEMV)):\n+            if isinstance(module, (AwqGEMMQuantLinear, AwqGEMVQuantLinear)):\n                 nb_awq_linear += 1\n \n         self.assertEqual(nb_linears, nb_awq_linear)\n@@ -183,7 +196,7 @@ def test_quantized_model_conversion(self):\n         )\n         nb_awq_linear = 0\n         for module in model.modules():\n-            if isinstance(module, (WQLinear_GEMM, WQLinear_GEMV)):\n+            if isinstance(module, (AwqGEMMQuantLinear, AwqGEMVQuantLinear)):\n                 nb_awq_linear += 1\n \n         self.assertEqual(nb_linears - 1, nb_awq_linear)\n@@ -222,7 +235,7 @@ def test_quantized_model_exllama(self):\n         \"\"\"\n         input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n \n-        quantization_config = AwqConfig(version=\"exllama\")\n+        quantization_config = AwqConfig(backend=AwqBackend.EXLLAMA_V2)\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name, quantization_config=quantization_config, device_map=torch_device\n         )\n@@ -290,270 +303,13 @@ def test_quantized_model_no_k_proj_quantized(self):\n \n @slow\n @require_torch_accelerator\n-@require_auto_awq\n-@require_accelerate\n-class AwqFusedTest(unittest.TestCase):\n-    model_name = \"TheBloke/Mistral-7B-OpenOrca-AWQ\"\n-    model_revision = \"7048b2af77d0dd1c81b000b19d73f9cc8950b510\"\n-\n-    custom_mapping_model_id = \"TheBloke/Mistral-7B-v0.1-AWQ\"\n-    custom_model_revision = \"f186bcfa9edbe2a4334262ec1e67f23e53ed1ae7\"\n-\n-    mixtral_model_name = \"casperhansen/mixtral-instruct-awq\"\n-    mixtral_model_revision = \"87dd4ec502dde74fb3a624835c776b000d190c3b\"\n-\n-    multi_modal_model_name = \"ybelkada/llava-1.5-7b-hf-awq\"\n-    multi_modal_model_code_revision = \"ad108a50f5b9e681bdd7378409f57b7fa59a7442\"\n-\n-    awq_rope_model_name = \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\"\n-    awq_rope_model_revision = \"db1f81ad4b8c7e39777509fac66c652eb0a52f91\"\n-\n-    prompt = (\n-        \"You're standing on the surface of the Earth. \"\n-        \"You walk one mile south, one mile west and one mile north. \"\n-        \"You end up exactly where you started. Where are you?\"\n-    )\n-\n-    EXPECTED_GENERATION = prompt + \"\\n\\nYou're at the center of a square.\"\n-    EXPECTED_GENERATION_CUSTOM_MODEL = \"Hello,\\n\\nI have a problem with my 20\"\n-    EXPECTED_GENERATION_MIXTRAL = prompt + \" You're on the North Pole.\\n\\nThe\"\n-    EXPECTED_GENERATION_AWQ_ROPE = prompt + \" [Note: You can't be in a city, and\"\n-\n-    def tearDown(self):\n-        gc.collect()\n-        backend_empty_cache(torch_device)\n-        gc.collect()\n-\n-    def _check_fused_modules(self, model):\n-        has_fused_modules = False\n-        fused_modules_name = [\"QuantAttentionFused\", \"QuantFusedMLP\", \"FasterTransformerRMSNorm\"]\n-\n-        for _, module in model.named_modules():\n-            if module.__class__.__name__ in fused_modules_name:\n-                has_fused_modules = True\n-                break\n-\n-        self.assertTrue(has_fused_modules, \"Modules fusing not performed correctly!\")\n-\n-    def test_raise_save_pretrained(self):\n-        \"\"\"\n-        Test that `save_pretrained` is effectively blocked for fused models\n-        \"\"\"\n-        quantization_config = AwqConfig(bits=4, fuse_max_seq_len=128, do_fuse=True)\n-\n-        model = AutoModelForCausalLM.from_pretrained(\n-            self.model_name,\n-            quantization_config=quantization_config,\n-            revision=self.model_revision,\n-        ).to(torch_device)\n-\n-        self._check_fused_modules(model)\n-\n-        with self.assertRaises(ValueError), tempfile.TemporaryDirectory() as tmpdirname:\n-            model.save_pretrained(tmpdirname)\n-\n-    def test_fused_modules_to_not_convert(self):\n-        \"\"\"\n-        Test if fused + modules to_not_convert work as expected\n-        \"\"\"\n-        model_id = \"hf-internal-testing/Mixtral-tiny-AWQ\"\n-\n-        quantization_config = AwqConfig(bits=4, fuse_max_seq_len=128, do_fuse=True)\n-        model = AutoModelForCausalLM.from_pretrained(\n-            model_id,\n-            quantization_config=quantization_config,\n-        ).to(torch_device)\n-\n-        # Check if model has been correctly fused\n-        self._check_fused_modules(model)\n-        # Checks if the modules_to_not_convert (here gate layer) is a Linear\n-        self.assertTrue(isinstance(model.model.layers[0].block_sparse_moe.gate, torch.nn.Linear))\n-\n-    @unittest.skipIf(\n-        get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 8,\n-        \"Skipping because RuntimeError: FlashAttention only supports Ampere GPUs or newer, so not supported on GPU with capability < 8.0\",\n-    )\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    def test_generation_fused(self):\n-        \"\"\"\n-        Test generation quality for fused models - single batch case\n-        \"\"\"\n-        quantization_config = AwqConfig(bits=4, fuse_max_seq_len=128, do_fuse=True)\n-\n-        model = AutoModelForCausalLM.from_pretrained(\n-            self.model_name,\n-            quantization_config=quantization_config,\n-            revision=self.model_revision,\n-        ).to(torch_device)\n-\n-        self._check_fused_modules(model)\n-\n-        tokenizer = AutoTokenizer.from_pretrained(self.model_name, revision=self.model_revision)\n-\n-        inputs = tokenizer(self.prompt, return_tensors=\"pt\").to(torch_device)\n-\n-        outputs = model.generate(**inputs, max_new_tokens=12)\n-\n-        self.assertEqual(tokenizer.decode(outputs[0], skip_special_tokens=True), self.EXPECTED_GENERATION)\n-\n-    @pytest.mark.flash_attn_test\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @unittest.skipIf(\n-        get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 8,\n-        \"Skipping because RuntimeError: FlashAttention only supports Ampere GPUs or newer, so not supported on GPU with capability < 8.0\",\n-    )\n-    def test_generation_fused_batched(self):\n-        \"\"\"\n-        Test generation quality for fused models - multi batch case\n-        \"\"\"\n-        quantization_config = AwqConfig(bits=4, fuse_max_seq_len=128, do_fuse=True)\n-\n-        model = AutoModelForCausalLM.from_pretrained(\n-            self.model_name,\n-            quantization_config=quantization_config,\n-            revision=self.model_revision,\n-        ).to(torch_device)\n-\n-        self._check_fused_modules(model)\n-\n-        tokenizer = AutoTokenizer.from_pretrained(self.model_name, revision=self.model_revision)\n-\n-        tokenizer.pad_token_id = tokenizer.eos_token_id\n-        inputs = tokenizer([self.prompt, self.prompt], return_tensors=\"pt\", padding=True).to(torch_device)\n-\n-        outputs = model.generate(**inputs, max_new_tokens=12)\n-\n-        self.assertEqual(tokenizer.decode(outputs[0], skip_special_tokens=True), self.EXPECTED_GENERATION)\n-\n-    def test_generation_llava_fused(self):\n-        from transformers import pipeline\n-\n-        quantization_config = AwqConfig(do_fuse=True, fuse_max_seq_len=2048)\n-\n-        pipe = pipeline(\n-            \"image-to-text\",\n-            model=self.multi_modal_model_name,\n-            device=0,\n-            model_kwargs={\n-                \"quantization_config\": quantization_config,\n-            },\n-            revision=self.multi_modal_model_code_revision,\n-        )\n-        url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/compel-neg.png\"\n-\n-        prompt = \"USER: <image>\\nCan you please describe this image?\\nASSISTANT:\"\n-\n-        outputs = pipe(url, prompt=prompt, generate_kwargs={\"max_new_tokens\": 100})\n-        EXPECTED_OUTPUT = \"USER:  \\nCan you please describe this image?\\nASSISTANT: The image features a brown and white cat sitting on a green surface, possibly a carpet or a grassy area. The cat is holding a red ball in its paws, seemingly playing with it. The cat appears to be focused on the ball, possibly preparing to play or just enjoying the toy.\"\n-\n-        self.assertEqual(outputs[0][\"generated_text\"], EXPECTED_OUTPUT)\n-\n-    @pytest.mark.flash_attn_test\n-    @require_flash_attn\n-    @require_torch_multi_gpu\n-    @unittest.skipIf(\n-        get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 8,\n-        \"Skipping because RuntimeError: FlashAttention only supports Ampere GPUs or newer, so not supported on GPU with capability < 8.0\",\n-    )\n-    def test_generation_custom_model(self):\n-        \"\"\"\n-        Test generation quality for fused models using custom fused map.\n-        \"\"\"\n-        quantization_config = AwqConfig(\n-            bits=4,\n-            fuse_max_seq_len=512,\n-            modules_to_fuse={\n-                \"attention\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n-                \"mlp\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n-                \"layernorm\": [\"input_layernorm\", \"post_attention_layernorm\", \"norm\"],\n-                \"use_alibi\": False,\n-                \"hidden_size\": 4096,\n-                \"num_attention_heads\": 32,\n-                \"num_key_value_heads\": 8,\n-            },\n-        )\n-\n-        model = AutoModelForCausalLM.from_pretrained(\n-            self.custom_mapping_model_id,\n-            quantization_config=quantization_config,\n-            device_map=\"balanced\",\n-            revision=self.custom_model_revision,\n-        )\n-\n-        self._check_fused_modules(model)\n-\n-        tokenizer = AutoTokenizer.from_pretrained(self.custom_mapping_model_id, revision=self.custom_model_revision)\n-\n-        prompt = \"Hello\"\n-        inputs = tokenizer(prompt, return_tensors=\"pt\").to(torch_device)\n-\n-        outputs = model.generate(**inputs, max_new_tokens=12)\n-        self.assertEqual(tokenizer.decode(outputs[0], skip_special_tokens=True), self.EXPECTED_GENERATION_CUSTOM_MODEL)\n-\n-    @pytest.mark.flash_attn_test\n-    @require_flash_attn\n-    @require_torch_multi_gpu\n-    @unittest.skip(reason=\"Not enough GPU memory on CI runners\")\n-    def test_generation_mixtral_fused(self):\n-        \"\"\"\n-        Text generation test for Mixtral + AWQ + fused\n-        \"\"\"\n-        quantization_config = AwqConfig(bits=4, fuse_max_seq_len=1024, do_fuse=True)\n-        model = AutoModelForCausalLM.from_pretrained(\n-            self.mixtral_model_name,\n-            quantization_config=quantization_config,\n-            device_map=\"auto\",\n-            revision=self.mixtral_model_revision,\n-        )\n-\n-        tokenizer = AutoTokenizer.from_pretrained(self.mixtral_model_name)\n-        tokenizer.pad_token = tokenizer.eos_token\n-\n-        inputs = tokenizer([self.prompt, self.prompt], return_tensors=\"pt\", padding=True).to(torch_device)\n-\n-        outputs = model.generate(**inputs, max_new_tokens=12)\n-        self.assertEqual(tokenizer.decode(outputs[0], skip_special_tokens=True), self.EXPECTED_GENERATION_MIXTRAL)\n-\n-    @pytest.mark.flash_attn_test\n-    @require_flash_attn\n-    @require_torch_multi_gpu\n-    @unittest.skipIf(\n-        get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 8,\n-        \"Skipping because RuntimeError: FlashAttention only supports Ampere GPUs or newer, so not supported on GPU with capability < 8.0\",\n-    )\n-    def test_generation_awq_rope_fused(self):\n-        \"\"\"\n-        Text generation test for AWQ model with special RoPE implementation (e.g. LLaMA3) + fused\n-        \"\"\"\n-        quantization_config = AwqConfig(bits=4, fuse_max_seq_len=1024, do_fuse=True)\n-        model = AutoModelForCausalLM.from_pretrained(\n-            self.awq_rope_model_name,\n-            quantization_config=quantization_config,\n-            device_map=\"auto\",\n-            revision=self.awq_rope_model_revision,\n-        )\n-\n-        tokenizer = AutoTokenizer.from_pretrained(self.awq_rope_model_name)\n-        tokenizer.pad_token = tokenizer.eos_token\n-\n-        inputs = tokenizer([self.prompt, self.prompt], return_tensors=\"pt\", padding=True).to(torch_device)\n-\n-        outputs = model.generate(**inputs, max_new_tokens=12, do_sample=False)\n-        self.assertEqual(tokenizer.decode(outputs[0], skip_special_tokens=True), self.EXPECTED_GENERATION_AWQ_ROPE)\n-\n-\n-@slow\n-@require_torch_accelerator\n-@require_auto_awq\n+@require_gptqmodel\n @require_accelerate\n class AwqScaleTest(unittest.TestCase):\n     model_name = \"TechxGenus/starcoder2-3b-AWQ\"\n \n     def test_load_quantized_model(self):\n-        from awq.modules.act import ScaledActivation\n+        from gptqmodel.quantization.awq.modules.act import ScaledActivation\n \n         \"\"\"\n         Simple test that checks if the scales have been replaced in the quantized model\n@@ -565,15 +321,14 @@ def test_load_quantized_model(self):\n \n \n @slow\n-@require_auto_awq\n+@require_gptqmodel\n @require_accelerate\n-@require_intel_extension_for_pytorch\n-class AwqIPEXTest(unittest.TestCase):\n-    def test_quantized_model_ipex(self):\n+class AwqTorchFusedTest(unittest.TestCase):\n+    def test_quantized_model_torch_fused(self):\n         \"\"\"\n-        Simple test that checks if the quantized model is working properly with ipex backend\n+        Simple test that checks if the quantized model is working properly with torch_fused backend\n         \"\"\"\n-        quantization_config = AwqConfig(version=\"ipex\")\n+        quantization_config = AwqConfig(backend=AwqBackend.TORCH_FUSED_AWQ)\n \n         model = AutoModelForCausalLM.from_pretrained(\n             \"TheBloke/TinyLlama-1.1B-Chat-v0.3-AWQ\","
        },
        {
            "sha": "2a1402bec29b58d72b1ceb38d56378db18636b09",
            "filename": "tests/quantization/gptq/test_gptq.py",
            "status": "modified",
            "additions": 82,
            "deletions": 117,
            "changes": 199,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/tests%2Fquantization%2Fgptq%2Ftest_gptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff/tests%2Fquantization%2Fgptq%2Ftest_gptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fgptq%2Ftest_gptq.py?ref=8ebfd84fa7f4d6c59f5059a439fad10ada26b3ff",
            "patch": "@@ -21,19 +21,25 @@\n from transformers.testing_utils import (\n     is_torch_available,\n     require_accelerate,\n-    require_gptq,\n+    require_gptqmodel,\n     require_optimum,\n     require_torch_gpu,\n     require_torch_multi_gpu,\n     slow,\n )\n-from transformers.utils import is_auto_gptq_available, is_gptqmodel_available, is_ipex_available\n+from transformers.utils import is_gptqmodel_available, is_ipex_available\n \n \n if is_torch_available():\n     import torch\n \n \n+if is_gptqmodel_available():\n+    from gptqmodel import BACKEND\n+    from gptqmodel.quantization import METHOD\n+    from gptqmodel.utils.importer import hf_select_quant_linear_v2\n+\n+\n class GPTQConfigTest(unittest.TestCase):\n     def test_bits(self):\n         with self.assertRaises(ValueError):\n@@ -76,14 +82,14 @@ def test_optimum_config(self):\n \n @slow\n @require_optimum\n-@require_gptq\n+@require_gptqmodel\n class GPTQTest(unittest.TestCase):\n     model_name = \"bigscience/bloom-560m\"\n \n     input_text = \"Hello my name is\"\n \n     EXPECTED_OUTPUTS = set()\n-    # flaky test: gptqmodel and auto-gptq are not output equivalent nor is string compare deterministic even between transformer/torch versions\n+    # flaky test: gptqmodel kernels are not always bitwise deterministic even between transformer/torch versions\n     EXPECTED_OUTPUTS.add(\"Hello my name is John and I am a professional photographer. I\")\n     EXPECTED_OUTPUTS.add(\"Hello my name is John, I am a professional photographer and I\")\n     EXPECTED_OUTPUTS.add(\"Hello my name is John, I am a student in the University of\")\n@@ -94,6 +100,10 @@ class GPTQTest(unittest.TestCase):\n     EXPECTED_OUTPUTS.add(\"Hello my name is Nate and I am a member of the N\")\n     EXPECTED_OUTPUTS.add(\"Hello my name is Nellie and I am a student at the\")\n     EXPECTED_OUTPUTS.add(\"Hello my name is Nate and I am a new member of the\")\n+    EXPECTED_OUTPUTS.add(\"Hello my name is Nils, I am a student of the University\")\n+    EXPECTED_OUTPUTS.add(\"Hello my name is John and I am a very friendly and caring\")\n+    EXPECTED_OUTPUTS.add(\"Hello my name is Nils, I am a student in the field\")\n+    EXPECTED_OUTPUTS.add(\"Hello my name is Michael, I am a professional photographer and I\")\n \n     # this seems a little small considering that we are doing 4bit quant but we have a small model and ww don't quantize the embeddings\n     EXPECTED_RELATIVE_DIFFERENCE = 1.664253062\n@@ -102,13 +112,14 @@ class GPTQTest(unittest.TestCase):\n     sym = True\n     group_size = 128\n     desc_act = False\n-    use_exllama = False\n-\n+    act_group_aware = True\n+    quant_backend = BACKEND.AUTO\n+    load_backend = BACKEND.AUTO\n     dataset = [\n-        \"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"\n+        \"gptqmodel is an easy-to-use model quantization library with user-friendly APIs, based on the GPTQ algorithm.\"\n     ]\n \n-    device_map = \"cpu\" if is_gptqmodel_available() else None\n+    device_map = \"cpu\"\n \n     # called only once for all test in this class\n     @classmethod\n@@ -130,8 +141,9 @@ def setUpClass(cls):\n             tokenizer=cls.tokenizer,\n             group_size=cls.group_size,\n             desc_act=cls.desc_act,\n+            act_group_aware=cls.act_group_aware,\n             sym=cls.sym,\n-            use_exllama=cls.use_exllama,\n+            backend=cls.quant_backend,\n         )\n \n         cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n@@ -170,44 +182,33 @@ def test_original_dtype(self):\n         \"\"\"\n         self.assertTrue(hasattr(self.quantized_model.config, \"_pre_quantization_dtype\"))\n         self.assertFalse(hasattr(self.model_fp16.config, \"_pre_quantization_dtype\"))\n-        self.assertTrue(self.quantized_model.config._pre_quantization_dtype == torch.float16)\n+        self.assertEqual(self.quantized_model.config._pre_quantization_dtype, torch.float16)\n \n     def test_quantized_layers_class(self):\n         \"\"\"\n         Simple test to check if the model conversion has been done correctly by checking on\n         the class type of the linear layers of the converted models\n         \"\"\"\n-        if is_gptqmodel_available():\n-            from gptqmodel.utils.importer import hf_select_quant_linear\n-\n-            if hasattr(self.config, \"quantization_config\"):\n-                checkpoint_format = self.config.quantization_config.get(\"checkpoint_format\")\n-                meta = self.config.quantization_config.get(\"meta\")\n-            else:\n-                checkpoint_format = \"gptq\"\n-                meta = None\n-            QuantLinear = hf_select_quant_linear(\n-                bits=self.bits,\n-                group_size=self.group_size,\n-                desc_act=self.desc_act,\n-                sym=self.sym,\n-                device_map=self.device_map,\n-                checkpoint_format=checkpoint_format,\n-                meta=meta,\n-                backend=self.quantization_config.backend,\n-            )\n-        elif is_auto_gptq_available():\n-            from auto_gptq.utils.import_utils import dynamically_import_QuantLinear as hf_select_quant_linear\n-\n-            QuantLinear = hf_select_quant_linear(\n-                use_triton=False,\n-                desc_act=self.desc_act,\n-                group_size=self.group_size,\n-                bits=self.bits,\n-                disable_exllama=not self.use_exllama,\n-                disable_exllamav2=True,\n-            )\n-        self.assertTrue(self.quantized_model.transformer.h[0].mlp.dense_4h_to_h.__class__ == QuantLinear)\n+        if hasattr(self.config, \"quantization_config\"):\n+            checkpoint_format = self.config.quantization_config.get(\"checkpoint_format\")\n+            meta = self.config.quantization_config.get(\"meta\")\n+        else:\n+            checkpoint_format = \"gptq\"\n+            meta = None\n+\n+        QuantLinear = hf_select_quant_linear_v2(\n+            bits=self.bits,\n+            group_size=self.group_size,\n+            desc_act=self.desc_act,\n+            sym=self.sym,\n+            device_map=self.device_map,\n+            format=checkpoint_format,\n+            quant_method=METHOD.GPTQ,\n+            meta=meta,\n+            backend=self.quantization_config.backend,\n+            pack=True,\n+        )\n+        self.assertEqual(self.quantized_model.transformer.h[0].mlp.dense_4h_to_h.__class__, QuantLinear)\n \n     def check_inference_correctness(self, model):\n         r\"\"\"\n@@ -225,7 +226,7 @@ def check_inference_correctness(self, model):\n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n \n     def check_quantized_layers_type(self, model, value):\n-        self.assertTrue(model.transformer.h[0].mlp.dense_4h_to_h.QUANT_TYPE == value)\n+        self.assertEqual(model.transformer.h[0].mlp.dense_4h_to_h.QUANT_TYPE, value)\n \n     def test_generate_quality(self):\n         \"\"\"\n@@ -243,29 +244,13 @@ def test_serialization(self):\n         Test the serialization of the model and the loading of the quantized weights works\n         \"\"\"\n         with tempfile.TemporaryDirectory() as tmpdirname:\n+            self.tokenizer.save_pretrained(tmpdirname)\n             self.quantized_model.save_pretrained(tmpdirname)\n-            if is_auto_gptq_available() and not is_gptqmodel_available():\n-                quant_type = \"cuda-old\" if not self.use_exllama else \"exllama\"\n-                if not self.use_exllama:\n-                    quantized_model_from_saved = AutoModelForCausalLM.from_pretrained(\n-                        tmpdirname, quantization_config=GPTQConfig(use_exllama=False, bits=4)\n-                    )\n-                    if self.device_map != \"cpu\":\n-                        quantized_model_from_saved = quantized_model_from_saved.to(0)\n-                else:\n-                    quantized_model_from_saved = AutoModelForCausalLM.from_pretrained(\n-                        tmpdirname, device_map=self.device_map\n-                    )\n+            if self.device_map == \"cpu\":\n+                quant_type = \"ipex\" if is_ipex_available() else \"torch_fused\"\n             else:\n-                if self.device_map == \"cpu\":\n-                    quant_type = \"ipex\" if is_ipex_available() else \"torch\"\n-                else:\n-                    # We expect tritonv2 to be used here, because exllama backend doesn't support packing https://github.com/ModelCloud/GPTQModel/issues/1354\n-                    # TODO: Remove this once GPTQModel exllama kernels supports packing\n-                    quant_type = \"tritonv2\"\n-                quantized_model_from_saved = AutoModelForCausalLM.from_pretrained(\n-                    tmpdirname, device_map=self.device_map\n-                )\n+                quant_type = \"exllamav2\"\n+            quantized_model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=self.device_map)\n \n             self.check_quantized_layers_type(quantized_model_from_saved, quant_type)\n             self.check_inference_correctness(quantized_model_from_saved)\n@@ -292,15 +277,15 @@ def test_change_loading_attributes(self):\n         \"\"\"\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             self.quantized_model.save_pretrained(tmpdirname)\n-            if is_auto_gptq_available() and not is_gptqmodel_available() and not self.use_exllama:\n-                self.check_quantized_layers_type(self.quantized_model, \"cuda-old\")\n-                # we need to put it directly to the gpu. Otherwise, we won't be able to initialize the exllama kernel\n-                quantized_model_from_saved = AutoModelForCausalLM.from_pretrained(\n-                    tmpdirname, quantization_config=GPTQConfig(use_exllama=True, bits=4), device_map=self.device_map\n-                )\n-                self.assertEqual(quantized_model_from_saved.config.quantization_config.bits, self.bits)\n-                self.check_quantized_layers_type(quantized_model_from_saved, \"exllama\")\n-                self.check_inference_correctness(quantized_model_from_saved)\n+            quantized_model_from_saved = AutoModelForCausalLM.from_pretrained(\n+                tmpdirname,\n+                quantization_config=GPTQConfig(bits=self.bits),\n+                device_map=self.device_map,\n+            )\n+            self.assertEqual(quantized_model_from_saved.config.quantization_config.bits, self.bits)\n+            quant_type = \"exllamav2\" if self.device_map != \"cpu\" else (\"ipex\" if is_ipex_available() else \"torch\")\n+            self.check_quantized_layers_type(quantized_model_from_saved, quant_type)\n+            self.check_inference_correctness(quantized_model_from_saved)\n \n \n @require_accelerate\n@@ -309,27 +294,25 @@ class GPTQTestDeviceMap(GPTQTestCUDA):\n     device_map = \"auto\"\n \n \n-@require_accelerate\n-@require_torch_multi_gpu\n-class GPTQTestDeviceMapExllama(GPTQTestCUDA):\n-    device_map = \"auto\"\n-    use_exllama = True\n-\n-\n @slow\n @require_optimum\n-@require_gptq\n+@require_gptqmodel\n @require_torch_gpu\n @require_accelerate\n-class GPTQTestActOrderExllama(unittest.TestCase):\n+class GPTQTestActOrderExllamaV2(unittest.TestCase):\n     \"\"\"\n-    Test GPTQ model with exllama kernel and desc_act=True (also known as act-order).\n+    Test GPTQ model with exllamav2 kernel and desc_act=True (also known as act-order).\n     More information on those arguments here:\n     https://huggingface.co/docs/transformers/main_classes/quantization#transformers.GPTQConfig\n     \"\"\"\n \n+    # `act_group_aware` == `True` requires `desc_act` == `False` when both are explicitly set\n+    desc_act = True\n+    act_group_aware = False\n+    load_backend = BACKEND.EXLLAMA_V2\n+\n     EXPECTED_OUTPUTS = set()\n-    # flaky test: gptqmodel and auto-gptq are not output equivalent nor is string compare deterministic even between transformer/torch versions\n+    # flaky test: gptqmodel kernels are not always bitwise deterministic even between transformer/torch versions\n     EXPECTED_OUTPUTS.add(\"Hello, how are you ? I'm doing good, thanks for asking.\")\n     # 4bit + act_order + 128g\n     model_name = \"hf-internal-testing/TinyLlama-1.1B-Chat-v0.3-GPTQ\"\n@@ -340,7 +323,13 @@ def setUpClass(cls):\n         \"\"\"\n         Setup quantized model\n         \"\"\"\n-        cls.quantization_config = GPTQConfig(bits=4, max_input_length=4028)\n+        cls.quantization_config = GPTQConfig(\n+            bits=4,\n+            max_input_length=4028,\n+            desc_act=cls.desc_act,\n+            act_group_aware=cls.act_group_aware,\n+            backend=cls.load_backend,\n+        )\n         cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n             cls.model_name,\n             dtype=torch.float16,\n@@ -366,35 +355,18 @@ def check_inference_correctness(self, model):\n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n \n     def test_quantized_layers_type(self):\n-        self.assertTrue(self.quantized_model.model.layers[0].self_attn.k_proj.QUANT_TYPE == \"exllama\")\n+        self.assertEqual(self.quantized_model.model.layers[0].self_attn.k_proj.QUANT_TYPE, \"exllamav2\")\n \n     def test_generate_quality(self):\n         \"\"\"\n         Simple test to check the quality of the model by comparing the generated tokens with the expected tokens\n         \"\"\"\n         self.check_inference_correctness(self.quantized_model)\n \n-    def test_max_input_length(self):\n-        \"\"\"\n-        Test if the max_input_length works. It modifies the maximum input length that of the model that runs with exllama backend.\n-        \"\"\"\n-\n-        prompt = \"I am in Paris and\" * 1000\n-        inp = self.tokenizer(prompt, return_tensors=\"pt\").to(0)\n-        self.assertTrue(inp[\"input_ids\"].shape[1] > 4028)\n-        with self.assertRaises(RuntimeError) as cm:\n-            self.quantized_model.generate(**inp, num_beams=1, min_new_tokens=3, max_new_tokens=3)\n-            self.assertTrue(\"temp_state buffer is too small\" in str(cm.exception))\n-\n-        prompt = \"I am in Paris and\"\n-        inp = self.tokenizer(prompt, return_tensors=\"pt\").to(0)\n-        self.assertTrue(inp[\"input_ids\"].shape[1] < 4028)\n-        self.quantized_model.generate(**inp, num_beams=1, min_new_tokens=3, max_new_tokens=3)\n-\n \n @slow\n @require_optimum\n-@require_gptq\n+@require_gptqmodel\n @require_torch_gpu\n @require_accelerate\n class GPTQTestExllamaV2(unittest.TestCase):\n@@ -404,8 +376,9 @@ class GPTQTestExllamaV2(unittest.TestCase):\n     https://huggingface.co/docs/transformers/main_classes/quantization#transformers.GPTQConfig\n     \"\"\"\n \n+    load_backend = BACKEND.EXLLAMA_V2\n     EXPECTED_OUTPUTS = set()\n-    # flaky test: gptqmodel and auto-gptq are not output equivalent nor is string compare deterministic even between transformer/torch versions\n+    # flaky test: gptqmodel kernels are not always bitwise deterministic even between transformer/torch versions\n     EXPECTED_OUTPUTS.add(\"Hello, how are you ? I'm doing good, thanks for asking.\")\n     # 4bit + act_order + 128g\n     model_name = \"hf-internal-testing/TinyLlama-1.1B-Chat-v0.3-GPTQ\"\n@@ -416,7 +389,7 @@ def setUpClass(cls):\n         \"\"\"\n         Setup quantized model\n         \"\"\"\n-        cls.quantization_config = GPTQConfig(bits=4, exllama_config={\"version\": 2})\n+        cls.quantization_config = GPTQConfig(bits=4, backend=cls.load_backend)\n         cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n             cls.model_name,\n             dtype=torch.float16,\n@@ -426,18 +399,10 @@ def setUpClass(cls):\n         cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name, use_fast=True)\n \n     def test_quantized_layers_type(self):\n-        if is_auto_gptq_available() and not is_gptqmodel_available():\n-            self.assertEqual(\n-                self.quantized_model.model.layers[0].self_attn.k_proj.QUANT_TYPE,\n-                \"exllamav2\",\n-            )\n-        else:\n-            # We expect tritonv2 to be used here, because exllama backend doesn't support packing https://github.com/ModelCloud/GPTQModel/issues/1354\n-            # TODO: Remove this once GPTQModel exllama kernels supports packing\n-            self.assertEqual(\n-                self.quantized_model.model.layers[0].self_attn.k_proj.QUANT_TYPE,\n-                \"tritonv2\",\n-            )\n+        self.assertEqual(\n+            self.quantized_model.model.layers[0].self_attn.k_proj.QUANT_TYPE,\n+            \"exllamav2\",\n+        )\n \n     def check_inference_correctness(self, model):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 1608,
        "additions": 295,
        "deletions": 1313
    }
}