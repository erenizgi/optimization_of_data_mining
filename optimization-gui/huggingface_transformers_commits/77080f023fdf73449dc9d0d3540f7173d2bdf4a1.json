{
    "author": "Uvi-12",
    "message": "Fixed typo in audio_classification.md (#35305)",
    "sha": "77080f023fdf73449dc9d0d3540f7173d2bdf4a1",
    "files": [
        {
            "sha": "138fed6a1c0d1dd550dd2569deeac5a8780ff69b",
            "filename": "docs/source/en/tasks/audio_classification.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/77080f023fdf73449dc9d0d3540f7173d2bdf4a1/docs%2Fsource%2Fen%2Ftasks%2Faudio_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/77080f023fdf73449dc9d0d3540f7173d2bdf4a1/docs%2Fsource%2Fen%2Ftasks%2Faudio_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Faudio_classification.md?ref=77080f023fdf73449dc9d0d3540f7173d2bdf4a1",
            "patch": "@@ -24,8 +24,8 @@ Audio classification - just like with text - assigns a class label output from t\n \n This guide will show you how to:\n \n-1. Finetune [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) on the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset to classify speaker intent.\n-2. Use your finetuned model for inference.\n+1. Fine-tune [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) on the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset to classify speaker intent.\n+2. Use your fine-tuned model for inference.\n \n <Tip>\n \n@@ -210,7 +210,7 @@ At this point, only three steps remain:\n \n 1. Define your training hyperparameters in [`TrainingArguments`]. The only required parameter is `output_dir`, which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [`Trainer`] will evaluate the accuracy and save the training checkpoint.\n 2. Pass the training arguments to [`Trainer`] along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n-3. Call [`~Trainer.train`] to finetune your model.\n+3. Call [`~Trainer.train`] to fine-tune your model.\n \n \n ```py\n@@ -252,13 +252,13 @@ Once training is completed, share your model to the Hub with the [`~transformers\n \n <Tip>\n \n-For a more in-depth example of how to finetune a model for audio classification, take a look at the corresponding [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb).\n+For a more in-depth example of how to fine-tune a model for audio classification, take a look at the corresponding [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb).\n \n </Tip>\n \n ## Inference\n \n-Great, now that you've finetuned a model, you can use it for inference!\n+Great, now that you've fine-tuned a model, you can use it for inference!\n \n Load an audio file you'd like to run inference on. Remember to resample the sampling rate of the audio file to match the sampling rate of the model if you need to!\n \n@@ -271,7 +271,7 @@ Load an audio file you'd like to run inference on. Remember to resample the samp\n >>> audio_file = dataset[0][\"audio\"][\"path\"]\n ```\n \n-The simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. Instantiate a `pipeline` for audio classification with your model, and pass your audio file to it:\n+The simplest way to try out your fine-tuned model for inference is to use it in a [`pipeline`]. Instantiate a `pipeline` for audio classification with your model, and pass your audio file to it:\n \n ```py\n >>> from transformers import pipeline"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 6,
        "deletions": 6
    }
}