{
    "author": "ducviet00",
    "message": "Fix HGNetV2 Model Card and Image Classification Pipeline Usage Tips (#39965)\n\n* fix hgnet docs and image-classification pipeline\n\n* use positional argument\n\n* fix dit close hfoptions tag\n\n* fix alphabet order\n\n* fix hgnnet modular docstring\n\n* Update hgnet_v2.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update hgnet_v2.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/hgnet_v2.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* fix: hgnet reference\n\n* change hgnet to en doc\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "692d33690855fadda046ca69ff012903d2bf9c3d",
    "files": [
        {
            "sha": "778d4255e6df7d35e0a969e2e9c423c1ded569f9",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=692d33690855fadda046ca69ff012903d2bf9c3d",
            "patch": "@@ -90,16 +90,16 @@\n       title: Tools and RAG\n     title: Chat with models\n   - sections:\n-      - local: serving\n-        title: Serving LLMs, VLMs, and other chat-based models\n-      - local: jan\n-        title: Jan\n-      - local: cursor\n-        title: Cursor\n-      - local: tiny_agents\n-        title: Tiny-Agents CLI and MCP tools\n-      - local: open_webui\n-        title: Open WebUI\n+    - local: serving\n+      title: Serving LLMs, VLMs, and other chat-based models\n+    - local: jan\n+      title: Jan\n+    - local: cursor\n+      title: Cursor\n+    - local: tiny_agents\n+      title: Tiny-Agents CLI and MCP tools\n+    - local: open_webui\n+      title: Open WebUI\n     title: Serving\n   - sections:\n     - local: perf_torch_compile\n@@ -529,8 +529,6 @@\n         title: Helium\n       - local: model_doc/herbert\n         title: HerBERT\n-      - local: model_doc/hgnet_v2\n-        title: HGNet-V2\n       - local: model_doc/ibert\n         title: I-BERT\n       - local: model_doc/jamba\n@@ -781,6 +779,8 @@\n         title: FocalNet\n       - local: model_doc/glpn\n         title: GLPN\n+      - local: model_doc/hgnet_v2\n+        title: HGNet-V2\n       - local: model_doc/hiera\n         title: Hiera\n       - local: model_doc/ijepa"
        },
        {
            "sha": "3332e832c2e1898e4ada34bdd5ce1ced7b74c3a6",
            "filename": "docs/source/en/model_doc/cvt.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2Fmodel_doc%2Fcvt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2Fmodel_doc%2Fcvt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcvt.md?ref=692d33690855fadda046ca69ff012903d2bf9c3d",
            "patch": "@@ -47,7 +47,7 @@ pipeline = pipeline(\n     torch_dtype=torch.float16,\n     device=0 \n )\n-pipeline(images=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n+pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n ```\n \n </hfoption>"
        },
        {
            "sha": "a48c8e9110052a2351aa7d0d601db27c21abca76",
            "filename": "docs/source/en/model_doc/dit.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2Fmodel_doc%2Fdit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdit.md?ref=692d33690855fadda046ca69ff012903d2bf9c3d",
            "patch": "@@ -47,7 +47,7 @@ pipeline = pipeline(\n     torch_dtype=torch.float16,\n     device=0\n )\n-pipeline(images=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dit-example.jpg\")\n+pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dit-example.jpg\")\n ```\n \n </hfoption>\n@@ -81,6 +81,7 @@ print(f\"The predicted class label is: {predicted_class_label}\")\n ```\n \n </hfoption>\n+</hfoptions>\n \n ## Notes\n "
        },
        {
            "sha": "d12a1712db9d3da2ec22fbba82763bd8b629ad89",
            "filename": "docs/source/en/model_doc/hgnet_v2.md",
            "status": "modified",
            "additions": 55,
            "deletions": 8,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md?ref=692d33690855fadda046ca69ff012903d2bf9c3d",
            "patch": "@@ -14,20 +14,67 @@ rendered properly in your Markdown viewer.\n \n -->\n \n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n # HGNet-V2\n \n-## Overview\n+[HGNetV2](https://github.com/PaddlePaddle/PaddleClas/blob/v2.6.0/docs/zh_CN/models/ImageNet1k/PP-HGNetV2.md) is a next-generation convolutional neural network (CNN) backbone built for optimal accuracy-latency tradeoff on NVIDIA GPUs. Building on the original[HGNet](https://github.com/PaddlePaddle/PaddleClas/blob/v2.6.0/docs/en/models/PP-HGNet_en.md), HGNetV2 delivers high accuracy at fast inference speeds and performs strongly on tasks like image classification, object detection, and segmentation, making it a practical choice for GPU-based computer vision applications.\n+\n+You can find all the original HGNet V2 models under the [USTC](https://huggingface.co/ustc-community/models?search=hgnet) organization.\n+\n+> [!TIP]\n+> This model was contributed by [VladOS95-cyber](https://github.com/VladOS95-cyber).\n+> Click on the HGNet V2 models in the right sidebar for more examples of how to apply HGNet V2 to different computer vision tasks.\n+\n+The example below demonstrates how to classify an image with [`Pipeline`] or the [`AutoModel`] class.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    task=\"image-classification\",\n+    model=\"ustc-community/hgnet-v2\",\n+    torch_dtype=torch.float16,\n+    device=0\n+)\n+pipeline(\"http://images.cocodataset.org/val2017/000000039769.jpg\")\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+import requests\n+from transformers import HGNetV2ForImageClassification, AutoImageProcessor\n+from PIL import Image\n+\n+url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n \n-A HGNet-V2 (High Performance GPU Net) image classification model.\n-HGNet arhtictecture was proposed in [HGNET: A Hierarchical Feature Guided Network for Occupancy Flow Field Prediction](https://huggingface.co/papers/2407.01097) by\n-Zhan Chen, Chen Tang, Lu Xiong\n+model = HGNetV2ForImageClassification.from_pretrained(\"ustc-community/hgnet-v2\")\n+processor = AutoImageProcessor.from_pretrained(\"ustc-community/hgnet-v2\")\n \n-The abstract from the HGNET paper is the following:\n+inputs = processor(images=image, return_tensors=\"pt\")\n+with torch.no_grad():\n+    logits = model(**inputs).logits\n+predicted_class_id = logits.argmax(dim=-1).item()\n \n-*Predicting the motion of multiple traffic participants has always been one of the most challenging tasks in autonomous driving. The recently proposed occupancy flow field prediction method has shown to be a more effective and scalable representation compared to general trajectory prediction methods. However, in complex multi-agent traffic scenarios, it remains difficult to model the interactions among various factors and the dependencies among prediction outputs at different time steps. In view of this, we propose a transformer-based hierarchical feature guided network (HGNET), which can efficiently extract features of agents and map information from visual and vectorized inputs, modeling multimodal interaction relationships. Second, we design the Feature-Guided Attention (FGAT) module to leverage the potential guiding effects between different prediction targets, thereby improving prediction accuracy. Additionally, to enhance the temporal consistency and causal relationships of the predictions, we propose a Time Series Memory framework to learn the conditional distribution models of the prediction outputs at future time steps from multivariate time series. The results demonstrate that our model exhibits competitive performance, which ranks 3rd in the 2024 Waymo Occupancy and Flow Prediction Challenge.*\n+class_labels = model.config.id2label\n+predicted_class_label = class_labels[predicted_class_id]\n+print(f\"The predicted class label is: {predicted_class_label}\")\n+```\n \n-This model was contributed by [VladOS95-cyber](https://github.com/VladOS95-cyber). \n-The original code can be found [here](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/backbones/hgnet_v2.py).\n+</hfoption>\n+</hfoptions>\n \n ## HGNetV2Config\n "
        },
        {
            "sha": "cd42629e401a3d878c4bee34d95a4826d54d0888",
            "filename": "docs/source/en/model_doc/mobilenet_v1.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md?ref=692d33690855fadda046ca69ff012903d2bf9c3d",
            "patch": "@@ -45,7 +45,7 @@ pipeline = pipeline(\n     torch_dtype=torch.float16,\n     device=0\n )\n-pipeline(images=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n+pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n ```\n \n </hfoption>"
        },
        {
            "sha": "a97a721d88d5a909308a292bc26eb9ebb0adc841",
            "filename": "docs/source/en/model_doc/mobilenet_v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md?ref=692d33690855fadda046ca69ff012903d2bf9c3d",
            "patch": "@@ -46,7 +46,7 @@ pipeline = pipeline(\n     torch_dtype=torch.float16,\n     device=0\n )\n-pipeline(images=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n+pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n ```\n \n </hfoption>"
        },
        {
            "sha": "2b7a7113976c6ef4346fe5fa684b4459fefa735f",
            "filename": "docs/source/en/model_doc/swin.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md?ref=692d33690855fadda046ca69ff012903d2bf9c3d",
            "patch": "@@ -45,7 +45,7 @@ pipeline = pipeline(\n     torch_dtype=torch.float16,\n     device=0\n )\n-pipeline(images=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n+pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n ```\n </hfoption>\n "
        },
        {
            "sha": "d1d3b15a7743cec1ea62426085dc4b7ab50e1246",
            "filename": "docs/source/en/model_doc/swinv2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2Fmodel_doc%2Fswinv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2Fmodel_doc%2Fswinv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fswinv2.md?ref=692d33690855fadda046ca69ff012903d2bf9c3d",
            "patch": "@@ -42,7 +42,7 @@ pipeline = pipeline(\n     torch_dtype=torch.float16,\n     device=0\n )\n-pipeline(images=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n+pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n ```\n \n </hfoption>"
        },
        {
            "sha": "d09fed4b3aeb7992a2257df87e223687074a9c83",
            "filename": "docs/source/en/model_doc/vit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/692d33690855fadda046ca69ff012903d2bf9c3d/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit.md?ref=692d33690855fadda046ca69ff012903d2bf9c3d",
            "patch": "@@ -48,7 +48,7 @@ pipeline = pipeline(\n     torch_dtype=torch.float16,\n     device=0\n )\n-pipeline(images=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n+pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n ```\n \n </hfoption>"
        },
        {
            "sha": "0baf0e7960ce6729ff5330600f46f99ed246b149",
            "filename": "src/transformers/models/hgnet_v2/modeling_hgnet_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/692d33690855fadda046ca69ff012903d2bf9c3d/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/692d33690855fadda046ca69ff012903d2bf9c3d/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py?ref=692d33690855fadda046ca69ff012903d2bf9c3d",
            "patch": "@@ -351,11 +351,11 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> from transformers import RTDetrResNetConfig, RTDetrResNetBackbone\n+        >>> from transformers import HGNetV2Config, HGNetV2Backbone\n         >>> import torch\n \n-        >>> config = RTDetrResNetConfig()\n-        >>> model = RTDetrResNetBackbone(config)\n+        >>> config = HGNetV2Config()\n+        >>> model = HGNetV2Backbone(config)\n \n         >>> pixel_values = torch.randn(1, 3, 224, 224)\n "
        },
        {
            "sha": "9ee306a27eed1e4beb0f26bedd105797766056a5",
            "filename": "src/transformers/models/hgnet_v2/modular_hgnet_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/692d33690855fadda046ca69ff012903d2bf9c3d/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/692d33690855fadda046ca69ff012903d2bf9c3d/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py?ref=692d33690855fadda046ca69ff012903d2bf9c3d",
            "patch": "@@ -474,11 +474,11 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> from transformers import RTDetrResNetConfig, RTDetrResNetBackbone\n+        >>> from transformers import HGNetV2Config, HGNetV2Backbone\n         >>> import torch\n \n-        >>> config = RTDetrResNetConfig()\n-        >>> model = RTDetrResNetBackbone(config)\n+        >>> config = HGNetV2Config()\n+        >>> model = HGNetV2Backbone(config)\n \n         >>> pixel_values = torch.randn(1, 3, 224, 224)\n "
        }
    ],
    "stats": {
        "total": 114,
        "additions": 81,
        "deletions": 33
    }
}