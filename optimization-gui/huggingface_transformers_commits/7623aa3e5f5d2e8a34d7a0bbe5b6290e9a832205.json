{
    "author": "ebezzam",
    "message": "Fix `Qwen2AudioForConditionalGeneration.forward()` and `test_flash_attn_kernels_inference_equivalence` (#39503)\n\n* Add missing cache_position argument.\n\n* Pass cache_position to language model.\n\n* Overwrite prepare_inputs_for_generation.\n\n* Set model to half precision for Flash Attention test.\n\n* Cast model to bfloat16.",
    "sha": "7623aa3e5f5d2e8a34d7a0bbe5b6290e9a832205",
    "files": [
        {
            "sha": "b4d1f41f3ec2bdf487048cd532d3ab5c3be1ac42",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 16,
            "deletions": 1,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/7623aa3e5f5d2e8a34d7a0bbe5b6290e9a832205/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7623aa3e5f5d2e8a34d7a0bbe5b6290e9a832205/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=7623aa3e5f5d2e8a34d7a0bbe5b6290e9a832205",
            "patch": "@@ -19,7 +19,6 @@\n from typing import Callable, Optional, Union\n \n import torch\n-import torch.utils.checkpoint\n from torch import nn\n \n from ...activations import ACT2FN\n@@ -727,6 +726,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[tuple, Qwen2AudioCausalLMOutputWithPast]:\n         r\"\"\"\n         feature_attention_mask (`torch.Tensor` of shape `(batch_size, feature_sequence_length)`):\n@@ -845,6 +845,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         logits = outputs[0]\n@@ -878,5 +879,19 @@ def forward(\n             attention_mask=attention_mask,\n         )\n \n+    def prepare_inputs_for_generation(self, *args, **kwargs):\n+        # Overwritten -- we should not pass input_features when we are in cached decoding stage\n+\n+        input_features = kwargs.pop(\"input_features\", None)\n+        cache_position = kwargs.get(\"cache_position\")\n+\n+        model_inputs = super().prepare_inputs_for_generation(*args, **kwargs)\n+\n+        if cache_position is not None and cache_position[0] == 0:\n+            # input_features should only be passed when we are not in cached decoding stage\n+            model_inputs[\"input_features\"] = input_features\n+\n+        return model_inputs\n+\n \n __all__ = [\"Qwen2AudioForConditionalGeneration\", \"Qwen2AudioPreTrainedModel\", \"Qwen2AudioEncoder\"]"
        },
        {
            "sha": "4533fbbf99d81fa25d58b0f4025534a555d7a21d",
            "filename": "tests/models/qwen2_audio/test_modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7623aa3e5f5d2e8a34d7a0bbe5b6290e9a832205/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7623aa3e5f5d2e8a34d7a0bbe5b6290e9a832205/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py?ref=7623aa3e5f5d2e8a34d7a0bbe5b6290e9a832205",
            "patch": "@@ -34,6 +34,7 @@\n     torch_device,\n )\n \n+from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n \n@@ -132,14 +133,12 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class Qwen2AudioForConditionalGenerationModelTest(ModelTesterMixin, unittest.TestCase):\n+class Qwen2AudioForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     \"\"\"\n     Model tester for `Qwen2AudioForConditionalGeneration`.\n     \"\"\"\n \n     all_model_classes = (Qwen2AudioForConditionalGeneration,) if is_torch_available() else ()\n-    # Doesn't run generation tests. TODO eustache/joao: some generation tests are broken, the errors seem cache-related\n-    all_generative_model_classes = ()\n     test_pruning = False\n     test_head_masking = False\n     _is_composite = True"
        },
        {
            "sha": "9454f4b4e5224b37d98721bd1a0c8c55af51d4ee",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7623aa3e5f5d2e8a34d7a0bbe5b6290e9a832205/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7623aa3e5f5d2e8a34d7a0bbe5b6290e9a832205/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=7623aa3e5f5d2e8a34d7a0bbe5b6290e9a832205",
            "patch": "@@ -3484,6 +3484,7 @@ def flash_attn_inference_equivalence(self, attn_implementation: str, padding_sid\n             model = model_class(config)\n \n             model.to(torch_device)\n+            model.to(torch.bfloat16)\n             dummy_input = inputs_dict[model.main_input_name][:1]\n             if dummy_input.dtype in [torch.float32, torch.float16]:\n                 dummy_input = dummy_input.to(torch.bfloat16)"
        }
    ],
    "stats": {
        "total": 23,
        "additions": 19,
        "deletions": 4
    }
}