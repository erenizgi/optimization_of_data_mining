{
    "author": "yao-matrix",
    "message": "fix asr ut failures (#41332)\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>",
    "sha": "73f8c4b8ad5df9fad09e010eb989002ccd02053b",
    "files": [
        {
            "sha": "ce150f790051ce9dfea794de2f122c63405252cb",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/73f8c4b8ad5df9fad09e010eb989002ccd02053b/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73f8c4b8ad5df9fad09e010eb989002ccd02053b/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=73f8c4b8ad5df9fad09e010eb989002ccd02053b",
            "patch": "@@ -1915,7 +1915,7 @@ def __init__(self, suppress_tokens, device: str = \"cpu\"):\n     @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n         vocab_tensor = torch.arange(scores.shape[-1], device=scores.device)\n-        suppress_token_mask = isin_mps_friendly(vocab_tensor, self.suppress_tokens)\n+        suppress_token_mask = isin_mps_friendly(vocab_tensor, self.suppress_tokens.to(scores.device))\n         scores = torch.where(suppress_token_mask, -float(\"inf\"), scores)\n         return scores\n "
        },
        {
            "sha": "9b55e52e6fa03478e65c0a01034123364cbab55e",
            "filename": "tests/pipelines/test_pipelines_automatic_speech_recognition.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/73f8c4b8ad5df9fad09e010eb989002ccd02053b/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73f8c4b8ad5df9fad09e010eb989002ccd02053b/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py?ref=73f8c4b8ad5df9fad09e010eb989002ccd02053b",
            "patch": "@@ -1104,7 +1104,7 @@ def test_whisper_language(self):\n     def test_speculative_decoding_whisper_non_distil(self):\n         # Load data:\n         dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:1]\")\n-        sample = dataset[0][\"audio\"]\n+        sample = dataset[0][\"audio\"].get_all_samples().data\n \n         # Load model:\n         model_id = \"openai/whisper-large-v2\"\n@@ -1133,8 +1133,8 @@ def test_speculative_decoding_whisper_non_distil(self):\n             num_beams=1,\n         )\n \n-        transcription_non_ass = pipe(sample.copy(), generate_kwargs={\"assistant_model\": assistant_model})[\"text\"]\n-        transcription_ass = pipe(sample)[\"text\"]\n+        transcription_ass = pipe(sample.clone().detach(), generate_kwargs={\"assistant_model\": assistant_model})[\"text\"]\n+        transcription_non_ass = pipe(sample)[\"text\"]\n \n         self.assertEqual(transcription_ass, transcription_non_ass)\n         self.assertEqual(\n@@ -1422,13 +1422,13 @@ def test_whisper_prompted(self):\n         )\n \n         dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n-        sample = dataset[0][\"audio\"]\n+        sample = dataset[0][\"audio\"].get_all_samples().data\n \n         # prompt the model to misspell \"Mr Quilter\" as \"Mr Quillter\"\n         whisper_prompt = \"Mr. Quillter.\"\n         prompt_ids = pipe.tokenizer.get_prompt_ids(whisper_prompt, return_tensors=\"pt\").to(torch_device)\n \n-        unprompted_result = pipe(sample.copy())[\"text\"]\n+        unprompted_result = pipe(sample.clone().detach())[\"text\"]\n         prompted_result = pipe(sample, generate_kwargs={\"prompt_ids\": prompt_ids})[\"text\"]\n \n         # fmt: off"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 6,
        "deletions": 6
    }
}