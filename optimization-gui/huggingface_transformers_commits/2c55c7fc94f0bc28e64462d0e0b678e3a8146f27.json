{
    "author": "Cyrilvallez",
    "message": "Reactivate a lot of tests skipped for no reason anymore (#40378)\n\n* reactivate all the tests\n\n* some tests still failing",
    "sha": "2c55c7fc94f0bc28e64462d0e0b678e3a8146f27",
    "files": [
        {
            "sha": "537bbd530f28aef79b47d1d9d9c57039257f787c",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=2c55c7fc94f0bc28e64462d0e0b678e3a8146f27",
            "patch": "@@ -113,7 +113,7 @@ class PaliGemmaPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"PaliGemmaMultiModalProjector\"]\n     _skip_keys_device_placement = \"past_key_values\"\n \n-    _can_compile_fullgraph = True\n+    _can_compile_fullgraph = False\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "2e7a807748c58a100356ccc49a5ff83e93bf719c",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 64,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=2c55c7fc94f0bc28e64462d0e0b678e3a8146f27",
            "patch": "@@ -16,7 +16,6 @@\n import unittest\n \n import pytest\n-from parameterized import parameterized\n \n from transformers import (\n     AutoProcessor,\n@@ -183,69 +182,6 @@ def setUp(self):\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n-    @unittest.skip(\"Failing because of unique cache (HybridCache)\")\n-    def test_model_outputs_equivalence(self, **kwargs):\n-        pass\n-\n-    @unittest.skip(\"Cohere2's forcefully disables sdpa due to softcapping\")\n-    def test_sdpa_can_dispatch_non_composite_models(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2's eager attn/sdpa attn outputs are expected to be different\")\n-    def test_eager_matches_sdpa_generate(self):\n-        pass\n-\n-    @parameterized.expand([(\"random\",), (\"same\",)])\n-    @pytest.mark.generate\n-    @unittest.skip(\"Cohere2 has HybridCache which is not compatible with assisted decoding\")\n-    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache which is not compatible with assisted decoding\")\n-    def test_prompt_lookup_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n-    @pytest.mark.generate\n-    @unittest.skip(\"Cohere2 has HybridCache which is not compatible with assisted decoding\")\n-    def test_assisted_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache which is not compatible with dola decoding\")\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache and doesn't support continue from past kv\")\n-    def test_generate_continue_from_past_key_values(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache and doesn't support low_memory generation\")\n-    def test_beam_search_low_memory(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_low_memory(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n-    def test_generate_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Failing because of unique cache (HybridCache)\")\n-    def test_multi_gpu_data_parallel_forward(self):\n-        pass\n-\n     @unittest.skip(reason=\"SiglipVisionModel does not support standalone training\")\n     def test_training(self):\n         pass"
        },
        {
            "sha": "e7a7cc5540aa82b8c7883a4278745157769527d4",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 66,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=2c55c7fc94f0bc28e64462d0e0b678e3a8146f27",
            "patch": "@@ -71,65 +71,6 @@ def setUp(self):\n         self.model_tester = Cohere2ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Cohere2Config, hidden_size=37)\n \n-    @unittest.skip(\"Failing because of unique cache (HybridCache)\")\n-    def test_model_outputs_equivalence(self, **kwargs):\n-        pass\n-\n-    @unittest.skip(\"Cohere2's forcefully disables sdpa due to softcapping\")\n-    def test_sdpa_can_dispatch_non_composite_models(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2's eager attn/sdpa attn outputs are expected to be different\")\n-    def test_eager_matches_sdpa_generate(self):\n-        pass\n-\n-    @parameterized.expand([(\"random\",), (\"same\",)])\n-    @pytest.mark.generate\n-    @unittest.skip(\"Cohere2 has HybridCache which is not compatible with assisted decoding\")\n-    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache which is not compatible with assisted decoding\")\n-    def test_prompt_lookup_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n-    @pytest.mark.generate\n-    @unittest.skip(\"Cohere2 has HybridCache which is not compatible with assisted decoding\")\n-    def test_assisted_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache which is not compatible with dola decoding\")\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache and doesn't support continue from past kv\")\n-    def test_generate_continue_from_past_key_values(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_low_memory(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n-    def test_generate_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Cohere2 has HybridCache and doesn't support progressive generation using input embeds.\")\n-    def test_generate_continue_from_inputs_embeds(self):\n-        pass\n-\n \n @slow\n @require_read_token\n@@ -287,18 +228,12 @@ def test_export_static_cache(self):\n     @require_read_token\n     def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         \"\"\"Test that we can correctly generate beyond the sliding window. This is non trivial as\n-        we need to correctly slice the attention mask in all cases (because we use a HybridCache).\n+        we need to correctly slice the attention mask in all cases (because we use a hybrid cache).\n         Outputs for every attention functions should be coherent and identical.\n         \"\"\"\n         if attn_implementation == \"flash_attention_2\" and not is_flash_attn_2_available():\n             self.skipTest(\"FlashAttention2 is required for this test.\")\n \n-        # TODO: if we can specify not to compile when `flex` attention is used?\n-        if attn_implementation == \"flex_attention\":\n-            self.skipTest(\n-                \"Flex attention will compile (see `compile_friendly_flex_attention`) which causes triton issue.\"\n-            )\n-\n         if torch_device == \"xpu\" and attn_implementation == \"flash_attention_2\":\n             self.skipTest(reason=\"Intel XPU doesn't support falsh_attention_2 as of now.\")\n "
        },
        {
            "sha": "03c3934555010ec827da2b73e9d76f303afaba06",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 31,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=2c55c7fc94f0bc28e64462d0e0b678e3a8146f27",
            "patch": "@@ -247,53 +247,27 @@ def setUp(self):\n         self.model_tester = DeepseekV3ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=DeepseekV3Config, hidden_size=37)\n \n-    @unittest.skip(\"Failing because of unique cache (HybridCache)\")\n-    def test_model_outputs_equivalence(self, **kwargs):\n-        pass\n-\n     @parameterized.expand([(\"random\",), (\"same\",)])\n-    @unittest.skip(\"DeepseekV3 has HybridCache which is not compatible with assisted decoding\")\n+    @unittest.skip(\"DeepseekV3 is not compatible with assisted decoding\")\n     def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n         pass\n \n-    @unittest.skip(\"DeepseekV3 has HybridCache which is not compatible with assisted decoding\")\n+    @unittest.skip(\"DeepseekV3 is not compatible with assisted decoding\")\n     def test_prompt_lookup_decoding_matches_greedy_search(self, assistant_type):\n         pass\n \n-    @unittest.skip(\"DeepseekV3 has HybridCache which is not compatible with assisted decoding\")\n+    @unittest.skip(\"DeepseekV3 is not compatible with assisted decoding\")\n     def test_assisted_decoding_sample(self):\n         pass\n \n-    @unittest.skip(\"DeepseekV3 has HybridCache which is not compatible with dola decoding\")\n+    @unittest.skip(\"DeepseekV3 is not compatible with dola decoding\")\n     def test_dola_decoding_sample(self):\n         pass\n \n-    @unittest.skip(\"DeepseekV3 has HybridCache and doesn't support continue from past kv\")\n-    def test_generate_continue_from_past_key_values(self):\n-        pass\n-\n-    @unittest.skip(\"DeepseekV3 has HybridCache and doesn't support low_memory generation\")\n-    def test_beam_search_low_memory(self):\n-        pass\n-\n-    @unittest.skip(\"DeepseekV3 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate(self):\n-        pass\n-\n-    @unittest.skip(\"DeepseekV3 has HybridCache and doesn't support contrastive generation\")\n+    @unittest.skip(\"DeepseekV3 doesn't support contrastive generation\")\n     def test_contrastive_generate_dict_outputs_use_cache(self):\n         pass\n \n-    @unittest.skip(\"DeepseekV3 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_low_memory(self):\n-        pass\n-\n-    @unittest.skip(\n-        \"DeepseekV3 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\"\n-    )\n-    def test_generate_continue_from_inputs_embeds(self):\n-        pass\n-\n     @unittest.skip(\"Deepseek-V3 uses MLA so it is not compatible with the standard cache format\")\n     def test_beam_search_generate_dict_outputs_use_cache(self):\n         pass"
        },
        {
            "sha": "1045c025b159f587b27b4e56640eff5dad579fc0",
            "filename": "tests/models/exaone4/test_modeling_exaone4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 73,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py?ref=2c55c7fc94f0bc28e64462d0e0b678e3a8146f27",
            "patch": "@@ -18,7 +18,6 @@\n \n import pytest\n from packaging import version\n-from parameterized import parameterized\n \n from transformers import (\n     AutoTokenizer,\n@@ -96,78 +95,6 @@ def setUp(self):\n         self.model_tester = Exaone4ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Exaone4Config, hidden_size=37)\n \n-    @unittest.skip(\"Failing because of unique cache (HybridCache)\")\n-    def test_model_outputs_equivalence(self, **kwargs):\n-        pass\n-\n-    @parameterized.expand([(\"random\",), (\"same\",)])\n-    @pytest.mark.generate\n-    @unittest.skip(\"EXAONE 4.0 has HybridCache which is not compatible with assisted decoding\")\n-    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n-    @unittest.skip(\"EXAONE 4.0 has HybridCache which is not compatible with assisted decoding\")\n-    def test_prompt_lookup_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n-    @pytest.mark.generate\n-    @unittest.skip(\"EXAONE 4.0 has HybridCache which is not compatible with assisted decoding\")\n-    def test_assisted_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"EXAONE 4.0 has HybridCache which is not compatible with dola decoding\")\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"EXAONE 4.0 has HybridCache and doesn't support continue from past kv\")\n-    def test_generate_continue_from_past_key_values(self):\n-        pass\n-\n-    @unittest.skip(\"EXAONE 4.0 has HybridCache and doesn't support low_memory generation\")\n-    def test_beam_search_low_memory(self):\n-        pass\n-\n-    @unittest.skip(\"EXAONE 4.0 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate(self):\n-        pass\n-\n-    @unittest.skip(\"EXAONE 4.0 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(\"EXAONE 4.0 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_low_memory(self):\n-        pass\n-\n-    @unittest.skip(\n-        \"EXAONE 4.0 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\"\n-    )\n-    def test_generate_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\n-        \"EXAONE 4.0 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\"\n-    )\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\n-        \"EXAONE 4.0 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\"\n-    )\n-    def test_generate_continue_from_inputs_embeds(self):\n-        pass\n-\n-    @unittest.skip(\"EXAONE 4.0 has HybridCache which auto-compiles. Compile and FA2 don't work together.\")\n-    def test_eager_matches_fa2_generate(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"HybridCache can't be gathered because it is not iterable. Adding a simple iter and dumping `distributed_iterator`\"\n-        \" as in Dynamic Cache doesnt work. NOTE: @gante all cache objects would need better compatibility with multi gpu setting\"\n-    )\n-    def test_multi_gpu_data_parallel_forward(self):\n-        pass\n-\n \n @require_torch\n class Exaone4IntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "59921594d691ab2bac0a4446fda478ecedb219ab",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 76,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=2c55c7fc94f0bc28e64462d0e0b678e3a8146f27",
            "patch": "@@ -102,80 +102,6 @@ def setUp(self):\n         self.model_tester = Gemma2ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Gemma2Config, hidden_size=37)\n \n-    @unittest.skip(\"Failing because of unique cache (HybridCache)\")\n-    def test_model_outputs_equivalence(self, **kwargs):\n-        pass\n-\n-    @unittest.skip(\"Gemma2's forcefully disables sdpa due to softcapping\")\n-    def test_sdpa_can_dispatch_non_composite_models(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma2's eager attn/sdpa attn outputs are expected to be different\")\n-    def test_eager_matches_sdpa_generate(self):\n-        pass\n-\n-    @parameterized.expand([(\"random\",), (\"same\",)])\n-    @pytest.mark.generate\n-    @unittest.skip(\"Gemma2 has HybridCache which is not compatible with assisted decoding\")\n-    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n-    @unittest.skip(\"Gemma2 has HybridCache which is not compatible with assisted decoding\")\n-    def test_prompt_lookup_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n-    @pytest.mark.generate\n-    @unittest.skip(\"Gemma2 has HybridCache which is not compatible with assisted decoding\")\n-    def test_assisted_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma2 has HybridCache which is not compatible with dola decoding\")\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma2 has HybridCache and doesn't support continue from past kv\")\n-    def test_generate_continue_from_past_key_values(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma2 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma2 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma2 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_low_memory(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma2 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n-    def test_generate_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma2 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma2 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n-    def test_generate_continue_from_inputs_embeds(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"HybridCache can't be gathered because it is not iterable. Adding a simple iter and dumping `distributed_iterator`\"\n-        \" as in Dynamic Cache doesn't work. NOTE: @gante all cache objects would need better compatibility with multi gpu setting\"\n-    )\n-    def test_multi_gpu_data_parallel_forward(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma2 has HybridCache which auto-compiles. Compile and FA2 don't work together.\")\n-    def test_eager_matches_fa2_generate(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma2 eager/FA2 attention outputs are expected to be different\")\n-    def test_flash_attn_2_equivalence(self):\n-        pass\n-\n \n @slow\n @require_torch_accelerator\n@@ -392,7 +318,7 @@ def test_export_hybrid_cache(self):\n         model = AutoModelForCausalLM.from_pretrained(model_id)\n         self.assertEqual(model.config.cache_implementation, \"hybrid\")\n \n-        # Export + HybridCache\n+        # Export + hybrid cache\n         model.eval()\n         exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n         exported_program = exportable_module.export(\n@@ -445,7 +371,7 @@ def test_model_9b_bf16_flex_attention(self):\n     @require_read_token\n     def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         \"\"\"Test that we can correctly generate beyond the sliding window. This is non trivial as\n-        we need to correctly slice the attention mask in all cases (because we use a HybridCache).\n+        we need to correctly slice the attention mask in all cases (because we use a hybrid cache).\n         Outputs for every attention functions should be coherent and identical.\n         \"\"\"\n         if attn_implementation == \"flash_attention_2\" and not is_flash_attn_2_available():"
        },
        {
            "sha": "b19390381efd7f7d89281d97da780f2a9f379212",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 119,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=2c55c7fc94f0bc28e64462d0e0b678e3a8146f27",
            "patch": "@@ -81,68 +81,6 @@ def setUp(self):\n         self.model_tester = Gemma3ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Gemma3Config, hidden_size=37)\n \n-    @unittest.skip(\"Failing because of unique cache (HybridCache)\")\n-    def test_model_outputs_equivalence(self, **kwargs):\n-        pass\n-\n-    @parameterized.expand([(\"random\",), (\"same\",)])\n-    @pytest.mark.generate\n-    @unittest.skip(\"Gemma3 has HybridCache which is not compatible with assisted decoding\")\n-    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache which is not compatible with assisted decoding\")\n-    def test_prompt_lookup_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n-    @pytest.mark.generate\n-    @unittest.skip(\"Gemma3 has HybridCache which is not compatible with assisted decoding\")\n-    def test_assisted_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache which is not compatible with dola decoding\")\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache and doesn't support continue from past kv\")\n-    def test_generate_continue_from_past_key_values(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache and doesn't support low_memory generation\")\n-    def test_beam_search_low_memory(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_low_memory(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n-    def test_generate_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache which auto-compiles. Compile and FA2 don't work together.\")\n-    def test_eager_matches_fa2_generate(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"HybridCache can't be gathered because it is not iterable. Adding a simple iter and dumping `distributed_iterator`\"\n-        \" as in Dynamic Cache doesn't work. NOTE: @gante all cache objects would need better compatibility with multi gpu setting\"\n-    )\n-    def test_multi_gpu_data_parallel_forward(self):\n-        pass\n-\n     @unittest.skip(\"Gemma3 applies key/query norm which doesn't work with packing\")\n     def test_eager_padding_matches_padding_free_with_position_ids(self):\n         pass\n@@ -375,68 +313,17 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip(\n-        reason=\"HybridCache can't be gathered because it is not iterable. Adding a simple iter and dumping `distributed_iterator`\"\n-        \" as in Dynamic Cache doesn't work. NOTE: @gante all cache objects would need better compatibility with multi gpu setting\"\n-    )\n-    def test_multi_gpu_data_parallel_forward(self):\n-        pass\n-\n-    @unittest.skip(\"Failing because of unique cache (HybridCache)\")\n-    def test_model_outputs_equivalence(self, **kwargs):\n-        pass\n-\n     @parameterized.expand([(\"random\",), (\"same\",)])\n     @pytest.mark.generate\n-    @unittest.skip(\"Gemma3 has HybridCache which is not compatible with assisted decoding\")\n+    @unittest.skip(\"Gemma3 does not seem to be compatible with assisted decoding\")\n     def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n         pass\n \n-    @unittest.skip(\"Gemma3 has HybridCache which is not compatible with assisted decoding\")\n-    def test_prompt_lookup_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n     @pytest.mark.generate\n-    @unittest.skip(\"Gemma3 has HybridCache which is not compatible with assisted decoding\")\n+    @unittest.skip(\"Gemma3 does not seem to be compatible with assisted decoding\")\n     def test_assisted_decoding_sample(self):\n         pass\n \n-    @unittest.skip(\"Gemma3 has HybridCache which is not compatible with dola decoding\")\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache and doesn't support continue from past kv\")\n-    def test_generate_continue_from_past_key_values(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache and doesn't support low_memory generation\")\n-    def test_beam_search_low_memory(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_low_memory(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n-    def test_generate_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3 has HybridCache which auto-compiles. Compile and FA2 don't work together.\")\n-    def test_eager_matches_fa2_generate(self):\n-        pass\n-\n     @unittest.skip(\n         reason=\"Siglip (vision backbone) uses the same initialization scheme as the Flax original implementation\"\n     )\n@@ -787,7 +674,7 @@ def test_model_4b_flash_attn(self):\n     @parameterized.expand([(\"flash_attention_2\",), (\"sdpa\",), (\"eager\",)])\n     def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         \"\"\"Test that we can correctly generate beyond the sliding window. This is non trivial as\n-        we need to correctly slice the attention mask in all cases (because we use a HybridCache).\n+        we need to correctly slice the attention mask in all cases (because we use a hybrid cache).\n         Outputs for every attention functions should be coherent and identical.\n         \"\"\"\n         model_id = \"google/gemma-3-1b-it\"\n@@ -810,8 +697,7 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         input_size = inputs.input_ids.shape[-1]\n         self.assertTrue(input_size > model.config.sliding_window)\n \n-        # cache_implementation=\"hybrid\" an in the original transformers implementation\n-        out = model.generate(**inputs, max_new_tokens=20, do_sample=False, cache_implementation=\"hybrid\")[\n+        out = model.generate(**inputs, max_new_tokens=20, do_sample=False, cache_implementation=\"static\")[\n             :, input_size:\n         ]\n         output_text = tokenizer.batch_decode(out)\n@@ -830,7 +716,7 @@ def test_export_text_only_with_hybrid_cache(self):\n         model = AutoModelForCausalLM.from_pretrained(model_id)\n         self.assertEqual(model.config.cache_implementation, \"hybrid\")\n \n-        # Export + HybridCache\n+        # Export + hybrid cache\n         model.eval()\n         exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n         exported_program = exportable_module.export("
        },
        {
            "sha": "11aa9a9bc45bfa01596991de5596fd3bbc54ba8b",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 61,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=2c55c7fc94f0bc28e64462d0e0b678e3a8146f27",
            "patch": "@@ -357,8 +357,6 @@ def _check_hidden_states_for_generate(\n \n         # When `output_hidden_states=True`, each iteration of generate appends the hidden states corresponding to the\n         # new token(s)\n-        # NOTE: `HybridCache` may have different lengths on different layers, if this test starts failing add more\n-        # elaborate checks\n         for generated_length, iter_hidden_states in enumerate(hidden_states):\n             # regardless of using cache, the first forward pass will have the full prompt as input\n             if use_cache and generated_length > 0:\n@@ -582,64 +580,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip(\n-        reason=\"HybridCache can't be gathered because it is not iterable. Adding a simple iter and dumping `distributed_iterator`\"\n-        \" as in Dynamic Cache doesnt work. NOTE: @gante all cache objects would need better compatibility with multi gpu setting\"\n-    )\n-    def test_multi_gpu_data_parallel_forward(self):\n-        pass\n-\n-    @unittest.skip(\"Failing because of unique cache (HybridCache)\")\n-    def test_model_outputs_equivalence(self, **kwargs):\n-        pass\n-\n-    @parameterized.expand([(\"random\",), (\"same\",)])\n-    @pytest.mark.generate\n-    @unittest.skip(\"Gemma3n has HybridCache which is not compatible with assisted decoding\")\n-    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n-    @unittest.skip(\"Gemma3n has HybridCache which is not compatible with assisted decoding\")\n-    def test_prompt_lookup_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n-    @pytest.mark.generate\n-    @unittest.skip(\"Gemma3n has HybridCache which is not compatible with assisted decoding\")\n-    def test_assisted_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3n has HybridCache which is not compatible with dola decoding\")\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3n has HybridCache and doesn't support continue from past kv\")\n-    def test_generate_continue_from_past_key_values(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3n has HybridCache and doesn't support low_memory generation\")\n-    def test_beam_search_low_memory(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3n has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3n has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3n has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_low_memory(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3n has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n-    def test_generate_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma3n has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n     @unittest.skip(\n         reason=\"Siglip (vision backbone) uses the same initialization scheme as the Flax original implementation\"\n     )\n@@ -913,7 +853,7 @@ def test_model_4b_flash_attn(self):\n     @parameterized.expand([(\"flash_attention_2\",), (\"sdpa\",), (\"eager\",)])\n     def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         \"\"\"Test that we can correctly generate beyond the sliding window. This is non trivial as\n-        we need to correctly slice the attention mask in all cases (because we use a HybridCache).\n+        we need to correctly slice the attention mask in all cases (because we use a hybrid cache).\n         Outputs for every attention functions should be coherent and identical.\n         \"\"\"\n         model_id = \"google/gemma-3-1b-it\""
        },
        {
            "sha": "02e245bbf75ba24dce9fa53fb9f15cbb317af61e",
            "filename": "tests/models/gpt_oss/test_modeling_gpt_oss.py",
            "status": "modified",
            "additions": 0,
            "deletions": 63,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py?ref=2c55c7fc94f0bc28e64462d0e0b678e3a8146f27",
            "patch": "@@ -22,7 +22,6 @@\n import unittest\n from pathlib import Path\n \n-import pytest\n from parameterized import parameterized\n \n from transformers import (\n@@ -105,10 +104,6 @@ def setUp(self):\n         self.model_tester = GptOssModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=GptOssConfig, hidden_size=37)\n \n-    @unittest.skip(\"Failing because of unique cache (HybridCache)\")\n-    def test_model_outputs_equivalence(self, **kwargs):\n-        pass\n-\n     @unittest.skip(\"GptOss's forcefully disables sdpa due to Sink\")\n     def test_sdpa_can_dispatch_non_composite_models(self):\n         pass\n@@ -117,64 +112,6 @@ def test_sdpa_can_dispatch_non_composite_models(self):\n     def test_eager_matches_sdpa_generate(self):\n         pass\n \n-    @parameterized.expand([(\"random\",), (\"same\",)])\n-    @pytest.mark.generate\n-    @unittest.skip(\"GptOss has HybridCache which is not compatible with assisted decoding\")\n-    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n-    @unittest.skip(\"GptOss has HybridCache which is not compatible with assisted decoding\")\n-    def test_prompt_lookup_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n-    @pytest.mark.generate\n-    @unittest.skip(\"GptOss has HybridCache which is not compatible with assisted decoding\")\n-    def test_assisted_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"GptOss has HybridCache which is not compatible with dola decoding\")\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"GptOss has HybridCache and doesn't support continue from past kv\")\n-    def test_generate_continue_from_past_key_values(self):\n-        pass\n-\n-    @unittest.skip(\"GptOss has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate(self):\n-        pass\n-\n-    @unittest.skip(\"GptOss has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(\"GptOss has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_low_memory(self):\n-        pass\n-\n-    @unittest.skip(\"GptOss has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n-    def test_generate_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\"GptOss has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\"GptOss has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n-    def test_generate_continue_from_inputs_embeds(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"HybridCache can't be gathered because it is not iterable. Adding a simple iter and dumping `distributed_iterator`\"\n-        \" as in Dynamic Cache doesn't work. NOTE: @gante all cache objects would need better compatibility with multi gpu setting\"\n-    )\n-    def test_multi_gpu_data_parallel_forward(self):\n-        pass\n-\n-    @unittest.skip(\"GptOss has HybridCache which auto-compiles. Compile and FA2 don't work together.\")\n-    def test_eager_matches_fa2_generate(self):\n-        pass\n-\n     @unittest.skip(\"GptOss eager/FA2 attention outputs are expected to be different\")\n     def test_flash_attn_2_equivalence(self):\n         pass"
        },
        {
            "sha": "f4c211a5a6c5ccf9af1804615b8db2c3de83984e",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 34,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c55c7fc94f0bc28e64462d0e0b678e3a8146f27/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=2c55c7fc94f0bc28e64462d0e0b678e3a8146f27",
            "patch": "@@ -279,43 +279,10 @@ def test_beam_search_low_memory(self):\n \n     @parameterized.expand([(\"random\",), (\"same\",)])\n     @pytest.mark.generate\n-    @unittest.skip(\"Gemma2 has HybridCache which is not compatible with assisted decoding\")\n+    @unittest.skip(\"Paligemma2 does not seem to be compatible with assisted decoding\")\n     def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n         pass\n \n-    @unittest.skip(\"Gemma2 has HybridCache which is not compatible with assisted decoding\")\n-    def test_prompt_lookup_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n-    @pytest.mark.generate\n-    @unittest.skip(\"Gemma2 has HybridCache which is not compatible with assisted decoding\")\n-    def test_assisted_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma2 has HybridCache which is not compatible with dola decoding\")\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma2 has HybridCache and doesn't support continue from past kv\")\n-    def test_generate_continue_from_past_key_values(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma2 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma2 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma2 has HybridCache and doesn't support contrastive generation\")\n-    def test_contrastive_generate_low_memory(self):\n-        pass\n-\n-    @unittest.skip(\"Gemma2 has HybridCache and doesn't support StaticCache\")\n-    def test_generate_with_static_cache(self):\n-        pass\n-\n     @unittest.skip(\"Paligemma position ids are 1 indexed\")\n     def test_eager_padding_matches_padding_free_with_position_ids(self):\n         pass"
        }
    ],
    "stats": {
        "total": 604,
        "additions": 16,
        "deletions": 588
    }
}