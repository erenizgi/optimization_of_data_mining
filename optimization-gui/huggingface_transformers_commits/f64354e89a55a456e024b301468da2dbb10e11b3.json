{
    "author": "cyyever",
    "message": "Format empty lines and white space in markdown files. (#41100)\n\n* Remove additional white space and empty lines from markdown files\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Add empty lines around code\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "f64354e89a55a456e024b301468da2dbb10e11b3",
    "files": [
        {
            "sha": "c87bd9fc2c3fff371e714e2ae02403bb55fe4637",
            "filename": "ISSUES.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/ISSUES.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/ISSUES.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/ISSUES.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -38,7 +38,6 @@ In particular all \"Please explain\" questions or objectively very user-specific f\n \n * \"How to train T5 on De->En translation?\"\n \n-\n ## The GitHub Issues\n \n Everything which hints at a bug should be opened as an [issue](https://github.com/huggingface/transformers/issues).\n@@ -247,7 +246,6 @@ You are not required to read the following guidelines before opening an issue. H\n \n     Try not use italics and bold text too much as these often make the text more difficult to read.\n \n-\n 12. If you are cross-referencing a specific comment in a given thread or another issue, always link to that specific comment, rather than using the issue link. If you do the latter it could be quite impossible to find which specific comment you're referring to.\n \n     To get the link to the specific comment do not copy the url from the location bar of your browser, but instead, click the `...` icon in the upper right corner of the comment and then select \"Copy Link\".\n@@ -257,7 +255,6 @@ You are not required to read the following guidelines before opening an issue. H\n     1. https://github.com/huggingface/transformers/issues/9257\n     2. https://github.com/huggingface/transformers/issues/9257#issuecomment-749945162\n \n-\n 13. If you are replying to a last comment, it's totally fine to make your reply with just your comment in it. The readers can follow the information flow here.\n \n     But if you're replying to a comment that happened some comments back it's always a good practice to quote just the relevant lines you're replying it. The `>` is used for quoting, or you can always use the menu to do so. For example your editor box will look like:"
        },
        {
            "sha": "8b09a84f29e7d1a686dcd4745b9ae0ab76426027",
            "filename": "README.md",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/README.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/README.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/README.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -63,12 +63,11 @@ limitations under the License.\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_as_a_model_definition.png\"/>\n </h3>\n \n+Transformers acts as the model-definition framework for state-of-the-art machine learning models in text, computer\n+vision, audio, video, and multimodal model, for both inference and training.\n \n-Transformers acts as the model-definition framework for state-of-the-art machine learning models in text, computer \n-vision, audio, video, and multimodal model, for both inference and training. \n-\n-It centralizes the model definition so that this definition is agreed upon across the ecosystem. `transformers` is the \n-pivot across frameworks: if a model definition is supported, it will be compatible with the majority of training \n+It centralizes the model definition so that this definition is agreed upon across the ecosystem. `transformers` is the\n+pivot across frameworks: if a model definition is supported, it will be compatible with the majority of training\n frameworks (Axolotl, Unsloth, DeepSpeed, FSDP, PyTorch-Lightning, ...), inference engines (vLLM, SGLang, TGI, ...),\n and adjacent modeling libraries (llama.cpp, mlx, ...) which leverage the model definition from `transformers`.\n \n@@ -194,7 +193,6 @@ pipeline(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.pn\n <details>\n <summary>Visual question answering</summary>\n \n-\n <h3 align=\"center\">\n     <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\"></a>\n </h3>"
        },
        {
            "sha": "d0398e7bde6a89e64e7cd26d0864019390035000",
            "filename": "awesome-transformers.md",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/awesome-transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/awesome-transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/awesome-transformers.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -6,7 +6,7 @@ developers, researchers, students, professors, engineers, and anyone else to bui\n \n In this list, we showcase incredibly impactful and novel projects that have pushed the field forward. We celebrate\n 100 of these projects as we reach the milestone of 100k stars as a community; but we're very open to pull requests\n-adding other projects to the list. If you believe a project should be here and it's not, then please, open a PR \n+adding other projects to the list. If you believe a project should be here and it's not, then please, open a PR\n to add it.\n \n ## [gpt4all](https://github.com/nomic-ai/gpt4all)\n@@ -49,7 +49,7 @@ Keywords: LLMs, Large Language Models, Agents, Chains\n \n [LlamaIndex](https://github.com/run-llama/llama_index) is a project that provides a central interface to connect your LLM's with external data. It provides various kinds of indices and retrieval mechanisms to perform different LLM tasks and obtain knowledge-augmented results.\n \n-Keywords: LLMs, Large Language Models, Data Retrieval, Indices, Knowledge Augmentation \n+Keywords: LLMs, Large Language Models, Data Retrieval, Indices, Knowledge Augmentation\n \n ## [ParlAI](https://github.com/facebookresearch/ParlAI)\n \n@@ -257,7 +257,7 @@ Stable-Dreamfusion is a pytorch implementation of the text-to-3D model Dreamfusi\n Keywords: Text-to-3D, Stable Diffusion\n \n ## [txtai](https://github.com/neuml/txtai)\n- \n+\n [txtai](https://github.com/neuml/txtai) is an open-source platform for semantic search and workflows powered by language models. txtai builds embeddings databases, which are a union of vector indexes and relational databases enabling similarity search with SQL. Semantic workflows connect language models together into unified applications.\n \n Keywords: Semantic search, LLM\n@@ -309,8 +309,8 @@ Keywords: OCR, LaTeX, Math formula\n \n OpenCLIP is an open source implementation of OpenAI's CLIP.\n \n-The goal of this repository is to enable training models with contrastive image-text supervision, and to investigate their properties such as robustness to distribution shift. \n-The starting point is an implementation of CLIP that matches the accuracy of the original CLIP models when trained on the same dataset. \n+The goal of this repository is to enable training models with contrastive image-text supervision, and to investigate their properties such as robustness to distribution shift.\n+The starting point is an implementation of CLIP that matches the accuracy of the original CLIP models when trained on the same dataset.\n \n Specifically, a ResNet-50 model trained with this codebase on OpenAI's 15 million image subset of YFCC achieves 32.7% top-1 accuracy on ImageNet.\n \n@@ -596,7 +596,7 @@ Keywords: Data-Centric AI, Data Quality, Noisy Labels, Outlier Detection, Active\n \n ## [BentoML](https://github.com/bentoml/BentoML)\n \n-[BentoML](https://github.com/bentoml) is the unified framework for building, shipping, and scaling production-ready AI applications incorporating traditional ML, pre-trained AI models, Generative and Large Language Models. \n+[BentoML](https://github.com/bentoml) is the unified framework for building, shipping, and scaling production-ready AI applications incorporating traditional ML, pre-trained AI models, Generative and Large Language Models.\n All Hugging Face models and pipelines can be seamlessly integrated into BentoML applications, enabling the running of models on the most suitable hardware and independent scaling based on usage.\n \n Keywords: BentoML, Framework, Deployment, AI Applications\n@@ -606,4 +606,3 @@ Keywords: BentoML, Framework, Deployment, AI Applications\n [LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory) offers a user-friendly fine-tuning framework that incorporates PEFT. The repository includes training(fine-tuning) and inference examples for LLaMA-2, BLOOM, Falcon, Baichuan, Qwen, and other LLMs. A ChatGLM version is also available in [ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning).\n \n Keywords: PEFT, fine-tuning, LLaMA-2, ChatGLM, Qwen\n-"
        },
        {
            "sha": "3cd809cba6a28454b7eec12b319e6c962ee71291",
            "filename": "docs/source/en/accelerator_selection.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Faccelerator_selection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Faccelerator_selection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Faccelerator_selection.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -69,7 +69,6 @@ CUDA_VISIBLE_DEVICES=0,2 torchrun trainer-program.py ...\n Only GPUs 0 and 2 are \"visible\" to PyTorch and are mapped to `cuda:0` and `cuda:1` respectively.  \n To reverse the order (use GPU 2 as `cuda:0` and GPU 0 as `cuda:1`):\n \n-\n ```bash\n CUDA_VISIBLE_DEVICES=2,0 torchrun trainer-program.py ...\n ```\n@@ -108,7 +107,6 @@ To reverse the order (use XPU 2 as `xpu:0` and XPU 0 as `xpu:1`):\n ZE_AFFINITY_MASK=2,0 torchrun trainer-program.py ...\n ```\n \n-\n You can also control the order of Intel XPUs with:\n \n ```bash\n@@ -120,7 +118,5 @@ For more information about device enumeration and sorting on Intel XPU, please r\n </hfoption>\n </hfoptions>\n \n-\n-\n > [!WARNING]\n > Environment variables can be exported instead of being added to the command line. This is not recommended because it can be confusing if you forget how the environment variable was set up and you end up using the wrong accelerators. Instead, it is common practice to set the environment variable for a specific training run on the same command line."
        },
        {
            "sha": "6445ee530146141178a3e725991a8f2b02491d4a",
            "filename": "docs/source/en/auto_docstring.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fauto_docstring.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fauto_docstring.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fauto_docstring.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -145,7 +145,6 @@ Arguments can also be passed directly to `@auto_docstring` for more control. Use\n \n The `Returns` and `Examples` parts of the docstring can also be manually specified.\n \n-\n ```python\n MODEL_COMMON_CUSTOM_ARGS = r\"\"\"\n     common_arg_1 (`torch.Tensor`, *optional*, defaults to `default_value`):\n@@ -202,7 +201,6 @@ There are some rules for documenting different types of arguments and they're li\n \n     If a standard argument behaves differently in your model, then you can override it locally in a `r\"\"\" \"\"\"` block. This local definition has a higher priority. For example, the `labels` argument is often customized per model and typically requires overriding.\n \n-\n - New or custom arguments should be documented within an `r\"\"\" \"\"\"` block after the signature if it is a function or in the `__init__` method's docstring if it is a class.\n \n     ```py"
        },
        {
            "sha": "77fc2c9c3288fcdc864981dec63c87156cd952e5",
            "filename": "docs/source/en/cache_explanation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcache_explanation.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -59,11 +59,9 @@ Refer to the table below to compare how caching improves efficiency.\n \n | without caching | with caching |\n |---|---|\n-| for each step, recompute all previous `K` and `V`  | for each step, only compute current `K` and `V` \n+| for each step, recompute all previous `K` and `V`  | for each step, only compute current `K` and `V`\n | attention cost per step is **quadratic** with sequence length | attention cost per step is **linear** with sequence length (memory grows linearly, but compute/token remains low) |\n \n-\n-\n ## Cache class\n \n A basic KV cache interface takes a key and value tensor for the current token and returns the updated `K` and `V` tensors. This is internally managed by a model's `forward` method.\n@@ -143,7 +141,6 @@ Cache position is used internally for two purposes:\n \n The generation loop usually takes care of the cache position, but if you're writing a custom generation method, it is important that cache positions are accurate since they are used to write and read key/value states into fixed slots.\n \n-\n ```py\n import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache, infer_device\n@@ -160,7 +157,6 @@ generated_ids = model.generate(**inputs, use_cache=True, max_new_tokens=10)\n \n ```\n \n-\n ## Legacy cache format\n \n Before the [`Cache`] class, the cache used to be stored as a tuple of tuples of tensors. This format is dynamic because it grows as text is generated, similar to [`DynamicCache`]."
        },
        {
            "sha": "20d5cf22ce4a82020f42bc034131aa9fdf929327",
            "filename": "docs/source/en/chat_extras.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fchat_extras.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fchat_extras.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_extras.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -29,7 +29,6 @@ the arguments, argument types, and function docstring are parsed in order to gen\n Although passing Python functions is very convenient, the parser can only handle [Google-style](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings)\n docstrings. Refer to the examples below for how to format a tool-ready function.\n \n-\n ```py\n def get_current_temperature(location: str, unit: str):\n     \"\"\"\n@@ -103,7 +102,6 @@ Hold the call in the `tool_calls` key of an `assistant` message. This is the rec\n > [!WARNING]\n > Although `tool_calls` is similar to the OpenAI API, the OpenAI API uses a JSON string as its `tool_calls` format. This may cause errors or strange model behavior if used in Transformers, which expects a dict.\n \n-\n ```py\n tool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}}\n messages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n@@ -131,7 +129,6 @@ The temperature in Paris, France right now is 22°C.<|im_end|>\n > Although the key in the assistant message is called `tool_calls`, in most cases, models only emit a single tool call at a time. Some older models emit multiple tool calls at the same time, but this is a\n > significantly more complex process, as you need to handle multiple tool responses at once and disambiguate them, often using tool call IDs. Please refer to the model card to see exactly what format a model expects for tool calls.\n \n-\n ## JSON schemas\n \n Another way to define tools is by passing a [JSON schema](https://json-schema.org/learn/getting-started-step-by-step)."
        },
        {
            "sha": "b1e8428afaa9524d49888d91fd06ceaf05692bda",
            "filename": "docs/source/en/chat_templating.md",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fchat_templating.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fchat_templating.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -16,13 +16,13 @@ rendered properly in your Markdown viewer.\n \n # Chat templates\n \n-The [chat basics](./conversations) guide covers how to store chat histories and generate text from chat models using [`TextGenerationPipeline`]. \n+The [chat basics](./conversations) guide covers how to store chat histories and generate text from chat models using [`TextGenerationPipeline`].\n \n This guide is intended for more advanced users, and covers the underlying classes and methods, as well as the key concepts for understanding what's actually going on when you chat with a model.\n \n The critical insight needed to understand chat models is this: All causal LMs, whether chat-trained or not, continue a sequence of tokens. When causal LMs are trained, the training usually begins with \"pre-training\" on a huge corpus of text, which creates a \"base\" model.\n These base models are then often \"fine-tuned\" for chat, which means training them on data that is formatted as a sequence of messages. The chat is still just a sequence of tokens, though! The list of `role` and `content` dictionaries that you pass\n-to a chat model get converted to a token sequence, often with control tokens like `<|user|>` or `<|assistant|>` or `<|end_of_message|>`, which allow the model to see the chat structure. \n+to a chat model get converted to a token sequence, often with control tokens like `<|user|>` or `<|assistant|>` or `<|end_of_message|>`, which allow the model to see the chat structure.\n There are many possible chat formats, and different models may use different formats or control tokens, even if they were fine-tuned from the same base model!\n \n Don't panic, though - you don't need to memorize every possible chat format in order to use chat models. Chat models come with **chat templates**, which indicate how they expect chats to be formatted.\n@@ -43,6 +43,7 @@ chat = [\n \n tokenizer.apply_chat_template(chat, tokenize=False)\n ```\n+\n ```md\n <s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\n ```\n@@ -62,6 +63,7 @@ chat = [\n \n tokenizer.apply_chat_template(chat, tokenize=False)\n ```\n+\n ```md\n <|user|>\\nHello, how are you?</s>\\n<|assistant|>\\nI'm doing great. How can I help you today?</s>\\n<|user|>\\nI'd like to show off how chat templating works!</s>\\n\n ```\n@@ -110,6 +112,7 @@ Pass the tokenized chat to [`~GenerationMixin.generate`] to generate a response.\n outputs = model.generate(tokenized_chat, max_new_tokens=128) \n print(tokenizer.decode(outputs[0]))\n ```\n+\n ```md\n <|system|>\n You are a friendly chatbot who always responds in the style of a pirate</s>\n@@ -125,16 +128,17 @@ Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopte\n \n ### add_generation_prompt\n \n-You may have noticed the [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) argument in the above examples. \n+You may have noticed the [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) argument in the above examples.\n This argument adds tokens to the end of the chat that indicate the start of an `assistant` response. Remember: Beneath all the chat abstractions, chat models are still just language models that continue a sequence of tokens!\n-If you include tokens that tell it that it's now in an `assistant` response, it will correctly write a response, but if you don't include these tokens, the model may get confused and do something strange, like **continuing** the user's message instead of replying to it! \n+If you include tokens that tell it that it's now in an `assistant` response, it will correctly write a response, but if you don't include these tokens, the model may get confused and do something strange, like **continuing** the user's message instead of replying to it!\n \n Let's see an example to understand what `add_generation_prompt` is actually doing. First, let's format a chat without `add_generation_prompt`:\n \n ```py\n tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n tokenized_chat\n ```\n+\n ```md\n <|im_start|>user\n Hi there!<|im_end|>\n@@ -150,6 +154,7 @@ Now, let's format the same chat with `add_generation_prompt=True`:\n tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n tokenized_chat\n ```\n+\n ```md\n <|im_start|>user\n Hi there!<|im_end|>\n@@ -186,7 +191,6 @@ model.generate(**formatted_chat)\n \n [`TextGenerationPipeline`] sets [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) to `True` by default to start a new message. However, if the final message in the chat has the `assistant` role, it assumes the message is a prefill and switches to `continue_final_message=True`. This is because most models don’t support multiple consecutive assistant messages. To override this behavior, explicitly pass the [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) argument to the pipeline.\n \n-\n ## Model training\n \n Training a model with a chat template is a good way to ensure the template matches the tokens the model was trained on. Apply the chat template as a preprocessing step to your dataset. Set `add_generation_prompt=False` because the additional tokens to prompt an assistant response aren't helpful during training.\n@@ -212,6 +216,7 @@ dataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\n dataset = dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\n print(dataset['formatted_chat'][0])\n ```\n+\n ```md\n <|user|>\n Which is bigger, the moon or the sun?</s>"
        },
        {
            "sha": "e469fde86b5343cfd88516b58e5523834b03724f",
            "filename": "docs/source/en/chat_templating_multimodal.md",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -18,8 +18,7 @@ rendered properly in your Markdown viewer.\n \n Multimodal chat models accept inputs like images, audio or video, in addition to text. The `content` key in a multimodal chat history is a list containing multiple items of different types. This is unlike text-only chat models whose `content` key is a single string.\n \n-\n-In the same way the [Tokenizer](./fast_tokenizer) class handles chat templates and tokenization for text-only models, \n+In the same way the [Tokenizer](./fast_tokenizer) class handles chat templates and tokenization for text-only models,\n the [Processor](./processors) class handles preprocessing, tokenization and chat templates for multimodal models. Their [`~ProcessorMixin.apply_chat_template`] methods are almost identical.\n \n This guide will show you how to chat with multimodal models with the high-level [`ImageTextToTextPipeline`] and at a lower level using the [`~ProcessorMixin.apply_chat_template`] and [`~GenerationMixin.generate`] methods.\n@@ -57,7 +56,6 @@ out = pipe(text=messages, max_new_tokens=128)\n print(out[0]['generated_text'][-1]['content'])\n ```\n \n-\n ```\n Ahoy, me hearty! These be two feline friends, likely some tabby cats, taking a siesta on a cozy pink blanket. They're resting near remote controls, perhaps after watching some TV or just enjoying some quiet time together. Cats sure know how to find comfort and relaxation, don't they?\n ```\n@@ -66,10 +64,9 @@ Aside from the gradual descent from pirate-speak into modern American English (i\n \n ## Using `apply_chat_template`\n \n-Like [text-only models](./chat_templating), use the [`~ProcessorMixin.apply_chat_template`] method to prepare the chat messages for multimodal models. \n+Like [text-only models](./chat_templating), use the [`~ProcessorMixin.apply_chat_template`] method to prepare the chat messages for multimodal models.\n This method handles the tokenization and formatting of the chat messages, including images and other media types. The resulting inputs are passed to the model for generation.\n \n-\n ```python\n from transformers import AutoProcessor, AutoModelForImageTextToText\n \n@@ -99,7 +96,6 @@ processed_chat = processor.apply_chat_template(messages, add_generation_prompt=T\n print(list(processed_chat.keys()))\n ```\n \n-\n ```\n ['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw']\n ```\n@@ -113,7 +109,6 @@ print(processor.decode(out[0]))\n \n The decoded output contains the full conversation so far, including the user message and the placeholder tokens that contain the image information. You may need to trim the previous conversation from the output before displaying it to the user.\n \n-\n ## Video inputs\n \n Some vision models also support video inputs. The message format is very similar to the format for [image inputs](#image-inputs).\n@@ -148,6 +143,7 @@ messages = [\n ```\n \n ### Example: Passing decoded video objects\n+\n ```python\n import numpy as np\n \n@@ -167,7 +163,9 @@ messages = [\n     },\n ]\n ```\n+\n You can also use existing (`\"load_video()\"`) function to load a video, edit the video in memory and pass it in the messages.\n+\n ```python\n \n # Make sure a video backend library (pyav, decord, or torchvision) is available.\n@@ -200,7 +198,6 @@ Pass `messages` to [`~ProcessorMixin.apply_chat_template`] to tokenize the input\n \n The `num_frames` parameter controls how many frames to uniformly sample from the video. Each checkpoint has a maximum frame count it was pretrained with and exceeding this count can significantly lower generation quality. It's important to choose a frame count that fits both the model capacity and your hardware resources. If `num_frames` isn't specified, the entire video is loaded without any frame sampling.\n \n-\n ```python\n processed_chat = processor.apply_chat_template(\n     messages,\n@@ -265,4 +262,3 @@ print(processed_chat.keys())\n \n </hfoption>\n </hfoptions>\n-"
        },
        {
            "sha": "936ce2a2c7f676e5e484964ed15b61277f7cdfdc",
            "filename": "docs/source/en/chat_templating_writing.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fchat_templating_writing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fchat_templating_writing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating_writing.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -18,7 +18,6 @@ rendered properly in your Markdown viewer.\n \n A chat template is a [Jinja](https://jinja.palletsprojects.com/en/stable/templates/) template stored in the tokenizer's [chat_template](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.chat_template) attribute. Jinja is a templating language that allows you to write Python-like code and syntax.\n \n-\n ```jinja\n {%- for message in messages %}\n     {{- '<|' + message['role'] + |>\\n' }}\n@@ -108,7 +107,6 @@ We strongly recommend using `-` to ensure only the intended content is printed.\n \n ### Special variables and callables\n \n-\n The only constants in a template are the `messages` variable and the `add_generation_prompt` boolean. However, you have\n access to **any other keyword arguments that are passed** to the [`~PreTrainedTokenizerBase.apply_chat_template`] method.\n "
        },
        {
            "sha": "a36be2203a5f8e2c96436e053b36f40078c64620",
            "filename": "docs/source/en/conversations.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fconversations.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fconversations.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fconversations.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -48,7 +48,6 @@ transformers chat -h\n \n The chat is implemented on top of the [AutoClass](./model_doc/auto), using tooling from [text generation](./llm_tutorial) and [chat](./chat_templating). It uses the `transformers serve` CLI under the hood ([docs](./serving.md#serve-cli)).\n \n-\n ## TextGenerationPipeline\n \n [`TextGenerationPipeline`] is a high-level text generation class with a \"chat mode\". Chat mode is enabled when a conversational model is detected and the chat prompt is [properly formatted](./llm_tutorial#wrong-prompt-format).\n@@ -109,7 +108,7 @@ quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n pipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\", model_kwargs={\"quantization_config\": quantization_config})\n ```\n \n-In general, model size and performance are directly correlated. Larger models are slower in addition to requiring more memory because each active parameter must be read from memory for every generated token. \n+In general, model size and performance are directly correlated. Larger models are slower in addition to requiring more memory because each active parameter must be read from memory for every generated token.\n This is a bottleneck for LLM text generation and the main options for improving generation speed are to either quantize a model or use hardware with higher memory bandwidth. Adding more compute power doesn't meaningfully help.\n \n You can also try techniques like [speculative decoding](./generation_strategies#speculative-decoding), where a smaller model generates candidate tokens that are verified by the larger model. If the candidate tokens are correct, the larger model can generate more than one token at a time. This significantly alleviates the bandwidth bottleneck and improves generation speed."
        },
        {
            "sha": "799e1715b3b1eaadd1e531b987253be0838fc742",
            "filename": "docs/source/en/cursor.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fcursor.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fcursor.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcursor.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -38,5 +38,3 @@ You are now ready to use your local model in Cursor! For instance, if you toggle\n <h3 align=\"center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_serve_cursor_chat.png\"/>\n </h3>\n-\n-"
        },
        {
            "sha": "3c277fa7df0c8a63063ee3a4602673977684e3ea",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -389,7 +389,6 @@ from .utils import some_function\n \n Only relative imports from the same-level `custom_generate` folder are supported. Parent/sibling folder imports are not valid. The `custom_generate` argument also works locally with any directory that contains a `custom_generate` structure. This is the recommended workflow for developing your custom generation method.\n \n-\n #### requirements.txt\n \n You can optionally specify additional Python requirements in a `requirements.txt` file inside the `custom_generate` folder. These are checked at runtime and an exception will be thrown if they're missing, nudging users to update their environment accordingly."
        },
        {
            "sha": "e9738f6ccfa40a360997af1d6bc0fd83e8aa3407",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_as_a_model_definition.png\"/>\n </h3>\n \n-\n Transformers acts as the model-definition framework for state-of-the-art machine learning models in text, computer\n vision, audio, video, and multimodal model, for both inference and training.\n "
        },
        {
            "sha": "63db5756a622a6b5e05a76fdbac23ba8c7e8f8cb",
            "filename": "docs/source/en/internal/file_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Finternal%2Ffile_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Finternal%2Ffile_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Ffile_utils.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -20,7 +20,6 @@ This page lists all of Transformers general utility functions that are found in\n \n Most of those are only useful if you are studying the general code in the library.\n \n-\n ## Enums and namedtuples\n \n [[autodoc]] utils.ExplicitEnum"
        },
        {
            "sha": "87b0111ff053e216e19f01dce56e02b974f7e2b4",
            "filename": "docs/source/en/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -65,7 +65,6 @@ values. Here, for instance, it has two keys that are `sequences` and `scores`.\n \n We document here all output types.\n \n-\n [[autodoc]] generation.GenerateDecoderOnlyOutput\n \n [[autodoc]] generation.GenerateEncoderDecoderOutput\n@@ -74,13 +73,11 @@ We document here all output types.\n \n [[autodoc]] generation.GenerateBeamEncoderDecoderOutput\n \n-\n ## LogitsProcessor\n \n A [`LogitsProcessor`] can be used to modify the prediction scores of a language model head for\n generation.\n \n-\n [[autodoc]] AlternatingCodebooksLogitsProcessor\n     - __call__\n \n@@ -174,8 +171,6 @@ generation.\n [[autodoc]] WatermarkLogitsProcessor\n     - __call__\n \n-\n-\n ## StoppingCriteria\n \n A [`StoppingCriteria`] can be used to change when to stop generation (other than EOS token). Please note that this is exclusively available to our PyTorch implementations.\n@@ -300,7 +295,6 @@ A [`Constraint`] can be used to force the generation to include specific tokens\n     - to_legacy_cache\n     - from_legacy_cache\n \n-\n ## Watermark Utils\n \n [[autodoc]] WatermarkingConfig"
        },
        {
            "sha": "1532581981701687f9d2446ed00b0b6b6e0ea19c",
            "filename": "docs/source/en/internal/import_utils.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -22,8 +22,8 @@ worked around. We don't want for all users of `transformers` to have to install\n we therefore mark those as soft dependencies rather than hard dependencies.\n \n The transformers toolkit is not made to error-out on import of a model that has a specific dependency; instead, an\n-object for which you are lacking a dependency will error-out when calling any method on it. As an example, if \n-`torchvision` isn't installed, the fast image processors will not be available. \n+object for which you are lacking a dependency will error-out when calling any method on it. As an example, if\n+`torchvision` isn't installed, the fast image processors will not be available.\n \n This object is still importable:\n \n@@ -55,7 +55,7 @@ All objects under a given filename have an automatic dependency to the tool link\n \n **Tokenizers**: All files starting with `tokenization_` and ending with `_fast` have an automatic `tokenizers` dependency\n \n-**Vision**: All files starting with `image_processing_` have an automatic dependency to the `vision` dependency group; \n+**Vision**: All files starting with `image_processing_` have an automatic dependency to the `vision` dependency group;\n at the time of writing, this only contains the `pillow` dependency.\n \n **Vision + Torch + Torchvision**: All files starting with `image_processing_` and ending with `_fast` have an automatic\n@@ -66,7 +66,7 @@ All of these automatic dependencies are added on top of the explicit dependencie\n ### Explicit Object Dependencies\n \n We add a method called `requires` that is used to explicitly specify the dependencies of a given object. As an\n-example, the `Trainer` class has two hard dependencies: `torch` and `accelerate`. Here is how we specify these \n+example, the `Trainer` class has two hard dependencies: `torch` and `accelerate`. Here is how we specify these\n required dependencies:\n \n ```python"
        },
        {
            "sha": "aa5371cd38e771ca7c368dad2c3aea2bc9b7a26c",
            "filename": "docs/source/en/internal/model_debugging_utils.md",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -21,10 +21,8 @@ provides for it.\n \n Most of those are only useful if you are adding new models in the library.\n \n-\n ## Model addition debuggers\n \n-\n ### Model addition debugger - context manager for model adders\n \n This context manager is a power user tool intended for model adders. It tracks all forward calls within a model forward\n@@ -72,7 +70,6 @@ with model_addition_debugger_context(\n \n ```\n \n-\n ### Reading results\n \n The debugger generates two files from the forward call, both with the same base name, but ending either with\n@@ -231,10 +228,8 @@ Once the forward passes of two models have been traced by the debugger, one can\n below: we can see slight differences between these two implementations' key projection layer. Inputs are mostly\n identical, but not quite. Looking through the file differences makes it easier to pinpoint which layer is wrong.\n \n-\n ![download-icon](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/files_difference_debugging.png)\n \n-\n ### Limitations and scope\n \n This feature will only work for torch-based models. Models relying heavily on external kernel calls may work, but trace will\n@@ -253,7 +248,7 @@ layers.\n \n This small util is a power user tool intended for model adders and maintainers. It lists all test methods\n existing in `test_modeling_common.py`, inherited by all model tester classes, and scans the repository to measure\n-how many tests are being skipped and for which models. \n+how many tests are being skipped and for which models.\n \n ### Rationale\n \n@@ -268,8 +263,7 @@ This utility:\n \n ![download-icon](https://huggingface.co/datasets/huggingface/documentation-images/resolve/f7f671f69b88ce4967e19179172c248958d35742/transformers/tests_skipped_visualisation.png)\n \n-\n-### Usage \n+### Usage\n \n You can run the skipped test analyzer in two ways:\n "
        },
        {
            "sha": "23856e5639c32b7fffda07fc491f44d2d06396b5",
            "filename": "docs/source/en/internal/pipelines_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Finternal%2Fpipelines_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Finternal%2Fpipelines_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fpipelines_utils.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -20,7 +20,6 @@ This page lists all the utility functions the library provides for pipelines.\n \n Most of those are only useful if you are studying the code of the models in the library.\n \n-\n ## Argument handling\n \n [[autodoc]] pipelines.ArgumentHandler"
        },
        {
            "sha": "a7c39a6a8d23bcd90d2b72acc309334e200f070f",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -67,7 +67,7 @@ out = model.generate(**inputs, do_sample=False, max_new_tokens=20, past_key_valu\n \n ## Fixed-size cache\n \n-The default [`DynamicCache`] prevents you from taking advantage of most just-in-time (JIT) optimizations because the cache size isn't fixed. JIT optimizations enable you to maximize latency at the expense of memory usage. All of the following cache types are compatible with JIT optimizations like [torch.compile](./llm_optims#static-kv-cache-and-torchcompile) to accelerate generation. \n+The default [`DynamicCache`] prevents you from taking advantage of most just-in-time (JIT) optimizations because the cache size isn't fixed. JIT optimizations enable you to maximize latency at the expense of memory usage. All of the following cache types are compatible with JIT optimizations like [torch.compile](./llm_optims#static-kv-cache-and-torchcompile) to accelerate generation.\n \n A fixed-size cache ([`StaticCache`]) pre-allocates a specific maximum cache size for the kv pairs. You can generate up to the maximum cache size without needing to modify it. However, having a fixed (usually large) size for the key/value states means that while generating, a lot of tokens will actually be masked as they should not take part in the attention. So this trick allows to easily `compile` the decoding stage, but it incurs a waste of tokens in the attention computation. As all things, it's then a trade-off which should be very good if you generate with several sequence of more or less the same lengths, but may be sub-optimal if you have for example 1 very large sequence, and then only short sequences (as the fix cache size would be large, a lot would be wasted for the short sequences). Make sure you understand the impact if you use it!\n "
        },
        {
            "sha": "0cbbbc6ac04f766b35787536caeb2ddf0467c9e2",
            "filename": "docs/source/en/llm_tutorial.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -24,6 +24,7 @@ In Transformers, the [`~GenerationMixin.generate`] API handles text generation,\n \n > [!TIP]\n > You can also chat with a model directly from the command line. ([reference](./conversations.md#transformers))\n+>\n > ```shell\n > transformers chat Qwen/Qwen2.5-0.5B-Instruct\n > ```\n@@ -35,6 +36,7 @@ Before you begin, it's helpful to install [bitsandbytes](https://hf.co/docs/bits\n ```bash\n !pip install -U transformers bitsandbytes\n ```\n+\n Bitsandbytes supports multiple backends in addition to CUDA-based GPUs. Refer to the multi-backend installation [guide](https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend) to learn more.\n \n Load a LLM with [`~PreTrainedModel.from_pretrained`] and add the following two parameters to reduce the memory requirements.\n@@ -154,7 +156,6 @@ print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n | `repetition_penalty` | `float` | Set it to `>1.0` if you're seeing the model repeat itself often. Larger values apply a larger penalty. |\n | `eos_token_id` | `list[int]` | The token(s) that will cause generation to stop. The default value is usually good, but you can specify a different token. |\n \n-\n ## Pitfalls\n \n The section below covers some common issues you may encounter during text generation and how to solve them."
        },
        {
            "sha": "04a61dd82cb5853f00fb85cb32c9110f26b6c5e4",
            "filename": "docs/source/en/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 19,
            "deletions": 3,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -66,6 +66,7 @@ If you have access to an 8 x 80GB A100 node, you could load BLOOM as follows\n ```bash\n !pip install transformers accelerate bitsandbytes optimum\n ```\n+\n ```python\n from transformers import AutoModelForCausalLM\n \n@@ -98,6 +99,7 @@ result\n ```\n \n **Output**:\n+\n ```\n Here is a Python function that transforms bytes to Giga bytes:\\n\\n```python\\ndef bytes_to_giga_bytes(bytes):\\n    return bytes / 1024 / 1024 / 1024\\n```\\n\\nThis function takes a single\n ```\n@@ -116,6 +118,7 @@ bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n ```\n \n **Output**:\n+\n ```bash\n 29.0260648727417\n ```\n@@ -127,7 +130,6 @@ Note that if we had tried to run the model in full float32 precision, a whopping\n \n If you are unsure in which format the model weights are stored on the Hub, you can always look into the checkpoint's config under `\"dtype\"`, *e.g.* [here](https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json#L21). It is recommended to set the model to the same precision type as written in the config when loading with `from_pretrained(..., dtype=...)` except when the original type is float32 in which case one can use both `float16` or `bfloat16` for inference.\n \n-\n Let's define a `flush(...)` function to free all allocated memory so that we can accurately measure the peak allocated GPU memory.\n \n ```python\n@@ -148,6 +150,7 @@ Let's call it now for the next experiment.\n ```python\n flush()\n ```\n+\n From the Accelerate library, you can also use a device-agnostic utility method called [release_memory](https://github.com/huggingface/accelerate/blob/29be4788629b772a3b722076e433b5b3b5c85da3/src/accelerate/utils/memory.py#L63), which takes various hardware backends like XPU, MLU, NPU, MPS, and more into account.\n \n ```python\n@@ -204,6 +207,7 @@ result\n ```\n \n **Output**:\n+\n ```\n Here is a Python function that transforms bytes to Giga bytes:\\n\\n```python\\ndef bytes_to_giga_bytes(bytes):\\n    return bytes / 1024 / 1024 / 1024\\n```\\n\\nThis function takes a single\n ```\n@@ -215,15 +219,16 @@ bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n ```\n \n **Output**:\n+\n ```\n 15.219234466552734\n ```\n \n Significantly less! We're down to just a bit over 15 GBs and could therefore run this model on consumer GPUs like the 4090.\n We're seeing a very nice gain in memory efficiency and more or less no degradation to the model's output. However, we can also notice a slight slow-down during inference.\n \n-\n We delete the models and flush the memory again.\n+\n ```python\n del model\n del pipe\n@@ -245,6 +250,7 @@ result\n ```\n \n **Output**:\n+\n ```\n Here is a Python function that transforms bytes to Giga bytes:\\n\\n```\\ndef bytes_to_gigabytes(bytes):\\n    return bytes / 1024 / 1024 / 1024\\n```\\n\\nThis function takes a single argument\n ```\n@@ -256,6 +262,7 @@ bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n ```\n \n **Output**:\n+\n ```\n 9.543574333190918\n ```\n@@ -270,6 +277,7 @@ Also note that inference here was again a bit slower compared to 8-bit quantizat\n del model\n del pipe\n ```\n+\n ```python\n flush()\n ```\n@@ -384,6 +392,7 @@ def alternating(list1, list2):\n -----\n \"\"\"\n ```\n+\n For demonstration purposes, we duplicate the system prompt by ten so that the input length is long enough to observe Flash Attention's memory savings.\n We append the original text prompt `\"Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer: Here\"`\n \n@@ -413,6 +422,7 @@ result\n ```\n \n **Output**:\n+\n ```\n Generated in 10.96854019165039 seconds.\n Sure. Here is a function that does that.\\n\\ndef bytes_to_giga(bytes):\\n   return bytes / 1024 / 1024 / 1024\\n\\nAnswer: Sure. Here is a function that does that.\\n\\ndef\n@@ -429,6 +439,7 @@ bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n ```\n \n **Output**:\n+\n ```bash\n 37.668193340301514\n ```\n@@ -460,6 +471,7 @@ result\n ```\n \n **Output**:\n+\n ```\n Generated in 3.0211617946624756 seconds.\n  Sure. Here is a function that does that.\\n\\ndef bytes_to_giga(bytes):\\n   return bytes / 1024 / 1024 / 1024\\n\\nAnswer: Sure. Here is a function that does that.\\n\\ndef\n@@ -474,6 +486,7 @@ bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n ```\n \n **Output**:\n+\n ```\n 32.617331981658936\n ```\n@@ -604,6 +617,7 @@ generated_text\n ```\n \n **Output**:\n+\n ```\n shape of input_ids torch.Size([1, 21])\n shape of input_ids torch.Size([1, 22])\n@@ -641,6 +655,7 @@ generated_text\n ```\n \n **Output**:\n+\n ```\n shape of input_ids torch.Size([1, 1])\n length of key-value cache 20\n@@ -712,6 +727,7 @@ tokenizer.batch_decode(generation_output.sequences)[0][len(prompt):]\n ```\n \n **Output**:\n+\n ```\n  is a modified version of the function that returns Mega bytes instead.\n \n@@ -733,6 +749,7 @@ config = model.config\n ```\n \n **Output**:\n+\n ```\n 7864320000\n ```\n@@ -773,7 +790,6 @@ The most notable application of GQA is [Llama-v2](https://huggingface.co/meta-ll\n \n > As a conclusion, it is strongly recommended to make use of either GQA or MQA if the LLM is deployed with auto-regressive decoding and is required to handle large input sequences as is the case for example for chat.\n \n-\n ## Conclusion\n \n The research community is constantly coming up with new, nifty ways to speed up inference time for ever-larger LLMs. As an example, one such promising research direction is [speculative decoding](https://huggingface.co/papers/2211.17192) where \"easy tokens\" are generated by smaller, faster language models and only \"hard tokens\" are generated by the LLM itself. Going into more detail is out of the scope of this notebook, but can be read upon in this [nice blog post](https://huggingface.co/blog/assisted-generation)."
        },
        {
            "sha": "bc1413a947422f01a48dfb16dcacc606419adf8d",
            "filename": "docs/source/en/main_classes/callback.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fcallback.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fcallback.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fcallback.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -54,7 +54,6 @@ The main class that implements callbacks is [`TrainerCallback`]. It gets the\n Trainer's internal state via [`TrainerState`], and can take some actions on the training loop via\n [`TrainerControl`].\n \n-\n ## Available Callbacks\n \n Here is the list of the available [`TrainerCallback`] in the library:"
        },
        {
            "sha": "933621f6a1443f5c288fecc0c6de8000a6e5e069",
            "filename": "docs/source/en/main_classes/configuration.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fconfiguration.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fconfiguration.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fconfiguration.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -24,7 +24,6 @@ Each derived config class implements model specific attributes. Common attribute\n `hidden_size`, `num_attention_heads`, and `num_hidden_layers`. Text models further implement:\n `vocab_size`.\n \n-\n ## PretrainedConfig\n \n [[autodoc]] PretrainedConfig"
        },
        {
            "sha": "33d156ec93fe0221f6fd115568d900fc6cdd034b",
            "filename": "docs/source/en/main_classes/data_collator.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fdata_collator.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fdata_collator.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fdata_collator.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -25,7 +25,6 @@ on the formed batch.\n \n Examples of use can be found in the [example scripts](../examples) or [example notebooks](../notebooks).\n \n-\n ## Default data collator\n \n [[autodoc]] data.data_collator.default_data_collator"
        },
        {
            "sha": "b04949229da4510419fcee04bbfd35641531fff9",
            "filename": "docs/source/en/main_classes/deepspeed.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fdeepspeed.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n \n # DeepSpeed\n \n-[DeepSpeed](https://github.com/deepspeedai/DeepSpeed), powered by Zero Redundancy Optimizer (ZeRO), is an optimization library for training and fitting very large models onto a GPU. It is available in several ZeRO stages, where each stage progressively saves more GPU memory by partitioning the optimizer state, gradients, parameters, and enabling offloading to a CPU or NVMe. DeepSpeed is integrated with the [`Trainer`] class and most of the setup is automatically taken care of for you. \n+[DeepSpeed](https://github.com/deepspeedai/DeepSpeed), powered by Zero Redundancy Optimizer (ZeRO), is an optimization library for training and fitting very large models onto a GPU. It is available in several ZeRO stages, where each stage progressively saves more GPU memory by partitioning the optimizer state, gradients, parameters, and enabling offloading to a CPU or NVMe. DeepSpeed is integrated with the [`Trainer`] class and most of the setup is automatically taken care of for you.\n \n However, if you want to use DeepSpeed without the [`Trainer`], Transformers provides a [`HfDeepSpeedConfig`] class.\n "
        },
        {
            "sha": "3406309aa3251997d5bbf3790014508277349c59",
            "filename": "docs/source/en/main_classes/executorch.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fexecutorch.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fexecutorch.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fexecutorch.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -15,14 +15,12 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-\n # ExecuTorch\n \n [`ExecuTorch`](https://github.com/pytorch/executorch) is an end-to-end solution for enabling on-device inference capabilities across mobile and edge devices including wearables, embedded devices and microcontrollers. It is part of the PyTorch ecosystem and supports the deployment of PyTorch models with a focus on portability, productivity, and performance.\n \n ExecuTorch introduces well defined entry points to perform model, device, and/or use-case specific optimizations such as backend delegation, user-defined compiler transformations, memory planning, and more. The first step in preparing a PyTorch model for execution on an edge device using ExecuTorch is to export the model. This is achieved through the use of a PyTorch API called [`torch.export`](https://pytorch.org/docs/stable/export.html).\n \n-\n ## ExecuTorch Integration\n \n An integration point is being developed to ensure that 🤗 Transformers can be exported using `torch.export`. The goal of this integration is not only to enable export but also to ensure that the exported artifact can be further lowered and optimized to run efficiently in `ExecuTorch`, particularly for mobile and edge use cases."
        },
        {
            "sha": "294ecad6309e3ccf9967a9f2ceb461d8bed847b5",
            "filename": "docs/source/en/main_classes/feature_extractor.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Ffeature_extractor.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Ffeature_extractor.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Ffeature_extractor.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -18,7 +18,6 @@ rendered properly in your Markdown viewer.\n \n A feature extractor is in charge of preparing input features for audio or vision models. This includes feature extraction from sequences, e.g., pre-processing audio files to generate Log-Mel Spectrogram features, feature extraction from images, e.g., cropping image files, but also padding, normalization, and conversion to NumPy and PyTorch tensors.\n \n-\n ## FeatureExtractionMixin\n \n [[autodoc]] feature_extraction_utils.FeatureExtractionMixin"
        },
        {
            "sha": "61be0306630d959c8c14b21758f914c209cba287",
            "filename": "docs/source/en/main_classes/image_processor.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fimage_processor.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fimage_processor.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fimage_processor.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -26,6 +26,7 @@ from transformers import AutoImageProcessor\n \n processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", use_fast=True)\n ```\n+\n Note that `use_fast` will be set to `True` by default in a future release.\n \n When using a fast image processor, you can also set the `device` argument to specify the device on which the processing should be done. By default, the processing is done on the same device as the inputs if the inputs are tensors, or on the CPU otherwise.\n@@ -57,7 +58,6 @@ Here are some speed comparisons between the base and fast image processors for t\n \n These benchmarks were run on an [AWS EC2 g5.2xlarge instance](https://aws.amazon.com/ec2/instance-types/g5/), utilizing an NVIDIA A10G Tensor Core GPU.\n \n-\n ## ImageProcessingMixin\n \n [[autodoc]] image_processing_utils.ImageProcessingMixin\n@@ -72,7 +72,6 @@ These benchmarks were run on an [AWS EC2 g5.2xlarge instance](https://aws.amazon\n \n [[autodoc]] image_processing_utils.BaseImageProcessor\n \n-\n ## BaseImageProcessorFast\n \n [[autodoc]] image_processing_utils_fast.BaseImageProcessorFast"
        },
        {
            "sha": "34da2ac9d1b833b789999572eba10c2f69518cb9",
            "filename": "docs/source/en/main_classes/logging.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Flogging.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Flogging.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Flogging.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -55,7 +55,6 @@ logger.info(\"INFO\")\n logger.warning(\"WARN\")\n ```\n \n-\n All the methods of this logging module are documented below, the main ones are\n [`logging.get_verbosity`] to get the current level of verbosity in the logger and\n [`logging.set_verbosity`] to set the verbosity to the level of your choice. In order (from the least"
        },
        {
            "sha": "e3e77a8e2e13aed4eb5645ad33b44247399bb46e",
            "filename": "docs/source/en/main_classes/model.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fmodel.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fmodel.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fmodel.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -26,7 +26,6 @@ file or directory, or from a pretrained model configuration provided by the libr\n \n The other methods that are common to each model are defined in [`~modeling_utils.ModuleUtilsMixin`] and [`~generation.GenerationMixin`].\n \n-\n ## PreTrainedModel\n \n [[autodoc]] PreTrainedModel"
        },
        {
            "sha": "5f8869948d2ba950de183bf3bfb0d9e2a173444e",
            "filename": "docs/source/en/main_classes/onnx.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fonnx.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fonnx.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fonnx.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -51,4 +51,3 @@ to export models for different types of topologies or tasks.\n ### FeaturesManager\n \n [[autodoc]] onnx.features.FeaturesManager\n-"
        },
        {
            "sha": "3bab249ab4ee65e192c4530f27a88a30bbdd314c",
            "filename": "docs/source/en/main_classes/optimizer_schedules.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Foptimizer_schedules.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Foptimizer_schedules.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Foptimizer_schedules.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -22,7 +22,6 @@ The `.optimization` module provides:\n - several schedules in the form of schedule objects that inherit from `_LRSchedule`:\n - a gradient accumulation class to accumulate the gradients of multiple batches\n \n-\n ## AdaFactor\n \n [[autodoc]] Adafactor"
        },
        {
            "sha": "8a9ae879fb19ba099973883337a1c8b8ebcd36e6",
            "filename": "docs/source/en/main_classes/output.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Foutput.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Foutput.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Foutput.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -47,7 +47,6 @@ However, this is not always the case. Some models apply normalization or subsequ\n \n </Tip>\n \n-\n You can access each attribute as you would usually do, and if that attribute has not been returned by the model, you\n will get `None`. Here for instance `outputs.loss` is the loss computed by the model, and `outputs.attentions` is\n `None`."
        },
        {
            "sha": "31139ddf429f0df156fbc8acd32dd8bcd1b9c156",
            "filename": "docs/source/en/main_classes/pipelines.md",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -81,7 +81,6 @@ for out in tqdm(pipe(KeyDataset(dataset, \"file\"))):\n \n For ease of use, a generator is also possible:\n \n-\n ```python\n from transformers import pipeline\n \n@@ -196,7 +195,6 @@ This is a occasional very long sentence compared to the other. In that case, the\n tokens long, so the whole batch will be [64, 400] instead of [64, 4], leading to the high slowdown. Even worse, on\n bigger batches, the program simply crashes.\n \n-\n ```\n ------------------------------\n Streaming no batching\n@@ -245,7 +243,6 @@ multiple forward pass of a model. Under normal circumstances, this would yield i\n In order to circumvent this issue, both of these pipelines are a bit specific, they are `ChunkPipeline` instead of\n regular `Pipeline`. In short:\n \n-\n ```python\n preprocessed = pipe.preprocess(inputs)\n model_outputs = pipe.forward(preprocessed)\n@@ -254,7 +251,6 @@ outputs = pipe.postprocess(model_outputs)\n \n Now becomes:\n \n-\n ```python\n all_model_outputs = []\n for preprocessed in pipe.preprocess(inputs):\n@@ -282,7 +278,6 @@ If you want to override a specific pipeline.\n Don't hesitate to create an issue for your task at hand, the goal of the pipeline is to be easy to use and support most\n cases, so `transformers` could maybe support your use case.\n \n-\n If you want to try simply you can:\n \n - Subclass your pipeline of choice\n@@ -302,7 +297,6 @@ my_pipeline = pipeline(model=\"xxxx\", pipeline_class=MyPipeline)\n \n That should enable you to do all the custom code you want.\n \n-\n ## Implementing a pipeline\n \n [Implementing a new pipeline](../add_new_pipeline)\n@@ -329,7 +323,6 @@ Pipelines available for audio tasks include the following.\n     - __call__\n     - all\n \n-\n ### ZeroShotAudioClassificationPipeline\n \n [[autodoc]] ZeroShotAudioClassificationPipeline"
        },
        {
            "sha": "8863a6326282c139adcc51ed06d7ef55b74f5c75",
            "filename": "docs/source/en/main_classes/processors.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fprocessors.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fprocessors.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fprocessors.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -71,7 +71,6 @@ Additionally, the following method can be used to load values from a data file a\n \n [[autodoc]] data.processors.glue.glue_convert_examples_to_features\n \n-\n ## XNLI\n \n [The Cross-Lingual NLI Corpus (XNLI)](https://www.nyu.edu/projects/bowman/xnli/) is a benchmark that evaluates the\n@@ -88,7 +87,6 @@ Please note that since the gold labels are available on the test set, evaluation\n \n An example using these processors is given in the [run_xnli.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification/run_xnli.py) script.\n \n-\n ## SQuAD\n \n [The Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer//) is a benchmark that\n@@ -115,11 +113,9 @@ Additionally, the following method can be used to convert SQuAD examples into\n \n [[autodoc]] data.processors.squad.squad_convert_examples_to_features\n \n-\n These processors as well as the aforementioned method can be used with files containing the data as well as with the\n *tensorflow_datasets* package. Examples are given below.\n \n-\n ### Example usage\n \n Here is an example using the processors as well as the conversion method using data files:"
        },
        {
            "sha": "52c9751226d40febe2f54f3665f72dd3d8c98fbe",
            "filename": "docs/source/en/main_classes/tokenizer.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Ftokenizer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Ftokenizer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Ftokenizer.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -22,7 +22,7 @@ Rust library [🤗 Tokenizers](https://github.com/huggingface/tokenizers). The \"\n \n 1. a significant speed-up in particular when doing batched tokenization and\n 2. additional methods to map between the original string (character and words) and the token space (e.g. getting the\n-   index of the token comprising a given character or the span of characters corresponding to a given token). \n+   index of the token comprising a given character or the span of characters corresponding to a given token).\n \n The base classes [`PreTrainedTokenizer`] and [`PreTrainedTokenizerFast`]\n implement the common methods for encoding string inputs in model inputs (see below) and instantiating/saving python and\n@@ -50,12 +50,11 @@ several advanced alignment methods which can be used to map between the original\n token space (e.g., getting the index of the token comprising a given character or the span of characters corresponding\n to a given token).\n \n-\n # Multimodal Tokenizer\n \n Apart from that each tokenizer can be a \"multimodal\" tokenizer which means that the tokenizer will hold all relevant special tokens\n as part of tokenizer attributes for easier access. For example, if the tokenizer is loaded from a vision-language model like LLaVA, you will\n-be able to access `tokenizer.image_token_id` to obtain the special image token used as a placeholder. \n+be able to access `tokenizer.image_token_id` to obtain the special image token used as a placeholder.\n \n To enable extra special tokens for any type of tokenizer, you have to add the following lines and save the tokenizer. Extra special tokens do not\n have to be modality related and can ne anything that the model often needs access to. In the below code, tokenizer at `output_dir` will have direct access"
        },
        {
            "sha": "29d29d0cb60555ed3b40f618a51f8863e855892b",
            "filename": "docs/source/en/main_classes/video_processor.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fvideo_processor.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmain_classes%2Fvideo_processor.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fvideo_processor.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -22,7 +22,6 @@ The video processor extends the functionality of image processors by allowing Vi\n \n When adding a new VLM or updating an existing one to enable distinct video preprocessing, saving and reloading the processor configuration will store the video related arguments in a dedicated file named `video_preprocessing_config.json`. Don't worry if you haven't updated your VLM, the processor will try to load video related configurations from a file named `preprocessing_config.json`.\n \n-\n ### Usage Example\n Here's an example of how to load a video processor with [`llava-hf/llava-onevision-qwen2-0.5b-ov-hf`](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf) model:\n \n@@ -59,7 +58,6 @@ The video processor can also sample video frames using the technique best suited\n \n </Tip>\n \n-\n ```python\n from transformers import AutoVideoProcessor\n \n@@ -92,4 +90,3 @@ print(processed_video_inputs.pixel_values_videos.shape)\n ## BaseVideoProcessor\n \n [[autodoc]] video_processing_utils.BaseVideoProcessor\n-"
        },
        {
            "sha": "acf9c4de12fe921b426b4a267a8378e75e94ec38",
            "filename": "docs/source/en/model_doc/aimv2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Faimv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Faimv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faimv2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -25,7 +25,6 @@ The abstract from the paper is the following:\n \n *We introduce a novel method for pre-training of large-scale vision encoders. Building on recent advancements in autoregressive pre-training of vision models, we extend this framework to a multimodal setting, i.e., images and text. In this paper, we present AIMV2, a family of generalist vision encoders characterized by a straightforward pre-training process, scalability, and remarkable performance across a range of downstream tasks. This is achieved by pairing the vision encoder with a multimodal decoder that autoregressively generates raw image patches and text tokens. Our encoders excel not only in multimodal evaluations but also in vision benchmarks such as localization, grounding, and classification. Notably, our AIMV2-3B encoder achieves 89.5% accuracy on ImageNet-1k with a frozen trunk. Furthermore, AIMV2 consistently outperforms state-of-the-art contrastive models (e.g., CLIP, SigLIP) in multimodal image understanding across diverse settings.*\n \n-\n This model was contributed by [Yaswanth Gali](https://huggingface.co/yaswanthgali).\n The original code can be found [here](https://github.com/apple/ml-aim).\n "
        },
        {
            "sha": "ddd0815aaa57ca173fd1368d15e167051393c76d",
            "filename": "docs/source/en/model_doc/aria.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -98,7 +98,7 @@ print(response)\n </hfoptions>\n \n Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n-\t\n+\n The example below uses [torchao](../quantization/torchao) to only quantize the weights to int4 and the [rhymes-ai/Aria-sequential_mlp](https://huggingface.co/rhymes-ai/Aria-sequential_mlp) checkpoint. This checkpoint replaces grouped GEMM with `torch.nn.Linear` layers for easier quantization.\n \n ```py\n@@ -142,7 +142,6 @@ response = processor.decode(output_ids, skip_special_tokens=True)\n print(response)\n ```\n \n-\n ## AriaImageProcessor\n \n [[autodoc]] AriaImageProcessor"
        },
        {
            "sha": "092bf3b26f38485c47d6e389c3cdcb582fd96424",
            "filename": "docs/source/en/model_doc/audio-spectrogram-transformer.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -52,13 +52,13 @@ the authors compute the stats for a downstream dataset.\n \n ### Using Scaled Dot Product Attention (SDPA)\n \n-PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n-encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n-[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n page for more information.\n \n-SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set \n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n \n ```"
        },
        {
            "sha": "c1db5e2541a68a711ecc4645efd36c1e6a6ad17d",
            "filename": "docs/source/en/model_doc/auto.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -23,7 +23,6 @@ automatically retrieve the relevant model given the name/path to the pretrained\n Instantiating one of [`AutoConfig`], [`AutoModel`], and\n [`AutoTokenizer`] will directly create a class of the relevant architecture. For instance\n \n-\n ```python\n model = AutoModel.from_pretrained(\"google-bert/bert-base-cased\")\n ```"
        },
        {
            "sha": "d0822173e8982803ce383bdfe2eafb8375729369",
            "filename": "docs/source/en/model_doc/aya_vision.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Faya_vision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Faya_vision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faya_vision.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -29,7 +29,7 @@ You can find all the original Aya Vision checkpoints under the [Aya Vision](http\n \n > [!TIP]\n > This model was contributed by [saurabhdash](https://huggingface.co/saurabhdash) and [yonigozlan](https://huggingface.co/yonigozlan).\n-> \n+>\n > Click on the Aya Vision models in the right sidebar for more examples of how to apply Aya Vision to different image-to-text tasks.\n \n The example below demonstrates how to generate text based on an image with [`Pipeline`] or the [`AutoModel`] class."
        },
        {
            "sha": "6024b0e83ed54288029592d7287f053228194a5d",
            "filename": "docs/source/en/model_doc/bark.md",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -76,7 +76,7 @@ Note that 🤗 Optimum must be installed before using this feature. [Here's how\n \n Flash Attention 2 is an even faster, optimized version of the previous optimization.\n \n-##### Installation \n+##### Installation\n \n First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features). If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered [above](https://huggingface.co/docs/transformers/main/en/model_doc/bark#using-better-transformer).\n \n@@ -86,7 +86,6 @@ Next, [install](https://github.com/Dao-AILab/flash-attention#installation-and-fe\n pip install -U flash-attn --no-build-isolation\n ```\n \n-\n ##### Usage\n \n To load a model using Flash Attention 2, we can pass the `attn_implementation=\"flash_attention_2\"` flag to [`.from_pretrained`](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained). We'll also load the model in half-precision (e.g. `torch.float16`), since it results in almost no degradation to audio quality but significantly lower memory usage and faster inference:\n@@ -97,7 +96,6 @@ model = BarkModel.from_pretrained(\"suno/bark-small\", dtype=torch.float16, attn_i\n \n ##### Performance comparison\n \n-\n The following diagram shows the latency for the native attention implementation (no optimisation) against Better Transformer and Flash Attention 2. In all cases, we generate 400 semantic tokens on a 40GB A100 GPU with PyTorch 2.1. Flash Attention 2 is also consistently faster than Better Transformer, and its performance improves even more as batch sizes increase:\n \n <div style=\"text-align: center\">\n@@ -108,7 +106,6 @@ To put this into perspective, on an NVIDIA A100 and when generating 400 semantic\n \n At batch size 8, on an NVIDIA A100, Flash Attention 2 is also 10% faster than Better Transformer, and at batch size 16, 25%.\n \n-\n #### Combining optimization techniques\n \n You can combine optimization techniques, and use CPU offload, half-precision and Flash Attention 2 (or 🤗 Better Transformer) all at once.\n@@ -147,7 +144,7 @@ These presets are also uploaded in the hub [here](https://huggingface.co/suno/ba\n >>> audio_array = audio_array.cpu().numpy().squeeze()\n ```\n \n-Bark can generate highly realistic, **multilingual** speech as well as other audio - including music, background noise and simple sound effects. \n+Bark can generate highly realistic, **multilingual** speech as well as other audio - including music, background noise and simple sound effects.\n \n ```python\n >>> # Multilingual speech - simplified Chinese\n@@ -165,7 +162,6 @@ Bark can generate highly realistic, **multilingual** speech as well as other aud\n \n The model can also produce **nonverbal communications** like laughing, sighing and crying.\n \n-\n ```python\n >>> # Adding non-speech cues to the input text\n >>> inputs = processor(\"Hello uh ... [clears throat], my dog is cute [laughter]\")\n@@ -235,4 +231,3 @@ To save the audio, simply take the sample rate from the model config and some sc\n \n [[autodoc]] BarkSemanticConfig\n     - all\n-"
        },
        {
            "sha": "f81eaae98fb38258df503dd8fda9b312c980630f",
            "filename": "docs/source/en/model_doc/bart.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -15,7 +15,6 @@ rendered properly in your Markdown viewer.\n -->\n *This model was released on 2019-10-29 and added to Hugging Face Transformers on 2020-11-16.*\n \n-\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n     <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n@@ -46,6 +45,7 @@ pipeline = pipeline(\n pipeline(\"Plants create <mask> through a process known as photosynthesis.\")\n \n ```\n+\n </hfoption>\n <hfoption id=\"AutoModel\">\n "
        },
        {
            "sha": "f7a100a4208cfd43daa4f6bd76df011d144df5be",
            "filename": "docs/source/en/model_doc/barthez.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbarthez.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbarthez.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbarthez.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -31,7 +31,6 @@ You can find all of the original BARThez checkpoints under the [BARThez](https:/\n > This model was contributed by [moussakam](https://huggingface.co/moussakam).\n > Refer to the [BART](./bart) docs for more usage examples.\n \n-\n The example below demonstrates how to predict the `<mask>` token with [`Pipeline`], [`AutoModel`], and from the command line.\n \n <hfoptions id=\"usage\">"
        },
        {
            "sha": "15e96c57669f6830d569d523644f931a6c5ed89a",
            "filename": "docs/source/en/model_doc/bartpho.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbartpho.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbartpho.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbartpho.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -33,12 +33,9 @@ You can find all the original checkpoints under the [VinAI](https://huggingface.\n \n The example below demonstrates how to summarize text with [`Pipeline`] or the [`AutoModel`] class.\n \n-\n <hfoptions id=\"usage\">\n <hfoption id=\"Pipeline\">\n \n-\n-\n ```python\n import torch\n from transformers import pipeline\n@@ -98,8 +95,6 @@ transformers run --task summarization --model vinai/bartpho-word --device 0\n </hfoption>\n </hfoptions>\n \n-\n-\n ## Notes\n \n - BARTpho uses the large architecture of BART with an additional layer-normalization layer on top of the encoder and decoder. The BART-specific classes should be replaced with the mBART-specific classes."
        },
        {
            "sha": "6599efa73e08cee75d25d4df9dcf6f0db2e31615",
            "filename": "docs/source/en/model_doc/bert-japanese.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-japanese.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-japanese.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-japanese.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -81,7 +81,6 @@ API reference information.\n \n </Tip>\n \n-\n ## BertJapaneseTokenizer\n \n [[autodoc]] BertJapaneseTokenizer"
        },
        {
            "sha": "223932877c0a291dabfb7297f16d814301d35693",
            "filename": "docs/source/en/model_doc/bertweet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -26,7 +26,6 @@ rendered properly in your Markdown viewer.\n \n [BERTweet](https://huggingface.co/papers/2005.10200) shares the same architecture as [BERT-base](./bert), but it’s pretrained like [RoBERTa](./roberta) on English Tweets. It performs really well on Tweet-related tasks like part-of-speech tagging, named entity recognition, and text classification.\n \n-\n You can find all the original BERTweet checkpoints under the [VinAI Research](https://huggingface.co/vinai?search_models=BERTweet) organization.\n \n > [!TIP]\n@@ -49,6 +48,7 @@ pipeline = pipeline(\n )\n pipeline(\"Plants create <mask> through a process known as photosynthesis.\")\n ```\n+\n </hfoption>\n <hfoption id=\"AutoModel\">\n "
        },
        {
            "sha": "877445a4ba584d5e1db693f451751e8f26433769",
            "filename": "docs/source/en/model_doc/big_bird.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -47,6 +47,7 @@ pipeline = pipeline(\n )\n pipeline(\"Plants create [MASK] through a process known as photosynthesis.\")\n ```\n+\n </hfoption>\n <hfoption id=\"AutoModel\">\n \n@@ -81,6 +82,7 @@ print(f\"The predicted token is: {predicted_token}\")\n ```bash\n !echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | transformers run --task fill-mask --model google/bigbird-roberta-base --device 0\n ```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "cfc55e361e772f799a51f57b7a50af8a81210ea3",
            "filename": "docs/source/en/model_doc/bigbird_pegasus.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -52,6 +52,7 @@ Through photosynthesis, plants capture energy from sunlight using a green pigmen\n These ingredients are then transformed into glucose, a type of sugar that serves as a source of chemical energy, and oxygen, which is released as a byproduct into the atmosphere. The glucose produced during photosynthesis is not just used immediately; plants also store it as starch or convert it into other organic compounds like cellulose, which is essential for building their cellular structure.\n This energy reserve allows them to grow, develop leaves, produce flowers, bear fruit, and carry out various physiological processes throughout their lifecycle.\"\"\")\n ```\n+\n </hfoption>\n <hfoption id=\"AutoModel\">\n \n@@ -77,6 +78,7 @@ input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n output = model.generate(**input_ids, cache_implementation=\"static\")\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n+\n </hfoption>\n <hfoption id=\"transformers\">\n "
        },
        {
            "sha": "9a664fa288f3ba6155051b94f50a67b23732b287",
            "filename": "docs/source/en/model_doc/biogpt.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -135,31 +135,26 @@ print(output)\n \n [[autodoc]] BioGptConfig\n \n-\n ## BioGptTokenizer\n \n [[autodoc]] BioGptTokenizer\n     - save_vocabulary\n \n-\n ## BioGptModel\n \n [[autodoc]] BioGptModel\n     - forward\n \n-\n ## BioGptForCausalLM\n \n [[autodoc]] BioGptForCausalLM\n     - forward\n \n-\n ## BioGptForTokenClassification\n \n [[autodoc]] BioGptForTokenClassification\n     - forward\n \n-\n ## BioGptForSequenceClassification\n \n [[autodoc]] BioGptForSequenceClassification"
        },
        {
            "sha": "69f9cb75131f52d169e09c7ae257a9aeb1aa7f1a",
            "filename": "docs/source/en/model_doc/bitnet.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbitnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbitnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbitnet.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -35,10 +35,8 @@ Several versions of the model weights are available on Hugging Face:\n \n * [**`microsoft/bitnet-b1.58-2B-4T-gguf`**](https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-gguf): Contains the model weights in GGUF format, compatible with the `bitnet.cpp` library for CPU inference.\n \n-\n ### Model Details\n \n-\n * **Architecture:** Transformer-based, modified with `BitLinear` layers (BitNet framework).\n     * Uses Rotary Position Embeddings (RoPE).\n     * Uses squared ReLU (ReLU²) activation in FFN layers.\n@@ -58,10 +56,8 @@ Several versions of the model weights are available on Hugging Face:\n     3.  **Direct Preference Optimization (DPO):** Aligned with human preferences using preference pairs.\n * **Tokenizer:** LLaMA 3 Tokenizer (vocab size: 128,256).\n \n-\n ## Usage tips\n \n-\n **VERY IMPORTANT NOTE ON EFFICIENCY**\n \n > Please do NOT expect performance efficiency gains (in terms of speed, latency, or energy consumption) when using this model with the standard transformers library.\n@@ -106,7 +102,6 @@ response = tokenizer.decode(chat_outputs[0][chat_input.shape[-1]:], skip_special\n print(\"\\nAssistant Response:\", response)\n ```\n \n-\n ## BitNetConfig\n \n [[autodoc]] BitNetConfig"
        },
        {
            "sha": "830db710e039e23d3dfbe54eb9e154a7922c53f3",
            "filename": "docs/source/en/model_doc/blenderbot-small.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot-small.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot-small.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot-small.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -55,7 +55,6 @@ found [here](https://github.com/facebookresearch/ParlAI).\n Blenderbot Small is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n the left.\n \n-\n ## Resources\n \n - [Causal language modeling task guide](../tasks/language_modeling)"
        },
        {
            "sha": "168c744235d8a7e3a44dfa8c31d7e00d5fcab766",
            "filename": "docs/source/en/model_doc/blenderbot.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -71,7 +71,6 @@ An example:\n   `facebook/blenderbot_small_90M`, have a different architecture and consequently should be used with\n   [BlenderbotSmall](blenderbot-small).\n \n-\n ## Resources\n \n - [Causal language modeling task guide](../tasks/language_modeling)"
        },
        {
            "sha": "faaaee7b084036a2627316f103f8828f126615d0",
            "filename": "docs/source/en/model_doc/blip-2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -26,14 +26,14 @@ rendered properly in your Markdown viewer.\n The BLIP-2 model was proposed in [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://huggingface.co/papers/2301.12597) by\n Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2 leverages frozen pre-trained image encoders and large language models (LLMs) by training a lightweight, 12-layer Transformer\n encoder in between them, achieving state-of-the-art performance on various vision-language tasks. Most notably, BLIP-2 improves upon [Flamingo](https://huggingface.co/papers/2204.14198), an 80 billion parameter model, by 8.7%\n-on zero-shot VQAv2 with 54x fewer trainable parameters. \n+on zero-shot VQAv2 with 54x fewer trainable parameters.\n \n The abstract from the paper is the following:\n \n *The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.*\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/blip2_architecture.jpg\"\n-alt=\"drawing\" width=\"600\"/> \n+alt=\"drawing\" width=\"600\"/>\n \n <small> BLIP-2 architecture. Taken from the <a href=\"https://huggingface.co/papers/2301.12597\">original paper.</a> </small>\n "
        },
        {
            "sha": "5ef7872899661ff5a49a4e167f63cb184c70f5cb",
            "filename": "docs/source/en/model_doc/blip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -25,7 +25,6 @@ rendered properly in your Markdown viewer.\n \n [BLIP](https://huggingface.co/papers/2201.12086) (Bootstrapped Language-Image Pretraining) is a vision-language pretraining (VLP) framework designed for *both* understanding and generation tasks. Most existing pretrained models are only good at one or the other. It uses a captioner to generate captions and a filter to remove the noisy captions. This increases training data quality and more effectively uses the messy web data.\n \n-\n You can find all the original BLIP checkpoints under the [BLIP](https://huggingface.co/collections/Salesforce/blip-models-65242f40f1491fbf6a9e9472) collection.\n \n > [!TIP]"
        },
        {
            "sha": "c78cb4447ebfc842c8dce2a7b205a0023d16fef1",
            "filename": "docs/source/en/model_doc/bloom.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbloom.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbloom.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbloom.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -48,7 +48,6 @@ See also:\n - [Token classification task guide](../tasks/token_classification)\n - [Question answering task guide](../tasks/question_answering)\n \n-\n ⚡️ Inference\n - A blog on [Optimization story: Bloom inference](https://huggingface.co/blog/bloom-inference-optimization).\n - A blog on [Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts)."
        },
        {
            "sha": "7e9052bcdd2ec51c65160c55d4cbe29660059dc2",
            "filename": "docs/source/en/model_doc/blt.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fblt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fblt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblt.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -83,7 +83,6 @@ print(tokenizer.decode(generated_ids[0]))\n This model was contributed by [itazap](https://huggingface.co/<itazap>).\n The original code can be found [here](<https://github.com/facebookresearch/blt>).\n \n-\n ## BltConfig\n \n [[autodoc]] BltConfig"
        },
        {
            "sha": "861dd32c16fef64d92579aadc15b908ae448ed55",
            "filename": "docs/source/en/model_doc/bridgetower.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbridgetower.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbridgetower.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbridgetower.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -26,7 +26,7 @@ rendered properly in your Markdown viewer.\n The BridgeTower model was proposed in [BridgeTower: Building Bridges Between Encoders in Vision-Language Representative Learning](https://huggingface.co/papers/2206.08657) by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan. The goal of this model is to build a\n bridge between each uni-modal encoder and the cross-modal encoder to enable comprehensive and detailed interaction at each layer of the cross-modal encoder thus achieving remarkable performance on various downstream tasks with almost negligible additional performance and computational costs.\n \n-This paper has been accepted to the [AAAI'23](https://aaai.org/Conferences/AAAI-23/) conference. \n+This paper has been accepted to the [AAAI'23](https://aaai.org/Conferences/AAAI-23/) conference.\n \n The abstract from the paper is the following:\n \n@@ -54,6 +54,7 @@ The [`BridgeTowerProcessor`] wraps [`RobertaTokenizer`] and [`BridgeTowerImagePr\n encode the text and prepare the images respectively.\n \n The following example shows how to run contrastive learning using [`BridgeTowerProcessor`] and [`BridgeTowerForContrastiveLearning`].\n+\n ```python\n >>> from transformers import BridgeTowerProcessor, BridgeTowerForContrastiveLearning\n >>> import requests\n@@ -76,6 +77,7 @@ The following example shows how to run contrastive learning using [`BridgeTowerP\n ```\n \n The following example shows how to run image-text retrieval using [`BridgeTowerProcessor`] and [`BridgeTowerForImageAndTextRetrieval`].\n+\n ```python\n >>> from transformers import BridgeTowerProcessor, BridgeTowerForImageAndTextRetrieval\n >>> import requests\n@@ -130,7 +132,6 @@ Tips:\n - Please refer to [Table 5](https://huggingface.co/papers/2206.08657) for BridgeTower's performance on Image Retrieval and other down stream tasks.\n - The PyTorch version of this model is only available in torch 1.10 and higher.\n \n-\n ## BridgeTowerConfig\n \n [[autodoc]] BridgeTowerConfig\n@@ -177,4 +178,3 @@ Tips:\n \n [[autodoc]] BridgeTowerForImageAndTextRetrieval\n     - forward\n-"
        },
        {
            "sha": "4ef3d3737ae204a4ae7bf3e1183cd648d61e03c5",
            "filename": "docs/source/en/model_doc/bros.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbros.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbros.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbros.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -57,7 +57,6 @@ def expand_and_normalize_bbox(bboxes, doc_width, doc_height):\n \n - [`~transformers.BrosForTokenClassification.forward`, `~transformers.BrosSpadeEEForTokenClassification.forward`, `~transformers.BrosSpadeEEForTokenClassification.forward`] require not only `input_ids` and `bbox` but also `box_first_token_mask` for loss calculation. It is a mask to filter out non-first tokens of each box. You can obtain this mask by saving start token indices of bounding boxes when creating `input_ids` from words. You can make `box_first_token_mask` with following code,\n \n-\n ```python\n def make_box_first_token_mask(bboxes, words, tokenizer, max_seq_length=512):\n \n@@ -102,7 +101,6 @@ def make_box_first_token_mask(bboxes, words, tokenizer, max_seq_length=512):\n [[autodoc]] BrosModel\n     - forward\n \n-\n ## BrosForTokenClassification\n \n [[autodoc]] BrosForTokenClassification"
        },
        {
            "sha": "971954ed52a135205f564c7f5c9a47043e94401d",
            "filename": "docs/source/en/model_doc/camembert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -50,6 +50,7 @@ from transformers import pipeline\n pipeline = pipeline(\"fill-mask\", model=\"camembert-base\", dtype=torch.float16, device=0)\n pipeline(\"Le camembert est un délicieux fromage <mask>.\")\n ```\n+\n </hfoption>\n \n <hfoption id=\"AutoModel\">\n@@ -72,6 +73,7 @@ predicted_token = tokenizer.decode(predicted_token_id)\n \n print(f\"The predicted token is: {predicted_token}\")\n ```\n+\n </hfoption>\n \n <hfoption id=\"transformers CLI\">\n@@ -84,7 +86,6 @@ echo -e \"Le camembert est un délicieux fromage <mask>.\" | transformers run --ta\n \n </hfoptions>\n \n-\n Quantization reduces the memory burden of large models by representing weights in lower precision. Refer to the [Quantization](../quantization/overview) overview for available options.\n \n The example below uses [bitsandbytes](../quantization/bitsandbytes) quantization to quantize the weights to 8-bits."
        },
        {
            "sha": "53691dcbc22cec81c128d8253b61f4d32f399356",
            "filename": "docs/source/en/model_doc/canine.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcanine.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcanine.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcanine.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -86,6 +86,7 @@ echo -e \"Plant create energy through a process known as photosynthesis.\" | trans\n     inputs = [\"Life is like a box of chocolates.\", \"You never know what you gonna get.\"]\n     encoding = tokenizer(inputs, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n     ```\n+\n - CANINE is primarily designed to be fine-tuned on a downstream task. The pretrained model can be used for either masked language modeling or next sentence prediction.\n \n ## CanineConfig"
        },
        {
            "sha": "dc573faa11129a219daab58775bb4b4e7e035b05",
            "filename": "docs/source/en/model_doc/chameleon.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -28,7 +28,6 @@ rendered properly in your Markdown viewer.\n The Chameleon model was proposed in [Chameleon: Mixed-Modal Early-Fusion Foundation Models\n ](https://huggingface.co/papers/2405.09818) by META AI Chameleon Team. Chameleon is a Vision-Language Model that use vector quantization to tokenize images which enables the model to generate multimodal output. The model takes images and texts as input, including an interleaved format, and generates textual response. Image generation module is not released yet.\n \n-\n The abstract from the paper is the following:\n \n *We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training\n@@ -43,7 +42,6 @@ including Gemini Pro and GPT-4V, according to human judgments on a new long-form\n generation evaluation, where either the prompt or outputs contain mixed sequences of both images and\n text. Chameleon marks a significant step forward in unified modeling of full multimodal documents*\n \n-\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/chameleon_arch.png\"\n alt=\"drawing\" width=\"600\"/>\n \n@@ -52,7 +50,6 @@ alt=\"drawing\" width=\"600\"/>\n This model was contributed by [joaogante](https://huggingface.co/joaogante) and [RaushanTurganbay](https://huggingface.co/RaushanTurganbay).\n The original code can be found [here](https://github.com/facebookresearch/chameleon).\n \n-\n ## Usage tips\n \n - We advise users to use `padding_side=\"left\"` when computing batched generation as it leads to more accurate results. Simply make sure to set `processor.tokenizer.padding_side = \"left\"` before generating."
        },
        {
            "sha": "7ca9b3926ac9c8502df778f0135f0c88a637786e",
            "filename": "docs/source/en/model_doc/clipseg.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -47,7 +47,7 @@ can be formulated. Finally, we find our system to adapt well\n to generalized queries involving affordances or properties*\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_architecture.png\"\n-alt=\"drawing\" width=\"600\"/> \n+alt=\"drawing\" width=\"600\"/>\n \n <small> CLIPSeg overview. Taken from the <a href=\"https://huggingface.co/papers/2112.10003\">original paper.</a> </small>\n "
        },
        {
            "sha": "eead4a5464354556303144ad3ea26ed7b995936e",
            "filename": "docs/source/en/model_doc/clvp.md",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fclvp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fclvp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclvp.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -29,29 +29,25 @@ The abstract from the paper is the following:\n \n *In recent years, the field of image generation has been revolutionized by the application of autoregressive transformers and DDPMs. These approaches model the process of image generation as a step-wise probabilistic processes and leverage large amounts of compute and data to learn the image distribution. This methodology of improving performance need not be confined to images. This paper describes a way to apply advances in the image generative domain to speech synthesis. The result is TorToise - an expressive, multi-voice text-to-speech system.*\n \n-\n This model was contributed by [Susnato Dhar](https://huggingface.co/susnato).\n The original code can be found [here](https://github.com/neonbjb/tortoise-tts).\n \n-\n ## Usage tips\n \n 1. CLVP is an integral part of the Tortoise TTS model.\n 2. CLVP can be used to compare different generated speech candidates with the provided text, and the best speech tokens are forwarded to the diffusion model.\n 3. The use of the [`ClvpModelForConditionalGeneration.generate()`] method is strongly recommended for tortoise usage.\n-4. Note that the CLVP model expects the audio to be sampled at 22.05 kHz contrary to other audio models which expects 16 kHz. \n-\n+4. Note that the CLVP model expects the audio to be sampled at 22.05 kHz contrary to other audio models which expects 16 kHz.\n \n ## Brief Explanation:\n \n - The [`ClvpTokenizer`] tokenizes the text input, and the [`ClvpFeatureExtractor`] extracts the log mel-spectrogram from the desired audio.\n - [`ClvpConditioningEncoder`] takes those text tokens and audio representations and converts them into embeddings conditioned on the text and audio.\n - The [`ClvpForCausalLM`] uses those embeddings to generate multiple speech candidates.\n-- Each speech candidate is passed through the speech encoder ([`ClvpEncoder`]) which converts them into a vector representation, and the text encoder ([`ClvpEncoder`]) converts the text tokens into the same latent space. \n-- At the end, we compare each speech vector with the text vector to see which speech vector is most similar to the text vector. \n+- Each speech candidate is passed through the speech encoder ([`ClvpEncoder`]) which converts them into a vector representation, and the text encoder ([`ClvpEncoder`]) converts the text tokens into the same latent space.\n+- At the end, we compare each speech vector with the text vector to see which speech vector is most similar to the text vector.\n - [`ClvpModelForConditionalGeneration.generate()`] compresses all of the logic described above into a single method.  \n \n-\n Example :\n \n ```python\n@@ -74,7 +70,6 @@ Example :\n >>> generated_output = model.generate(**processor_output)\n ```\n \n-\n ## ClvpConfig\n \n [[autodoc]] ClvpConfig\n@@ -128,4 +123,3 @@ Example :\n ## ClvpDecoder\n \n [[autodoc]] ClvpDecoder\n-"
        },
        {
            "sha": "a46e1f05b32a042ae0ea03ce4c7f12617210a340",
            "filename": "docs/source/en/model_doc/code_llama.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcode_llama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcode_llama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcode_llama.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -143,6 +143,7 @@ visualizer(\"\"\"def func(a, b):\n \n - Infilling is only available in the 7B and 13B base models, and not in the Python, Instruct, 34B, or 70B models.\n - Use the `<FILL_ME>` token where you want your input to be filled. The tokenizer splits this token to create a formatted input string that follows the [original training pattern](https://github.com/facebookresearch/codellama/blob/cb51c14ec761370ba2e2bc351374a79265d0465e/llama/generation.py#L402). This is more robust than preparing the pattern yourself.\n+\n     ```py\n     from transformers import LlamaForCausalLM, CodeLlamaTokenizer\n \n@@ -158,6 +159,7 @@ visualizer(\"\"\"def func(a, b):\n     filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]\n     print(PROMPT.replace(\"<FILL_ME>\", filling))\n     ```\n+\n - Use `bfloat16` for further training or fine-tuning and `float16` for inference.\n - The `BOS` character is not used for infilling when encoding the prefix or suffix, but only at the beginning of each prompt.\n - The tokenizer is a byte-pair encoding model based on [SentencePiece](https://github.com/google/sentencepiece). During decoding, if the first token is the start of the word (for example, “Banana”), the tokenizer doesn’t prepend the prefix space to the string."
        },
        {
            "sha": "c341154921e356d21e2379ef4afa11a727a3c118",
            "filename": "docs/source/en/model_doc/codegen.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcodegen.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcodegen.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcodegen.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -29,7 +29,7 @@ CodeGen is an autoregressive language model for program synthesis trained sequen\n \n The abstract from the paper is the following:\n \n-*Program synthesis strives to generate a computer program as a solution to a given problem specification. We propose a conversational program synthesis approach via large language models, which addresses the challenges of searching over a vast program space and user intent specification faced in prior approaches. Our new approach casts the process of writing a specification and program as a multi-turn conversation between a user and a system. It treats program synthesis as a sequence prediction problem, in which the specification is expressed in natural language and the desired program is conditionally sampled. We train a family of large language models, called CodeGen, on natural language and programming language data. With weak supervision in the data and the scaling up of data size and model size, conversational capacities emerge from the simple autoregressive language modeling. To study the model behavior on conversational program synthesis, we develop a multi-turn programming benchmark (MTPB), where solving each problem requires multi-step synthesis via multi-turn conversation between the user and the model. Our findings show the emergence of conversational capabilities and the effectiveness of the proposed conversational program synthesis paradigm. In addition, our model CodeGen (with up to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the HumanEval benchmark. We make the training library JaxFormer including checkpoints available as open source contribution: [this https URL](https://github.com/salesforce/codegen).* \n+*Program synthesis strives to generate a computer program as a solution to a given problem specification. We propose a conversational program synthesis approach via large language models, which addresses the challenges of searching over a vast program space and user intent specification faced in prior approaches. Our new approach casts the process of writing a specification and program as a multi-turn conversation between a user and a system. It treats program synthesis as a sequence prediction problem, in which the specification is expressed in natural language and the desired program is conditionally sampled. We train a family of large language models, called CodeGen, on natural language and programming language data. With weak supervision in the data and the scaling up of data size and model size, conversational capacities emerge from the simple autoregressive language modeling. To study the model behavior on conversational program synthesis, we develop a multi-turn programming benchmark (MTPB), where solving each problem requires multi-step synthesis via multi-turn conversation between the user and the model. Our findings show the emergence of conversational capabilities and the effectiveness of the proposed conversational program synthesis paradigm. In addition, our model CodeGen (with up to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the HumanEval benchmark. We make the training library JaxFormer including checkpoints available as open source contribution: [this https URL](https://github.com/salesforce/codegen).*\n \n This model was contributed by [Hiroaki Hayashi](https://huggingface.co/rooa).\n The original code can be found [here](https://github.com/salesforce/codegen).\n@@ -39,7 +39,7 @@ The original code can be found [here](https://github.com/salesforce/codegen).\n * CodeGen model [checkpoints](https://huggingface.co/models?other=codegen) are available on different pre-training data with variable sizes.\n * The format is: `Salesforce/codegen-{size}-{data}`, where\n   * `size`: `350M`, `2B`, `6B`, `16B`\n-  * `data`: \n+  * `data`:\n     * `nl`: Pre-trained on the Pile\n     * `multi`: Initialized with `nl`, then further pre-trained on multiple programming languages data\n     * `mono`: Initialized with `multi`, then further pre-trained on Python data"
        },
        {
            "sha": "b8ccf20706af12cec295474958f91cc253c3b03d",
            "filename": "docs/source/en/model_doc/cohere.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -22,14 +22,12 @@ rendered properly in your Markdown viewer.\n     </div>\n </div>\n \n-\n # Cohere\n \n Cohere [Command-R](https://cohere.com/blog/command-r) is a 35B parameter multilingual large language model designed for long context tasks like retrieval-augmented generation (RAG) and calling external APIs and tools. The model is specifically trained for grounded generation and supports both single-step and multi-step tool use. It supports a context length of 128K tokens.\n \n You can find all the original Command-R checkpoints under the [Command Models](https://huggingface.co/collections/CohereForAI/command-models-67652b401665205e17b192ad) collection.\n \n-\n > [!TIP]\n > Click on the Cohere models in the right sidebar for more examples of how to apply Cohere to different language tasks.\n \n@@ -123,7 +121,6 @@ visualizer(\"Plants create energy through a process known as\")\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/cohere-attn-mask.png\"/>\n </div>\n \n-\n ## Notes\n - Don’t use the dtype parameter in [`~AutoModel.from_pretrained`] if you’re using FlashAttention-2 because it only supports fp16 or bf16. You should use [Automatic Mixed Precision](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html), set fp16 or bf16 to True if using [`Trainer`], or use [torch.autocast](https://pytorch.org/docs/stable/amp.html#torch.autocast).\n \n@@ -145,7 +142,6 @@ visualizer(\"Plants create energy through a process known as\")\n [[autodoc]] CohereModel\n     - forward\n \n-\n ## CohereForCausalLM\n \n [[autodoc]] CohereForCausalLM"
        },
        {
            "sha": "ed94fef1da133119def775cbfcd2e7d12f4d3714",
            "filename": "docs/source/en/model_doc/cohere2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -22,7 +22,6 @@ rendered properly in your Markdown viewer.\n     </div>\n </div>\n \n-\n # Cohere 2\n \n [Cohere Command R7B](https://cohere.com/blog/command-r7b) is an open weights research release of a 7B billion parameter model. It is a multilingual model trained on 23 languages and has a context window of 128k. The model features three layers with sliding window attention and ROPE for efficient local context modeling and relative positional encoding. A fourth layer uses global attention without positional embeddings, enabling unrestricted token interactions across the entire sequence.\n@@ -31,7 +30,6 @@ This model is optimized for speed, cost-performance, and compute resources.\n \n You can find all the original Command-R checkpoints under the [Command Models](https://huggingface.co/collections/CohereForAI/command-models-67652b401665205e17b192ad) collection.\n \n-\n > [!TIP]\n > Click on the Cohere models in the right sidebar for more examples of how to apply Cohere to different language tasks.\n \n@@ -136,7 +134,6 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n [[autodoc]] Cohere2Model\n     - forward\n \n-\n ## Cohere2ForCausalLM\n \n [[autodoc]] Cohere2ForCausalLM"
        },
        {
            "sha": "e466ce6a5f097803c780b8763f2983528fc0c5bb",
            "filename": "docs/source/en/model_doc/cohere2_vision.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2_vision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2_vision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2_vision.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -113,6 +113,7 @@ outputs = pipe(text=messages, max_new_tokens=300, return_full_text=False)\n \n print(outputs)\n ```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "275f5629db13e58f180547378f24cc7009c4e604",
            "filename": "docs/source/en/model_doc/cpm.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpm.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -42,15 +42,13 @@ NLP tasks in the settings of few-shot (even zero-shot) learning.*\n This model was contributed by [canwenxu](https://huggingface.co/canwenxu). The original implementation can be found\n here: https://github.com/TsinghuaAI/CPM-Generate\n \n-\n <Tip>\n \n CPM's architecture is the same as GPT-2, except for tokenization method. Refer to [GPT-2 documentation](gpt2) for\n API reference information.\n \n </Tip>\n \n-\n ## CpmTokenizer\n \n [[autodoc]] CpmTokenizer"
        },
        {
            "sha": "47eec6e79d691d625676f60ecd2cd08cdff3f2bf",
            "filename": "docs/source/en/model_doc/cpmant.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpmant.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpmant.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpmant.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -45,7 +45,7 @@ This model was contributed by [OpenBMB](https://huggingface.co/openbmb). The ori\n \n [[autodoc]] CpmAntModel\n     - all\n-    \n+\n ## CpmAntForCausalLM\n \n [[autodoc]] CpmAntForCausalLM"
        },
        {
            "sha": "162832470482f4408fadc2fa1fe73de4e56317a2",
            "filename": "docs/source/en/model_doc/csm.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -346,7 +346,6 @@ out.loss.backward()\n This model was contributed by [Eustache Le Bihan](https://huggingface.co/eustlb).\n The original code can be found [here](https://github.com/SesameAILabs/csm).\n \n-\n ## CsmConfig\n \n [[autodoc]] CsmConfig"
        },
        {
            "sha": "6244ee0a59efa498f0e75a92901a5f18508b732e",
            "filename": "docs/source/en/model_doc/ctrl.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fctrl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fctrl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fctrl.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -55,7 +55,6 @@ This model was contributed by [keskarnitishr](https://huggingface.co/keskarnitis\n   pre-computed values in the context of text generation. See the [`forward`](model_doc/ctrl#transformers.CTRLModel.forward)\n   method for more information on the usage of this argument.\n \n-\n ## Resources\n \n - [Text classification task guide](../tasks/sequence_classification)"
        },
        {
            "sha": "05e855d333b59996aa850becf43bc4f1d77eec48",
            "filename": "docs/source/en/model_doc/d_fine.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fd_fine.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fd_fine.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fd_fine.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -24,13 +24,13 @@ Yansong Peng, Hebei Li, Peixi Wu, Yueyi Zhang, Xiaoyan Sun, Feng Wu\n \n The abstract from the paper is the following:\n \n-*We introduce D-FINE, a powerful real-time object detector that achieves outstanding localization precision by redefining the bounding box regression task in DETR models. D-FINE comprises two key components: Fine-grained Distribution Refinement (FDR) and Global Optimal Localization Self-Distillation (GO-LSD). \n+*We introduce D-FINE, a powerful real-time object detector that achieves outstanding localization precision by redefining the bounding box regression task in DETR models. D-FINE comprises two key components: Fine-grained Distribution Refinement (FDR) and Global Optimal Localization Self-Distillation (GO-LSD).\n FDR transforms the regression process from predicting fixed coordinates to iteratively refining probability distributions, providing a fine-grained intermediate representation that significantly enhances localization accuracy. GO-LSD is a bidirectional optimization strategy that transfers localization knowledge from refined distributions to shallower layers through self-distillation, while also simplifying the residual prediction tasks for deeper layers. Additionally, D-FINE incorporates lightweight optimizations in computationally intensive modules and operations, achieving a better balance between speed and accuracy. Specifically, D-FINE-L / X achieves 54.0% / 55.8% AP on the COCO dataset at 124 / 78 FPS on an NVIDIA T4 GPU. When pretrained on Objects365, D-FINE-L / X attains 57.1% / 59.3% AP, surpassing all existing real-time detectors. Furthermore, our method significantly enhances the performance of a wide range of DETR models by up to 5.3% AP with negligible extra parameters and training costs. Our code and pretrained models: this https URL.*\n \n-This model was contributed by [VladOS95-cyber](https://github.com/VladOS95-cyber). \n+This model was contributed by [VladOS95-cyber](https://github.com/VladOS95-cyber).\n The original code can be found [here](https://github.com/Peterande/D-FINE).\n \n-## Usage tips \n+## Usage tips\n \n ```python\n >>> import torch"
        },
        {
            "sha": "d85988ec1f55408420bb5f9e8dd2ccf1a2c9a318",
            "filename": "docs/source/en/model_doc/dab-detr.md",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdab-detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdab-detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdab-detr.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -77,7 +77,9 @@ for result in results:\n         box = [round(i, 2) for i in box.tolist()]\n         print(f\"{model.config.id2label[label]}: {score:.2f} {box}\")\n ```\n+\n This should output\n+\n ```\n cat: 0.87 [14.7, 49.39, 320.52, 469.28]\n remote: 0.86 [41.08, 72.37, 173.39, 117.2]\n@@ -89,26 +91,29 @@ couch: 0.59 [-0.04, 1.34, 639.9, 477.09]\n There are three other ways to instantiate a DAB-DETR model (depending on what you prefer):\n \n Option 1: Instantiate DAB-DETR with pre-trained weights for entire model\n+\n ```py\n >>> from transformers import DabDetrForObjectDetection\n \n >>> model = DabDetrForObjectDetection.from_pretrained(\"IDEA-Research/dab-detr-resnet-50\")\n ```\n \n Option 2: Instantiate DAB-DETR with randomly initialized weights for Transformer, but pre-trained weights for backbone\n+\n ```py\n >>> from transformers import DabDetrConfig, DabDetrForObjectDetection\n \n >>> config = DabDetrConfig()\n >>> model = DabDetrForObjectDetection(config)\n ```\n+\n Option 3: Instantiate DAB-DETR with randomly initialized weights for backbone + Transformer\n+\n ```py\n >>> config = DabDetrConfig(use_pretrained_backbone=False)\n >>> model = DabDetrForObjectDetection(config)\n ```\n \n-\n ## DabDetrConfig\n \n [[autodoc]] DabDetrConfig"
        },
        {
            "sha": "94f70fdff32a568256cba19e3b8da12a8c0a17c3",
            "filename": "docs/source/en/model_doc/dac.md",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdac.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdac.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdac.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -23,7 +23,6 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-\n The DAC model was proposed in [Descript Audio Codec: High-Fidelity Audio Compression with Improved RVQGAN](https://huggingface.co/papers/2306.06546) by Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, Kundan Kumar.\n \n The Descript Audio Codec (DAC) model is a powerful tool for compressing audio data, making it highly efficient for storage and transmission. By compressing 44.1 KHz audio into tokens at just 8kbps bandwidth, the DAC model enables high-quality audio processing while significantly reducing the data footprint. This is particularly useful in scenarios where bandwidth is limited or storage space is at a premium, such as in streaming applications, remote conferencing, and archiving large audio datasets.\n@@ -35,7 +34,6 @@ The abstract from the paper is the following:\n This model was contributed by [Kamil Akesbi](https://huggingface.co/kamilakesbi).\n The original code can be found [here](https://github.com/descriptinc/descript-audio-codec/tree/main?tab=readme-ov-file).\n \n-\n ## Model structure\n \n The Descript Audio Codec (DAC) model is structured into three distinct stages:\n@@ -44,11 +42,11 @@ The Descript Audio Codec (DAC) model is structured into three distinct stages:\n 2. Residual Vector Quantizer (RVQ) Model: Working in tandem with the encoder, this model quantizes the latent codes of the audio, refining the compression and ensuring high-quality reconstruction.\n 3. Decoder Model: This final stage reconstructs the audio from its compressed form, restoring it to a state that closely resembles the original input.\n \n-## Usage example \n+## Usage example\n \n-Here is a quick example of how to encode and decode an audio using this model: \n+Here is a quick example of how to encode and decode an audio using this model:\n \n-```python \n+```python\n >>> from datasets import load_dataset, Audio\n >>> from transformers import DacModel, AutoProcessor\n >>> librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
        },
        {
            "sha": "a97e594e415aa080fa4e4635f3bd74e9d76facd6",
            "filename": "docs/source/en/model_doc/dbrx.md",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdbrx.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdbrx.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdbrx.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -35,7 +35,6 @@ We estimate that this data is at least 2x better token-for-token than the data w\n This new dataset was developed using the full suite of Databricks tools, including Apache Spark™ and Databricks notebooks for data processing, and Unity Catalog for data management and governance.\n We used curriculum learning for pretraining, changing the data mix during training in ways we found to substantially improve model quality.\n \n-\n More detailed information about DBRX Instruct and DBRX Base can be found in our [technical blog post](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm).\n \n This model was contributed by [eitan-turok](https://huggingface.co/eitanturok) and [abhi-db](https://huggingface.co/abhi-db). The original code can be found [here](https://github.com/databricks/dbrx-instruct), though this may not be up to date.\n@@ -65,6 +64,7 @@ print(tokenizer.decode(outputs[0]))\n ```\n \n If you have flash-attention installed (`pip install flash-attn`), it is possible to generate faster. (The HuggingFace documentation for flash-attention can be found [here](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2).)\n+\n ```python\n from transformers import DbrxForCausalLM, AutoTokenizer\n import torch\n@@ -87,6 +87,7 @@ print(tokenizer.decode(outputs[0]))\n ```\n \n You can also generate faster using the PyTorch scaled dot product attention. (The HuggingFace documentation for scaled dot product attention can be found [here](https://huggingface.co/docs/transformers/perf_infer_gpu_one#pytorch-scaled-dot-product-attention).)\n+\n ```python\n from transformers import DbrxForCausalLM, AutoTokenizer\n import torch\n@@ -112,15 +113,12 @@ print(tokenizer.decode(outputs[0]))\n \n [[autodoc]] DbrxConfig\n \n-\n ## DbrxModel\n \n [[autodoc]] DbrxModel\n     - forward\n \n-\n ## DbrxForCausalLM\n \n [[autodoc]] DbrxForCausalLM\n     - forward\n-"
        },
        {
            "sha": "6ec0c0e51176b3553641e8539638cfc2379f8c24",
            "filename": "docs/source/en/model_doc/deberta-v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -21,14 +21,12 @@ rendered properly in your Markdown viewer.\n     </div>\n </div>\n \n-\n # DeBERTa-v2\n \n [DeBERTa-v2](https://huggingface.co/papers/2006.03654) improves on the original [DeBERTa](./deberta) architecture by using a SentencePiece-based tokenizer and a new vocabulary size of 128K. It also adds an additional convolutional layer within the first transformer layer to better learn local dependencies of input tokens. Finally, the position projection and content projection matrices are shared in the attention layer to reduce the number of parameters.\n \n You can find all the original [DeBERTa-v2] checkpoints under the [Microsoft](https://huggingface.co/microsoft?search_models=deberta-v2) organization.\n \n-\n > [!TIP]\n > This model was contributed by [Pengcheng He](https://huggingface.co/DeBERTa).\n >\n@@ -86,6 +84,7 @@ print(f\"Predicted label: {predicted_label}\")\n ```bash\n echo -e \"DeBERTa-v2 is great at understanding context!\" | transformers run --task fill-mask --model microsoft/deberta-v2-xlarge-mnli --device 0\n ```\n+\n </hfoption>\n </hfoptions>\n \n@@ -119,7 +118,6 @@ print(f\"Predicted label: {predicted_label}\")\n \n ```\n \n-\n ## DebertaV2Config\n \n [[autodoc]] DebertaV2Config"
        },
        {
            "sha": "76fe8e1a3b631ab200261a0a8c9fcfe928564418",
            "filename": "docs/source/en/model_doc/deberta.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -31,7 +31,6 @@ Even with less training data than RoBERTa, DeBERTa manages to outperform it on s\n \n You can find all the original DeBERTa checkpoints under the [Microsoft](https://huggingface.co/microsoft?search_models=deberta) organization.\n \n-\n > [!TIP]\n > Click on the DeBERTa models in the right sidebar for more examples of how to apply DeBERTa to different language tasks.\n "
        },
        {
            "sha": "349b8eaae2e7232d8cfc1d229c98ac5abf64121f",
            "filename": "docs/source/en/model_doc/decision_transformer.md",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdecision_transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdecision_transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdecision_transformer.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -28,14 +28,14 @@ by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael La\n \n The abstract from the paper is the following:\n \n-*We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. \n+*We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem.\n This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances\n- in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that \n- casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or \n- compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked \n- Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our \n- Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, \n- Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on \n+ in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that\n+ casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or\n+ compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked\n+ Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our\n+ Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity,\n+ Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on\n  Atari, OpenAI Gym, and Key-to-Door tasks.*\n \n This version of the model is for tasks where the state is a vector.\n@@ -46,7 +46,6 @@ This model was contributed by [edbeeching](https://huggingface.co/edbeeching). T\n \n [[autodoc]] DecisionTransformerConfig\n \n-\n ## DecisionTransformerGPT2Model\n \n [[autodoc]] DecisionTransformerGPT2Model"
        },
        {
            "sha": "81724e3994352bcf924ededb5892a4f33319eea0",
            "filename": "docs/source/en/model_doc/deepseek_v3.md",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -26,17 +26,17 @@ We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 67\n \n ## Limitations and call for contribution!\n \n-We are super happy to make this code community-powered, and would love to see how you can best optimize the following: \n+We are super happy to make this code community-powered, and would love to see how you can best optimize the following:\n \n - current implementation uses the \"naive\" attention compution (so not really MLA)\n-- current implementation loops through the experts. This should be replaced. Pointers to use `get_packed_weights` from `integrations/tensor_parallel`. \n+- current implementation loops through the experts. This should be replaced. Pointers to use `get_packed_weights` from `integrations/tensor_parallel`.\n - current implementation uses the eleuther formula for ROPE, using the original one would be more efficient! (should still follow our API)\n - static cache is not supported (this should be just a generation config issue / config shape issues)\n \n ### Usage tips\n The model uses Multi-head Latent Attention (MLA) and DeepSeekMoE architectures for efficient inference and cost-effective training. It employs an auxiliary-loss-free strategy for load balancing and multi-token prediction training objective. The model can be used for various language tasks after being pre-trained on 14.8 trillion tokens and going through Supervised Fine-Tuning and Reinforcement Learning stages.\n \n-You can run the model in `FP8` automatically, using 2 nodes of 8 H100 should be more than enough! \n+You can run the model in `FP8` automatically, using 2 nodes of 8 H100 should be more than enough!\n \n ```python\n # `run_deepseek_v1.py`\n@@ -61,7 +61,8 @@ outputs = model.generate(inputs, max_new_tokens=50)\n print(tokenizer.batch_decode(outputs))\n print(time.time()-start)\n ```\n-This generated: \n+\n+This generated:\n \n ``````\n <｜Assistant｜><think>\n@@ -157,18 +158,20 @@ Want to dive deeper or see a specific framework’s implementation (e.g., OpenAI\n ``````\n \n Use the following to run it\n+\n ```bash\n torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0|1 --rdzv-id an_id --rdzv-backend c10d --rdzv-endpoint master_addr:master_port run_deepseek_r1.py\n ```\n \n-If you have: \n+If you have:\n+\n ```bash\n [rank0]: ncclInternalError: Internal check failed.\n [rank0]: Last error:\n [rank0]: Bootstrap : no socket interface found\n ```\n-error, it means NCCL was probably not loaded. \n \n+error, it means NCCL was probably not loaded.\n \n ## DeepseekV3Config\n "
        },
        {
            "sha": "710e6144bb0e6935b38b2a4310ff24a42a5eaa7c",
            "filename": "docs/source/en/model_doc/deepseek_vl.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -63,6 +63,7 @@ messages = [\n \n pipe(text=messages, max_new_tokens=20, return_full_text=False)\n ```\n+\n </hfoption>\n \n <hfoption id=\"AutoModel\">\n@@ -115,6 +116,7 @@ output_text = processor.batch_decode(\n \n print(output_text)\n ```\n+\n </hfoption>\n </hfoptions>\n \n@@ -138,9 +140,11 @@ model = DeepseekVLForConditionalGeneration.from_pretrained(\n     quantization_config=quantization_config\n )\n ```\n+\n ### Notes\n \n - Do inference with multiple images in a single conversation.\n+\n     ```py\n     import torch\n     from transformers import DeepseekVLForConditionalGeneration, AutoProcessor"
        },
        {
            "sha": "0613b50f1ad877f67a757324b8852fb98f66a057",
            "filename": "docs/source/en/model_doc/deepseek_vl_hybrid.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -62,6 +62,7 @@ messages = [\n \n pipe(text=messages, max_new_tokens=20, return_full_text=False)\n ```\n+\n </hfoption>\n \n <hfoption id=\"AutoModel\">\n@@ -114,6 +115,7 @@ output_text = processor.batch_decode(\n \n print(output_text)\n ```\n+\n </hfoption>\n </hfoptions>\n \n@@ -137,9 +139,11 @@ model = DeepseekVLHybridForConditionalGeneration.from_pretrained(\n     quantization_config=quantization_config\n )\n ```\n+\n ### Notes\n \n - Do inference with multiple images in a single conversation.\n+\n     ```py\n     import torch\n     from transformers import DeepseekVLHybridForConditionalGeneration, AutoProcessor"
        },
        {
            "sha": "0eb3975530ab6e1dd0d23d8b48ba487ca0614e84",
            "filename": "docs/source/en/model_doc/deplot.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeplot.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeplot.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeplot.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -21,7 +21,7 @@ rendered properly in your Markdown viewer.\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n-## Overview \n+## Overview\n \n DePlot was proposed in the paper [DePlot: One-shot visual language reasoning by plot-to-table translation](https://huggingface.co/papers/2212.10505) from Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.\n \n@@ -36,8 +36,7 @@ DePlot is a Visual Question Answering subset of `Pix2Struct` architecture. It re\n \n Currently one checkpoint is available for DePlot:\n \n-- `google/deplot`: DePlot fine-tuned on ChartQA dataset \n-\n+- `google/deplot`: DePlot fine-tuned on ChartQA dataset\n \n ```python\n from transformers import AutoProcessor, Pix2StructForConditionalGeneration\n@@ -57,6 +56,7 @@ print(processor.decode(predictions[0], skip_special_tokens=True))\n ## Fine-tuning\n \n To fine-tune DePlot, refer to the pix2struct [fine-tuning notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb). For `Pix2Struct` models, we have found out that fine-tuning the model with Adafactor and cosine learning rate scheduler leads to faster convergence:\n+\n ```python\n from transformers.optimization import Adafactor, get_cosine_schedule_with_warmup\n "
        },
        {
            "sha": "6872fca5138bf2a57aa71c1495de5e4ea981e197",
            "filename": "docs/source/en/model_doc/depth_pro.md",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -102,19 +102,22 @@ The network is supplemented with a focal length estimation head. A small convolu\n The `use_fov_model` parameter in `DepthProConfig` controls whether **FOV prediction** is enabled. By default, it is set to `False` to conserve memory and computation. When enabled, the **FOV encoder** is instantiated based on the `fov_model_config` parameter, which defaults to a `Dinov2Model`. The `use_fov_model` parameter can also be passed when initializing the `DepthProForDepthEstimation` model.\n \n The pretrained model at checkpoint `apple/DepthPro-hf` uses the FOV encoder. To use the pretrained-model without FOV encoder, set `use_fov_model=False` when loading the model, which saves computation.\n+\n ```py\n >>> from transformers import DepthProForDepthEstimation\n >>> model = DepthProForDepthEstimation.from_pretrained(\"apple/DepthPro-hf\", use_fov_model=False)\n ```\n \n To instantiate a new model with FOV encoder, set `use_fov_model=True` in the config.\n+\n ```py\n >>> from transformers import DepthProConfig, DepthProForDepthEstimation\n >>> config = DepthProConfig(use_fov_model=True)\n >>> model = DepthProForDepthEstimation(config)\n ```\n \n Or set `use_fov_model=True` when initializing the model, which overrides the value in config.\n+\n ```py\n >>> from transformers import DepthProConfig, DepthProForDepthEstimation\n >>> config = DepthProConfig()\n@@ -123,13 +126,13 @@ Or set `use_fov_model=True` when initializing the model, which overrides the val\n \n ### Using Scaled Dot Product Attention (SDPA)\n \n-PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n-encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n-[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n page for more information.\n \n-SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set \n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n \n ```py"
        },
        {
            "sha": "6d7792803c59081f895808f7b2db2b0aab3532fa",
            "filename": "docs/source/en/model_doc/detr.md",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -113,13 +113,15 @@ DETR can be naturally extended to perform panoptic segmentation (which unifies s\n There are three other ways to instantiate a DETR model (depending on what you prefer):\n \n - Option 1: Instantiate DETR with pre-trained weights for entire model\n+\n ```python\n from transformers import DetrForObjectDetection\n \n model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n ```\n \n - Option 2: Instantiate DETR with randomly initialized weights for Transformer, but pre-trained weights for backbone\n+\n ```python\n from transformers import DetrConfig, DetrForObjectDetection\n \n@@ -128,6 +130,7 @@ model = DetrForObjectDetection(config)\n ```\n \n - Option 3: Instantiate DETR with randomly initialized weights for backbone + Transformer\n+\n ```python\n config = DetrConfig(use_pretrained_backbone=False)\n model = DetrForObjectDetection(config)\n@@ -144,7 +147,7 @@ As a summary, consider the following table:\n | **Postprocessing** (i.e. converting the output of the model to Pascal VOC format) | [`~transformers.DetrImageProcessor.post_process`] | [`~transformers.DetrImageProcessor.post_process_segmentation`] | [`~transformers.DetrImageProcessor.post_process_segmentation`], [`~transformers.DetrImageProcessor.post_process_panoptic`] |\n | **evaluators** | `CocoEvaluator` with `iou_types=\"bbox\"` | `CocoEvaluator` with `iou_types=\"bbox\"` or `\"segm\"` | `CocoEvaluator` with `iou_tupes=\"bbox\"` or `\"segm\"`, `PanopticEvaluator` |\n \n-- In short, one should prepare the data either in COCO detection or COCO panoptic format, then use [`~transformers.DetrImageProcessor`] to create `pixel_values`, `pixel_mask` and optional `labels`, which can then be used to train (or fine-tune) a model. \n+- In short, one should prepare the data either in COCO detection or COCO panoptic format, then use [`~transformers.DetrImageProcessor`] to create `pixel_values`, `pixel_mask` and optional `labels`, which can then be used to train (or fine-tune) a model.\n - For evaluation, one should first convert the outputs of the model using one of the postprocessing methods of [`~transformers.DetrImageProcessor`]. These can be provided to either `CocoEvaluator` or `PanopticEvaluator`, which allow you to calculate metrics like mean Average Precision (mAP) and Panoptic Quality (PQ). The latter objects are implemented in the [original repository](https://github.com/facebookresearch/detr). See the [example notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETR) for more info regarding evaluation.\n \n ## Resources"
        },
        {
            "sha": "bab0cb4a72d3f20f878aeed492b9e1318c1bb113",
            "filename": "docs/source/en/model_doc/dia.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdia.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdia.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdia.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -117,11 +117,9 @@ out = model(**inputs)\n out.loss.backward()\n ```\n \n-\n This model was contributed by [Jaeyong Sung](https://huggingface.co/buttercrab), [Arthur Zucker](https://huggingface.co/ArthurZ),\n and [Anton Vlasjuk](https://huggingface.co/AntonV). The original code can be found [here](https://github.com/nari-labs/dia/).\n \n-\n ## DiaConfig\n \n [[autodoc]] DiaConfig"
        },
        {
            "sha": "79b8314d0ae22f4fadffa2da843559e26afb40fe",
            "filename": "docs/source/en/model_doc/diffllama.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdiffllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdiffllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdiffllama.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -35,7 +35,6 @@ The abstract from the paper is the following:\n ### Usage tips\n The hyperparameters of this model is the same as Llama model.\n \n-\n ## DiffLlamaConfig\n \n [[autodoc]] DiffLlamaConfig"
        },
        {
            "sha": "0968641326af4550d54e9e57c2c535c63afeb0d1",
            "filename": "docs/source/en/model_doc/dinov2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -19,7 +19,6 @@ specific language governing permissions and limitations under the License.\n     </div>\n </div>\n \n-\n # DINOv2\n \n [DINOv2](https://huggingface.co/papers/2304.07193) is a vision foundation model that uses [ViT](./vit) as a feature extractor for multiple downstream tasks like image classification and depth estimation. It focuses on stabilizing and accelerating training through techniques like a faster memory-efficient attention, sequence packing, improved stochastic depth, Fully Sharded Data Parallel (FSDP), and model distillation."
        },
        {
            "sha": "fcafc6df30616d5e589e5a35d5606eb11020e7dd",
            "filename": "docs/source/en/model_doc/dinov2_with_registers.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -45,7 +45,6 @@ Tips:\n This model was contributed by [nielsr](https://huggingface.co/nielsr).\n The original code can be found [here](https://github.com/facebookresearch/dinov2).\n \n-\n ## Dinov2WithRegistersConfig\n \n [[autodoc]] Dinov2WithRegistersConfig"
        },
        {
            "sha": "94e5316515666c0d397b48bfa55705fa077c6ae9",
            "filename": "docs/source/en/model_doc/dinov3.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -19,7 +19,6 @@ specific language governing permissions and limitations under the License.\n     </div>\n </div>\n \n-\n # DINOv3\n \n [DINOv3](https://huggingface.co/papers/2508.10104) is a family of versatile vision foundation models that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models."
        },
        {
            "sha": "574ffe3ef11a96a51e3fa9b180f4477c0e17c010",
            "filename": "docs/source/en/model_doc/dit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdit.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -85,6 +85,7 @@ print(f\"The predicted class label is: {predicted_class_label}\")\n ## Notes\n \n - The pretrained DiT weights can be loaded in a [BEiT] model with a modeling head to predict visual tokens.\n+\n    ```py\n    from transformers import BeitForMaskedImageModeling\n "
        },
        {
            "sha": "ffa9ced7913aa508dbd661d58e7c398de4d8cecd",
            "filename": "docs/source/en/model_doc/doge.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdoge.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdoge.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdoge.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -17,7 +17,6 @@ rendered properly in your Markdown viewer.\n \n # Doge\n \n-\n ## Overview\n \n Doge is a series of small language models based on the [Doge](https://github.com/SmallDoges/small-doge) architecture, aiming to combine the advantages of state-space and self-attention algorithms, calculate dynamic masks from cached value states using the zero-order hold method, and solve the problem of existing mainstream language models getting lost in context. It uses the `wsd_scheduler` scheduler to pre-train on the `smollm-corpus`, and can continue training on new datasets or add sparse activation feedforward networks from stable stage checkpoints.\n@@ -28,7 +27,6 @@ As shown in the figure below, the sequence transformation part of the Doge archi\n \n Checkout all Doge model checkpoints [here](https://huggingface.co/collections/SmallDoge/doge-slm-679cc991f027c4a3abbded4a).\n \n-\n ## Usage\n \n <details>\n@@ -44,6 +42,7 @@ inputs = tokenizer(\"Hey how are you doing?\", return_tensors=\"pt\")\n outputs = model.generate(**inputs, max_new_tokens=100)\n print(tokenizer.batch_decode(outputs))\n ```\n+\n </details>\n \n <details>\n@@ -82,6 +81,7 @@ outputs = model.generate(\n     streamer=steamer\n )\n ```\n+\n </details>\n \n ## DogeConfig"
        },
        {
            "sha": "e582dab748ae18212c580c4c653322472e3ca62e",
            "filename": "docs/source/en/model_doc/donut.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -22,7 +22,7 @@ specific language governing permissions and limitations under the License. -->\n \n # Donut\n \n-[Donut (Document Understanding Transformer)](https://huggingface.co/papers/2111.15664) is a visual document understanding model that doesn't require an Optical Character Recognition (OCR) engine. Unlike traditional approaches that extract text using OCR before processing, Donut employs an end-to-end Transformer-based architecture to directly analyze document images. This eliminates OCR-related inefficiencies making it more accurate and adaptable to diverse languages and formats. \n+[Donut (Document Understanding Transformer)](https://huggingface.co/papers/2111.15664) is a visual document understanding model that doesn't require an Optical Character Recognition (OCR) engine. Unlike traditional approaches that extract text using OCR before processing, Donut employs an end-to-end Transformer-based architecture to directly analyze document images. This eliminates OCR-related inefficiencies making it more accurate and adaptable to diverse languages and formats.\n \n Donut features vision encoder ([Swin](./swin)) and a text decoder ([BART](./bart)). Swin converts document images into embeddings and BART processes them into meaningful text sequences.\n "
        },
        {
            "sha": "316ab3b1f5b9689ae0acacd2fc354ea34ad5f679",
            "filename": "docs/source/en/model_doc/dots1.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdots1.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fdots1.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdots1.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -25,7 +25,6 @@ The abstract from the report is the following:\n \n *Mixture of Experts (MoE) models have emerged as a promising paradigm for scaling language models efficiently by activating only a subset of parameters for each input token. In this report, we present dots.llm1, a large-scale MoE model that activates 14B parameters out of a total of 142B parameters, delivering performance on par with state-of-the-art models while reducing training and inference costs. Leveraging our meticulously crafted and efficient data processing pipeline, dots.llm1 achieves performance comparable to Qwen2.5-72B after pretraining on high-quality corpus and post-training to fully unlock its capabilities. Notably, no synthetic data is used during pretraining. To foster further research, we open-source intermediate training checkpoints spanning the entire training process, providing valuable insights into the learning dynamics of large language models.*\n \n-\n ## Dots1Config\n \n [[autodoc]] Dots1Config"
        },
        {
            "sha": "faf71f4bac0476877a0e3ec8b4b33f2129a1ca2c",
            "filename": "docs/source/en/model_doc/efficientloftr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -45,6 +45,7 @@ results = keypoint_matcher([url_0, url_1], threshold=0.9)\n print(results[0])\n # {'keypoint_image_0': {'x': ..., 'y': ...}, 'keypoint_image_1': {'x': ..., 'y': ...}, 'score': ...}\n ```\n+\n </hfoption>\n <hfoption id=\"AutoModel\">\n \n@@ -167,4 +168,3 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n [[autodoc]] EfficientLoFTRForKeypointMatching\n \n - forward\n-"
        },
        {
            "sha": "b4fbe8225625c26cf8452d044ad277e2bfa1e26e",
            "filename": "docs/source/en/model_doc/efficientnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientnet.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The EfficientNet model was proposed in [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://huggingface.co/papers/1905.11946) \n+The EfficientNet model was proposed in [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://huggingface.co/papers/1905.11946)\n by Mingxing Tan and Quoc V. Le. EfficientNets are a family of image classification models, which achieve state-of-the-art accuracy, yet being an order-of-magnitude smaller and faster than previous models.\n \n The abstract from the paper is the following:\n@@ -34,7 +34,6 @@ To go even further, we use neural architecture search to design a new baseline n\n This model was contributed by [adirik](https://huggingface.co/adirik).\n The original code can be found [here](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet).\n \n-\n ## EfficientNetConfig\n \n [[autodoc]] EfficientNetConfig\n@@ -58,4 +57,3 @@ The original code can be found [here](https://github.com/tensorflow/tpu/tree/mas\n \n [[autodoc]] EfficientNetForImageClassification\n     - forward\n-"
        },
        {
            "sha": "0c95bc6d98772cd55495d06d6797d95abf47cad1",
            "filename": "docs/source/en/model_doc/emu3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -27,8 +27,7 @@ rendered properly in your Markdown viewer.\n \n The Emu3 model was proposed in [Emu3: Next-Token Prediction is All You Need](https://huggingface.co/papers/2409.18869) by Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, Zhongyuan Wang.\n \n-Emu3 is a multimodal LLM that uses vector quantization to tokenize images into discrete tokens. Discretized image tokens are later fused with text token ids for image and text generation. The model can additionally generate images by predicting image token ids. \n-\n+Emu3 is a multimodal LLM that uses vector quantization to tokenize images into discrete tokens. Discretized image tokens are later fused with text token ids for image and text generation. The model can additionally generate images by predicting image token ids.\n \n The abstract from the paper is the following:\n \n@@ -45,11 +44,9 @@ Tips:\n > [!TIP]\n > Emu3 implementation in Transformers uses a special image token to indicate where to merge image embeddings. The special image token isn't new and uses one of the reserved tokens: `<|extra_0|>`. You have to add `<image>` to your prompt in the place where the image should be embedded for correct generation.\n \n-\n This model was contributed by [RaushanTurganbay](https://huggingface.co/RaushanTurganbay).\n The original code can be found [here](https://github.com/baaivision/Emu3).\n \n-\n ## Usage example\n \n ### Text generation inference\n@@ -143,7 +140,6 @@ for i, image in enumerate(images['pixel_values']):\n \n ```\n \n-\n ## Emu3Config\n \n [[autodoc]] Emu3Config"
        },
        {
            "sha": "9fc6c2c97e94d35abf819e3ddec410c186a429a9",
            "filename": "docs/source/en/model_doc/encodec.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fencodec.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fencodec.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fencodec.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -29,14 +29,14 @@ The abstract from the paper is the following:\n \n *We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio.*\n \n-This model was contributed by [Matthijs](https://huggingface.co/Matthijs), [Patrick Von Platen](https://huggingface.co/patrickvonplaten) and [Arthur Zucker](https://huggingface.co/ArthurZ). \n+This model was contributed by [Matthijs](https://huggingface.co/Matthijs), [Patrick Von Platen](https://huggingface.co/patrickvonplaten) and [Arthur Zucker](https://huggingface.co/ArthurZ).\n The original code can be found [here](https://github.com/facebookresearch/encodec).\n \n-## Usage example \n+## Usage example\n \n Here is a quick example of how to encode and decode an audio using this model:\n \n-```python \n+```python\n >>> from datasets import load_dataset, Audio\n >>> from transformers import EncodecModel, AutoProcessor\n >>> librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
        },
        {
            "sha": "199d87dc794ec3f6eaf6138e5c2929e034b77bc7",
            "filename": "docs/source/en/model_doc/eomt.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Feomt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Feomt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Feomt.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -39,7 +39,6 @@ Architecturally, EoMT introduces a small set of **learned queries** and a lightw\n        alt=\"drawing\" width=\"500\"/>\n </div>\n \n-\n The model supports semantic, instance, and panoptic segmentation using a unified architecture and task-specific post-processing.\n \n ## Usage Examples"
        },
        {
            "sha": "bf71049148d32dcf642901350c3ee21f5bc43eb2",
            "filename": "docs/source/en/model_doc/ernie4_5.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -38,7 +38,6 @@ Other models from the family can be found at [Ernie 4.5 Moe](./ernie4_5_moe).\n     <img src=\"https://ernie.baidu.com/blog/posts/ernie4.5/overview.png\"/>\n </div>\n \n-\n ## Usage Tips\n \n ### Generate text\n@@ -84,7 +83,6 @@ generate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n This model was contributed by [Anton Vlasjuk](https://huggingface.co/AntonV).\n The original code can be found [here](https://github.com/PaddlePaddle/ERNIE).\n \n-\n ## Ernie4_5Config\n \n [[autodoc]] Ernie4_5Config"
        },
        {
            "sha": "fb6b8d791bec3a614cc4bc5e08a6ae95fb3a40ea",
            "filename": "docs/source/en/model_doc/ernie4_5_moe.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_moe.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -40,7 +40,6 @@ Other models from the family can be found at [Ernie 4.5](./ernie4_5).\n     <img src=\"https://ernie.baidu.com/blog/posts/ernie4.5/overview.png\"/>\n </div>\n \n-\n ## Usage Tips\n \n ### Generate text\n@@ -167,7 +166,6 @@ generate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n This model was contributed by [Anton Vlasjuk](https://huggingface.co/AntonV).\n The original code can be found [here](https://github.com/PaddlePaddle/ERNIE).\n \n-\n ## Ernie4_5_MoeConfig\n \n [[autodoc]] Ernie4_5_MoeConfig"
        },
        {
            "sha": "e044614e76445fb02c35f2342475a7a1f4619bae",
            "filename": "docs/source/en/model_doc/ernie_m.md",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie_m.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie_m.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie_m.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -40,7 +40,6 @@ The abstract from the paper is the following:\n *Recent studies have demonstrated that pre-trained cross-lingual models achieve impressive performance in downstream cross-lingual tasks. This improvement benefits from learning a large amount of monolingual and parallel corpora. Although it is generally acknowledged that parallel corpora are critical for improving the model performance, existing methods are often constrained by the size of parallel corpora, especially for lowresource languages. In this paper, we propose ERNIE-M, a new training method that encourages the model to align the representation of multiple languages with monolingual corpora, to overcome the constraint that the parallel corpus size places on the model performance. Our key insight is to integrate back-translation into the pre-training process. We generate pseudo-parallel sentence pairs on a monolingual corpus to enable the learning of semantic alignments between different languages, thereby enhancing the semantic modeling of cross-lingual models. Experimental results show that ERNIE-M outperforms existing cross-lingual models and delivers new state-of-the-art results in various cross-lingual downstream tasks.*\n This model was contributed by [Susnato Dhar](https://huggingface.co/susnato). The original code can be found [here](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/paddlenlp/transformers/ernie_m).\n \n-\n ## Usage tips\n \n - Ernie-M is a BERT-like model so it is a stacked Transformer Encoder.\n@@ -59,7 +58,6 @@ This model was contributed by [Susnato Dhar](https://huggingface.co/susnato). Th\n \n [[autodoc]] ErnieMConfig\n \n-\n ## ErnieMTokenizer\n \n [[autodoc]] ErnieMTokenizer\n@@ -68,7 +66,6 @@ This model was contributed by [Susnato Dhar](https://huggingface.co/susnato). Th\n     - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n-\n ## ErnieMModel\n \n [[autodoc]] ErnieMModel\n@@ -79,19 +76,16 @@ This model was contributed by [Susnato Dhar](https://huggingface.co/susnato). Th\n [[autodoc]] ErnieMForSequenceClassification\n     - forward\n \n-\n ## ErnieMForMultipleChoice\n \n [[autodoc]] ErnieMForMultipleChoice\n     - forward\n \n-\n ## ErnieMForTokenClassification\n \n [[autodoc]] ErnieMForTokenClassification\n     - forward\n \n-\n ## ErnieMForQuestionAnswering\n \n [[autodoc]] ErnieMForQuestionAnswering"
        },
        {
            "sha": "a6190a71f020f063dda422bfd3dda25ab864844e",
            "filename": "docs/source/en/model_doc/esm.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fesm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fesm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fesm.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -44,12 +44,10 @@ sequence alignment (MSA) step at inference time, which means that ESMFold checkp\n they do not require a database of known protein sequences and structures with associated external query tools\n to make predictions, and are much faster as a result.\n \n-\n The abstract from\n \"Biological structure and function emerge from scaling unsupervised learning to 250\n million protein sequences\" is\n \n-\n *In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised\n learning has led to major advances in representation learning and statistical generation. In the life sciences, the\n anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling\n@@ -63,7 +61,6 @@ can be identified by linear projections. Representation learning produces featur\n applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and\n improving state-of-the-art features for long-range contact prediction.*\n \n-\n The abstract from\n \"Language models of protein sequences at the scale of evolution enable accurate structure prediction\" is\n "
        },
        {
            "sha": "56f1d2755e19690e4b16cd8be6cbbba967e2230a",
            "filename": "docs/source/en/model_doc/evolla.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fevolla.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fevolla.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fevolla.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -75,7 +75,6 @@ Tips:\n - This model was contributed by [Xibin Bayes Zhou](https://huggingface.co/XibinBayesZhou).\n - The original code can be found [here](https://github.com/westlake-repl/Evolla).\n \n-\n ## EvollaConfig\n \n [[autodoc]] EvollaConfig"
        },
        {
            "sha": "93ca33babd3ceec45ca1486d0d73fee9f9f7887e",
            "filename": "docs/source/en/model_doc/exaone4.md",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fexaone4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fexaone4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fexaone4.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -20,7 +20,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n **[EXAONE 4.0](https://github.com/LG-AI-EXAONE/EXAONE-4.0)** model is the language model, which integrates a **Non-reasoning mode** and **Reasoning mode** to achieve both the excellent usability of [EXAONE 3.5](https://github.com/LG-AI-EXAONE/EXAONE-3.5) and the advanced reasoning abilities of [EXAONE Deep](https://github.com/LG-AI-EXAONE/EXAONE-Deep). To pave the way for the agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool use, and its multilingual capabilities are extended\n-to support Spanish in addition to English and Korean. \n+to support Spanish in addition to English and Korean.\n \n The EXAONE 4.0 model series consists of two sizes: a mid-size **32B** model optimized for high performance, and a small-size **1.2B** model designed for on-device applications.\n \n@@ -33,7 +33,6 @@ For more details, please refer to our [technical report](https://huggingface.co/\n \n All model weights including quantized versions are available at [Huggingface Collections](https://huggingface.co/collections/LGAI-EXAONE/exaone-40-686b2e0069800c835ed48375).\n \n-\n ## Model Details\n \n ### Model Specifications\n@@ -57,7 +56,6 @@ All model weights including quantized versions are available at [Huggingface Col\n | Tied word embedding | False | True |\n | Knowledge cut-off | Nov. 2024 | Nov. 2024 |\n \n-\n ## Usage tips\n \n ### Non-reasoning mode"
        },
        {
            "sha": "c17ecea1cc0e84611fc9d2a6e40b4a96c1009fce",
            "filename": "docs/source/en/model_doc/falcon_h1.md",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_h1.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_h1.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_h1.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -21,7 +21,6 @@ The [FalconH1](https://huggingface.co/blog/tiiuae/falcon-h1) model was developed\n This model was contributed by [DhiyaEddine](https://huggingface.co/DhiyaEddine), [ybelkada](https://huggingface.co/ybelkada), [JingweiZuo](https://huggingface.co/JingweiZuo), [IlyasChahed](https://huggingface.co/IChahed), and [MaksimVelikanov](https://huggingface.co/yellowvm).\n The original code can be found [here](https://github.com/tiiuae/Falcon-H1).\n \n-\n ## FalconH1Config\n \n | Model     | Depth | Dim  | Attn Heads | KV | Mamba Heads | d_head       | d_state | Ctx Len        |\n@@ -33,8 +32,6 @@ The original code can be found [here](https://github.com/tiiuae/Falcon-H1).\n | H1 7B     | 44     | 3072 | 12         | 2  | 24           | 128 / 128    | 256  | 256K            |\n | H1 34B    | 72     | 5120 | 20         | 4  | 32           | 128 / 128    | 256  | 256K            |\n \n-\n-\n [[autodoc]] FalconH1Config\n \n <!---\n@@ -63,4 +60,4 @@ print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\n [[autodoc]] FalconH1ForCausalLM\n     - forward\n \n-This HF implementation is contributed by [younesbelkada](https://github.com/younesbelkada) and [DhiaEddineRhaiem](https://github.com/dhiaEddineRhaiem). \n\\ No newline at end of file\n+This HF implementation is contributed by [younesbelkada](https://github.com/younesbelkada) and [DhiaEddineRhaiem](https://github.com/dhiaEddineRhaiem).\n\\ No newline at end of file"
        },
        {
            "sha": "91dbfaac82750055452a9e9fa184007d3d1c24d0",
            "filename": "docs/source/en/model_doc/fastspeech2_conformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ffastspeech2_conformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ffastspeech2_conformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffastspeech2_conformer.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -27,7 +27,6 @@ The abstract from the original FastSpeech2 paper is the following:\n \n This model was contributed by [Connor Henderson](https://huggingface.co/connor-henderson). The original code can be found [here](https://github.com/espnet/espnet/blob/master/espnet2/tts/fastspeech2/fastspeech2.py).\n \n-\n ## 🤗 Model Architecture\n FastSpeech2's general structure with a Mel-spectrogram decoder was implemented, and the traditional transformer blocks were replaced with conformer blocks as done in the ESPnet library.\n \n@@ -90,6 +89,7 @@ sf.write(\"speech.wav\", waveform.squeeze().detach().numpy(), samplerate=22050)\n ```\n \n 4. Run inference with a pipeline and specify which vocoder to use\n+\n ```python\n from transformers import pipeline, FastSpeech2ConformerHifiGan\n import soundfile as sf\n@@ -102,7 +102,6 @@ speech = synthesiser(\"Hello, my dog is cooler than you!\")\n sf.write(\"speech.wav\", speech[\"audio\"].squeeze(), samplerate=speech[\"sampling_rate\"])\n ```\n \n-\n ## FastSpeech2ConformerConfig\n \n [[autodoc]] FastSpeech2ConformerConfig"
        },
        {
            "sha": "b4cbac713a38a8f293867affa822f566fd91b586",
            "filename": "docs/source/en/model_doc/flan-ul2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fflan-ul2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fflan-ul2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflan-ul2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -35,7 +35,6 @@ Google has released the following variants:\n \n The original checkpoints can be found [here](https://github.com/google-research/google-research/tree/master/ul2).\n \n-\n ## Running on low resource devices\n \n The model is pretty heavy (~40GB in half precision) so if you just want to run the model, make sure you load your model in 8bit, and use `device_map=\"auto\"` to make sure  you don't have any OOM issue!"
        },
        {
            "sha": "d6cf9ffe3050f67a993ac88de58317b58632928d",
            "filename": "docs/source/en/model_doc/flex_olmo.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fflex_olmo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fflex_olmo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflex_olmo.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -12,7 +12,6 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n \n-\n ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n \n -->\n@@ -90,6 +89,7 @@ echo -e \"Plants create energy through a process known as\" | transformers run --t\n Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n The example below uses [torchao](../quantization/torchao) to only quantize the weights to 4-bits.\n+\n ```py\n \n #pip install torchao\n@@ -119,7 +119,6 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n ```\n \n-\n ## FlexOlmoConfig\n \n [[autodoc]] FlexOlmoConfig"
        },
        {
            "sha": "e89a410b105bc8b530b4a596a434a5b386f44ba9",
            "filename": "docs/source/en/model_doc/fnet.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ffnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ffnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffnet.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -46,8 +46,8 @@ This model was contributed by [gchhablani](https://huggingface.co/gchhablani). T\n \n ## Usage tips\n \n-The model was trained without an attention mask as it is based on Fourier Transform. The model was trained with \n-maximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum \n+The model was trained without an attention mask as it is based on Fourier Transform. The model was trained with\n+maximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum\n sequence length for fine-tuning and inference.\n \n ## Resources"
        },
        {
            "sha": "13a99ae40da73736d222e92df485be4c0b376195",
            "filename": "docs/source/en/model_doc/fsmt.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ffsmt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ffsmt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffsmt.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -41,7 +41,6 @@ This model was contributed by [stas](https://huggingface.co/stas). The original\n   either. Its tokenizer is very similar to [`XLMTokenizer`] and the main model is derived from\n   [`BartModel`].\n \n-\n ## FSMTConfig\n \n [[autodoc]] FSMTConfig"
        },
        {
            "sha": "57b011b9400c46ce244753648e57684d635d0e6a",
            "filename": "docs/source/en/model_doc/funnel.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ffunnel.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ffunnel.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffunnel.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -67,7 +67,6 @@ This model was contributed by [sgugger](https://huggingface.co/sgugger). The ori\n - [Masked language modeling task guide](../tasks/masked_language_modeling)\n - [Multiple choice task guide](../tasks/multiple_choice)\n \n-\n ## FunnelConfig\n \n [[autodoc]] FunnelConfig"
        },
        {
            "sha": "34202b022f7e14ddfe0c1072c54319474ec8d325",
            "filename": "docs/source/en/model_doc/fuyu.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -40,7 +40,6 @@ Finetuning the model in `float16` is not recommended and known to produce `nan`,\n \n </Tip>\n \n-\n Tips:\n \n - To convert the model, you need to clone the original repository using `git clone https://github.com/persimmon-ai-labs/adept-inference`, then get the checkpoints:\n@@ -55,10 +54,12 @@ python src/transformers/models/fuyu/convert_fuyu_weights_to_hf.py  --input_dir /\n ```\n \n For the chat model:\n+\n ```bash\n wget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_chat_model_release.tar\n tar -xvf 8b_base_model_release.tar\n ```\n+\n Then, model can be loaded via:\n \n ```py\n@@ -99,7 +100,6 @@ The `LlamaTokenizer` is used as it is a standard wrapper around sentencepiece.\n \n - The authors suggest to use the following prompt for image captioning: `f\"Generate a coco-style caption.\\\\n\"`\n \n-\n ## FuyuConfig\n \n [[autodoc]] FuyuConfig"
        },
        {
            "sha": "f1c088caf300374a922e47d371fcee9e64e796cd",
            "filename": "docs/source/en/model_doc/gemma.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -33,7 +33,6 @@ The instruction-tuned variant was fine-tuned with supervised learning on instruc\n \n You can find all the original Gemma checkpoints under the [Gemma](https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b) release.\n \n-\n > [!TIP]\n > Click on the Gemma models in the right sidebar for more examples of how to apply Gemma to different language tasks.\n \n@@ -163,7 +162,6 @@ visualizer(\"LLMs generate text through a process known as\")\n \n [[autodoc]] GemmaTokenizer\n \n-\n ## GemmaTokenizerFast\n \n [[autodoc]] GemmaTokenizerFast"
        },
        {
            "sha": "5b4430296dcfdc2b2b8f743170ca91d9ca0b92e3",
            "filename": "docs/source/en/model_doc/gemma2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -40,7 +40,6 @@ The example below demonstrates how to chat with the model with [`Pipeline`] or t\n <hfoptions id=\"usage\">\n <hfoption id=\"Pipeline\">\n \n-\n ```python\n import torch\n from transformers import pipeline\n@@ -84,6 +83,7 @@ print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```\n echo -e \"Explain quantum computing simply.\" | transformers run --task text-generation --model google/gemma-2-2b --device 0\n ```\n+\n </hfoption>\n </hfoptions>\n \n@@ -113,7 +113,6 @@ print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n \n Use the [AttentionMaskVisualizer](https://github.com/huggingface/transformers/blob/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/src/transformers/utils/attention_visualizer.py#L139) to better understand what tokens the model can and cannot attend to.\n \n-\n ```python\n from transformers.utils.attention_visualizer import AttentionMaskVisualizer\n visualizer = AttentionMaskVisualizer(\"google/gemma-2b\")"
        },
        {
            "sha": "3c69cc1604ff77bf63c3bfa881a51bf12dba546c",
            "filename": "docs/source/en/model_doc/gemma3.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -195,6 +195,7 @@ visualizer(\"<img>What is shown in this image?\")\n         },\n     ]\n     ```\n+\n - Text passed to the processor should have a `<start_of_image>` token wherever an image should be inserted.\n - The processor has its own [`~ProcessorMixin.apply_chat_template`] method to convert chat messages to model inputs.\n - By default, images aren't cropped and only the base image is forwarded to the model. In high resolution images or images with non-square aspect ratios, artifacts can result because the vision encoder uses a fixed resolution of 896x896. To prevent these artifacts and improve performance during inference, set `do_pan_and_scan=True` to crop the image into multiple smaller patches and concatenate them with the base image embedding. You can disable pan and scan for faster inference.\n@@ -209,6 +210,7 @@ visualizer(\"<img>What is shown in this image?\")\n     +   do_pan_and_scan=True,\n         ).to(model.device)\n     ```\n+\n - For Gemma-3 1B checkpoint trained in text-only mode, use [`AutoModelForCausalLM`] instead.\n \n     ```py"
        },
        {
            "sha": "7c2e3ecc9269387a306191793345ff84d01377b1",
            "filename": "docs/source/en/model_doc/gemma3n.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -147,6 +147,7 @@ echo -e \"Plants create energy through a process known as\" | transformers run --t\n         },\n     ]\n     ```\n+\n -   Text passed to the processor should have a `<image_soft_token>` token wherever an image should be inserted.\n -   Gemma 3n accept at most one target audio clip per input, though multiple audio clips can be provided in few-shot\n     prompts, for example."
        },
        {
            "sha": "87daea7289a9ce5d41f11cb35be3957645c027cc",
            "filename": "docs/source/en/model_doc/glm.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -53,7 +53,6 @@ Tips:\n - This model was contributed by [THUDM](https://huggingface.co/THUDM). The most recent code can be\n   found [here](https://github.com/thudm/GLM-4).\n \n-  \n ## Usage tips\n \n `GLM-4` can be found on the [Huggingface Hub](https://huggingface.co/collections/THUDM/glm-4-665fcf188c414b03c2f7e3b7)"
        },
        {
            "sha": "1f80d4b2584e701768d0178b8fb05b62cad1dfe4",
            "filename": "docs/source/en/model_doc/glm4v.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -75,6 +75,7 @@ messages = [\n ]\n pipe(text=messages,max_new_tokens=20, return_full_text=False)\n ```\n+\n </hfoption>\n <hfoption id=\"AutoModel\">\n \n@@ -123,6 +124,7 @@ output_text = processor.batch_decode(\n )\n print(output_text)\n ```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "f8d6d69b0f6dcb16d1b1eb8c9bc0bd9d9cead1b3",
            "filename": "docs/source/en/model_doc/got_ocr2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -34,7 +34,6 @@ alt=\"drawing\" width=\"600\"/>\n \n <small> GOT-OCR2 training stages. Taken from the <a href=\"https://huggingface.co/papers/2409.01704\">original paper.</a> </small>\n \n-\n Tips:\n \n GOT-OCR2 works on a wide range of tasks, including plain document OCR, scene text OCR, formatted document OCR, and even OCR for tables, charts, mathematical formulas, geometric shapes, molecular formulas and sheet music. While this implementation of the model will only output plain text, the outputs can be further processed to render the desired format, with packages like `pdftex`, `mathpix`, `matplotlib`, `tikz`, `verovio` or `pyecharts`.\n@@ -129,7 +128,6 @@ GOT-OCR2 can also generate formatted text, such as markdown or LaTeX. Here is an\n Although it might be reasonable in most cases to use a “for loop” for multi-page processing, some text data with formatting across several pages make it necessary to process all pages at once. GOT introduces a multi-page OCR (without “for loop”) feature, where multiple pages can be processed by the model at once, with the output being one continuous text.\n Here is an example of how to process multiple pages at once:\n \n-\n ```python\n >>> import torch\n >>> from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n@@ -254,6 +252,7 @@ Here is an example of how to process sheet music:\n >>> with open(\"output.svg\", \"w\") as f:\n >>>     f.write(svg)\n ```\n+\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/sheet_music.svg\"\n alt=\"drawing\" width=\"600\"/>\n \n@@ -285,4 +284,3 @@ alt=\"drawing\" width=\"600\"/>\n \n [[autodoc]] GotOcr2ForConditionalGeneration\n     - forward\n-"
        },
        {
            "sha": "aaf2a50a1731c29e8f6481d5312f5813c2f5abf9",
            "filename": "docs/source/en/model_doc/gpt2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -23,7 +23,6 @@ rendered properly in your Markdown viewer.\n   </div>\n </div>\n \n-\n # GPT-2\n \n [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) is a scaled up version of GPT, a causal transformer language model, with 10x more parameters and training data. The model was pretrained on a 40GB dataset to predict the next word in a sequence based on all the previous words. This approach enabled the model to perform many downstream tasks in a zero-shot setting. The blog post released by OpenAI can be found [here](https://openai.com/index/better-language-models/).\n@@ -47,6 +46,7 @@ from transformers import pipeline\n pipeline = pipeline(task=\"text-generation\", model=\"openai-community/gpt2\", dtype=torch.float16, device=0)\n pipeline(\"Hello, I'm a language model\")\n ```\n+\n </hfoption>\n <hfoption id=\"AutoModel\">\n "
        },
        {
            "sha": "e837f2a08f528d899e148ed6560c79e2bd795c3b",
            "filename": "docs/source/en/model_doc/gpt_bigcode.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -47,7 +47,6 @@ The main differences compared to GPT2.\n - Merge the key and value caches into one (this changes the format of layer_past/ present, does it risk creating problems?)\n - Use the memory layout (self.num_heads, 3, self.head_dim) instead of `(3, self.num_heads, self.head_dim)` for the QKV tensor with MHA. (prevents an overhead with the merged key and values, but makes the checkpoints incompatible with the original openai-community/gpt2 model).\n \n-\n You can read more about the optimizations in the [original pull request](https://github.com/huggingface/transformers/pull/22575)\n \n > [!NOTE]\n@@ -91,7 +90,6 @@ Below is a expected speedup diagram that compares pure inference time between th\n <img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/starcoder-speedup.png\">\n </div>\n \n-\n ## GPTBigCodeConfig\n \n [[autodoc]] GPTBigCodeConfig"
        },
        {
            "sha": "4df9cf69842dbb95b1d8e00ec1d7cdf4f61af4e7",
            "filename": "docs/source/en/model_doc/gpt_neo.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neo.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -22,12 +22,10 @@ rendered properly in your Markdown viewer.\n     </div>\n </div>\n \n-\n ## GPT-Neo\n \n [GPT-Neo](https://zenodo.org/records/5297715) is an open-source alternative to GPT-2 and GPT-3 models, built with Mesh TensorFlow for TPUs. GPT-Neo uses local attention in every other layer for more efficiency. It is trained on the [Pile](https://huggingface.co/datasets/EleutherAI/pile), a diverse dataset consisting of 22 smaller high-quality datasets. The original github repository can be found [here](https://github.com/EleutherAI/gpt-neo/tree/v1.1)\n \n-\n You can find all the original GPT-Neo checkpoints under the [EleutherAI](https://huggingface.co/EleutherAI?search_models=gpt-neo) organization.\n \n > [!TIP]\n@@ -45,6 +43,7 @@ from transformers import pipeline\n pipeline = pipeline(task=\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\", dtype=torch.float16, device=0)\n pipeline(\"Hello, I'm a language model\")\n ```\n+\n </hfoption>\n <hfoption id=\"AutoModel\">\n "
        },
        {
            "sha": "fb2ff7093040b1f4123943b0a34802f5f87ebedf",
            "filename": "docs/source/en/model_doc/gpt_neox.md",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -71,7 +71,7 @@ The `generate()` method can be used to generate text using GPT Neo model.\n \n Flash Attention 2 is an faster, optimized version of the model.\n \n-### Installation \n+### Installation\n \n First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features). If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered [above](https://huggingface.co/docs/transformers/main/en/model_doc/bark#using-better-transformer).\n \n@@ -92,7 +92,6 @@ model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\", dtype=torc\n ...\n ```\n \n-\n ### Expected speedups\n \n Below is an expected speedup diagram that compares pure inference time between the native implementation in transformers using `stockmark/gpt-neox-japanese-1.4b` checkpoint and the Flash Attention 2 version of the model using a sequence length of 2048.\n@@ -101,7 +100,6 @@ Below is an expected speedup diagram that compares pure inference time between t\n <img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/gpt-neox-1.8b-speedup.jpg\">\n </div>\n \n-\n ## Using Scaled Dot Product Attention (SDPA)\n PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n@@ -162,7 +160,6 @@ following speedups during training and inference.\n |             4 |         1024 |                          11.765 |                         11.303 |           4.09 |           2558.96 |         2546.04 |            0.508 |\n |             4 |         2048 |                          19.568 |                         17.735 |          10.33 |            4175.5 |         4165.26 |            0.246 |\n \n-\n ## Resources\n \n - [Causal language modeling task guide](../tasks/language_modeling)"
        },
        {
            "sha": "bf786f7561d4ca44b78cdc6b473f7628165b7454",
            "filename": "docs/source/en/model_doc/gpt_neox_japanese.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -27,8 +27,6 @@ rendered properly in your Markdown viewer.\n GPT-NeoX-Japanese, a Japanese language model based on [GPT-NeoX](./gpt_neox).\n Japanese uses three types of characters (hiragana, katakana, kanji) and has a huge vocabulary. This model uses [BPEEncoder V2](https://github.com/tanreinama/Japanese-BPEEncoder_V2), a sub-word tokenizer to handle the different characters.\n \n-\n-\n The model also removes some bias parameters for better performance.\n \n You can find all the original GPT-NeoX-Japanese checkpoints under the [ABEJA](https://huggingface.co/abeja/models?search=gpt-neo-x) organization."
        },
        {
            "sha": "47c970eb17e60c916266868903a6668499e833a3",
            "filename": "docs/source/en/model_doc/gpt_oss.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_oss.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_oss.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_oss.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -41,7 +41,6 @@ Tips:\n This model was contributed by [INSERT YOUR HF USERNAME HERE](https://huggingface.co/<INSERT YOUR HF USERNAME HERE>).\n The original code can be found [here](<INSERT LINK TO GITHUB REPO HERE>).\n \n-\n ## GptOssConfig\n \n [[autodoc]] GptOssConfig"
        },
        {
            "sha": "ef8bb0867b6ea793e670d608a48aa5b619512757",
            "filename": "docs/source/en/model_doc/granite.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -15,7 +15,6 @@ rendered properly in your Markdown viewer.\n -->\n *This model was released on 2024-08-23 and added to Hugging Face Transformers on 2024-08-27.*\n \n-\n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n@@ -69,12 +68,14 @@ inputs = tokenizer(\"Explain quantum computing in simple terms\", return_tensors=\"\n outputs = model.generate(**inputs, max_length=50, cache_implementation=\"static\")\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```\n+\n </hfoption>\n <hfoption id=\"transformers CLI\">\n \n ```python\n echo -e \"Explain quantum computing simply.\" | transformers run --task text-generation --model ibm-granite/granite-3.3-8b-instruct --device 0\n ```\n+\n </hfoption>\n </hfoptions>\n \n@@ -110,7 +111,6 @@ outputs = model.generate(**inputs, max_length=50, cache_implementation=\"static\")\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```\n \n-\n ## GraniteConfig\n \n [[autodoc]] GraniteConfig"
        },
        {
            "sha": "680dba3a4732057e90e8c5c65c493ef52a007f70",
            "filename": "docs/source/en/model_doc/granite_speech.md",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -32,10 +32,8 @@ The [Granite Speech](https://huggingface.co/papers/2505.08699) model ([blog post\n \n 4. LoRA adapter(s): The Granite Speech model contains a modality specific LoRA, which will be enabled when audio features are provided, and disabled otherwise.\n \n-\n Note that most of the aforementioned components are implemented generically to enable compatibility and potential integration with other model architectures in transformers.\n \n-\n This model was contributed by [Alexander Brooks](https://huggingface.co/abrooks9944), [Avihu Dekel](https://huggingface.co/Avihu), and [George Saon](https://huggingface.co/gsaon).\n \n ## Usage tips\n@@ -47,22 +45,18 @@ This model was contributed by [Alexander Brooks](https://huggingface.co/abrooks9\n \n [[autodoc]] GraniteSpeechConfig\n \n-\n ## GraniteSpeechEncoderConfig\n \n [[autodoc]] GraniteSpeechEncoderConfig\n \n-\n ## GraniteSpeechProcessor\n \n [[autodoc]] GraniteSpeechProcessor\n \n-\n ## GraniteSpeechFeatureExtractor\n \n [[autodoc]] GraniteSpeechFeatureExtractor\n \n-\n ## GraniteSpeechForConditionalGeneration\n \n [[autodoc]] GraniteSpeechForConditionalGeneration"
        },
        {
            "sha": "32616c07a28940c62a24ed3eda337b03dbe8a0e9",
            "filename": "docs/source/en/model_doc/granitemoe.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoe.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -65,7 +65,6 @@ for i in output:\n \n This model was contributed by [mayank-mishra](https://huggingface.co/mayank-mishra).\n \n-\n ## GraniteMoeConfig\n \n [[autodoc]] GraniteMoeConfig"
        },
        {
            "sha": "cb3db122e65da1b10999aa6b534adb61514634c4",
            "filename": "docs/source/en/model_doc/granitemoehybrid.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoehybrid.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoehybrid.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoehybrid.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -19,10 +19,8 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-\n The [GraniteMoeHybrid](https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek) model builds on top of GraniteMoeSharedModel and Bamba. Its decoding layers consist of state space layers or MoE attention layers with shared experts. By default, the attention layers do not use positional encoding.\n \n-\n ```python\n from transformers import AutoModelForCausalLM, AutoTokenizer\n "
        },
        {
            "sha": "8b256de647f65114c66f3787cd1d7363675f1a02",
            "filename": "docs/source/en/model_doc/granitemoeshared.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoeshared.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoeshared.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoeshared.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-\n The GraniteMoe model was proposed in [Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler](https://huggingface.co/papers/2408.13359) by Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox and Rameswar Panda.\n \n Additionally this class GraniteMoeSharedModel adds shared experts for Moe.\n@@ -51,7 +50,6 @@ for i in output:\n \n This HF implementation is contributed by [Mayank Mishra](https://huggingface.co/mayank-mishra), [Shawn Tan](https://huggingface.co/shawntan) and [Sukriti Sharma](https://huggingface.co/SukritiSharma).\n \n-\n ## GraniteMoeSharedConfig\n \n [[autodoc]] GraniteMoeSharedConfig"
        },
        {
            "sha": "f5a6316a22c003a24d582b647595ce3c34ed6a0b",
            "filename": "docs/source/en/model_doc/granitevision.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -25,11 +25,13 @@ Tips:\n - This model is loaded into Transformers as an instance of LlaVA-Next. The usage and tips from [LLaVA-NeXT](llava_next) apply to this model as well.\n \n - You can apply the chat template on the tokenizer / processor in the same way as well. Example chat format:\n+\n ```bash\n \"<|user|>\\nWhat’s shown in this image?\\n<|assistant|>\\nThis image shows a red stop sign.<|end_of_text|><|user|>\\nDescribe the image in more details.\\n<|assistant|>\\n\"\n ```\n \n Sample inference:\n+\n ```python\n from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration, infer_device\n "
        },
        {
            "sha": "10748f27be43a135ac15f5fd0d31c0ad52124e9c",
            "filename": "docs/source/en/model_doc/helium.md",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fhelium.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fhelium.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhelium.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -27,7 +27,6 @@ rendered properly in your Markdown viewer.\n \n Helium was proposed in [Announcing Helium-1 Preview](https://kyutai.org/2025/01/13/helium.html) by the Kyutai Team.\n \n-\n Helium-1 preview is a lightweight language model with 2B parameters, targeting edge and mobile devices.\n It supports the following languages: English, French, German, Italian, Portuguese, Spanish.\n \n@@ -36,9 +35,6 @@ It supports the following languages: English, French, German, Italian, Portugues\n - **Language(s) (NLP):** English, French, German, Italian, Portuguese, Spanish\n - **License:** CC-BY 4.0\n \n-\n-\n-\n ## Evaluation\n \n <!-- This section describes the evaluation protocols and provides the results. -->\n@@ -47,7 +43,7 @@ It supports the following languages: English, French, German, Italian, Portugues\n \n <!-- This should link to a Dataset Card if possible. -->\n \n-The model was evaluated on MMLU, TriviaQA, NaturalQuestions, ARC Easy & Challenge, Open Book QA, Common Sense QA, \n+The model was evaluated on MMLU, TriviaQA, NaturalQuestions, ARC Easy & Challenge, Open Book QA, Common Sense QA,\n Physical Interaction QA, Social Interaction QA, HellaSwag, WinoGrande, Multilingual Knowledge QA, FLORES 200.\n \n #### Metrics\n@@ -92,7 +88,6 @@ We report BLEU on FLORES.\n || HS | 58.6 | 40.8 | 60.5 | 61.1 | 51.4 |\n || MKQA | 16.0 | 7.9 | 18.5 | 20.6 | 10.6 |\n \n-\n ## Technical Specifications\n \n ### Model Architecture and Objective\n@@ -110,12 +105,11 @@ Tips:\n \n - This model was contributed by [Laurent Mazare](https://huggingface.co/lmz)\n \n-  \n ## Usage tips\n \n `Helium` can be found on the [Huggingface Hub](https://huggingface.co/models?other=helium)\n \n-In the following, we demonstrate how to use `helium-1-preview` for the inference. \n+In the following, we demonstrate how to use `helium-1-preview` for the inference.\n \n ```python\n >>> from transformers import AutoModelForCausalLM, AutoTokenizer"
        },
        {
            "sha": "aa6a4bf96adf50ffb0636939a9a8bf05bde69160",
            "filename": "docs/source/en/model_doc/herbert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fherbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fherbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fherbert.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -45,7 +45,6 @@ models.*\n This model was contributed by [rmroczkowski](https://huggingface.co/rmroczkowski). The original code can be found\n [here](https://github.com/allegro/HerBERT).\n \n-\n ## Usage example\n \n ```python"
        },
        {
            "sha": "e5da5a0582d0a0257b24d293f1751009eb0c7536",
            "filename": "docs/source/en/model_doc/hgnet_v2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -81,13 +81,11 @@ print(f\"The predicted class label is: {predicted_class_label}\")\n \n [[autodoc]] HGNetV2Config\n \n-\n ## HGNetV2Backbone\n \n [[autodoc]] HGNetV2Backbone\n     - forward\n \n-\n ## HGNetV2ForImageClassification\n \n [[autodoc]] HGNetV2ForImageClassification"
        },
        {
            "sha": "b8fd9c141839a1b3e69d83f814d22bec2820114a",
            "filename": "docs/source/en/model_doc/hiera.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fhiera.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fhiera.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhiera.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n Hiera was proposed in [Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles](https://huggingface.co/papers/2306.00989) by Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, Christoph Feichtenhofer\n \n-The paper introduces \"Hiera,\" a hierarchical Vision Transformer that simplifies the architecture of modern hierarchical vision transformers by removing unnecessary components without compromising on accuracy or efficiency. Unlike traditional transformers that add complex vision-specific components to improve supervised classification performance, Hiera demonstrates that such additions, often termed \"bells-and-whistles,\" are not essential for high accuracy. By leveraging a strong visual pretext task (MAE) for pretraining, Hiera retains simplicity and achieves superior accuracy and speed both in inference and training across various image and video recognition tasks. The approach suggests that spatial biases required for vision tasks can be effectively learned through proper pretraining, eliminating the need for added architectural complexity. \n+The paper introduces \"Hiera,\" a hierarchical Vision Transformer that simplifies the architecture of modern hierarchical vision transformers by removing unnecessary components without compromising on accuracy or efficiency. Unlike traditional transformers that add complex vision-specific components to improve supervised classification performance, Hiera demonstrates that such additions, often termed \"bells-and-whistles,\" are not essential for high accuracy. By leveraging a strong visual pretext task (MAE) for pretraining, Hiera retains simplicity and achieves superior accuracy and speed both in inference and training across various image and video recognition tasks. The approach suggests that spatial biases required for vision tasks can be effectively learned through proper pretraining, eliminating the need for added architectural complexity.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "5a072214406c20a6a13e3da258a26bc0bf609487",
            "filename": "docs/source/en/model_doc/hubert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -115,6 +115,7 @@ print(transcription[0])\n \n - HuBERT models expect raw audio input as a 1D float array sampled at 16kHz.\n - If you want to use a `head_mask`, use the model with `attn_implementation=\"eager\"`.\n+\n   ```python\n   model = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\", attn_implementation=\"eager\")\n   ```"
        },
        {
            "sha": "84f9e44e522514001d9bce017220a108287dadce",
            "filename": "docs/source/en/model_doc/hunyuan_v1_dense.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_dense.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_dense.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_dense.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -25,7 +25,6 @@ To be released with the official model launch.\n \n To be released with the official model launch.\n \n-\n ## Usage tips\n \n To be released with the official model launch.\n@@ -48,4 +47,3 @@ To be released with the official model launch.\n \n [[autodoc]] HunYuanDenseV1ForSequenceClassification\n     - forward\n-"
        },
        {
            "sha": "e9bff74fe1bc1d583c3e8aebbfb57aa9b9b97139",
            "filename": "docs/source/en/model_doc/hunyuan_v1_moe.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_moe.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -25,7 +25,6 @@ To be released with the official model launch.\n \n To be released with the official model launch.\n \n-\n ## Usage tips\n \n To be released with the official model launch.\n@@ -48,4 +47,3 @@ To be released with the official model launch.\n \n [[autodoc]] HunYuanMoEV1ForSequenceClassification\n     - forward\n-"
        },
        {
            "sha": "fdb6e5de46596ce1ce44845ae928dd4698f17e29",
            "filename": "docs/source/en/model_doc/idefics.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -34,7 +34,6 @@ The abstract from the paper is the following:\n \n This model was contributed by [HuggingFaceM4](https://huggingface.co/HuggingFaceM4). The original code can be found [here](<INSERT LINK TO GITHUB REPO HERE>). (TODO: don't have a public link yet).\n \n-\n <Tip warning={true}>\n \n IDEFICS modeling code in Transformers is for finetuning and inferencing the pre-trained IDEFICS models.\n@@ -43,7 +42,6 @@ To train a new IDEFICS model from scratch use the m4 codebase (a link will be pr\n \n </Tip>\n \n-\n ## IdeficsConfig\n \n [[autodoc]] IdeficsConfig"
        },
        {
            "sha": "696ad7c5d2bd338dd8397b63d5415942d0f194d8",
            "filename": "docs/source/en/model_doc/idefics2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -202,19 +202,16 @@ A list of official Hugging Face and community (indicated by 🌎) resources to h\n \n [[autodoc]] Idefics2Config\n \n-\n ## Idefics2Model\n \n [[autodoc]] Idefics2Model\n     - forward\n \n-\n ## Idefics2ForConditionalGeneration\n \n [[autodoc]] Idefics2ForConditionalGeneration\n     - forward\n \n-\n ## Idefics2ImageProcessor\n [[autodoc]] Idefics2ImageProcessor\n     - preprocess"
        },
        {
            "sha": "0c8f46a9aeef2e9db9ce4593c2c9a468ea5926ad",
            "filename": "docs/source/en/model_doc/idefics3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics3.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -45,6 +45,7 @@ If `do_resize` is set to `True`, the model resizes images so that the longest ed\n The default resizing behavior can be customized by passing a dictionary to the `size` parameter. For example, `{\"longest_edge\": 4 * 364}` is the default, but you can change it to a different value if needed.\n \n Here’s how to control resizing and set a custom size:\n+\n ```python\n image_processor = Idefics3ImageProcessor(do_resize=True, size={\"longest_edge\": 2 * 364}, max_image_size=364)\n ```\n@@ -53,7 +54,6 @@ Additionally, the `max_image_size` parameter, which controls the size of each sq\n \n This model was contributed by [amyeroberts](https://huggingface.co/amyeroberts) and [andimarafioti](https://huggingface.co/andito).\n \n-\n ## Idefics3Config\n \n [[autodoc]] Idefics3Config\n@@ -76,7 +76,6 @@ This model was contributed by [amyeroberts](https://huggingface.co/amyeroberts)\n [[autodoc]] Idefics3ForConditionalGeneration\n     - forward\n \n-\n ## Idefics3ImageProcessor\n [[autodoc]] Idefics3ImageProcessor\n     - preprocess"
        },
        {
            "sha": "a81e7c3ab281671b20a014652194c36c650d2b33",
            "filename": "docs/source/en/model_doc/ijepa.md",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -31,10 +31,8 @@ You can find the original I-JEPA checkpoints under the [AI at Meta](https://hugg\n > [!TIP]\n > This model was contributed by [jmtzt](https://huggingface.co/jmtzt).\n \n-\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/ijepa_architecture.jpg\">\n \n-\n > Click on the I-JEPA models in the right sidebar for more examples of how to apply I-JEPA to different image representation and classification tasks.\n \n The example below demonstrates how to extract image features with [`Pipeline`] or the [`AutoModel`] class.\n@@ -88,10 +86,10 @@ embed_2 = infer(image_2)\n similarity = cosine_similarity(embed_1, embed_2)  \n print(similarity)\n ```\n+\n </hfoption>\n </hfoptions>\n \n-\n Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quantize the weights to 4-bits.\n \n@@ -142,4 +140,3 @@ print(similarity)\n \n [[autodoc]] IJepaForImageClassification\n     - forward\n-"
        },
        {
            "sha": "d22d8df0d39759b001dd09612555cab348da24dd",
            "filename": "docs/source/en/model_doc/instructblip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -59,7 +59,6 @@ The attributes can be obtained from model config, as `model.config.num_query_tok\n \n [[autodoc]] InstructBlipProcessor\n \n-\n ## InstructBlipVisionModel\n \n [[autodoc]] InstructBlipVisionModel"
        },
        {
            "sha": "d4d868b7f90e293fd4a819ba06b4ffccf66e4b1a",
            "filename": "docs/source/en/model_doc/instructblipvideo.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -59,7 +59,6 @@ The attributes can be obtained from model config, as `model.config.num_query_tok\n \n [[autodoc]] InstructBlipVideoProcessor\n \n-\n ## InstructBlipVideoVideoProcessor\n \n [[autodoc]] InstructBlipVideoVideoProcessor"
        },
        {
            "sha": "7e9fea7f4f20f986f971c349f88ea0a06e271e33",
            "filename": "docs/source/en/model_doc/internvl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -15,7 +15,6 @@ rendered properly in your Markdown viewer.\n -->\n *This model was released on 2025-04-14 and added to Hugging Face Transformers on 2025-04-18.*\n \n-\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n@@ -32,19 +31,14 @@ The abstract from the paper is the following:\n \n *We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs.*\n \n-\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/internvl_architecture.png\" alt=\"drawing\" width=\"600\"/>\n \n <small> Overview of InternVL3 models architecture, which is the same as InternVL2.5. Taken from the <a href=\"https://huggingface.co/OpenGVLab/InternVL3-1B\">original checkpoint.</a> </small>\n \n-\n-\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/internvl_overview_performance.png\" alt=\"drawing\" width=\"600\"/>\n \n <small> Comparison of InternVL3 performance on OpenCompass against other SOTA VLLMs. Taken from the <a href=\"https://huggingface.co/OpenGVLab/InternVL3-1B\">original checkpoint.</a> </small>\n \n-\n-\n This model was contributed by [yonigozlan](https://huggingface.co/yonigozlan).\n The original code can be found [here](https://github.com/OpenGVLab/InternVL).\n \n@@ -75,6 +69,7 @@ Here is how you can use the `image-text-to-text` pipeline to perform inference w\n >>> outputs[0][\"generated_text\"]\n 'The image showcases a vibrant scene of nature, featuring several flowers and a bee. \\n\\n1. **Foreground Flowers**: \\n   - The primary focus is on a large, pink cosmos flower with a prominent yellow center. The petals are soft and slightly r'\n ```\n+\n ### Inference on a single image\n \n This example demonstrates how to perform inference on a single image with the InternVL models using chat templates.\n@@ -112,7 +107,6 @@ This example demonstrates how to perform inference on a single image with the In\n ### Text-only generation\n This example shows how to generate text using the InternVL model without providing any image input.\n \n-\n ```python\n >>> from transformers import AutoProcessor, AutoModelForImageTextToText\n >>> import torch"
        },
        {
            "sha": "f85d08c5f64db04e2a49ac2e1a2530ec64124a9e",
            "filename": "docs/source/en/model_doc/jamba.md",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fjamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fjamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjamba.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -75,6 +75,7 @@ input_ids = tokenizer(\"Plants create energy through a process known as\", return_\n output = model.generate(**input_ids, cache_implementation=\"static\")\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n+\n </hfoption>\n <hfoption id=\"transformers CLI\">\n \n@@ -140,19 +141,16 @@ print(assistant_response)\n \n [[autodoc]] JambaConfig\n \n-\n ## JambaModel\n \n [[autodoc]] JambaModel\n     - forward\n \n-\n ## JambaForCausalLM\n \n [[autodoc]] JambaForCausalLM\n     - forward\n \n-\n ## JambaForSequenceClassification\n \n [[autodoc]] transformers.JambaForSequenceClassification"
        },
        {
            "sha": "3fca2c2d6764ca0a66025cd25b331d126431f54d",
            "filename": "docs/source/en/model_doc/jetmoe.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fjetmoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fjetmoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjetmoe.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -27,15 +27,14 @@ rendered properly in your Markdown viewer.\n \n **JetMoe-8B** is an 8B Mixture-of-Experts (MoE) language model developed by [Yikang Shen](https://scholar.google.com.hk/citations?user=qff5rRYAAAAJ) and [MyShell](https://myshell.ai/).\n JetMoe project aims to provide a LLaMA2-level performance and efficient language model with a limited budget.\n-To achieve this goal, JetMoe uses a sparsely activated architecture inspired by the [ModuleFormer](https://huggingface.co/papers/2306.04640). \n+To achieve this goal, JetMoe uses a sparsely activated architecture inspired by the [ModuleFormer](https://huggingface.co/papers/2306.04640).\n Each JetMoe block consists of two MoE layers: Mixture of Attention Heads and Mixture of MLP Experts.\n Given the input tokens, it activates a subset of its experts to process them.\n-This sparse activation schema enables JetMoe to achieve much better training throughput than similar size dense models. \n+This sparse activation schema enables JetMoe to achieve much better training throughput than similar size dense models.\n The training throughput of JetMoe-8B is around 100B tokens per day on a cluster of 96 H100 GPUs with a straightforward 3-way pipeline parallelism strategy.\n \n This model was contributed by [Yikang Shen](https://huggingface.co/YikangS).\n \n-\n ## JetMoeConfig\n \n [[autodoc]] JetMoeConfig"
        },
        {
            "sha": "706ce04cef4d3a90337be1d853207c0c57725643",
            "filename": "docs/source/en/model_doc/kosmos2_5.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -19,7 +19,6 @@ specific language governing permissions and limitations under the License.\n     </div>\n </div>\n \n-\n # KOSMOS-2.5\n \n The Kosmos-2.5 model was proposed in [KOSMOS-2.5: A Multimodal Literate Model](https://huggingface.co/papers/2309.11419/) by Microsoft.\n@@ -159,7 +158,6 @@ image.save(\"output.png\")\n </hfoption>\n </hfoptions>\n \n-\n ## Chat version\n \n The authors also released Kosmos-2.5 Chat, which is a chat version optimized for document understanding. You can use it like so:"
        },
        {
            "sha": "f3428f6b86ff7110427043434679414ada3e4236",
            "filename": "docs/source/en/model_doc/kyutai_speech_to_text.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -15,7 +15,7 @@ rendered properly in your Markdown viewer.\n -->\n *This model was released on 2025-06-17 and added to Hugging Face Transformers on 2025-06-25.*\n \n-# Kyutai Speech-To-Text \n+# Kyutai Speech-To-Text\n ## Overview\n \n [Kyutai STT](https://kyutai.org/next/stt) is a speech-to-text model architecture based on the [Mimi codec](https://huggingface.co/docs/transformers/en/model_doc/mimi), which encodes audio into discrete tokens in a streaming fashion, and a [Moshi-like](https://huggingface.co/docs/transformers/en/model_doc/moshi) autoregressive decoder. Kyutai’s lab has released two model checkpoints:\n@@ -98,7 +98,6 @@ for output in decoded_outputs:\n This model was contributed by [Eustache Le Bihan](https://huggingface.co/eustlb).\n The original code can be found [here](https://github.com/kyutai-labs/moshi).\n \n-\n ## KyutaiSpeechToTextConfig\n \n [[autodoc]] KyutaiSpeechToTextConfig"
        },
        {
            "sha": "88dde323e2998175125adde6ffd8b07c0fc62890",
            "filename": "docs/source/en/model_doc/layoutlm.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -116,7 +116,6 @@ A list of official Hugging Face and community (indicated by 🌎) resources to h\n - Refer to this [notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb) for an example of how to fine-tune LayoutLM for token classification.\n - Read [Deploy LayoutLM with Hugging Face Inference Endpoints](https://www.philschmid.de/inference-endpoints-layoutlm) to learn how to deploy LayoutLM.\n \n-\n ## LayoutLMConfig\n \n [[autodoc]] LayoutLMConfig"
        },
        {
            "sha": "f74d3b4294ee6b6da453a94ead658bbcc5f130f3",
            "filename": "docs/source/en/model_doc/layoutlmv2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -55,10 +55,12 @@ this https URL.*\n \n LayoutLMv2 depends on `detectron2`, `torchvision` and `tesseract`. Run the\n following to install them:\n+\n ```bash\n python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n python -m pip install torchvision tesseract\n ```\n+\n (If you are developing for LayoutLMv2, note that passing the doctests also requires the installation of these packages.)\n \n ## Usage tips\n@@ -145,7 +147,6 @@ A list of official Hugging Face and community (indicated by 🌎) resources to h\n - See also: [Question answering task guide](../tasks/question_answering)\n - See also: [Document question answering task guide](../tasks/document_question_answering)\n \n-\n <PipelineTag pipeline=\"token-classification\"/>\n \n - A notebook on how to [finetune LayoutLMv2 for token-classification on CORD dataset](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/CORD/Fine_tuning_LayoutLMv2ForTokenClassification_on_CORD.ipynb)."
        },
        {
            "sha": "ce1baa619a8867a0f248be479112fe055806c3e7",
            "filename": "docs/source/en/model_doc/led.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -89,6 +89,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```bash\n !echo -e \"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet. Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts.\" | transformers run --task summarization --model allenai/led-base-16384 --device 0\n ```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "0e78f9935f925661ae62d03a1bd20751c842ea91",
            "filename": "docs/source/en/model_doc/lfm2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-[LFM2](https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models) represents a new generation of Liquid Foundation Models developed by [Liquid AI](https://liquid.ai/), specifically designed for edge AI and on-device deployment. \n+[LFM2](https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models) represents a new generation of Liquid Foundation Models developed by [Liquid AI](https://liquid.ai/), specifically designed for edge AI and on-device deployment.\n \n The models are available in three sizes (350M, 700M, and 1.2B parameters) and are engineered to run efficiently on CPU, GPU, and NPU hardware, making them particularly well-suited for applications requiring low latency, offline operation, and privacy.\n "
        },
        {
            "sha": "2e25d94e883ae08d0052d4ac60d15ceb8da854bf",
            "filename": "docs/source/en/model_doc/lfm2_vl.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_vl.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -19,7 +19,7 @@ rendered properly in your Markdown viewer.\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n-# LFM2-VL   \n+# LFM2-VL\n \n ## Overview\n \n@@ -31,7 +31,7 @@ LFM2-VL consists of three main components: a language model backbone, a vision e\n * Shape-optimized (400M) for more fine-grained vision capabilities for LFM2-VL-1.6B\n * Base (86M) for fast image processing for LFM2-VL-450M\n \n-The encoder processes images at their native resolution up to 512×512 pixels, efficiently handling smaller images without upscaling and supporting non-standard aspect ratios without distortion. Larger images are split into non-overlapping square patches of 512×512 each, preserving detail. In LFM2-VL-1.6B, the model also receives a thumbnail (a small, downscaled version of the original image capturing the overall scene) to enhance global context understanding and alignment. Special tokens mark each patch’s position and indicate the thumbnail’s start. The multimodal connector is a 2-layer MLP connector with pixel unshuffle to reduce image token count. \n+The encoder processes images at their native resolution up to 512×512 pixels, efficiently handling smaller images without upscaling and supporting non-standard aspect ratios without distortion. Larger images are split into non-overlapping square patches of 512×512 each, preserving detail. In LFM2-VL-1.6B, the model also receives a thumbnail (a small, downscaled version of the original image capturing the overall scene) to enhance global context understanding and alignment. Special tokens mark each patch’s position and indicate the thumbnail’s start. The multimodal connector is a 2-layer MLP connector with pixel unshuffle to reduce image token count.\n \n ## Example\n "
        },
        {
            "sha": "16827345ef0562ed0b1ea5d49e9291f87a2d635a",
            "filename": "docs/source/en/model_doc/lightglue.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -153,4 +153,3 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n [[autodoc]] LightGlueForKeypointMatching\n \n - forward\n-"
        },
        {
            "sha": "c66667f235f64ae77b72482c1a315fd6ffa3d181",
            "filename": "docs/source/en/model_doc/llama2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -130,19 +130,20 @@ visualizer(\"Plants create energy through a process known as\")\n     # update model config with padding token\n     model.config.pad_token_id\n     ```\n+\n - It is recommended to initialize the `embed_tokens` layer with the following code to ensure encoding the padding token outputs zeros.\n \n     ```py\n     self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)\n     ```\n+\n - The tokenizer is a byte-pair encoding model based on [SentencePiece](https://github.com/google/sentencepiece). During decoding, if the first token is the start of the word (for example, \"Banana\"), the tokenizer doesn't prepend the prefix space to the string.\n - Don't use the `dtype` parameter in [`~AutoModel.from_pretrained`] if you're using FlashAttention-2 because it only supports fp16 or bf16. You should use [Automatic Mixed Precision](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html), set fp16 or bf16 to `True` if using [`Trainer`], or use [torch.autocast](https://pytorch.org/docs/stable/amp.html#torch.autocast).\n \n ## LlamaConfig\n \n [[autodoc]] LlamaConfig\n \n-\n ## LlamaTokenizer\n \n [[autodoc]] LlamaTokenizer\n@@ -165,7 +166,6 @@ visualizer(\"Plants create energy through a process known as\")\n [[autodoc]] LlamaModel\n     - forward\n \n-\n ## LlamaForCausalLM\n \n [[autodoc]] LlamaForCausalLM"
        },
        {
            "sha": "84812a41997f69e05b5b732fe0c89e6d7fa084aa",
            "filename": "docs/source/en/model_doc/llama4.md",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -17,7 +17,6 @@ rendered properly in your Markdown viewer.\n \n # Llama4\n \n-\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n@@ -53,7 +52,6 @@ The examples below demonstrates how to generate with [`Pipeline`] or the [`AutoM\n showcasing how to toggle the right attributes to enable very long-context generations, as some flavors of Llama 4\n have context lengths going up to 10 million tokens.\n \n-\n <hfoptions id=\"usage\">\n <hfoption id=\"Pipeline\">\n \n@@ -255,7 +253,6 @@ Updating the default attention function can significantly improve compute perfor\n As of release, the Llama 4 model supports the following attention methods: `eager`, `flex_attention`, `sdpa`. We recommend using `flex_attention` for best results.\n Switching attention mechanism is done at the model initialization step:\n \n-\n <hfoptions id=\"Attention\">\n <hfoption id=\"Flex Attention\">\n \n@@ -278,6 +275,7 @@ model = Llama4ForConditionalGeneration.from_pretrained(\n     dtype=torch.bfloat16,\n )\n ```\n+\n </hfoption>\n <hfoption id=\"SDPA\">\n The `sdpa` attention method is generally more compute-efficient than the `eager` method.\n@@ -293,6 +291,7 @@ model = Llama4ForConditionalGeneration.from_pretrained(\n     dtype=torch.bfloat16,\n )\n ```\n+\n </hfoption>\n <hfoption id=\"Eager\">\n The `eager` attention method is set by default, so no need for anything different when loading the model:\n@@ -307,19 +306,17 @@ model = Llama4ForConditionalGeneration.from_pretrained(\n     dtype=torch.bfloat16,\n )\n ```\n+\n </hfoption>\n </hfoptions>\n \n-\n ### Quantization\n \n Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for available quantization backends.\n At time of release, both FBGEMM and LLM-Compressor are supported; more quantization methods will be supported in the days that follow the release.\n \n See below for examples using both:\n \n-\n-\n Here is an example loading an BF16 model in FP8 using the FBGEMM approach:\n \n <hfoptions id=\"Quantization\">\n@@ -378,6 +375,7 @@ outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n outputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])\n print(outputs[0])\n ```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "e4ef7d770694f29322799273bab2f2a35f1174b1",
            "filename": "docs/source/en/model_doc/llava.md",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -47,13 +47,11 @@ The original code can be found [here](https://github.com/haotian-liu/LLaVA/tree/\n \n - Note the model has not been explicitly trained to process multiple images in the same prompt, although this is technically possible, you may experience inaccurate results.\n \n-\n > [!NOTE]\n > LLaVA models after release v4.46 will raise warnings about adding `processor.patch_size = {{patch_size}}`, `processor.num_additional_image_tokens = {{num_additional_image_tokens}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you.\n Adding these attributes means that LLaVA will try to infer the number of image tokens required per image and expand the text with as many `<image>` placeholders as there will be tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.\n The attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n \n-\n ### Formatting Prompts with Chat Templates  \n \n Each **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor’s `apply_chat_template` method.  \n@@ -63,11 +61,9 @@ Each **checkpoint** is trained with a specific prompt format, depending on the u\n - Each message should be a dictionary with `\"role\"` and `\"content\"` keys.  \n - The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.  \n \n-\n-Here’s an example of how to structure your input. \n+Here’s an example of how to structure your input.\n We will use [llava-hf/llava-1.5-7b-hf](https://huggingface.co/llava-hf/llava-1.5-7b-hf) and a conversation history of text and image. Each content field has to be a list of dicts, as follows:\n \n-\n ```python\n from transformers import AutoProcessor\n \n@@ -104,6 +100,7 @@ print(text_prompt)\n - If you want to construct a chat prompt yourself, below is a list of prompt formats accepted by each llava checkpoint:\n \n [llava-interleave models](https://huggingface.co/collections/llava-hf/llava-interleave-668e19a97da0036aad4a2f19) requires the following format:\n+\n ```bash\n \"<|im_start|>user <image>\\nWhat is shown in this image?<|im_end|><|im_start|>assistant\"\n ```\n@@ -115,6 +112,7 @@ For multiple turns conversation:\n ```\n \n [llava-1.5 models](https://huggingface.co/collections/llava-hf/llava-15-65f762d5b6941db5c2ba07e0) requires the following format:\n+\n ```bash\n \"USER: <image>\\n<prompt> ASSISTANT:\"\n ```\n@@ -127,12 +125,10 @@ For multiple turns conversation:\n \n 🚀 **Bonus:** If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the **Usage Examples** below for more details on how to use it.\n \n-\n ## Usage examples\n \n ### Single input inference\n \n-\n ```python\n import torch\n from transformers import AutoProcessor, LlavaForConditionalGeneration\n@@ -164,7 +160,6 @@ generate_ids = model.generate(**inputs, max_new_tokens=30)\n processor.batch_decode(generate_ids, skip_special_tokens=True)\n ```\n \n-\n ### Batched inference\n \n LLaVa also supports batched inference. Here is how you can do it:\n@@ -214,7 +209,6 @@ generate_ids = model.generate(**inputs, max_new_tokens=30)\n processor.batch_decode(generate_ids, skip_special_tokens=True)\n ```\n \n-\n ## Note regarding reproducing original implementation\n \n In order to match the logits of the [original implementation](https://github.com/haotian-liu/LLaVA/tree/main), one needs to additionally specify `do_pad=True` when instantiating `LlavaImageProcessor`:\n@@ -238,7 +232,6 @@ A list of official Hugging Face and community (indicated by 🌎) resources to h\n - A [Google Colab demo](https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing) on how to run Llava on a free-tier Google colab instance leveraging 4-bit inference.\n - A [similar notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LLaVa/Inference_with_LLaVa_for_multimodal_generation.ipynb) showcasing batched inference. 🌎\n \n-\n ## LlavaConfig\n \n [[autodoc]] LlavaConfig"
        },
        {
            "sha": "3857f154cf4b889ffc9903fc5a3908728953003c",
            "filename": "docs/source/en/model_doc/llava_next.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -141,7 +141,6 @@ with torch.inference_mode():\n print(processor.decode(output[0], skip_special_tokens=True))\n ```\n \n-\n ## Notes\n \n * Different checkpoints (Mistral, Vicuna, etc.) require a specific prompt format depending on the underlying LLM. Always use [`~ProcessorMixin.apply_chat_template`] to ensure correct formatting. Refer to the [Templates](../chat_templating) guide for more details.\n@@ -189,7 +188,6 @@ output = model.generate(**inputs, max_new_tokens=100)\n print(processor.decode(output[0], skip_special_tokens=True))\n ```\n \n-\n ## LlavaNextConfig\n \n [[autodoc]] LlavaNextConfig"
        },
        {
            "sha": "131dd1aba50e24e2af4325e981c793ce889235e8",
            "filename": "docs/source/en/model_doc/llava_next_video.md",
            "status": "modified",
            "additions": 2,
            "deletions": 14,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -30,7 +30,6 @@ The LLaVa-NeXT-Video model was proposed in [LLaVA-NeXT: A Strong Zero-shot Video\n \n [LLaVA-NeXT](llava_next) surprisingly has strong performance in understanding video content in zero-shot fashion with the AnyRes technique that it uses. The AnyRes technique naturally represents a high-resolution image into multiple images. This technique is naturally generalizable to represent videos because videos can be considered as a set of frames (similar to a set of images in LLaVa-NeXT). The current version of LLaVA-NeXT makes use of AnyRes and trains with supervised fine-tuning (SFT) on top of LLaVA-Next on video data to achieves better video understanding capabilities.The model is a current SOTA among open-source models on [VideoMME bench](https://huggingface.co/papers/2405.21075).\n \n-\n The introduction from the blog is the following:\n \n On January 30, 2024, we released LLaVA-NeXT, an open-source Large Multimodal Model (LMM) that has been trained exclusively on text-image data. With the proposed AnyRes technique, it boosts capabilities in reasoning, OCR, and world knowledge, demonstrating remarkable performance across a spectrum of image-based multimodal understanding tasks, and even exceeding Gemini-Pro on several image benchmarks, e.g. MMMU and MathVista.\n@@ -42,7 +41,6 @@ On January 30, 2024, we released LLaVA-NeXT, an open-source Large Multimodal Mod\n - Strong video understanding ability. (1) LLaVA-Next-Image, which combines the above two techniques, yields superior zero-shot performance than open-source LMMs tuned on videos. (2) LLaVA-Next-Video, further supervised fine-tuning (SFT) LLaVA-Next-Image on video data, achieves better video understanding capabilities compared to LLaVA-Next-Image. (3) LLaVA-Next-Video-DPO, which aligns the model response with AI feedback using direct preference optimization (DPO), showing significant performance boost.\n - Efficient deployment and inference with SGLang. It allows 5x faster inference on video tasks, allowing more scalable serving such as million-level video re-captioning. See instructions in our repo.**\n \n-\n This model was contributed by [RaushanTurganbay](https://huggingface.co/RaushanTurganbay).\n The original code can be found [here](https://github.com/LLaVA-VL/LLaVA-NeXT/tree/inference).\n \n@@ -56,13 +54,11 @@ The original code can be found [here](https://github.com/LLaVA-VL/LLaVA-NeXT/tre\n \n </Tip>\n \n-\n > [!NOTE]\n > LLaVA models after release v4.46 will raise warnings about adding `processor.patch_size = {{patch_size}}`, `processor.num_additional_image_tokens = {{num_additional_image_tokens}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you.\n Adding these attributes means that LLaVA will try to infer the number of image tokens required per image and expand the text with as many `<image>` placeholders as there will be tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.\n The attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n \n-\n ### Formatting Prompts with Chat Templates  \n \n Each **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor’s `apply_chat_template` method.  \n@@ -72,7 +68,6 @@ Each **checkpoint** is trained with a specific prompt format, depending on the u\n - Each message should be a dictionary with `\"role\"` and `\"content\"` keys.  \n - The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.  \n \n-\n Here’s an example of how to structure your input. We will use [LLaVA-NeXT-Video-7B-hf](https://huggingface.co/llava-hf/LLaVA-NeXT-Video-7B-hf) and a conversation history of videos and images.\n \n ```python\n@@ -116,8 +111,6 @@ print(text_prompt)\n \n 🚀 **Bonus:** If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the **Usage Examples** below for more details on how to use it.\n \n-\n-\n ## Usage example\n \n ### Single Media Mode\n@@ -153,10 +146,9 @@ out = model.generate(**inputs, max_new_tokens=60)\n processor.batch_decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n ```\n \n-\n ### Mixed Media Mode\n \n-The model can also generate from an interleaved image-video inputs. However note, that it was not trained in interleaved image-video setting which might affect the performance. Below is an example usage for mixed media input, add the following lines to the above code snippet: \n+The model can also generate from an interleaved image-video inputs. However note, that it was not trained in interleaved image-video setting which might affect the performance. Below is an example usage for mixed media input, add the following lines to the above code snippet:\n \n ```python\n \n@@ -196,7 +188,7 @@ processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokeniza\n \n ### Quantization using Bitsandbytes for memory efficiency\n \n-The model can be loaded in lower bits, significantly reducing memory burden while maintaining the performance of the original model. This allows for efficient deployment on resource-constrained cases. \n+The model can be loaded in lower bits, significantly reducing memory burden while maintaining the performance of the original model. This allows for efficient deployment on resource-constrained cases.\n \n First, make sure to install bitsandbytes by running `pip install bitsandbytes` and to have access to a GPU/accelerator that is supported by the library.\n \n@@ -210,7 +202,6 @@ We value your feedback to help identify bugs before the full release! Check out\n \n Then simply load the quantized model by adding [`BitsAndBytesConfig`](../main_classes/quantization#transformers.BitsAndBytesConfig) as shown below:\n \n-\n ```python\n from transformers import LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor\n \n@@ -224,7 +215,6 @@ quantization_config = BitsAndBytesConfig(\n model = LlavaNextVideoForConditionalGeneration.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\", quantization_config=quantization_config, device_map=\"auto\")\n ```\n \n-\n ### Flash-Attention 2 to speed-up generation\n \n Additionally, we can greatly speed-up model inference by using [Flash Attention](../perf_train_gpu_one#flash-attention-2), which is a faster implementation of the attention mechanism used inside the model.\n@@ -249,8 +239,6 @@ model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n ).to(0)\n ```\n \n-\n-\n ## LlavaNextVideoConfig\n \n [[autodoc]] LlavaNextVideoConfig"
        },
        {
            "sha": "48fa769835f371b3e00dd235a9a64b69fae62125",
            "filename": "docs/source/en/model_doc/llava_onevision.md",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -54,7 +54,6 @@ Tips:\n \n </Tip>\n \n-\n ### Formatting Prompts with Chat Templates  \n \n Each **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor’s `apply_chat_template` method.  \n@@ -64,8 +63,7 @@ Each **checkpoint** is trained with a specific prompt format, depending on the u\n - Each message should be a dictionary with `\"role\"` and `\"content\"` keys.  \n - The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.  \n \n-\n-Here’s an example of how to structure your input. \n+Here’s an example of how to structure your input.\n We will use [llava-onevision-qwen2-7b-si-hf](https://huggingface.co/llava-hf/llava-onevision-qwen2-7b-si-hf) and a conversation history of text and image. Each content field has to be a list of dicts, as follows:\n \n ```python\n@@ -103,11 +101,9 @@ print(text_prompt)\n \n 🚀 **Bonus:** If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the **Usage Examples** below for more details on how to use it.\n \n-\n This model was contributed by [RaushanTurganbay](https://huggingface.co/RaushanTurganbay).\n The original code can be found [here](https://github.com/LLaVA-VL/LLaVA-NeXT/tree/main).\n \n-\n ## Usage example\n \n ### Single image inference\n@@ -293,7 +289,6 @@ model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n ).to(0)\n ```\n \n-\n ## LlavaOnevisionConfig\n \n [[autodoc]] LlavaOnevisionConfig"
        },
        {
            "sha": "651f3386f161a1a6ec0c56897081b52ccba3880e",
            "filename": "docs/source/en/model_doc/longcat_flash.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Flongcat_flash.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Flongcat_flash.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flongcat_flash.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -12,7 +12,6 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n \n-\n ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n \n -->\n@@ -43,6 +42,7 @@ The original code can be found [here](https://huggingface.co/meituan-longcat/Lon\n ## Usage examples\n \n The model is large: you will need 2x8 H100 to run inference.\n+\n ```python\n # launch_longcat.py\n from transformers import LongcatFlashForCausalLM, AutoTokenizer\n@@ -76,6 +76,7 @@ torchrun  --nproc_per_node=8 --nnodes=2 --node_rank=0 | 1  --rdzv-id <an_id> --r\n ```\n \n And you'll get a nice generation:\n+\n ```json\n [Round 0] USER:Hello! What is the capital of France? What can you tell me about it? ASSISTANT:Hello! 😊 The capital of France is Paris, one of the most famous and beloved cities in the world. Here’s a quick overview of what makes Paris special:\n 1. Iconic Landmarks"
        },
        {
            "sha": "b8375998a06b93d303ab46f55d7e589ddecb9cf4",
            "filename": "docs/source/en/model_doc/longformer.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Flongformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Flongformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flongformer.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -85,7 +85,6 @@ echo -e \"San Francisco 49ers cornerback Shawntae Spencer will miss the rest of t\n </hfoption>\n </hfoptions>\n \n-\n ## Notes\n \n - Longformer is based on [RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/roberta) and doesn't have `token_type_ids`. You don't need to indicate which token belongs to which segment. You only need to separate the segments with the separation token `</s>` or `tokenizer.sep_token`."
        },
        {
            "sha": "a197de15a5762d1bc58e24d24df48f982ef854c5",
            "filename": "docs/source/en/model_doc/longt5.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Flongt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Flongt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flongt5.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -29,7 +29,6 @@ encoder-decoder transformer pre-trained in a text-to-text denoising generative s\n T5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\n Transient-Global attention.\n \n-\n The abstract from the paper is the following:\n \n *Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\n@@ -95,7 +94,6 @@ The complexity of this mechanism is `O(l(r + l/k))`.\n >>> rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"])\n ```\n \n-\n ## Resources\n \n - [Translation task guide](../tasks/translation)"
        },
        {
            "sha": "f9ac7e5ebe92a70932379be7fd064e8f1b00892f",
            "filename": "docs/source/en/model_doc/m2m_100.md",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -44,7 +44,6 @@ open-source our scripts so that others may reproduce the data, evaluation, and f\n \n This model was contributed by [valhalla](https://huggingface.co/valhalla).\n \n-\n ## Usage tips and examples\n \n M2M100 is a multilingual encoder-decoder (seq-to-seq) model primarily intended for translation tasks. As the model is\n@@ -76,9 +75,9 @@ loss = model(**model_inputs).loss  # forward pass\n \n **Generation**\n \n-M2M100 uses the `eos_token_id` as the `decoder_start_token_id` for generation with the target language id \n-being forced as the first generated token. To force the target language id as the first generated token, pass the \n-*forced_bos_token_id* parameter to the *generate* method. The following example shows how to translate between \n+M2M100 uses the `eos_token_id` as the `decoder_start_token_id` for generation with the target language id\n+being forced as the first generated token. To force the target language id as the first generated token, pass the\n+*forced_bos_token_id* parameter to the *generate* method. The following example shows how to translate between\n Hindi to French and Chinese to English using the *facebook/m2m100_418M* checkpoint.\n \n ```python\n@@ -136,7 +135,7 @@ Hindi to French and Chinese to English using the *facebook/m2m100_418M* checkpoi\n \n Flash Attention 2 is a faster, optimized version of the attention scores computation which relies on `cuda` kernels.\n \n-### Installation \n+### Installation\n \n First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features).\n "
        },
        {
            "sha": "031e353c93daa206c341a380b3e6c48a85a23bac",
            "filename": "docs/source/en/model_doc/mamba.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -27,7 +27,6 @@ rendered properly in your Markdown viewer.\n \n You can find all the original Mamba checkpoints under the [State Space Models](https://huggingface.co/state-spaces) organization.\n \n-\n > [!TIP]\n > This model was contributed by [Molbap](https://huggingface.co/Molbap) and [AntonV](https://huggingface.co/AntonV).\n > Click on the Mamba models in the right sidebar for more examples of how to apply Mamba to different language tasks.\n@@ -93,6 +92,7 @@ input_ids = tokenizer(\"Plants create energy through a process known as\", return_\n output = model.generate(**input_ids)\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n+\n ## Notes\n \n - The current implementation uses the original CUDA kernels. The FlashAttention equivalent implementation is hosted in the [mamba-ssm](https://github.com/state-spaces/mamba) and [causal_conv1d](https://github.com/Dao-AILab/causal-conv1d) repositories. Make sure to install them if your hardware supports it!"
        },
        {
            "sha": "56a33dfbe0b9354ac466a5b64079edc96a38e076",
            "filename": "docs/source/en/model_doc/mamba2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -91,6 +91,7 @@ input_ids = tokenizer(\"Plants create energy through a process known as\", return_\n output = model.generate(**input_ids)\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n+\n ## Notes\n \n - Codestral Mamba has `groups=8` which are similar to the number of kv heads in an attention-based model.\n@@ -124,7 +125,6 @@ trainer = SFTTrainer(\n trainer.train()\n ```\n \n-\n ## Mamba2Config\n \n [[autodoc]] Mamba2Config"
        },
        {
            "sha": "00b2f91677d4246e0575f51c9a420726590eda8b",
            "filename": "docs/source/en/model_doc/marian.md",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarian.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarian.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarian.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -25,23 +25,17 @@ rendered properly in your Markdown viewer.\n \n # MarianMT\n \n-\n-\n [MarianMT](https://huggingface.co/papers/1804.00344) is a machine translation model trained with the Marian framework which is written in pure C++. The framework includes its own custom auto-differentiation engine and efficient meta-algorithms to train encoder-decoder models like BART.\n \n All MarianMT models are transformer encoder-decoders with 6 layers in each component, use static sinusoidal positional embeddings, don't have a layernorm embedding, and the model starts generating with the prefix `pad_token_id` instead of `<s/>`.\n \n-\n-\n You can find all the original MarianMT checkpoints under the [Language Technology Research Group at the University of Helsinki](https://huggingface.co/Helsinki-NLP/models?search=opus-mt) organization.\n \n-\n > [!TIP]\n > This model was contributed by [sshleifer](https://huggingface.co/sshleifer).\n >\n > Click on the MarianMT models in the right sidebar for more examples of how to apply MarianMT to translation tasks.\n \n-\n The example below demonstrates how to translate text using [`Pipeline`] or the [`AutoModel`] class.\n \n <hfoptions id=\"usage\">\n@@ -78,7 +72,6 @@ print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n </hfoption>\n </hfoptions>\n \n-\n Use the [AttentionMaskVisualizer](https://github.com/huggingface/transformers/blob/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/src/transformers/utils/attention_visualizer.py#L139) to better understand what tokens the model can and cannot attend to.\n \n ```python\n@@ -87,6 +80,7 @@ from transformers.utils.attention_visualizer import AttentionMaskVisualizer\n visualizer = AttentionMaskVisualizer(\"Helsinki-NLP/opus-mt-en-de\")\n visualizer(\"Hello, how are you?\")\n ```\n+\n <div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/marianmt-attn-mask.png\"/>\n </div>"
        },
        {
            "sha": "c7608f397f69d1f179f482c802dd74cf0ad7103f",
            "filename": "docs/source/en/model_doc/markuplm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarkuplm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarkuplm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarkuplm.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -54,7 +54,7 @@ These are the XPATH tags and subscripts respectively for each token in the input\n - One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info.\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg\"\n-alt=\"drawing\" width=\"600\"/> \n+alt=\"drawing\" width=\"600\"/>\n \n <small> MarkupLM architecture. Taken from the <a href=\"https://huggingface.co/papers/2110.08518\">original paper.</a> </small>\n "
        },
        {
            "sha": "9180d765c2bc8beb6db16263d5a653317621e504",
            "filename": "docs/source/en/model_doc/matcha.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmatcha.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmatcha.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmatcha.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -42,7 +42,7 @@ Currently 6 checkpoints are available for MatCha:\n - `google/matcha-chartqa`: MatCha model fine-tuned on ChartQA dataset. It can be used to answer questions about charts.\n - `google/matcha-plotqa-v1`: MatCha model fine-tuned on PlotQA dataset. It can be used to answer questions about plots.\n - `google/matcha-plotqa-v2`: MatCha model fine-tuned on PlotQA dataset. It can be used to answer questions about plots.\n-- `google/matcha-chart2text-statista`: MatCha model fine-tuned on Statista dataset. \n+- `google/matcha-chart2text-statista`: MatCha model fine-tuned on Statista dataset.\n - `google/matcha-chart2text-pew`: MatCha model fine-tuned on Pew dataset.\n \n The models finetuned on `chart2text-pew` and `chart2text-statista` are more suited for summarization, whereas the models finetuned on `plotqa` and `chartqa` are more suited for question answering.\n@@ -67,6 +67,7 @@ print(processor.decode(predictions[0], skip_special_tokens=True))\n ## Fine-tuning\n \n To fine-tune MatCha, refer to the pix2struct [fine-tuning notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb). For `Pix2Struct` models, we have found out that fine-tuning the model with Adafactor and cosine learning rate scheduler leads to faster convergence:\n+\n ```python\n from transformers.optimization import Adafactor, get_cosine_schedule_with_warmup\n "
        },
        {
            "sha": "d6580427778ada91522fbb8417e03461694bf94c",
            "filename": "docs/source/en/model_doc/mega.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmega.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmega.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmega.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -44,19 +44,16 @@ The abstract from the paper is the following:\n This model was contributed by [mnaylor](https://huggingface.co/mnaylor).\n The original code can be found [here](https://github.com/facebookresearch/mega).\n \n-\n ## Usage tips\n \n - MEGA can perform quite well with relatively few parameters. See Appendix D in the MEGA paper for examples of architectural specs which perform well in various settings. If using MEGA as a decoder, be sure to set `bidirectional=False` to avoid errors with default bidirectional.\n - Mega-chunk is a variant of mega that reduces time and spaces complexity from quadratic to linear. Utilize chunking with MegaConfig.use_chunking and control chunk size with MegaConfig.chunk_size\n \n-\n ## Implementation Notes\n \n - The original implementation of MEGA had an inconsistent expectation of attention masks for padding and causal self-attention between the softmax attention and Laplace/squared ReLU method. This implementation addresses that inconsistency.\n - The original implementation did not include token type embeddings; this implementation adds support for these, with the option controlled by MegaConfig.add_token_type_embeddings\n \n-\n ## MegaConfig\n \n [[autodoc]] MegaConfig"
        },
        {
            "sha": "5307fdcd491a292c568571d3c42a9cb58a8e9ce8",
            "filename": "docs/source/en/model_doc/megatron-bert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmegatron-bert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmegatron-bert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmegatron-bert.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -45,8 +45,8 @@ achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.\n accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy\n of 89.4%).*\n \n-This model was contributed by [jdemouth](https://huggingface.co/jdemouth). The original code can be found [here](https://github.com/NVIDIA/Megatron-LM). \n-That repository contains a multi-GPU and multi-node implementation of the Megatron Language models. In particular, \n+This model was contributed by [jdemouth](https://huggingface.co/jdemouth). The original code can be found [here](https://github.com/NVIDIA/Megatron-LM).\n+That repository contains a multi-GPU and multi-node implementation of the Megatron Language models. In particular,\n it contains a hybrid model parallel approach using \"tensor parallel\" and \"pipeline parallel\" techniques.\n \n ## Usage tips"
        },
        {
            "sha": "440f89b2c56fc0dfc29818e57b9705f83c2d8edb",
            "filename": "docs/source/en/model_doc/mimi.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmimi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmimi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmimi.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -39,7 +39,7 @@ The example below demonstrates how to encode and decode audio with the [`AutoMod\n <hfoptions id=\"usage\">\n <hfoption id=\"AutoModel\">\n \n-```python \n+```python\n >>> from datasets import load_dataset, Audio\n >>> from transformers import MimiModel, AutoFeatureExtractor\n >>> librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
        },
        {
            "sha": "a27d45089ced35c046d682dfd0688eec1c352e9c",
            "filename": "docs/source/en/model_doc/minimax.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -109,8 +109,8 @@ To load and run a model using Flash Attention-2, refer to the snippet below:\n \n ### Sliding window Attention\n \n-The current implementation supports the sliding window attention mechanism and memory efficient cache management. \n-To enable sliding window attention, just make sure to have a `flash-attn` version that is compatible with sliding window attention (`>=2.3.0`). \n+The current implementation supports the sliding window attention mechanism and memory efficient cache management.\n+To enable sliding window attention, just make sure to have a `flash-attn` version that is compatible with sliding window attention (`>=2.3.0`).\n \n The Flash Attention-2 model uses also a more memory efficient cache slicing mechanism - as recommended per the official implementation of Mistral model that use rolling cache mechanism we keep the cache size fixed (`self.config.sliding_window`), support batched generation only for `padding_side=\"left\"` and use the absolute position of the current token to compute the positional embedding.\n "
        },
        {
            "sha": "c2128512586fe22268db2367610f6dba7adda84a",
            "filename": "docs/source/en/model_doc/ministral.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -30,7 +30,6 @@ rendered properly in your Markdown viewer.\n \n This architecture turns out to coincide with Qwen2, with the main difference being the presence of biases in attention projections in Ministral.\n \n-\n You can find the Ministral checkpoints under the [Mistral AI](https://huggingface.co/mistralai) organization.\n \n ## Usage"
        },
        {
            "sha": "865ee414532cf15bb1ec64b83e5e4b702fb4a7a9",
            "filename": "docs/source/en/model_doc/mistral.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -86,7 +86,6 @@ echo -e \"My favorite condiment is\" | transformers chat mistralai/Mistral-7B-v0.3\n </hfoption>\n </hfoptions>\n \n-\n Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quantize the weights to 4-bits."
        },
        {
            "sha": "4ac264ac9854bc16eeac36aedaf11a03d49cc4ec",
            "filename": "docs/source/en/model_doc/mistral3.md",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -27,7 +27,6 @@ rendered properly in your Markdown viewer.\n \n You can find the original Mistral 3 checkpoints under the [Mistral AI](https://huggingface.co/mistralai/models?search=mistral-small-3) organization.\n \n-\n > [!TIP]\n > This model was contributed by [cyrilvallez](https://huggingface.co/cyrilvallez) and [yonigozlan](https://huggingface.co/yonigozlan).\n > Click on the Mistral3 models in the right sidebar for more examples of how to apply Mistral3 to different tasks.\n@@ -62,6 +61,7 @@ outputs = pipeline(text=messages, max_new_tokens=50, return_full_text=False)\n outputs[0][\"generated_text\"]\n 'The image depicts a vibrant and lush garden scene featuring a variety of wildflowers and plants. The central focus is on a large, pinkish-purple flower, likely a Greater Celandine (Chelidonium majus), with a'\n ```\n+\n </hfoption>\n <hfoption id=\"AutoModel\">\n \n@@ -100,13 +100,15 @@ decoded_output = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :\n decoded_output\n 'The image depicts a vibrant and lush garden scene featuring a variety of wildflowers and plants. The central focus is on a large, pinkish-purple flower, likely a Greater Celandine (Chelidonium majus), with a'\n ```\n+\n </hfoption>\n </hfoptions>\n \n-## Notes \n+## Notes\n+\n+- Mistral 3 supports text-only generation.\n \n-- Mistral 3 supports text-only generation. \n-```py \n+```py\n import torch\n from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n \n@@ -136,13 +138,16 @@ print(decoded_output)\n  5. Je me casse, à plus!\n \n ```\n+\n  /\\_/\\\n ( o.o )\n  > ^ <\n+\n ```\"\n ````\n \n-- Mistral 3 accepts batched image and text inputs. \n+- Mistral 3 accepts batched image and text inputs.\n+\n ```py\n import torch\n from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n@@ -184,7 +189,7 @@ messages = [\n , \"Describe this imageThe image depicts a vibrant street scene in what appears to be a Chinatown district. The focal point is a traditional Chinese\"]\n ```\n \n-- Mistral 3 also supported batched image and text inputs with a different number of images for each text. The example below quantizes the model with bitsandbytes. \n+- Mistral 3 also supported batched image and text inputs with a different number of images for each text. The example below quantizes the model with bitsandbytes.\n \n ```py\n import torch"
        },
        {
            "sha": "7665b5901a6ad1319236f53bdaec0e49d89fe732",
            "filename": "docs/source/en/model_doc/mixtral.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -39,7 +39,7 @@ Mixtral-8x7B is the second large language model (LLM) released by [mistral.ai](h\n Mixtral-8x7B is a decoder-only Transformer with the following architectural choices:\n \n - Mixtral is a Mixture of Experts (MoE) model with 8 experts per MLP, with a total of 45 billion parameters. To learn more about mixture-of-experts, refer to the [blog post](https://huggingface.co/blog/moe).\n-- Despite the model having 45 billion parameters, the compute required for a single forward pass is the same as that of a 14 billion parameter model. This is because even though each of the experts have to be loaded in RAM (70B like ram requirement) each token from the hidden states are dispatched twice (top 2 routing) and thus the compute (the operation required at each forward computation) is just 2 X sequence_length. \n+- Despite the model having 45 billion parameters, the compute required for a single forward pass is the same as that of a 14 billion parameter model. This is because even though each of the experts have to be loaded in RAM (70B like ram requirement) each token from the hidden states are dispatched twice (top 2 routing) and thus the compute (the operation required at each forward computation) is just 2 X sequence_length.\n \n The following implementation details are shared with Mistral AI's first model [Mistral-7B](mistral):\n - Sliding Window Attention - Trained with 8k context length and fixed cache size, with a theoretical attention span of 128K tokens\n@@ -138,8 +138,8 @@ Below is a expected speedup diagram that compares pure inference time between th\n \n ### Sliding window Attention\n \n-The current implementation supports the sliding window attention mechanism and memory efficient cache management. \n-To enable sliding window attention, just make sure to have a `flash-attn` version that is compatible with sliding window attention (`>=2.3.0`). \n+The current implementation supports the sliding window attention mechanism and memory efficient cache management.\n+To enable sliding window attention, just make sure to have a `flash-attn` version that is compatible with sliding window attention (`>=2.3.0`).\n \n The Flash Attention-2 model uses also a more memory efficient cache slicing mechanism - as recommended per the official implementation of Mistral model that use rolling cache mechanism we keep the cache size fixed (`self.config.sliding_window`), support batched generation only for `padding_side=\"left\"` and use the absolute position of the current token to compute the positional embedding.\n "
        },
        {
            "sha": "7ff2fb434da0ac09e0b5a4e4fde5a6c889e4d78c",
            "filename": "docs/source/en/model_doc/mlcd.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmlcd.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmlcd.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmlcd.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -32,9 +32,9 @@ Tips:\n \n - We adopted the official [LLaVA-NeXT](https://github.com/LLaVA-VL/LLaVA-NeXT) and the official training dataset [LLaVA-NeXT-Data](https://huggingface.co/datasets/lmms-lab/LLaVA-NeXT-Data) for evaluating the foundational visual models.\n \n-- The language model is [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct). \n+- The language model is [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct).\n \n-Result: \n+Result:\n \n | Vision Tower                                                                                  | RoPE2D | ChartQA   | DocVQA    | InfoVQA   | OCRBench   | MMMU      |\n | :-------------------------------------------------------------------------------------------- | :----: | :-------- | :-------- | :-------- | :--------- | :-------- |\n@@ -45,7 +45,6 @@ Result:\n | **[MLCD (ViT-bigG-14-336px)](https://huggingface.co/DeepGlint-AI/mlcd-vit-bigG-patch14-336)** |   √    | 71.07     | 79.63     | 44.38     | 572.00     | 46.78     |\n | **[MLCD (ViT-bigG-14-448px)](https://huggingface.co/DeepGlint-AI/mlcd-vit-bigG-patch14-448)** |   √    | **73.80** | **83.34** | **46.59** | **582.00** | 46.00     |\n \n-\n ## Usage\n \n ```python"
        },
        {
            "sha": "a0fc5db41cfe4fa5a40b5d94700dd08a49485b1a",
            "filename": "docs/source/en/model_doc/mllama.md",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -35,15 +35,12 @@ The [Llama 3.2-Vision](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-ed\n - The text passed to the processor should have the `\"<|image|>\"` tokens where the images should be inserted.\n - The processor has its own `apply_chat_template` method to convert chat messages to text that can then be passed as text to the processor. If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the **Usage Examples** below for more details on how to use it.\n \n-\n-\n <Tip warning={true}>\n \n Mllama has an extra token used as a placeholder for image positions in the text. It means that input ids and an input embedding layer will have an extra token. But since the weights for input and output embeddings are not tied, the `lm_head` layer has one less token and will fail if you want to calculate loss on image tokens or apply some logit processors. In case you are training, make sure to mask out special `\"<|image|>\"` tokens in the `labels` as the model should not be trained on predicting them.\n \n Otherwise if you see CUDA-side index errors when generating, use the below code to expand the `lm_head` by one more token.\n \n-\n ```python\n old_embeddings = model.get_output_embeddings()\n \n@@ -52,12 +49,13 @@ resized_embeddings = model._get_resized_lm_head(old_embeddings, new_num_tokens=n\n resized_embeddings.requires_grad_(old_embeddings.weight.requires_grad)\n model.set_output_embeddings(resized_embeddings)\n ```\n-</Tip>\n \n+</Tip>\n \n ## Usage Example\n \n #### Instruct model\n+\n ```python\n import torch\n from transformers import MllamaForConditionalGeneration, AutoProcessor\n@@ -83,6 +81,7 @@ print(processor.decode(output[0]))\n ```\n \n #### Base model\n+\n ```python\n import requests\n import torch\n@@ -102,7 +101,6 @@ output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n print(processor.decode(output[0], skip_special_tokens=True))\n ```\n \n-\n ## MllamaConfig\n \n [[autodoc]] MllamaConfig\n@@ -111,7 +109,6 @@ print(processor.decode(output[0], skip_special_tokens=True))\n \n [[autodoc]] MllamaProcessor\n \n-\n ## MllamaImageProcessor\n \n [[autodoc]] MllamaImageProcessor"
        },
        {
            "sha": "0d628c3b31deeb2f626cd56768f8f3c986ea275a",
            "filename": "docs/source/en/model_doc/mm-grounding-dino.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmm-grounding-dino.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmm-grounding-dino.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmm-grounding-dino.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -100,7 +100,6 @@ for box, score, labels in zip(result[\"boxes\"], result[\"scores\"], result[\"labels\"\n     |  [mm_grounding_dino_tiny_o365v1_goldg_v3det](https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det)           |   O365,GoldG,V3Det    |    33.0     |    36.0     |    45.9     | 40.5(+11.7) |    21.5    |    25.5    |    40.2    | 30.6(+10.5) |\n     |  [mm_grounding_dino_tiny_o365v1_goldg_grit_v3det](https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_grit_v3det) | O365,GoldG,GRIT,V3Det |    34.2     |    37.4     |    46.2     | 41.4(+12.6) |    23.6    |    27.6    |    40.5    | 31.9(+11.8) |\n \n-\n - This implementation also supports inference for [LLMDet](https://github.com/iSEE-Laboratory/LLMDet). Here's a table of LLMDet models and their performance on LVIS (results from [official repo](https://github.com/iSEE-Laboratory/LLMDet)):\n \n     |                             Model                         | Pre-Train Data            |  MiniVal APr | MiniVal APc | MiniVal APf | MiniVal AP  | Val1.0 APr | Val1.0 APc | Val1.0 APf |  Val1.0 AP  |\n@@ -109,7 +108,6 @@ for box, score, labels in zip(result[\"boxes\"], result[\"scores\"], result[\"labels\"\n     | [llmdet_base](https://huggingface.co/iSEE-Laboratory/llmdet_base)   | (O365,GoldG,V3Det) + GroundingCap-1M         | 48.3         | 40.8        | 43.1        | 54.3        | 38.5       | 28.2       | 34.3       | 47.8        |\n     | [llmdet_large](https://huggingface.co/iSEE-Laboratory/llmdet_large) | (O365V2,OpenImageV6,GoldG) + GroundingCap-1M | 51.1         | 45.1        | 46.1        | 56.6        | 42.0       | 31.6       | 38.8       | 50.2        |\n \n-\n ## MMGroundingDinoConfig\n \n [[autodoc]] MMGroundingDinoConfig"
        },
        {
            "sha": "171beaf440d13f7b1031f77b24724e7d20e0c06e",
            "filename": "docs/source/en/model_doc/mms.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmms.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmms.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmms.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -376,6 +376,7 @@ detected_lang = model.config.id2label[lang_id]\n ```\n \n To see all the supported languages of a checkpoint, you can print out the language ids as follows:\n+\n ```py\n processor.id2label.values()\n ```"
        },
        {
            "sha": "08486ace56ebafaf079c0718a55c5e6ed4d7edfd",
            "filename": "docs/source/en/model_doc/mobilebert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -15,7 +15,6 @@ rendered properly in your Markdown viewer.\n -->\n *This model was released on 2020-04-06 and added to Hugging Face Transformers on 2020-11-16.*\n \n-\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n@@ -47,6 +46,7 @@ pipeline = pipeline(\n )\n pipeline(\"The capital of France is [MASK].\")\n ```\n+\n </hfoption>\n <hfoption id=\"AutoModel\">\n \n@@ -85,7 +85,6 @@ echo -e \"The capital of France is [MASK].\" | transformers run --task fill-mask -\n </hfoption>\n </hfoptions>\n \n-\n ## Notes\n \n - Inputs should be padded on the right because BERT uses absolute position embeddings."
        },
        {
            "sha": "809be7f652a0b6089b7a8d0d96a39ffcc1f60cd7",
            "filename": "docs/source/en/model_doc/mobilenet_v1.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -32,7 +32,6 @@ You can all the original MobileNet checkpoints under the [Google](https://huggin\n \n The example below demonstrates how to classify an image with [`Pipeline`] or the [`AutoModel`] class.\n \n-\n <hfoptions id=\"usage\">\n <hfoption id=\"Pipeline\">\n \n@@ -84,18 +83,19 @@ print(f\"The predicted class label is: {predicted_class_label}\")\n <!-- Quantization - Not applicable -->\n <!-- Attention Visualization - Not applicable for this model type -->\n \n-\n ## Notes\n \n -   Checkpoint names follow the pattern `mobilenet_v1_{depth_multiplier}_{resolution}`, like `mobilenet_v1_1.0_224`. `1.0` is the depth multiplier and `224` is the image resolution.\n -   While trained on images of a specific sizes, the model architecture works with images of different sizes (minimum 32x32). The [`MobileNetV1ImageProcessor`] handles the necessary preprocessing.\n -   MobileNet is pretrained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k), a dataset with 1000 classes. However, the model actually predicts 1001 classes. The additional class is an extra \"background\" class (index 0).\n -   The original TensorFlow checkpoints determines the padding amount at inference because it depends on the input image size. To use the native PyTorch padding behavior, set `tf_padding=False` in [`MobileNetV1Config`].\n+\n     ```python\n     from transformers import MobileNetV1Config\n \n     config = MobileNetV1Config.from_pretrained(\"google/mobilenet_v1_1.0_224\", tf_padding=True)\n     ```\n+\n -   The Transformers implementation does not support the following features.\n     -   Uses global average pooling instead of the optional 7x7 average pooling with stride 2. For larger inputs, this gives a pooled output that is larger than a 1x1 pixel.\n     -   Does not support other `output_stride` values (fixed at 32). For smaller `output_strides`, the original implementation uses dilated convolution to prevent spatial resolution from being reduced further. (which would require dilated convolutions)."
        },
        {
            "sha": "2039f9e4413f6da43114fa0ac440cca1f48a7046",
            "filename": "docs/source/en/model_doc/mobilenet_v2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -30,10 +30,8 @@ You can all the original MobileNet checkpoints under the [Google](https://huggin\n > [!TIP]\n > Click on the MobileNet V2 models in the right sidebar for more examples of how to apply MobileNet to different vision tasks.\n \n-\n The examples below demonstrate how to classify an image with [`Pipeline`] or the [`AutoModel`] class.\n \n-\n <hfoptions id=\"usage-img-class\">\n <hfoption id=\"Pipeline\">\n \n@@ -82,19 +80,20 @@ print(f\"The predicted class label is: {predicted_class_label}\")\n </hfoption>\n </hfoptions>\n \n-\n ## Notes\n \n -   Classification checkpoint names follow the pattern `mobilenet_v2_{depth_multiplier}_{resolution}`, like `mobilenet_v2_1.4_224`. `1.4` is the depth multiplier and `224` is the image resolution. Segmentation checkpoint names follow the pattern `deeplabv3_mobilenet_v2_{depth_multiplier}_{resolution}`.\n -   While trained on images of a specific sizes, the model architecture works with images of different sizes (minimum 32x32). The [`MobileNetV2ImageProcessor`] handles the necessary preprocessing.\n -   MobileNet is pretrained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k), a dataset with 1000 classes. However, the model actually predicts 1001 classes. The additional class is an extra \"background\" class (index 0).\n -   The segmentation models use a [DeepLabV3+](https://huggingface.co/papers/1802.02611) head which is often pretrained on datasets like [PASCAL VOC](https://huggingface.co/datasets/merve/pascal-voc).\n -   The original TensorFlow checkpoints determines the padding amount at inference because it depends on the input image size. To use the native PyTorch padding behavior, set `tf_padding=False` in [`MobileNetV2Config`].\n+\n     ```python\n     from transformers import MobileNetV2Config\n \n     config = MobileNetV2Config.from_pretrained(\"google/mobilenet_v2_1.4_224\", tf_padding=True)\n     ```\n+\n -   The Transformers implementation does not support the following features.\n     -   Uses global average pooling instead of the optional 7x7 average pooling with stride 2. For larger inputs, this gives a pooled output that is larger than a 1x1 pixel.\n     -   `output_hidden_states=True` returns *all* intermediate hidden states. It is not possible to extract the output from specific layers for other downstream purposes."
        },
        {
            "sha": "9975cf68155ac065fe0a561904a90fbb5a5a418b",
            "filename": "docs/source/en/model_doc/mobilevit.md",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -11,11 +11,8 @@ Unless required by applicable law or agreed to in writing, software distributed\n -->\n *This model was released on 2021-10-05 and added to Hugging Face Transformers on 2022-06-29.*\n \n-\n-\n # MobileViT\n \n-\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-2\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n@@ -24,21 +21,17 @@ Unless required by applicable law or agreed to in writing, software distributed\n \n [MobileViT](https://huggingface.co/papers/2110.02178) is a lightweight vision transformer for mobile devices that merges CNNs's efficiency and inductive biases with transformers global context modeling. It treats transformers as convolutions, enabling global information processing without the heavy computational cost of standard ViTs.\n \n-\n <div class=\"flex justify-center\">\n    <img src = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/MobileViT.png\">\n </div>\n \n-\n You can find all the original MobileViT checkpoints under the [Apple](https://huggingface.co/apple/models?search=mobilevit) organization.\n \n-\n > [!TIP]\n > - This model was contributed by [matthijs](https://huggingface.co/Matthijs).\n >\n > Click on the MobileViT models in the right sidebar for more examples of how to apply MobileViT to different vision tasks.\n \n-\n The example below demonstrates how to do [Image Classification] with [`Pipeline`] and the [`AutoModel`] class.\n \n <hfoptions id=\"usage\">\n@@ -92,7 +85,6 @@ print(f\"The predicted class label is:{predicted_class_label}\")\n </hfoption>\n </hfoptions>\n \n-\n ## Notes\n \n - Does **not** operate on sequential data, it's purely designed for image tasks.\n@@ -102,8 +94,6 @@ print(f\"The predicted class label is:{predicted_class_label}\")\n - The classification models are pretrained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k).\n - The segmentation models use a [DeepLabV3](https://huggingface.co/papers/1706.05587) head and are pretrained on [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).\n \n-\n-\n ## MobileViTConfig\n \n [[autodoc]] MobileViTConfig"
        },
        {
            "sha": "ff61362a520335738a4af5aec46ef51b6df1bf6d",
            "filename": "docs/source/en/model_doc/modernbert-decoder.md",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert-decoder.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert-decoder.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert-decoder.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -36,7 +36,7 @@ You can find all the original ModernBERT Decoder checkpoints under the [jhu-clsp\n >\n > Click on the ModernBERT Decoder models in the right sidebar for more examples of how to apply ModernBERT Decoder to different text generation tasks.\n \n-The example below demonstrates how to use ModernBERT Decoder for text generation with [`Pipeline`], [`AutoModel`] (with and without quantization), and from the command line. \n+The example below demonstrates how to use ModernBERT Decoder for text generation with [`Pipeline`], [`AutoModel`] (with and without quantization), and from the command line.\n \n <hfoptions id=\"usage\">\n <hfoption id=\"Pipeline\">\n@@ -151,6 +151,7 @@ with torch.no_grad():\n generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n print(f\"Generated text: {generated_text}\")\n ```\n+\n </hfoption>\n \n <hfoption id=\"transformers CLI\">\n@@ -162,12 +163,10 @@ echo \"The future of artificial intelligence is\" | transformers run --task text-g\n </hfoption>\n </hfoptions>\n \n-\n ## ModernBertDecoderConfig\n \n [[autodoc]] ModernBertDecoderConfig\n \n-\n ## ModernBertDecoderModel\n \n [[autodoc]] ModernBertDecoderModel\n@@ -182,4 +181,3 @@ echo \"The future of artificial intelligence is\" | transformers run --task text-g\n \n [[autodoc]] ModernBertDecoderForSequenceClassification\n     - forward\n-"
        },
        {
            "sha": "4be8d97f5e954843c958caf9cd7f2cacba06a7b3",
            "filename": "docs/source/en/model_doc/modernbert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -93,7 +93,6 @@ echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | tran\n \n [[autodoc]] ModernBertConfig\n \n-\n ## ModernBertModel\n \n [[autodoc]] ModernBertModel\n@@ -127,5 +126,3 @@ echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | tran\n ### Usage tips\n \n The ModernBert model can be fine-tuned using the HuggingFace Transformers library with its [official script](https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py) for question-answering tasks.\n-\n-"
        },
        {
            "sha": "b85a174a86fbc873ccbc1c97003ed5ea9eeefb91",
            "filename": "docs/source/en/model_doc/moonshine.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoonshine.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoonshine.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoonshine.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -83,6 +83,7 @@ predicted_ids = model.generate(**input_features, cache_implementation=\"static\")\n transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n transcription[0]\n ```\n+\n </hfoption>\n </hfoptions>\n \n@@ -101,4 +102,3 @@ transcription[0]\n [[autodoc]] MoonshineForConditionalGeneration\n     - forward\n     - generate\n-"
        },
        {
            "sha": "49fae1c539d77ea455d5e1e11c11dbfd33625006",
            "filename": "docs/source/en/model_doc/moshi.md",
            "status": "modified",
            "additions": 4,
            "deletions": 12,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoshi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoshi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoshi.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -35,7 +35,7 @@ Moshi is a speech-text foundation model that casts spoken dialogue as speech-to-\n \n The abstract from the paper is the following:\n \n-*We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning— such as emotion or non-speech sounds— is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this “Inner Monologue” method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at github.com/kyutai-labs/moshi.* \n+*We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning— such as emotion or non-speech sounds— is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this “Inner Monologue” method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at github.com/kyutai-labs/moshi.*\n \n Moshi deals with 3 streams of information:\n 1. The user's audio\n@@ -63,11 +63,9 @@ Note that each timestamp - i.e each codebook - gets its own set of Linear Layers\n \n It's the audio encoder from Kyutai, that has recently been integrated to transformers, which is used to \"tokenize\" audio. It has the same use that [`~EncodecModel`] has in [`~MusicgenModel`].\n \n-\n ## Tips:\n \n-The original checkpoints can be converted using the conversion script `src/transformers/models/moshi/convert_moshi_transformers.py` \n-\n+The original checkpoints can be converted using the conversion script `src/transformers/models/moshi/convert_moshi_transformers.py`\n \n ### How to use the model:\n \n@@ -108,12 +106,9 @@ To follow the example of the following image, `\"Hello, I'm Moshi\"` could be tran\n <img src=\"https://huggingface.co/datasets/ylacombe/benchmark-comparison/resolve/main/moshi_text_sync.png\">\n </div>\n \n-\n [`MoshiForConditionalGeneration.generate`] then auto-regressively feeds to itself its own audio stream, but since it doesn't have access to the user input stream while using `transformers`, it will thus **assume that the user is producing blank audio**.\n \n-\n-\n-```python \n+```python\n >>> from datasets import load_dataset, Audio\n >>> import torch, math\n >>> from transformers import MoshiForConditionalGeneration, AutoFeatureExtractor, AutoTokenizer, infer_device\n@@ -149,7 +144,7 @@ To follow the example of the following image, `\"Hello, I'm Moshi\"` could be tran\n Most of the work has to be done during data creation/pre-processing, because of the need to align/synchronize streams.\n \n Once it's done, you can simply forward `text_labels` and `audio_labels` to [`MoshiForConditionalGeneration.forward`], alongside the usual inputs, to get the model loss.\n- \n+\n A training guide will come soon, but user contributions are welcomed!\n \n ### How does the model forward the inputs / generate:\n@@ -162,13 +157,10 @@ A training guide will come soon, but user contributions are welcomed!\n \n 3. The depth decoder switches the dimension on which we forward / generate (codebooks instead of time). It uses the token generated from `text logits`  and the `temporal context` to auto-regressively generate audio codebooks.\n \n-\n This model was contributed by [Yoach Lacombe (ylacombe)](https://huggingface.co/ylacombe).\n \n The original code can be found [here](https://github.com/kyutai-labs/moshi).\n \n-\n-\n ## MoshiConfig\n \n [[autodoc]] MoshiConfig"
        },
        {
            "sha": "60d14641177c1f76bd0769a6da1654f9fe25808f",
            "filename": "docs/source/en/model_doc/mpt.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpt.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -23,11 +23,11 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The MPT model was proposed by the [MosaicML](https://www.mosaicml.com/) team and released with multiple sizes and finetuned variants. The MPT models are a series of open source and commercially usable LLMs pre-trained on 1T tokens. \n+The MPT model was proposed by the [MosaicML](https://www.mosaicml.com/) team and released with multiple sizes and finetuned variants. The MPT models are a series of open source and commercially usable LLMs pre-trained on 1T tokens.\n \n-MPT models are GPT-style decoder-only transformers with several improvements: performance-optimized layer implementations, architecture changes that provide greater training stability, and the elimination of context length limits by replacing positional embeddings with ALiBi. \n+MPT models are GPT-style decoder-only transformers with several improvements: performance-optimized layer implementations, architecture changes that provide greater training stability, and the elimination of context length limits by replacing positional embeddings with ALiBi.\n \n-- MPT base: MPT base pre-trained models on next token prediction \n+- MPT base: MPT base pre-trained models on next token prediction\n - MPT instruct: MPT base models fine-tuned on instruction based tasks\n - MPT storywriter: MPT base models fine-tuned for 2500 steps on 65k-token excerpts of fiction books contained in the books3 corpus, this enables the model to handle very long sequences\n "
        },
        {
            "sha": "4e652458e1b386495c28a4e6502ccb8a67c973c1",
            "filename": "docs/source/en/model_doc/mt5.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmt5.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -133,7 +133,6 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n See [`T5Tokenizer`] for all details.\n \n-\n ## MT5TokenizerFast\n \n [[autodoc]] MT5TokenizerFast"
        },
        {
            "sha": "0ec3cb200d1e906286db5b4bd5a303d5b960dec2",
            "filename": "docs/source/en/model_doc/musicgen.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -77,9 +77,9 @@ Generation is limited by the sinusoidal positional embeddings to 30 second input\n than 30 seconds of audio (1503 tokens), and input audio passed by Audio-Prompted Generation contributes to this limit so,\n given an input of 20 seconds of audio, MusicGen cannot generate more than 10 seconds of additional audio.\n \n-Transformers supports both mono (1-channel) and stereo (2-channel) variants of MusicGen. The mono channel versions \n-generate a single set of codebooks. The stereo versions generate 2 sets of codebooks, 1 for each channel (left/right), \n-and each set of codebooks is decoded independently through the audio compression model. The audio streams for each \n+Transformers supports both mono (1-channel) and stereo (2-channel) variants of MusicGen. The mono channel versions\n+generate a single set of codebooks. The stereo versions generate 2 sets of codebooks, 1 for each channel (left/right),\n+and each set of codebooks is decoded independently through the audio compression model. The audio streams for each\n channel are combined to give the final stereo output.\n \n ### Unconditional Generation\n@@ -208,7 +208,7 @@ For batched audio-prompted generation, the generated `audio_values` can be post-\n \n ### Generation Configuration\n \n-The default parameters that control the generation process, such as sampling, guidance scale and number of generated \n+The default parameters that control the generation process, such as sampling, guidance scale and number of generated\n tokens, can be found in the model's generation config, and updated as desired:\n \n ```python\n@@ -226,8 +226,8 @@ tokens, can be found in the model's generation config, and updated as desired:\n >>> model.generation_config.max_length = 256\n ```\n \n-Note that any arguments passed to the generate method will **supersede** those in the generation config, so setting \n-`do_sample=False` in the call to generate will supersede the setting of `model.generation_config.do_sample` in the \n+Note that any arguments passed to the generate method will **supersede** those in the generation config, so setting\n+`do_sample=False` in the call to generate will supersede the setting of `model.generation_config.do_sample` in the\n generation config.\n \n ## Model Structure\n@@ -239,7 +239,7 @@ The MusicGen model can be de-composed into three distinct stages:\n \n Thus, the MusicGen model can either be used as a standalone decoder model, corresponding to the class [`MusicgenForCausalLM`],\n or as a composite model that includes the text encoder and audio encoder/decoder, corresponding to the class\n-[`MusicgenForConditionalGeneration`]. If only the decoder needs to be loaded from the pre-trained checkpoint, it can be loaded by first \n+[`MusicgenForConditionalGeneration`]. If only the decoder needs to be loaded from the pre-trained checkpoint, it can be loaded by first\n specifying the correct config, or be accessed through the `.decoder` attribute of the composite model:\n \n ```python"
        },
        {
            "sha": "f43bfee433484270a530090196bb7eb334d4c367",
            "filename": "docs/source/en/model_doc/musicgen_melody.md",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -35,10 +35,8 @@ The abstract from the paper is the following:\n \n *We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen.*\n \n-\n This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The original code can be found [here](https://github.com/facebookresearch/audiocraft). The pre-trained checkpoints can be found on the [Hugging Face Hub](https://huggingface.co/models?sort=downloads&search=facebook%2Fmusicgen).\n \n-\n ## Difference with [MusicGen](https://huggingface.co/docs/transformers/main/en/model_doc/musicgen)\n \n There are two key differences with MusicGen:\n@@ -54,7 +52,6 @@ MusicGen Melody is compatible with two generation modes: greedy and sampling. In\n \n Transformers supports both mono (1-channel) and stereo (2-channel) variants of MusicGen Melody. The mono channel versions generate a single set of codebooks. The stereo versions generate 2 sets of codebooks, 1 for each channel (left/right), and each set of codebooks is decoded independently through the audio compression model. The audio streams for each channel are combined to give the final stereo output.\n \n-\n #### Audio Conditional Generation\n \n The model can generate an audio sample conditioned on a text and an audio prompt through use of the [`MusicgenMelodyProcessor`] to pre-process the inputs.\n@@ -67,6 +64,7 @@ pip install datasets[audio]\n ```\n \n The audio file we are about to use is loaded as follows:\n+\n ```python\n >>> from datasets import load_dataset\n \n@@ -147,10 +145,9 @@ Or save them as a `.wav` file using a third-party library, e.g. `soundfile`:\n >>> sf.write(\"musicgen_out.wav\", audio_values[0].T.numpy(), sampling_rate)\n ```\n \n-\n ### Text-only Conditional Generation\n \n-The same [`MusicgenMelodyProcessor`] can be used to pre-process a text-only prompt. \n+The same [`MusicgenMelodyProcessor`] can be used to pre-process a text-only prompt.\n \n ```python\n >>> from transformers import AutoProcessor, MusicgenMelodyForConditionalGeneration\n@@ -168,7 +165,6 @@ The same [`MusicgenMelodyProcessor`] can be used to pre-process a text-only prom\n \n The `guidance_scale` is used in classifier free guidance (CFG), setting the weighting between the conditional logits (which are predicted from the text prompts) and the unconditional logits (which are predicted from an unconditional or 'null' prompt). Higher guidance scale encourages the model to generate samples that are more closely linked to the input prompt, usually at the expense of poorer audio quality. CFG is enabled by setting `guidance_scale > 1`. For best results, use `guidance_scale=3` (default).\n \n-\n You can also generate in batch:\n \n ```python\n@@ -263,7 +259,6 @@ Tips:\n * MusicGen is trained on the 32kHz checkpoint of Encodec. You should ensure you use a compatible version of the Encodec model.\n * Sampling mode tends to deliver better results than greedy - you can toggle sampling with the variable `do_sample` in the call to [`MusicgenMelodyForConditionalGeneration.generate`]\n \n-\n ## MusicgenMelodyDecoderConfig\n \n [[autodoc]] MusicgenMelodyDecoderConfig"
        },
        {
            "sha": "26aa2f29b76df9df931bb06a955d6e0308d2754c",
            "filename": "docs/source/en/model_doc/mvp.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmvp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmvp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmvp.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -25,7 +25,6 @@ rendered properly in your Markdown viewer.\n \n The MVP model was proposed in [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://huggingface.co/papers/2206.12131) by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\n \n-\n According to the abstract,\n \n - MVP follows a standard Transformer encoder-decoder architecture.\n@@ -67,6 +66,7 @@ For summarization, it is an example to use MVP and MVP with summarization-specif\n ```\n \n For data-to-text generation, it is an example to use MVP and multi-task pre-trained variants.\n+\n ```python\n >>> from transformers import MvpTokenizerFast, MvpForConditionalGeneration\n "
        },
        {
            "sha": "35ab716a8e71d13f4770da4e2b414e72c600f353",
            "filename": "docs/source/en/model_doc/myt5.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmyt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmyt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmyt5.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -44,4 +44,3 @@ The original code can be found [here](https://github.com/tomlimi/MYTE).\n ## MyT5Tokenizer\n \n [[autodoc]] MyT5Tokenizer\n-"
        },
        {
            "sha": "0a2104c5855230a307c25bd1ff7da72de59e5247",
            "filename": "docs/source/en/model_doc/nemotron.md",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -97,7 +97,6 @@ Minitron is released under the [NVIDIA Open Model License Agreement](https://dev\n | :------------- | :------------- | :------------- | :------------- | :------------- |\n | 75.0 | 74.0 | 24.1  | 50.9 | 29.5\n \n-\n *Code generation performance*. Evaluated using [HumanEval](https://github.com/openai/human-eval):\n \n | p@1, 0-Shot |\n@@ -109,6 +108,7 @@ Please refer to our [paper](https://huggingface.co/papers/2407.14679) for the fu\n ### Citation\n \n If you find our work helpful, please consider citing our paper:\n+\n ```\n @article{minitron2024,\n       title={Compact Language Models via Pruning and Knowledge Distillation},\n@@ -123,13 +123,11 @@ If you find our work helpful, please consider citing our paper:\n \n [[autodoc]] NemotronConfig\n \n-\n ## NemotronModel\n \n [[autodoc]] NemotronModel\n     - forward\n \n-\n ## NemotronForCausalLM\n \n [[autodoc]] NemotronForCausalLM\n@@ -140,13 +138,11 @@ If you find our work helpful, please consider citing our paper:\n [[autodoc]] NemotronForSequenceClassification\n     - forward\n \n-\n ## NemotronForQuestionAnswering\n \n [[autodoc]] NemotronForQuestionAnswering\n     - forward\n \n-\n ## NemotronForTokenClassification\n \n [[autodoc]] NemotronForTokenClassification"
        },
        {
            "sha": "d8c44a5fc0f805c12dd352fe7935b94552a77514",
            "filename": "docs/source/en/model_doc/nllb-moe.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb-moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb-moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb-moe.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -110,7 +110,6 @@ See example below for a translation from romanian to german:\n - [Translation task guide](../tasks/translation)\n - [Summarization task guide](../tasks/summarization)\n \n-\n ## NllbMoeConfig\n \n [[autodoc]] NllbMoeConfig\n@@ -135,4 +134,3 @@ See example below for a translation from romanian to german:\n \n [[autodoc]] NllbMoeForConditionalGeneration\n     - forward\n-"
        },
        {
            "sha": "77fffafde673194ffb1a5828e4f6dc1aae65911b",
            "filename": "docs/source/en/model_doc/nllb.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -29,7 +29,6 @@ rendered properly in your Markdown viewer.\n \n [NLLB: No Language Left Behind](https://huggingface.co/papers/2207.04672) is a multilingual translation model. It's trained on data using data mining techniques tailored for low-resource languages and supports over 200 languages. NLLB features a conditional compute architecture using a Sparsely Gated Mixture of Experts.\n \n-\n You can find all the original NLLB checkpoints under the [AI at Meta](https://huggingface.co/facebook/models?search=nllb) organization.\n \n > [!TIP]\n@@ -132,6 +131,7 @@ visualizer(\"UN Chief says there is no military solution in Syria\")\n  - For non-English languages, specify the language's [BCP-47](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200) code with the `src_lang` keyword as shown below.\n \n  - See example below for a translation from Romanian to German.\n+\n     ```python\n     >>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n "
        },
        {
            "sha": "7ecaa0e98fa8e9e625584583c7416862fd06f185",
            "filename": "docs/source/en/model_doc/olmo2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -87,6 +87,7 @@ echo -e \"Plants create energy through a process known as\" | transformers run --t\n Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n The example below uses [torchao](../quantization/torchao) to only quantize the weights to 4-bits.\n+\n ```py\n \n #pip install torchao\n@@ -116,7 +117,6 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n ```\n \n-\n ## Notes\n \n - OLMo2 uses RMSNorm instead of standard layer norm. The RMSNorm is applied to attention queries and keys, and it is applied after the attention and feedforward layers rather than before.\n@@ -129,7 +129,6 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n     model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-2-0425-1B\", revision=\"stage1-step140000-tokens294B\")\n     ```\n \n-\n ## Olmo2Config\n \n [[autodoc]] Olmo2Config"
        },
        {
            "sha": "57f3309e748069a910ec836239e90213c8b374cb",
            "filename": "docs/source/en/model_doc/olmo3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo3.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -12,7 +12,6 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n \n-\n ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n \n -->\n@@ -88,6 +87,7 @@ echo -e \"Plants create energy through a process known as\" | transformers run --t\n Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n The example below uses [torchao](../quantization/torchao) to only quantize the weights to 4-bits.\n+\n ```py\n \n #pip install torchao\n@@ -117,7 +117,6 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n ```\n \n-\n ## Notes\n \n - Load specific intermediate checkpoints by adding the `revision` parameter to [`~PreTrainedModel.from_pretrained`].\n@@ -128,7 +127,6 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n     model = AutoModelForCausalLM.from_pretrained(\"allenai/TBA\", revision=\"stage1-step140000-tokens294B\")\n     ```\n \n-\n ## Olmo3Config\n \n [[autodoc]] Olmo3Config"
        },
        {
            "sha": "fba08ceca00efff7ff5069f35604079ae6827659",
            "filename": "docs/source/en/model_doc/openai-gpt.md",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -15,7 +15,6 @@ rendered properly in your Markdown viewer.\n -->\n *This model was released on 2018-06-11 and added to Hugging Face Transformers on 2023-06-20.*\n \n-\n <div style=\"float: right;\">\n   <div class=\"flex flex-wrap space-x-1\">\n     <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n@@ -24,8 +23,6 @@ rendered properly in your Markdown viewer.\n   </div>\n </div>\n \n-\n-\n # GPT\n \n [GPT (Generative Pre-trained Transformer)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) ([blog post](https://openai.com/index/language-unsupervised/)) focuses on effectively learning text representations and transferring them to tasks. This model trains the Transformer decoder to predict the next word, and then fine-tuned on labeled data.\n@@ -39,12 +36,9 @@ You can find all the original GPT checkpoints under the [OpenAI community](https\n \n The example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`], and from the command line.\n \n-\n-\n <hfoptions id=\"usage\">\n <hfoption id=\"Pipeline\">\n \n-\n ```python\n import torch\n from transformers import pipeline\n@@ -75,6 +69,7 @@ print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n echo -e \"The future of AI is\" | transformers run --task text-generation --model openai-community/openai-gpt --device 0\n \n ```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "7c65689594e4d027894424a35eb4ea88ed86c29f",
            "filename": "docs/source/en/model_doc/opt.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -36,7 +36,6 @@ You can find all the original OPT checkpoints under the [OPT](https://huggingfac\n \n The example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`], and from the command line.\n \n-\n <hfoptions id=\"usage\">\n <hfoption id=\"Pipeline\">\n \n@@ -65,12 +64,14 @@ model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n generated_ids = model.generate(**model_inputs, max_new_tokens=30, do_sample=False)\n tokenizer.batch_decode(generated_ids)[0]\n ```\n+\n </hfoption>\n <hfoption id=\"transformers CLI\">\n \n ```py\n echo -e \"Plants create energy through a process known as\" | transformers run --task text-generation --model facebook/opt-125m --device 0\n ```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "731ebbb83f08e436083204daed71ab720da0494e",
            "filename": "docs/source/en/model_doc/ovis2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -19,7 +19,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The [Ovis2](https://github.com/AIDC-AI/Ovis) is an updated version of the [Ovis](https://huggingface.co/papers/2405.20797) model developed by the AIDC-AI team at Alibaba International Digital Commerce Group. \n+The [Ovis2](https://github.com/AIDC-AI/Ovis) is an updated version of the [Ovis](https://huggingface.co/papers/2405.20797) model developed by the AIDC-AI team at Alibaba International Digital Commerce Group.\n \n Ovis2 is the latest advancement in multi-modal large language models (MLLMs), succeeding Ovis1.6. It retains the architectural design of the Ovis series, which focuses on aligning visual and textual embeddings, and introduces major improvements in data curation and training methods.\n "
        },
        {
            "sha": "fa7c193da4532c60b7735cd123d19cb42da1d833",
            "filename": "docs/source/en/model_doc/paligemma.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -140,6 +140,7 @@ visualizer(\"<img> What is in this image?\")\n     answer = \"a pallas cat\"\n     inputs = processor(images=image, text=prompt, suffix=answer, return_tensors=\"pt\")\n     ```\n+\n - PaliGemma can support multiple input images if it is fine-tuned to accept multiple images. For example, the [NLVR2](https://huggingface.co/google/paligemma-3b-ft-nlvr2-448) checkpoint supports multiple images. Pass the images as a list to the processor.\n \n     ```py"
        },
        {
            "sha": "23ebb89b6ade2ffe54677abbd3a3cd7cb50731d4",
            "filename": "docs/source/en/model_doc/patchtsmixer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtsmixer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtsmixer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtsmixer.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -25,15 +25,13 @@ rendered properly in your Markdown viewer.\n \n The PatchTSMixer model was proposed in [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://huggingface.co/papers/2306.09364) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.\n \n-\n PatchTSMixer is a lightweight time-series modeling approach based on the MLP-Mixer architecture. In this HuggingFace implementation, we provide PatchTSMixer's capabilities to effortlessly facilitate lightweight mixing across patches, channels, and hidden features for effective multivariate time-series modeling. It also supports various attention mechanisms starting from simple gated attention to more complex self-attention blocks that can be customized accordingly. The model can be pretrained and subsequently used for various downstream tasks such as forecasting, classification and regression.\n \n-\n The abstract from the paper is the following:\n \n *TSMixer is a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules designed for multivariate forecasting and representation learning on patched time series. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approach to effectively handle noisy channel interactions and generalization across diverse datasets, a common challenge in existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2%) with a significant reduction in memory and runtime (2-3X).*\n \n-This model was contributed by [ajati](https://huggingface.co/ajati), [vijaye12](https://huggingface.co/vijaye12), \n+This model was contributed by [ajati](https://huggingface.co/ajati), [vijaye12](https://huggingface.co/vijaye12),\n [gsinthong](https://huggingface.co/gsinthong), [namctin](https://huggingface.co/namctin),\n [wmgifford](https://huggingface.co/wmgifford), [kashif](https://huggingface.co/kashif).\n \n@@ -68,31 +66,26 @@ The model can also be used for time series classification and time series regres\n \n [[autodoc]] PatchTSMixerConfig\n \n-\n ## PatchTSMixerModel\n \n [[autodoc]] PatchTSMixerModel\n     - forward\n \n-\n ## PatchTSMixerForPrediction\n \n [[autodoc]] PatchTSMixerForPrediction\n     - forward\n \n-\n ## PatchTSMixerForTimeSeriesClassification\n \n [[autodoc]] PatchTSMixerForTimeSeriesClassification\n     - forward\n \n-\n ## PatchTSMixerForPretraining\n \n [[autodoc]] PatchTSMixerForPretraining\n     - forward\n \n-\n ## PatchTSMixerForRegression\n \n [[autodoc]] PatchTSMixerForRegression"
        },
        {
            "sha": "783581ad96dcba997bb653e79f635f740e38bc4a",
            "filename": "docs/source/en/model_doc/pegasus_x.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -53,6 +53,7 @@ Through photosynthesis, plants capture energy from sunlight using a green pigmen\n These ingredients are then transformed into glucose, a type of sugar that serves as a source of chemical energy, and oxygen, which is released as a byproduct into the atmosphere. The glucose produced during photosynthesis is not just used immediately; plants also store it as starch or convert it into other organic compounds like cellulose, which is essential for building their cellular structure.\n This energy reserve allows them to grow, develop leaves, produce flowers, bear fruit, and carry out various physiological processes throughout their lifecycle.\"\"\")\n ```\n+\n </hfoption>\n <hfoption id=\"AutoModel\">\n \n@@ -78,12 +79,14 @@ input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n output = model.generate(**input_ids, cache_implementation=\"static\")\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n+\n </hfoption>\n <hfoption id=\"transformers\">\n \n ```bash\n echo -e \"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet. Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts.\" | transformers run --task summarization --model google/pegasus-x-large --device 0\n ```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "7d3d608253fc12af847504be2da73ab27ec123a6",
            "filename": "docs/source/en/model_doc/perception_lm.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fperception_lm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fperception_lm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fperception_lm.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -38,11 +38,9 @@ video captions. Additionally, we introduce PLM–VideoBench, a suite for evaluat\n understanding tasks focusing on the ability to reason about “what”, “where”, “when”, and “how” of a\n video. We make our work fully reproducible by providing data, training recipes, code & models.*\n \n-\n This model was contributed by [shumingh](https://huggingface.co/shumingh).\n The original code can be found [here](https://github.com/facebookresearch/perception_models).\n \n-\n ## PerceptionLMConfig\n \n [[autodoc]] PerceptionLMConfig"
        },
        {
            "sha": "854eaee835dfa482c6e0b00f3bb710a7c58948a9",
            "filename": "docs/source/en/model_doc/persimmon.md",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpersimmon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpersimmon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpersimmon.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -39,15 +39,14 @@ The original code can be found [here](https://github.com/persimmon-ai-labs/adept\n <Tip warning={true}>\n \n The `Persimmon` models were trained using `bfloat16`, but the original inference uses `float16` The checkpoints uploaded on the hub use `dtype = 'float16'` which will be\n-used by the `AutoModel` API to cast the checkpoints from `torch.float32` to `torch.float16`. \n+used by the `AutoModel` API to cast the checkpoints from `torch.float32` to `torch.float16`.\n \n The `dtype` of the online weights is mostly irrelevant, unless you are using `dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online) then it will be cast to the default `dtype` of `torch` (becomes `torch.float32`). Users should specify the `dtype` they want, and if they don't it will be `torch.float32`.\n \n Finetuning the model in `float16` is not recommended and known to produce `nan`, as such the model should be fine-tuned in `bfloat16`.\n \n </Tip>\n \n-\n Tips:\n \n - To convert the model, you need to clone the original repository using `git clone https://github.com/persimmon-ai-labs/adept-inference`, then get the checkpoints:\n@@ -62,6 +61,7 @@ python src/transformers/models/persimmon/convert_persimmon_weights_to_hf.py  --i\n ```\n \n For the chat model:\n+\n ```bash\n wget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_chat_model_release.tar\n tar -xvf 8b_base_model_release.tar\n@@ -76,13 +76,11 @@ model = PersimmonForCausalLM.from_pretrained(\"/output/path\")\n tokenizer = PersimmonTokenizer.from_pretrained(\"/output/path\")\n ```\n \n-\n - Perismmon uses a `sentencepiece` based tokenizer, with a `Unigram` model. It supports bytefallback, which is only available in `tokenizers==0.14.0` for the fast tokenizer.\n The `LlamaTokenizer` is used as it is a standard wrapper around sentencepiece. The `chat` template will be updated with the templating functions in a follow up PR!\n \n - The authors suggest to use the following prompt format for the chat mode: `f\"human: {prompt}\\n\\nadept:\"`\n \n-\n ## PersimmonConfig\n \n [[autodoc]] PersimmonConfig"
        },
        {
            "sha": "9a045e6f184ded919428e93c75d7e6fe9e894b47",
            "filename": "docs/source/en/model_doc/phi3.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi3.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -72,7 +72,6 @@ Phi-3 has been integrated in the development version (4.40.0.dev) of `transforme\n \n [[autodoc]] Phi3Config\n \n-\n ## Phi3Model\n \n [[autodoc]] Phi3Model\n@@ -93,4 +92,3 @@ Phi-3 has been integrated in the development version (4.40.0.dev) of `transforme\n \n [[autodoc]] Phi3ForTokenClassification\n     - forward\n-"
        },
        {
            "sha": "3d414d7c43b1484a4777bd73f762553c822e9ecc",
            "filename": "docs/source/en/model_doc/phimoe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -50,6 +50,7 @@ Phi-3.5-MoE-instruct has been integrated in the development version (4.44.2.dev)\n The current `transformers` version can be verified with: `pip list | grep transformers`.\n \n Examples of required packages:\n+\n ```\n flash_attn==2.5.8\n torch==2.3.1\n@@ -101,7 +102,6 @@ print(output[0]['generated_text'])\n \n [[autodoc]] PhimoeConfig\n \n-\n ## PhimoeModel\n \n [[autodoc]] PhimoeModel\n@@ -117,4 +117,3 @@ print(output[0]['generated_text'])\n \n [[autodoc]] PhimoeForSequenceClassification\n     - forward\n-"
        },
        {
            "sha": "bb175973bd239ae45ee4c570aba5475d7150309f",
            "filename": "docs/source/en/model_doc/pixtral.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -15,7 +15,6 @@ rendered properly in your Markdown viewer.\n -->\n *This model was released on 2024-09-17 and added to Hugging Face Transformers on 2024-09-14.*\n \n-\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">"
        },
        {
            "sha": "90e0cd3f0633a1598d0ae12a0afb3876e6cfdf11",
            "filename": "docs/source/en/model_doc/pop2piano.md",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpop2piano.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpop2piano.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpop2piano.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -21,14 +21,14 @@ specific language governing permissions and limitations under the License.\n \n The Pop2Piano model was proposed in [Pop2Piano : Pop Audio-based Piano Cover Generation](https://huggingface.co/papers/2211.00895) by Jongho Choi and Kyogu Lee.\n \n-Piano covers of pop music are widely enjoyed, but generating them from music is not a trivial task. It requires great \n-expertise with playing piano as well as knowing different characteristics and melodies of a song. With Pop2Piano you \n-can directly generate a cover from a song's audio waveform. It is the first model to directly generate a piano cover \n-from pop audio without melody and chord extraction modules. \n-\n-Pop2Piano is an encoder-decoder Transformer model based on [T5](https://huggingface.co/papers/1910.10683). The input audio \n-is transformed to its waveform and passed to the encoder, which transforms it to a latent representation. The decoder \n-uses these latent representations to generate token ids in an autoregressive way. Each token id corresponds to one of four \n+Piano covers of pop music are widely enjoyed, but generating them from music is not a trivial task. It requires great\n+expertise with playing piano as well as knowing different characteristics and melodies of a song. With Pop2Piano you\n+can directly generate a cover from a song's audio waveform. It is the first model to directly generate a piano cover\n+from pop audio without melody and chord extraction modules.\n+\n+Pop2Piano is an encoder-decoder Transformer model based on [T5](https://huggingface.co/papers/1910.10683). The input audio\n+is transformed to its waveform and passed to the encoder, which transforms it to a latent representation. The decoder\n+uses these latent representations to generate token ids in an autoregressive way. Each token id corresponds to one of four\n different token types: time, velocity, note and 'special'. The token ids are then decoded to their equivalent MIDI file.\n \n The abstract from the paper is the following:\n@@ -53,9 +53,11 @@ The original code can be found [here](https://github.com/sweetcocoa/pop2piano).\n ## Usage tips\n \n * To use Pop2Piano, you will need to install the 🤗 Transformers library, as well as the following third party modules:  \n+\n ```bash\n pip install pretty-midi==0.2.9 essentia==2.1b6.dev1034 librosa scipy\n ```\n+\n Please note that you may need to restart your runtime after installation.\n * Pop2Piano is an Encoder-Decoder based model like T5.\n * Pop2Piano can be used to generate midi-audio files for a given audio sequence.\n@@ -131,7 +133,6 @@ Please note that you may need to restart your runtime after installation.\n >>> tokenizer_output[1].write(\"./Outputs/midi_output2.mid\")\n ```\n \n-\n - Example of processing multiple audio files in batch (Using `Pop2PianoFeatureExtractor` and `Pop2PianoTokenizer`):\n \n ```python\n@@ -166,7 +167,6 @@ Please note that you may need to restart your runtime after installation.\n >>> tokenizer_output[1].write(\"./Outputs/midi_output2.mid\")\n ```\n \n-\n ## Pop2PianoConfig\n \n [[autodoc]] Pop2PianoConfig"
        },
        {
            "sha": "0ac26609b4d029d37a30d04356b8800d9b12139b",
            "filename": "docs/source/en/model_doc/prompt_depth_anything.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fprompt_depth_anything.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fprompt_depth_anything.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fprompt_depth_anything.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -19,8 +19,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Prompt Depth Anything model was introduced in [Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation](https://huggingface.co/papers/2412.14015) by Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, Bingyi Kang. \n-\n+The Prompt Depth Anything model was introduced in [Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation](https://huggingface.co/papers/2412.14015) by Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, Bingyi Kang.\n \n The abstract from the paper is as follows:\n "
        },
        {
            "sha": "38858db55529f1b576bf0038a5fc8a7052168d63",
            "filename": "docs/source/en/model_doc/pvt.md",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -29,23 +29,22 @@ is used to further reduce the resource consumption when learning high-resolution\n \n The abstract from the paper is the following:\n \n-*Although convolutional neural networks (CNNs) have achieved great success in computer vision, this work investigates a \n-simpler, convolution-free backbone network useful for many dense prediction tasks. Unlike the recently proposed Vision \n-Transformer (ViT) that was designed for image classification specifically, we introduce the Pyramid Vision Transformer \n-(PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several \n-merits compared to current state of the arts. Different from ViT that typically yields low resolution outputs and \n-incurs high computational and memory costs, PVT not only can be trained on dense partitions of an image to achieve high \n-output resolution, which is important for dense prediction, but also uses a progressive shrinking pyramid to reduce the \n-computations of large feature maps. PVT inherits the advantages of both CNN and Transformer, making it a unified \n-backbone for various vision tasks without convolutions, where it can be used as a direct replacement for CNN backbones. \n+*Although convolutional neural networks (CNNs) have achieved great success in computer vision, this work investigates a\n+simpler, convolution-free backbone network useful for many dense prediction tasks. Unlike the recently proposed Vision\n+Transformer (ViT) that was designed for image classification specifically, we introduce the Pyramid Vision Transformer\n+(PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several\n+merits compared to current state of the arts. Different from ViT that typically yields low resolution outputs and\n+incurs high computational and memory costs, PVT not only can be trained on dense partitions of an image to achieve high\n+output resolution, which is important for dense prediction, but also uses a progressive shrinking pyramid to reduce the\n+computations of large feature maps. PVT inherits the advantages of both CNN and Transformer, making it a unified\n+backbone for various vision tasks without convolutions, where it can be used as a direct replacement for CNN backbones.\n We validate PVT through extensive experiments, showing that it boosts the performance of many downstream tasks, including\n-object detection, instance and semantic segmentation. For example, with a comparable number of parameters, PVT+RetinaNet \n-achieves 40.4 AP on the COCO dataset, surpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute AP (see Figure 2). We hope \n+object detection, instance and semantic segmentation. For example, with a comparable number of parameters, PVT+RetinaNet\n+achieves 40.4 AP on the COCO dataset, surpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute AP (see Figure 2). We hope\n that PVT could serve as an alternative and useful backbone for pixel-level predictions and facilitate future research.*\n \n This model was contributed by [Xrenya](https://huggingface.co/Xrenya). The original code can be found [here](https://github.com/whai362/PVT).\n \n-\n - PVTv1 on ImageNet-1K\n \n | **Model variant**  |**Size** |**Acc@1**|**Params (M)**|\n@@ -55,7 +54,6 @@ This model was contributed by [Xrenya](https://huggingface.co/Xrenya). The origi\n | PVT-Medium         |    224  |   81.2  |     44.2     |\n | PVT-Large          |    224  |   81.7  |     61.4     |\n \n-\n ## PvtConfig\n \n [[autodoc]] PvtConfig"
        },
        {
            "sha": "5be8998f4cc2c7c645a2ede2d189c1730b8fa081",
            "filename": "docs/source/en/model_doc/pvt_v2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt_v2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -26,7 +26,7 @@ The PVTv2 encoder structure has been successfully deployed to achieve state-of-t\n \n PVTv2 belongs to a family of models called [hierarchical transformers](https://natecibik.medium.com/the-rise-of-vision-transformers-f623c980419f) , which make adaptations to transformer layers in order to generate multi-scale feature maps. Unlike the columnal structure of Vision Transformer ([ViT](https://huggingface.co/papers/2010.11929)) which loses fine-grained detail, multi-scale feature maps are known preserve this detail and aid performance in dense prediction tasks. In the case of PVTv2, this is achieved by generating image patch tokens using 2D convolution with overlapping kernels in each encoder layer.\n \n-The multi-scale features of hierarchical transformers allow them to be easily swapped in for traditional workhorse computer vision backbone models like ResNet in larger architectures. Both Segformer and Panoptic Segformer demonstrated that configurations using PVTv2 for a backbone consistently outperformed those with similarly sized ResNet backbones. \n+The multi-scale features of hierarchical transformers allow them to be easily swapped in for traditional workhorse computer vision backbone models like ResNet in larger architectures. Both Segformer and Panoptic Segformer demonstrated that configurations using PVTv2 for a backbone consistently outperformed those with similarly sized ResNet backbones.\n \n Another powerful feature of the PVTv2 is the complexity reduction in the self-attention layers called Spatial Reduction Attention (SRA), which uses 2D convolution layers to project hidden states to a smaller resolution before attending to them with the queries, improving the $O(n^2)$ complexity of self-attention to $O(n^2/R)$, with $R$ being the spatial reduction ratio (`sr_ratio`, aka kernel size and stride in the 2D convolution).\n \n@@ -48,6 +48,7 @@ This model was contributed by [FoamoftheSea](https://huggingface.co/FoamoftheSea\n - ImageNet pretrained weights for all model sizes can be found on the [hub](https://huggingface.co/models?other=pvt_v2).\n \n  The best way to get started with the PVTv2 is to load the pretrained checkpoint with the size of your choosing using `AutoModelForImageClassification`:\n+\n ```python\n import requests\n import torch\n@@ -99,7 +100,6 @@ outputs = model(torch.tensor(processed[\"pixel_values\"]))\n | PVT-V2-B4        |  224 |  83.6 |     62.6    |\n | PVT-V2-B5        |  224 |  83.8 |     82.0    |\n \n-\n ## PvtV2Config\n \n [[autodoc]] PvtV2Config"
        },
        {
            "sha": "feeb69959b2157cab4e5af2ba9a3369d726743ae",
            "filename": "docs/source/en/model_doc/qwen2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -142,7 +142,6 @@ outputs = model.generate(**inputs, max_new_tokens=100)\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```\n \n-\n ## Notes\n \n - Ensure your Transformers library version is up-to-date. Qwen2 requires Transformers>=4.37.0 for full support."
        },
        {
            "sha": "7a0836592d45e71842c5d7c4e1ba65000142d780",
            "filename": "docs/source/en/model_doc/qwen2_5_omni.md",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -31,16 +31,13 @@ The abstract from the technical report is the following:\n \n *We present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. This strategy effectively decouples the handling of long sequences of multimodal data, assigning the perceptual responsibilities to the multimodal encoder and entrusting the modeling of extended sequences to a large language model. Such a division of labor enhances the fusion of different modalities via the shared attention mechanism. To synchronize the timestamps of video inputs with audio, we organized the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE (Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni outperforms the similarly sized Qwen2-VL and Qwen2-Audio in both image and audio capabilities. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni is the first open-source model to achieve a level of performance in end-to-end speech instruction following that is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni’s streaming Talker outperform most existing streaming and non-streaming alternatives in robustness and naturalness.*\n \n-\n-\n ## Notes\n \n - Use [`Qwen2_5OmniForConditionalGeneration`] to generate audio and text output. To generate only one output type, use [`Qwen2_5OmniThinkerForConditionalGeneration`] for text-only and [`Qwen2_5OmniTalkersForConditionalGeneration`] for audio-only outputs.\n - Audio generation with [`Qwen2_5OmniForConditionalGeneration`] supports only single batch size at the moment.\n - In case out out-of-memory errors hwen working with video input, decrease `processor.max_pixels`. By default the maximum is set to a very arge value and high resolution visuals will not be resized, unless resolution exceeds `processor.max_pixels`.\n - The processor has its own [`~ProcessorMixin.apply_chat_template`] method to convert chat messages to model inputs.\n \n-\n ## Usage example\n \n `Qwen2.5-Omni` can be found on the [Huggingface Hub](https://huggingface.co/Qwen).\n@@ -275,6 +272,7 @@ processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\", min_pixels=min\n \n #### Prompt for audio output\n If users need audio output, the system prompt must be set as \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\", otherwise the audio output may not work as expected.\n+\n ```\n {\n     \"role\": \"system\",\n@@ -285,6 +283,7 @@ If users need audio output, the system prompt must be set as \"You are Qwen, a vi\n #### Use audio output or not\n \n The model supports both text and audio outputs, if users do not need audio outputs, they can set `enable_audio_output` in the `from_pretrained` function. This option will save about `~2GB` of GPU memory but the `return_audio` option for `generate` function will only allow to be set at `False`.\n+\n ```python\n model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n     \"Qwen/Qwen2.5-Omni-7B\",\n@@ -341,8 +340,6 @@ model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n )\n ```\n \n-\n-\n ## Qwen2_5OmniConfig\n \n [[autodoc]] Qwen2_5OmniConfig"
        },
        {
            "sha": "7f682bf802011a22e2833ba04446e52d5f85ade4",
            "filename": "docs/source/en/model_doc/qwen2_5_vl.md",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -26,7 +26,6 @@ rendered properly in your Markdown viewer.\n \n [Qwen2.5-VL](https://huggingface.co/papers/2502.13923) is a multimodal vision-language model, available in 3B, 7B, and 72B parameters, pretrained on 4.1T tokens. The model introduces window attention in the ViT encoder to accelerate training and inference, dynamic FPS sampling on the spatial and temporal dimensions for better video understanding across different sampling rates, and an upgraded MRoPE (multi-resolutional rotary positional encoding) mechanism to better capture and learn temporal dynamics.\n \n-\n You can find all the original Qwen2.5-VL checkpoints under the [Qwen2.5-VL](https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5) collection.\n \n > [!TIP]\n@@ -61,6 +60,7 @@ messages = [\n pipe(text=messages,max_new_tokens=20, return_full_text=False)\n \n ```\n+\n </hfoption>\n \n <hfoption id=\"AutoModel\">\n@@ -110,6 +110,7 @@ output_text = processor.batch_decode(\n )\n print(output_text)\n ```\n+\n </hfoption>\n </hfoptions>\n \n@@ -130,9 +131,11 @@ model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n )\n \n ```\n+\n ### Notes\n \n - Use Qwen2.5-VL for video inputs by setting `\"type\": \"video\"` as shown below.\n+\n     ```python\n     conversation = [\n         {\n@@ -159,8 +162,10 @@ model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n     output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n     print(output_text)\n     ```\n+\n - Use Qwen2.5-VL for a mixed batch of inputs (images, videos, text). Add labels when handling multiple images or videos for better reference\n  as show below.\n+\n     ```python\n     import torch\n     from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n@@ -221,14 +226,15 @@ model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n     max_pixels = 2048*2048\n     processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n     ```\n-    \n+\n     Higher resolution can require more compute whereas reducing the resolution can save memory as follows:\n-    \n+\n     ```python\n     min_pixels = 256*28*28\n     max_pixels = 1024*28*28 \n     processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n     ```\n+\n ## Qwen2_5_VLConfig\n \n [[autodoc]] Qwen2_5_VLConfig"
        },
        {
            "sha": "9b9dd43a919d0b2725b14d0ed9925819763be455",
            "filename": "docs/source/en/model_doc/qwen2_audio.md",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -36,7 +36,6 @@ The abstract from the paper is the following:\n \n *We introduce the latest progress of Qwen-Audio, a large-scale audio-language model called Qwen2-Audio, which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions. In contrast to complex hierarchical tags, we have simplified the pre-training process by utilizing natural language prompts for different data and tasks, and have further expanded the data volume. We have boosted the instruction-following capability of Qwen2-Audio and implemented two distinct audio interaction modes for voice chat and audio analysis. In the voice chat mode, users can freely engage in voice interactions with Qwen2-Audio without text input. In the audio analysis mode, users could provide audio and text instructions for analysis during the interaction. Note that we do not use any system prompts to switch between voice chat and audio analysis modes. Qwen2-Audio is capable of intelligently comprehending the content within audio and following voice commands to respond appropriately. For instance, in an audio segment that simultaneously contains sounds, multi-speaker conversations, and a voice command, Qwen2-Audio can directly understand the command and provide an interpretation and response to the audio. Additionally, DPO has optimized the model's performance in terms of factuality and adherence to desired behavior. According to the evaluation results from AIR-Bench, Qwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests focused on audio-centric instruction-following capabilities. Qwen2-Audio is open-sourced with the aim of fostering the advancement of the multi-modal language community. *\n \n-\n ## Usage tips\n \n `Qwen2-Audio-7B` and `Qwen2-Audio-7B-Instruct` can be found on the [Huggingface Hub](https://huggingface.co/Qwen)\n@@ -79,6 +78,7 @@ In the following, we demonstrate how to use `Qwen2-Audio-7B-Instruct` for the in\n \n ### Voice Chat Inference\n In the voice chat mode, users can freely engage in voice interactions with Qwen2-Audio without text input:\n+\n ```python\n from io import BytesIO\n from urllib.request import urlopen\n@@ -119,6 +119,7 @@ response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_\n \n ### Audio Analysis Inference\n In the audio analysis, users could provide both audio and text instructions for analysis:\n+\n ```python\n from io import BytesIO\n from urllib.request import urlopen\n@@ -167,6 +168,7 @@ response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_\n \n ### Batch Inference\n We also support batch inference:\n+\n ```python\n from io import BytesIO\n from urllib.request import urlopen"
        },
        {
            "sha": "9d55de63e16d7f35251e748381b5757ddf72dd48",
            "filename": "docs/source/en/model_doc/qwen2_moe.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -24,7 +24,6 @@ rendered properly in your Markdown viewer.\n \n # Qwen2MoE\n \n-\n [Qwen2MoE](https://huggingface.co/papers/2407.10671) is a Mixture-of-Experts (MoE) variant of [Qwen2](./qwen2), available as a base model and an aligned chat model. It uses SwiGLU activation, group query attention and a mixture of sliding window attention and full attention. The tokenizer can also be adapted to multiple languages and codes.\n \n The MoE architecture uses upcyled models from the dense language models. For example, Qwen1.5-MoE-A2.7B is upcycled from Qwen-1.8B. It has 14.3B parameters but only 2.7B parameters are activated during runtime.\n@@ -57,6 +56,7 @@ messages = [\n outputs = pipe(messages, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n print(outputs[0][\"generated_text\"][-1]['content'])\n ```\n+\n </hfoption>\n <hfoption id=\"AutoModel\">\n \n@@ -100,14 +100,14 @@ generated_ids = [\n response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n print(response)\n ```\n-</hfoption> \n+\n+</hfoption>\n <hfoption id=\"transformers CLI\">\n ```bash\n transformers chat Qwen/Qwen1.5-MoE-A2.7B-Chat --dtype auto --attn_implementation flash_attention_2\n ```\n </hfoption>\n- </hfoptions> \n-\n+ </hfoptions>\n \n Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n "
        },
        {
            "sha": "59dc25b5e0853330a09fd0d3f5b6e1e3f30e189f",
            "filename": "docs/source/en/model_doc/qwen2_vl.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The [Qwen2-VL](https://huggingface.co/papers/2409.12191) ([blog post](https://qwenlm.github.io/blog/qwen2-vl/)) model is a major update to [Qwen-VL](https://huggingface.co/papers/2308.12966) from the Qwen team at Alibaba Research. \n+The [Qwen2-VL](https://huggingface.co/papers/2409.12191) ([blog post](https://qwenlm.github.io/blog/qwen2-vl/)) model is a major update to [Qwen-VL](https://huggingface.co/papers/2308.12966) from the Qwen team at Alibaba Research.\n \n The abstract from the blog is the following:\n \n@@ -203,8 +203,8 @@ min_pixels = 256*28*28\n max_pixels = 1024*28*28 \n processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n ```\n-This ensures each image gets encoded using a number between 256-1024 tokens. The 28 comes from the fact that the model uses a patch size of 14 and a temporal patch size of 2 (14 x 2 = 28).\n \n+This ensures each image gets encoded using a number between 256-1024 tokens. The 28 comes from the fact that the model uses a patch size of 14 and a temporal patch size of 2 (14 x 2 = 28).\n \n #### Multiple Image Inputs\n \n@@ -307,7 +307,7 @@ model = Qwen2VLForConditionalGeneration.from_pretrained(\n \n [[autodoc]] Qwen2VLTextModel\n     - forward\n-    \n+\n ## Qwen2VLModel\n \n [[autodoc]] Qwen2VLModel"
        },
        {
            "sha": "0141388fb97f37044209bd362eae10004c076b9d",
            "filename": "docs/source/en/model_doc/qwen3.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -25,7 +25,6 @@ rendered properly in your Markdown viewer.\n \n To be released with the official model launch.\n \n-\n ## Usage tips\n \n To be released with the official model launch."
        },
        {
            "sha": "cd5506802d5ac2fcf929e62e66f20109de927a13",
            "filename": "docs/source/en/model_doc/qwen3_omni_moe.md",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -31,16 +31,13 @@ The abstract from the technical report is the following:\n \n *We present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. This strategy effectively decouples the handling of long sequences of multimodal data, assigning the perceptual responsibilities to the multimodal encoder and entrusting the modeling of extended sequences to a large language model. Such a division of labor enhances the fusion of different modalities via the shared attention mechanism. To synchronize the timestamps of video inputs with audio, we organized the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE (Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni outperforms the similarly sized Qwen2-VL and Qwen2-Audio in both image and audio capabilities. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni is the first open-source model to achieve a level of performance in end-to-end speech instruction following that is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni’s streaming Talker outperform most existing streaming and non-streaming alternatives in robustness and naturalness.*\n \n-\n-\n ## Notes\n \n - Use [`Qwen2_5OmniForConditionalGeneration`] to generate audio and text output. To generate only one output type, use [`Qwen2_5OmniThinkerForConditionalGeneration`] for text-only and [`Qwen2_5OmniTalkersForConditionalGeneration`] for audio-only outputs.\n - Audio generation with [`Qwen2_5OmniForConditionalGeneration`] supports only single batch size at the moment.\n - In case out out-of-memory errors hwen working with video input, decrease `processor.max_pixels`. By default the maximum is set to a very arge value and high resolution visuals will not be resized, unless resolution exceeds `processor.max_pixels`.\n - The processor has its own [`~ProcessorMixin.apply_chat_template`] method to convert chat messages to model inputs.\n \n-\n ## Usage example\n \n `Qwen2.5-Omni` can be found on the [Huggingface Hub](https://huggingface.co/Qwen).\n@@ -275,6 +272,7 @@ processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\", min_pixels=min\n \n #### Prompt for audio output\n If users need audio output, the system prompt must be set as \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\", otherwise the audio output may not work as expected.\n+\n ```\n {\n     \"role\": \"system\",\n@@ -285,6 +283,7 @@ If users need audio output, the system prompt must be set as \"You are Qwen, a vi\n #### Use audio output or not\n \n The model supports both text and audio outputs, if users do not need audio outputs, they can set `enable_audio_output` in the `from_pretrained` function. This option will save about `~2GB` of GPU memory but the `return_audio` option for `generate` function will only allow to be set at `False`.\n+\n ```python\n model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n     \"Qwen/Qwen2.5-Omni-7B\",\n@@ -341,8 +340,6 @@ model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n )\n ```\n \n-\n-\n ## Qwen3OmniMoeConfig\n \n [[autodoc]] Qwen3OmniMoeConfig\n@@ -410,5 +407,3 @@ model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n ## Qwen3OmniMoeTalkerCodePredictorModelForConditionalGeneration\n \n [[autodoc]] Qwen3OmniMoeTalkerCodePredictorModelForConditionalGeneration\n-\n-"
        },
        {
            "sha": "dc9ecafeb44a023444cc30837b61b57183586182",
            "filename": "docs/source/en/model_doc/qwen3_vl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -77,6 +77,7 @@ output_text = processor.batch_decode(\n )\n print(output_text)\n ```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "e36336d90a44a727d74f56acdf4d67722e7933cb",
            "filename": "docs/source/en/model_doc/qwen3_vl_moe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl_moe.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -77,6 +77,7 @@ output_text = processor.batch_decode(\n )\n print(output_text)\n ```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "2d7c940e00a9ddeb047867f3c91ec823521e07dd",
            "filename": "docs/source/en/model_doc/recurrent_gemma.md",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Frecurrent_gemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Frecurrent_gemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frecurrent_gemma.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -31,16 +31,14 @@ The abstract from the paper is the following:\n \n Tips:\n \n-- The original checkpoints can be converted using the conversion script [`src/transformers/models/recurrent_gemma/convert_recurrent_gemma_weights_to_hf.py`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/recurrent_gemma/convert_recurrent_gemma_to_hf.py). \n+- The original checkpoints can be converted using the conversion script [`src/transformers/models/recurrent_gemma/convert_recurrent_gemma_weights_to_hf.py`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/recurrent_gemma/convert_recurrent_gemma_to_hf.py).\n \n This model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ). The original code can be found [here](https://github.com/google-deepmind/recurrentgemma).\n \n-\n ## RecurrentGemmaConfig\n \n [[autodoc]] RecurrentGemmaConfig\n \n-\n ## RecurrentGemmaModel\n \n [[autodoc]] RecurrentGemmaModel\n@@ -50,4 +48,3 @@ This model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ). T\n \n [[autodoc]] RecurrentGemmaForCausalLM\n     - forward\n-"
        },
        {
            "sha": "c48de93d47da92cfb77ef26346d45a72bfb00edd",
            "filename": "docs/source/en/model_doc/reformer.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -89,7 +89,6 @@ equal to `config.hidden_size` and `config.axial_pos_shape` is set to a tuple \\\\(\n product has to be equal to `config.max_embedding_size`, which during training has to be equal to the *sequence\n length* of the `input_ids`.\n \n-\n ### LSH Self Attention\n \n In Locality sensitive hashing (LSH) self attention the key and query projection weights are tied. Therefore, the key\n@@ -122,7 +121,6 @@ Using LSH self attention, the memory and time complexity of the query-key matmul\n \\\\(\\mathcal{O}(n_s \\times n_s)\\\\) to \\\\(\\mathcal{O}(n_s \\times \\log(n_s))\\\\), which usually represents the memory\n and time bottleneck in a transformer model, with \\\\(n_s\\\\) being the sequence length.\n \n-\n ### Local Self Attention\n \n Local self attention is essentially a \"normal\" self attention layer with key, query and value projections, but is\n@@ -134,7 +132,6 @@ Using Local self attention, the memory and time complexity of the query-key matm\n \\\\(\\mathcal{O}(n_s \\times n_s)\\\\) to \\\\(\\mathcal{O}(n_s \\times \\log(n_s))\\\\), which usually represents the memory\n and time bottleneck in a transformer model, with \\\\(n_s\\\\) being the sequence length.\n \n-\n ### Training\n \n During training, we must ensure that the sequence length is set to a value that can be divided by the least common"
        },
        {
            "sha": "829fed24215f9ff28584ed0cf1894c43dbf6e18a",
            "filename": "docs/source/en/model_doc/retribert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fretribert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fretribert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fretribert.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -39,7 +39,6 @@ pair of BERT encoders with lower-dimension projection for dense semantic indexin\n This model was contributed by [yjernite](https://huggingface.co/yjernite). Code to train and use the model can be\n found [here](https://github.com/huggingface/transformers/tree/main/examples/research-projects/distillation).\n \n-\n ## RetriBertConfig\n \n [[autodoc]] RetriBertConfig"
        },
        {
            "sha": "896156520c5d744b17e1bd0212ced7603240b5a0",
            "filename": "docs/source/en/model_doc/roberta.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -28,7 +28,6 @@ rendered properly in your Markdown viewer.\n \n You can find all the original RoBERTa checkpoints under the [Facebook AI](https://huggingface.co/FacebookAI) organization.\n \n-\n > [!TIP]\n > Click on the RoBERTa models in the right sidebar for more examples of how to apply RoBERTa to different language tasks.\n "
        },
        {
            "sha": "d4c85f63fc370dca72f0fb11abb5629522e0f0b3",
            "filename": "docs/source/en/model_doc/rt_detr.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -23,7 +23,6 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-\n The RT-DETR model was proposed in [DETRs Beat YOLOs on Real-time Object Detection](https://huggingface.co/papers/2304.08069) by Wenyu Lv, Yian Zhao, Shangliang Xu, Jinman Wei, Guanzhong Wang, Cheng Cui, Yuning Du, Qingqing Dang, Yi Liu.\n \n RT-DETR is an object detection model that stands for \"Real-Time DEtection Transformer.\" This model is designed to perform object detection tasks with a focus on achieving real-time performance while maintaining high accuracy. Leveraging the transformer architecture, which has gained significant popularity in various fields of deep learning, RT-DETR processes images to identify and locate multiple objects within them.\n@@ -39,7 +38,6 @@ alt=\"drawing\" width=\"600\"/>\n \n The model version was contributed by [rafaelpadilla](https://huggingface.co/rafaelpadilla) and [sangbumchoi](https://github.com/SangbumChoi). The original code can be found [here](https://github.com/lyuwenyu/RT-DETR/).\n \n-\n ## Usage tips\n \n Initially, an image is processed using a pre-trained convolutional neural network, specifically a Resnet-D variant as referenced in the original code. This network extracts features from the final three layers of the architecture. Following this, a hybrid encoder is employed to convert the multi-scale features into a sequential array of image features. Then, a decoder, equipped with auxiliary prediction heads is used to refine the object queries. This process facilitates the direct generation of bounding boxes, eliminating the need for any additional post-processing to acquire the logits and coordinates for the bounding boxes."
        },
        {
            "sha": "3f814ce0d6496abf98acac69708a2dd6c9bbd78f",
            "filename": "docs/source/en/model_doc/rt_detr_v2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -34,9 +34,9 @@ The abstract from the paper is the following:\n This model was contributed by [jadechoghari](https://huggingface.co/jadechoghari).\n The original code can be found [here](https://github.com/lyuwenyu/RT-DETR).\n \n-## Usage tips \n+## Usage tips\n \n-This second version of RT-DETR improves how the decoder finds objects in an image. \n+This second version of RT-DETR improves how the decoder finds objects in an image.\n \n - **better sampling** – adjusts offsets so the model looks at the right areas\n - **flexible attention** – can use smooth (bilinear) or fixed (discrete) sampling\n@@ -85,17 +85,15 @@ A list of official Hugging Face and community (indicated by 🌎) resources to h\n - See also: [Object detection task guide](../tasks/object_detection).\n - Notebooks for [inference](https://github.com/qubvel/transformers-notebooks/blob/main/notebooks/RT_DETR_v2_inference.ipynb) and [fine-tuning](https://github.com/qubvel/transformers-notebooks/blob/main/notebooks/RT_DETR_v2_finetune_on_a_custom_dataset.ipynb) RT-DETRv2 on a custom dataset (🌎).\n \n-\n ## RTDetrV2Config\n \n [[autodoc]] RTDetrV2Config\n \n-\n ## RTDetrV2Model\n \n [[autodoc]] RTDetrV2Model\n     - forward\n- \n+\n ## RTDetrV2ForObjectDetection\n \n [[autodoc]] RTDetrV2ForObjectDetection"
        },
        {
            "sha": "c0bd1273f615bf3fb391d4c9fea5eaa9def79572",
            "filename": "docs/source/en/model_doc/rwkv.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Frwkv.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Frwkv.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frwkv.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -58,7 +58,7 @@ torch.allclose(torch.cat([output_one, output_two], dim=1), output_whole, atol=1e\n \n If you want to make sure the model stops generating when `'\\n\\n'` is detected, we recommend using the following stopping criteria:\n \n-```python \n+```python\n from transformers import StoppingCriteria\n \n class RwkvStoppingCriteria(StoppingCriteria):"
        },
        {
            "sha": "65286eb8428d43a41c722cb094cd854991d321da",
            "filename": "docs/source/en/model_doc/sam.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -41,7 +41,6 @@ Tips:\n - Fine-tuning the model is not supported yet\n - According to the paper, textual input should be also supported. However, at this time of writing this seems not to be supported according to [the official repository](https://github.com/facebookresearch/segment-anything/issues/4#issuecomment-1497626844).\n \n-\n This model was contributed by [ybelkada](https://huggingface.co/ybelkada) and [ArthurZ](https://huggingface.co/ArthurZ).\n The original code can be found [here](https://github.com/facebookresearch/segment-anything).\n \n@@ -98,6 +97,7 @@ masks = processor.image_processor.post_process_masks(\n )\n scores = outputs.iou_scores\n ```\n+\n ## Resources\n \n A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with SAM."
        },
        {
            "sha": "9dea1de7a77e3c0e88ee1055190cd97aefa4fe95",
            "filename": "docs/source/en/model_doc/sam_hq.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -25,7 +25,6 @@ The model is an enhancement to the original SAM model that produces significantl\n \n ![example image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam-output.png)\n \n-\n SAM-HQ introduces several key improvements over the original SAM model:\n \n 1. High-Quality Output Token: A learnable token injected into SAM's mask decoder for higher quality mask prediction\n@@ -105,7 +104,6 @@ masks = processor.image_processor.post_process_masks(\n scores = outputs.iou_scores\n ```\n \n-\n ## Resources\n \n A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with SAM-HQ:\n@@ -137,7 +135,6 @@ A list of official Hugging Face and community (indicated by 🌎) resources to h\n \n [[autodoc]] SamHQVisionModel\n \n-\n ## SamHQModel\n \n [[autodoc]] SamHQModel"
        },
        {
            "sha": "e7fc00d047c35b754937c9745cb5a78a43117cd7",
            "filename": "docs/source/en/model_doc/seamless_m4t.md",
            "status": "modified",
            "additions": 2,
            "deletions": 14,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -67,7 +67,6 @@ Here is how to use the processor to process text and audio:\n >>> text_inputs = processor(text = \"Hello, my dog is cute\", src_lang=\"eng\", return_tensors=\"pt\")\n ```\n \n-\n ### Speech\n \n [`SeamlessM4TModel`] can *seamlessly* generate text or speech with few or no changes. Let's target Russian voice translation:\n@@ -84,7 +83,7 @@ With basically the same code, I've translated English text and Arabic speech to\n Similarly, you can generate translated text from audio files or from text with the same model. You only have to pass `generate_speech=False` to [`SeamlessM4TModel.generate`].\n This time, let's translate to French.\n \n-```python \n+```python\n >>> # from audio\n >>> output_tokens = model.generate(**audio_inputs, tgt_lang=\"fra\", generate_speech=False)\n >>> translated_text_from_audio = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n@@ -96,11 +95,10 @@ This time, let's translate to French.\n \n ### Tips\n \n-\n #### 1. Use dedicated models\n \n [`SeamlessM4TModel`] is transformers top level model to generate speech and text, but you can also use dedicated models that perform the task without additional components, thus reducing the memory footprint.\n-For example, you can replace the audio-to-audio generation snippet with the model dedicated to the S2ST task, the rest is exactly the same code: \n+For example, you can replace the audio-to-audio generation snippet with the model dedicated to the S2ST task, the rest is exactly the same code:\n \n ```python\n >>> from transformers import SeamlessM4TForSpeechToSpeech\n@@ -130,7 +128,6 @@ Use `return_intermediate_token_ids=True` with [`SeamlessM4TModel`] to return bot\n \n ## Model architecture\n \n-\n SeamlessM4T features a versatile architecture that smoothly handles the sequential generation of text and speech. This setup comprises two sequence-to-sequence (seq2seq) models. The first model translates the input modality into translated text, while the second model generates speech tokens, known as \"unit tokens,\" from the translated text.\n \n Each modality has its own dedicated encoder with a unique architecture. Additionally, for speech output, a vocoder inspired by the [HiFi-GAN](https://huggingface.co/papers/2010.05646) architecture is placed on top of the second seq2seq model.\n@@ -142,27 +139,23 @@ Here's how the generation process works:\n - If speech generation is required, the second seq2seq model, following a standard encoder-decoder structure, generates unit tokens.\n - These unit tokens are then passed through the final vocoder to produce the actual speech.\n \n-\n This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The original code can be found [here](https://github.com/facebookresearch/seamless_communication).\n \n ## SeamlessM4TModel\n \n [[autodoc]] SeamlessM4TModel\n     - generate\n \n-\n ## SeamlessM4TForTextToSpeech\n \n [[autodoc]] SeamlessM4TForTextToSpeech\n     - generate\n \n-\n ## SeamlessM4TForSpeechToSpeech\n \n [[autodoc]] SeamlessM4TForSpeechToSpeech\n     - generate\n \n-\n ## SeamlessM4TForTextToText\n \n [[autodoc]] transformers.SeamlessM4TForTextToText\n@@ -179,7 +172,6 @@ This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The o\n \n [[autodoc]] SeamlessM4TConfig\n \n-\n ## SeamlessM4TTokenizer\n \n [[autodoc]] SeamlessM4TTokenizer\n@@ -189,7 +181,6 @@ This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The o\n     - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n-\n ## SeamlessM4TTokenizerFast\n \n [[autodoc]] SeamlessM4TTokenizerFast\n@@ -209,7 +200,6 @@ This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The o\n \n [[autodoc]] SeamlessM4TCodeHifiGan\n \n-\n ## SeamlessM4THifiGan\n \n [[autodoc]] SeamlessM4THifiGan\n@@ -221,5 +211,3 @@ This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The o\n ## SeamlessM4TTextToUnitForConditionalGeneration\n \n [[autodoc]] SeamlessM4TTextToUnitForConditionalGeneration\n-\n-"
        },
        {
            "sha": "716718072a4be529f0c23e3845004d40207bb829",
            "filename": "docs/source/en/model_doc/seamless_m4t_v2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -67,7 +67,6 @@ Here is how to use the processor to process text and audio:\n >>> text_inputs = processor(text = \"Hello, my dog is cute\", src_lang=\"eng\", return_tensors=\"pt\")\n ```\n \n-\n ### Speech\n \n [`SeamlessM4Tv2Model`] can *seamlessly* generate text or speech with few or no changes. Let's target Russian voice translation:\n@@ -84,7 +83,7 @@ With basically the same code, I've translated English text and Arabic speech to\n Similarly, you can generate translated text from audio files or from text with the same model. You only have to pass `generate_speech=False` to [`SeamlessM4Tv2Model.generate`].\n This time, let's translate to French.\n \n-```python \n+```python\n >>> # from audio\n >>> output_tokens = model.generate(**audio_inputs, tgt_lang=\"fra\", generate_speech=False)\n >>> translated_text_from_audio = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n@@ -96,11 +95,10 @@ This time, let's translate to French.\n \n ### Tips\n \n-\n #### 1. Use dedicated models\n \n [`SeamlessM4Tv2Model`] is transformers top level model to generate speech and text, but you can also use dedicated models that perform the task without additional components, thus reducing the memory footprint.\n-For example, you can replace the audio-to-audio generation snippet with the model dedicated to the S2ST task, the rest is exactly the same code: \n+For example, you can replace the audio-to-audio generation snippet with the model dedicated to the S2ST task, the rest is exactly the same code:\n \n ```python\n >>> from transformers import SeamlessM4Tv2ForSpeechToSpeech\n@@ -161,27 +159,23 @@ Here's how the generation process works:\n - If speech generation is required, the second seq2seq model, generates unit tokens in an non auto-regressive way.\n - These unit tokens are then passed through the final vocoder to produce the actual speech.\n \n-\n This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The original code can be found [here](https://github.com/facebookresearch/seamless_communication).\n \n ## SeamlessM4Tv2Model\n \n [[autodoc]] SeamlessM4Tv2Model\n     - generate\n \n-\n ## SeamlessM4Tv2ForTextToSpeech\n \n [[autodoc]] SeamlessM4Tv2ForTextToSpeech\n     - generate\n \n-\n ## SeamlessM4Tv2ForSpeechToSpeech\n \n [[autodoc]] SeamlessM4Tv2ForSpeechToSpeech\n     - generate\n \n-\n ## SeamlessM4Tv2ForTextToText\n \n [[autodoc]] transformers.SeamlessM4Tv2ForTextToText"
        },
        {
            "sha": "a6b407e58793082f357ee6cc579832208121105f",
            "filename": "docs/source/en/model_doc/segformer.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -71,8 +71,6 @@ logits = outputs.logits # shape [batch, num_labels, height, width]\n \n </hfoptions>\n \n-\n-\n ## Notes\n \n - SegFormer works with **any input size**, padding inputs to be divisible by `config.patch_sizes`."
        },
        {
            "sha": "a5568d5c80ec30c2f3b79ba7b0ea66a42a5e2916",
            "filename": "docs/source/en/model_doc/seggpt.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fseggpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fseggpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseggpt.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -74,7 +74,6 @@ mask = image_processor.post_process_semantic_segmentation(outputs, target_sizes,\n This model was contributed by [EduardoPacheco](https://huggingface.co/EduardoPacheco).\n The original code can be found [here]([(https://github.com/baaivision/Painter/tree/main)).\n \n-\n ## SegGptConfig\n \n [[autodoc]] SegGptConfig"
        },
        {
            "sha": "871cdd31db78640fc742fe4b35d554db9d93035b",
            "filename": "docs/source/en/model_doc/shieldgemma2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -86,7 +86,6 @@ output = model(**inputs)\n print(output.probabilities)\n ```\n \n-\n ## ShieldGemma2Processor\n \n [[autodoc]] ShieldGemma2Processor"
        },
        {
            "sha": "bf9c0a46034861c0bb7215ffbd94af8e95f0ed5d",
            "filename": "docs/source/en/model_doc/siglip.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -31,7 +31,6 @@ Unlike CLIP, SigLIP employs a pairwise sigmoid loss on image-text pairs during t\n \n You can find all the original SigLIP checkpoints under the [SigLIP](https://huggingface.co/collections/google/siglip-659d5e62f0ae1a57ae0e83ba) collection.\n \n-\n > [!TIP]\n > Click on the SigLIP models in the right sidebar for more examples of how to apply SigLIP to different image and text tasks.\n \n@@ -107,12 +106,14 @@ logits_per_image = outputs.logits_per_image\n probs = torch.sigmoid(logits_per_image)\n print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n ```\n+\n ## Notes\n \n - Training is supported for DDP and FSDP on single-node multi-GPU setups. However, it does not use [torch.distributed](https://pytorch.org/tutorials/beginner/dist_overview.html) utilities which may limit the scalability of batch size.\n - When using the standalone [`SiglipTokenizer`] or [`SiglipProcessor`], make sure to pass `padding=\"max_length\"` because that is how the model was trained.\n - To get the same results as the [`Pipeline`], a prompt template of `\"This is a photo of {label}.\"` should be passed to the processor.\n - Toggle the `attn_implementation` parameter to either `\"sdpa\"` or `\"flash_attention_2\"` to use a more memory-efficient attention.\n+\n     ```py\n     # pip install -U flash-attn --no-build-isolation\n \n@@ -126,7 +127,6 @@ print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n     )\n     ```\n \n-\n ## SiglipConfig\n \n [[autodoc]] SiglipConfig\n@@ -179,7 +179,6 @@ print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n [[autodoc]] SiglipVisionModel\n     - forward\n \n-\n ## SiglipForImageClassification\n \n [[autodoc]] SiglipForImageClassification"
        },
        {
            "sha": "6a058f8907a4a13c1aea042e3861214a24a9d267",
            "filename": "docs/source/en/model_doc/siglip2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -32,7 +32,6 @@ rendered properly in your Markdown viewer.\n - NaFlex supports different resolutions and maintains the native image aspect ratio\n - FixRes supports fixed resolutions and is backwards compatible with [SigLIP](./siglip)\n \n-\n You can find all the original SigLIP2 checkpoints under the [SigLIP2](https://huggingface.co/collections/google/siglip2-67b5dcef38c175486e240107) collection.\n \n > [!TIP]\n@@ -157,6 +156,7 @@ print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n \n    NaFlex resizes the input image so the height and width are multiples of the patch size after resizing. It keeps the aspect ratio distortion as low as possible and produces a sequence length of at most the desired target sequence length (`max_num_patches`). After resizing, the image is split into a sequence of patches and a mask with padding information is added.\n - Toggle the `attn_implementation` parameter to either `\"sdpa\"` or `\"flash_attention_2\"` to use a more memory-efficient attention.\n+\n     ```py\n     # pip install -U flash-attn --no-build-isolation\n \n@@ -169,6 +169,7 @@ print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n         device_map=device,\n     )\n     ```\n+\n ## Siglip2Config\n \n [[autodoc]] Siglip2Config"
        },
        {
            "sha": "db2ddd336013b6e3688c9ad5e469b8d37dad8e23",
            "filename": "docs/source/en/model_doc/smollm3.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmollm3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmollm3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmollm3.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -139,7 +139,6 @@ outputs = model.generate(**inputs, max_new_tokens=100)\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```\n \n-\n ## Notes\n \n - Ensure your Transformers library version is up-to-date. SmolLM3 requires Transformers>=4.53.0 for full support."
        },
        {
            "sha": "5f74fa60ba0c4372ede29f7ba5d06fb3e54de5f0",
            "filename": "docs/source/en/model_doc/smolvlm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -39,6 +39,7 @@ If `do_resize` is set to `True`, the model resizes images so that the longest ed\n The default resizing behavior can be customized by passing a dictionary to the `size` parameter. For example, `{\"longest_edge\": 4 * 512}` is the default, but you can change it to a different value if needed.\n \n Here’s how to control resizing and set a custom size:\n+\n ```python\n image_processor = SmolVLMImageProcessor(do_resize=True, size={\"longest_edge\": 2 * 512}, max_image_size=512)\n ```\n@@ -47,8 +48,6 @@ Additionally, the `max_image_size` parameter, which controls the size of each sq\n \n This model was contributed by [orrzohar](https://huggingface.co/orrzohar).\n \n-\n-\n ## Usage example\n \n ### Single Media inference"
        },
        {
            "sha": "e47598a8f852dd25268f049bf3bcd72389a102ca",
            "filename": "docs/source/en/model_doc/stablelm.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fstablelm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fstablelm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fstablelm.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -92,7 +92,6 @@ Now, to run the model with Flash Attention 2, refer to the snippet below:\n ['The weather is always wonderful in Costa Rica, which makes it a prime destination for retirees. That’s where the Pensionado program comes in, offering']\n ```\n \n-\n ## StableLmConfig\n \n [[autodoc]] StableLmConfig"
        },
        {
            "sha": "b67e5dedd2cc84e2c22894f68c2c73019d51c5b2",
            "filename": "docs/source/en/model_doc/starcoder2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fstarcoder2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fstarcoder2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fstarcoder2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -34,7 +34,7 @@ The abstract of the paper is the following:\n ## License\n \n The models are licensed under the [BigCode OpenRAIL-M v1 license agreement](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement).\n- \n+\n ## Usage tips\n \n The StarCoder2 models can be found in the [HuggingFace hub](https://huggingface.co/collections/bigcode/starcoder2-65de6da6e87db3383572be1a). You can find some examples for inference and fine-tuning in StarCoder2's [GitHub repo](https://github.com/bigcode-project/starcoder2)."
        },
        {
            "sha": "d25ca822e4c60e6f4aca8532eacbff43d25befe6",
            "filename": "docs/source/en/model_doc/superglue.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -153,4 +153,3 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n [[autodoc]] SuperGlueForKeypointMatching\n \n - forward\n-"
        },
        {
            "sha": "26ffb2c8b4bdee4e49eb7da57ffe356b67e72664",
            "filename": "docs/source/en/model_doc/superpoint.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -33,8 +33,6 @@ You can find all the original SuperPoint checkpoints under the [Magic Leap Commu\n >\n > Click on the SuperPoint models in the right sidebar for more examples of how to apply SuperPoint to different computer vision tasks.\n \n-\n-\n The example below demonstrates how to detect interest points in an image with the [`AutoModel`] class.\n <hfoptions id=\"usage\">\n <hfoption id=\"AutoModel\">\n@@ -101,6 +99,7 @@ processed_outputs = processor.post_process_keypoint_detection(outputs, [image_si\n     ```\n \n - You can then print the keypoints on the image of your choice to visualize the result:\n+\n     ```py\n     import matplotlib.pyplot as plt\n     plt.axis(\"off\")"
        },
        {
            "sha": "81142f6c4111967c90c132d2943f1682cfda1be9",
            "filename": "docs/source/en/model_doc/swin.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -47,6 +47,7 @@ pipeline = pipeline(\n )\n pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n ```\n+\n </hfoption>\n \n <hfoption id=\"AutoModel\">\n@@ -79,6 +80,7 @@ class_labels = model.config.id2label\n predicted_class_label = class_labels[predicted_class_id]\n print(f\"The predicted class label is: {predicted_class_label}\")\n ```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "0dc008767ac305bf12219107aae7942592fcdee2",
            "filename": "docs/source/en/model_doc/swinv2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fswinv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fswinv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fswinv2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -81,7 +81,7 @@ print(f\"The predicted class label is: {predicted_class_label}\")\n \n ## Notes\n \n-- Swin Transformer V2 can pad the inputs for any input height and width divisible by `32`. \n+- Swin Transformer V2 can pad the inputs for any input height and width divisible by `32`.\n - Swin Transformer V2 can be used as a [backbone](../backbones). When `output_hidden_states = True`, it outputs both `hidden_states` and `reshaped_hidden_states`. The `reshaped_hidden_states` have a shape of `(batch, num_channels, height, width)` rather than `(batch_size, sequence_length, num_channels)`.\n \n ## Swinv2Config"
        },
        {
            "sha": "5eb27a9e7d8c33cc0e651dca8f59ba06438cbe64",
            "filename": "docs/source/en/model_doc/switch_transformers.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fswitch_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fswitch_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fswitch_transformers.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -27,7 +27,6 @@ rendered properly in your Markdown viewer.\n \n You can find all the original Switch Transformers checkpoints under the [Switch Transformer](https://huggingface.co/collections/google/switch-transformers-release-6548c35c6507968374b56d1f) collection.\n \n-\n > [!TIP]\n > This model was contributed by [ybelkada](https://huggingface.co/ybelkada) and [ArthurZ](https://huggingface.co/ArthurZ).\n >\n@@ -99,7 +98,6 @@ outputs = model.generate(input_ids)\n print(tokenizer.decode(outputs[0]))\n ```\n \n-\n ## SwitchTransformersConfig\n \n [[autodoc]] SwitchTransformersConfig"
        },
        {
            "sha": "00dde7ab93af88aede86fd63bef7b7babb76786a",
            "filename": "docs/source/en/model_doc/t5gemma.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -39,7 +39,6 @@ The example below demonstrates how to chat with the model with [`Pipeline`] or t\n <hfoptions id=\"usage\">\n <hfoption id=\"Pipeline\">\n \n-\n ```python\n import torch\n from transformers import pipeline\n@@ -89,6 +88,7 @@ print(tokenizer.decode(outputs[0]))\n ```\n echo -e \"Write me a poem about Machine Learning. Answer:\" | transformers run --task text2text-generation --model google/t5gemma-2b-2b-prefixlm --device 0\n ```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "62787d5f9d621f26e40419ca69ed0a2c9c447a3e",
            "filename": "docs/source/en/model_doc/t5v1.1.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5v1.1.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5v1.1.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5v1.1.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -68,7 +68,6 @@ Google has released the following variants:\n \n - [google/t5-v1_1-xxl](https://huggingface.co/google/t5-v1_1-xxl).\n \n-\n <Tip>\n \n Refer to [T5's documentation page](t5) for all API reference, tips, code examples and notebooks."
        },
        {
            "sha": "c982d30590729d7d3ccfc6eb4f83a2d973cff59f",
            "filename": "docs/source/en/model_doc/table-transformer.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftable-transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftable-transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftable-transformer.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -43,8 +43,8 @@ alt=\"drawing\" width=\"600\"/>\n \n <small> Table detection and table structure recognition clarified. Taken from the <a href=\"https://huggingface.co/papers/2110.00061\">original paper</a>. </small>\n \n-The authors released 2 models, one for [table detection](https://huggingface.co/microsoft/table-transformer-detection) in \n-documents, one for [table structure recognition](https://huggingface.co/microsoft/table-transformer-structure-recognition) \n+The authors released 2 models, one for [table detection](https://huggingface.co/microsoft/table-transformer-detection) in\n+documents, one for [table structure recognition](https://huggingface.co/microsoft/table-transformer-structure-recognition)\n (the task of recognizing the individual rows, columns etc. in a table).\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be"
        },
        {
            "sha": "c5144121df6ce783855dd26455e490ee3d57ce06",
            "filename": "docs/source/en/model_doc/tapas.md",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapas.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapas.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapas.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -76,7 +76,6 @@ To summarize:\n | Weak supervision for aggregation    | WTQ                 | Questions might involve aggregation, and the model must learn this given only the answer as supervision |\n | Strong supervision for aggregation  | WikiSQL-supervised  | Questions might involve aggregation, and the model must learn this given the gold aggregation operator  |\n \n-\n Initializing a model with a pre-trained base and randomly initialized classification heads from the hub can be done as shown below.\n \n ```py\n@@ -105,7 +104,6 @@ Of course, you don't necessarily have to follow one of these three ways in which\n >>> model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base\", config=config)\n ```\n \n-\n What you can also do is start from an already fine-tuned checkpoint. A note here is that the already fine-tuned checkpoint on WTQ has some issues due to the L2-loss which is somewhat brittle. See [here](https://github.com/google-research/tapas/issues/91#issuecomment-735719340) for more info.\n \n For a list of all pre-trained and fine-tuned TAPAS checkpoints available on HuggingFace's  hub, see [here](https://huggingface.co/models?search=tapas).\n@@ -128,7 +126,6 @@ The tables themselves should be present in a folder, each table being a separate\n \n **STEP 3: Convert your data into tensors using TapasTokenizer**\n \n-\n Third, given that you've prepared your data in this TSV/CSV format (and corresponding CSV files containing the tabular data), you can then use [`TapasTokenizer`] to convert table-question pairs into `input_ids`, `attention_mask`, `token_type_ids` and so on. Again, based on which of the three cases you picked above, [`TapasForQuestionAnswering`] requires different\n inputs to be fine-tuned:\n \n@@ -214,13 +211,11 @@ Of course, this only shows how to encode a single training example. It is advise\n >>> train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n ```\n \n-\n Note that here, we encode each table-question pair independently. This is fine as long as your dataset is **not conversational**. In case your dataset involves conversational questions (such as in SQA), then you should first group together the `queries`, `answer_coordinates` and `answer_text` per table (in the order of their `position`\n index) and batch encode each table with its questions. This will make sure that the `prev_labels` token types (see docs of [`TapasTokenizer`]) are set correctly. See [this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) for more info.\n \n **STEP 4: Train (fine-tune) the model\n \n-\n You can then fine-tune [`TapasForQuestionAnswering`] as follows (shown here for the weak supervision for aggregation case):\n \n ```py\n@@ -272,10 +267,8 @@ You can then fine-tune [`TapasForQuestionAnswering`] as follows (shown here for\n ...         optimizer.step()\n ```\n \n-\n ## Usage: inference\n \n-\n Here we explain how you can use [`TapasForQuestionAnswering`] for inference (i.e. making predictions on new data). For inference, only `input_ids`, `attention_mask` and `token_type_ids` (which you can obtain using [`TapasTokenizer`]) have to be provided to the model to obtain the logits. Next, you can use the handy [`~models.tapas.tokenization_tapas.convert_logits_to_predictions`] method to convert these into predicted coordinates and optional aggregation indices.\n \n However, note that inference is **different** depending on whether or not the setup is conversational. In a non-conversational set-up, inference can be done in parallel on all table-question pairs of a batch. Here's an example of that:\n@@ -333,7 +326,6 @@ What is the total number of movies?\n Predicted answer: SUM > 87, 53, 69\n ```\n \n-\n In case of a conversational set-up, then each table-question pair must be provided **sequentially** to the model, such that the `prev_labels` token types can be overwritten by the predicted `labels` of the previous table-question pair. Again, more info can be found in [this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb).\n \n ## Resources"
        },
        {
            "sha": "c986b17dbff06f7f198ea499fdfec3ba2eabd146",
            "filename": "docs/source/en/model_doc/textnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftextnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftextnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftextnet.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -34,7 +34,7 @@ This model was contributed by [Raghavan](https://huggingface.co/Raghavan), [jade\n \n ## Usage tips\n \n-TextNet is mainly used as a backbone network for the architecture search of text detection. Each stage of the backbone network is comprised of a stride-2 convolution and searchable blocks. \n+TextNet is mainly used as a backbone network for the architecture search of text detection. Each stage of the backbone network is comprised of a stride-2 convolution and searchable blocks.\n Specifically, we present a layer-level candidate set, defined as {conv3×3, conv1×3, conv3×1, identity}. As the 1×3 and 3×1 convolutions have asymmetric kernels and oriented structure priors, they may help to capture the features of extreme aspect-ratio and rotated text lines.\n \n TextNet is the backbone for Fast, but can also be used as an efficient text/image classification, we add a `TextNetForImageClassification` as is it would allow people to train an image classifier on top of the pre-trained textnet weights\n@@ -62,4 +62,3 @@ TextNet is the backbone for Fast, but can also be used as an efficient text/imag\n \n [[autodoc]] TextNetForImageClassification\n     - forward\n-"
        },
        {
            "sha": "921b7e01d4b64df00038dfea759e661207da7107",
            "filename": "docs/source/en/model_doc/time_series_transformer.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftime_series_transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftime_series_transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftime_series_transformer.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -61,7 +61,6 @@ A list of official Hugging Face and community (indicated by 🌎) resources to h\n \n - Check out the Time Series Transformer blog-post in HuggingFace blog: [Probabilistic Time Series Forecasting with 🤗 Transformers](https://huggingface.co/blog/time-series-transformers)\n \n-\n ## TimeSeriesTransformerConfig\n \n [[autodoc]] TimeSeriesTransformerConfig"
        },
        {
            "sha": "e8938202ee9e8a5a289dfae262b6ac24ba1dc4fc",
            "filename": "docs/source/en/model_doc/timesfm.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimesfm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimesfm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimesfm.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -25,16 +25,13 @@ rendered properly in your Markdown viewer.\n \n TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model proposed in [A decoder-only foundation model for time-series forecasting](https://huggingface.co/papers/2310.10688) by Abhimanyu Das, Weihao Kong, Rajat Sen, and  Yichen Zhou. It is a decoder only model that uses non-overlapping patches of time-series data as input and outputs some output patch length prediction in an autoregressive fashion.\n \n-\n The abstract from the paper is the following:\n \n *Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.*\n \n-\n This model was contributed by [kashif](https://huggingface.co/kashif).\n The original code can be found [here](https://github.com/google-research/timesfm).\n \n-\n To use the model:\n \n ```python"
        },
        {
            "sha": "0bd1b0f57e1dd91b1e5ad84a59f5bc5a9635218b",
            "filename": "docs/source/en/model_doc/transfo-xl.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftransfo-xl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftransfo-xl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftransfo-xl.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -90,7 +90,6 @@ This model was contributed by [thomwolf](https://huggingface.co/thomwolf). The o\n - Basically, the hidden states of the previous segment are concatenated to the current input to compute the attention scores. This allows the model to pay attention to information that was in the previous segment as well as the current one. By stacking multiple attention layers, the receptive field can be increased to multiple previous segments.\n - This changes the positional embeddings to positional relative embeddings (as the regular positional embeddings would give the same results in the current input and the current hidden state at a given position) and needs to make some adjustments in the way attention scores are computed.\n \n-\n <Tip warning={true}>\n \n TransformerXL does **not** work with *torch.nn.DataParallel* due to a bug in PyTorch, see [issue #36035](https://github.com/pytorch/pytorch/issues/36035)"
        },
        {
            "sha": "da5c71edde36f92dca1de47ebf4f85c54deb9d8a",
            "filename": "docs/source/en/model_doc/trocr.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftrocr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftrocr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftrocr.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -14,8 +14,6 @@ rendered properly in your Markdown viewer.\n specific language governing permissions and limitations under the License. -->\n *This model was released on 2021-09-21 and added to Hugging Face Transformers on 2021-10-13.*\n \n-\n-\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n            <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n@@ -32,13 +30,11 @@ You can find all the original TrOCR checkpoints under the [Microsoft](https://hu\n alt=\"drawing\" width=\"600\"/>\n <small> TrOCR architecture. Taken from the <a href=\"https://huggingface.co/papers/2109.10282\">original paper</a>. </small>\n \n-\n > [!TIP]\n > This model was contributed by [nielsr](https://huggingface.co/nielsr).\n >\n > Click on the TrOCR models in the right sidebar for more examples of how to apply TrOCR to different image and text tasks.\n \n-\n The example below demonstrates how to perform optical character recognition (OCR) with the [`AutoModel`] class.\n \n <hfoptions id=\"usage\">\n@@ -113,7 +109,6 @@ print(generated_text)\n - A notebook on [inference with TrOCR](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Inference_with_TrOCR_%2B_Gradio_demo.ipynb) and Gradio demo.\n - A notebook on [evaluating TrOCR on the IAM test set](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Evaluating_TrOCR_base_handwritten_on_the_IAM_test_set.ipynb).\n \n-\n ## TrOCRConfig\n \n [[autodoc]] TrOCRConfig"
        },
        {
            "sha": "2df4da02555a068a4f0ab16d78f6ea6b1b5ecf96",
            "filename": "docs/source/en/model_doc/tvp.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvp.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -47,6 +47,7 @@ The [`TvpProcessor`] wraps [`BertTokenizer`] and [`TvpImageProcessor`] into a si\n encode the text and prepare the images respectively.\n \n The following example shows how to run temporal video grounding using [`TvpProcessor`] and [`TvpForVideoGrounding`].\n+\n ```python\n import av\n import cv2\n@@ -165,7 +166,6 @@ Tips:\n - Checkpoints for pre-trained [tvp-base](https://huggingface.co/Intel/tvp-base) is released.\n - Please refer to [Table 2](https://huggingface.co/papers/2303.04995) for TVP's performance on Temporal Video Grounding task.\n \n-\n ## TvpConfig\n \n [[autodoc]] TvpConfig"
        },
        {
            "sha": "784cc9974df192e2a78878b5e2c988932005ad6a",
            "filename": "docs/source/en/model_doc/umt5.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fumt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fumt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fumt5.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -39,7 +39,7 @@ Google has released the following variants:\n This model was contributed by [agemagician](https://huggingface.co/agemagician) and [stefan-it](https://huggingface.co/stefan-it). The original code can be\n found [here](https://github.com/google-research/t5x).\n \n-## Usage tips \n+## Usage tips\n \n - UMT5 was only pre-trained on [mC4](https://huggingface.co/datasets/mc4) excluding any supervised training.\n Therefore, this model has to be fine-tuned before it is usable on a downstream task, unlike the original T5 model.\n@@ -67,7 +67,7 @@ The conversion script is also different because the model was saved in t5x's lat\n ['<pad><extra_id_0>nyone who<extra_id_1> drink<extra_id_2> a<extra_id_3> alcohol<extra_id_4> A<extra_id_5> A. This<extra_id_6> I<extra_id_7><extra_id_52><extra_id_53></s>']\n ```\n \n-<Tip> \n+<Tip>\n \n Refer to [T5's documentation page](t5) for more tips, code examples and notebooks.\n </Tip>\n@@ -105,4 +105,3 @@ Refer to [T5's documentation page](t5) for more tips, code examples and notebook\n \n [[autodoc]] UMT5ForQuestionAnswering\n     - forward\n-"
        },
        {
            "sha": "7a5806928335166c63c6f8646c46e55cba44530f",
            "filename": "docs/source/en/model_doc/univnet.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Funivnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Funivnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Funivnet.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -69,7 +69,6 @@ write(\"sample_audio.wav\", feature_extractor.sampling_rate, audio)\n This model was contributed by [dg845](https://huggingface.co/dg845).\n To the best of my knowledge, there is no official code release, but an unofficial implementation can be found at [maum-ai/univnet](https://github.com/maum-ai/univnet) with pretrained checkpoints [here](https://github.com/maum-ai/univnet#pre-trained-model).\n \n-\n ## UnivNetConfig\n \n [[autodoc]] UnivNetConfig"
        },
        {
            "sha": "0a4ded430211a2328b05c04968eaed0237e44747",
            "filename": "docs/source/en/model_doc/van.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvan.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvan.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvan.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -74,4 +74,3 @@ If you're interested in submitting a resource to be included here, please feel f\n \n [[autodoc]] VanForImageClassification\n     - forward\n-"
        },
        {
            "sha": "9d39a5eb7ee32a0bd48c51d6f449423db745f11a",
            "filename": "docs/source/en/model_doc/vaultgemma.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvaultgemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvaultgemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvaultgemma.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -12,7 +12,6 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n \n-\n ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n \n -->\n@@ -45,7 +44,6 @@ command line.\n <hfoptions id=\"usage\">\n <hfoption id=\"Pipeline\">\n \n-\n ```python\n from transformers import pipeline\n "
        },
        {
            "sha": "5b792b33733f539f417cdd66f091e7dfaaf64254",
            "filename": "docs/source/en/model_doc/video_llava.md",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -27,7 +27,6 @@ rendered properly in your Markdown viewer.\n \n Video-LLaVa is an open-source multimodal LLM trained by fine-tuning LlamA/Vicuna on multimodal instruction-following data generated by Llava1.5 and VideChat. It is an auto-regressive language model, based on the transformer architecture. Video-LLaVa unifies visual representations to the language feature space, and enables an LLM to perform visual reasoning capabilities on both images and videos simultaneously.\n \n-\n The Video-LLaVA model was proposed in [Video-LLaVA: Learning United Visual Representation by Alignment Before Projection](https://huggingface.co/papers/2311.10122) by Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munang Ning, Peng Jin, Li Yuan.\n \n The abstract from the paper is the following:\n@@ -55,18 +54,16 @@ for the LLM*\n \n - Note the model has not been explicitly trained to process multiple images/videos in the same prompt, although this is technically possible, you may experience inaccurate results.\n \n-- Note that the video inputs should have exactly 8 frames at the input, since the models were trained in that setting. \n+- Note that the video inputs should have exactly 8 frames at the input, since the models were trained in that setting.\n \n This model was contributed by [RaushanTurganbay](https://huggingface.co/RaushanTurganbay).\n The original code can be found [here](https://github.com/PKU-YuanGroup/Video-LLaVA).\n \n-\n > [!NOTE]\n > LLaVA models after release v4.46 will raise warnings about adding `processor.patch_size = {{patch_size}}`, `processor.num_additional_image_tokens = {{num_additional_image_tokens}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you.\n Adding these attributes means that LLaVA will try to infer the number of image tokens required per image and expand the text with as many `<image>` placeholders as there will be tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.\n The attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n \n-\n ## Usage example\n \n ### Single Media Mode\n@@ -126,7 +123,7 @@ For multiple turns conversation change the prompt format to:\n \n ### Mixed Media Mode\n \n-The model can also generate from an interleaved image-video inputs. However note, that it was not trained in interleaved image-video setting which might affect the performance. Below is an example usage for mixed media input, add the following lines to the above code snippet: \n+The model can also generate from an interleaved image-video inputs. However note, that it was not trained in interleaved image-video setting which might affect the performance. Below is an example usage for mixed media input, add the following lines to the above code snippet:\n \n ```python\n from PIL import Image\n@@ -150,7 +147,7 @@ processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokeniza\n \n ### Quantization using Bitsandbytes for memory efficiency\n \n-The model can be loaded in lower bits, significantly reducing memory burden while maintaining the performance of the original model. his allows for efficient deployment on resource-constrained cases. \n+The model can be loaded in lower bits, significantly reducing memory burden while maintaining the performance of the original model. his allows for efficient deployment on resource-constrained cases.\n \n First make sure to install bitsandbytes by running `pip install bitsandbytes` and to have access to a GPU/accelerator that is supported by the library.\n \n@@ -164,7 +161,6 @@ We value your feedback to help identify bugs before the full release! Check out\n \n Load the quantized model by simply adding [`BitsAndBytesConfig`](../main_classes/quantization#transformers.BitsAndBytesConfig) as shown below:\n \n-\n ```python\n from transformers import VideoLlavaForConditionalGeneration, BitsAndBytesConfig\n \n@@ -178,7 +174,6 @@ quantization_config = BitsAndBytesConfig(\n model = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", quantization_config=quantization_config, device_map=\"auto\")\n ```\n \n-\n ### Flash-Attention 2 to speed-up generation\n \n Additionally, we can greatly speed-up model inference by using [Flash Attention](../perf_train_gpu_one#flash-attention-2), which is a faster implementation of the attention mechanism used inside the model.\n@@ -203,7 +198,6 @@ model = VideoLlavaForConditionalGeneration.from_pretrained(\n ).to(0)\n ```\n \n-\n ## VideoLlavaConfig\n \n [[autodoc]] VideoLlavaConfig\n@@ -212,7 +206,6 @@ model = VideoLlavaForConditionalGeneration.from_pretrained(\n \n [[autodoc]] VideoLlavaImageProcessor\n \n-\n ## VideoLlavaVideoProcessor\n \n [[autodoc]] VideoLlavaVideoProcessor"
        },
        {
            "sha": "44fc8b8b5beba1d5100ef986672155321cc50eaa",
            "filename": "docs/source/en/model_doc/videomae.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -42,13 +42,13 @@ The original code can be found [here](https://github.com/MCG-NJU/VideoMAE).\n \n ## Using Scaled Dot Product Attention (SDPA)\n \n-PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n-encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n-[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n page for more information.\n \n-SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set \n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n \n ```"
        },
        {
            "sha": "fc4aec6ae9b109e2e60099568a776ca179482089",
            "filename": "docs/source/en/model_doc/vipllava.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvipllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvipllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvipllava.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -37,7 +37,6 @@ The original code can be found [here](https://github.com/mu-cai/ViP-LLaVA).\n \n This model was contributed by [Younes Belkada](https://huggingface.co/ybelkada)\n \n-\n ## Usage tips:\n \n - The architecture is similar than llava architecture except that the multi-modal projector takes a set of concatenated vision hidden states and has an additional layernorm layer on that module.\n@@ -51,7 +50,6 @@ This model was contributed by [Younes Belkada](https://huggingface.co/ybelkada)\n Adding these attributes means that LLaVA will try to infer the number of image tokens required per image and expand the text with as many `<image>` placeholders as there will be tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.\n The attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n \n-\n - For better results, we recommend users to use the processor's `apply_chat_template()` method to format your prompt correctly. For that you need to construct a conversation history, passing in a plain string will not format your prompt. Each message in the conversation history for chat templates is a dictionary with keys \"role\" and \"content\". The \"content\" should be a list of dictionaries, for \"text\" and \"image\" modalities, as follows:\n \n ```python\n@@ -88,16 +86,17 @@ print(text_prompt)\n ```\n \n - If you want to construct a chat prompt yourself, below is a list of prompt formats accepted by VipLLaVa checkpoints:\n+\n ```bash\n A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: <image>\\n<prompt>###Assistant:\n ```\n \n For multiple turns conversation:\n+\n ```bash\n A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: <image>\\n<prompt1>###Assistant: <answer1>###Human: <prompt2>###Assistant:\n ```\n \n-\n ## VipLlavaConfig\n \n [[autodoc]] VipLlavaConfig"
        },
        {
            "sha": "a9912144c4f9fe6039752932138a77173e9a069a",
            "filename": "docs/source/en/model_doc/visual_bert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvisual_bert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvisual_bert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvisual_bert.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -27,7 +27,6 @@ rendered properly in your Markdown viewer.\n \n You can find all the original VisualBERT checkpoints under the [UCLA NLP](https://huggingface.co/uclanlp/models?search=visualbert) organization.\n \n-\n > [!TIP]\n > This model was contributed by [gchhablani](https://huggingface.co/gchhablani).\n > Click on the VisualBERT models in the right sidebar for more examples of how to apply VisualBERT to different image and language tasks."
        },
        {
            "sha": "15fa6fad474959835c8733b2dca0fb9de0f0c899",
            "filename": "docs/source/en/model_doc/vit_hybrid.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_hybrid.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_hybrid.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_hybrid.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -55,13 +55,13 @@ found [here](https://github.com/google-research/vision_transformer).\n \n ## Using Scaled Dot Product Attention (SDPA)\n \n-PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n-encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n-[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n page for more information.\n \n-SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set \n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n \n ```"
        },
        {
            "sha": "1099019a842e9a0ff716ea323b9c0efd56ec1b67",
            "filename": "docs/source/en/model_doc/vit_mae.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -15,7 +15,6 @@ rendered properly in your Markdown viewer.\n -->\n *This model was released on 2021-11-11 and added to Hugging Face Transformers on 2022-01-18.*\n \n-\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">"
        },
        {
            "sha": "6d10dd59a994b3c30f72adb924bcc619e439f15d",
            "filename": "docs/source/en/model_doc/vit_msn.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_msn.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_msn.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_msn.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -40,11 +40,11 @@ while producing representations of a high semantic level that perform competitiv\n on ImageNet-1K, with only 5,000 annotated images, our base MSN model achieves 72.4% top-1 accuracy,\n and with 1% of ImageNet-1K labels, we achieve 75.7% top-1 accuracy, setting a new state-of-the-art for self-supervised learning on this benchmark.*\n \n-<img src=\"https://i.ibb.co/W6PQMdC/Screenshot-2022-09-13-at-9-08-40-AM.png\" alt=\"drawing\" width=\"600\"/> \n+<img src=\"https://i.ibb.co/W6PQMdC/Screenshot-2022-09-13-at-9-08-40-AM.png\" alt=\"drawing\" width=\"600\"/>\n \n <small> MSN architecture. Taken from the <a href=\"https://huggingface.co/papers/2204.07141\">original paper.</a> </small>\n \n-This model was contributed by [sayakpaul](https://huggingface.co/sayakpaul). The original code can be found [here](https://github.com/facebookresearch/msn). \n+This model was contributed by [sayakpaul](https://huggingface.co/sayakpaul). The original code can be found [here](https://github.com/facebookresearch/msn).\n \n ## Usage tips\n \n@@ -58,13 +58,13 @@ labels when fine-tuned.\n \n ### Using Scaled Dot Product Attention (SDPA)\n \n-PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n-encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n-[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n page for more information.\n \n-SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set \n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n \n ```"
        },
        {
            "sha": "664edcb92ae8964578862c6a3217a95c2ddb87ca",
            "filename": "docs/source/en/model_doc/vits.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvits.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvits.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvits.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -156,4 +156,3 @@ Audio(waveform, rate=model.config.sampling_rate)\n \n [[autodoc]] VitsModel\n - forward\n-"
        },
        {
            "sha": "9ee5a10a19f182de6de38455f468a5a8d3be1e06",
            "filename": "docs/source/en/model_doc/vivit.md",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -32,13 +32,13 @@ This model was contributed by [jegormeister](https://huggingface.co/jegormeister\n \n ### Using Scaled Dot Product Attention (SDPA)\n \n-PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n-encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n-[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n page for more information.\n \n-SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set \n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n \n ```\n@@ -56,16 +56,13 @@ On a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with `float32`\n |---------------------:|-------------:|----------:|--------------:|----------------------:|---------------------:|-----------------:|\n |                  100 |            1 |      True |         7.122 |               2575.28 |              5932.54 |           130.364 |\n \n-\n-\n ### Inference\n |   num_batches |   batch_size |   is cuda |   is half |   Speedup (%) |   Mem eager (MB) |   Mem BT (MB) |   Mem saved (%) |\n |---------------|--------------|-----------|-----------|---------------|------------------|---------------|-----------------|\n |            20 |             1 |   True    |   False   |      15.422   |     715.807      |    317.079    |      125.75     |\n |            20 |             2 |   True    |   False   |      17.146   |    1234.75       |    447.175    |      176.122    |\n |            20 |             4 |   True    |   False   |      18.093   |    2275.82       |    709.864    |      220.6      |\n |            20 |             8 |   True    |   False   |      19.284   |    4358.19       |   1233.24     |      253.393    |\n-           \n \n ## VivitConfig\n "
        },
        {
            "sha": "049c7ff98f21051b28b807f74dd4f96c09b3c307",
            "filename": "docs/source/en/model_doc/vjepa2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -15,7 +15,6 @@ rendered properly in your Markdown viewer.\n -->\n *This model was released on 2025-06-11 and added to Hugging Face Transformers on 2025-06-11.*\n \n-\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n@@ -34,7 +33,6 @@ rendered properly in your Markdown viewer.\n \n You can find all original V-JEPA2 checkpoints under the [V-JEPA 2](https://huggingface.co/collections/facebook/v-jepa-2-6841bad8413014e185b497a6) collection.\n \n-\n This model was contributed by [koustuvs](https://huggingface.co/koustuvs), [yonigozlan](https://huggingface.co/yonigozlan) and [qubvel](https://huggingface.co/qubvel-hf). The original code can be found [here](https://github.com/facebookresearch/vjepa2).\n \n ## Usage example"
        },
        {
            "sha": "56fc84d30d0d82172d5d5b13c8bc92cf12572a91",
            "filename": "docs/source/en/model_doc/voxtral.md",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -43,6 +43,7 @@ Voxtral builds on Ministral-3B by adding audio processing capabilities:\n The model supports audio-text instructions, including multi-turn and multi-audio interactions, all processed in batches.\n \n ➡️ audio + text instruction\n+\n ```python\n import torch\n from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device\n@@ -78,7 +79,8 @@ print(decoded_outputs[0])\n print(\"=\" * 80)\n ```\n \n-➡️ multi-audio + text instruction \n+➡️ multi-audio + text instruction\n+\n ```python\n import torch\n from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device\n@@ -119,6 +121,7 @@ print(\"=\" * 80)\n ```\n \n ➡️ multi-turn:\n+\n ```python\n import torch\n from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device\n@@ -173,6 +176,7 @@ print(\"=\" * 80)\n ```\n \n ➡️ text only:\n+\n ```python\n import torch\n from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device\n@@ -208,6 +212,7 @@ print(\"=\" * 80)\n ```\n \n ➡️ audio only:\n+\n ```python\n import torch\n from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device\n@@ -243,6 +248,7 @@ print(\"=\" * 80)\n ```\n \n ➡️ batched inference!\n+\n ```python\n import torch\n from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device()"
        },
        {
            "sha": "4a2c8de89c3cd7c8c4967dda67db568022c2cd2b",
            "filename": "docs/source/en/model_doc/wav2vec2-bert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2-bert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2-bert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2-bert.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -54,7 +54,6 @@ This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The o\n - [`Wav2Vec2BertForSequenceClassification`] can be used by adapting this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification).\n - See also: [Audio classification task guide](../tasks/audio_classification)\n \n-\n ## Wav2Vec2BertConfig\n \n [[autodoc]] Wav2Vec2BertConfig"
        },
        {
            "sha": "663b6163011bdb5a7e380acbbec777640fa8f2ec",
            "filename": "docs/source/en/model_doc/wav2vec2-conformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2-conformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2-conformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2-conformer.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -38,7 +38,7 @@ Note: Meta (FAIR) released a new version of [Wav2Vec2-BERT 2.0](https://huggingf\n \n - Wav2Vec2-Conformer follows the same architecture as Wav2Vec2, but replaces the *Attention*-block with a *Conformer*-block\n   as introduced in [Conformer: Convolution-augmented Transformer for Speech Recognition](https://huggingface.co/papers/2005.08100).\n-- For the same number of layers, Wav2Vec2-Conformer requires more parameters than Wav2Vec2, but also yields \n+- For the same number of layers, Wav2Vec2-Conformer requires more parameters than Wav2Vec2, but also yields\n an improved word error rate.\n - Wav2Vec2-Conformer uses the same tokenizer and feature extractor as Wav2Vec2.\n - Wav2Vec2-Conformer can use either no relative position embeddings, Transformer-XL-like position embeddings, or"
        },
        {
            "sha": "1f5f4a9057673853e4d789434dacf05d5bb1779a",
            "filename": "docs/source/en/model_doc/wav2vec2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -80,13 +80,10 @@ model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\",\n \n Below is an expected speedup diagram comparing the pure inference time between the native implementation in transformers of the `facebook/wav2vec2-large-960h-lv60-self` model and the flash-attention-2 and sdpa (scale-dot-product-attention) versions. . We show the average speedup obtained on the `librispeech_asr` `clean` validation split:\n \n-\n <div style=\"text-align: center\">\n <img src=\"https://huggingface.co/datasets/kamilakesbi/transformers_image_doc/resolve/main/data/Wav2Vec2_speedup.png\">\n </div>\n \n-\n-\n ## Resources\n \n A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Wav2Vec2. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource."
        },
        {
            "sha": "c2621f8924c34c5c882a97580ce53df222b81f46",
            "filename": "docs/source/en/model_doc/wav2vec2_phoneme.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2_phoneme.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2_phoneme.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2_phoneme.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -53,7 +53,6 @@ The original code can be found [here](https://github.com/pytorch/fairseq/tree/ma\n - By default, the model outputs a sequence of phonemes. In order to transform the phonemes to a sequence of words one\n   should make use of a dictionary and language model.\n \n-\n <Tip>\n \n Wav2Vec2Phoneme's architecture is based on the Wav2Vec2 model, for API reference, check out [`Wav2Vec2`](wav2vec2)'s documentation page"
        },
        {
            "sha": "5e19e870bddc6617f735c39fe28cb194c6ef92c6",
            "filename": "docs/source/en/model_doc/whisper.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -15,7 +15,6 @@ rendered properly in your Markdown viewer.\n -->\n *This model was released on 2022-12-06 and added to Hugging Face Transformers on 2022-10-05.*\n \n-\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">"
        },
        {
            "sha": "ca6d6e473fc129b91bd443f9420cebf434810d97",
            "filename": "docs/source/en/model_doc/xcodec.md",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fxcodec.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fxcodec.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxcodec.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -33,7 +33,7 @@ The X-Codec model is a neural audio codec that integrates semantic information f\n \n The abstract of the paper states the following:\n \n-*Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation.* \n+*Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation.*\n \n Model cards:\n - [xcodec-hubert-librispeech](https://huggingface.co/hf-audio/xcodec-hubert-librispeech) (for speech)\n@@ -46,12 +46,11 @@ This model was contributed by [Manal El Aidouni](https://huggingface.co/Manel).\n \n Demos can be found on this [page](https://x-codec-audio.github.io/).\n \n-\n-## Usage example \n+## Usage example\n \n Here is a quick example of how to encode and decode an audio using this model:\n \n-```python \n+```python\n from datasets import load_dataset, Audio\n from transformers import XcodecModel, AutoFeatureExtractor\n dummy_dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n@@ -75,6 +74,7 @@ audio_values = decoder_outputs.audio_values\n audio_values = model(inputs[\"input_values\"]).audio_values\n \n ```\n+\n To listen to the original and reconstructed audio, run the snippet below and then open the generated `original.wav` and `reconstruction.wav` files in your music player to compare.\n \n ```python\n@@ -88,12 +88,10 @@ sf.write(\"original.wav\", original, sampling_rate)\n sf.write(\"reconstruction.wav\", reconstruction.T, sampling_rate)\n ```\n \n-\n ## XcodecConfig\n \n [[autodoc]] XcodecConfig\n \n-\n ## XcodecModel\n \n [[autodoc]] XcodecModel"
        },
        {
            "sha": "370055c90ea03de383a8ca0aaa6461bd7e83a1cd",
            "filename": "docs/source/en/model_doc/xglm.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fxglm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fxglm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxglm.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -44,7 +44,6 @@ showing in particular that it enables cross-lingual in-context learning on some\n on surface form robustness and adaptation to tasks that do not have a natural cloze form. Finally, we evaluate our models\n in social value tasks such as hate speech detection in five languages and find it has limitations similar to comparable sized GPT-3 models.*\n \n-\n This model was contributed by [Suraj](https://huggingface.co/valhalla). The original code can be found [here](https://github.com/pytorch/fairseq/tree/main/examples/xglm).\n \n ## Resources\n@@ -67,7 +66,6 @@ This model was contributed by [Suraj](https://huggingface.co/valhalla). The orig\n \n [[autodoc]] XGLMTokenizerFast\n \n-\n ## XGLMModel\n \n [[autodoc]] XGLMModel"
        },
        {
            "sha": "fbf47d8c422a001bbbf91cf6654265eb46b8c725",
            "filename": "docs/source/en/model_doc/xlm-prophetnet.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-prophetnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-prophetnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-prophetnet.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -41,7 +41,6 @@ You can do so by running the following command: `pip install -U transformers==4.\n **DISCLAIMER:** If you see something strange, file a [Github Issue](https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title) and assign\n @patrickvonplaten\n \n-\n ## Overview\n \n The XLM-ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,](https://huggingface.co/papers/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei"
        },
        {
            "sha": "5e1f0bbda2889b5a002abd938350b1fea1d0fd22",
            "filename": "docs/source/en/model_doc/xlm-roberta-xl.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta-xl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta-xl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta-xl.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -77,13 +77,15 @@ predicted_token = tokenizer.decode(predicted_token_id)\n \n print(f\"The predicted token is: {predicted_token}\")\n ```\n+\n </hfoption>\n \n <hfoption id=\"transformers CLI\">\n \n ```bash\n echo -e \"Plants create <mask> through a process known as photosynthesis.\" | transformers run --task fill-mask --model facebook/xlm-roberta-xl --device 0\n ```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "0e9867636892a1022ed7c99006cea3882d993772",
            "filename": "docs/source/en/model_doc/xlm-roberta.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -87,6 +87,7 @@ print(f\"The predicted token is: {predicted_token}\")\n ```bash\n echo -e \"Plants create <mask> through a process known as photosynthesis.\" | transformers run --task fill-mask --model FacebookAI/xlm-roberta-base --device 0\n ```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "ff8f8c46024d105d8347b2e60ce5f9c160d953de",
            "filename": "docs/source/en/model_doc/xlm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -79,6 +79,7 @@ print(f\"Predicted token: {predicted_token}\")\n ```bash\n echo -e \"Plants create <mask> through a process known as photosynthesis.\" | transformers run --task fill-mask --model FacebookAI/xlm-mlm-en-2048 --device 0\n ```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "e1ba3195eccfbe68a8769ea98c2b2574ae25de5b",
            "filename": "docs/source/en/model_doc/xlstm.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlstm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlstm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlstm.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -15,7 +15,6 @@ rendered properly in your Markdown viewer.\n -->\n *This model was released on 2024-05-07 and added to Hugging Face Transformers on 2025-07-25.*\n \n-\n # xLSTM\n \n ## Overview\n@@ -32,7 +31,6 @@ The abstract from the paper is the following:\n This model was contributed by [NX-AI](https://huggingface.co/NX-AI).\n The original code can be found [here](https://github.com/NX-AI/xlstm).\n \n-\n ## xLSTMConfig\n \n [[autodoc]] xLSTMConfig"
        },
        {
            "sha": "666f9674332b65d576c7f0c8bccdbc4d36cfcf26",
            "filename": "docs/source/en/model_doc/yolos.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -26,14 +26,12 @@ rendered properly in your Markdown viewer.\n \n [YOLOS](https://huggingface.co/papers/2106.00666) uses a [Vision Transformer (ViT)](./vit) for object detection with minimal modifications and region priors. It can achieve performance comparable to specialized object detection models and frameworks with knowledge about 2D spatial structures.\n \n-\n You can find all the original YOLOS checkpoints under the [HUST Vision Lab](https://huggingface.co/hustvl/models?search=yolos) organization.\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/yolos_architecture.png\" alt=\"drawing\" width=\"600\"/>\n \n <small> YOLOS architecture. Taken from the <a href=\"https://huggingface.co/papers/2106.00666\">original paper</a>.</small>\n \n-\n > [!TIP]\n > This model wasa contributed by [nielsr](https://huggingface.co/nielsr).\n > Click on the YOLOS models in the right sidebar for more examples of how to apply YOLOS to different object detection tasks.\n@@ -98,7 +96,6 @@ for score, label, box in zip(filtered_scores, filtered_labels, pixel_boxes):\n </hfoption>\n </hfoptions>\n \n-\n ## Notes\n - Use [`YolosImageProcessor`] for preparing images (and optional targets) for the model. Contrary to [DETR](./detr), YOLOS doesn't require a `pixel_mask`.\n "
        },
        {
            "sha": "8e121dd88cddc0c585b35a8450777bf33df098ca",
            "filename": "docs/source/en/model_doc/yoso.md",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fyoso.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fyoso.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fyoso.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -26,20 +26,20 @@ rendered properly in your Markdown viewer.\n The YOSO model was proposed in [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://huggingface.co/papers/2111.09714)  \n by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh. YOSO approximates standard softmax self-attention\n via a Bernoulli sampling scheme based on Locality Sensitive Hashing (LSH). In principle, all the Bernoulli random variables can be sampled with\n-a single hash. \n+a single hash.\n \n The abstract from the paper is the following:\n \n-*Transformer-based models are widely used in natural language processing (NLP). Central to the transformer model is \n-the self-attention mechanism, which captures the interactions of token pairs in the input sequences and depends quadratically \n-on the sequence length. Training such models on longer sequences is expensive. In this paper, we show that a Bernoulli sampling \n-attention mechanism based on Locality Sensitive Hashing (LSH), decreases the quadratic complexity of such models to linear. \n-We bypass the quadratic cost by considering self-attention as a sum of individual tokens associated with Bernoulli random \n-variables that can, in principle, be sampled at once by a single hash (although in practice, this number may be a small constant). \n-This leads to an efficient sampling scheme to estimate self-attention which relies on specific modifications of \n-LSH (to enable deployment on GPU architectures). We evaluate our algorithm on the GLUE benchmark with standard 512 sequence \n-length where we see favorable performance relative to a standard pretrained Transformer. On the Long Range Arena (LRA) benchmark, \n-for evaluating performance on long sequences, our method achieves results consistent with softmax self-attention but with sizable \n+*Transformer-based models are widely used in natural language processing (NLP). Central to the transformer model is\n+the self-attention mechanism, which captures the interactions of token pairs in the input sequences and depends quadratically\n+on the sequence length. Training such models on longer sequences is expensive. In this paper, we show that a Bernoulli sampling\n+attention mechanism based on Locality Sensitive Hashing (LSH), decreases the quadratic complexity of such models to linear.\n+We bypass the quadratic cost by considering self-attention as a sum of individual tokens associated with Bernoulli random\n+variables that can, in principle, be sampled at once by a single hash (although in practice, this number may be a small constant).\n+This leads to an efficient sampling scheme to estimate self-attention which relies on specific modifications of\n+LSH (to enable deployment on GPU architectures). We evaluate our algorithm on the GLUE benchmark with standard 512 sequence\n+length where we see favorable performance relative to a standard pretrained Transformer. On the Long Range Arena (LRA) benchmark,\n+for evaluating performance on long sequences, our method achieves results consistent with softmax self-attention but with sizable\n speed-ups and memory savings and often outperforms other efficient self-attention methods. Our code is available at this https URL*\n \n This model was contributed by [novice03](https://huggingface.co/novice03). The original code can be found [here](https://github.com/mlpen/YOSO).\n@@ -50,12 +50,12 @@ This model was contributed by [novice03](https://huggingface.co/novice03). The o\n in parallel on a GPU.\n - The kernels provide a `fast_hash` function, which approximates the random projections of the queries and keys using the Fast Hadamard Transform. Using these\n hash codes, the `lsh_cumulation` function approximates self-attention via LSH-based Bernoulli sampling.\n-- To use the custom kernels, the user should set `config.use_expectation = False`. To ensure that the kernels are compiled successfully, \n-the user must install the correct version of PyTorch and cudatoolkit. By default, `config.use_expectation = True`, which uses YOSO-E and \n+- To use the custom kernels, the user should set `config.use_expectation = False`. To ensure that the kernels are compiled successfully,\n+the user must install the correct version of PyTorch and cudatoolkit. By default, `config.use_expectation = True`, which uses YOSO-E and\n does not require compiling CUDA kernels.\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/yoso_architecture.jpg\"\n-alt=\"drawing\" width=\"600\"/> \n+alt=\"drawing\" width=\"600\"/>\n \n <small> YOSO Attention Algorithm. Taken from the <a href=\"https://huggingface.co/papers/2111.09714\">original paper</a>.</small>\n "
        },
        {
            "sha": "635bc76fb0ca569476104a42900e46da5c35492e",
            "filename": "docs/source/en/model_doc/zamba.md",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -24,7 +24,6 @@ rendered properly in your Markdown viewer.\n \n This model was contributed by [pglo](https://huggingface.co/pglo).\n \n-\n ## Model details\n \n Zamba-7B-v1 is a hybrid between state-space models (Specifically [Mamba](https://github.com/state-spaces/mamba)) and transformer, and was trained using next-token prediction. Zamba uses a shared transformer layer after every 6 mamba blocks. It uses the [Mistral v0.1 tokenizer](https://huggingface.co/mistralai/Mistral-7B-v0.1). We came to this architecture after a series of ablations at small scales. Zamba-7B-v1 was pre-trained on 1T tokens of text and code data.\n@@ -33,23 +32,24 @@ Zamba-7B-v1 is a hybrid between state-space models (Specifically [Mamba](https:/\n \n ## Quick start\n \n-\n ### Presequities\n \n Zamba requires you use `transformers` version 4.46.0 or higher:\n+\n ```bash\n pip install transformers>=4.45.0\n ```\n \n In order to run optimized Mamba implementations, you first need to install `mamba-ssm` and `causal-conv1d`:\n+\n ```bash\n pip install mamba-ssm causal-conv1d>=1.2.0\n ```\n+\n You also have to have the model on a CUDA device.\n \n You can run the model not using the optimized Mamba kernels, but it is **not** recommended as it will result in significantly lower latencies. In order to do that, you'll need to specify `use_mamba_kernels=False` when loading the model.\n \n-\n ## Inference\n \n ```python\n@@ -66,39 +66,32 @@ outputs = model.generate(**input_ids, max_new_tokens=100)\n print(tokenizer.decode(outputs[0]))\n ```\n \n-\n ## Model card\n \n The model cards can be found at:\n * [Zamba-7B](https://huggingface.co/Zyphra/Zamba-7B-v1)\n \n-\n ## Issues\n For issues with model output, or community discussion, please use the Hugging Face community [forum](https://huggingface.co/Zyphra/Zamba-7B-v1/discussions)\n \n-\n ## License\n \n The model weights are open-sourced via an Apache 2.0 license.\n \n-\n ## ZambaConfig\n \n [[autodoc]] ZambaConfig\n \n-\n ## ZambaModel\n \n [[autodoc]] ZambaModel\n     - forward\n \n-\n ## ZambaForCausalLM\n \n [[autodoc]] ZambaForCausalLM\n     - forward\n \n-\n ## ZambaForSequenceClassification\n \n [[autodoc]] transformers.ZambaForSequenceClassification"
        },
        {
            "sha": "7296ef1b250028e5382a8d18b704e6a876fcb2bc",
            "filename": "docs/source/en/model_doc/zamba2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba2.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -26,7 +26,6 @@ rendered properly in your Markdown viewer.\n \n This model was contributed by [pglo](https://huggingface.co/pglo).\n \n-\n ## Model details\n \n [Zamba2-1.2B](https://www.zyphra.com/post/zamba2-mini), [Zamba2-2.7B](https://www.zyphra.com/post/zamba2-small) and [Zamba2-7B](https://www.zyphra.com/post/zamba2-7b) are hybrid models combining state-space models (Specifically [Mamba2](https://github.com/state-spaces/mamba)) and transformer, and were trained using next-token prediction. Zamba2 uses shared transformer layers after every 6 mamba blocks. It uses the [Mistral v0.1 tokenizer](https://huggingface.co/mistralai/Mistral-7B-v0.1). We came to this architecture after a series of ablations at small scales. Zamba2-1.2B, Zamba2-2.7B and Zamba2-7B were pre-trained on 2T and 3T tokens, respectively.\n@@ -35,10 +34,10 @@ This model was contributed by [pglo](https://huggingface.co/pglo).\n \n ## Quick start\n \n-\n ### Presequities\n \n Zamba2 requires you use `transformers` version 4.48.0 or higher:\n+\n ```bash\n pip install transformers>=4.48.0\n ```\n@@ -59,41 +58,34 @@ outputs = model.generate(**input_ids, max_new_tokens=100)\n print(tokenizer.decode(outputs[0]))\n ```\n \n-\n ## Model card\n \n The model cards can be found at:\n * [Zamba2-1.2B](https://huggingface.co/Zyphra/Zamba2-1.2B)\n * [Zamba2-2.7B](https://huggingface.co/Zyphra/Zamba2-2.7B)\n * [Zamba2-7B](https://huggingface.co/Zyphra/Zamba2-7B)\n \n-\n ## Issues\n For issues with model output, or community discussion, please use the Hugging Face community [forum](https://huggingface.co/Zyphra/Zamba2-7B/discussions)\n \n-\n ## License\n \n The model weights are open-sourced via an Apache 2.0 license.\n \n-\n ## Zamba2Config\n \n [[autodoc]] Zamba2Config\n \n-\n ## Zamba2Model\n \n [[autodoc]] Zamba2Model\n     - forward\n \n-\n ## Zamba2ForCausalLM\n \n [[autodoc]] Zamba2ForCausalLM\n     - forward\n \n-\n ## Zamba2ForSequenceClassification\n \n [[autodoc]] transformers.Zamba2ForSequenceClassification"
        },
        {
            "sha": "5252d2b4d3674a9b10e95a06a67a7e5b12f8fe3a",
            "filename": "docs/source/en/model_doc/zoedepth.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -15,7 +15,6 @@ rendered properly in your Markdown viewer.\n -->\n *This model was released on 2023-02-23 and added to Hugging Face Transformers on 2024-07-08.*\n \n-\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n            <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n@@ -97,6 +96,7 @@ Image.fromarray(depth.astype(\"uint8\"))\n ## Notes\n \n - In the [original implementation](https://github.com/isl-org/ZoeDepth/blob/edb6daf45458569e24f50250ef1ed08c015f17a7/zoedepth/models/depth_model.py#L131) ZoeDepth performs inference on both the original and flipped images and averages the results. The `post_process_depth_estimation` function handles this by passing the flipped outputs to the optional `outputs_flipped` argument as shown below.\n+\n    ```py\n     with torch.no_grad():\n         outputs = model(pixel_values)\n@@ -107,7 +107,7 @@ Image.fromarray(depth.astype(\"uint8\"))\n             outputs_flipped=outputs_flipped,\n         )\n    ```\n-   \n+\n ## Resources\n - Refer to this [notebook](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ZoeDepth) for an inference example.\n "
        },
        {
            "sha": "9b2e4b4b6225e76e9b7a74361f003105b8c3650d",
            "filename": "docs/source/en/model_memory_anatomy.md",
            "status": "modified",
            "additions": 33,
            "deletions": 37,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_memory_anatomy.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodel_memory_anatomy.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_memory_anatomy.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -16,24 +16,23 @@ limitations under the License.\n \n # Model training anatomy\n \n-To understand performance optimization techniques that one can apply to improve efficiency of model training \n-speed and memory utilization, it's helpful to get familiar with how GPU is utilized during training, and how compute \n+To understand performance optimization techniques that one can apply to improve efficiency of model training\n+speed and memory utilization, it's helpful to get familiar with how GPU is utilized during training, and how compute\n intensity varies depending on an operation performed.\n \n-Let's start by exploring a motivating example of GPU utilization and the training run of a model. For the demonstration, \n-we'll need to install a few libraries: \n+Let's start by exploring a motivating example of GPU utilization and the training run of a model. For the demonstration,\n+we'll need to install a few libraries:\n \n ```bash\n pip install transformers datasets accelerate nvidia-ml-py\n ```\n \n-The `nvidia-ml-py` library allows us to monitor the memory usage of the models from within Python. You might be familiar \n+The `nvidia-ml-py` library allows us to monitor the memory usage of the models from within Python. You might be familiar\n with the `nvidia-smi` command in the terminal - this library allows to access the same information in Python directly.\n \n-Then, we create some dummy data: random token IDs between 100 and 30000 and binary labels for a classifier. \n+Then, we create some dummy data: random token IDs between 100 and 30000 and binary labels for a classifier.\n In total, we get 512 sequences each with length 512 and store them in a [`~datasets.Dataset`] with PyTorch format.\n \n-\n ```py\n >>> import numpy as np\n >>> from datasets import Dataset\n@@ -74,9 +73,9 @@ Let's verify that we start with a free GPU memory:\n GPU memory occupied: 0 MB.\n ```\n \n-That looks good: the GPU memory is not occupied as we would expect before we load any models. If that's not the case on \n-your machine make sure to stop all processes that are using GPU memory. However, not all free GPU memory can be used by \n-the user. When a model is loaded to the GPU the kernels are also loaded, which can take up 1-2GB of memory. To see how \n+That looks good: the GPU memory is not occupied as we would expect before we load any models. If that's not the case on\n+your machine make sure to stop all processes that are using GPU memory. However, not all free GPU memory can be used by\n+the user. When a model is loaded to the GPU the kernels are also loaded, which can take up 1-2GB of memory. To see how\n much it is we load a tiny tensor into the GPU which triggers the kernels to be loaded as well.\n \n ```py\n@@ -92,10 +91,9 @@ We see that the kernels alone take up 1.3GB of GPU memory. Now let's see how muc\n \n ## Load Model\n \n-First, we load the `google-bert/bert-large-uncased` model. We load the model weights directly to the GPU so that we can check \n+First, we load the `google-bert/bert-large-uncased` model. We load the model weights directly to the GPU so that we can check\n how much space just the weights use.\n \n-\n ```py\n >>> from transformers import AutoModelForSequenceClassification\n \n@@ -105,12 +103,11 @@ how much space just the weights use.\n GPU memory occupied: 2631 MB.\n ```\n \n-We can see that the model weights alone take up 1.3 GB of GPU memory. The exact number depends on the specific \n-GPU you are using. Note that on newer GPUs a model can sometimes take up more space since the weights are loaded in an \n-optimized fashion that speeds up the usage of the model. Now we can also quickly check if we get the same result \n+We can see that the model weights alone take up 1.3 GB of GPU memory. The exact number depends on the specific\n+GPU you are using. Note that on newer GPUs a model can sometimes take up more space since the weights are loaded in an\n+optimized fashion that speeds up the usage of the model. Now we can also quickly check if we get the same result\n as with `nvidia-smi` CLI:\n \n-\n ```bash\n nvidia-smi\n ```\n@@ -138,8 +135,8 @@ Tue Jan 11 08:58:05 2022\n +-----------------------------------------------------------------------------+\n ```\n \n-We get the same number as before and you can also see that we are using a V100 GPU with 16GB of memory. So now we can \n-start training the model and see how the GPU memory consumption changes. First, we set up a few standard training \n+We get the same number as before and you can also see that we are using a V100 GPU with 16GB of memory. So now we can\n+start training the model and see how the GPU memory consumption changes. First, we set up a few standard training\n arguments:\n \n ```py\n@@ -154,7 +151,7 @@ default_args = {\n \n <Tip>\n \n- If you plan to run multiple experiments, in order to properly clear the memory between experiments, restart the Python \n+ If you plan to run multiple experiments, in order to properly clear the memory between experiments, restart the Python\n  kernel between experiments.\n \n </Tip>\n@@ -181,9 +178,9 @@ Samples/second: 8.86\n GPU memory occupied: 14949 MB.\n ```\n \n-We see that already a relatively small batch size almost fills up our GPU's entire memory. However, a larger batch size \n+We see that already a relatively small batch size almost fills up our GPU's entire memory. However, a larger batch size\n can often result in faster model convergence or better end performance. So ideally we want to tune the batch size to our\n-model's needs and not to the GPU limitations. What's interesting is that we use much more memory than the size of the model. \n+model's needs and not to the GPU limitations. What's interesting is that we use much more memory than the size of the model.\n To understand a bit better why this is the case let's have a look at a model's operations and memory needs.\n \n ## Anatomy of Model's Operations\n@@ -206,10 +203,9 @@ This knowledge can be helpful to know when analyzing performance bottlenecks.\n \n This summary is derived from [Data Movement Is All You Need: A Case Study on Optimizing Transformers 2020](https://huggingface.co/papers/2007.00072)\n \n-\n ## Anatomy of Model's Memory\n \n-We've seen that training the model uses much more memory than just putting the model on the GPU. This is because there \n+We've seen that training the model uses much more memory than just putting the model on the GPU. This is because there\n are many components during training that use GPU memory. The components on GPU memory are the following:\n \n 1. model weights\n@@ -219,8 +215,8 @@ are many components during training that use GPU memory. The components on GPU m\n 5. temporary buffers\n 6. functionality-specific memory\n \n-A typical model trained in mixed precision with AdamW requires 18 bytes per model parameter plus activation memory. For \n-inference there are no optimizer states and gradients, so we can subtract those. And thus we end up with 6 bytes per \n+A typical model trained in mixed precision with AdamW requires 18 bytes per model parameter plus activation memory. For\n+inference there are no optimizer states and gradients, so we can subtract those. And thus we end up with 6 bytes per\n model parameter for mixed precision inference, plus activation memory.\n \n Let's look at the details.\n@@ -244,29 +240,29 @@ Let's look at the details.\n \n - size depends on many factors, the key ones being sequence length, hidden size and batch size.\n \n-There are the input and output that are being passed and returned by the forward and the backward functions and the \n+There are the input and output that are being passed and returned by the forward and the backward functions and the\n forward activations saved for gradient computation.\n \n **Temporary Memory**\n \n-Additionally, there are all kinds of temporary variables which get released once the calculation is done, but in the \n-moment these could require additional memory and could push to OOM. Therefore, when coding it's crucial to think \n+Additionally, there are all kinds of temporary variables which get released once the calculation is done, but in the\n+moment these could require additional memory and could push to OOM. Therefore, when coding it's crucial to think\n strategically about such temporary variables and sometimes to explicitly free those as soon as they are no longer needed.\n \n **Functionality-specific memory**\n \n-Then, your software could have special memory needs. For example, when generating text using beam search, the software \n+Then, your software could have special memory needs. For example, when generating text using beam search, the software\n needs to maintain multiple copies of inputs and outputs.\n \n **`forward` vs `backward` Execution Speed**\n \n-For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally translates \n-into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually \n-bandwidth-limited, and it’s typical for an activation to have to read more data in the backward than in the forward \n-(e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward, \n+For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally translates\n+into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually\n+bandwidth-limited, and it’s typical for an activation to have to read more data in the backward than in the forward\n+(e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward,\n and writes once, gradInput).\n \n-As you can see, there are potentially a few places where we could save GPU memory or speed up operations. \n-Now that you understand what affects GPU utilization and computation speed, refer to \n-the [Methods and tools for efficient training on a single GPU](perf_train_gpu_one) documentation page to learn about \n-performance optimization techniques. \n+As you can see, there are potentially a few places where we could save GPU memory or speed up operations.\n+Now that you understand what affects GPU utilization and computation speed, refer to\n+the [Methods and tools for efficient training on a single GPU](perf_train_gpu_one) documentation page to learn about\n+performance optimization techniques."
        },
        {
            "sha": "ae5572c0c77afc507c23568a08d1cd8c1b78cf96",
            "filename": "docs/source/en/models.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodels.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fmodels.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodels.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -45,7 +45,6 @@ There are two general types of models you can load:\n 1. A barebones model, like [`AutoModel`] or [`LlamaModel`], that outputs hidden states.\n 2. A model with a specific *head* attached, like [`AutoModelForCausalLM`] or [`LlamaForCausalLM`], for performing specific tasks.\n \n-\n ## Model classes\n \n To get a pretrained model, you need to load the weights into the model. This is done by calling [`~PreTrainedModel.from_pretrained`] which accepts weights from the Hugging Face Hub or a local directory.\n@@ -111,7 +110,6 @@ You need enough memory to hold two copies of the model weights (random and pretr\n \n Transformers reduces some of these memory-related challenges with fast initialization, sharded checkpoints, Accelerate's [Big Model Inference](https://hf.co/docs/accelerate/usage_guides/big_modeling) feature, and supporting lower bit data types.\n \n-\n ### Sharded checkpoints\n \n The [`~PreTrainedModel.save_pretrained`] method automatically shards checkpoints larger than 10GB."
        },
        {
            "sha": "1ab8957f9d7ce4aa9e577df0a67bb1bca88bc3a9",
            "filename": "docs/source/en/perf_train_gaudi.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fperf_train_gaudi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f64354e89a55a456e024b301468da2dbb10e11b3/docs%2Fsource%2Fen%2Fperf_train_gaudi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_gaudi.md?ref=f64354e89a55a456e024b301468da2dbb10e11b3",
            "patch": "@@ -20,14 +20,17 @@ The Intel Gaudi AI accelerator family includes [Intel Gaudi 1](https://habana.ai\n [`TrainingArguments`], [`Trainer`] and [`Pipeline`] detect and set the backend device to `hpu` if an Intel Gaudi device is available. No additional changes are required to enable training and inference on your device.\n \n Some modeling code in Transformers is not optimized for HPU lazy mode. If you encounter any errors, set the environment variable below to use eager mode:\n+\n ```\n PT_HPU_LAZY_MODE=0\n ```\n \n In some cases, you'll also need to enable int64 support to avoid casting issues with long integers:\n+\n ```\n PT_ENABLE_INT64_SUPPORT=1\n ```\n+\n Refer to the [Gaudi docs](https://docs.habana.ai/en/latest/index.html) for more details.\n \n > [!TIP]"
        }
    ],
    "stats": {
        "total": 1769,
        "additions": 675,
        "deletions": 1094
    }
}