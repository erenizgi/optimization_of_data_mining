{
    "author": "AhmedAlmaghz",
    "message": "[i18n-ar] Translated file: `docs/source/ar/tasks/multiple_choice.md` into Arabic (#35199)\n\n* Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: multiple_choice.md\n\n* Update multiple_choice.md\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/multiple_choice.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update _toctree.yml\n\n* Add files via upload\n\n* Update _toctree.yml\n\n---------\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>",
    "sha": "a6256ec0982fee2c57cc41237bff7e64ed4dcda9",
    "files": [
        {
            "sha": "fdb0ca002841c1da77fc63ddc58e30a9d908ec5f",
            "filename": "docs/source/ar/_toctree.yml",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/a6256ec0982fee2c57cc41237bff7e64ed4dcda9/docs%2Fsource%2Far%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/a6256ec0982fee2c57cc41237bff7e64ed4dcda9/docs%2Fsource%2Far%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2F_toctree.yml?ref=a6256ec0982fee2c57cc41237bff7e64ed4dcda9",
            "patch": "@@ -43,12 +43,12 @@\n #       title: Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø³Ø¨Ø¨ÙŠØ©\n #     - local: tasks/masked_language_modeling\n #       title: Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø©\n-#     - local: tasks/translation\n-#       title: Ø§Ù„ØªØ±Ø¬Ù…Ø©\n+    - local: tasks/translation\n+      title: Ø§Ù„ØªØ±Ø¬Ù…Ø©\n     - local: tasks/summarization\n       title: Ø§Ù„ØªÙ„Ø®ÙŠØµ\n-#     - local: tasks/multiple_choice\n-#       title: Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ØªØ¹Ø¯Ø¯\n+    - local: tasks/multiple_choice\n+      title: Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ØªØ¹Ø¯Ø¯\n     title: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©\n #   - isExpanded: false\n #     sections:"
        },
        {
            "sha": "78f98560754f11d79394fb64881f676506a40db0",
            "filename": "docs/source/ar/tasks/multiple_choice.md",
            "status": "added",
            "additions": 452,
            "deletions": 0,
            "changes": 452,
            "blob_url": "https://github.com/huggingface/transformers/blob/a6256ec0982fee2c57cc41237bff7e64ed4dcda9/docs%2Fsource%2Far%2Ftasks%2Fmultiple_choice.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a6256ec0982fee2c57cc41237bff7e64ed4dcda9/docs%2Fsource%2Far%2Ftasks%2Fmultiple_choice.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftasks%2Fmultiple_choice.md?ref=a6256ec0982fee2c57cc41237bff7e64ed4dcda9",
            "patch": "@@ -0,0 +1,452 @@\n+<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ (Multiple choice)\n+\n+[[open-in-colab]]\n+\n+Ù…Ù‡Ù…Ø© Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ ÙˆÙ„ÙƒÙ† Ù…Ø¹ ØªÙˆÙÙŠØ± Ø¹Ø¯Ø© Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…Ø­ØªÙ…Ù„Ø© Ù…Ø¹ Ø³ÙŠØ§Ù‚ØŒ ÙˆÙŠÙØ¯Ø±Ù‘Ø¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©.\n+\n+Ø³ÙŠÙˆØ¶Ø­ Ù„Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ©:\n+\n+1. Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ [BERT](https://huggingface.co/google-bert/bert-base-uncased)  Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ `regular` Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [SWAG](https://huggingface.co/datasets/swag) Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø£ÙØ¶Ù„ Ù…Ù† Ø¨ÙŠÙ† Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù…ØªØ§Ø­Ø© Ù…Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚.\n+2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¶Ø¨ÙˆØ· Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„.\n+\n+Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©:\n+\n+```bash\n+pip install transformers datasets evaluate\n+```\n+\n+Ù†Ø´Ø¬Ø¹Ùƒ Ø¹Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Ø­Ø³Ø§Ø¨ Hugging Face Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø´Ø§Ø±ÙƒØªÙ‡ Ù…Ø¹ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹. Ø¹Ù†Ø¯ Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©ØŒ Ø£Ø¯Ø®Ù„ Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ù…Ù…ÙŠØ² Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„:\n+\n+```py\n+>>> from huggingface_hub import notebook_login\n+\n+>>> notebook_login()\n+```\n+\n+## ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª SWAG\n+\n+Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ ØªÙ‡ÙŠØ¦Ø© `regular` Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª SWAG Ù…Ù† Ù…ÙƒØªØ¨Ø© ğŸ¤— Datasets:\n+\n+```py\n+>>> from datasets import load_dataset\n+\n+>>> swag = load_dataset(\"swag\", \"regular\")\n+```\n+\n+Ø«Ù… Ø£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„:\n+\n+```py\n+>>> swag[\"train\"][0]\n+{'ending0': 'passes by walking down the street playing their instruments.',\n+ 'ending1': 'has heard approaching them.',\n+ 'ending2': \"arrives and they're outside dancing and asleep.\",\n+ 'ending3': 'turns the lead singer watches the performance.',\n+ 'fold-ind': '3416',\n+ 'gold-source': 'gold',\n+ 'label': 0,\n+ 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n+ 'sent2': 'A drum line',\n+ 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n+ 'video-id': 'anetv_jkn6uvmqwh4'}\n+```\n+\n+Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù† Ø§Ù„Ø­Ù‚ÙˆÙ„ ØªØ¨Ø¯Ùˆ ÙƒØ«ÙŠØ±Ø©ØŒ Ø¥Ù„Ø§ Ø£Ù†Ù‡Ø§ ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø¨Ø³ÙŠØ·Ø© Ø¬Ø¯Ø§Ù‹:\n+\n+- `sent1` Ùˆ `sent2`: ÙŠØ¹Ø±Ø¶ Ù‡Ø°Ø§Ù† Ø§Ù„Ø­Ù‚Ù„Ø§Ù† Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø¬Ù…Ù„Ø©ØŒ ÙˆØ¨Ø¯Ù…Ø¬Ù‡Ù…Ø§ Ù…Ø¹Ù‹Ø§ØŒ Ù†Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø­Ù‚Ù„ `startphrase`.\n+- `ending`: ÙŠÙ‚ØªØ±Ø­ Ù†Ù‡Ø§ÙŠØ© Ù…Ø­ØªÙ…Ù„Ø© Ù„Ù„Ø¬Ù…Ù„Ø©ØŒ ÙˆØ§Ø­Ø¯Ø© Ù…Ù†Ù‡Ø§ ÙÙ‚Ø· Ù‡ÙŠ Ø§Ù„ØµØ­ÙŠØ­Ø©.\n+- `label`: ÙŠØ­Ø¯Ø¯ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©.\n+\n+## Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© (Preprocess)\n+\n+Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù‡ÙŠ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù…ÙØ¬Ø²Ø¦ BERT Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø¯Ø§ÙŠØ§Øª Ø§Ù„Ø¬Ù…Ù„ ÙˆØ§Ù„Ù†Ù‡Ø§ÙŠØ§Øª Ø§Ù„Ø£Ø±Ø¨Ø¹ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©:\n+\n+```py\n+>>> from transformers import AutoTokenizer\n+\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n+```\n+\n+ØªØ­ØªØ§Ø¬ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø¥Ù†Ø´Ø§Ø¡Ù‡Ø§ Ø¥Ù„Ù‰:\n+\n+1.  Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø±Ø¨Ø¹ Ù†Ø³Ø® Ù…Ù† Ø­Ù‚Ù„ `sent1` ÙˆØ¯Ù…Ø¬ ÙƒÙ„ Ù…Ù†Ù‡Ø§ Ù…Ø¹ `sent2` Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ù†Ø´Ø§Ø¡ ÙƒÙŠÙÙŠØ© Ø¨Ø¯Ø¡ Ø§Ù„Ø¬Ù…Ù„Ø©.\n+2. Ø¯Ù…Ø¬ `sent2` Ù…Ø¹ ÙƒÙ„ Ù…Ù† Ù†Ù‡Ø§ÙŠØ§Øª Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø£Ø±Ø¨Ø¹ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©.\n+3. ØªØªØ¬Ù…ÙŠØ¹ Ù‡Ø§ØªÙŠÙ† Ø§Ù„Ù‚Ø§Ø¦Ù…ØªÙŠÙ† Ù„ØªØªÙ…ÙƒÙ† Ù…Ù† ØªØ¬Ø²Ø¦ØªÙ‡Ù…Ø§ØŒ Ø«Ù… Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø¨Ø­ÙŠØ« ÙŠÙƒÙˆÙ† Ù„ÙƒÙ„ Ù…Ø«Ø§Ù„ Ø­Ù‚ÙˆÙ„ `input_ids` Ùˆ `attention_mask` Ùˆ `labels` Ù…Ù‚Ø§Ø¨Ù„Ø©.\n+\n+\n+```py\n+>>> ending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n+\n+>>> def preprocess_function(examples):\n+...     first_sentences = [[context] * 4 for context in examples[\"sent1\"]]\n+...     question_headers = examples[\"sent2\"]\n+...     second_sentences = [\n+...         [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n+...     ]\n+\n+...     first_sentences = sum(first_sentences, [])\n+...     second_sentences = sum(second_sentences, [])\n+\n+...     tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n+...     return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n+```\n+\n+Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© [`~datasets.Dataset.map`] Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ ğŸ¤— Datasets. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ³Ø±ÙŠØ¹ Ø¯Ø§Ù„Ø© `map` Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `batched=True` Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù†Ø§ØµØ± Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ ÙˆÙ‚Øª ÙˆØ§Ø­Ø¯:\n+\n+```py\n+tokenized_swag = swag.map(preprocess_function, batched=True)\n+```\n+\n+Ù„Ø§ ÙŠØ­ØªÙˆÙŠ ğŸ¤— Transformers Ø¹Ù„Ù‰ Ù…Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ØŒ Ù„Ø°Ù„Ùƒ Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙƒÙŠÙŠÙ [`DataCollatorWithPadding`] Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„Ø£Ù…Ø«Ù„Ø©. Ù…Ù† Ø§Ù„Ø£ÙƒÙØ£ Ø¥Ø¶Ø§ÙØ© Ø­Ø´Ùˆ (padding) Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ù„Ù„Ø¬Ù…Ù„ Ø¥Ù„Ù‰ Ø£Ø·ÙˆÙ„ Ø·ÙˆÙ„ ÙÙŠ Ø¯ÙØ¹Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ØŒ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø­Ø´Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø·ÙˆÙ„.\n+\n+ÙŠÙ‚ÙˆÙ… `DataCollatorForMultipleChoice` Ø¨ØªØ¬Ù…ÙŠØ¹ Ø¬Ù…ÙŠØ¹ Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙˆÙŠØ·Ø¨Ù‚ Ø§Ù„Ø­Ø´ÙˆØŒ Ø«Ù… ÙŠØ¹ÙŠØ¯ ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø´ÙƒÙ„Ù‡Ø§ Ø§Ù„Ø£ØµÙ„ÙŠ:\n+\n+<frameworkcontent>\n+<pt>\n+\n+```py\n+>>> from dataclasses import dataclass\n+>>> from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n+>>> from typing import Optional, Union\n+>>> import torch\n+\n+>>> @dataclass\n+... class DataCollatorForMultipleChoice:\n+...     \"\"\"\n+...     Data collator that will dynamically pad the inputs for multiple choice received.\n+...     \"\"\"\n+\n+...     tokenizer: PreTrainedTokenizerBase\n+...     padding: Union[bool, str, PaddingStrategy] = True\n+...     max_length: Optional[int] = None\n+...     pad_to_multiple_of: Optional[int] = None\n+\n+...     def __call__(self, features):\n+...         label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n+...         labels = [feature.pop(label_name) for feature in features]\n+...         batch_size = len(features)\n+...         num_choices = len(features[0][\"input_ids\"])\n+...         flattened_features = [\n+...             [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n+...         ]\n+...         flattened_features = sum(flattened_features, [])\n+\n+...         batch = self.tokenizer.pad(\n+...             flattened_features,\n+...             padding=self.padding,\n+...             max_length=self.max_length,\n+...             pad_to_multiple_of=self.pad_to_multiple_of,\n+...             return_tensors=\"pt\",\n+...         )\n+\n+...         batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n+...         batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n+...         return batch\n+```\n+</pt>\n+<tf>\n+ \n+```py\n+>>> from dataclasses import dataclass\n+>>> from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n+>>> from typing import Optional, Union\n+>>> import tensorflow as tf\n+\n+>>> @dataclass\n+... class DataCollatorForMultipleChoice:\n+...     \"\"\"\n+...     Data collator that will dynamically pad the inputs for multiple choice received.\n+...     \"\"\"\n+\n+...     tokenizer: PreTrainedTokenizerBase\n+...     padding: Union[bool, str, PaddingStrategy] = True\n+...     max_length: Optional[int] = None\n+...     pad_to_multiple_of: Optional[int] = None\n+\n+...     def __call__(self, features):\n+...         label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n+...         labels = [feature.pop(label_name) for feature in features]\n+...         batch_size = len(features)\n+...         num_choices = len(features[0][\"input_ids\"])\n+...         flattened_features = [\n+...             [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n+...         ]\n+...         flattened_features = sum(flattened_features, [])\n+\n+...         batch = self.tokenizer.pad(\n+...             flattened_features,\n+...             padding=self.padding,\n+...             max_length=self.max_length,\n+...             pad_to_multiple_of=self.pad_to_multiple_of,\n+...             return_tensors=\"tf\",\n+...         )\n+\n+...         batch = {k: tf.reshape(v, (batch_size, num_choices, -1)) for k, v in batch.items()}\n+...         batch[\"labels\"] = tf.convert_to_tensor(labels, dtype=tf.int64)\n+...         return batch\n+```\n+</tf>\n+</frameworkcontent>\n+\n+## Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (Evaluate)\n+\n+ÙŠÙÙØ¶Ù„ ØºØ§Ù„Ø¨Ù‹Ø§ ØªØ¶Ù…ÙŠÙ† Ù…Ù‚ÙŠØ§Ø³ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬Ùƒ. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ø·Ø±ÙŠÙ‚Ø© ØªÙ‚ÙŠÙŠÙ… Ø¨Ø³Ø±Ø¹Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index). Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ù‚ÙŠØ§Ø³ [Ø§Ù„Ø¯Ù‚Ø©](https://huggingface.co/spaces/evaluate-metric/accuracy) (Ø§Ù†Ø¸Ø± Ø¥Ù„Ù‰ [Ø§Ù„Ø¬ÙˆÙ„Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©](https://huggingface.co/docs/evaluate/a_quick_tour) Ù„Ù€ ğŸ¤— Evaluate Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ ÙˆØ­Ø³Ø§Ø¨Ù‡):\n+\n+```py\n+>>> import evaluate\n+\n+>>> accuracy = evaluate.load(\"accuracy\")\n+```\n+\n+Ø«Ù… Ø£Ù†Ø´Ø¦ Ø¯Ø§Ù„Ø© Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª ÙˆØ§Ù„ØªØ³Ù…ÙŠØ§Øª Ø¥Ù„Ù‰ [`~evaluate.EvaluationModule.compute`] Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø©:\n+\n+```py\n+>>> import numpy as np\n+\n+>>> def compute_metrics(eval_pred):\n+...     predictions, labels = eval_pred\n+...     predictions = np.argmax(predictions, axis=1)\n+...     return accuracy.compute(predictions=predictions, references=labels)\n+```\n+\n+Ø¯Ø§Ù„ØªÙƒ `compute_metrics` Ø¬Ø§Ù‡Ø²Ø© Ø§Ù„Ø¢Ù†ØŒ ÙˆØ³ØªØ¹ÙˆØ¯ Ø¥Ù„ÙŠÙ‡Ø§ Ø¹Ù†Ø¯ Ø¥Ø¹Ø¯Ø§Ø¯ ØªØ¯Ø±ÙŠØ¨Ùƒ.\n+\n+## Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Train)\n+\n+<frameworkcontent>\n+<pt>\n+\n+<Tip>\n+\n+Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø¹ØªØ§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Trainer`], ÙØ±Ø§Ø¬Ø¹ Ø§Ù„Ø¯Ø±Ø³ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-with-pytorch-trainer)!\n+\n+</Tip>\n+\n+Ø£Ù†Øª Ø¬Ø§Ù‡Ø² Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø§Ù„Ø¢Ù†! Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ BERT Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`AutoModelForMultipleChoice`]:\n+\n+```py\n+>>> from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n+\n+>>> model = AutoModelForMultipleChoice.from_pretrained(\"google-bert/bert-base-uncased\")\n+```\n+\n+ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø­Ù„Ø©ØŒ ØªØ¨Ù‚Ù‰ Ø«Ù„Ø§Ø« Ø®Ø·ÙˆØ§Øª ÙÙ‚Ø·:\n+\n+1. Ø­Ø¯Ø¯ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ÙÙŠ [`TrainingArguments`]. Ø§Ù„Ù…Ø¹Ù„Ù…Ø© Ø§Ù„ÙˆØ­ÙŠØ¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù‡ÙŠ `output_dir` Ø§Ù„ØªÙŠ ØªØ­Ø¯Ø¯ Ù…ÙƒØ§Ù† Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬Ùƒ. Ø³ØªØ¯ÙØ¹ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Hub Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `push_to_hub=True` (ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Hugging Face Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ). ÙÙŠ Ù†Ù‡Ø§ÙŠØ© ÙƒÙ„ Ø­Ù‚Ø¨Ø©ØŒ Ø³ÙŠÙ‚ÙˆÙ… [`Trainer`] Ø¨ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¯Ù‚Ø© ÙˆØ­ÙØ¸ Ù†Ù‚Ø·Ø© ÙØ­Øµ Ø§Ù„ØªØ¯Ø±ÙŠØ¨.\n+2. Ù…Ø±Ø± Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ù„Ù‰ [`Trainer`] Ø¬Ù†Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…ÙØ¬Ù…ÙÙ‘Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬ ÙˆØ¯Ø§Ù„Ø© ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¯Ø§Ù„Ø© `compute_metrics`.\n+3. Ø§Ø³ØªØ¯Ø¹ÙŠ [`~Trainer.train`] Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬Ùƒ.\n+\n+```py\n+>>> training_args = TrainingArguments(\n+...     output_dir=\"my_awesome_swag_model\",\n+...     eval_strategy=\"epoch\",\n+...     save_strategy=\"epoch\",\n+...     load_best_model_at_end=True,\n+...     learning_rate=5e-5,\n+...     per_device_train_batch_size=16,\n+...     per_device_eval_batch_size=16,\n+...     num_train_epochs=3,\n+...     weight_decay=0.01,\n+...     push_to_hub=True,\n+... )\n+\n+>>> trainer = Trainer(\n+...     model=model,\n+...     args=training_args,\n+...     train_dataset=tokenized_swag[\"train\"],\n+...     eval_dataset=tokenized_swag[\"validation\"],\n+...     processing_class=tokenizer,\n+...     data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n+...     compute_metrics=compute_metrics,\n+... )\n+\n+>>> trainer.train()\n+```\n+\n+Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ø´Ø§Ø±Ùƒ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ù…Ø¹ Hub Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.Trainer.push_to_hub`] Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬Ùƒ:\n+\n+```py\n+>>> trainer.push_to_hub()\n+```\n+</pt>\n+<tf>\n+<Tip>\n+\n+Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø¹ØªØ§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ ÙØ±Ø§Ø¬Ø¹ Ø§Ù„Ø¯Ø±Ø³ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-a-tensorflow-model-with-keras)!\n+\n+</Tip>\n+Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ TensorFlowØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ù…ÙØ­Ø³ÙÙ‘Ù† ÙˆØ¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ¨Ø¹Ø¶ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\n+\n+```py\n+>>> from transformers import create_optimizer\n+\n+>>> batch_size = 16\n+>>> num_train_epochs = 2\n+>>> total_train_steps = (len(tokenized_swag[\"train\"]) // batch_size) * num_train_epochs\n+>>> optimizer, schedule = create_optimizer(init_lr=5e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n+```\n+\n+Ø«Ù… ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ BERT Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFAutoModelForMultipleChoice`]:\n+\n+```py\n+>>> from transformers import TFAutoModelForMultipleChoice\n+\n+>>> model = TFAutoModelForMultipleChoice.from_pretrained(\"google-bert/bert-base-uncased\")\n+```\n+\n+Ø­ÙˆÙ‘Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ `tf.data.Dataset` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n+\n+```py\n+>>> data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n+>>> tf_train_set = model.prepare_tf_dataset(\n+...     tokenized_swag[\"train\"],\n+...     shuffle=True,\n+...     batch_size=batch_size,\n+...     collate_fn=data_collator,\n+... )\n+\n+>>> tf_validation_set = model.prepare_tf_dataset(\n+...     tokenized_swag[\"validation\"],\n+...     shuffle=False,\n+...     batch_size=batch_size,\n+...     collate_fn=data_collator,\n+... )\n+```\n+\n+Ù‚Ù… Ø¨ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Ù„Ø§Ø­Ø¸ Ø£Ù† Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Transformers ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù…Ù‡Ù…Ø© Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ù…Ø§ Ù„Ù… ØªØ±ØºØ¨ ÙÙŠ Ø°Ù„Ùƒ:\n+\n+```py\n+>>> model.compile(optimizer=optimizer)  # Ù„Ø§ ØªÙˆØ¬Ø¯ ÙˆØ³ÙŠØ·Ø© Ø®Ø³Ø§Ø±Ø©!\n+```\n+\n+Ø§Ù„Ø®Ø·ÙˆØªØ§Ù† Ø§Ù„Ø£Ø®ÙŠØ±ØªØ§Ù† Ù‚Ø¨Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù‡Ù…Ø§: Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤Ø§ØªØŒ ÙˆØªÙˆÙÙŠØ± Ø·Ø±ÙŠÙ‚Ø© Ù„Ø±ÙØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Hub. ÙˆÙŠÙ…ÙƒÙ† ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Keras](../main_classes/keras_callbacks)\n+\n+Ù…Ø±Ø± Ø¯Ø§Ù„ØªÙƒ `compute_metrics` Ø¥Ù„Ù‰ [`~transformers.KerasMetricCallback`]:\n+\n+```py\n+>>> from transformers.keras_callbacks import KerasMetricCallback\n+\n+>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n+```\n+\n+Ø­Ø¯Ø¯ Ù…ÙƒØ§Ù† Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ùƒ ÙÙŠ [`~transformers.PushToHubCallback`]:\n+\n+```py\n+>>> from transformers.keras_callbacks import PushToHubCallback\n+\n+>>> push_to_hub_callback = PushToHubCallback(\n+...     output_dir=\"my_awesome_model\",\n+...     tokenizer=tokenizer,\n+... )\n+```\n+\n+Ø«Ù… Ù‚Ù… Ø¨ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Ù…Ø¹Ù‹Ø§:\n+\n+```py\n+>>> callbacks = [metric_callback, push_to_hub_callback]\n+```\n+\n+Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ø£Ù†Øª Ø¬Ø§Ù‡Ø² Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ø§Ø³ØªØ¯Ø¹Ù[`fit`](https://keras.io/api/models/model_training_apis/#fit-method) Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø© ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø­Ù‚Ø¨ ÙˆØ§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Ù„Ø¶Ø¨Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\n+\n+```py\n+>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=2, callbacks=callbacks)\n+```\n+\n+Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Hub Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡!\n+</tf>\n+</frameworkcontent>\n+\n+<Tip>\n+\n+Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„ Ø£ÙƒØ«Ø± ØªØ¹Ù…Ù‚Ù‹Ø§ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ØŒ Ø£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ [Ø¯ÙØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª PyTorch](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)\n+Ø£Ùˆ [Ø¯ÙØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª TensorFlow](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb) Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„.\n+\n+</Tip>\n+\n+## Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„  (Inference)\n+\n+Ø±Ø§Ø¦Ø¹ØŒ Ø§Ù„Ø¢Ù† Ø¨Ø¹Ø¯ Ø£Ù† Ù‚Ù…Øª Ø¨Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„!\n+\n+Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ ÙˆØ§Ù‚ØªØ±Ø§Ø­ Ø¥Ø¬Ø§Ø¨ØªÙŠÙ† Ù…Ø­ØªÙ…Ù„ØªÙŠÙ†:\n+\n+```py\n+>>> prompt = \"France has a bread law, Le DÃ©cret Pain, with strict rules on what is allowed in a traditional baguette.\"\n+>>> candidate1 = \"The law does not apply to croissants and brioche.\"\n+>>> candidate2 = \"The law applies to baguettes.\"\n+```\n+\n+<frameworkcontent>\n+<pt>\n+Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ø·Ø§Ù„Ø¨Ø© ÙˆØ²ÙˆØ¬ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø±Ø´Ø­ ÙˆØ£Ø¹Ø¯ ØªÙ†Ø³ÙˆØ±Ø§Øª PyTorch. ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ø¹Ø¶ `Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª`:\n+\n+```py\n+>>> from transformers import AutoTokenizer\n+\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_swag_model\")\n+>>> inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"pt\", padding=True)\n+>>> labels = torch.tensor(0).unsqueeze(0)\n+```\n+\n+Ù…Ø±Ø± Ù…Ø¯Ø®Ù„Ø§ØªÙƒ ÙˆØ§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ£Ø±Ø¬Ø¹`logits`:\n+\n+```py\n+>>> from transformers import AutoModelForMultipleChoice\n+\n+>>> model = AutoModelForMultipleChoice.from_pretrained(\"username/my_awesome_swag_model\")\n+>>> outputs = model(**{k: v.unsqueeze(0) for k, v in inputs.items()}, labels=labels)\n+>>> logits = outputs.logits\n+```\n+\n+Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„ÙØ¦Ø© Ø°Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø£ÙƒØ¨Ø±:\n+\n+```py\n+>>> predicted_class = logits.argmax().item()\n+>>> predicted_class\n+0\n+```\n+</pt>\n+<tf>\n+Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ø·Ø§Ù„Ø¨Ø© ÙˆØ²ÙˆØ¬ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø±Ø´Ø­ ÙˆØ£Ø¹Ø¯ Ù…ÙˆØªØ±Ø§Øª TensorFlow:\n+\n+```py\n+>>> from transformers import AutoTokenizer\n+\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_swag_model\")\n+>>> inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"tf\", padding=True)\n+```\n+\n+Ù…Ø±Ø± Ù…Ø¯Ø®Ù„Ø§ØªÙƒ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ£Ø¹Ø¯ Ø§Ù„Ù‚ÙŠÙ… logits:\n+\n+```py\n+>>> from transformers import TFAutoModelForMultipleChoice\n+\n+>>> model = TFAutoModelForMultipleChoice.from_pretrained(\"username/my_awesome_swag_model\")\n+>>> inputs = {k: tf.expand_dims(v, 0) for k, v in inputs.items()}\n+>>> outputs = model(inputs)\n+>>> logits = outputs.logits\n+```\n+\n+Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„ÙØ¦Ø© Ø°Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø£ÙƒØ¨Ø±:\n+\n+```py\n+>>> predicted_class = int(tf.math.argmax(logits, axis=-1)[0])\n+>>> predicted_class\n+0\n+```\n+</tf>\n+</frameworkcontent>"
        },
        {
            "sha": "6245b903c22d63ef295cff91a98bb505ef6ee2ca",
            "filename": "docs/source/ar/tasks/translation.md",
            "status": "added",
            "additions": 407,
            "deletions": 0,
            "changes": 407,
            "blob_url": "https://github.com/huggingface/transformers/blob/a6256ec0982fee2c57cc41237bff7e64ed4dcda9/docs%2Fsource%2Far%2Ftasks%2Ftranslation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a6256ec0982fee2c57cc41237bff7e64ed4dcda9/docs%2Fsource%2Far%2Ftasks%2Ftranslation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftasks%2Ftranslation.md?ref=a6256ec0982fee2c57cc41237bff7e64ed4dcda9",
            "patch": "@@ -0,0 +1,407 @@\n+<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Ø§Ù„ØªØ±Ø¬Ù…Ø©(Translation)\n+\n+[[open-in-colab]]\n+\n+<Youtube id=\"1JvfrvZgi6c\"/>\n+\n+Ø§Ù„ØªØ±Ø¬Ù…Ø© Ù‡ÙŠ Ø¹Ù…Ù„ÙŠØ© ØªØ­ÙˆÙŠÙ„ Ø³Ù„Ø³Ù„Ø© Ù†ØµÙŠØ© Ù…Ù† Ù„ØºØ© Ø¥Ù„Ù‰ Ø£Ø®Ø±Ù‰. ÙˆÙ‡ÙŠ Ø¥Ø­Ø¯Ù‰ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† ØµÙŠØ§ØºØªÙ‡Ø§ ÙƒÙ…Ø³Ø£Ù„Ø© ØªØ³Ù„Ø³Ù„ Ø¥Ù„Ù‰ ØªØ³Ù„Ø³Ù„ØŒ ÙˆÙ‡Ùˆ Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ Ù‚ÙˆÙŠ Ù„Ø¥Ù†ØªØ§Ø¬ Ù…Ø®Ø±Ø¬Ø§Øª Ù…Ù† Ù…Ø¯Ø®Ù„Ø§ØªØŒ Ù…Ø«Ù„ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø£Ùˆ Ø§Ù„ØªÙ„Ø®ÙŠØµ. ØªÙØ³ØªØ®Ø¯Ù… Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø¹Ø§Ø¯Ø©Ù‹ Ù„Ù„ØªØ±Ø¬Ù…Ø© Ø¨ÙŠÙ† Ù†ØµÙˆØµ Ù„ØºØ§Øª Ù…Ø®ØªÙ„ÙØ©ØŒ ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø£ÙŠØ¶Ù‹Ø§ Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„ÙƒÙ„Ø§Ù… Ø£Ùˆ Ù„Ù…Ù‡Ø§Ù… ØªØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ÙƒÙ„Ø§Ù…ØŒ Ù…Ø«Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø£Ùˆ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¥Ù„Ù‰ Ù†Øµ.\n+\n+Ø³ÙŠÙˆØ¶Ø­ Ù„Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ©:\n+\n+1. Ø¶Ø¨Ø· Ø¯Ù‚ÙŠÙ‚ Ù„Ù†Ù…ÙˆØ°Ø¬ [T5](https://huggingface.co/google-t5/t5-small) Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©-Ø§Ù„ÙØ±Ù†Ø³ÙŠØ© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [OPUS Books](https://huggingface.co/datasets/opus_books) Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ Ø¥Ù„Ù‰ Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©.\n+2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¶Ø¨ÙˆØ· Ø¨Ø¯Ù‚Ø© Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„.\n+\n+<Tip>\n+\n+Ù„Ù…Ø´Ø§Ù‡Ø¯Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨Ù†Ù‰ ÙˆØ§Ù„Ù†Ø³Ø® Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù†ÙˆØµÙŠ Ø¨Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† [ØµÙØ­Ø© Ø§Ù„Ù…Ù‡Ù…Ø©](https://huggingface.co/tasks/translation).\n+\n+</Tip>\n+\n+Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©:\n+\n+```bash\n+pip install transformers datasets evaluate sacrebleu\n+```\n+\n+Ù†Ø´Ø¬Ø¹Ùƒ Ø¹Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Ø­Ø³Ø§Ø¨ Hugging Face Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø´Ø§Ø±ÙƒØªÙ‡ Ù…Ø¹ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹. Ø¹Ù†Ø¯ Ø§Ù„Ø·Ù„Ø¨ØŒ Ø£Ø¯Ø®Ù„ Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ù…Ù…ÙŠØ² Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„:\n+\n+```py\n+>>> from huggingface_hub import notebook_login\n+\n+>>> notebook_login()\n+```\n+\n+## ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª OPUS Books\n+\n+Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©-Ø§Ù„ÙØ±Ù†Ø³ÙŠØ© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [OPUS Books](https://huggingface.co/datasets/opus_books) Ù…Ù† Ù…ÙƒØªØ¨Ø© ğŸ¤— Datasets:\n+\n+```py\n+>>> from datasets import load_dataset\n+\n+>>> books = load_dataset(\"opus_books\", \"en-fr\")\n+```\n+\n+Ù‚Ø³Ù‘Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© ØªØ¯Ø±ÙŠØ¨ ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø© Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~datasets.Dataset.train_test_split`]:\n+\n+```py\n+>>> books = books[\"train\"].train_test_split(test_size=0.2)\n+```\n+\n+Ø«Ù… Ø£Ù„Ù‚Ù Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„:\n+\n+```py\n+>>> books[\"train\"][0]\n+{'id': '90560',\n+ 'translation': {'en': 'But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.',\n+  'fr': 'Mais ce plateau Ã©levÃ© ne mesurait que quelques toises, et bientÃ´t nous fÃ»mes rentrÃ©s dans notre Ã©lÃ©ment.'}}\n+```\n+\n+`translation`: ØªØ±Ø¬Ù…Ø© Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙˆÙØ±Ù†Ø³ÙŠØ© Ù„Ù„Ù†Øµ.\n+\n+## Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø©(Preprocess)\n+\n+<Youtube id=\"XAR8jnZZuUs\"/>\n+\n+Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù‡ÙŠ ØªØ­Ù…ÙŠÙ„ Ù…ÙØ¬Ø²Ø¦ T5 Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©-Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©:\n+\n+```py\n+>>> from transformers import AutoTokenizer\n+\n+>>> checkpoint = \"google-t5/t5-small\"\n+>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n+```\n+\n+ÙŠØ¬Ø¨ Ø£Ù† ØªÙ‚ÙˆÙ… Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø§Ù„ØªÙŠ ØªÙØ±ÙŠØ¯ Ø¥Ù†Ø´Ø§Ø¡Ù‡Ø§ Ø¨Ù…Ø§ ÙŠÙ„ÙŠ:\n+\n+1. Ø¥Ø¶Ø§ÙØ© Ø¨Ø§Ø¯Ø¦Ø© Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙØ¯Ø®Ù„ Ø¨Ù…ÙÙˆØ¬Ù‡ Ø­ØªÙ‰ ÙŠØ¹Ø±Ù T5 Ø£Ù† Ù‡Ø°Ù‡ Ù…Ù‡Ù…Ø© ØªØ±Ø¬Ù…Ø©. ØªØªØ·Ù„Ø¨ Ø¨Ø¹Ø¶ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡ Ù…Ù‡Ø§Ù… Ù…ØªØ¹Ø¯Ø¯Ø© ØªÙˆØ¬ÙŠÙ‡Ù‹Ø§ Ù„Ù…Ù‡Ø§Ù… Ù…ÙØ­Ø¯Ø¯Ø©.\n+2. ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù„ØºØ© Ø§Ù„Ù‡Ø¯Ù (Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©) ÙÙŠ Ù…Ø¹Ø§Ù…Ù„ `text_target` Ù„Ø¶Ù…Ø§Ù† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙØ¬Ø²Ø¦ Ù„Ù„Ù†Øµ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­. Ø¥Ø°Ø§ Ù„Ù… ØªÙØ¹ÙŠÙ‘Ù† `text_target`ØŒ ÙØ³ÙŠÙØ¹Ø§Ù„Ø¬ Ø§Ù„Ù…ÙØ¬Ø²Ø¦ Ø§Ù„Ù†Øµ Ø¹Ù„Ù‰ Ø£Ù†Ù‡ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ.\n+3. Ø§Ù‚ØªØ·Ø§Ø¹ Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª Ø¨Ø­ÙŠØ« Ù„Ø§ ÙŠØ²ÙŠØ¯ Ø·ÙˆÙ„Ù‡Ø§ Ø¹Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ø§Ù„Ø°ÙŠ ÙŠØ­Ø¯Ø¯Ù‡ Ù…Ø¹Ø§Ù…Ù„ `max_length`.\n+\n+```py\n+>>> source_lang = \"en\"\n+>>> target_lang = \"fr\"\n+>>> prefix = \"translate English to French: \"\n+\n+>>> def preprocess_function(examples):\n+...     inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n+...     targets = [example[target_lang] for example in examples[\"translation\"]]\n+...     model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n+...     return model_inputs\n+```\n+\n+Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© [`~datasets.Dataset.map`] Ù…Ù† ğŸ¤— Datasets. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ³Ø±ÙŠØ¹ Ø¯Ø§Ù„Ø© `map` Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `batched=True` Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù†Ø§ØµØ± Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ ÙˆÙ‚Øª ÙˆØ§Ø­Ø¯:\n+\n+```py\n+>>> tokenized_books = books.map(preprocess_function, batched=True)\n+```\n+\n+Ø§Ù„Ø¢Ù† Ø£Ù†Ø´Ø¦ Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`DataCollatorForSeq2Seq`]. Ù…Ù† Ø§Ù„Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© *Ø§Ù„Ø­Ø´Ùˆ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ* Ù„Ù„Ø¬Ù…Ù„ Ø¥Ù„Ù‰ Ø£Ø·ÙˆÙ„ Ø·ÙˆÙ„ ÙÙŠ Ø¯ÙØ¹Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ØŒ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø­Ø´Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø·ÙˆÙ„.\n+\n+<frameworkcontent>\n+<pt>\n+\n+```py\n+>>> from transformers import DataCollatorForSeq2Seq\n+\n+>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n+```\n+</pt>\n+<tf>\n+\n+```py\n+>>> from transformers import DataCollatorForSeq2Seq\n+\n+>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")\n+```\n+</tf>\n+</frameworkcontent>\n+\n+## Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (Evaluate)\n+\n+ØºØ§Ù„Ø¨Ø§Ù‹ Ù…Ø§ ÙŠÙƒÙˆÙ† ØªØ¶Ù…ÙŠÙ† Ù…Ù‚ÙŠØ§Ø³ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…ÙÙŠØ¯Ø§Ù‹ Ù„ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬Ùƒ. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ø·Ø±ÙŠÙ‚Ø© ØªÙ‚ÙŠÙŠÙ… Ø¨Ø³Ø±Ø¹Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index). Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ø­Ù…Ù‘Ù„ Ù…Ù‚ÙŠØ§Ø³ [SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu) (Ø±Ø§Ø¬Ø¹ [Ø§Ù„Ø¬ÙˆÙ„Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©](https://huggingface.co/docs/evaluate/a_quick_tour) Ù„Ù€ ğŸ¤— Evaluate Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© ØªØ­Ù…ÙŠÙ„ ÙˆØ­Ø³Ø§Ø¨ Ù…Ù‚ÙŠØ§Ø³):\n+\n+```py\n+>>> import evaluate\n+\n+>>> metric = evaluate.load(\"sacrebleu\")\n+```\n+\n+Ø«Ù… Ø£Ù†Ø´Ø¦ Ø¯Ø§Ù„Ø© ØªÙÙ…Ø±Ø± ØªÙ†Ø¨Ø¤Ø§ØªÙƒ ÙˆØªØ³Ù…ÙŠØ§ØªÙƒ Ø¥Ù„Ù‰ [`~evaluate.EvaluationModule.compute`] Ù„Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© SacreBLEU:\n+\n+```py\n+>>> import numpy as np\n+\n+>>> def postprocess_text(preds, labels):\n+...     preds = [pred.strip() for pred in preds]\n+...     labels = [[label.strip()] for label in labels]\n+\n+...     return preds, labels\n+\n+>>> def compute_metrics(eval_preds):\n+...     preds, labels = eval_preds\n+...     if isinstance(preds, tuple):\n+...         preds = preds[0]\n+...     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n+\n+...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n+...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n+\n+...     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n+\n+...     result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n+...     result = {\"bleu\": result[\"score\"]}\n+\n+...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n+...     result[\"gen_len\"] = np.mean(prediction_lens)\n+...     result = {k: round(v, 4) for k, v in result.items()}\n+...     return result\n+```\n+\n+Ø¯Ø§Ù„Ø© `compute_metrics` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¬Ø§Ù‡Ø²Ø© Ø§Ù„Ø¢Ù†ØŒ ÙˆØ³ÙˆÙ ØªØ¹ÙˆØ¯ Ø¥Ù„ÙŠÙ‡Ø§ Ø¹Ù†Ø¯ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨.\n+\n+## Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Train)\n+\n+<frameworkcontent>\n+<pt>\n+\n+<Tip>\n+\n+Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø¹ØªØ§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø¶Ø¨Ø· Ø¯Ù‚ÙŠÙ‚ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Trainer`], ÙØ£Ù„Ù‚Ù Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-with-pytorch-trainer)!\n+\n+</Tip>\n+\n+Ø£Ù†Øª Ø¬Ø§Ù‡Ø² Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø§Ù„Ø¢Ù†! Ø­Ù…Ù‘Ù„ T5 Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`AutoModelForSeq2SeqLM`]:\n+\n+```py\n+>>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n+\n+>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n+```\n+\n+ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø­Ù„Ø©ØŒ ØªØ¨Ù‚Ù‰ Ø«Ù„Ø§Ø« Ø®Ø·ÙˆØ§Øª ÙÙ‚Ø·:\n+\n+1. Ø­Ø¯Ø¯ Ù…ÙØ¹Ø§Ù…Ù„Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙŠ [`Seq2SeqTrainingArguments`]. Ø§Ù„Ù…ÙØ¹Ø§Ù…Ù„ Ø§Ù„ÙˆØ­ÙŠØ¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù‡ÙŠ `output_dir` Ø§Ù„ØªÙŠ ØªØ­Ø¯Ø¯ Ù…ÙƒØ§Ù† Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ. Ø³ØªÙ‚ÙˆÙ… Ø¨Ø¯ÙØ¹ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Hub Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `push_to_hub=True` (ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Hugging Face Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ). ÙÙŠ Ù†Ù‡Ø§ÙŠØ© ÙƒÙ„ Ø­Ù‚Ø¨Ø©ØŒ Ø³ÙŠÙ‚ÙˆÙ… [`Trainer`] Ø¨ØªÙ‚ÙŠÙŠÙ… Ù…Ù‚ÙŠØ§Ø³ SacreBLEU ÙˆØ­ÙØ¸ Ù†Ù‚Ø·Ø© ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„ØªØ¯Ø±ÙŠØ¨.\n+2. Ù…Ø±Ø± Ù…ÙØ¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ù„Ù‰ [`Seq2SeqTrainer`] Ø¬Ù†Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ ÙˆØ¬Ø§Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙˆØ¸ÙŠÙØ© `compute_metrics`.\n+3. Ù†ÙÙ‘Ø° [`~Trainer.train`] Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬Ùƒ.\n+\n+```py\n+>>> training_args = Seq2SeqTrainingArguments(\n+...     output_dir=\"my_awesome_opus_books_model\",\n+...     eval_strategy=\"epoch\",\n+...     learning_rate=2e-5,\n+...     per_device_train_batch_size=16,\n+...     per_device_eval_batch_size=16,\n+...     weight_decay=0.01,\n+...     save_total_limit=3,\n+...     num_train_epochs=2,\n+...     predict_with_generate=True,\n+...     fp16=True, #change to bf16=True for XPU\n+...     push_to_hub=True,\n+... )\n+\n+>>> trainer = Seq2SeqTrainer(\n+...     model=model,\n+...     args=training_args,\n+...     train_dataset=tokenized_books[\"train\"],\n+...     eval_dataset=tokenized_books[\"test\"],\n+...     processing_class=tokenizer,\n+...     data_collator=data_collator,\n+...     compute_metrics=compute_metrics,\n+... )\n+\n+>>> trainer.train()\n+```\n+\n+Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ø´Ø§Ø±Ùƒ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ù…Ø¹ Hub Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.Trainer.push_to_hub`] Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬Ùƒ:\n+\n+```py\n+>>> trainer.push_to_hub()\n+```\n+</pt>\n+<tf>\n+<Tip>\n+\n+Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø¹ØªØ§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ ÙØ£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-a-tensorflow-model-with-keras)!\n+\n+</Tip>\n+Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ TensorFlowØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ù…ÙØ­Ø³ÙÙ‘Ù† ÙˆØ¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ ØªØ¹Ù„Ù… ÙˆØ¨Ø¹Ø¶ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨:\n+\n+```py\n+>>> from transformers import AdamWeightDecay\n+\n+>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n+```\n+\n+Ø«Ù… ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ T5 Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFAutoModelForSeq2SeqLM`]:\n+\n+```py\n+>>> from transformers import TFAutoModelForSeq2SeqLM\n+\n+>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n+```\n+\n+Ø­ÙˆÙ‘Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ `tf.data.Dataset` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n+\n+```py\n+>>> tf_train_set = model.prepare_tf_dataset(\n+...     tokenized_books[\"train\"],\n+...     shuffle=True,\n+...     batch_size=16,\n+...     collate_fn=data_collator,\n+... )\n+\n+>>> tf_test_set = model.prepare_tf_dataset(\n+...     tokenized_books[\"test\"],\n+...     shuffle=False,\n+...     batch_size=16,\n+...     collate_fn=data_collator,\n+... )\n+```\n+\n+Ù‚Ù… Ø¨ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Ù„Ø§Ø­Ø¸ Ø£Ù† Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Transformers ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ø¥Ù„Ø§ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ØºØ¨ ÙÙŠ Ø°Ù„Ùƒ:\n+\n+```py\n+>>> import tensorflow as tf\n+\n+>>> model.compile(optimizer=optimizer)  # No loss argument!\n+```\n+\n+Ø¢Ø®Ø± Ø´ÙŠØ¦ÙŠÙ† ÙŠØ¬Ø¨ Ø¥Ø¹Ø¯Ø§Ø¯Ù‡Ù…Ø§ Ù‚Ø¨Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù‡Ù…Ø§ Ø­Ø³Ø§Ø¨ Ù…Ù‚ÙŠØ§Ø³ SacreBLEU Ù…Ù† Ø§Ù„ØªÙˆÙ‚Ø¹Ø§ØªØŒ ÙˆØªÙˆÙÙŠØ± Ø·Ø±ÙŠÙ‚Ø© Ù„Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¥Ù„Ù‰ Hub. ÙŠØªÙ… ÙƒÙ„Ø§Ù‡Ù…Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Keras](../main_classes/keras_callbacks).\n+\n+Ù…Ø±Ø± Ø¯Ø§Ù„Ø© `compute_metrics` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ [`~transformers.KerasMetricCallback`]:\n+\n+```py\n+>>> from transformers.keras_callbacks import KerasMetricCallback\n+\n+>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_test_set)\n+```\n+\n+Ø­Ø¯Ø¯ Ù…ÙƒØ§Ù† Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ùƒ Ø§Ù„Ù„ØºÙˆÙŠ ÙÙŠ [`~transformers.PushToHubCallback`]:\n+\n+```py\n+>>> from transformers.keras_callbacks import PushToHubCallback\n+\n+>>> push_to_hub_callback = PushToHubCallback(\n+...     output_dir=\"my_awesome_opus_books_model\",\n+...     tokenizer=tokenizer,\n+... )\n+```\n+\n+Ø«Ù… Ø§Ø¬Ù…Ø¹ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§ØªÙƒ Ù…Ø¹Ù‹Ø§:\n+\n+```py\n+>>> callbacks = [metric_callback, push_to_hub_callback]\n+```\n+\n+Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ø£Ù†Øª Ø¬Ø§Ù‡Ø² Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ø§ØªØµÙ„ Ø¨Ù€ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø© ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø­Ù‚Ø¨ ÙˆØ§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§ØªÙƒ Ù„Ø¶Ø¨Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\n+\n+```py\n+>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)\n+```\n+\n+Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Hub Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡!\n+</tf>\n+</frameworkcontent>\n+\n+<Tip>\n+\n+Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„ Ø£ÙƒØ«Ø± ØªØ¹Ù…Ù‚Ù‹Ø§ Ù„ÙƒÙŠÙÙŠØ© Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ±Ø¬Ù…Ø©ØŒ Ø£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ [Ø¯ÙØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª PyTorch](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb) Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„\n+Ø£Ùˆ [Ø¯ÙØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª TensorFlow](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb).\n+\n+</Tip>\n+\n+## Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ (Inference)\n+\n+Ø±Ø§Ø¦Ø¹ØŒ Ø§Ù„Ø¢Ù† Ø¨Ø¹Ø¯ Ø£Ù† Ù‚Ù…Øª Ø¨Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„!\n+\n+Ø£Ø­Ø¶Ø± Ø¨Ø¹Ø¶ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ ØªØ±ØºØ¨ ÙÙŠ ØªØ±Ø¬Ù…ØªÙ‡Ø§ Ø¥Ù„Ù‰ Ù„ØºØ© Ø£Ø®Ø±Ù‰. Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù€ T5ØŒ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ø¶Ø§ÙØ© Ø¨Ø§Ø¯Ø¦Ø© Ø¥Ù„Ù‰ Ù…Ø¯Ø®Ù„Ø§ØªÙƒ Ø§Ø¹ØªÙ…Ø§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙŠ ØªØ¹Ù…Ù„ Ø¹Ù„ÙŠÙ‡Ø§. Ù„Ù„ØªØ±Ø¬Ù…Ø© Ù…Ù† Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©ØŒ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø¥Ø¶Ø§ÙØ© Ø¨Ø§Ø¯Ø¦Ø© Ø¥Ù„Ù‰ Ù…Ø¯Ø®Ù„Ø§ØªÙƒ ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ Ø£Ø¯Ù†Ø§Ù‡:\n+\n+```py\n+>>> text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\"\n+```\n+\n+Ø£Ø¨Ø³Ø· Ø·Ø±ÙŠÙ‚Ø© Ù„ØªØ¬Ø±Ø¨Ø© Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø§Ù„Ù…Ø¶Ø¨ÙˆØ· Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù‡ÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ [`pipeline`]. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ù„Ù€ `pipeline` Ù„Ù„ØªØ±Ø¬Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ÙƒØŒ ÙˆÙ…Ø±Ø± Ø§Ù„Ù†Øµ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø¥Ù„ÙŠÙ‡:\n+\n+```py\n+>>> from transformers import pipeline\n+\n+# ØªØºÙŠÙŠØ± `xx` Ø¥Ù„Ù‰ Ù„ØºØ© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ùˆ `yy` Ø¥Ù„Ù‰ Ù„ØºØ© Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©.\n+# Ø£Ù…Ø«Ù„Ø©: \"en\" Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŒ \"fr\" Ù„Ù„ØºØ© Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©ØŒ \"de\" Ù„Ù„ØºØ© Ø§Ù„Ø£Ù„Ù…Ø§Ù†ÙŠØ©ØŒ \"es\" Ù„Ù„ØºØ© Ø§Ù„Ø¥Ø³Ø¨Ø§Ù†ÙŠØ©ØŒ \"zh\" Ù„Ù„ØºØ© Ø§Ù„ØµÙŠÙ†ÙŠØ©ØŒ Ø¥Ù„Ø®Ø› translation_en_to_fr ØªØªØ±Ø¬Ù… Ù…Ù† Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©\n+# ÙŠÙ…ÙƒÙ†Ùƒ Ø¹Ø±Ø¶ Ø¬Ù…ÙŠØ¹ Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù„ØºØ§Øª Ù‡Ù†Ø§ - https://huggingface.co/languages\n+>>> translator = pipeline(\"translation_xx_to_yy\", model=\"username/my_awesome_opus_books_model\")\n+>>> translator(text)\n+[{'translation_text': 'Legumes partagent des ressources avec des bactÃ©ries azotantes.'}]\n+```\n+\n+ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªÙƒØ±Ø§Ø± Ù†ØªØ§Ø¦Ø¬ `pipeline` ÙŠØ¯ÙˆÙŠÙ‹Ø§ Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª:\n+\n+<frameworkcontent>\n+<pt>\n+Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ² ÙˆØ¥Ø±Ø¬Ø§Ø¹ `input_ids` ÙƒÙ…ÙˆØªØ±Ø§Øª PyTorch:\n+\n+```py\n+>>> from transformers import AutoTokenizer\n+\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_opus_books_model\")\n+>>> inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n+```\n+\n+Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¯Ø§Ù„Ø© [`~generation.GenerationMixin.generate`] Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ±Ø¬Ù…Ø©. Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø­ÙˆÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ØŒ ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª [ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ](../main_classes/text_generation).\n+\n+```py\n+>>> from transformers import AutoModelForSeq2SeqLM\n+\n+>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"username/my_awesome_opus_books_model\")\n+>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n+```\n+\n+ÙÙƒ ØªØ´ÙÙŠØ± Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¥Ù„Ù‰ Ù†Øµ:\n+\n+```py\n+>>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n+'Les lignÃ©es partagent des ressources avec des bactÃ©ries enfixant l'azote.'\n+```\n+</pt>\n+<tf>\n+Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ² ÙˆØ¥Ø±Ø¬Ø§Ø¹ `input_ids` ÙƒÙ…ÙˆØªØ±Ø§Øª TensorFlow:\n+\n+```py\n+>>> from transformers import AutoTokenizer\n+\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_opus_books_model\")\n+>>> inputs = tokenizer(text, return_tensors=\"tf\").input_ids\n+```\n+\n+Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ±Ø¬Ù…Ø©. Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø­ÙˆÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ØŒ ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª [ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ](../main_classes/text_generation).\n+\n+```py\n+>>> from transformers import TFAutoModelForSeq2SeqLM\n+\n+>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(\"username/my_awesome_opus_books_model\")\n+>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n+```\n+\n+ÙÙƒ ØªØ´ÙÙŠØ± Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¥Ù„Ù‰ Ù†Øµ:\n+\n+```py\n+>>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n+'Les lugumes partagent les ressources avec des bactÃ©ries fixatrices d'azote.'\n+```\n+</tf>\n+</frameworkcontent>\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 867,
        "additions": 863,
        "deletions": 4
    }
}