{
    "author": "merveenoyan",
    "message": "Vision docs üìù (#42096)\n\n* add mask generation fine-tuning docs\n\n* initial commit\n\n* update video text to text\n\n* fix autoprocessor\n\n* bump model, update API\n\n* add torch.compile\n\n* Add results\n\n* Update docs/source/en/tasks/image_text_to_text.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/tasks/image_text_to_text.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/tasks/image_text_to_text.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/tasks/mask_generation.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/tasks/mask_generation.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/tasks/mask_generation.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/tasks/mask_generation.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/tasks/video_text_to_text.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/tasks/mask_generation.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/tasks/mask_generation.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/tasks/mask_generation.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/tasks/video_text_to_text.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/tasks/video_text_to_text.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/tasks/image_text_to_text.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/tasks/mask_generation.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update image_text_to_text.md\n\n* Update docs/source/en/tasks/video_text_to_text.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "eaa3d4dd35d0ef856ee976cb1d2018e816e439c4",
    "files": [
        {
            "sha": "ef9e4b2e68823168b01a0f305b2182920fb90813",
            "filename": "docs/source/en/tasks/image_text_to_text.md",
            "status": "modified",
            "additions": 48,
            "deletions": 21,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/eaa3d4dd35d0ef856ee976cb1d2018e816e439c4/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/eaa3d4dd35d0ef856ee976cb1d2018e816e439c4/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md?ref=eaa3d4dd35d0ef856ee976cb1d2018e816e439c4",
            "patch": "@@ -33,7 +33,8 @@ This guide focuses on inference with an instruction-tuned model.\n Let's begin installing the dependencies.\n \n ```bash\n-pip install -q transformers accelerate flash_attn\n+pip install -q transformers accelerate \n+pip install flash-attn --no-build-isolation\n ```\n \n Let's initialize the model and the processor.\n@@ -45,12 +46,12 @@ import torch\n \n device = Accelerator().device\n model = AutoModelForImageTextToText.from_pretrained(\n-    \"HuggingFaceM4/idefics2-8b\",\n+    \"Qwen/Qwen3-VL-4B-Instruct\",\n     dtype=torch.bfloat16,\n     attn_implementation=\"flash_attention_2\",\n ).to(device)\n \n-processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics2-8b\")\n+processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-4B-Instruct\")\n ```\n \n This model has a [chat template](./chat_templating) that helps user parse chat outputs. Moreover, the model can also accept multiple images as input in a single conversation or message. We will now prepare the inputs.\n@@ -65,24 +66,29 @@ The image inputs look like the following.\n      <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\" alt=\"A bee on a pink flower\"/>\n </div>\n \n-```python\n-from PIL import Image\n-import requests\n \n-img_urls =[\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\",\n-           \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"]\n-images = [Image.open(requests.get(img_urls[0], stream=True).raw),\n-          Image.open(requests.get(img_urls[1], stream=True).raw)]\n+Structure your conversation as shown below for a single prompt with image and text inputs.\n+\n+```python\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\"},\n+            {\"type\": \"text\", \"text\": \"What do we see in this image?\"},\n+        ]\n+    }\n+]\n ```\n \n-Below is an example of the chat template. We can feed conversation turns and the last message as an input by appending it at the end of the template.\n+Alternate between the `user` and `assistant` role to ground the model with prior context to generate better responses.\n \n ```python\n messages = [\n     {\n         \"role\": \"user\",\n         \"content\": [\n-            {\"type\": \"image\"},\n+            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\"},\n             {\"type\": \"text\", \"text\": \"What do we see in this image?\"},\n         ]\n     },\n@@ -95,7 +101,7 @@ messages = [\n     {\n         \"role\": \"user\",\n         \"content\": [\n-            {\"type\": \"image\"},\n+            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n             {\"type\": \"text\", \"text\": \"And how about this image?\"},\n         ]\n     },\n@@ -105,19 +111,20 @@ messages = [\n We will now call the processors' [`~ProcessorMixin.apply_chat_template`] method to preprocess its output along with the image inputs.\n \n ```python\n-prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n-inputs = processor(text=prompt, images=[images[0], images[1]], return_tensors=\"pt\").to(device)\n+inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(device)\n ```\n \n We can now pass the preprocessed inputs to the model.\n \n ```python\n+input_len = len(inputs.input_ids[0])\n+\n with torch.no_grad():\n-    generated_ids = model.generate(**inputs, max_new_tokens=500)\n-generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+    generated_ids = model.generate(**inputs, max_new_tokens=200)\n+generated_texts = processor.batch_decode(generated_ids[:, input_len:], skip_special_tokens=True)\n \n print(generated_texts)\n-## ['User: What do we see in this image? \\nAssistant: In this image we can see two cats on the nets. \\nUser: And how about this image? \\nAssistant: In this image we can see flowers, plants and insect.']\n+## ['In this image we can see flowers, plants and insect.']\n ```\n \n ## Pipeline\n@@ -289,19 +296,38 @@ VLMs are often large and need to be optimized to fit on smaller hardware. Transf\n First, install dependencies.\n \n ```bash\n-pip install -U quanto bitsandbytes\n+pip install -U optimum-quanto bitsandbytes\n ```\n \n-To quantize a model during loading, we need to first create [`QuantoConfig`]. Then load the model as usual, but pass `quantization_config`¬†during model initialization.\n+To quantize a model during loading, we need to first create [`QuantoConfig`]. Then load the model as usual, but pass `quantization_config`¬†during model initialization. \n \n ```python\n from transformers import AutoModelForImageTextToText, QuantoConfig\n \n-model_id = \"HuggingFaceM4/idefics2-8b\"\n+model_id = \"Qwen/Qwen3-VL-4B-Instruct\"\n quantization_config = QuantoConfig(weights=\"int8\")\n quantized_model = AutoModelForImageTextToText.from_pretrained(\n     model_id, device_map=\"auto\", quantization_config=quantization_config\n )\n+\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\"},\n+            {\"type\": \"text\", \"text\": \"What do we see in this image?\"},\n+        ]\n+    },\n+]\n+inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device)\n+input_len = len(inputs.input_ids[0])\n+\n+with torch.no_grad():\n+    generated_ids = model.generate(**inputs, cache_implementation=\"static\", max_new_tokens=100)\n+generated_texts = processor.batch_decode(generated_ids[:, input_len:], skip_special_tokens=True)\n+\n+print(generated_texts[0])\n+## ['In this image, we see two tabby cats resting on a large, tangled pile of fishing nets. The nets are a mix of brown, orange, and red colors, with some blue and green ropes visible in the background. The cats appear relaxed and comfortable, nestled into the fibers of the nets. One cat is in the foreground, looking slightly to the side, while the other is positioned further back, looking directly at the camera. The scene suggests a coastal or fishing-related setting, possibly near']\n ```\n \n And that's it, we can use the model the same way with no changes.\n@@ -312,3 +338,4 @@ Here are some more resources for the image-text-to-text task.\n \n - [Image-text-to-text¬†task page](https://huggingface.co/tasks/image-text-to-text) covers model types, use cases, datasets, and more.\n - [Vision Language Models Explained](https://huggingface.co/blog/vlms) is a blog post that covers everything about vision language models and supervised fine-tuning using [TRL](https://huggingface.co/docs/trl/en/index).\n+- [Learn how to fine-tune vision language models using TRL](https://huggingface.co/blog/trl-vlm-alignment)"
        },
        {
            "sha": "00658864a0f5a7cc4434cd653769638616b48575",
            "filename": "docs/source/en/tasks/mask_generation.md",
            "status": "modified",
            "additions": 334,
            "deletions": 19,
            "changes": 353,
            "blob_url": "https://github.com/huggingface/transformers/blob/eaa3d4dd35d0ef856ee976cb1d2018e816e439c4/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/eaa3d4dd35d0ef856ee976cb1d2018e816e439c4/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md?ref=eaa3d4dd35d0ef856ee976cb1d2018e816e439c4",
            "patch": "@@ -24,8 +24,9 @@ Mask generation models are trained on large amounts of data and operate in two m\n - Prompting mode: In this mode, the model takes in an image and a prompt, where a prompt can be a 2D point location (XY coordinates) in the image within an object or a bounding box surrounding an object. In prompting mode, the model only returns the mask over the object\n that the prompt is pointing out.\n - Segment Everything mode: In segment everything, given an image, the model generates every mask in the image. To do so, a grid of points is generated and overlaid on the image for inference.\n+- Video Inference: The model accepts a video, and a point or box prompt in a video frame, which is tracked throughout the video. You can get more information on how to do video inference by following [SAM 2 docs](../model_doc/sam2).\n \n-Mask generation task is supported by [Segment Anything Model (SAM)](model_doc/sam). It's a powerful model that consists of a Vision Transformer-based image encoder, a prompt encoder, and a two-way transformer mask decoder. Images and prompts are encoded, and the decoder takes these embeddings and generates valid masks.\n+Mask generation task is supported by [Segment Anything Model (SAM)](../model_doc/sam) and [Segment Anything Model 2 (SAM2)](../model_doc/sam2), while video inference is supported by [Segment Anything Model 2 (SAM2)](../model_doc/sam2). SAM is a powerful model that consists of a Vision Transformer-based image encoder, a prompt encoder, and a two-way transformer mask decoder. Images and prompts are encoded, and the decoder takes these embeddings and generates valid masks.  Meanwhile, SAM 2 extends SAM by adding a memory module to track the masks. \n \n <div class=\"flex justify-center\">\n      <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sam.png\" alt=\"SAM Architecture\"/>\n@@ -53,7 +54,7 @@ The easiest way to infer mask generation models is to use the `mask-generation`\n ```python\n >>> from transformers import pipeline\n \n->>> checkpoint = \"facebook/sam-vit-base\"\n+>>> checkpoint = \"facebook/sam2-hiera-base-plus\"\n >>> mask_generator = pipeline(model=checkpoint, task=\"mask-generation\")\n ```\n \n@@ -80,20 +81,12 @@ masks = mask_generator(image, points_per_batch=128, pred_iou_thresh=0.88)\n The `masks` looks like the following:\n \n ```bash\n-{'masks': [array([[False, False, False, ...,  True,  True,  True],\n-         [False, False, False, ...,  True,  True,  True],\n-         [False, False, False, ...,  True,  True,  True],\n-         ...,\n-         [False, False, False, ..., False, False, False],\n-         [False, False, False, ..., False, False, False],\n-         [False, False, False, ..., False, False, False]]),\n-  array([[False, False, False, ..., False, False, False],\n-         [False, False, False, ..., False, False, False],\n-         [False, False, False, ..., False, False, False],\n-         ...,\n-'scores': tensor([0.9972, 0.9917,\n-        ...,\n-}\n+{'masks': [tensor([[False, False, False,  ...,  True,  True,  True],\n+          [False, False, False,  ...,  True,  True,  True],\n+          [False, False, False,  ...,  True,  True,  True],\n+          ...,\n+          [False, False, False,  ..., False, False, False], .. \n+ 'scores': tensor([0.9874, 0.9793, 0.9780, 0.9776, ... 0.9016])}\n ```\n \n We can visualize them like this:\n@@ -134,7 +127,7 @@ processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n \n To do point prompting, pass the input point to the processor, then take the processor output\n and pass it to the model for inference. To post-process the model output, pass the outputs and\n-`original_sizes` and `reshaped_input_sizes` we take from the processor's initial output. We need to pass these\n+`original_sizes` are taken from the processor's initial output. We need to pass these\n since the processor resizes the image, and the output needs to be extrapolated.\n \n ```python\n@@ -143,7 +136,7 @@ input_points = [[[2592, 1728]]] # point location of the bee\n inputs = processor(image, input_points=input_points, return_tensors=\"pt\").to(device)\n with torch.no_grad():\n     outputs = model(**inputs)\n-masks = processor.image_processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu())\n+masks = processor.image_processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu())\n ```\n \n We can visualize the three masks in the `masks`¬†output.\n@@ -199,7 +192,6 @@ with torch.no_grad():\n mask = processor.image_processor.post_process_masks(\n     outputs.pred_masks.cpu(),\n     inputs[\"original_sizes\"].cpu(),\n-    inputs[\"reshaped_input_sizes\"].cpu()\n )[0][0][0].numpy()\n ```\n \n@@ -235,3 +227,326 @@ plt.show()\n <div class=\"flex justify-center\">\n      <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/box_inference.png\" alt=\"Visualized Inference\"/>\n </div>\n+\n+## Fine-tuning for Mask Generation \n+\n+We will fine-tune SAM2.1 on small part of MicroMat dataset for image matting. We need to install the [monai](https://github.com/Project-MONAI/MONAI) library to use DICE loss, and [trackio](https://huggingface.co/docs/trackio/index) for logging the masks during training.\n+\n+```bash \n+pip install -q datasets monai trackio\n+```¬†\n+We can now load our dataset and take a look.\n+\n+```python\n+from datasets import load_dataset\n+\n+dataset = load_dataset(\"merve/MicroMat-mini\", split=\"train\")\n+dataset\n+# Dataset({\n+#    features: ['image', 'mask', 'prompt', 'image_id', 'object_id', 'sample_idx', 'granularity', \n+# 'image_path', 'mask_path', 'prompt_path'],  num_rows: 94\n+#})\n+```\n+We need image, mask and prompt columns. We split for train and test.\n+\n+```python\n+dataset = dataset.train_test_split(test_size=0.1)\n+train_ds = dataset[\"train\"]\n+val_ds = dataset[\"test\"]\n+```\n+\n+Let's take a look at a sample.\n+```python\n+train_ds[0]\n+``` \n+```\n+ {'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=2040x1356>,\n+ 'mask': <PIL.PngImagePlugin.PngImageFile image mode=L size=2040x1356>,\n+ 'prompt': '{\"point\": [[137, 1165, 1], [77, 1273, 0], [58, 1351, 0]], \"bbox\": [0, 701, 251, 1356]}',\n+ 'image_id': '0034',\n+ 'object_id': '34',\n+ 'sample_idx': 1,\n+ 'granularity': 'fine',\n+ 'image_path': '/content/MicroMat-mini/img/0034.png',\n+ 'mask_path': '/content/MicroMat-mini/mask/0034_34.png',\n+ 'prompt_path': '/content/MicroMat-mini/prompt/0034_34.json'}\n+```\n+Prompts are string of dictionaries, so you can get the bounding boxes as shown below.\n+```python\n+import json\n+\n+json.loads(train_ds[\"prompt\"][0])[\"bbox\"]\n+# [0, 701, 251, 1356]\n+``` \n+\n+Visualize an example image, prompt and mask.\n+\n+```python\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+def show_mask(mask, ax):\n+    color = np.array([0.12, 0.56, 1.0, 0.6])\n+    mask = np.array(mask)\n+    h, w = mask.shape\n+    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, 4)\n+    ax.imshow(mask_image)\n+    x0, y0, x1, y1 = eval(train_ds[\"prompt\"][0])[\"bbox\"]\n+    ax.add_patch(\n+        plt.Rectangle((x0, y0), x1 - x0, y1 - y0,\n+                      fill=False, edgecolor=\"lime\", linewidth=2))\n+\n+example = train_ds[0]\n+image = np.array(example[\"image\"])\n+ground_truth_mask = np.array(example[\"mask\"])\n+\n+fig, ax = plt.subplots()\n+ax.imshow(image)\n+show_mask(ground_truth_mask, ax)\n+ax.set_title(\"Ground truth mask\")\n+ax.set_axis_off()\n+\n+plt.show() \n+```\n+\n+Now we can define our dataset for loading the data. SAMDataset wraps our dataset and formats each sample the way the SAM processor expects. So instead of raw images and masks, you get processed images, bounding boxes, and ground-truth masks ready for training.\n+\n+By default, processor resizes images, so on top of images and masks, it also returns original sizes. We also need to binarize the mask as it has values [0, 255].\n+\n+```python\n+from torch.utils.data import Dataset\n+import torch\n+\n+class SAMDataset(Dataset):\n+  def __init__(self, dataset, processor):\n+    self.dataset = dataset\n+    self.processor = processor\n+\n+  def __len__(self):\n+    return len(self.dataset)\n+\n+  def __getitem__(self, idx):\n+    item = self.dataset[idx]\n+    image = item[\"image\"]\n+    prompt = eval(item[\"prompt\"])[\"bbox\"]\n+    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n+    inputs[\"ground_truth_mask\"] = (np.array(item[\"mask\"]) > 0).astype(np.float32)\n+    inputs[\"original_image_size\"] = torch.tensor(image.size[::-1])\n+\n+\n+    return inputs\n+``` \n+\n+We can initialize the processor and the dataset with it. \n+\n+```python \n+from transformers import Sam2Processor\n+\n+processor = Sam2Processor.from_pretrained(\"facebook/sam2.1-hiera-small\")\n+train_dataset = SAMDataset(dataset=train_ds, processor=processor)\n+```¬†\n+\n+We need to define a data collator that will turn varying size of ground truth masks to batches of reshaped masks in same shape. We reshape them using nearest neighbor interpolation. We also make batched tensors for rest of the elements in the batch. If your masks are all of same size, feel free to skip this step.\n+\n+```python\n+import torch.nn.functional as F\n+\n+def collate_fn(batch, target_hw=(256, 256)):\n+\n+    pixel_values = torch.cat([item[\"pixel_values\"] for item in batch], dim=0)\n+    original_sizes = torch.stack([item[\"original_sizes\"] for item in batch])\n+    input_boxes = torch.cat([item[\"input_boxes\"] for item in batch], dim=0)\n+    ground_truth_masks = torch.cat([\n+        F.interpolate(\n+            torch.as_tensor(x[\"ground_truth_mask\"]).unsqueeze(0).unsqueeze(0).float(),\n+            size=(256, 256),\n+            mode=\"nearest\"\n+        )\n+        for x in batch\n+    ], dim=0).long()\n+\n+    return {\n+        \"pixel_values\": pixel_values,\n+        \"original_sizes\": original_sizes,\n+        \"input_boxes\": input_boxes,\n+        \"ground_truth_mask\": ground_truth_masks,\n+        \"original_image_size\": torch.stack([item[\"original_image_size\"] for item in batch]),\n+    }\n+\n+from torch.utils.data import DataLoader\n+train_dataloader = DataLoader(\n+    train_dataset,\n+    batch_size=4,\n+    shuffle=True,\n+    collate_fn=collate_fn,\n+)\n+``` \n+\n+Let's take a look at what the data loader yields.\n+\n+```python\n+batch = next(iter(train_dataloader))\n+for k,v in batch.items():\n+  print(k,v.shape)\n+\n+# pixel_values torch.Size([4, 3, 1024, 1024])\n+# original_sizes torch.Size([4, 1, 2])\n+# input_boxes torch.Size([4, 1, 4])\n+# ground_truth_mask torch.Size([4, 1, 256, 256])\n+#original_image_size torch.Size([4, 2])\n+```\n+We will now load the model and freeze the vision and the prompt encoder to only train the mask decoder. \n+\n+```python\n+from transformers import Sam2Model\n+\n+model = Sam2Model.from_pretrained(\"facebook/sam2.1-hiera-small\")\n+\n+for name, param in model.named_parameters():\n+  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n+    param.requires_grad_(False)\n+```¬†\n+\n+We can now define the optimizer and the loss function.\n+```python \n+from torch.optim import Adam\n+import monai\n+\n+optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n+seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n+```\n+\n+Let's see how the model performs before training.\n+\n+```python\n+import matplotlib.pyplot as plt\n+\n+item = val_ds[1]\n+img = item[\"image\"]\n+bbox = json.loads(item[\"prompt\"])[\"bbox\"]\n+inputs = processor(images=img, input_boxes=[[bbox]], return_tensors=\"pt\").to(model.device)\n+\n+with torch.no_grad():\n+  outputs = model(**inputs)\n+\n+masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])[0]\n+preds = masks.squeeze(0)\n+mask = (preds[0] > 0).cpu().numpy()\n+\n+overlay = np.asarray(img, dtype=np.uint8).copy()\n+overlay[mask] = 0.55 * overlay[mask] + 0.45 * np.array([0, 255, 0], dtype=np.float32)\n+\n+plt.imshow(overlay)\n+plt.axis(\"off\")\n+plt.show()\n+```\n+\n+![SAM2 result after training](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sam2_before_training.png)\n+\n+We need to log our predictions to trackio so we can monitor the model improvement in the middle of the training. \n+\n+```python\n+from PIL import Image\n+import trackio\n+import json\n+\n+\n+@torch.no_grad()\n+def predict_fn(img, bbox):\n+\n+  inputs = processor(images=img, input_boxes=[[bbox]], return_tensors=\"pt\").to(model.device)\n+\n+  with torch.no_grad():\n+      outputs = model(**inputs)\n+\n+  masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])[0]\n+  return masks\n+\n+def log_eval_masks_trackio(dataset, indices, step, predict_fn,  project=None, sample_cap=8):\n+    logs = {\"eval/step\": int(step)}\n+    for idx in indices[:sample_cap]:\n+        item = dataset[idx] \n+        img = item[\"image\"]\n+        bbox = json.loads(item[\"prompt\"])[\"bbox\"]\n+        preds = predict_fn(img, bbox)\n+        preds = preds.squeeze(0)\n+        mask = (preds[0] > 0).cpu().numpy()  \n+\n+        overlay = np.asarray(img, dtype=np.uint8).copy()\n+        overlay[mask] = 0.55 * overlay[mask] + 0.45 * np.array([0, 255, 0], dtype=np.float32)\n+        logs[f\"{idx}/overlay\"] = trackio.Image(overlay, caption=\"overlay\")\n+        \n+    trackio.log(logs)\n+```\n+We can now write our training loop and train!\n+\n+Notice how we log our loss and evaluation masks with trackio.\n+\n+```python\n+from tqdm import tqdm\n+from statistics import mean\n+import trackio\n+import torch\n+\n+num_epochs = 30\n+\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+model.to(device)\n+\n+model.train()\n+trackio.init(project=\"mask-eval\")\n+for epoch in range(num_epochs):\n+    epoch_losses = []\n+    for batch in tqdm(train_dataloader):\n+      outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n+                      input_boxes=batch[\"input_boxes\"].to(device),\n+                      multimask_output=False)\n+\n+      predicted_masks = outputs.pred_masks.squeeze(1)\n+      ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n+      loss = seg_loss(predicted_masks, ground_truth_masks)\n+\n+      optimizer.zero_grad()\n+      loss.backward()\n+\n+      optimizer.step()\n+      epoch_losses.append(loss.item())\n+      \n+    log_eval_masks_trackio(dataset=val_ds, indices=[0, 3, 6, 9], step=epoch, predict_fn=predict_fn, project=\"mask-eval\")\n+    print(f'Epoch: {epoch}')\n+    print(f'Mean loss: {mean(epoch_losses)}')\n+    trackio.log({\"loss\": mean(epoch_losses)})\n+\n+trackio.finish()\n+```\n+\n+\n+Let's put the trained model to test.\n+\n+```python\n+import matplotlib.pyplot as plt\n+\n+item = val_ds[1]\n+img = item[\"image\"]\n+bbox = json.loads(item[\"prompt\"])[\"bbox\"]\n+\n+inputs = processor(images=img, input_boxes=[[bbox]], return_tensors=\"pt\").to(model.device)\n+\n+with torch.no_grad():\n+  outputs = model(**inputs)\n+\n+preds = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])[0]\n+\n+preds = preds.squeeze(0)\n+mask = (preds[0] > 0).cpu().numpy()\n+\n+overlay = np.asarray(img, dtype=np.uint8).copy()\n+overlay[mask] = 0.55 * overlay[mask] + 0.45 * np.array([0, 255, 0], dtype=np.float32)\n+\n+plt.imshow(overlay)\n+plt.axis(\"off\")\n+plt.show()\n+```\n+Great improvement after only training for 20 epochs on a small dataset!\n+\n+![SAM2 result after training](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sam2_after_training.png)"
        },
        {
            "sha": "9ada977ef6d93b120e390406413940aca2c036f8",
            "filename": "docs/source/en/tasks/semantic_segmentation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eaa3d4dd35d0ef856ee976cb1d2018e816e439c4/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/eaa3d4dd35d0ef856ee976cb1d2018e816e439c4/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md?ref=eaa3d4dd35d0ef856ee976cb1d2018e816e439c4",
            "patch": "@@ -219,7 +219,7 @@ Start by loading a smaller subset of the SceneParse150 dataset from the ü§ó Dat\n ```py\n >>> from datasets import load_dataset\n \n->>> ds = load_dataset(\"scene_parse_150\", split=\"train[:50]\")\n+>>> ds = load_dataset(\"merve/scene_parse_150\", split=\"train[:50]\")\n ```\n \n Split the dataset's `train` split into a train and test set with the [`~datasets.Dataset.train_test_split`] method:"
        },
        {
            "sha": "77cd96991d31351016e1aa6564093e2902d3a2e1",
            "filename": "docs/source/en/tasks/video_text_to_text.md",
            "status": "modified",
            "additions": 92,
            "deletions": 74,
            "changes": 166,
            "blob_url": "https://github.com/huggingface/transformers/blob/eaa3d4dd35d0ef856ee976cb1d2018e816e439c4/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/eaa3d4dd35d0ef856ee976cb1d2018e816e439c4/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md?ref=eaa3d4dd35d0ef856ee976cb1d2018e816e439c4",
            "patch": "@@ -18,9 +18,13 @@ rendered properly in your Markdown viewer.\n \n [[open-in-colab]]\n \n-Video-text-to-text models, also known as video language models or vision language models with video input, are language models that take a video input. These models can tackle various tasks, from video question answering to video captioning.\n+Video-text-to-text, also known as video language models are models that can process video and output text. These models can tackle various tasks, from video question answering to video captioning. \n \n-These models have nearly the same architecture as [image-text-to-text](../image_text_to_text) models except for some changes to accept video data, since video data is essentially image frames with temporal dependencies. Some image-text-to-text models take in multiple images, but this alone is inadequate for a model to accept videos. Moreover, video-text-to-text models are often trained with all vision modalities. Each example might have videos, multiple videos, images and multiple images. Some of these models can also take interleaved inputs. For example, you can refer to a specific video inside a string of text by adding a video token in text like \"What is happening in this video? `<video>`\".\n+These models have nearly the same architecture as [image-text-to-text](../image_text_to_text) models except for some changes to accept video data, since video data is essentially image frames with temporal dependencies. Some image-text-to-text models take in multiple images, but this alone is inadequate for a model to accept videos. \n+\n+Moreover, video-text-to-text models are often trained with all vision modalities. Each example might have videos, multiple videos, images and multiple images. Some of these models can also take interleaved inputs. For example, you can refer to a specific video inside a string of text by adding a video token in text like \"What is happening in this video? `<video>`\".\n+\n+Note that these models process videos with no audio. [Any-to-any](../any-to-any) models on the other hand can process videos with audio in them.\n \n In this guide, we provide a brief overview of video LMs and show how to use them with Transformers for inference.\n \n@@ -30,81 +34,27 @@ To begin with, there are multiple types of video LMs:\n - chat fine-tuned models for conversation\n - instruction fine-tuned models\n \n-This guide focuses on inference with an instruction-tuned model, [llava-hf/llava-interleave-qwen-7b-hf](https://huggingface.co/llava-hf/llava-interleave-qwen-7b-hf) which can take in interleaved data. Alternatively, you can try [llava-interleave-qwen-0.5b-hf](https://huggingface.co/llava-hf/llava-interleave-qwen-0.5b-hf) if your hardware doesn't allow running a 7B model.\n+This guide focuses on inference with an instruction-tuned model, [llava-hf/llava-onevision-qwen2-0.5b-ov-hf](https://huggingface.co/llava-hf/llava-interleave-qwen-7b-hf) which can take in interleaved data. Alternatively, you can try [llava-interleave-qwen-0.5b-hf](https://huggingface.co/llava-hf/llava-interleave-qwen-0.5b-hf) if your hardware doesn't allow running a 7B model.\n \n Let's begin installing the dependencies.\n \n ```bash\n-pip install -q transformers accelerate flash_attn \n+pip install -q transformers accelerate flash_attn torchcodec\n ```\n \n Let's initialize the model and the processor.\n \n ```python\n-from transformers import LlavaProcessor, LlavaForConditionalGeneration\n+from transformers import AutoProcessor, LlavaForConditionalGeneration\n import torch\n-model_id = \"llava-hf/llava-interleave-qwen-0.5b-hf\"\n+model_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\n \n-processor = LlavaProcessor.from_pretrained(model_id)\n+processor = AutoProcessor.from_pretrained(model_id, device=\"cuda\")\n \n model = LlavaForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", dtype=torch.float16)\n ```\n \n-Some models directly consume the `<video>` token, and others accept `<image>` tokens equal to the number of sampled frames. This model handles videos in the latter fashion. We will write a simple utility to handle image tokens, and another utility to get a video from a url and sample frames from it.\n-\n-```python\n-import uuid\n-import requests\n-import cv2\n-from PIL import Image\n-\n-def replace_video_with_images(text, frames):\n-  return text.replace(\"<video>\", \"<image>\" * frames)\n-\n-def sample_frames(url, num_frames):\n-\n-    response = requests.get(url)\n-    path_id = str(uuid.uuid4())\n-\n-    path = f\"./{path_id}.mp4\" \n-\n-    with open(path, \"wb\") as f:\n-      f.write(response.content)\n-\n-    video = cv2.VideoCapture(path)\n-    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n-    interval = total_frames // num_frames\n-    frames = []\n-    for i in range(total_frames):\n-        ret, frame = video.read()\n-        pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n-        if not ret:\n-            continue\n-        if i % interval == 0:\n-            frames.append(pil_img)\n-    video.release()\n-    return frames[:num_frames]\n-```\n-\n-Let's get our inputs. We will sample frames and concatenate them.\n-\n-```python\n-video_1 = \"https://huggingface.co/spaces/merve/llava-interleave/resolve/main/cats_1.mp4\"\n-video_2 = \"https://huggingface.co/spaces/merve/llava-interleave/resolve/main/cats_2.mp4\"\n-\n-video_1 = sample_frames(video_1, 6)\n-video_2 = sample_frames(video_2, 6)\n-\n-videos = video_1 + video_2\n-\n-videos\n-\n-# [<PIL.Image.Image image mode=RGB size=1920x1080>,\n-# <PIL.Image.Image image mode=RGB size=1920x1080>,\n-# <PIL.Image.Image image mode=RGB size=1920x1080>, ...]\n-```\n-\n-Both videos have cats.\n+We will infer with two videos, both have cats.\n \n <div class=\"container\">\n   <div class=\"video-container\">\n@@ -120,28 +70,96 @@ Both videos have cats.\n   </div>\n </div>\n \n-Now we can preprocess the inputs.\n \n-This model has a prompt template that looks like following. First, we'll put all the sampled frames into one list. Since we have eight frames in each video, we will insert 12 `<image>`¬†tokens to our prompt. Add `assistant` at the end of the prompt to trigger the model to give answers. Then we can preprocess.\n+Videos are series of image frames. Depending on the hardware limitations, downsampling is required. If the number of downsampled frames are too little, predictions will be low quality. \n+\n \n+Video-text-to-text¬†models have processors with video processor abstracted in them. You can pass video inference related arguments to [`~ProcessorMixin.apply_chat_template`] function.\n+\n+> [!WARNING]\n+> You can learn more about video processors [here](../main_classes/video_processor).\n+\n+We can define our chat history, passing in video with a URL like below.\n ```python\n-user_prompt = \"Are these two cats in these two videos doing the same thing?\"\n-toks = \"<image>\" * 12\n-prompt = \"<|im_start|>user\"+ toks + f\"\\n{user_prompt}<|im_end|><|im_start|>assistant\"\n-inputs = processor(text=prompt, images=videos, return_tensors=\"pt\").to(model.device, model.dtype)\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"video\", \"video\": \"https://huggingface.co/spaces/merve/llava-interleave/resolve/main/cats_1.mp4\"},\n+            {\"type\": \"text\", \"text\": \"Describe what is happening in this video.\"},\n+        ],\n+    }\n+]\n ```\n \n-We can now call [`~GenerationMixin.generate`] for inference. The model outputs the question in our input and answer, so we only take the text after the prompt and `assistant` part from the model output.\n+You can preprocess the videos by passing in messages, setting `do_sample_frames` to True and passing in `num_frames`. Here we sample 10 frames. \n \n ```python\n-output = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n-print(processor.decode(output[0][2:], skip_special_tokens=True)[len(user_prompt)+10:])\n+inputs = processor.apply_chat_template(\n+    messages,\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+    num_frames=10,\n+    do_sample_frames=True\n+)\n+inputs.to(model.device)\n+```\n+The inputs contain `input_ids` for tokenized text, `pixel_values_videos` for 10 frames and `attention_mask` for which tokens . \n \n-# The first cat is shown in a relaxed state, with its eyes closed and a content expression, while the second cat is shown in a more active state, with its mouth open wide, possibly in a yawn or a vocalization.\n+We can now infer with our preprocessed inputs and decode them.\n \n+```python\n+generated_ids = model.generate(**inputs, max_new_tokens=128)\n+input_length = len(inputs[\"input_ids\"][0])\n+output_text = processor.batch_decode(\n+    generated_ids[:, input_length:], skip_special_tokens=True, clean_up_tokenization_spaces=False\n+)\n+output_text = processor.batch_decode(\n+    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+)\n+print(output_text[0])\n+\n+#\"The video features a fluffy, long-haired cat with a mix of brown and white fur, lying on a beige carpeted floor. The cat's eyes are wide open, and its whiskers are prominently visible. The cat appears to be in a relaxed state, with its head slightly\"\n+```\n+\n+You can also interleave multiple videos with text directly in chat template like below.\n \n+```python\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": \"Here's a video.\"},\n+            {\"type\": \"video\", \"video\": \"https://huggingface.co/spaces/merve/llava-interleave/resolve/main/cats_1.mp4\"},\n+            {\"type\": \"text\", \"text\": \"Here's another video.\"},\n+            {\"type\": \"video\", \"video\": \"https://huggingface.co/spaces/merve/llava-interleave/resolve/main/cats_2.mp4\"},\n+            {\"type\": \"text\", \"text\": \"Describe similarities in these videos.\"},\n+        ],\n+    }\n+]\n ```\n \n-And voila!\n+The inference remains the same as the previous example.\n \n-To learn more about chat templates and token streaming for video-text-to-text models, refer to the [image-text-to-text](../tasks/image_text_to_text) task guide because these models work similarly.\n+```python\n+inputs = processor.apply_chat_template(\n+    messages,\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+    num_frames=100,\n+    do_sample_frames=True\n+)\n+inputs.to(model.device)\n+\n+generated_ids = model.generate(**inputs, max_new_tokens=50)\n+input_length = len(inputs[\"input_ids\"][0])\n+output_text = processor.batch_decode(\n+    generated_ids[:, input_length:], skip_special_tokens=True, clean_up_tokenization_spaces=False\n+)\n+print(output_text)\n+#['Both videos feature a cat with a similar appearance, characterized by a fluffy white coat with black markings, a pink nose, and a pink tongue. The cat\\'s eyes are wide open, and it appears to be in a state of alertness or excitement. ']\n+```"
        }
    ],
    "stats": {
        "total": 590,
        "additions": 475,
        "deletions": 115
    }
}