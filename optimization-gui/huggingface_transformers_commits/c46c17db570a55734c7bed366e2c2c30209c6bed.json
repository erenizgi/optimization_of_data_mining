{
    "author": "winglian",
    "message": "revert change to cu_seqlen_k and max_k when preparing from position_ids (#39653)",
    "sha": "c46c17db570a55734c7bed366e2c2c30209c6bed",
    "files": [
        {
            "sha": "0a91532cee88d779a696be2553a54521d7600110",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c46c17db570a55734c7bed366e2c2c30209c6bed/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c46c17db570a55734c7bed366e2c2c30209c6bed/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=c46c17db570a55734c7bed366e2c2c30209c6bed",
            "patch": "@@ -223,11 +223,6 @@ def _prepare_from_posids(query, key, value, position_ids):\n     key = key.contiguous().view(-1, key.size(-2), key.size(-1))\n     value = value.contiguous().view(-1, value.size(-2), value.size(-1))\n \n-    cu_seqlens_k = torch.cat(\n-        [torch.tensor([0], dtype=torch.int32, device=query.device), position_ids[:, -1].cumsum(dim=0) + 1], dim=0\n-    )\n-    max_k = torch.max(position_ids, dim=1).values.max().item() + 1\n-\n     position_ids = position_ids.flatten()\n     indices_q = torch.arange(position_ids.size(0), device=position_ids.device, dtype=torch.int32)\n \n@@ -246,7 +241,7 @@ def _prepare_from_posids(query, key, value, position_ids):\n     # We should use cu_seq_lens instead of position_ids to get the max length since position_ids is not always increasing\n     # for some models (e.g. qwen2-vl).\n     max_length = cu_seq_lens.diff().max().item()\n-    return (query, key, value, indices_q, (cu_seq_lens, cu_seqlens_k), (max_length, max_k))\n+    return (query, key, value, indices_q, (cu_seq_lens, cu_seq_lens), (max_length, max_length))\n \n \n def _prepare_flash_attention_from_position_ids(query, key, value, position_ids):"
        }
    ],
    "stats": {
        "total": 7,
        "additions": 1,
        "deletions": 6
    }
}