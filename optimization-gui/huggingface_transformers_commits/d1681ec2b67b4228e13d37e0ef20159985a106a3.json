{
    "author": "zucchini-nlp",
    "message": "VLMs: major clean up ðŸ§¼  (#34502)\n\nonly lllava models are modified",
    "sha": "d1681ec2b67b4228e13d37e0ef20159985a106a3",
    "files": [
        {
            "sha": "cd015f83ae97c03d12d9bfd2272555134f3164b9",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 63,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -461,85 +461,24 @@ def forward(\n                 \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n             )\n \n-        legacy_processing = False\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n-            # if the number of image tokens is more than image embeddings seq length, then prob we expanded it in processing\n-            # not very reliable, but we don't expect one to actually pass 500+ images for one prompt\n-            # In case we're in decoding stage, legacy behavior is checked by presence of pixel values even if use_cache=True\n-            legacy_processing = (\n-                (input_ids == self.config.image_token_index).sum(1).max() < self.config.image_seq_length\n-            ) or (input_ids.shape[-1] == 1 and pixel_values is not None)\n-\n-        image_features = None\n         if pixel_values is not None:\n             image_features = self.get_image_features(\n                 pixel_values=pixel_values,\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n             )\n \n-        if legacy_processing:\n-            logger.warning_once(\n-                \"Expanding inputs for image tokens in LLaVa should be done in processing. \"\n-                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n-                \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n-            )\n-            # prefill stage vs decoding stage (legacy behavior copied)\n-            if input_ids.shape[1] != 1:\n-                inputs_embeds, attention_mask, labels, position_ids = self._merge_input_ids_with_image_features(\n-                    image_features, inputs_embeds, input_ids, attention_mask, labels\n-                )\n-                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n-            else:\n-                # Retrieve the first layer to inspect the logits and mask out the hidden states\n-                # that are set to 0\n-                first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n-\n-                # Sum all dimensions of head_dim (-2) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n-                batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n-\n-                # Get the target length\n-                target_length = input_ids.shape[1]\n-                past_length = first_layer_past_key_value.shape[-1]\n-\n-                extended_attention_mask = torch.ones(\n-                    (attention_mask.shape[0], past_length),\n-                    dtype=attention_mask.dtype,\n-                    device=attention_mask.device,\n-                )\n-\n-                # Filter out only the tokens that can be un-attended, this can happen\n-                # if one uses Llava + Fused modules where the cache on the\n-                # first iteration is already big enough, or if one passes custom cache\n-                valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n-                new_batch_index = batch_index[valid_indices]\n-                new_non_attended_tokens = non_attended_tokens[valid_indices]\n-\n-                # Zero-out the places where we don't need to attend\n-                extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n-\n-                attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n-                position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n-                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[-target_length:]\n-\n-        # TODO: @raushan retain only the new behavior after v4.47\n-        elif image_features is not None:\n             n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n             n_image_features = image_features.shape[0] * image_features.shape[1]\n-\n             if n_image_tokens != n_image_features:\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                 )\n-            special_image_mask = (\n-                (input_ids == self.config.image_token_index)\n-                .unsqueeze(-1)\n-                .expand_as(inputs_embeds)\n-                .to(inputs_embeds.device)\n-            )\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n "
        },
        {
            "sha": "098b6fb379b6b250862b0f15f0fb685d71a69b79",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 11,
            "deletions": 21,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -154,27 +154,17 @@ def __call__(\n         # try to expand inputs in processing if we have the necessary parts\n         prompt_strings = text\n         if image_inputs.get(\"pixel_values\") is not None:\n-            if self.patch_size is not None and self.vision_feature_select_strategy is not None:\n-                # Replace the image token with the expanded image token sequence\n-                pixel_values = image_inputs[\"pixel_values\"]\n-                height, width = get_image_size(to_numpy_array(pixel_values[0]))\n-                num_image_tokens = (height // self.patch_size) * (\n-                    width // self.patch_size\n-                ) + self.num_additional_image_tokens\n-                if self.vision_feature_select_strategy == \"default\":\n-                    num_image_tokens -= self.num_additional_image_tokens\n-\n-                prompt_strings = []\n-                for sample in text:\n-                    sample = sample.replace(self.image_token, self.image_token * num_image_tokens)\n-                    prompt_strings.append(sample)\n-            else:\n-                logger.warning_once(\n-                    \"Expanding inputs for image tokens in LLaVa should be done in processing. \"\n-                    \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n-                    \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                    \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n-                )\n+            # Replace the image token with the expanded image token sequence\n+            pixel_values = image_inputs[\"pixel_values\"]\n+            height, width = get_image_size(to_numpy_array(pixel_values[0]))\n+            num_image_tokens = (height // self.patch_size) * (width // self.patch_size) + 1\n+            if self.vision_feature_select_strategy == \"default\":\n+                num_image_tokens -= 1\n+\n+            prompt_strings = []\n+            for sample in text:\n+                sample = sample.replace(self.image_token, self.image_token * num_image_tokens)\n+                prompt_strings.append(sample)\n \n         text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n         return BatchFeature(data={**text_inputs, **image_inputs})"
        },
        {
            "sha": "1a1223e9c2cfb0efaf9c5779061b8dbcb334322f",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 5,
            "deletions": 68,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -689,7 +689,9 @@ def pack_image_features(self, image_features, image_sizes, vision_feature_select\n                     image_feature = torch.cat(\n                         (\n                             image_feature,\n-                            image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.dtype),\n+                            image_newline[:, None, None]\n+                            .expand(*image_feature.shape[:-1], 1)\n+                            .to(image_feature.device, image_feature.dtype),\n                         ),\n                         dim=-1,\n                     )\n@@ -835,18 +837,9 @@ def forward(\n                 \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n             )\n \n-        legacy_processing = False\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n-            # if the number of image tokens is more than image embeddings seq length, then prob we expanded it in processing\n-            # not very reliable, but we don't expect one to actually pass 500+ images for one prompt\n-            # In case we're in decoding stage, legacy behavior is checked by presence of pixel values even if use_cache=True\n-            legacy_processing = (\n-                (input_ids == self.config.image_token_index).sum(1).max() < self.config.image_seq_length\n-            ) or (input_ids.shape[-1] == 1 and pixel_values is not None)\n-\n-        image_features = None\n         if pixel_values is not None and pixel_values.size(0) > 0:\n             image_features = self.get_image_features(\n                 pixel_values,\n@@ -863,70 +856,14 @@ def forward(\n                 image_newline=self.image_newline,\n             )\n \n-        if legacy_processing:\n-            logger.warning_once(\n-                \"Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. \"\n-                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n-                \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n-            )\n-            if input_ids.shape[1] != 1:\n-                inputs_embeds = inputs_embeds.to(image_features.dtype)\n-                inputs_embeds, attention_mask, position_ids, labels, _ = self._merge_input_ids_with_image_features(\n-                    image_features,\n-                    feature_lens,\n-                    inputs_embeds,\n-                    input_ids,\n-                    attention_mask,\n-                    position_ids,\n-                    labels=labels,\n-                )\n-                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n-            else:\n-                # Retrieve the first layer to inspect the logits and mask out the hidden states\n-                # that are set to 0\n-                first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n-\n-                # Sum all dimensions of head_dim (-2) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n-                batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n-\n-                # Get the target length\n-                target_length = input_ids.shape[1]\n-                past_length = first_layer_past_key_value.shape[-1]\n-\n-                extended_attention_mask = torch.ones(\n-                    (attention_mask.shape[0], past_length),\n-                    dtype=attention_mask.dtype,\n-                    device=attention_mask.device,\n-                )\n-\n-                # Filter out only the tokens that can be un-attended, this can happen\n-                # if one uses Llava + Fused modules where the cache on the\n-                # first iteration is already big enough, or if one passes custom cache\n-                valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n-                new_batch_index = batch_index[valid_indices]\n-                new_non_attended_tokens = non_attended_tokens[valid_indices]\n-\n-                # Zero-out the places where we don't need to attend\n-                extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n-                attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n-                position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n-                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[-target_length:]\n-\n-        # TODO: @raushan retain only the new behavior after v4.47\n-        elif image_features is not None:\n             n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n             n_image_features = image_features.shape[0]\n             if n_image_tokens != n_image_features:\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                 )\n-            special_image_mask = (\n-                (input_ids == self.config.image_token_index)\n-                .unsqueeze(-1)\n-                .expand_as(inputs_embeds)\n-                .to(inputs_embeds.device)\n-            )\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n "
        },
        {
            "sha": "cc293a416b389c965a0275c8dfc9baf390d0875c",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 13,
            "deletions": 24,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -149,30 +149,19 @@ def __call__(\n \n         prompt_strings = text\n         if image_inputs:\n-            if self.patch_size is None or self.vision_feature_select_strategy is None:\n-                logger.warning_once(\n-                    \"Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. \"\n-                    \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n-                    \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                    \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n-                )\n-            else:\n-                image_sizes = iter(image_inputs[\"image_sizes\"])\n-                height, width = get_image_size(to_numpy_array(image_inputs[\"pixel_values\"][0][0]))\n-                prompt_strings = []\n-                for sample in text:\n-                    while self.image_token in sample:\n-                        image_size = next(image_sizes)\n-                        if not isinstance(image_size, (list, tuple)):\n-                            # cast to list to avoid numerical precision errors when calculating unpadding\n-                            image_size = image_size.tolist()\n-                        orig_height, orig_width = image_size\n-                        num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n-                        if self.vision_feature_select_strategy == \"default\":\n-                            num_image_tokens -= self.num_additional_image_tokens\n-                        sample = sample.replace(self.image_token, \"<placeholder>\" * num_image_tokens, 1)\n-                    prompt_strings.append(sample)\n-                prompt_strings = [sample.replace(\"<placeholder>\", self.image_token) for sample in prompt_strings]\n+            image_sizes = iter(image_inputs[\"image_sizes\"])\n+            height, width = get_image_size(to_numpy_array(image_inputs[\"pixel_values\"][0][0]))\n+            prompt_strings = []\n+            for sample in text:\n+                while self.image_token in sample:\n+                    image_size = next(image_sizes)\n+                    orig_height, orig_width = image_size\n+                    num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n+                    if self.vision_feature_select_strategy == \"default\":\n+                        num_image_tokens -= 1\n+                    sample = sample.replace(self.image_token, \"<placeholder>\" * num_image_tokens, 1)\n+                prompt_strings.append(sample)\n+            prompt_strings = [sample.replace(\"<placeholder>\", self.image_token) for sample in prompt_strings]\n \n         text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n "
        },
        {
            "sha": "f6a66a7a9b1182375984b988fa11c1d43429d4a1",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 23,
            "deletions": 106,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -722,7 +722,9 @@ def pack_image_features(self, image_features, image_sizes, vision_feature_select\n                     image_feature = torch.cat(\n                         (\n                             image_feature,\n-                            image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.dtype),\n+                            image_newline[:, None, None]\n+                            .expand(*image_feature.shape[:-1], 1)\n+                            .to(image_feature.device, image_feature.dtype),\n                         ),\n                         dim=-1,\n                     )\n@@ -909,25 +911,9 @@ def forward(\n                 \"and must specify either one\"\n             )\n \n-        legacy_processing = False\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n-            # if the number of image/video tokens is more than image embeddings seq length, then prob we expanded it in processing\n-            # not very reliable, but we don't expect one to actually pass 500+ images for one prompt\n-            img_token_not_enough = (input_ids == self.config.image_token_index).sum(\n-                1\n-            ).max() < self.config.image_seq_length\n-            video_token_not_enough = (input_ids == self.config.video_token_index).sum(\n-                1\n-            ).max() < self.config.video_seq_length\n-            inputs_not_expanded = (img_token_not_enough and pixel_values is not None) or (\n-                video_token_not_enough and pixel_values_videos is not None\n-            )\n-            pixels_present = input_ids.shape[-1] == 1 and (pixel_values is not None or pixel_values_videos is not None)\n-            legacy_processing = inputs_not_expanded or pixels_present\n-\n-        image_features = feature_lens = None\n         if pixel_values is not None and pixel_values.size(0) > 0:\n             image_features = self.get_image_features(\n                 pixel_values,\n@@ -942,7 +928,17 @@ def forward(\n                 image_newline=self.image_newline,\n             )\n \n-        video_features = video_feature_lens = None\n+            n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n+            n_image_features = image_features.shape[0]\n+            if n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n         if pixel_values_videos is not None and pixel_values_videos.size(0) > 0:\n             video_features = self.get_video_features(\n                 pixel_values_videos,\n@@ -954,95 +950,16 @@ def forward(\n             video_features = torch.cat(video_features, dim=0)\n             video_feature_lens = torch.tensor(video_feature_lens, dtype=torch.long, device=video_features.device)\n \n-        if legacy_processing:\n-            logger.warning_once(\n-                \"Expanding inputs for image.video tokens in LLaVa-NeXT-Video should be done in processing. \"\n-                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n-                \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n-            )\n-            if input_ids.shape[1] != 1:\n-                iterator = (\n-                    (image_features, feature_lens, self.config.image_token_index),\n-                    (video_features, video_feature_lens, self.config.video_token_index),\n-                )\n-                for features, lens, special_token in iterator:\n-                    if features is not None:\n-                        (\n-                            inputs_embeds,\n-                            attention_mask,\n-                            position_ids,\n-                            labels,\n-                            input_ids,\n-                        ) = self._merge_input_ids_with_image_features(\n-                            features,\n-                            lens,\n-                            inputs_embeds,\n-                            input_ids,\n-                            attention_mask,\n-                            position_ids,\n-                            labels=labels,\n-                            image_token_index=special_token,\n-                        )\n-                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n-            else:\n-                # Retrieve the first layer to inspect the logits and mask out the hidden states that are set to 0\n-                first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n-                # Sum all dimensions of head_dim (-2) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n-                batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n-                # Get the target length\n-                target_length = input_ids.shape[1]\n-                past_length = first_layer_past_key_value.shape[-1]\n-                extended_attention_mask = torch.ones(\n-                    (attention_mask.shape[0], past_length),\n-                    dtype=attention_mask.dtype,\n-                    device=attention_mask.device,\n-                )\n-                # Filter out only the tokens that can be un-attended, this can happen\n-                # if one uses Llava + Fused modules where the cache on the\n-                # first iteration is already big enough, or if one passes custom cache\n-                valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n-                new_batch_index = batch_index[valid_indices]\n-                new_non_attended_tokens = non_attended_tokens[valid_indices]\n-                # Zero-out the places where we don't need to attend\n-                extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n-                attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n-                position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n-                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[-target_length:]\n-\n-        # TODO: @raushan retain only the new behavior after v4.47\n-        else:\n-            if image_features is not None:\n-                n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n-                n_image_features = image_features.shape[0]\n-\n-                if n_image_tokens != n_image_features:\n-                    raise ValueError(\n-                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                    )\n-                special_image_mask = (\n-                    (input_ids == self.config.image_token_index)\n-                    .unsqueeze(-1)\n-                    .expand_as(inputs_embeds)\n-                    .to(inputs_embeds.device)\n-                )\n-                image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-            if video_features is not None:\n-                n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n-                n_video_features = video_features.shape[0]\n-                if n_video_tokens != n_video_features:\n-                    raise ValueError(\n-                        f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                    )\n-                special_image_mask = (\n-                    (input_ids == self.config.video_token_index)\n-                    .unsqueeze(-1)\n-                    .expand_as(inputs_embeds)\n-                    .to(inputs_embeds.device)\n+            n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n+            n_video_features = video_features.shape[0]\n+            if n_video_tokens != n_video_features:\n+                raise ValueError(\n+                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n                 )\n-                video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n+            special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n \n         outputs = self.language_model(\n             attention_mask=attention_mask,"
        },
        {
            "sha": "5c04c96b887707dfd28cb2c8f662c9bbd0db2317",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 20,
            "deletions": 105,
            "changes": 125,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -431,25 +431,9 @@ def forward(\n                 \"and must specify either one\"\n             )\n \n-        legacy_processing = False\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n-            # if the number of image/video tokens is more than image embeddings seq length, then prob we expanded it in processing\n-            # not very reliable, but we don't expect one to actually pass 500+ images for one prompt\n-            img_token_not_enough = (input_ids == self.config.image_token_index).sum(\n-                1\n-            ).max() < self.config.image_seq_length\n-            video_token_not_enough = (input_ids == self.config.video_token_index).sum(\n-                1\n-            ).max() < self.config.video_seq_length\n-            inputs_not_expanded = (img_token_not_enough and pixel_values is not None) or (\n-                video_token_not_enough and pixel_values_videos is not None\n-            )\n-            pixels_present = input_ids.shape[-1] == 1 and (pixel_values is not None or pixel_values_videos is not None)\n-            legacy_processing = inputs_not_expanded or pixels_present\n-\n-        image_features = feature_lens = None\n         if pixel_values is not None and pixel_values.size(0) > 0:\n             image_features = self.get_image_features(\n                 pixel_values,\n@@ -464,7 +448,17 @@ def forward(\n                 image_newline=self.image_newline,\n             )\n \n-        video_features = video_feature_lens = None\n+            n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n+            n_image_features = image_features.shape[0]\n+            if n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n         if pixel_values_videos is not None and pixel_values_videos.size(0) > 0:\n             video_features = self.get_video_features(\n                 pixel_values_videos,\n@@ -476,95 +470,16 @@ def forward(\n             video_features = torch.cat(video_features, dim=0)\n             video_feature_lens = torch.tensor(video_feature_lens, dtype=torch.long, device=video_features.device)\n \n-        if legacy_processing:\n-            logger.warning_once(\n-                \"Expanding inputs for image.video tokens in LLaVa-NeXT-Video should be done in processing. \"\n-                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n-                \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n-            )\n-            if input_ids.shape[1] != 1:\n-                iterator = (\n-                    (image_features, feature_lens, self.config.image_token_index),\n-                    (video_features, video_feature_lens, self.config.video_token_index),\n-                )\n-                for features, lens, special_token in iterator:\n-                    if features is not None:\n-                        (\n-                            inputs_embeds,\n-                            attention_mask,\n-                            position_ids,\n-                            labels,\n-                            input_ids,\n-                        ) = self._merge_input_ids_with_image_features(\n-                            features,\n-                            lens,\n-                            inputs_embeds,\n-                            input_ids,\n-                            attention_mask,\n-                            position_ids,\n-                            labels=labels,\n-                            image_token_index=special_token,\n-                        )\n-                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n-            else:\n-                # Retrieve the first layer to inspect the logits and mask out the hidden states that are set to 0\n-                first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n-                # Sum all dimensions of head_dim (-2) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n-                batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n-                # Get the target length\n-                target_length = input_ids.shape[1]\n-                past_length = first_layer_past_key_value.shape[-1]\n-                extended_attention_mask = torch.ones(\n-                    (attention_mask.shape[0], past_length),\n-                    dtype=attention_mask.dtype,\n-                    device=attention_mask.device,\n-                )\n-                # Filter out only the tokens that can be un-attended, this can happen\n-                # if one uses Llava + Fused modules where the cache on the\n-                # first iteration is already big enough, or if one passes custom cache\n-                valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n-                new_batch_index = batch_index[valid_indices]\n-                new_non_attended_tokens = non_attended_tokens[valid_indices]\n-                # Zero-out the places where we don't need to attend\n-                extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n-                attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n-                position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n-                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[-target_length:]\n-\n-        # TODO: @raushan retain only the new behavior after v4.47\n-        else:\n-            if image_features is not None:\n-                n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n-                n_image_features = image_features.shape[0]\n-\n-                if n_image_tokens != n_image_features:\n-                    raise ValueError(\n-                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                    )\n-                special_image_mask = (\n-                    (input_ids == self.config.image_token_index)\n-                    .unsqueeze(-1)\n-                    .expand_as(inputs_embeds)\n-                    .to(inputs_embeds.device)\n-                )\n-                image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-            if video_features is not None:\n-                n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n-                n_video_features = video_features.shape[0]\n-                if n_video_tokens != n_video_features:\n-                    raise ValueError(\n-                        f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                    )\n-                special_image_mask = (\n-                    (input_ids == self.config.video_token_index)\n-                    .unsqueeze(-1)\n-                    .expand_as(inputs_embeds)\n-                    .to(inputs_embeds.device)\n+            n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n+            n_video_features = video_features.shape[0]\n+            if n_video_tokens != n_video_features:\n+                raise ValueError(\n+                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n                 )\n-                video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n+            special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n \n         outputs = self.language_model(\n             attention_mask=attention_mask,"
        },
        {
            "sha": "f3b2b78f7aa681ab4ad0625ea533bdce86c12d33",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 27,
            "deletions": 42,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -173,48 +173,33 @@ def __call__(\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n             raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n-        if self.patch_size is None or self.vision_feature_select_strategy is None:\n-            logger.warning_once(\n-                \"Expanding inputs for image/video tokens in LLaVa-NeXT-Video should be done in processing. \"\n-                \"Please add `patch_size`, `num_additional_image_tokens` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n-                \"with `processor.patch_size = {{patch_size}}`, `processor.num_additional_image_tokens = {{num_additional_image_tokens}}` \"\n-                \"and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n-            )\n-        else:\n-            # images expand taking into account num_of_patches in each image\n-            if image_inputs:\n-                image_sizes = iter(image_inputs[\"image_sizes\"])\n-                height, width = get_image_size(to_numpy_array(image_inputs[\"pixel_values\"][0][0]))\n-                prompt_strings = []\n-                for sample in text:\n-                    while self.image_token in sample:\n-                        image_size = next(image_sizes)\n-                        if not isinstance(image_size, (list, tuple)):\n-                            # cast to list to avoid numerical precision errors when calculating unpadding\n-                            image_size = image_size.tolist()\n-                        orig_height, orig_width = image_size\n-                        num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n-                        if self.vision_feature_select_strategy == \"default\":\n-                            num_image_tokens -= self.num_additional_image_tokens\n-                        sample = sample.replace(self.image_token, \"<placeholder>\" * num_image_tokens, 1)\n-                    prompt_strings.append(sample)\n-                text = [sample.replace(\"<placeholder>\", self.image_token) for sample in prompt_strings]\n-\n-            # videos are easier, simply get frames and multiply\n-            if videos_inputs:\n-                one_video = to_numpy_array(videos_inputs.get(\"pixel_values_videos\")[0])\n-                height, width = get_image_size(one_video[0])\n-                num_frames = one_video.shape[0]  # frame dim is always after batch dim\n-\n-                # no `self.num_additional_image_tokens` added because video always has a default feature selection strategy\n-                num_image_tokens = (height // self.patch_size) * (width // self.patch_size)\n-                num_video_tokens = num_image_tokens // 4 * num_frames  # divide by 4 needed for avg pooling layer\n-                prompt_strings = []\n-                for sample in text:\n-                    sample = sample.replace(self.video_token, self.video_token * num_video_tokens)\n-                    prompt_strings.append(sample)\n-                text = prompt_strings\n+        if image_inputs:\n+            image_sizes = iter(image_inputs[\"image_sizes\"])\n+            height, width = get_image_size(to_numpy_array(image_inputs[\"pixel_values\"][0][0]))\n+            prompt_strings = []\n+            for sample in text:\n+                while self.image_token in sample:\n+                    image_size = next(image_sizes)\n+                    orig_height, orig_width = image_size\n+                    num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n+                    if self.vision_feature_select_strategy == \"default\":\n+                        num_image_tokens -= 1\n+                    sample = sample.replace(self.image_token, \"<placeholder>\" * num_image_tokens, 1)\n+                prompt_strings.append(sample)\n+            text = [sample.replace(\"<placeholder>\", self.image_token) for sample in prompt_strings]\n+\n+        # videos are easier, simply get frames and multiply\n+        if videos_inputs:\n+            one_video = to_numpy_array(videos_inputs.get(\"pixel_values_videos\")[0])\n+            height, width = get_image_size(one_video[0])\n+            num_frames = one_video.shape[0]  # frame dim is always after batch dim\n+            num_image_tokens = (height // self.patch_size) * (width // self.patch_size)\n+            num_video_tokens = num_image_tokens // 4 * num_frames  # divide by 4 needed for avg pooling layer\n+            prompt_strings = []\n+            for sample in text:\n+                sample = sample.replace(self.video_token, self.video_token * num_video_tokens)\n+                prompt_strings.append(sample)\n+            text = prompt_strings\n \n         text_inputs = self.tokenizer(\n             text,"
        },
        {
            "sha": "80dfaa2f0e7422c981a09f45fd0c979f285169fd",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 19,
            "deletions": 105,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -538,127 +538,41 @@ def forward(\n                 \"time, and must specify either one\"\n             )\n \n-        legacy_processing = False\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n-            # if the number of image/video tokens is more than image embeddings seq length, then prob we expanded it in processing\n-            # not very reliable, but we don't expect one to actually pass 500+ images for one prompt\n-            img_token_not_enough = (input_ids == self.config.image_token_index).sum(\n-                1\n-            ).max() < self.config.image_seq_length\n-            video_token_not_enough = (input_ids == self.config.video_token_index).sum(\n-                1\n-            ).max() < self.config.video_seq_length\n-            inputs_not_expanded = (img_token_not_enough and pixel_values_images is not None) or (\n-                video_token_not_enough and pixel_values_videos is not None\n-            )\n-            pixels_present = input_ids.shape[-1] == 1 and (\n-                pixel_values_images is not None or pixel_values_videos is not None\n-            )\n-            legacy_processing = inputs_not_expanded or pixels_present\n-\n-        image_features = None\n         if pixel_values_images is not None:\n             image_features = self.get_image_features(\n                 pixel_values_images,\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n             )\n+            n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n+            n_image_features = image_features.shape[0] * image_features.shape[1]\n+            if n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n-        video_features = None\n-        num_frames = 0\n         if pixel_values_videos is not None:\n             video_features, num_frames = self.get_video_features(\n                 pixel_values_videos=pixel_values_videos, vision_feature_layer=vision_feature_layer\n             )\n \n-        if legacy_processing:\n-            logger.warning_once(\n-                \"Expanding inputs for image tokens in Video-LLaVa should be done in processing. \"\n-                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n-                \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n-            )\n-            if input_ids.shape[1] != 1:\n-                for features, frames in ((image_features, 1), (video_features, num_frames)):\n-                    if features is not None:\n-                        (\n-                            inputs_embeds,\n-                            attention_mask,\n-                            labels,\n-                            position_ids,\n-                            input_ids,\n-                        ) = self._merge_input_ids_with_visual_features(\n-                            features,\n-                            inputs_embeds,\n-                            input_ids,\n-                            attention_mask,\n-                            labels,\n-                            num_frames=frames,\n-                        )\n-                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n-            else:\n-                # Retrieve the first layer to inspect the logits and mask out the hidden states\n-                # that are set to 0\n-                first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n-\n-                # Sum all dimensions of head_dim (-2) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n-                batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n-                target_length = input_ids.shape[1]\n-                past_length = first_layer_past_key_value.shape[-1]\n-                extended_attention_mask = torch.ones(\n-                    (attention_mask.shape[0], past_length),\n-                    dtype=attention_mask.dtype,\n-                    device=attention_mask.device,\n-                )\n-\n-                # Filter out only the tokens that can be un-attended, this can happen\n-                # if one uses Llava + Fused modules where the cache on the\n-                # first iteration is already big enough, or if one passes custom cache\n-                valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n-                new_batch_index = batch_index[valid_indices]\n-                new_non_attended_tokens = non_attended_tokens[valid_indices]\n-\n-                # Zero-out the places where we don't need to attend\n-                extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n-                attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n-                position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n-                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[-target_length:]\n-\n-        # TODO: @raushan retain only the new behavior after v4.47\n-        else:\n-            if pixel_values_images is not None:\n-                n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                if n_image_tokens != n_image_features:\n-                    raise ValueError(\n-                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                    )\n-                special_image_mask = (\n-                    (input_ids == self.config.image_token_index)\n-                    .unsqueeze(-1)\n-                    .expand_as(inputs_embeds)\n-                    .to(inputs_embeds.device)\n-                )\n-                image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-            if pixel_values_videos is not None:\n-                n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n-                n_video_features = video_features.shape[0] * video_features.shape[1]\n-                if n_video_tokens != n_video_features:\n-                    raise ValueError(\n-                        f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                    )\n-                special_image_mask = (\n-                    (input_ids == self.config.video_token_index)\n-                    .unsqueeze(-1)\n-                    .expand_as(inputs_embeds)\n-                    .to(inputs_embeds.device)\n+            n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n+            n_video_features = video_features.shape[0] * video_features.shape[1]\n+            if n_video_tokens != n_video_features:\n+                raise ValueError(\n+                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n                 )\n-                video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n+            special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n \n         outputs = self.language_model(\n             attention_mask=attention_mask,"
        },
        {
            "sha": "3f58675d047a346cf200edd867cd6bda7790635a",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -158,16 +158,8 @@ def __call__(\n             raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n         prompt_strings = text\n-        if encoded_images is not None and (self.patch_size is None or self.vision_feature_select_strategy is None):\n-            logger.warning_once(\n-                \"Expanding inputs for image tokens in Video-LLaVa should be done in processing. \"\n-                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set \"\n-                \"directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = \"\n-                \"{{vision_feature_select_strategy}}`. Using processors without these attributes in the config is \"\n-                \"deprecated and will throw an error in v4.50.\"\n-            )\n-        # Replace the image/video tokens with the expanded token sequence\n-        elif encoded_images is not None:\n+\n+        if encoded_images is not None:\n             if \"pixel_values_images\" in encoded_images.keys():\n                 height, width = get_image_size(to_numpy_array(encoded_images.get(\"pixel_values_images\")[0]))\n                 num_frames = 1"
        },
        {
            "sha": "0eb65b0fc72292ff9ac7a04c1d485e19d8d08a70",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 60,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -455,80 +455,22 @@ def forward(\n                 \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n             )\n \n-        legacy_processing = False\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n-            # if the number of image tokens is more than image embeddings seq length, then prob we expanded it in processing\n-            # not very reliable, but we don't expect one to actually pass 500+ images for one prompt\n-            # In case we're in decoding stage, legacy behavior is checked by presence of pixel values even if use_cache=True\n-            legacy_processing = (\n-                (input_ids == self.config.image_token_index).sum(1).max() < self.config.image_seq_length\n-            ) or (input_ids.shape[-1] == 1 and pixel_values is not None)\n-\n-        image_features = None\n         if pixel_values is not None:\n             image_features = self.get_image_features(\n                 pixel_values=pixel_values, vision_feature_layers=vision_feature_layers\n             )\n \n-        if legacy_processing:\n-            logger.warning_once(\n-                \"Expanding inputs for image tokens in VipLLaVa should be done in processing. \"\n-                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's image processing config. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n-            )\n-            # prefill stage vs decoding stage (legacy behavior copied)\n-            if input_ids.shape[1] != 1:\n-                inputs_embeds, attention_mask, labels, position_ids = self._merge_input_ids_with_image_features(\n-                    image_features, inputs_embeds, input_ids, attention_mask, labels\n-                )\n-                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n-            else:\n-                # Retrieve the first layer to inspect the logits and mask out the hidden states\n-                # that are set to 0\n-                first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n-\n-                # Sum all dimensions of head_dim (-1) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n-                batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n-\n-                target_length = input_ids.shape[1]\n-                past_length = first_layer_past_key_value.shape[-1]\n-\n-                extended_attention_mask = torch.ones(\n-                    (attention_mask.shape[0], past_length),\n-                    dtype=attention_mask.dtype,\n-                    device=attention_mask.device,\n-                )\n-\n-                # Filter out only the tokens that can be un-attended, this can happen\n-                # in the case one uses Llava + Fused modules where the cache on the\n-                # first iteration is already big enough, or if one passes custom cache\n-                valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n-                new_batch_index = batch_index[valid_indices]\n-                new_non_attended_tokens = non_attended_tokens[valid_indices]\n-\n-                # Zero-out the places where we don't need to attend\n-                extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n-\n-                attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n-                position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n-                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[-target_length:]\n-\n-        # TODO: @raushan retain only the new behavior after v4.47\n-        elif image_features is not None:\n             n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n             n_image_features = image_features.shape[0] * image_features.shape[1]\n             if n_image_tokens != n_image_features:\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                 )\n-            special_image_mask = (\n-                (input_ids == self.config.image_token_index)\n-                .unsqueeze(-1)\n-                .expand_as(inputs_embeds)\n-                .to(inputs_embeds.device)\n-            )\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n "
        },
        {
            "sha": "e91b76f7d9f5a6177761b9c537382cce52649600",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 16,
            "deletions": 61,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -327,10 +327,7 @@ def test_small_model_integration_test(self):\n         prompt = \"<image>\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT:\"\n         image_file = \"https://llava-vl.github.io/static/images/view.jpg\"\n         raw_image = Image.open(requests.get(image_file, stream=True).raw)\n-        inputs = self.processor(images=raw_image, text=prompt, return_tensors=\"pt\")\n-\n-        EXPECTED_INPUT_IDS = torch.tensor([[1, 32000, 28705, 13, 11123, 28747, 1824, 460, 272, 1722,315, 1023, 347, 13831, 925, 684, 739, 315, 3251, 456,1633, 28804, 13, 4816, 8048, 12738, 28747]])  # fmt: skip\n-        self.assertTrue(torch.equal(inputs[\"input_ids\"], EXPECTED_INPUT_IDS))\n+        inputs = self.processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device)\n \n         output = model.generate(**inputs, max_new_tokens=20)\n         EXPECTED_DECODED_TEXT = \"\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT: When visiting this place, there are a few things one should be cautious about. Firstly,\"  # fmt: skip\n@@ -378,7 +375,7 @@ def test_small_model_integration_test_llama_batched(self):\n         image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n         image2 = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n \n-        inputs = processor(images=[image1, image2], text=prompts, return_tensors=\"pt\", padding=True)\n+        inputs = processor(images=[image1, image2], text=prompts, return_tensors=\"pt\", padding=True).to(torch_device)\n \n         output = model.generate(**inputs, max_new_tokens=20)\n \n@@ -402,7 +399,9 @@ def test_small_model_integration_test_batch(self):\n         image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n         image2 = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n \n-        inputs = self.processor(images=[image1, image2], text=prompts, return_tensors=\"pt\", padding=True)\n+        inputs = self.processor(images=[image1, image2], text=prompts, return_tensors=\"pt\", padding=True).to(\n+            torch_device\n+        )\n \n         output = model.generate(**inputs, max_new_tokens=20)\n \n@@ -434,7 +433,9 @@ def test_small_model_integration_test_llama_batched_regression(self):\n         image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n         image2 = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n \n-        inputs = processor(images=[image1, image2, image1], text=prompts, return_tensors=\"pt\", padding=True)\n+        inputs = processor(images=[image1, image2, image1], text=prompts, return_tensors=\"pt\", padding=True).to(\n+            torch_device\n+        )\n \n         output = model.generate(**inputs, max_new_tokens=20)\n \n@@ -508,32 +509,18 @@ def test_llava_merge_inputs_error_bug(self):\n         # This is a reproducer of https://github.com/huggingface/transformers/pull/28333 and makes sure it does not happen anymore\n         model_id = \"llava-hf/llava-1.5-7b-hf\"\n         model = LlavaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+        processor = AutoProcessor.from_pretrained(model_id)\n \n-        # Simulate some user inputs\n-        pixel_values = torch.randn(\n-            (1, 3, 336, 336),\n-            dtype=torch.float,\n-            device=torch_device,\n-        )\n-        input_ids = torch.tensor(\n-            [\n-                [32001, 32001, 1, 15043, 7084, 32000, 29871, 13, 7900],\n-            ],\n-            dtype=torch.long,\n-            device=torch_device,\n-        )\n-        attention_mask = torch.tensor(\n-            [[0, 0, 1, 1, 1, 1, 1, 1, 1]],\n-            dtype=torch.long,\n-            device=torch_device,\n-        )\n+        prompt = \"USER: <image>\\nDescribe the imageASSISTANT:\"\n+        image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+\n+        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n+        inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n \n         # Make sure that the loss is properly computed\n         loss = model(\n-            pixel_values=pixel_values,\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            labels=input_ids,\n+            **inputs,\n+            labels=inputs.input_ids.clone(),\n         ).loss\n         loss.backward()\n \n@@ -593,38 +580,6 @@ def test_generation_siglip_backbone(self):\n         EXPECTED_DECODED_TEXT = \"user\\n\\nWhat are these?\\nassistant The image shows two cats, one on the left and one on the right. They appear to be resting or sleeping on a pink blanket. The cat\"\n         self.assertTrue(processor.batch_decode(output, skip_special_tokens=True)[0] == EXPECTED_DECODED_TEXT)\n \n-    @slow\n-    @require_bitsandbytes\n-    def test_expansion_in_processing(self):\n-        model_id = \"llava-hf/llava-1.5-7b-hf\"\n-        model = LlavaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n-        processor = AutoProcessor.from_pretrained(model_id)\n-\n-        prompt = \"USER: <image>\\nDescribe the image:\\nASSISTANT:\"\n-        image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n-\n-        # check processing with expansion of inputs\n-        processor.vision_feature_select_strategy = \"default\"\n-        processor.num_additional_image_tokens = 1\n-        processor.patch_size = 14\n-        inputs_expanded = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n-        self.assertTrue(inputs_expanded.input_ids.shape[-1] == 593)\n-\n-        # check processing without expansion of inputs (legacy behavior)\n-        processor.vision_feature_select_strategy = None\n-        processor.patch_size = None\n-        processor.num_additional_image_tokens = None\n-        inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n-        self.assertTrue(inputs.input_ids.shape[-1] == 18)\n-\n-        # generate exactly 20 tokens\n-        output = model.generate(**inputs, min_new_tokens=20, max_new_tokens=20)\n-        output_expanded = model.generate(**inputs_expanded, min_new_tokens=20, max_new_tokens=20)\n-\n-        # check that both inputs are handled correctly and generate the same output\n-        self.assertListEqual(output_expanded[:, -20:].tolist(), output[:, -20:].tolist())\n-\n     @slow\n     @require_bitsandbytes\n     def test_pixtral(self):"
        },
        {
            "sha": "3e6c1a9a969f0b4c3a87fcfc47987a59e3495eca",
            "filename": "tests/models/llava/test_processor_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -50,7 +50,7 @@ def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n     def prepare_processor_dict(self):\n-        return {\"chat_template\": \"dummy_template\"}\n+        return {\"chat_template\": \"dummy_template\", \"patch_size\": 3, \"vision_feature_select_strategy\": \"default\"}\n \n     @unittest.skip(\n         \"Skip because the model has no processor kwargs except for chat template and\""
        },
        {
            "sha": "c90dbe056e51bba0e0e13339682cc69d96f27396",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 8,
            "deletions": 80,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -396,8 +396,10 @@ def test_small_model_integration_test(self):\n         )\n         original_input_ids = torch.load(filepath, map_location=\"cpu\")\n         # replace -200 by image_token_index (since we use token ID = 32000 for the image token)\n-        original_input_ids[original_input_ids == -200] = model.config.image_token_index\n-        assert original_input_ids[0].tolist() == inputs.input_ids[0].tolist()\n+        # remove image token indices because HF impl expands image tokens `image_seq_length` times\n+        original_input_ids = original_input_ids[original_input_ids != -200]\n+        observed_input_ids = inputs.input_ids[inputs.input_ids != model.config.image_token_index]\n+        assert original_input_ids[0].tolist() == observed_input_ids[0].tolist()\n \n         filepath = hf_hub_download(\n             repo_id=\"nielsr/test-image\",\n@@ -414,7 +416,7 @@ def test_small_model_integration_test(self):\n \n         expected_slice = torch.tensor(\n             [[-4.7695, -4.5664, -0.2788], [-10.6172, -10.8828, -2.5273], [-6.7383, -7.2422, -0.6694]],\n-            dtype=torch.float32,\n+            dtype=torch.float16,\n             device=torch_device,\n         )\n         assert torch.allclose(output.logits[0, :3, :3], expected_slice, atol=1e-3)\n@@ -518,11 +520,11 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n \n         expected_slice = torch.tensor(\n             [[-0.1287, -0.1294, -0.1284], [-0.2744, -0.2698, -0.2671], [-0.1071, -0.1091, -0.1056]],\n-            dtype=torch.float32,\n+            dtype=torch.float16,\n             device=torch_device,\n         )\n         assert torch.allclose(output.logits[0, -3:, -3:], expected_slice, atol=1e-3)\n-        assert torch.allclose(output.loss, torch.tensor(7.0206, device=torch_device), atol=1e-3)\n+        assert torch.allclose(output.loss, torch.tensor(7.0206, dtype=torch.float16, device=torch_device), atol=1e-3)\n \n         # verify generation\n         output = model.generate(**inputs, max_new_tokens=50)\n@@ -601,80 +603,6 @@ def test_padding_side_when_merging_inputs(self):\n \n             self.assertIn(\"Padding side is set to 'right' but the model is in inference mode. For correct\", logs)\n \n-    @slow\n-    @require_bitsandbytes\n-    def test_expansion_in_processing_multiimage(self):\n-        model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n-        model = LlavaNextForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n-        processor = AutoProcessor.from_pretrained(model_id)\n-\n-        prompt = \"USER: <image><image>\\nDescribe the similarity between the two images:\\nASSISTANT:\"\n-        image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n-        deer_image = Image.open(\n-            requests.get(\n-                \"https://4.img-dpreview.com/files/p/TS560x560~forums/56876524/03975b28741443319e9a94615e35667e\",\n-                stream=True,\n-            ).raw\n-        )\n-\n-        # check processing with expansion of inputs\n-        processor.vision_feature_select_strategy = \"default\"\n-        processor.patch_size = 14\n-        processor.num_additional_image_tokens = 1\n-        inputs_expanded = processor(text=prompt, images=[raw_image, deer_image], return_tensors=\"pt\").to(\n-            torch_device, torch.float16\n-        )\n-        self.assertTrue(inputs_expanded.input_ids.shape[-1] == 3969)\n-\n-        # check processing without expansion of inputs (legacy behavior)\n-        processor.vision_feature_select_strategy = None\n-        processor.patch_size = None\n-        processor.num_additional_image_tokens = None\n-        inputs = processor(text=prompt, images=[raw_image, deer_image], return_tensors=\"pt\").to(\n-            torch_device, torch.float16\n-        )\n-        self.assertTrue(inputs.input_ids.shape[-1] == 23)\n-\n-        # generate exactly 20 tokens\n-        output = model.generate(**inputs, min_new_tokens=20, max_new_tokens=20)\n-        output_expanded = model.generate(**inputs_expanded, min_new_tokens=20, max_new_tokens=20)\n-\n-        # check that both inputs are handled correctly and generate the same output\n-        self.assertListEqual(output_expanded[:, -20:].tolist(), output[:, -20:].tolist())\n-\n-    @slow\n-    @require_bitsandbytes\n-    def test_expansion_in_processing(self):\n-        model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n-        model = LlavaNextForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n-        processor = AutoProcessor.from_pretrained(model_id)\n-\n-        prompt = \"USER: <image>\\nDescribe the image:\\nASSISTANT:\"\n-        image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n-\n-        # check processing with expansion of inputs\n-        processor.vision_feature_select_strategy = \"default\"\n-        processor.patch_size = 14\n-        processor.num_additional_image_tokens = 1\n-        inputs_expanded = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n-        self.assertTrue(inputs_expanded.input_ids.shape[-1] == 2356)\n-\n-        # check processing without expansion of inputs (legacy behavior)\n-        processor.vision_feature_select_strategy = None\n-        processor.patch_size = None\n-        processor.num_additional_image_tokens = None\n-        inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n-        self.assertTrue(inputs.input_ids.shape[-1] == 17)\n-\n-        # generate exactly 20 tokens\n-        output = model.generate(**inputs, min_new_tokens=20, max_new_tokens=20)\n-        output_expanded = model.generate(**inputs_expanded, min_new_tokens=20, max_new_tokens=20)\n-\n-        # check that both inputs are handled correctly and generate the same output\n-        self.assertListEqual(output_expanded[:, -20:].tolist(), output[:, -20:].tolist())\n-\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_full_vision_state_selection(self):\n@@ -685,7 +613,7 @@ def test_small_model_integration_test_full_vision_state_selection(self):\n         # test that changing `strategy` won't error out\n         model.vision_feature_select_strategy = \"full\"\n \n-        inputs = self.processor(self.prompt, self.image, return_tensors=\"pt\")\n+        inputs = self.processor(self.prompt, self.image, return_tensors=\"pt\").to(model.device)\n \n         # verify generation\n         output = model.generate(**inputs, max_new_tokens=30)"
        },
        {
            "sha": "234e479110005417993f5f2365c3712a8a205fa9",
            "filename": "tests/models/llava_next/test_processor_llava_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -27,7 +27,7 @@\n \n \n if is_vision_available():\n-    from transformers import CLIPImageProcessor\n+    from transformers import LlavaNextImageProcessor\n \n \n @require_vision\n@@ -37,7 +37,7 @@ class LlavaNextProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n \n-        image_processor = CLIPImageProcessor()\n+        image_processor = LlavaNextImageProcessor()\n         tokenizer = LlamaTokenizerFast.from_pretrained(\"huggyllama/llama-7b\")\n         processor_kwargs = self.prepare_processor_dict()\n         processor = LlavaNextProcessor(image_processor, tokenizer, **processor_kwargs)\n@@ -50,7 +50,7 @@ def get_image_processor(self, **kwargs):\n         return LlavaNextProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n \n     def prepare_processor_dict(self):\n-        return {\"chat_template\": \"dummy_template\"}\n+        return {\"chat_template\": \"dummy_template\", \"patch_size\": 3, \"vision_feature_select_strategy\": \"default\"}\n \n     @unittest.skip(\n         \"Skip because the model has no processor kwargs except for chat template and\""
        },
        {
            "sha": "b0234fef34e806a90e2a1a0929eac1d44cc10183",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 105,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -17,7 +17,6 @@\n import unittest\n \n import numpy as np\n-import requests\n from huggingface_hub import hf_hub_download\n \n from transformers import (\n@@ -543,107 +542,3 @@ def test_padding_side_when_merging_inputs(self):\n                 model(**inputs_batched, output_hidden_states=True)\n \n             self.assertIn(\"Padding side is set to 'right' but the model is in inference mode. For correct\", logs)\n-\n-    @slow\n-    @require_bitsandbytes\n-    def test_expansion_in_processing(self):\n-        model_id = \"llava-hf/LLaVA-NeXT-Video-7B-hf\"\n-        model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n-            \"llava-hf/LLaVA-NeXT-Video-7B-hf\", load_in_4bit=True\n-        )\n-        processor = AutoProcessor.from_pretrained(model_id)\n-\n-        # check processing with expansion of inputs\n-        processor.vision_feature_select_strategy = \"default\"\n-        processor.patch_size = 14\n-        processor.num_additional_image_tokens = 1\n-        inputs_expanded = processor(self.prompt_video, videos=[self.video], return_tensors=\"pt\").to(torch_device)\n-        self.assertTrue(inputs_expanded.input_ids.shape[-1] == 1170)\n-\n-        # check processing without expansion of inputs (legacy behavior)\n-        processor.vision_feature_select_strategy = None\n-        processor.patch_size = None\n-        processor.num_additional_image_tokens = None\n-        inputs = processor(self.prompt_video, videos=[self.video], return_tensors=\"pt\").to(torch_device)\n-        self.assertTrue(inputs.input_ids.shape[-1] == 19)\n-\n-        # generate exactly 20 tokens\n-        output = model.generate(**inputs, min_new_tokens=20, max_new_tokens=20)\n-        output_expanded = model.generate(**inputs_expanded, min_new_tokens=20, max_new_tokens=20)\n-\n-        # check that both inputs are handled correctly and generate the same output\n-        self.assertListEqual(output_expanded[:, -20:].tolist(), output[:, -20:].tolist())\n-\n-    @slow\n-    @require_bitsandbytes\n-    def test_expansion_in_processing_images(self):\n-        model_id = \"llava-hf/LLaVA-NeXT-Video-7B-hf\"\n-        model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n-            \"llava-hf/LLaVA-NeXT-Video-7B-hf\", load_in_4bit=True\n-        )\n-        processor = AutoProcessor.from_pretrained(model_id)\n-\n-        # check processing with expansion of inputs\n-        processor.vision_feature_select_strategy = \"default\"\n-        processor.patch_size = 14\n-        processor.num_additional_image_tokens = 1\n-        inputs_expanded = processor(self.prompt_image, images=[self.image], return_tensors=\"pt\").to(torch_device)\n-        self.assertTrue(inputs_expanded.input_ids.shape[-1] == 2652)\n-\n-        # check processing without expansion of inputs (legacy behavior)\n-        processor.vision_feature_select_strategy = None\n-        processor.patch_size = None\n-        processor.num_additional_image_tokens = None\n-        inputs = processor(self.prompt_image, images=[self.image], return_tensors=\"pt\").to(torch_device)\n-        self.assertTrue(inputs.input_ids.shape[-1] == 19)\n-\n-        # generate exactly 20 tokens\n-        output = model.generate(**inputs, min_new_tokens=20, max_new_tokens=20)\n-        output_expanded = model.generate(**inputs_expanded, min_new_tokens=20, max_new_tokens=20)\n-\n-        # check that both inputs are handled correctly and generate the same output\n-        self.assertListEqual(output_expanded[:, -20:].tolist(), output[:, -20:].tolist())\n-\n-    @slow\n-    @require_bitsandbytes\n-    def test_expansion_in_processing_multiimage(self):\n-        model_id = \"llava-hf/LLaVA-NeXT-Video-7B-hf\"\n-        model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n-            \"llava-hf/LLaVA-NeXT-Video-7B-hf\", load_in_4bit=True\n-        )\n-        processor = AutoProcessor.from_pretrained(model_id)\n-\n-        prompt = \"USER: <image><image>\\nDescribe the similarity between the two images:\\nASSISTANT:\"\n-        image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n-        deer_image = Image.open(\n-            requests.get(\n-                \"https://4.img-dpreview.com/files/p/TS560x560~forums/56876524/03975b28741443319e9a94615e35667e\",\n-                stream=True,\n-            ).raw\n-        )\n-\n-        # check processing with expansion of inputs\n-        processor.vision_feature_select_strategy = \"default\"\n-        processor.patch_size = 14\n-        processor.num_additional_image_tokens = 1\n-        inputs_expanded = processor(text=prompt, images=[raw_image, deer_image], return_tensors=\"pt\").to(\n-            torch_device, torch.float16\n-        )\n-        self.assertTrue(inputs_expanded.input_ids.shape[-1] == 3968)\n-\n-        # check processing without expansion of inputs (legacy behavior)\n-        processor.vision_feature_select_strategy = None\n-        processor.patch_size = None\n-        processor.num_additional_image_tokens = None\n-        inputs = processor(text=prompt, images=[raw_image, deer_image], return_tensors=\"pt\").to(\n-            torch_device, torch.float16\n-        )\n-        self.assertTrue(inputs.input_ids.shape[-1] == 22)\n-\n-        # generate exactly 20 tokens\n-        output = model.generate(**inputs, min_new_tokens=20, max_new_tokens=20)\n-        output_expanded = model.generate(**inputs_expanded, min_new_tokens=20, max_new_tokens=20)\n-\n-        # check that both inputs are handled correctly and generate the same output\n-        self.assertListEqual(output_expanded[:, -20:].tolist(), output[:, -20:].tolist())"
        },
        {
            "sha": "4a7bcb45b01098a35f42b30d20aa5a9532070b65",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 16,
            "deletions": 111,
            "changes": 127,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -127,7 +127,6 @@ def __init__(\n         self.num_image_tokens = (vision_config[\"image_size\"] // vision_config[\"patch_size\"]) ** 2\n         self.num_video_tokens = (self.num_image_tokens + 1) * self.num_frames\n         self.seq_length = seq_length + self.num_image_tokens + self.num_video_tokens\n-        self.encoder_seq_length = self.seq_length\n \n     def get_config(self):\n         return VideoLlavaConfig(\n@@ -185,22 +184,6 @@ def prepare_config_and_inputs_for_common(self):\n         }\n         return config, inputs_dict\n \n-    def prepare_config_and_inputs_for_batched_test(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, _, pixel_values_videos = config_and_inputs\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 1) + 1\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-\n-        # make sure no other special tokens are set\n-        input_ids[(input_ids == 0) | (input_ids == 1)] = 3\n-        input_ids[:, 0] = config.video_token_index\n-        inputs_dict = {\n-            \"pixel_values_videos\": pixel_values_videos,\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": attention_mask,\n-        }\n-        return config, inputs_dict\n-\n \n @require_torch\n class VideoLlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n@@ -339,7 +322,7 @@ def recursive_check(batched_object, single_row_object, model_name, key):\n                     ),\n                 )\n \n-        config, batched_input = self.model_tester.prepare_config_and_inputs_for_batched_test()\n+        config, batched_input = self.model_tester.prepare_config_and_inputs_for_common()\n \n         for model_class in self.all_model_classes:\n             config.output_hidden_states = True\n@@ -457,11 +440,11 @@ def test_small_model_integration_test(self):\n             repo_id=\"raushan-testing-hf/videos-test\", filename=\"video_demo.npy\", repo_type=\"dataset\"\n         )\n         video_file = np.load(video_file)\n-        inputs = self.processor(prompt, videos=video_file, return_tensors=\"pt\")\n-\n-        EXPECTED_INPUT_IDS = torch.tensor([[1,  3148, 1001, 29901, 29871, 32001, 13, 11008, 338, 445, 4863, 2090, 1460, 29973, 319, 1799, 9047, 13566, 29901]])  # fmt: skip\n+        inputs = self.processor(prompt, videos=video_file, return_tensors=\"pt\").to(torch_device)\n \n-        self.assertTrue(torch.equal(inputs[\"input_ids\"], EXPECTED_INPUT_IDS))\n+        EXPECTED_INPUT_IDS = torch.tensor([1,  3148, 1001, 29901, 29871, 13, 11008, 338, 445, 4863, 2090, 1460, 29973, 319, 1799, 9047, 13566, 29901], device=torch_device)  # fmt: skip\n+        non_video_inputs = inputs[\"input_ids\"][inputs[\"input_ids\"] != 32001]\n+        self.assertTrue(torch.equal(non_video_inputs, EXPECTED_INPUT_IDS))\n \n         output = model.generate(**inputs, do_sample=False, max_new_tokens=20)\n         EXPECTED_DECODED_TEXT = \"USER: \\nWhy is this video funny? ASSISTANT: The video is funny because it shows a baby sitting on a bed and reading a book, which\"  # fmt: skip\n@@ -487,7 +470,9 @@ def test_small_model_integration_test_mixed_inputs(self):\n         url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         image = Image.open(requests.get(url, stream=True).raw)\n \n-        inputs = self.processor(prompts, images=[image], videos=[video_file], padding=True, return_tensors=\"pt\")\n+        inputs = self.processor(prompts, images=[image], videos=[video_file], padding=True, return_tensors=\"pt\").to(\n+            torch_device\n+        )\n         output = model.generate(**inputs, do_sample=False, max_new_tokens=20)\n \n         EXPECTED_DECODED_TEXT = [\n@@ -543,7 +528,7 @@ def test_small_model_integration_test_llama_batched(self):\n             hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"video_demo_2.npy\", repo_type=\"dataset\")\n         )\n \n-        inputs = processor(prompts, videos=[video_1, video_2], return_tensors=\"pt\", padding=True)\n+        inputs = processor(prompts, videos=[video_1, video_2], return_tensors=\"pt\", padding=True).to(torch_device)\n \n         output = model.generate(**inputs, max_new_tokens=20)\n \n@@ -583,96 +568,16 @@ def test_video_llava_merge_inputs_error_bug(self):\n         # This is a reproducer of https://github.com/huggingface/transformers/pull/28333 and makes sure it does not happen anymore\n         model = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", load_in_4bit=True)\n \n-        # Simulate some user inputs\n-        pixel_values_videos = torch.randn(\n-            (1, 8, 3, 224, 224),\n-            dtype=torch.float,\n-            device=torch_device,\n-        )\n-        # fmt: off\n-        input_ids = torch.tensor(\n-            [[32002, 32002, 1, 15043, 7084, 32001, 29871, 13, 7900]],\n-            dtype=torch.long,\n-            device=torch_device,\n-        )\n-        # fmt: on\n-        attention_mask = torch.tensor(\n-            [[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n-            dtype=torch.long,\n-            device=torch_device,\n+        prompt = \"USER: <video>\\nDescribe the video:? ASSISTANT:\"\n+        video_file = hf_hub_download(\n+            repo_id=\"raushan-testing-hf/videos-test\", filename=\"video_demo.npy\", repo_type=\"dataset\"\n         )\n+        video_file = np.load(video_file)\n+        inputs = self.processor(prompt, videos=video_file, return_tensors=\"pt\").to(torch_device, torch.float16)\n \n         # Make sure that the loss is properly computed\n         loss = model(\n-            pixel_values_videos=pixel_values_videos,\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            labels=input_ids,\n+            **inputs,\n+            labels=inputs.input_ids.clone(),\n         ).loss\n         loss.backward()\n-\n-    @slow\n-    @require_bitsandbytes\n-    def test_expansion_in_processing_images(self):\n-        model_id = \"LanguageBind/Video-LLaVA-7B-hf\"\n-        model = VideoLlavaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n-        processor = VideoLlavaProcessor.from_pretrained(model_id)\n-\n-        prompt = \"USER: <image>\\nDescribe the image in details. ASSISTANT:\"\n-        url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        image = Image.open(requests.get(url, stream=True).raw)\n-\n-        # check processing with expansion of inputs\n-        processor.vision_feature_select_strategy = \"default\"\n-        processor.patch_size = 14\n-        processor.num_additional_image_tokens = 1\n-        inputs_expanded = processor(prompt, images=image, return_tensors=\"pt\").to(torch_device, torch.float16)\n-        self.assertTrue(inputs_expanded.input_ids.shape[-1] == 274)\n-\n-        # check processing without expansion of inputs (legacy behavior)\n-        processor.vision_feature_select_strategy = None\n-        processor.patch_size = None\n-        processor.num_additional_image_tokens = None\n-        inputs = processor(prompt, images=image, return_tensors=\"pt\").to(torch_device, torch.float16)\n-        self.assertTrue(inputs.input_ids.shape[-1] == 19)\n-\n-        # generate exactly 20 tokens\n-        output = model.generate(**inputs, min_new_tokens=20, max_new_tokens=20)\n-        output_expanded = model.generate(**inputs_expanded, min_new_tokens=20, max_new_tokens=20)\n-\n-        # check that both inputs are handled correctly and generate the same output\n-        self.assertListEqual(output_expanded[:, -20:].tolist(), output[:, -20:].tolist())\n-\n-    @slow\n-    @require_bitsandbytes\n-    def test_expansion_in_processing(self):\n-        model_id = \"LanguageBind/Video-LLaVA-7B-hf\"\n-        model = VideoLlavaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n-        processor = VideoLlavaProcessor.from_pretrained(model_id)\n-\n-        prompt = \"USER: <video>\\nDescribe the video in details. ASSISTANT:\"\n-        video_file = hf_hub_download(\n-            repo_id=\"raushan-testing-hf/videos-test\", filename=\"video_demo.npy\", repo_type=\"dataset\"\n-        )\n-        video_file = np.load(video_file)\n-\n-        # check processing with expansion of inputs\n-        processor.vision_feature_select_strategy = \"default\"\n-        processor.patch_size = 14\n-        processor.num_additional_image_tokens = 1\n-        inputs_expanded = processor(prompt, videos=video_file, return_tensors=\"pt\").to(torch_device, torch.float16)\n-        self.assertTrue(inputs_expanded.input_ids.shape[-1] == 2074)\n-\n-        # check processing without expansion of inputs (legacy behavior)\n-        processor.vision_feature_select_strategy = None\n-        processor.patch_size = None\n-        processor.num_additional_image_tokens = None\n-        inputs = processor(prompt, videos=video_file, return_tensors=\"pt\").to(torch_device, torch.float16)\n-        self.assertTrue(inputs.input_ids.shape[-1] == 19)\n-\n-        # generate exactly 20 tokens\n-        output = model.generate(**inputs, min_new_tokens=20, max_new_tokens=20)\n-        output_expanded = model.generate(**inputs_expanded, min_new_tokens=20, max_new_tokens=20)\n-\n-        # check that both inputs are handled correctly and generate the same output\n-        self.assertListEqual(output_expanded[:, -20:].tolist(), output[:, -20:].tolist())"
        },
        {
            "sha": "a676defe3195f530f6854cf44d8a7ecfc473951f",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 9,
            "deletions": 55,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -320,7 +320,7 @@ def test_small_model_integration_test(self):\n \n         outputs = model.generate(**inputs, max_new_tokens=10)\n \n-        EXPECTED_OUTPUT = \"USER: <image> \\nCan you please describe this image?\\nASSISTANT: The image features a brown and white cat sitting on\"\n+        EXPECTED_OUTPUT = \"USER:  \\nCan you please describe this image?\\nASSISTANT: The image features a brown and white cat sitting on\"\n         self.assertEqual(processor.decode(outputs[0], skip_special_tokens=True), EXPECTED_OUTPUT)\n \n     @slow\n@@ -329,63 +329,17 @@ def test_vipllava_merge_inputs_error_bug(self):\n         # This is a reproducer of https://github.com/huggingface/transformers/pull/28333 and makes sure it does not happen anymore\n         model_id = \"llava-hf/vip-llava-7b-hf\"\n         model = VipLlavaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+        processor = AutoProcessor.from_pretrained(model_id)\n \n-        # Simulate some user inputs\n-        pixel_values = torch.randn(\n-            (1, 3, 336, 336),\n-            dtype=torch.float,\n-            device=torch_device,\n-        )\n-        input_ids = torch.tensor(\n-            [\n-                [32001, 32001, 1, 15043, 7084, 32000, 29871, 13, 7900],\n-            ],\n-            dtype=torch.long,\n-            device=torch_device,\n-        )\n-        attention_mask = torch.tensor(\n-            [[0, 0, 1, 1, 1, 1, 1, 1, 1]],\n-            dtype=torch.long,\n-            device=torch_device,\n-        )\n+        url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/compel-neg.png\"\n+        image = Image.open(requests.get(url, stream=True).raw)\n+        prompt = \"USER: <image>\\nCan you please describe this image?\\nASSISTANT:\"\n+\n+        inputs = processor(prompt, image, return_tensors=\"pt\").to(torch_device, torch.float16)\n \n         # Make sure that the loss is properly computed\n         loss = model(\n-            pixel_values=pixel_values,\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            labels=input_ids,\n+            **inputs,\n+            labels=inputs.input_ids.clone(),\n         ).loss\n         loss.backward()\n-\n-    @slow\n-    @require_bitsandbytes\n-    def test_expansion_in_processing(self):\n-        model_id = \"llava-hf/vip-llava-7b-hf\"\n-        model = VipLlavaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n-        processor = AutoProcessor.from_pretrained(model_id)\n-\n-        prompt = \"USER: <image>\\nDescribe the image:\\nASSISTANT:\"\n-        image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n-\n-        # check processing with expansion of inputs\n-        processor.vision_feature_select_strategy = \"default\"\n-        processor.patch_size = 14\n-        processor.num_additional_image_tokens = 1\n-        inputs_expanded = processor(prompt, raw_image, return_tensors=\"pt\").to(torch_device, torch.float16)\n-        self.assertTrue(inputs_expanded.input_ids.shape[-1] == 593)\n-\n-        # check processing without expansion of inputs (legacy behavior)\n-        processor.vision_feature_select_strategy = None\n-        processor.patch_size = None\n-        processor.num_additional_image_tokens = None\n-        inputs = processor(prompt, raw_image, return_tensors=\"pt\").to(torch_device, torch.float16)\n-        self.assertTrue(inputs.input_ids.shape[-1] == 18)\n-\n-        # generate exactly 20 tokens\n-        output = model.generate(**inputs, min_new_tokens=20, max_new_tokens=20)\n-        output_expanded = model.generate(**inputs_expanded, min_new_tokens=20, max_new_tokens=20)\n-\n-        # check that both inputs are handled correctly and generate the same output\n-        self.assertListEqual(output_expanded[:, -20:].tolist(), output[:, -20:].tolist())"
        },
        {
            "sha": "d81386f9e0c7db3969b0bd770f41fbe31e0c3027",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 18,
            "deletions": 9,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -174,8 +174,9 @@ def test_tokenizer_defaults_preserved_by_kwargs(self):\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        processor_kwargs = self.prepare_processor_dict()\n \n-        processor = self.processor_class(**processor_components)\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n         input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n@@ -195,8 +196,9 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n             \"image_processor\", do_rescale=True, rescale_factor=-1\n         )\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        processor_kwargs = self.prepare_processor_dict()\n \n-        processor = self.processor_class(**processor_components)\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = self.prepare_text_inputs()\n@@ -210,8 +212,9 @@ def test_kwargs_overrides_default_tokenizer_kwargs(self):\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", padding=\"longest\")\n+        processor_kwargs = self.prepare_processor_dict()\n \n-        processor = self.processor_class(**processor_components)\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n         input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n@@ -228,8 +231,9 @@ def test_kwargs_overrides_default_image_processor_kwargs(self):\n             \"image_processor\", do_rescale=True, rescale_factor=1\n         )\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        processor_kwargs = self.prepare_processor_dict()\n \n-        processor = self.processor_class(**processor_components)\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = self.prepare_text_inputs()\n@@ -242,7 +246,8 @@ def test_unstructured_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n-        processor = self.processor_class(**processor_components)\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = self.prepare_text_inputs()\n@@ -264,7 +269,8 @@ def test_unstructured_kwargs_batched(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n-        processor = self.processor_class(**processor_components)\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = self.prepare_text_inputs(batch_size=2)\n@@ -289,7 +295,8 @@ def test_doubly_passed_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n-        processor = self.processor_class(**processor_components)\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = [self.prepare_text_inputs()]\n@@ -307,7 +314,8 @@ def test_structured_kwargs_nested(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n-        processor = self.processor_class(**processor_components)\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = self.prepare_text_inputs()\n@@ -330,7 +338,8 @@ def test_structured_kwargs_nested_from_dict(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n-        processor = self.processor_class(**processor_components)\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n         input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()"
        },
        {
            "sha": "8fb00079f6b36afe2f9b8765aeba17379dc49142",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1681ec2b67b4228e13d37e0ef20159985a106a3/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1681ec2b67b4228e13d37e0ef20159985a106a3/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=d1681ec2b67b4228e13d37e0ef20159985a106a3",
            "patch": "@@ -299,6 +299,9 @@ def check_attribute_being_used(config_class, attributes, default_value, source_s\n         \"unk_index\",\n         \"mask_index\",\n         \"image_token_index\",  # for VLMs\n+        \"video_token_index\",\n+        \"image_seq_length\",\n+        \"video_seq_length\",\n         \"image_size\",\n         \"use_cache\",\n         \"out_features\","
        }
    ],
    "stats": {
        "total": 1227,
        "additions": 198,
        "deletions": 1029
    }
}