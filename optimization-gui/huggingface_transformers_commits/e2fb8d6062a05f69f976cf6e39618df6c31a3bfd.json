{
    "author": "zucchini-nlp",
    "message": "ðŸš¨  Generalize `get_decoder()` for multimodal and delete redundant code ðŸ”ª  (#42156)\n\n* update some models\n\n* update the rest\n\n* add helper for encoder\n\n* delete encoder code from models\n\n* fix copies\n\n* fix some tests but VLM will fail\n\n* add encider tests simialr to decoder\n\n* no print\n\n* fix overwritten models\n\n* and a million exceptions with old audio models, revert back",
    "sha": "e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
    "files": [
        {
            "sha": "333fa759734f541935a67f1385b4e46099241eee",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 72,
            "deletions": 21,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -2205,26 +2205,78 @@ def disable_input_require_grads(self):\n         \"\"\"\n         self._require_grads_hook.remove()\n \n+    def get_encoder(self, modality: Optional[str] = None):\n+        \"\"\"\n+        Best-effort lookup of the *encoder* module. If provided with `modality` argument,\n+        it looks for a modality-specific encoder in multimodal models (e.g. \"image_encoder\")\n+        By default the function returns model's text encoder if any, and otherwise returns `self`.\n+\n+        Possible `modality` values are \"image\", \"video\" and \"audio\".\n+        \"\"\"\n+        # NOTE: new models need to use existing names for layers if possible, so this list doesn't grow infinitely\n+        if modality in [\"image\", \"video\"]:\n+            possible_module_names = [\"vision_tower\", \"visual\", \"vision_model\", \"vision_encoder\", \"image_tower\"]\n+        elif modality == \"audio\":\n+            possible_module_names = [\"audio_tower\", \"audio_encoder\", \"speech_encoder\"]\n+        elif modality is None:\n+            possible_module_names = [\"text_encoder\", \"encoder\"]\n+        else:\n+            raise ValueError(f'Unnrecognized modality, has to be \"image\", \"video\" or \"audio\" but found {modality}')\n+\n+        for name in possible_module_names:\n+            if hasattr(self, name):\n+                return getattr(self, name)\n+\n+        if self.base_model is not self and hasattr(self.base_model, \"get_encoder\"):\n+            return self.base_model.get_encoder(modality=modality)\n+\n+        # If this is a base transformer model (no encoder/model attributes), return self\n+        return self\n+\n+    def set_encoder(self, encoder, modality: Optional[str] = None):\n+        \"\"\"\n+        Symmetric setter. Mirrors the lookup logic used in `get_encoder`.\n+        \"\"\"\n+\n+        # NOTE: new models need to use existing names for layers if possible, so this list doesn't grow infinitely\n+        if modality in [\"image\", \"video\"]:\n+            possible_module_names = [\"vision_tower\", \"visual\", \"vision_model\", \"vision_encoder\", \"image_tower\"]\n+        if modality == \"audio\":\n+            possible_module_names = [\"audio_tower\", \"audio_encoder\"]\n+        elif modality is None:\n+            possible_module_names = [\"text_encoder\", \"encoder\"]\n+        else:\n+            raise ValueError(f'Unnrecognized modality, has to be \"image\", \"video\" or \"audio\" but found {modality}')\n+\n+        for name in possible_module_names:\n+            if hasattr(self, name):\n+                setattr(self, name, encoder)\n+                return\n+\n+        if self.base_model is not self:\n+            if hasattr(self.base_model, \"set_encoder\"):\n+                self.base_model.set_encoder(encoder, modality=modality)\n+            else:\n+                self.model = encoder\n+\n     def get_decoder(self):\n         \"\"\"\n         Best-effort lookup of the *decoder* module.\n \n         Order of attempts (covers ~85 % of current usages):\n \n-        1. `self.decoder`\n-        2. `self.model`                       (many wrappers store the decoder here)\n-        3. `self.model.get_decoder()`         (nested wrappers)\n+        1. `self.decoder/self.language_model/self.text_model`\n+        2. `self.base_model`                  (many wrappers store the decoder here)\n+        3. `self.base_model.get_decoder()`    (nested wrappers)\n         4. fallback: raise for the few exotic models that need a bespoke rule\n         \"\"\"\n-        if hasattr(self, \"decoder\"):\n-            return self.decoder\n+        possible_module_names = [\"language_model\", \"text_model\", \"decoder\", \"text_decoder\"]\n+        for name in possible_module_names:\n+            if hasattr(self, name):\n+                return getattr(self, name)\n \n-        if hasattr(self, \"model\"):\n-            inner = self.model\n-            # See: https://github.com/huggingface/transformers/issues/40815\n-            if hasattr(inner, \"get_decoder\") and type(inner) is not type(self):\n-                return inner.get_decoder()\n-            return inner\n+        if self.base_model is not self and hasattr(self.base_model, \"get_decoder\"):\n+            return self.base_model.get_decoder()\n \n         # If this is a base transformer model (no decoder/model attributes), return self\n         # This handles cases like MistralModel which is itself the decoder\n@@ -2235,19 +2287,18 @@ def set_decoder(self, decoder):\n         Symmetric setter. Mirrors the lookup logic used in `get_decoder`.\n         \"\"\"\n \n-        if hasattr(self, \"decoder\"):\n-            self.decoder = decoder\n-            return\n+        possible_module_names = [\"language_model\", \"text_model\", \"decoder\"]\n+        for name in possible_module_names:\n+            if hasattr(self, name):\n+                print(name)\n+                setattr(self, name, decoder)\n+                return\n \n-        if hasattr(self, \"model\"):\n-            inner = self.model\n-            if hasattr(inner, \"set_decoder\"):\n-                inner.set_decoder(decoder)\n+        if self.base_model is not self:\n+            if hasattr(self.base_model, \"set_decoder\"):\n+                self.base_model.set_decoder(decoder)\n             else:\n                 self.model = decoder\n-            return\n-\n-        return\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "dbf73ce85b0a3f9d0b11e7a6da7545a1294bdb2e",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -910,12 +910,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1075,12 +1069,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1093,19 +1081,6 @@ def get_image_features(\n             vision_feature_layer=vision_feature_layer,\n         )\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        return self.model.multi_modal_projector\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "c026c8a6d206b7a1e1153f7d70cf96406d5770d2",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1342,9 +1342,6 @@ def create_network_inputs(\n             )\n         return reshaped_lagged_sequence, features, loc, scale, static_feat\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1588,12 +1585,6 @@ def __init__(self, config: AutoformerConfig):\n     def output_params(self, decoder_output):\n         return self.parameter_projection(decoder_output[:, -self.config.prediction_length :, :])\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     @torch.jit.ignore\n     def output_distribution(self, params, loc=None, scale=None, trailing_n=None) -> torch.distributions.Distribution:\n         sliced_params = params"
        },
        {
            "sha": "742d7374aef2c48dd2c27e3833cb1ee63154d34a",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -181,12 +181,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -357,12 +351,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -377,19 +365,6 @@ def get_image_features(\n             **kwargs,\n         )\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        return self.model.multi_modal_projector\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "d663821578a223d2fb2d0062c8a7b9bb5210c9d0",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -905,9 +905,6 @@ def set_input_embeddings(self, value):\n         self.encoder.embed_tokens = self.shared\n         self.decoder.embed_tokens = self.shared\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1037,12 +1034,6 @@ def __init__(self, config: BartConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def resize_token_embeddings(\n         self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n     ) -> nn.Embedding:\n@@ -1498,12 +1489,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def set_decoder(self, decoder):\n-        self.model.decoder = decoder\n-\n-    def get_decoder(self):\n-        return self.model.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "263d308c47d1664e20688ffd36723a6e44d649a7",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -2083,9 +2083,6 @@ def set_input_embeddings(self, value):\n         self.encoder.embed_tokens = self.shared\n         self.decoder.embed_tokens = self.shared\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -2205,12 +2202,6 @@ def __init__(self, config: BigBirdPegasusConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def resize_token_embeddings(\n         self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n     ) -> nn.Embedding:\n@@ -2609,12 +2600,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def set_decoder(self, decoder):\n-        self.model.decoder = decoder\n-\n-    def get_decoder(self):\n-        return self.model.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "833d529671c9cf8910af1a2c66da96cde9d29199",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -869,9 +869,6 @@ def set_input_embeddings(self, value):\n         self.encoder.embed_tokens = self.shared\n         self.decoder.embed_tokens = self.shared\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1009,12 +1006,6 @@ def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.P\n \n         return super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def resize_token_embeddings(\n         self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n     ) -> nn.Embedding:\n@@ -1189,12 +1180,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def set_decoder(self, decoder):\n-        self.model.decoder = decoder\n-\n-    def get_decoder(self):\n-        return self.model.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "92f2f993fafac25bfc4589abce7bb52db1d83ff3",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -842,9 +842,6 @@ def set_input_embeddings(self, value):\n         self.encoder.embed_tokens = self.shared\n         self.decoder.embed_tokens = self.shared\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -969,12 +966,6 @@ def __init__(self, config: BlenderbotSmallConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def resize_token_embeddings(\n         self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n     ) -> nn.Embedding:\n@@ -1149,12 +1140,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def set_decoder(self, decoder):\n-        self.model.decoder = decoder\n-\n-    def get_decoder(self):\n-        return self.model.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "d6ee68a6680b00a83071ab0f698f6173a5ee6fa7",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1058,11 +1058,11 @@ def set_output_embeddings(self, new_embeddings):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.language_model.get_output_embeddings()\n \n-    def get_encoder(self):\n-        return self.language_model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n+    def get_encoder(self, modality=None):\n+        if modality is None:\n+            return self.language_model.get_encoder()\n+        else:\n+            return super().get_encoder(modality=modality)\n \n     @filter_out_non_signature_kwargs()\n     @auto_docstring\n@@ -1579,11 +1579,11 @@ def set_output_embeddings(self, new_embeddings):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.language_model.get_output_embeddings()\n \n-    def get_encoder(self):\n-        return self.language_model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n+    def get_encoder(self, modality=None):\n+        if modality is None:\n+            return self.language_model.get_encoder()\n+        else:\n+            return super().get_encoder(modality=modality)\n \n     def _preprocess_accelerate(self):\n         r\"\"\""
        },
        {
            "sha": "2ebfa7f044cd87f447cbce90e3c78807665e823e",
            "filename": "src/transformers/models/cohere2_vision/modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -167,12 +167,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_image_features(self, pixel_values: torch.FloatTensor):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n@@ -285,28 +279,9 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_image_features(self, pixel_values: torch.FloatTensor):\n         return self.model.get_image_features(pixel_values=pixel_values)\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        return self.model.multi_modal_projector\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "3d0ca8d3a84be077a7b5f286fab4bca25bf5b95b",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -171,10 +171,10 @@ def forward(\n \n         # Custom data preparation to fix an issue with the gradient flow when training with multiple GPUs.\n         if inputs_embeds is None:\n-            inputs_embeds = self.vlm.language_model.embed_tokens(input_ids)\n+            inputs_embeds = self.vlm.get_input_embeddings()(input_ids)\n \n             if pixel_values is not None:\n-                image_embeds = self.vlm.visual(pixel_values, grid_thw=image_grid_thw)\n+                image_embeds = self.vlm.model.visual(pixel_values, grid_thw=image_grid_thw)\n                 image_mask = (\n                     (input_ids == self.config.vlm_config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n                 )"
        },
        {
            "sha": "6e468e6ae3fac4381db36f97042c2d6102532a6b",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -352,10 +352,10 @@ def forward(\n \n         # Custom data preparation to fix an issue with the gradient flow when training with multiple GPUs.\n         if inputs_embeds is None:\n-            inputs_embeds = self.vlm.language_model.embed_tokens(input_ids)\n+            inputs_embeds = self.vlm.get_input_embeddings()(input_ids)\n \n             if pixel_values is not None:\n-                image_embeds = self.vlm.visual(pixel_values, grid_thw=image_grid_thw)\n+                image_embeds = self.vlm.model.visual(pixel_values, grid_thw=image_grid_thw)\n                 image_mask = (\n                     (input_ids == self.config.vlm_config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n                 )"
        },
        {
            "sha": "3721678dd68869f3aa7169af664a7394c6280a62",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1324,9 +1324,6 @@ def __init__(self, config: ConditionalDetrConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def freeze_backbone(self):\n         for name, param in self.backbone.conv_encoder.model.named_parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "86953cd47ecf70feb2505cc33059235fabef91b4",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1198,9 +1198,6 @@ def __init__(self, config: DFineConfig):\n \n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def freeze_backbone(self):\n         for param in self.backbone.parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "a1337acefd7adc77dc2aa79bb3d9abc9f5bbb44f",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1202,9 +1202,6 @@ def __init__(self, config: DabDetrConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def freeze_backbone(self):\n         for name, param in self.backbone.conv_encoder.model.named_parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "a8133c85b57341d1467d4cf7cde7029ae377380a",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1359,9 +1359,6 @@ def __init__(self, config: DeformableDetrConfig):\n \n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def freeze_backbone(self):\n         for name, param in self.backbone.conv_encoder.model.named_parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "1727c9b337005ac588d79af695ce77bd68a7a48f",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1058,9 +1058,6 @@ def __init__(self, config: DetrConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def freeze_backbone(self):\n         for name, param in self.backbone.conv_encoder.model.named_parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "3a0ddf6e3f90e5cacf18b6002a1a17edb219a0ca",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -686,9 +686,6 @@ def __init__(self, config: DiaConfig):\n         self.decoder = DiaDecoder(config.decoder_config)\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     @can_return_tuple\n     def forward(\n@@ -825,12 +822,6 @@ def __init__(self, config: DiaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     @auto_docstring\n     @can_return_tuple\n     def forward("
        },
        {
            "sha": "3c6a6b3d17cb5fac639f5a4cd490082368261ab8",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -478,9 +478,6 @@ def __init__(self, config: DiaConfig):\n         self.decoder = DiaDecoder(config.decoder_config)\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     @can_return_tuple\n     def forward(\n@@ -617,12 +614,6 @@ def __init__(self, config: DiaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     @auto_docstring\n     @can_return_tuple\n     def forward("
        },
        {
            "sha": "750c844e65aefeb4f447442a6ab71e003aa24019",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1354,12 +1354,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.text_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.text_model = decoder\n-\n-    def get_decoder(self):\n-        return self.text_model\n-\n     def get_image_tokens(self, pixel_values: torch.FloatTensor, image_sizes: torch.LongTensor):\n         \"\"\"\n         Tokenizes images into discrete tokens with VQGAN module. Converts\n@@ -1514,25 +1508,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n-    # Make modules available through conditional class for BC\n-    @property\n-    def text_model(self):\n-        return self.model.text_model\n-\n-    @property\n-    def vqmodel(self):\n-        return self.model.vqmodel\n-\n-    @property\n-    def vocabulary_mapping(self):\n-        return self.model.vocabulary_mapping\n-\n     def decode_image_tokens(self, **kwargs):\n         return self.model.decode_image_tokens(**kwargs)\n "
        },
        {
            "sha": "9e1e2ba4f5f0e5f8d6afc7e024c89e9d6e1b8abf",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -908,12 +908,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.text_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.text_model = decoder\n-\n-    def get_decoder(self):\n-        return self.text_model\n-\n     def get_image_tokens(self, pixel_values: torch.FloatTensor, image_sizes: torch.LongTensor):\n         \"\"\"\n         Tokenizes images into discrete tokens with VQGAN module. Converts\n@@ -1068,25 +1062,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n-    # Make modules available through conditional class for BC\n-    @property\n-    def text_model(self):\n-        return self.model.text_model\n-\n-    @property\n-    def vqmodel(self):\n-        return self.model.vqmodel\n-\n-    @property\n-    def vocabulary_mapping(self):\n-        return self.model.vocabulary_mapping\n-\n     def decode_image_tokens(self, **kwargs):\n         return self.model.decode_image_tokens(**kwargs)\n "
        },
        {
            "sha": "50fb2c56dce8d864c282c938d2474eaa41d6d283",
            "filename": "src/transformers/models/encodec/modeling_encodec.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -498,9 +498,6 @@ def __init__(self, config: EncodecConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def _encode_frame(\n         self, input_values: torch.Tensor, bandwidth: float\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:"
        },
        {
            "sha": "64b972c69047aafd245804568a71818b99b3ee5d",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -173,9 +173,6 @@ def _init_weights(self, module):\n         elif module in self.decoder.modules():\n             self.decoder._init_weights(module)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def get_input_embeddings(self):\n         return self.encoder.get_input_embeddings()\n "
        },
        {
            "sha": "9e761c0a8c20c2be991579356869c08775a35e9e",
            "filename": "src/transformers/models/florence2/modeling_florence2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 30,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -652,12 +652,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n-\n     def get_image_features(self, pixel_values: torch.Tensor, **kwargs):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n@@ -775,8 +769,11 @@ def forward(\n             image_hidden_states=image_features if pixel_values is not None else None,\n         )\n \n-    def get_encoder(self):\n-        return self.language_model.get_encoder()\n+    def get_encoder(self, modality=None):\n+        if modality is None:\n+            return self.language_model.get_encoder()\n+        else:\n+            return super().get_encoder(modality=modality)\n \n \n def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n@@ -821,28 +818,9 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_image_features(self, pixel_values: torch.Tensor, **kwargs):\n         return self.model.get_image_features(pixel_values=pixel_values, **kwargs)\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        return self.model.multi_modal_projector\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -979,9 +957,6 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n     def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):"
        },
        {
            "sha": "86bd02bda077248e8ecc86c1507ed07914cd1286",
            "filename": "src/transformers/models/florence2/modular_florence2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1513,11 +1513,11 @@ def __init__(self, config: Florence2Config):\n         super().__init__(config)\n         self.vision_tower = Florence2VisionBackbone(config=config.vision_config)\n \n-    def get_encoder(self):\n-        return self.language_model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n+    def get_encoder(self, modality=None):\n+        if modality is None:\n+            return self.language_model.get_encoder()\n+        else:\n+            return super().get_encoder(modality=modality)\n \n     def get_image_features(self, pixel_values: torch.Tensor, **kwargs):\n         \"\"\"\n@@ -1624,9 +1624,6 @@ class Florence2ForConditionalGeneration(LlavaForConditionalGeneration):\n         \"lm_head.weight\": \"model.language_model.shared.weight\",\n     }\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n     def get_image_features(self, pixel_values: torch.Tensor, **kwargs):\n         return self.model.get_image_features(pixel_values=pixel_values, **kwargs)\n "
        },
        {
            "sha": "4bac57096cb4fc97d1e310a3f8bb463779c80f8a",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -827,9 +827,6 @@ def __init__(self, config: FSMTConfig):\n         self.decoder = FSMTDecoder(config)\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1066,12 +1063,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id)\n \n-    def get_encoder(self):\n-        return self.model.encoder\n-\n-    def get_decoder(self):\n-        return self.model.decoder\n-\n     def get_output_embeddings(self):\n         return self.model.decoder.embed_tokens\n "
        },
        {
            "sha": "c4983e007ba7d49281fd3afcc21da8e4eb62ea9b",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -72,12 +72,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def gather_continuous_embeddings(\n         self,\n         word_embeddings: torch.Tensor,\n@@ -260,12 +254,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "8e93ef9231b5fd9617f63c37be74982a54595877",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -872,12 +872,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n         Projects the last hidden state from the vision model into language model space.\n@@ -1062,28 +1056,9 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_image_features(self, pixel_values):\n         return self.model.get_image_features(pixel_values)\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        return self.model.multi_modal_projector\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "cc0b919bc85cf7276543368aba2522e4b4ca0cfc",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -2107,12 +2107,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n         Projects the last hidden state from the vision model into language model space.\n@@ -2361,28 +2355,9 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_image_features(self, pixel_values):\n         return self.model.get_image_features(pixel_values)\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        raise AttributeError(\"Use embed_vision instead of multi_modal_projector.\")\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -2557,10 +2532,6 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    @property\n-    def audio_tower(self):\n-        return self.model.audio_tower\n-\n \n __all__ = [\n     \"Gemma3nAudioEncoder\","
        },
        {
            "sha": "9dc324b694115519c1fc47cd864d15fe1e0b70f7",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -2421,14 +2421,6 @@ def get_audio_features(\n class Gemma3nForConditionalGeneration(PaliGemmaForConditionalGeneration):\n     _checkpoint_conversion_mapping = {}\n \n-    @property\n-    def audio_tower(self):\n-        return self.model.audio_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        raise AttributeError(\"Use embed_vision instead of multi_modal_projector.\")\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "2f00d22fb040432c47b4826317220cf790d61c7d",
            "filename": "src/transformers/models/glm46v/modeling_glm46v.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fglm46v%2Fmodeling_glm46v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fglm46v%2Fmodeling_glm46v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm46v%2Fmodeling_glm46v.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -100,12 +100,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_rope_index(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -533,12 +527,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_video_features(\n         self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n     ):\n@@ -547,15 +535,6 @@ def get_video_features(\n     def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n         return self.model.get_image_features(pixel_values, image_grid_thw)\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def visual(self):\n-        return self.model.visual\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "ff5e0a00cc0de1fdadf6346276b95fbec5599656",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -948,12 +948,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_rope_index(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1381,12 +1375,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_video_features(\n         self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n     ):\n@@ -1395,15 +1383,6 @@ def get_video_features(\n     def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n         return self.model.get_image_features(pixel_values, image_grid_thw)\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def visual(self):\n-        return self.model.visual\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "3ea0ef86c23f1ee8c98e5d044eaf38e24be697d8",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1112,12 +1112,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_rope_index(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1598,12 +1592,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_video_features(\n         self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n     ):\n@@ -1612,15 +1600,6 @@ def get_video_features(\n     def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n         return self.model.get_image_features(pixel_values, image_grid_thw)\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def visual(self):\n-        return self.model.visual\n-\n     @auto_docstring\n     @check_model_inputs()\n     def forward("
        },
        {
            "sha": "3fd5a2c2dbfb67ea262b1cc746ae24a50ebc956d",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -551,12 +551,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -684,12 +678,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -704,19 +692,6 @@ def get_image_features(\n             **kwargs,\n         )\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        return self.model.multi_modal_projector\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "7e3bc1d577bffbcd3c794cff9a0a54abb46f78df",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1278,9 +1278,6 @@ def create_network_inputs(\n \n         return transformer_inputs, loc, scale, static_feat\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1550,12 +1547,6 @@ def __init__(self, config: InformerConfig):\n     def output_params(self, dec_output):\n         return self.parameter_projection(dec_output)\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     @torch.jit.ignore\n     def output_distribution(self, params, loc=None, scale=None, trailing_n=None) -> torch.distributions.Distribution:\n         sliced_params = params"
        },
        {
            "sha": "5e4a2a7e864bc81745e7f45039e626fde6d48ed1",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1142,8 +1142,11 @@ def set_output_embeddings(self, new_embeddings):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.language_model.get_output_embeddings()\n \n-    def get_encoder(self):\n-        return self.language_model.get_encoder()\n+    def get_encoder(self, modality=None):\n+        if modality is None:\n+            return self.language_model.get_encoder()\n+        else:\n+            return super().get_encoder(modality=modality)\n \n     def get_decoder(self):\n         return self.language_model.get_decoder()"
        },
        {
            "sha": "2268ba28893ba825440ba435cc3bb457c5d8fda3",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1172,8 +1172,11 @@ def set_output_embeddings(self, new_embeddings):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.language_model.get_output_embeddings()\n \n-    def get_encoder(self):\n-        return self.language_model.get_encoder()\n+    def get_encoder(self, modality=None):\n+        if modality is None:\n+            return self.language_model.get_encoder()\n+        else:\n+            return super().get_encoder(modality=modality)\n \n     def get_decoder(self):\n         return self.language_model.get_decoder()"
        },
        {
            "sha": "51691e0ba4abf0805d8a76408da688c0912c6236",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -549,12 +549,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -787,12 +781,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -807,19 +795,6 @@ def get_image_features(\n             **kwargs,\n         )\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        return self.model.multi_modal_projector\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "409caf2091ad534fa37e8e479f26952dcc5c05b5",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1771,9 +1771,6 @@ def set_input_embeddings(self, value):\n         self.encoder.embed_tokens = self.shared\n         self.decoder.embed_tokens = self.shared\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1907,12 +1904,6 @@ def __init__(self, config: LEDConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.led.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.led.get_decoder()\n-\n     def resize_token_embeddings(\n         self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n     ) -> nn.Embedding:"
        },
        {
            "sha": "ce46c62baeab37c7d2742dbff1236d78b9297830",
            "filename": "src/transformers/models/lfm2_vl/modeling_lfm2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -161,12 +161,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -324,12 +318,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -344,19 +332,6 @@ def get_image_features(\n             **kwargs,\n         )\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        return self.model.multi_modal_projector\n-\n     @can_return_tuple\n     def forward(\n         self,"
        },
        {
            "sha": "2947295a4775c5f2d9f9d712dc264a1c5577f390",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -147,12 +147,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -332,12 +326,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -352,19 +340,6 @@ def get_image_features(\n             **kwargs,\n         )\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        return self.model.multi_modal_projector\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "fffd56a941c53acc0f3b4ff08e3011b56c1e05cb",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -279,12 +279,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n         \"\"\"\n         Reshape, unpad and then pack each image_feature into a single image_features tensor containing all visual vectors.\n@@ -562,12 +556,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n         return self.model.pack_image_features(\n             image_features=image_features,\n@@ -590,19 +578,6 @@ def get_image_features(\n             vision_feature_select_strategy=vision_feature_select_strategy,\n         )\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        return self.model.multi_modal_projector\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "6e79d602eb940c67b1fae05e24939be21176b6ee",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -331,12 +331,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n         \"\"\"\n         Reshape, unpad and then pack each image_feature into a single image_features tensor containing all visual vectors.\n@@ -701,12 +695,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n         return self.model.pack_image_features(\n             image_features=image_features,\n@@ -729,19 +717,6 @@ def get_image_features(\n             vision_feature_select_strategy=vision_feature_select_strategy,\n         )\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        return self.model.multi_modal_projector\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "70d17ff3e6d44f8b1d49fb1443efb6fcc1c3123b",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -290,12 +290,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def pack_image_features(self, image_features, image_sizes, image_newline=None, vision_aspect_ratio=\"anyres_max_9\"):\n         \"\"\"\n         Reshape, unpad and then pack each image_feature into a single image_features tensor containing all visual vectors.\n@@ -689,12 +683,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n         return self.model.pack_image_features(\n             image_features=image_features,\n@@ -717,19 +705,6 @@ def get_image_features(\n             vision_feature_select_strategy=vision_feature_select_strategy,\n         )\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        return self.model.multi_modal_projector\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "12c7df3310eb55b2274637ed598fdfae3315bf73",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1602,9 +1602,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1769,9 +1766,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1943,9 +1937,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.shared = new_embeddings\n         self.encoder.set_input_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "4fe228b4021d19dd108220451f2729537bdf460c",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -925,9 +925,6 @@ def set_input_embeddings(self, value):\n         self.encoder.embed_tokens = self.shared\n         self.decoder.embed_tokens = self.shared\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1032,12 +1029,6 @@ def __init__(self, config: M2M100Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "960898693b4d2140e8d24c2979b61b43779603e4",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -885,9 +885,6 @@ def set_decoder_input_embeddings(self, value):\n             )\n         self.decoder.embed_tokens = value\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def resize_decoder_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n         if self.config.share_encoder_decoder_embeddings:\n             raise ValueError(\n@@ -1051,12 +1048,6 @@ def __init__(self, config: MarianConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def resize_token_embeddings(\n         self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n     ) -> nn.Embedding:\n@@ -1281,12 +1272,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def set_decoder(self, decoder):\n-        self.model.decoder = decoder\n-\n-    def get_decoder(self):\n-        return self.model.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "dd8619fc6c94aff820b5629cdba6b946198409aa",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -903,9 +903,6 @@ def set_input_embeddings(self, value):\n         self.encoder.embed_tokens = self.shared\n         self.decoder.embed_tokens = self.shared\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1022,12 +1019,6 @@ def __init__(self, config: MBartConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def resize_token_embeddings(\n         self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n     ) -> nn.Embedding:\n@@ -1473,12 +1464,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def set_decoder(self, decoder):\n-        self.model.decoder = decoder\n-\n-    def get_decoder(self):\n-        return self.model.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "82f2868a1421519608f9e1e07f9314ffb819474c",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1453,9 +1453,6 @@ def __init__(self, config: MimiConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def _encode_frame(\n         self,\n         input_values: torch.Tensor,"
        },
        {
            "sha": "0f6e2a1d3efc34d49a394d32d36e0c0e0fa8fda1",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -213,12 +213,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -383,12 +377,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -403,19 +391,6 @@ def get_image_features(\n             **kwargs,\n         )\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        return self.model.multi_modal_projector\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "a2d303782bdde314f5701e6611c9ad96faf9e1c4",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1465,12 +1465,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     @check_model_inputs()\n     @can_return_tuple\n     @auto_docstring\n@@ -1600,21 +1594,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_model(self):\n-        return self.model.vision_model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "373e1db4a217ea262d310f072ff4f044fc9ac54c",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -860,9 +860,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.decoder.embed_tokens = value\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def freeze_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the Moonshine encoder so that its parameters will\n@@ -1019,12 +1016,6 @@ def __init__(self, config: MoonshineConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_output_embeddings(self):\n         return self.proj_out\n "
        },
        {
            "sha": "1922428f8f8abbb156fb3a98ae86b310bd5dec91",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -774,12 +774,6 @@ def __init__(self, config: MoonshineConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_output_embeddings(self):\n         return self.proj_out\n "
        },
        {
            "sha": "259caa7a5dafc85e7d48860a1b547dcb3b9c021c",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1601,9 +1601,6 @@ def __init__(self, config: MoshiConfig):\n         self.num_codebooks = config.num_codebooks\n         self.post_init()\n \n-    def get_audio_encoder(self):\n-        return self.audio_encoder\n-\n     def get_depth_decoder(self):\n         return self.depth_decoder\n "
        },
        {
            "sha": "1f05521accbe023054024c47a419490f1583828c",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -881,10 +881,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    # Copied from transformers.models.t5.modeling_t5.T5Model.get_encoder\n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     # Copied from transformers.models.t5.modeling_t5.T5Model.forward with google-t5/->google/, T5->MT5, t5->mt5\n     def forward(\n@@ -1067,10 +1063,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_encoder\n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.forward with google-t5/->google/, T5->MT5, t5->mt5\n     def forward(\n@@ -1266,10 +1258,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.shared = new_embeddings\n         self.encoder.set_input_embeddings(new_embeddings)\n \n-    # Copied from transformers.models.t5.modeling_t5.T5EncoderModel.get_encoder\n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     # Copied from transformers.models.t5.modeling_t5.T5EncoderModel.forward with google-t5/->google/, T5->MT5, t5->mt5\n     def forward(\n@@ -1582,10 +1570,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    # Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.get_encoder\n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     # Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.forward\n     def forward("
        },
        {
            "sha": "1bb423c63d8b9c1b64613d72e93c0204ed0874f9",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 18,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -815,12 +815,6 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_heads = new_embeddings\n \n-    def set_decoder(self, decoder):\n-        self.model.decoder = decoder\n-\n-    def get_decoder(self):\n-        return self.model.decoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1398,16 +1392,6 @@ def __init__(\n         # tie text encoder, decoder weights if config set accordingly\n         self.post_init()\n \n-    def get_audio_encoder(self):\n-        return self.audio_encoder\n-\n-    def get_text_encoder(self):\n-        return self.text_encoder\n-\n-    def get_encoder(self):\n-        # get the text encoder to compute the encoder hidden-states for generation\n-        return self.get_text_encoder()\n-\n     def get_input_embeddings(self):\n         return self.text_encoder.get_input_embeddings()\n \n@@ -1898,7 +1882,7 @@ def _prepare_text_encoder_kwargs_for_generation(\n         generation_config: GenerationConfig,\n     ) -> dict[str, Any]:\n         # 1. get text encoder\n-        encoder = self.get_text_encoder()\n+        encoder = self.get_encoder()\n         # Compatibility with Accelerate big model inference: we need the encoder to outputs stuff on the same device\n         # as the inputs.\n         if hasattr(encoder, \"_hf_hook\"):\n@@ -1943,7 +1927,7 @@ def _prepare_audio_encoder_kwargs_for_generation(\n         self, input_values, model_kwargs, model_input_name: Optional[str] = None\n     ):\n         # 1. get audio encoder\n-        encoder = self.get_audio_encoder()\n+        encoder = self.get_encoder(modality=\"audio\")\n         # Compatibility with Accelerate big model inference: we need the encoder to outputs stuff on the same device\n         # as the inputs.\n         if hasattr(encoder, \"_hf_hook\"):"
        },
        {
            "sha": "279c984fe6b6870e792e9800fda7519d9f605c8d",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -769,12 +769,6 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_heads = new_embeddings\n \n-    def set_decoder(self, decoder):\n-        self.model.decoder = decoder\n-\n-    def get_decoder(self):\n-        return self.model.decoder\n-\n     @auto_docstring\n     # Ignore copy\n     def forward(\n@@ -1318,13 +1312,6 @@ def _init_weights(self, module):\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n \n-    def get_text_encoder(self):\n-        return self.text_encoder\n-\n-    def get_encoder(self):\n-        # get the text encoder to compute the conditioning hidden-states for generation\n-        return self.get_text_encoder()\n-\n     def get_input_embeddings(self):\n         return self.text_encoder.get_input_embeddings()\n \n@@ -1824,7 +1811,7 @@ def _prepare_encoder_hidden_states_kwargs_for_generation(\n \n         # 1. condition on text\n         if inputs_tensor is not None:\n-            encoder = self.get_text_encoder()\n+            encoder = self.get_encoder()\n             # Compatibility with Accelerate big model inference: we need the encoder to outputs stuff on the same device\n             # as the inputs.\n             if hasattr(encoder, \"_hf_hook\"):"
        },
        {
            "sha": "1d315d3059121313dafc8ff02b35f665be81990b",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -893,9 +893,6 @@ def set_input_embeddings(self, value):\n         self.encoder.embed_tokens = self.shared\n         self.decoder.embed_tokens = self.shared\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def set_lightweight_tuning(self):\n         assert self.use_prompt, \"If you want to use lightweight tuning, make sure that `use_prompt=True`.\"\n \n@@ -1031,12 +1028,6 @@ def __init__(self, config: MvpConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def resize_token_embeddings(\n         self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n     ) -> nn.Embedding:\n@@ -1537,12 +1528,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def set_decoder(self, decoder):\n-        self.model.decoder = decoder\n-\n-    def get_decoder(self):\n-        return self.model.decoder\n-\n     def set_lightweight_tuning(self):\n         self.model.set_lightweight_tuning()\n         self.lm_head.requires_grad_(False)"
        },
        {
            "sha": "a457abddc6cb67d2cf958357354563e45b6a4a2e",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -893,9 +893,6 @@ def set_input_embeddings(self, value):\n         self.encoder.embed_tokens = self.shared\n         self.decoder.embed_tokens = self.shared\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     @can_return_tuple\n     def forward(\n@@ -1066,12 +1063,6 @@ def __init__(self, config: NllbMoeConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "6368c1b1131d0d1f0513e25a2dbf0f772364931d",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -720,12 +720,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def set_decoder(self, decoder):\n-        self.model.decoder = decoder\n-\n-    def get_decoder(self):\n-        return self.model.decoder\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "d990e08190b659ea4d96a992effa6ed580a811c3",
            "filename": "src/transformers/models/ovis2/modeling_ovis2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -518,12 +518,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -688,28 +682,9 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_image_features(self, pixel_values: torch.FloatTensor):\n         return self.model.get_image_features(pixel_values=pixel_values)\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        raise AttributeError(\"Not needed for Ovis2\")\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "0acf99daafe687590931dec3e58a5a44ae96181a",
            "filename": "src/transformers/models/ovis2/modular_ovis2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -338,10 +338,6 @@ def __init__(self, config: Ovis2Config):\n         super().__init__(config)\n         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n \n-    @property\n-    def multi_modal_projector(self):\n-        raise AttributeError(\"Not needed for Ovis2\")\n-\n     def get_image_features(self, pixel_values: torch.FloatTensor):\n         return self.model.get_image_features(pixel_values=pixel_values)\n "
        },
        {
            "sha": "63538043506bcdeff41a1766024087f29aae0cde",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -257,12 +257,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_image_features(self, pixel_values: torch.FloatTensor):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n@@ -450,28 +444,9 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_image_features(self, pixel_values):\n         return self.model.get_image_features(pixel_values)\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        return self.model.multi_modal_projector\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "41ff955d574f1d90162aa371aba66e4da91b7751",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -907,9 +907,6 @@ def set_input_embeddings(self, value):\n         self.encoder.embed_tokens = self.shared\n         self.decoder.embed_tokens = self.shared\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def resize_position_embeddings(self, new_num_position_embeddings: int):\n         \"\"\"\n         Resizes position embeddings matrix of the model if `new_num_position_embeddings !=\n@@ -1058,12 +1055,6 @@ def __init__(self, config: PegasusConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def resize_token_embeddings(\n         self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n     ) -> nn.Embedding:\n@@ -1253,12 +1244,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def set_decoder(self, decoder):\n-        self.model.decoder = decoder\n-\n-    def get_decoder(self):\n-        return self.model.decoder\n-\n     def get_position_embeddings(self) -> nn.Embedding:\n         \"\"\"\n         Returns the position embeddings matrix"
        },
        {
            "sha": "ffe26a3be7cead6016125c3e861ed8002c062d2a",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1202,9 +1202,6 @@ def set_input_embeddings(self, value):\n         self.encoder.embed_tokens = self.shared\n         self.decoder.embed_tokens = self.shared\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def resize_position_embeddings(self, new_num_position_embeddings: int):\n         \"\"\"\n         Resizes position embeddings matrix of the model if `new_num_position_embeddings !=\n@@ -1351,12 +1348,6 @@ def __init__(self, config: PegasusXConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def resize_position_embeddings(self, new_num_position_embeddings: int):\n         \"\"\"\n         Resizes position embeddings matrix of the model if `new_num_position_embeddings !="
        },
        {
            "sha": "6e6724288b06b2cfd0aa0103aa5b6085c6e8e44f",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -180,12 +180,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -340,12 +334,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "823163e23749992e665c4dd1b0f627e022e4b1e5",
            "filename": "src/transformers/models/perception_lm/modular_perception_lm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -424,15 +424,6 @@ def forward(\n     def get_image_features(self, **kwargs):\n         raise AttributeError(\"Not needed for PerceptionLM\")\n \n-    def language_model(self):\n-        raise AttributeError(\"Not needed for PerceptionLM\")\n-\n-    def vision_tower(self):\n-        raise AttributeError(\"Not needed for PerceptionLM\")\n-\n-    def multi_modal_projector(self):\n-        raise AttributeError(\"Not needed for PerceptionLM\")\n-\n \n __all__ = [\n     \"PerceptionLMForConditionalGeneration\","
        },
        {
            "sha": "79565625b6ff61e706a3f48bd7368e71821d6f08",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1343,9 +1343,6 @@ def get_output_embeddings(self) -> nn.Module:\n     def set_output_embeddings(self, new_embeddings):\n         self.decoder.set_output_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "13f033b67fcc4c05c510299d42651652f88139c9",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -851,9 +851,6 @@ def set_input_embeddings(self, value):\n         self.encoder.embed_tokens = self.shared\n         self.decoder.embed_tokens = self.shared\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -972,12 +969,6 @@ def __init__(self, config: PLBartConfig):\n \n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def resize_token_embeddings(\n         self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n     ) -> nn.Embedding:\n@@ -1309,12 +1300,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def set_decoder(self, decoder):\n-        self.model.decoder = decoder\n-\n-    def get_decoder(self):\n-        return self.model.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "e060fbb1d27810a44e2311bb27344f657eba9990",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -92,9 +92,6 @@ def set_input_embeddings(self, value):\n         self.encoder.embed_tokens = self.shared\n         self.decoder.embed_tokens = self.shared\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -213,12 +210,6 @@ def __init__(self, config: PLBartConfig):\n \n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def resize_token_embeddings(\n         self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n     ) -> nn.Embedding:"
        },
        {
            "sha": "546e64af7550ab970fe43cb000e9ea459d8dc638",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -984,9 +984,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def get_mel_conditioner_outputs(\n         self,\n         input_features: torch.FloatTensor,"
        },
        {
            "sha": "5aec96458d766b57c0a89f559118035b1065783c",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1400,9 +1400,6 @@ def set_input_embeddings(self, value):\n         self.encoder.word_embeddings = self.word_embeddings\n         self.decoder.word_embeddings = self.word_embeddings\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1671,11 +1668,11 @@ def _compute_loss(self, logits, labels, ignore_index=-100):\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return self._shift_right(labels)\n \n-    def get_encoder(self):\n-        return self.prophetnet.encoder\n-\n-    def get_decoder(self):\n-        return self.prophetnet.decoder\n+    def get_encoder(self, modality=None):\n+        if modality is None:\n+            return self.prophetnet.encoder\n+        else:\n+            return super().get_encoder(modality=modality)\n \n \n @auto_docstring(\n@@ -1711,12 +1708,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.prophetnet.decoder.word_embeddings = value\n \n-    def set_decoder(self, decoder):\n-        self.prophetnet.decoder = decoder\n-\n-    def get_decoder(self):\n-        return self.prophetnet.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "b18e1e9f24dd9caa4ab018ab69598ce11e7178f7",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -972,12 +972,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_rope_index(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1390,12 +1384,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_video_features(\n         self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n     ):\n@@ -1404,15 +1392,6 @@ def get_video_features(\n     def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n         return self.model.get_image_features(pixel_values, image_grid_thw)\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def visual(self):\n-        return self.model.visual\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "e058cceb1fa9810b52711cfcfe46c04b858d6bfe",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -943,12 +943,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_rope_index(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1288,12 +1282,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_video_features(\n         self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n     ):\n@@ -1302,15 +1290,6 @@ def get_video_features(\n     def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n         return self.model.get_image_features(pixel_values, image_grid_thw)\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def visual(self):\n-        return self.model.visual\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "d215f689da656a80632d6db118999d4b853cf750",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -937,12 +937,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_rope_index(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1318,12 +1312,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_video_features(\n         self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n     ):\n@@ -1332,15 +1320,6 @@ def get_video_features(\n     def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n         return self.model.get_image_features(pixel_values, image_grid_thw)\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def visual(self):\n-        return self.model.visual\n-\n     @check_model_inputs()\n     def forward(\n         self,"
        },
        {
            "sha": "4d7b41f290a5d8ad3a0ba5187e3d0c35b3a08084",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1094,12 +1094,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_rope_index(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1528,12 +1522,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_video_features(\n         self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n     ):\n@@ -1542,15 +1530,6 @@ def get_video_features(\n     def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n         return self.model.get_image_features(pixel_values, image_grid_thw)\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def visual(self):\n-        return self.model.visual\n-\n     @check_model_inputs()\n     def forward(\n         self,"
        },
        {
            "sha": "bfdf4dd75faea1c59da3e419125d180527facb8b",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1543,9 +1543,6 @@ def __init__(self, config: RTDetrConfig):\n \n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def freeze_backbone(self):\n         for param in self.backbone.parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "33cefeb1729c72b542861482e739f7d6c436e5f9",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1439,9 +1439,6 @@ def __init__(self, config: RTDetrV2Config):\n \n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def freeze_backbone(self):\n         for param in self.backbone.parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "7f317bb3b5f03750f3ff76bfbac11f76dded8e25",
            "filename": "src/transformers/models/shieldgemma2/modeling_shieldgemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -70,12 +70,6 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.model.language_model.set_output_embeddings(new_embeddings)\n \n-    def set_decoder(self, decoder):\n-        self.model.language_model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.language_model.get_decoder()\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "5a2babb00d7b6b9de6445681021b48391e162317",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -148,9 +148,6 @@ def __init__(\n \n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def get_input_embeddings(self):\n         return self.decoder.get_input_embeddings()\n "
        },
        {
            "sha": "e2078910afe5a9bd90e17cdc59e21b867072e917",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -884,9 +884,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.decoder.embed_tokens = value\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1022,12 +1019,6 @@ def __init__(self, config: Speech2TextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "6c7ce5b911ae353943818dff2e5f229e3cfe9bc7",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1882,9 +1882,6 @@ def set_input_embeddings(self, value):\n         if isinstance(self.decoder, SpeechT5DecoderWithTextPrenet):\n             self.decoder.set_input_embeddings(value)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -2021,12 +2018,6 @@ def __init__(self, config: SpeechT5Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.speecht5.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.speecht5.get_decoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -2348,12 +2339,6 @@ def can_generate(cls) -> bool:\n         # but we need to override it so as to do `GenerationConfig` handling in multiple parts of the codebase.\n         return True\n \n-    def get_encoder(self):\n-        return self.speecht5.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.speecht5.get_decoder()\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -2685,12 +2670,6 @@ def __init__(self, config: SpeechT5Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.speecht5.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.speecht5.get_decoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will"
        },
        {
            "sha": "9867677e703ac672385312c92115086decb028a3",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -940,9 +940,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     @can_return_tuple\n     def forward(\n@@ -1098,9 +1095,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     @can_return_tuple\n     def forward(\n@@ -1246,9 +1240,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.shared = new_embeddings\n         self.encoder.set_input_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     @check_model_inputs()\n     def forward("
        },
        {
            "sha": "e5004f41d2012ab3e1804e07767b782b95910aef",
            "filename": "src/transformers/models/switch_transformers/modular_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -696,9 +696,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     @can_return_tuple\n     def forward(\n@@ -789,9 +786,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     @can_return_tuple\n     def forward(\n@@ -937,9 +931,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.shared = new_embeddings\n         self.encoder.set_input_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     @check_model_inputs()\n     def forward("
        },
        {
            "sha": "051fd8a5e7d032133dd1c781b96613441938a88b",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -863,9 +863,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1030,9 +1027,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1206,9 +1200,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.shared = new_embeddings\n         self.encoder.set_input_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1513,9 +1504,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "ac9b64929280e05678e045a1ef6ecbeb8a276bd9",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -885,9 +885,6 @@ def __init__(self, config: T5GemmaConfig):\n \n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def get_input_embeddings(self):\n         return self.encoder.get_input_embeddings()\n \n@@ -1014,12 +1011,6 @@ def set_output_embeddings(self, new_embeddings):\n     def get_output_embeddings(self):\n         return self.lm_head.out_proj\n \n-    def get_encoder(self):\n-        return self.model.encoder\n-\n-    def get_decoder(self):\n-        return self.model.decoder\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "3fa256a3c99f44f7d2623c09cfe0341a4be4e338",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -892,9 +892,6 @@ def __init__(self, config: T5GemmaConfig):\n \n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def get_input_embeddings(self):\n         return self.encoder.get_input_embeddings()\n \n@@ -1021,12 +1018,6 @@ def set_output_embeddings(self, new_embeddings):\n     def get_output_embeddings(self):\n         return self.lm_head.out_proj\n \n-    def get_encoder(self):\n-        return self.model.encoder\n-\n-    def get_decoder(self):\n-        return self.model.decoder\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "b788b2c663592753235e7face66b7600c58cdc87",
            "filename": "src/transformers/models/table_transformer/modeling_table_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1023,9 +1023,6 @@ def __init__(self, config: TableTransformerConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def freeze_backbone(self):\n         for name, param in self.backbone.conv_encoder.model.named_parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "d2476457d7f9715eef87b5f9ce148407fb3922d8",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1057,9 +1057,6 @@ def create_network_inputs(\n \n         return transformer_inputs, loc, scale, static_feat\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1297,12 +1294,6 @@ def __init__(self, config: TimeSeriesTransformerConfig):\n     def output_params(self, dec_output):\n         return self.parameter_projection(dec_output)\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     @torch.jit.ignore\n     def output_distribution(self, params, loc=None, scale=None, trailing_n=None) -> torch.distributions.Distribution:\n         sliced_params = params"
        },
        {
            "sha": "b968922713560ac28a454be7ab968ea5245aaed1",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -671,12 +671,6 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.output_projection = new_embeddings\n \n-    def set_decoder(self, decoder):\n-        self.model.decoder = decoder\n-\n-    def get_decoder(self):\n-        return self.model.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "c20fae835ad5fe9f4afb9343f90daa1a340a3439",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1455,9 +1455,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1635,9 +1632,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1815,9 +1809,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.shared = new_embeddings\n         self.encoder.set_input_embeddings(new_embeddings)\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "0b7b03ec1c08878953c4b31094de494fe1cf4eb3",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -950,10 +950,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    # Copied from transformers.models.t5.modeling_t5.T5Model.get_encoder\n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1134,10 +1130,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_encoder\n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1330,10 +1322,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.shared = new_embeddings\n         self.encoder.set_input_embeddings(new_embeddings)\n \n-    # Copied from transformers.models.t5.modeling_t5.T5EncoderModel.get_encoder\n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     # Copied from transformers.models.t5.modeling_t5.T5EncoderModel.forward with T5->UMT5, google-t5/t5-small->google/umt5-small, t5#training->umt5#training\n     def forward(\n@@ -1645,10 +1633,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    # Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.get_encoder\n-    def get_encoder(self):\n-        return self.encoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "8442a4a7c175ddfe7a0c414887cd499fa4d828dc",
            "filename": "src/transformers/models/video_llama_3/modeling_video_llama_3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -537,12 +537,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_video_features(\n         self,\n         pixel_values_videos: torch.FloatTensor,\n@@ -761,12 +755,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_video_features(\n         self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n     ):\n@@ -775,11 +763,6 @@ def get_video_features(\n     def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n         return self.model.get_image_features(pixel_values, image_grid_thw)\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1103,10 +1086,6 @@ def _expand_dict_for_generation(dict_to_expand):\n \n         return input_ids, model_kwargs\n \n-    @property\n-    def vision_model(self):\n-        return self.model.vision_model\n-\n \n __all__ = [\n     \"VideoLlama3VisionModel\","
        },
        {
            "sha": "0bd05ec9acf86012a3abc4568572b28cef042360",
            "filename": "src/transformers/models/video_llama_3/modular_video_llama_3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -750,13 +750,6 @@ class VideoLlama3ForConditionalGeneration(Qwen2VLForConditionalGeneration):\n     def __init__(self, config: VideoLlama3Config):\n         super().__init__(config)  # just to add type hint on config\n \n-    def visual(self):\n-        raise AttributeError(\"Not needed for VideoLLaMA3\")\n-\n-    @property\n-    def vision_model(self):\n-        return self.model.vision_model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "29184ca8a165ae4f71827610e793bc692b95fc08",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -172,12 +172,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_image_features(\n         self,\n         pixel_values_images: torch.FloatTensor,\n@@ -432,12 +426,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_image_features(\n         self,\n         pixel_values_images: torch.FloatTensor,\n@@ -450,23 +438,6 @@ def get_image_features(\n             vision_feature_select_strategy=vision_feature_select_strategy,\n         )\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def video_tower(self):\n-        return self.model.video_tower\n-\n-    @property\n-    def image_tower(self):\n-        return self.model.image_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        return self.model.multi_modal_projector\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "1f1d5b04b5a5f3d2710db8b0ccee2f080489fc13",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -150,12 +150,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_image_features(\n         self, pixel_values: torch.FloatTensor, vision_feature_layers: Optional[Union[int, list[int]]] = None\n     ):\n@@ -310,30 +304,11 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_image_features(\n         self, pixel_values: torch.FloatTensor, vision_feature_layers: Optional[Union[int, list[int]]] = None\n     ):\n         return self.model.get_image_features(pixel_values=pixel_values, vision_feature_layers=vision_feature_layers)\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        return self.model.multi_modal_projector\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "87fb80dd2aa84c4fee25a18ba90982bd4845e212",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -147,9 +147,6 @@ def __init__(\n \n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def get_input_embeddings(self):\n         return self.decoder.get_input_embeddings()\n "
        },
        {
            "sha": "1f1b9a96a94e1c16975268d256ea4c130a9f83f9",
            "filename": "src/transformers/models/vits/modeling_vits.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -1265,9 +1265,6 @@ def __init__(self, config: VitsConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.text_encoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "c143e7a2ec8fdb34f0f1f16bfdba98133c8df967",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -916,9 +916,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.decoder.embed_tokens = value\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def freeze_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the Whisper encoder so that its parameters will\n@@ -1099,12 +1096,6 @@ def __init__(self, config: WhisperConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.model.get_encoder()\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_output_embeddings(self):\n         return self.proj_out\n \n@@ -1294,12 +1285,6 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.model.decoder = decoder\n-\n-    def get_decoder(self):\n-        return self.model.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "a7a40d8877874e0ac33787ab665c2f6e0e2bfede",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -3030,7 +3030,7 @@ def test_sdpa_can_dispatch_composite_models(self):\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n                 model_sdpa = model_class.from_pretrained(tmpdirname)\n-                model_sdpa = model_sdpa.eval().to(torch_device)\n+                model_sdpa = model_sdpa.base_model\n \n                 vision_model_names = {\"visual\", \"image_tower\", \"vision_tower\", \"vision_model\"}\n                 language_model_names = {\"language_model\", \"model\", \"text_model\"}\n@@ -3048,7 +3048,7 @@ def test_sdpa_can_dispatch_composite_models(self):\n                 self.assertTrue(vision_model_sdpa.config._attn_implementation == vision_attn)\n \n                 model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n-                model_eager = model_eager.eval().to(torch_device)\n+                model_eager = model_eager.base_model\n                 self.assertTrue(getattr(model_eager, language_model_name).config._attn_implementation == \"eager\")\n                 self.assertTrue(getattr(model_eager, vision_model_name).config._attn_implementation == \"eager\")\n "
        },
        {
            "sha": "cb6111722a33741c67167376a6329b9a93dab7d9",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 230,
            "deletions": 20,
            "changes": 250,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2fb8d6062a05f69f976cf6e39618df6c31a3bfd/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=e2fb8d6062a05f69f976cf6e39618df6c31a3bfd",
            "patch": "@@ -40,6 +40,7 @@\n     AutoModelForSequenceClassification,\n     BartConfig,\n     BartForConditionalGeneration,\n+    BartModel,\n     CLIPTextModelWithProjection,\n     DynamicCache,\n     GPT2Config,\n@@ -109,6 +110,8 @@\n         BertModel,\n         CLIPTextModel,\n         GenerationMixin,\n+        MusicgenConfig,\n+        MusicgenForConditionalGeneration,\n         PreTrainedModel,\n         T5Config,\n         T5ForConditionalGeneration,\n@@ -578,37 +581,37 @@ def test_model_from_config_dtype_composite(self):\n         \"\"\"\n         # Load without dtype specified\n         model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA)\n-        self.assertEqual(model.language_model.dtype, torch.float32)\n-        self.assertEqual(model.vision_tower.dtype, torch.float32)\n+        self.assertEqual(model.model.language_model.dtype, torch.float32)\n+        self.assertEqual(model.model.vision_tower.dtype, torch.float32)\n         self.assertIsInstance(model.config.dtype, torch.dtype)\n \n         # should be able to set dtype as a simple string and the model loads it correctly\n         model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, dtype=\"float32\")\n-        self.assertEqual(model.language_model.dtype, torch.float32)\n-        self.assertEqual(model.vision_tower.dtype, torch.float32)\n+        self.assertEqual(model.model.language_model.dtype, torch.float32)\n+        self.assertEqual(model.model.vision_tower.dtype, torch.float32)\n         self.assertIsInstance(model.config.dtype, torch.dtype)\n \n         model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, dtype=torch.float16)\n-        self.assertEqual(model.language_model.dtype, torch.float16)\n-        self.assertEqual(model.vision_tower.dtype, torch.float16)\n+        self.assertEqual(model.model.language_model.dtype, torch.float16)\n+        self.assertEqual(model.model.vision_tower.dtype, torch.float16)\n         self.assertIsInstance(model.config.dtype, torch.dtype)\n \n         # should be able to set dtype as a dict for each sub-config\n         model = LlavaForConditionalGeneration.from_pretrained(\n             TINY_LLAVA, dtype={\"text_config\": \"float32\", \"vision_config\": \"float16\", \"\": \"bfloat16\"}\n         )\n-        self.assertEqual(model.language_model.dtype, torch.float32)\n-        self.assertEqual(model.vision_tower.dtype, torch.float16)\n-        self.assertEqual(model.multi_modal_projector.linear_1.weight.dtype, torch.bfloat16)\n+        self.assertEqual(model.model.language_model.dtype, torch.float32)\n+        self.assertEqual(model.model.vision_tower.dtype, torch.float16)\n+        self.assertEqual(model.model.multi_modal_projector.linear_1.weight.dtype, torch.bfloat16)\n         self.assertIsInstance(model.config.dtype, torch.dtype)\n \n         # should be able to set the values as torch.dtype (not str)\n         model = LlavaForConditionalGeneration.from_pretrained(\n             TINY_LLAVA, dtype={\"text_config\": torch.float32, \"vision_config\": torch.float16, \"\": torch.bfloat16}\n         )\n-        self.assertEqual(model.language_model.dtype, torch.float32)\n-        self.assertEqual(model.vision_tower.dtype, torch.float16)\n-        self.assertEqual(model.multi_modal_projector.linear_1.weight.dtype, torch.bfloat16)\n+        self.assertEqual(model.model.language_model.dtype, torch.float32)\n+        self.assertEqual(model.model.vision_tower.dtype, torch.float16)\n+        self.assertEqual(model.model.multi_modal_projector.linear_1.weight.dtype, torch.bfloat16)\n         self.assertIsInstance(model.config.dtype, torch.dtype)\n \n         # should be able to set the values in configs directly and pass it to `from_pretrained`\n@@ -617,17 +620,17 @@ def test_model_from_config_dtype_composite(self):\n         config.vision_config.dtype = torch.bfloat16\n         config.dtype = torch.float16\n         model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, config=config, dtype=\"auto\")\n-        self.assertEqual(model.language_model.dtype, torch.float32)\n-        self.assertEqual(model.vision_tower.dtype, torch.bfloat16)\n-        self.assertEqual(model.multi_modal_projector.linear_1.weight.dtype, torch.float16)\n+        self.assertEqual(model.model.language_model.dtype, torch.float32)\n+        self.assertEqual(model.model.vision_tower.dtype, torch.bfloat16)\n+        self.assertEqual(model.model.multi_modal_projector.linear_1.weight.dtype, torch.float16)\n         self.assertIsInstance(model.config.dtype, torch.dtype)\n \n         # but if the model has `_keep_in_fp32_modules` then those modules should be in fp32 no matter what\n         LlavaForConditionalGeneration._keep_in_fp32_modules = [\"multi_modal_projector\"]\n         model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, config=config, dtype=\"auto\")\n-        self.assertEqual(model.language_model.dtype, torch.float32)\n-        self.assertEqual(model.vision_tower.dtype, torch.bfloat16)\n-        self.assertEqual(model.multi_modal_projector.linear_1.weight.dtype, torch.float32)\n+        self.assertEqual(model.model.language_model.dtype, torch.float32)\n+        self.assertEqual(model.model.vision_tower.dtype, torch.bfloat16)\n+        self.assertEqual(model.model.multi_modal_projector.linear_1.weight.dtype, torch.float32)\n         self.assertIsInstance(model.config.dtype, torch.dtype)\n \n         # torch.set_default_dtype() supports only float dtypes, so will fail with non-float type\n@@ -3131,7 +3134,7 @@ def test_nested_wrapper_recursion(self):\n         model = GPT2LMHeadModel(cfg)\n         dec = model.get_decoder()\n \n-        assert dec is model, f\"GPT2 get_decoder() should return self (fallback), got {type(dec)}\"\n+        assert dec is model.transformer, f\"GPT2 get_decoder() should return self (fallback), got {type(dec)}\"\n \n     def test_model_without_get_decoder(self):\n         \"\"\"Test edge case where model has model attribute but no get_decoder method.\"\"\"\n@@ -3191,4 +3194,211 @@ def test_vision_language_model(self):\n         model = LlavaForConditionalGeneration(cfg)\n         dec = model.get_decoder()\n \n-        assert dec is model.language_model, f\"LLaVA get_decoder() should return language_model, got {type(dec)}\"\n+        assert dec is model.model.language_model, f\"LLaVA get_decoder() should return language_model, got {type(dec)}\"\n+\n+\n+class TestGetEncoder(unittest.TestCase):\n+    def test_seq2seq_lm_get_encoder_returns_encoder(self):\n+        cfg = BartConfig(\n+            vocab_size=128,\n+            d_model=32,\n+            encoder_layers=2,\n+            decoder_layers=2,\n+            encoder_attention_heads=4,\n+            decoder_attention_heads=4,\n+            encoder_ffn_dim=64,\n+            decoder_ffn_dim=64,\n+        )\n+        model = BartForConditionalGeneration(cfg)\n+        encoder = model.get_encoder()\n+\n+        assert encoder is model.model.encoder, (\n+            f\"Expected get_encoder() to return model.model.encoder, got {type(encoder)}\"\n+        )\n+\n+    def test_base_model_returns_encoder(self):\n+        cfg = BartConfig(\n+            vocab_size=128,\n+            d_model=32,\n+            encoder_layers=2,\n+            decoder_layers=2,\n+            encoder_attention_heads=4,\n+            decoder_attention_heads=4,\n+            encoder_ffn_dim=64,\n+            decoder_ffn_dim=64,\n+        )\n+        model = BartModel(cfg)\n+        encoder = model.get_encoder()\n+\n+        assert encoder is model.encoder, f\"Expected get_encoder() to return  model.encoder, got {type(encoder)}\"\n+\n+    def test_decoder_only_model_returns_self(self):\n+        \"\"\"Test that decoder-only models (no encoder) return self.\"\"\"\n+        cfg = MistralConfig(\n+            vocab_size=128,\n+            hidden_size=32,\n+            intermediate_size=64,\n+            num_hidden_layers=2,\n+            num_attention_heads=4,\n+        )\n+        model = MistralModel(cfg)\n+        encoder = model.get_encoder()\n+\n+        assert encoder is model, f\"Base model get_encoder() should return self, got {type(encoder)}\"\n+\n+    def test_when_encoder_has_different_name(self):\n+        \"\"\"Test models with non-standard name for encoder modular (Musicgen has `self.model.text_encoder`).\"\"\"\n+        cfg = MusicgenConfig(\n+            text_encoder={\n+                \"model_type\": \"t5\",\n+                \"vocab_size\": 99,\n+                \"d_model\": 32,\n+                \"d_ff\": 37,\n+                \"num_layers\": 2,\n+                \"num_heads\": 2,\n+            },\n+            audio_encoder={\n+                \"model_type\": \"encodec\",\n+                \"hidden_size\": 99,\n+                \"compress\": 1,\n+                \"num_filters\": 2,\n+                \"codebook_size\": 32,\n+                \"codebook_dim\": 32,\n+            },\n+            decoder={\n+                \"vocab_size\": 99,\n+                \"ffn_dim\": 32,\n+                \"num_attention_heads\": 2,\n+                \"hidden_size\": 32,\n+                \"num_hidden_layers\": 2,\n+            },\n+        )\n+        model = MusicgenForConditionalGeneration(cfg)\n+        encoder = model.get_encoder()\n+\n+        assert encoder is model.text_encoder, (\n+            f\"MusicgenForConditionalGeneration get_encoder() should return model.model.text_encoder, got {type(encoder)}\"\n+        )\n+\n+    def test_audio_encoder(self):\n+        \"\"\"Test models with multiple modality encoders (Musicgen has `self.model.audio_encoder`).\"\"\"\n+        cfg = MusicgenConfig(\n+            text_encoder={\n+                \"model_type\": \"t5\",\n+                \"vocab_size\": 99,\n+                \"d_model\": 32,\n+                \"d_ff\": 37,\n+                \"num_layers\": 2,\n+                \"num_heads\": 2,\n+            },\n+            audio_encoder={\n+                \"model_type\": \"encodec\",\n+                \"hidden_size\": 99,\n+                \"compress\": 1,\n+                \"num_filters\": 2,\n+                \"codebook_size\": 32,\n+                \"codebook_dim\": 32,\n+            },\n+            decoder={\n+                \"vocab_size\": 99,\n+                \"ffn_dim\": 32,\n+                \"num_attention_heads\": 2,\n+                \"hidden_size\": 32,\n+                \"num_hidden_layers\": 2,\n+            },\n+        )\n+        model = MusicgenForConditionalGeneration(cfg)\n+        encoder = model.get_encoder(modality=\"audio\")\n+\n+        assert encoder is model.audio_encoder, (\n+            f\"MusicgenForConditionalGeneration get_encoder(modality='audio') should return model.model.audio_encoder, got {type(encoder)}\"\n+        )\n+\n+    def test_non_existant_modality_throws_error(self):\n+        \"\"\"Test that an error is thrown when a rquested modality does not exist.\"\"\"\n+        cfg = MistralConfig(\n+            vocab_size=128,\n+            hidden_size=32,\n+            intermediate_size=64,\n+            num_hidden_layers=2,\n+            num_attention_heads=4,\n+        )\n+        model = MistralModel(cfg)\n+        with self.assertRaises(ValueError):\n+            _ = model.get_encoder(modality=\"3d\")\n+\n+    def test_encoder_return_self_when_modality_not_found(self):\n+        \"\"\"Test that `self` is returned if the model has no encoder for requested modality.\"\"\"\n+        cfg = MistralConfig(\n+            vocab_size=128,\n+            hidden_size=32,\n+            intermediate_size=64,\n+            num_hidden_layers=2,\n+            num_attention_heads=4,\n+        )\n+        model = MistralModel(cfg)\n+        encoder = model.get_encoder(modality=\"image\")\n+\n+        assert encoder is model, f\"Mistral get_encoder(modality='image') should return self, got {type(encoder)}\"\n+\n+    def test_model_without_get_encoder(self):\n+        \"\"\"Test edge case where model has model attribute but no get_encoder method.\"\"\"\n+\n+        class MockInnerModel:\n+            \"\"\"Mock model without get_encoder method.\"\"\"\n+\n+            pass\n+\n+        class MockWrapperModel:\n+            \"\"\"Mock wrapper with model attribute but inner has no get_encoder.\"\"\"\n+\n+            def __init__(self):\n+                self.model = MockInnerModel()\n+\n+            def get_encoder(self):\n+                if hasattr(self, \"encoder\"):\n+                    return self.encoder\n+                if hasattr(self, \"model\"):\n+                    inner = self.model\n+                    if hasattr(inner, \"get_encoder\") and type(inner) is not type(self):\n+                        return inner.get_encoder()\n+                    return inner\n+                return self\n+\n+        wrapper = MockWrapperModel()\n+        encoder = wrapper.get_encoder()\n+\n+        assert encoder is wrapper.model, f\"Should return inner model when no get_encoder, got {type(encoder)}\"\n+\n+    def test_vision_language_model(self):\n+        \"\"\"Test vision-language models like LLaVA can find the modality encoder (\"image\").\"\"\"\n+        text_config = MistralConfig(\n+            vocab_size=128,\n+            hidden_size=32,\n+            intermediate_size=64,\n+            num_hidden_layers=2,\n+            num_attention_heads=4,\n+        )\n+\n+        vision_config = {\n+            \"hidden_size\": 32,\n+            \"intermediate_size\": 64,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"num_channels\": 3,\n+            \"image_size\": 224,\n+            \"patch_size\": 16,\n+        }\n+\n+        cfg = LlavaConfig(\n+            text_config=text_config.to_dict(),\n+            vision_config=vision_config,\n+            vocab_size=128,\n+        )\n+\n+        model = LlavaForConditionalGeneration(cfg)\n+        image_encoder = model.get_encoder(modality=\"image\")\n+\n+        assert image_encoder is model.model.vision_tower, (\n+            f\"LLaVA get_encoder(modality='image') should return vision_tower, got {type(image_encoder)}\"\n+        )"
        }
    ],
    "stats": {
        "total": 1676,
        "additions": 346,
        "deletions": 1330
    }
}