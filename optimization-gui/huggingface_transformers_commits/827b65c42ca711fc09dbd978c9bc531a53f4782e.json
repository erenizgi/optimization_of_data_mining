{
    "author": "Cyrilvallez",
    "message": "Add VideoProcessors to auto-backend requirements (#40843)\n\n* add it\n\n* fix existing ones\n\n* add perception to auto_mapping...",
    "sha": "827b65c42ca711fc09dbd978c9bc531a53f4782e",
    "files": [
        {
            "sha": "b9a5c2204fd13b05e80a2e29f6eb8e9542846a40",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=827b65c42ca711fc09dbd978c9bc531a53f4782e",
            "patch": "@@ -52,6 +52,7 @@\n             (\"internvl\", \"InternVLVideoProcessor\"),\n             (\"llava_next_video\", \"LlavaNextVideoVideoProcessor\"),\n             (\"llava_onevision\", \"LlavaOnevisionVideoProcessor\"),\n+            (\"perception_lm\", \"PerceptionLMVideoProcessor\"),\n             (\"qwen2_5_omni\", \"Qwen2VLVideoProcessor\"),\n             (\"qwen2_5_vl\", \"Qwen2VLVideoProcessor\"),\n             (\"qwen2_vl\", \"Qwen2VLVideoProcessor\"),"
        },
        {
            "sha": "0986c414f1d374589c2f14cdcc514917c09344a2",
            "filename": "src/transformers/models/glm4v/video_processing_glm4v.py",
            "status": "modified",
            "additions": 6,
            "deletions": 25,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py?ref=827b65c42ca711fc09dbd978c9bc531a53f4782e",
            "patch": "@@ -18,40 +18,22 @@\n from typing import Optional, Union\n \n import numpy as np\n+import torch\n \n-from ...image_processing_utils import (\n-    BatchFeature,\n-)\n+from ...image_processing_utils import BatchFeature\n from ...image_utils import (\n     OPENAI_CLIP_MEAN,\n     OPENAI_CLIP_STD,\n     ChannelDimension,\n+    PILImageResampling,\n     SizeDict,\n     get_image_size,\n )\n from ...processing_utils import Unpack, VideosKwargs\n-from ...utils import (\n-    TensorType,\n-    add_start_docstrings,\n-    is_torch_available,\n-    is_vision_available,\n-)\n-from .image_processing_glm4v import smart_resize\n-\n-\n-if is_torch_available():\n-    import torch\n-\n-from ...utils.import_utils import requires\n-from ...video_processing_utils import (\n-    BASE_VIDEO_PROCESSOR_DOCSTRING,\n-    BaseVideoProcessor,\n-)\n+from ...utils import TensorType, add_start_docstrings\n+from ...video_processing_utils import BASE_VIDEO_PROCESSOR_DOCSTRING, BaseVideoProcessor\n from ...video_utils import VideoMetadata, group_videos_by_shape, reorder_videos\n-\n-\n-if is_vision_available():\n-    from ...image_utils import PILImageResampling\n+from .image_processing_glm4v import smart_resize\n \n \n class Glm4vVideoProcessorInitKwargs(VideosKwargs):\n@@ -75,7 +57,6 @@ class Glm4vVideoProcessorInitKwargs(VideosKwargs):\n             The merge size of the vision encoder to llm encoder.\n     \"\"\",\n )\n-@requires(backends=(\"torchvision\",))\n class Glm4vVideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.BICUBIC\n     size = {\"shortest_edge\": 112 * 112, \"longest_edge\": 28 * 28 * 2 * 30000}"
        },
        {
            "sha": "f4f482c5631332673880868fb4f2b6b6aa40ad4e",
            "filename": "src/transformers/models/instructblipvideo/video_processing_instructblipvideo.py",
            "status": "modified",
            "additions": 8,
            "deletions": 26,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py?ref=827b65c42ca711fc09dbd978c9bc531a53f4782e",
            "patch": "@@ -19,43 +19,25 @@\n \n from typing import Optional, Union\n \n+import torch\n+\n from ...image_processing_utils import BatchFeature\n-from ...image_utils import (\n-    OPENAI_CLIP_MEAN,\n-    OPENAI_CLIP_STD,\n-    SizeDict,\n-)\n+from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling, SizeDict\n from ...processing_utils import Unpack, VideosKwargs\n-from ...utils import (\n-    TensorType,\n-    is_torch_available,\n-    is_torchvision_available,\n-    is_torchvision_v2_available,\n-    is_vision_available,\n-)\n-from ...utils.import_utils import requires\n+from ...utils import TensorType, is_torchvision_v2_available\n from ...video_processing_utils import BaseVideoProcessor\n from ...video_utils import group_videos_by_shape, reorder_videos\n \n \n-if is_vision_available():\n-    from ...image_utils import PILImageResampling\n-\n-if is_torchvision_available():\n-    if is_torchvision_v2_available():\n-        from torchvision.transforms.v2 import functional as F\n-    else:\n-        from torchvision.transforms import functional as F\n-\n-\n-if is_torch_available():\n-    import torch\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+else:\n+    from torchvision.transforms import functional as F\n \n \n class InstructBlipVideoVideoProcessorInitKwargs(VideosKwargs): ...\n \n \n-@requires(backends=(\"torchvision\",))\n class InstructBlipVideoVideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.BICUBIC\n     image_mean = OPENAI_CLIP_MEAN"
        },
        {
            "sha": "3c0ee8de1befa314e15c6bb109482adda0f51e0d",
            "filename": "src/transformers/models/internvl/video_processing_internvl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 26,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py?ref=827b65c42ca711fc09dbd978c9bc531a53f4782e",
            "patch": "@@ -16,44 +16,26 @@\n \n from typing import Optional, Union\n \n+import torch\n+\n from ...image_processing_utils import BatchFeature\n-from ...image_utils import (\n-    OPENAI_CLIP_MEAN,\n-    OPENAI_CLIP_STD,\n-    SizeDict,\n-)\n+from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling, SizeDict\n from ...processing_utils import Unpack, VideosKwargs\n-from ...utils import (\n-    TensorType,\n-    is_torch_available,\n-    is_torchvision_available,\n-    is_torchvision_v2_available,\n-    is_vision_available,\n-)\n-from ...utils.import_utils import requires\n+from ...utils import TensorType, is_torchvision_v2_available\n from ...video_processing_utils import BaseVideoProcessor\n from ...video_utils import VideoMetadata, group_videos_by_shape, reorder_videos\n \n \n-if is_torchvision_available():\n-    if is_torchvision_v2_available():\n-        from torchvision.transforms.v2 import functional as F\n-    else:\n-        from torchvision.transforms import functional as F\n-\n-\n-if is_torch_available():\n-    import torch\n-\n-if is_vision_available():\n-    from ...image_utils import PILImageResampling\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+else:\n+    from torchvision.transforms import functional as F\n \n \n class InternVLVideoProcessorInitKwargs(VideosKwargs):\n     initial_shift: Union[bool, float, int]\n \n \n-@requires(backends=(\"torchvision\",))\n class InternVLVideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.BICUBIC\n     image_mean = OPENAI_CLIP_MEAN"
        },
        {
            "sha": "80ed1e5b81b9c71d045c3d5918050062ddc84e51",
            "filename": "src/transformers/models/llava_next_video/video_processing_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 14,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fvideo_processing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fvideo_processing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fvideo_processing_llava_next_video.py?ref=827b65c42ca711fc09dbd978c9bc531a53f4782e",
            "patch": "@@ -14,26 +14,14 @@\n # limitations under the License.\n \"\"\"Video processor class for LLaVa-NeXT-Video.\"\"\"\n \n-from ...image_utils import (\n-    OPENAI_CLIP_MEAN,\n-    OPENAI_CLIP_STD,\n-)\n+from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling\n from ...processing_utils import Unpack, VideosKwargs\n-from ...utils import is_vision_available\n-from ...utils.import_utils import requires\n-from ...video_processing_utils import (\n-    BaseVideoProcessor,\n-)\n-\n-\n-if is_vision_available():\n-    from ...image_utils import PILImageResampling\n+from ...video_processing_utils import BaseVideoProcessor\n \n \n class LlavaNextVideoFastVideoProcessorInitKwargs(VideosKwargs): ...\n \n \n-@requires(backends=(\"torchvision\",))\n class LlavaNextVideoVideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.BICUBIC\n     image_mean = OPENAI_CLIP_MEAN"
        },
        {
            "sha": "ddae0fcd3b6f54dd5ca772c616f05042c25ac60f",
            "filename": "src/transformers/models/llava_onevision/video_processing_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 14,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py?ref=827b65c42ca711fc09dbd978c9bc531a53f4782e",
            "patch": "@@ -14,26 +14,14 @@\n # limitations under the License.\n \"\"\"Video processor class for LLaVa-Onevision.\"\"\"\n \n-from ...image_utils import (\n-    OPENAI_CLIP_MEAN,\n-    OPENAI_CLIP_STD,\n-)\n+from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling\n from ...processing_utils import Unpack, VideosKwargs\n-from ...utils import is_vision_available\n-from ...utils.import_utils import requires\n-from ...video_processing_utils import (\n-    BaseVideoProcessor,\n-)\n-\n-\n-if is_vision_available():\n-    from ...image_utils import PILImageResampling\n+from ...video_processing_utils import BaseVideoProcessor\n \n \n class LlavaOnevisionFastVideoProcessorInitKwargs(VideosKwargs): ...\n \n \n-@requires(backends=(\"torchvision\",))\n class LlavaOnevisionVideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.BICUBIC\n     image_mean = OPENAI_CLIP_MEAN"
        },
        {
            "sha": "1023aa7c589dab5b249c6dff3d61e4d6e523eda8",
            "filename": "src/transformers/models/perception_lm/video_processing_perception_lm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 14,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fvideo_processing_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fvideo_processing_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fvideo_processing_perception_lm.py?ref=827b65c42ca711fc09dbd978c9bc531a53f4782e",
            "patch": "@@ -13,26 +13,14 @@\n # limitations under the License.\n \"\"\"Video processor class for PerceptionLM.\"\"\"\n \n-from ...image_utils import (\n-    IMAGENET_STANDARD_MEAN,\n-    IMAGENET_STANDARD_STD,\n-)\n+from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, PILImageResampling\n from ...processing_utils import Unpack, VideosKwargs\n-from ...utils import is_vision_available\n-from ...utils.import_utils import requires\n-from ...video_processing_utils import (\n-    BaseVideoProcessor,\n-)\n-\n-\n-if is_vision_available():\n-    from ...image_utils import PILImageResampling\n+from ...video_processing_utils import BaseVideoProcessor\n \n \n class PerceptionLMFastVideoProcessorInitKwargs(VideosKwargs): ...\n \n \n-@requires(backends=(\"torchvision\",))\n class PerceptionLMVideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.BICUBIC\n     image_mean = IMAGENET_STANDARD_MEAN"
        },
        {
            "sha": "ba87909740a82884698468f75d12800777a92efe",
            "filename": "src/transformers/models/qwen2_vl/video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 11,
            "deletions": 30,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py?ref=827b65c42ca711fc09dbd978c9bc531a53f4782e",
            "patch": "@@ -22,46 +22,28 @@\n import math\n from typing import Optional, Union\n \n-from ...image_processing_utils import (\n-    BatchFeature,\n-)\n+import torch\n+\n+from ...image_processing_utils import BatchFeature\n from ...image_utils import (\n     OPENAI_CLIP_MEAN,\n     OPENAI_CLIP_STD,\n     ChannelDimension,\n+    PILImageResampling,\n     SizeDict,\n     get_image_size,\n )\n from ...processing_utils import Unpack, VideosKwargs\n-from ...utils import (\n-    TensorType,\n-    add_start_docstrings,\n-    is_torch_available,\n-    is_torchvision_available,\n-    is_torchvision_v2_available,\n-    is_vision_available,\n-)\n-from ...utils.import_utils import requires\n-from ...video_processing_utils import (\n-    BASE_VIDEO_PROCESSOR_DOCSTRING,\n-    BaseVideoProcessor,\n-)\n+from ...utils import TensorType, add_start_docstrings, is_torchvision_v2_available\n+from ...video_processing_utils import BASE_VIDEO_PROCESSOR_DOCSTRING, BaseVideoProcessor\n from ...video_utils import VideoMetadata, group_videos_by_shape, reorder_videos\n+from .image_processing_qwen2_vl import smart_resize\n \n \n-if is_vision_available():\n-    from ...image_utils import PILImageResampling\n-    from .image_processing_qwen2_vl import smart_resize\n-\n-if is_torchvision_available():\n-    if is_torchvision_v2_available():\n-        from torchvision.transforms.v2 import functional as F\n-    else:\n-        from torchvision.transforms import functional as F\n-\n-\n-if is_torch_available():\n-    import torch\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+else:\n+    from torchvision.transforms import functional as F\n \n \n class Qwen2VLVideoProcessorInitKwargs(VideosKwargs):\n@@ -94,7 +76,6 @@ class Qwen2VLVideoProcessorInitKwargs(VideosKwargs):\n             The maximum number of frames that can be sampled.\n     \"\"\",\n )\n-@requires(backends=(\"torchvision\",))\n class Qwen2VLVideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.BICUBIC\n     size = {\"shortest_edge\": 128 * 28 * 28, \"longest_edge\": 28 * 28 * 768}"
        },
        {
            "sha": "b0280828cb66352495d44a07461f820ee627cbce",
            "filename": "src/transformers/models/sam2_video/video_processing_sam2_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 21,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fvideo_processing_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fvideo_processing_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fvideo_processing_sam2_video.py?ref=827b65c42ca711fc09dbd978c9bc531a53f4782e",
            "patch": "@@ -17,32 +17,15 @@\n from typing import Optional, Union\n \n import numpy as np\n+import torch\n+from torch.nn import functional as F_t\n \n from ...image_processing_utils import BatchFeature\n-from ...image_utils import (\n-    IMAGENET_DEFAULT_MEAN,\n-    IMAGENET_DEFAULT_STD,\n-    SizeDict,\n-)\n-from ...utils import (\n-    TensorType,\n-    is_torch_available,\n-    is_vision_available,\n-)\n-from ...utils.import_utils import requires\n+from ...image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling, SizeDict\n+from ...utils import TensorType\n from ...video_processing_utils import BaseVideoProcessor\n \n \n-if is_torch_available():\n-    import torch\n-    from torch.nn import functional as F_t\n-\n-\n-if is_vision_available():\n-    from ...image_utils import PILImageResampling\n-\n-\n-@requires(backends=(\"torchvision\",))\n class Sam2VideoVideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.BILINEAR\n     image_mean = IMAGENET_DEFAULT_MEAN"
        },
        {
            "sha": "44d7ab9cef3750d74acf7e5917b1f5ab47f89f01",
            "filename": "src/transformers/models/smolvlm/video_processing_smolvlm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 33,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py?ref=827b65c42ca711fc09dbd978c9bc531a53f4782e",
            "patch": "@@ -16,43 +16,20 @@\n from typing import Optional, Union\n \n import numpy as np\n+import torch\n \n-from ...image_processing_utils import (\n-    BatchFeature,\n-    get_size_dict,\n-)\n-from ...image_utils import (\n-    IMAGENET_STANDARD_MEAN,\n-    IMAGENET_STANDARD_STD,\n-    SizeDict,\n-)\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, PILImageResampling, SizeDict\n from ...processing_utils import Unpack, VideosKwargs\n-from ...utils import (\n-    TensorType,\n-    is_torch_available,\n-    is_torchvision_available,\n-    is_torchvision_v2_available,\n-    is_vision_available,\n-)\n-from ...utils.import_utils import requires\n-from ...video_processing_utils import (\n-    BaseVideoProcessor,\n-)\n+from ...utils import TensorType, is_torchvision_v2_available\n+from ...video_processing_utils import BaseVideoProcessor\n from ...video_utils import VideoMetadata, group_videos_by_shape, reorder_videos\n \n \n-if is_vision_available():\n-    from ...image_utils import PILImageResampling\n-\n-if is_torchvision_available():\n-    if is_torchvision_v2_available():\n-        from torchvision.transforms.v2 import functional as F\n-    else:\n-        from torchvision.transforms import functional as F\n-\n-\n-if is_torch_available():\n-    import torch\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+else:\n+    from torchvision.transforms import functional as F\n \n from ...utils import logging\n \n@@ -124,7 +101,6 @@ class SmolVLMVideoProcessorInitKwargs(VideosKwargs):\n     max_image_size: dict[str, int] = None\n \n \n-@requires(backends=(\"torchvision\",))\n class SmolVLMVideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.LANCZOS\n     size = {\"longest_edge\": 4 * 364}"
        },
        {
            "sha": "1e5deb54365411827ed2c89ae8c5a15d56159e10",
            "filename": "src/transformers/models/video_llava/video_processing_video_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 14,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fvideo_processing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fvideo_processing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fvideo_processing_video_llava.py?ref=827b65c42ca711fc09dbd978c9bc531a53f4782e",
            "patch": "@@ -14,26 +14,14 @@\n # limitations under the License.\n \"\"\"Video processor class for Video-LLaVA.\"\"\"\n \n-from ...image_utils import (\n-    OPENAI_CLIP_MEAN,\n-    OPENAI_CLIP_STD,\n-)\n+from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling\n from ...processing_utils import Unpack, VideosKwargs\n-from ...utils import is_vision_available\n-from ...utils.import_utils import requires\n-from ...video_processing_utils import (\n-    BaseVideoProcessor,\n-)\n-\n-\n-if is_vision_available():\n-    from ...image_utils import PILImageResampling\n+from ...video_processing_utils import BaseVideoProcessor\n \n \n class VideoLlavaFastVideoProcessorInitKwargs(VideosKwargs): ...\n \n \n-@requires(backends=(\"torchvision\",))\n class VideoLlavaVideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.BICUBIC\n     image_mean = OPENAI_CLIP_MEAN"
        },
        {
            "sha": "3a5f5509ba6b363934e522903b650cdf8b00e1d4",
            "filename": "src/transformers/models/vjepa2/video_processing_vjepa2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fvideo_processing_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fvideo_processing_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fvideo_processing_vjepa2.py?ref=827b65c42ca711fc09dbd978c9bc531a53f4782e",
            "patch": "@@ -14,24 +14,14 @@\n # limitations under the License.\n \"\"\"Fast Video processor class for VJEPA2.\"\"\"\n \n-from ...image_utils import (\n-    IMAGENET_DEFAULT_MEAN,\n-    IMAGENET_DEFAULT_STD,\n-)\n+from ...image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling\n from ...processing_utils import Unpack, VideosKwargs\n-from ...utils import is_vision_available\n-from ...utils.import_utils import requires\n from ...video_processing_utils import BaseVideoProcessor\n \n \n-if is_vision_available():\n-    from ...image_utils import PILImageResampling\n-\n-\n class VJEPA2VideoProcessorInitKwargs(VideosKwargs): ...\n \n \n-@requires(backends=(\"torchvision\",))\n class VJEPA2VideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.BILINEAR\n     image_mean = IMAGENET_DEFAULT_MEAN"
        },
        {
            "sha": "2f6dc0b8e71407a5c610cf8cafff063317d13d20",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/827b65c42ca711fc09dbd978c9bc531a53f4782e/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=827b65c42ca711fc09dbd978c9bc531a53f4782e",
            "patch": "@@ -2470,6 +2470,7 @@ def inner_fn(fun):\n     lambda e: e.startswith(\"tokenization_\") and e.endswith(\"_fast\"): (\"tokenizers\",),\n     lambda e: e.startswith(\"image_processing_\") and e.endswith(\"_fast\"): (\"vision\", \"torch\", \"torchvision\"),\n     lambda e: e.startswith(\"image_processing_\"): (\"vision\",),\n+    lambda e: e.startswith(\"video_processing_\"): (\"vision\", \"torch\", \"torchvision\"),\n }\n \n "
        }
    ],
    "stats": {
        "total": 285,
        "additions": 57,
        "deletions": 228
    }
}