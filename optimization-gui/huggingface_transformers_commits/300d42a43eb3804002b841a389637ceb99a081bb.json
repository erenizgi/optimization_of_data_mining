{
    "author": "ArthurZucker",
    "message": "Add ep (#39501)\n\n* EP + updates\n\nCo-authored-by: Nouamane Tazi <NouamaneTazi@users.noreply.github.com>\nCo-authored-by: drbh <drbh@users.noreply.github.com>\n\n* remove unrelated change\n\n* not working yet but let's see where it goes!\n\n* update the api a bit\n\n* udpate\n\n* where I am at for now\n\n* fix ep\n\n* refactor the API\n\n* yups\n\n* fix\n\n* fixup\n\n* clean modeling\n\n* just support llama4 for now!\n\n* properly avoid\n\n* fix\n\n* nits\n\n* Update src/transformers/models/llama4/modeling_llama4.py\n\n* Update src/transformers/integrations/tensor_parallel.py\n\n* style\n\n* ,,,,\n\n* update\n\n---------\n\nCo-authored-by: Nouamane Tazi <NouamaneTazi@users.noreply.github.com>\nCo-authored-by: drbh <drbh@users.noreply.github.com>",
    "sha": "300d42a43eb3804002b841a389637ceb99a081bb",
    "files": [
        {
            "sha": "ba6db8358d2b126293c061b9ca99368a247c6104",
            "filename": "src/transformers/distributed/__init__.py",
            "status": "added",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/300d42a43eb3804002b841a389637ceb99a081bb/src%2Ftransformers%2Fdistributed%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/300d42a43eb3804002b841a389637ceb99a081bb/src%2Ftransformers%2Fdistributed%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdistributed%2F__init__.py?ref=300d42a43eb3804002b841a389637ceb99a081bb",
            "patch": "@@ -0,0 +1,33 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import TYPE_CHECKING\n+\n+from ..utils import _LazyModule\n+\n+\n+_import_structure = {\n+    \"configuration_utils\": [\"DistributedConfig\"],\n+}\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_utils import (\n+        DistributedConfig,\n+    )\n+\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)"
        },
        {
            "sha": "4b98c175e1b1acca1994e3077e7434b9d892ed9d",
            "filename": "src/transformers/distributed/configuration_utils.py",
            "status": "added",
            "additions": 111,
            "deletions": 0,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/300d42a43eb3804002b841a389637ceb99a081bb/src%2Ftransformers%2Fdistributed%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/300d42a43eb3804002b841a389637ceb99a081bb/src%2Ftransformers%2Fdistributed%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdistributed%2Fconfiguration_utils.py?ref=300d42a43eb3804002b841a389637ceb99a081bb",
            "patch": "@@ -0,0 +1,111 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import copy\n+import json\n+import os\n+from dataclasses import dataclass\n+from typing import Any, Union\n+\n+\n+@dataclass\n+class DistributedConfig:\n+    \"\"\"\n+    Base class for distributed configs\n+    \"\"\"\n+\n+    enable_expert_parallel: bool = False\n+    # TODO: add tp_plan, pp_plan, device_mesh etc..\n+\n+    @classmethod\n+    def from_dict(cls, config_dict, **kwargs):\n+        \"\"\"\n+        Constructs a DistributedConfig instance from a dictionary of parameters.\n+        Args:\n+            config_dict (Dict[str, Any]): Dictionary containing configuration parameters.\n+            **kwargs: Additional keyword arguments to override dictionary values.\n+        Returns:\n+            DistributedConfig: Instance of DistributedConfig constructed from the dictionary.\n+        \"\"\"\n+        config = cls(**config_dict)\n+        to_remove = []\n+        for key, value in kwargs.items():\n+            if hasattr(config, key):\n+                setattr(config, key, value)\n+                to_remove.append(key)\n+        for key in to_remove:\n+            kwargs.pop(key, None)\n+        return config\n+\n+    # Copied from transformers.utils.quantization_config.QuantizationConfigMixin.to_json_file\n+    def to_json_file(self, json_file_path: Union[str, os.PathLike]):\n+        \"\"\"\n+        Save this instance to a JSON file.\n+        Args:\n+            json_file_path (`str` or `os.PathLike`):\n+                Path to the JSON file in which this configuration instance's parameters will be saved.\n+            use_diff (`bool`, *optional*, defaults to `True`):\n+                If set to `True`, only the difference between the config instance and the default\n+                `QuantizationConfig()` is serialized to JSON file.\n+        \"\"\"\n+        with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n+            config_dict = self.to_dict()\n+            json_string = json.dumps(config_dict, indent=2, sort_keys=True) + \"\\n\"\n+\n+            writer.write(json_string)\n+\n+    def to_dict(self) -> dict[str, Any]:\n+        \"\"\"\n+        Serializes this instance to a Python dictionary. Returns:\n+            `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n+        \"\"\"\n+        return copy.deepcopy(self.__dict__)\n+\n+    # Copied from transformers.utils.quantization_config.QuantizationConfigMixin.__iter__\n+    def __iter__(self):\n+        \"\"\"allows `dict(obj)` for situations where obj may be a dict or QuantizationConfigMixin\"\"\"\n+        for attr, value in copy.deepcopy(self.__dict__).items():\n+            yield attr, value\n+\n+    # Copied from transformers.utils.quantization_config.QuantizationConfigMixin.__repr__\n+    def __repr__(self):\n+        return f\"{self.__class__.__name__} {self.to_json_string()}\"\n+\n+    def to_json_string(self):\n+        \"\"\"\n+        Serializes this instance to a JSON formatted string.\n+        Returns:\n+            str: JSON formatted string representing the configuration instance.\n+        \"\"\"\n+        return json.dumps(self.__dict__, indent=2) + \"\\n\"\n+\n+    def update(self, **kwargs):\n+        \"\"\"\n+        Updates attributes of this class instance with attributes from `kwargs` if they match existing attributes,\n+        returning all the unused kwargs.\n+        Args:\n+            kwargs (`Dict[str, Any]`):\n+                Dictionary of attributes to tentatively update this class.\n+        Returns:\n+            `Dict[str, Any]`: Dictionary containing all the key-value pairs that were not used to update the instance.\n+        \"\"\"\n+        to_remove = []\n+        for key, value in kwargs.items():\n+            if hasattr(self, key):\n+                setattr(self, key, value)\n+                to_remove.append(key)\n+\n+        # Remove all the attributes that were updated, without modifying the input dict\n+        unused_kwargs = {key: value for key, value in kwargs.items() if key not in to_remove}\n+        return unused_kwargs"
        },
        {
            "sha": "e824a5ab1f0e37e0f4f8cfff8c313703b59bf038",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/300d42a43eb3804002b841a389637ceb99a081bb/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/300d42a43eb3804002b841a389637ceb99a081bb/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=300d42a43eb3804002b841a389637ceb99a081bb",
            "patch": "@@ -52,6 +52,12 @@\n                 layer_name=\"TritonLlamaMLP\",\n             )\n         },\n+        \"MegaBlocksMoeMLP\": {\n+            \"cuda\": LayerRepository(\n+                repo_id=\"kernels-community/megablocks\",\n+                layer_name=\"MegaBlocksMoeMLP\",\n+            )\n+        },\n     }\n \n     register_kernel_mapping(_KERNEL_MAPPING)"
        },
        {
            "sha": "33fe4bbf6841c890d606844be79e284e49776f8b",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 216,
            "deletions": 65,
            "changes": 281,
            "blob_url": "https://github.com/huggingface/transformers/blob/300d42a43eb3804002b841a389637ceb99a081bb/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/300d42a43eb3804002b841a389637ceb99a081bb/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=300d42a43eb3804002b841a389637ceb99a081bb",
            "patch": "@@ -23,6 +23,7 @@\n import torch.distributed as dist\n from torch import nn\n \n+from ..distributed import DistributedConfig\n from ..utils import is_torch_greater_or_equal, logging\n from ..utils.generic import GeneralInterface\n \n@@ -90,7 +91,7 @@ def initialize_tensor_parallelism(tp_plan, tp_size=None):\n     device_map = tp_device\n     tp_size = tp_size if tp_size is not None else torch.distributed.get_world_size()\n     device_mesh = torch.distributed.init_device_mesh(tp_device.type, (tp_size,))\n-    return tp_device, device_map, device_mesh\n+    return tp_device, device_map, device_mesh, tp_size\n \n \n def _blocks_to_block_sizes(total_size: int, blocks: int | list[int]) -> list[int]:\n@@ -119,20 +120,22 @@ def _blocks_to_block_sizes(total_size: int, blocks: int | list[int]) -> list[int\n         return [single_size] * blocks\n \n \n-def _get_parameter_tp_plan(parameter_name: str, tp_plan: dict[str, str]) -> str | None:\n+def _get_parameter_tp_plan(parameter_name: str, tp_plan: dict[str, str], is_weight=True) -> str | None:\n     \"\"\"\n     Get the TP style for a parameter from the TP plan.\n \n     The TP plan is a dictionary that maps parameter names to TP styles.\n     The parameter name can be a generic name with wildcards (e.g. \"*.weight\") or a specific name (e.g. \"layer_1.weight\").\n+\n+    The `is_weight` is important because for weights, we want to support `.weights` and `.bias` cases seamlessly! but\n+    not parrent classes for `post_init` calls\n     \"\"\"\n     generic_param_name = re.sub(r\"\\d+\", \"*\", parameter_name)\n     if generic_param_name in tp_plan:\n         return tp_plan[generic_param_name]\n-    elif \".\" in generic_param_name and generic_param_name.rsplit(\".\", 1)[0] in tp_plan:\n+    elif \".\" in generic_param_name and generic_param_name.rsplit(\".\", 1)[0] in tp_plan and is_weight:\n         return tp_plan[generic_param_name.rsplit(\".\", 1)[0]]\n-    else:\n-        return None\n+    return None\n \n \n str_to_torch_dtype = {\n@@ -198,8 +201,10 @@ def get_packed_weights(param, empty_param, device_mesh, rank, dim):\n     slice_dtype = slice_.get_dtype()\n     # Handle F8_E4M3 dtype by converting to float16 before slicing\n     # Without upcasting, the slicing causes : RuntimeError: \"index_cpu\" not implemented for 'Float8_e4m3fn'\n-    if slice_dtype == \"F8_E4M3\":\n+    casted = False\n+    if slice_dtype == \"F8_E4M3\" or slice_dtype == \"F8_E5M2\":\n         slice_ = slice_[...].to(torch.float16)\n+        casted = True\n \n     if dim == 0:\n         tensor = slice_[tensors_slices, ...]\n@@ -209,7 +214,11 @@ def get_packed_weights(param, empty_param, device_mesh, rank, dim):\n         tensor = slice_[..., tensors_slices]\n     else:\n         raise ValueError(f\"Unsupported dim {dim}, only dim 0, 1 or 2 are supported\")\n-    return tensor.to(str_to_torch_dtype[slice_dtype])\n+\n+    if casted:\n+        return tensor\n+    else:\n+        return tensor.to(str_to_torch_dtype[slice_dtype])\n \n \n def repack_weights(\n@@ -423,16 +432,27 @@ def __init__(\n \n     @staticmethod\n     def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n+        mod.expert_parallel_group = device_mesh.get_group()\n         if inputs and isinstance(inputs[0], DTensor):\n             inputs = inputs[0].to_local()\n         return inputs\n \n     @staticmethod\n     def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n-        # this op cannot be async, otherwise it completely breaks the outputs of models\n-        torch.distributed.all_reduce(outputs[0], op=torch.distributed.ReduceOp.SUM, async_op=False)\n+        if isinstance(outputs, torch.Tensor):\n+            dist.all_reduce(outputs, op=dist.ReduceOp.SUM, async_op=False)\n+        else:\n+            dist.all_reduce(outputs[0], op=dist.ReduceOp.SUM, async_op=False)\n         return outputs\n \n+    def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n+        distribute_module(\n+            module,\n+            device_mesh,\n+            partial(self._prepare_input_fn, None, None),\n+            partial(self._prepare_output_fn, None, None),\n+        )\n+\n \n class IsolatedParallel(TensorParallelLayer):\n     \"\"\"\n@@ -453,6 +473,14 @@ def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_me\n         # TODO: figure out dynamo support for instance method and switch this to instance method\n         return outputs\n \n+    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+        param = param[...].to(param_casting_dtype)\n+        if to_contiguous:\n+            param = param.contiguous()\n+        param = param / device_mesh.size()  # TODO should be optionable\n+        # TODO: assumes parent module will allreduce the output afterwards (e.g rowlinear bias is IsolatedParallel and parent module is GatherParallel)\n+        return param\n+\n     def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n         distribute_module(\n             module,\n@@ -773,6 +801,108 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n         return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n \n+class GroupedGemmParallel(TensorParallelLayer):\n+    \"\"\"\n+    Applies Expert Parallelism to MoE experts by loading the correct experts on each device.\n+    \"\"\"\n+\n+    def __init__(self):\n+        super().__init__()\n+        self.use_dtensor = False\n+\n+    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+        ep_rank = rank\n+        global_num_experts = empty_param.shape[0]\n+        if global_num_experts % device_mesh.size() != 0:\n+            raise ValueError(\n+                f\"Global number of experts must be divisible by number of devices: {global_num_experts} % {device_mesh.size()} != 0\"\n+            )\n+        local_num_experts = global_num_experts // device_mesh.size()\n+        param = param[ep_rank * local_num_experts : (ep_rank + 1) * local_num_experts].to(param_casting_dtype)\n+        if to_contiguous:\n+            param = param.contiguous()\n+        if \"gate_up\" in param_type and False:\n+            param = torch.cat([param[..., ::2], param[..., 1::2]], dim=-1)\n+        return param\n+\n+\n+class RouterParallel(TensorParallelLayer):\n+    \"\"\"\n+    Allows to reshape the router scores to support running expert parallel.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        self.args = args\n+        self.kwargs = kwargs\n+        self.use_dtensor = False\n+\n+    @staticmethod\n+    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n+        input_tensor = inputs[0]\n+        if isinstance(input_tensor, DTensor):\n+            raise NotImplementedError(\"RouterParallel does not support DTensor input for now\")\n+        return input_tensor\n+\n+    @staticmethod\n+    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n+        \"\"\"\n+        Imagine if you had 4 tokens, top_k = 4, and 128experts.\n+        With EP = 8.\n+        Imagine router_indices being:\n+        [ 52,  42, 119,  67],\n+        [102,  89,  61,  40],\n+        [ 82, 103,   4,  34],\n+        [ 93,  23, 109,  11],\n+\n+        then you can map which rank should be getting which values\n+\n+        [3, 2, 7, 4],\n+        [6, 5, 3, 2],\n+        [5, 6, 0, 2],\n+        [5, 1, 6, 0],\n+\n+        Thus for say rank 0, you fill with 0 the index tensor\n+\n+        [ 0, 0, 0, 0],\n+        [ 0, 0, 0, 0],\n+        [ 0, 0, 4, 0],\n+        [ 0, 0, 0, 11],\n+\n+        This works well. For another rank you need to make sure you round to num_local_expert\n+        because the next operation will one hot encode the router index vector.\n+\n+        This allows us to know directly which local expert is hit.\n+        Similarly the scores are indexed with something created form\n+        router_indices.\n+\n+        The kinda naive training loop that we use for device_map \"auto\" uses a similar logic.\n+        Here we are just making each rank believe that he is alone, and he computes his part of the hiddenstates.\n+        \"\"\"\n+        ep_rank, ep_size = device_mesh.get_local_rank(), device_mesh.size()\n+        num_local_experts = mod.num_experts // ep_size\n+        router_scores, router_indices = outputs\n+        router_scores = router_scores[:, ep_rank * num_local_experts : (ep_rank + 1) * num_local_experts]\n+        router_indices = router_indices.masked_fill((router_indices // num_local_experts) != ep_rank, 0)\n+        router_indices = router_indices % num_local_experts\n+        return router_scores, router_indices\n+\n+    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+        # TODO: i'd like for this to be the default\n+        param = param[...].to(param_casting_dtype)\n+        if to_contiguous:\n+            param = param.contiguous()\n+        return param\n+\n+    def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n+        # TODO: need an abstract Parallel class that is different from TensorParallelLayer\n+        distribute_module(\n+            module,\n+            device_mesh,\n+            partial(self._prepare_input_fn, None, None),\n+            partial(self._prepare_output_fn, None, None),\n+        )\n+\n+\n class ParallelInterface(GeneralInterface):\n     # Class instance object, so that a call to `register` can be reflected into all other files correctly, even if\n     # a new instance is created (in order to locally override a given entry)\n@@ -789,6 +919,8 @@ class ParallelInterface(GeneralInterface):\n             \"local_packed_rowwise\": PackedRowwiseParallel(use_dtensor=False),\n             \"sequence_parallel\": SequenceParallel(),\n             \"replicate\": ReplicateParallel(),\n+            \"grouped_gemm\": GroupedGemmParallel(),\n+            \"ep_router\": RouterParallel(),\n         }\n         if is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available\n         else {}\n@@ -841,25 +973,17 @@ def replace_state_dict_local_with_dtensor(\n     return state_dict\n \n \n-def add_tensor_parallel_hooks_to_module(model, module, tp_plan, layer_name, current_module_plan, device_mesh):\n-    \"\"\"\n-    Add hooks to the module holding the layer. Meaning:\n-    ```\n-    class MyModel(nn.Module):\n-        def __init__(self):\n-            self.layer = nn.Linear(10, 10)\n-    ```\n-    has state_dict like:\n-    ```\n-    {\n-        \"layer.weight\": torch.Tensor,\n-        \"layer.bias\": torch.Tensor\n-    }\n-    ```\n-    we add hooks to `MyModel` as well as `layer` to make sure that the tensors are correctly sharded and gathered.\n-    \"\"\"\n+def add_tensor_parallel_hooks_to_module(\n+    model, module, tp_plan, layer_name, current_module_plan, device_mesh, parameter_name=None\n+):\n+    r\"\"\"\n+    This function is called in `PretrainedModel.post_init()`. It is responsible of adding hooks\n+    to the modules of the `model`, based on the `PretrainedModel._tp_plan`.\n+\n+    This is the place where we add the `pre_forward` and `post_forwards` hooks. These are defined\n+    for each `TensorParallelLayer` as `_prepare_input_fn` and `_prepare_output_fn`.\n \n-    # 1. We add hooks to the layer being loaded:\n+    \"\"\"\n     if current_module_plan is not None:\n         tp_layer = ALL_PARALLEL_STYLES[current_module_plan]\n         try:\n@@ -868,26 +992,19 @@ def __init__(self):\n             print(\n                 f\"Trying to prepare {layer_name}, but it's not supported. Corresponding module: {module} Fix it's TP plan: {e}\"\n             )\n+\n         module._hf_tp_plan = current_module_plan\n         module.__repr__ = lambda: f\"{module.__repr__()}\\nTP Plan: {current_module_plan}\"\n \n-    # 2. We add hooks to the parent module if needed\n-    if \".\" in layer_name:\n-        parent_layer_name = layer_name.rsplit(\".\", 1)[0]\n-        generic_name = re.sub(r\"\\d+\", \"*\", parent_layer_name)\n-        # The module itself needs hooks\n-        if module_plan := tp_plan.get(generic_name, False):\n-            tp_layer = ALL_PARALLEL_STYLES[module_plan]\n-            module_to_tp_ = model.get_submodule(parent_layer_name)\n-            tp_layer.prepare_module_tp(module_to_tp_, device_mesh)\n-            module_to_tp_._hf_tp_plan = current_module_plan\n-            module_to_tp_.__repr__ = lambda: f\"{module_to_tp_.__repr__()}\\nTP Plan: {current_module_plan}\"\n-\n \n def shard_and_distribute_module(\n     model, param, empty_param, parameter_name, param_casting_dtype, is_contiguous, rank, device_mesh\n-):\n+):  # TODO: rename to shard_and_distribute_param\n     r\"\"\"\n+    This function is called in `from_pretrained` when loading a model's checkpoints.\n+    It receives the pointer to the parameter (or the parameter itself) and takes care of \"sharding\".\n+    All process run this function, so they just load the partition of the tensor that they require.\n+\n     Main uses cases:\n     - column / rowise parallelism, you just shard all the weights of the layer (weight and bias)\n     - packed layers: you slice the weights, then shard like above\n@@ -898,39 +1015,33 @@ def shard_and_distribute_module(\n     \"\"\"\n     param_name, param_type = parameter_name.rsplit(\".\", 1) if \".\" in parameter_name else parameter_name\n     tp_plan = model._tp_plan\n-    module_to_tp = model.get_submodule(param_name)\n+    module_to_tp = model.get_submodule(param_name)  # TODO: can i loop over modules?\n     rank = int(rank)\n+    current_shard_plan = _get_parameter_tp_plan(parameter_name, tp_plan)\n \n-    current_module_plan = _get_parameter_tp_plan(parameter_name, tp_plan)\n+    if dist.get_rank() == 0:\n+        if current_shard_plan is None:\n+            logger.info(f\"Tensor sharding plan for {param_name} not found, using default 'replicate' plan.\")\n+        else:\n+            logger.info(f\"Tensor sharding plan for {param_name}: {current_shard_plan}\")\n \n-    if current_module_plan is None:\n-        current_module_plan = \"replicate\"\n-        if dist.get_rank() == 0:\n-            logger.info(f\"Tensor parallel plan for {param_name} not found, using default 'replicate' plan.\")\n+    if current_shard_plan is not None:\n+        try:\n+            tp_layer = ALL_PARALLEL_STYLES[current_shard_plan]\n+            param = tp_layer.partition_tensor(\n+                param, empty_param, param_type, param_casting_dtype, is_contiguous, rank, device_mesh\n+            )\n+        except NotImplementedError as e:\n+            print(\n+                f\"Trying to prepare {parameter_name}, but it's not supported. Corresponding module: {module_to_tp} Fix it's TP plan, current layer: {tp_layer} : {e}\"\n+            )\n     else:\n-        if dist.get_rank() == 0:\n-            logger.info(f\"Tensor parallel plan for {param_name}: {current_module_plan}\")\n-\n-    # Add hooks to the module if not done yet\n-    # add_tensor_parallel_hooks_to_module(model, module_to_tp, tp_plan, param_name, current_module_plan, device_mesh)\n-    if not getattr(module_to_tp, \"_is_hooked\", False):\n-        add_tensor_parallel_hooks_to_module(model, module_to_tp, tp_plan, param_name, current_module_plan, device_mesh)\n-        module_to_tp._is_hooked = True\n-\n-    try:\n-        tp_layer = ALL_PARALLEL_STYLES[current_module_plan]\n-        param = tp_layer.partition_tensor(\n-            param, empty_param, param_type, param_casting_dtype, is_contiguous, rank, device_mesh\n-        )\n-    except NotImplementedError as e:\n-        print(\n-            f\"Trying to prepare {parameter_name}, but it's not supported. Corresponding module: {module_to_tp} Fix it's TP plan, current layer: {tp_layer} : {e}\"\n-        )\n+        param = param[:].to(param_casting_dtype)\n \n     # SUPER IMPORTANT we have to use setattr\n     # otherwise loading is crazy slow\n     if not isinstance(param, torch.nn.Parameter):\n-        param = torch.nn.Parameter(param, requires_grad=param.is_floating_point())\n+        param = torch.nn.Parameter(param, requires_grad=empty_param.is_floating_point())\n     setattr(module_to_tp, param_type, param)\n     # module_to_tp.load_state_dict({param_type: param}, strict=False, assign=True)\n     return param\n@@ -965,3 +1076,43 @@ def verify_tp_plan(expected_keys: list[str], tp_plan: dict[str, str] | None):\n         logger.warning(f\"The following TP rules were not applied on any of the layers: {unused_rules}\")\n     if len(unsharded_layers) > 0:\n         logger.warning(f\"The following layers were not sharded: {', '.join(unsharded_layers)}\")\n+\n+\n+def distribute_model(model, distributed_config, device_mesh, tp_size):\n+    _plan = \"_tp_plan\"\n+    model._tp_plan = getattr(model.config, \"base_model_tp_plan\").copy()\n+    if distributed_config is not None:\n+        distributed_config = DistributedConfig.from_config(distributed_config)\n+        if distributed_config.enable_expert_parallel:\n+            _plan = \"_ep_plan\"\n+            model._tp_plan = getattr(model.config, \"base_model_ep_plan\", model._tp_plan).copy()\n+\n+    # now fetch my childrens\n+    for name, module in model.named_children():\n+        if plan := getattr(module, _plan, getattr(module, \"tp_plan\", None)):\n+            model._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n+        if hasattr(module, \"config\"):\n+            plan = getattr(module.config, f\"base_model{_plan}\", {})\n+            if plan == {}:\n+                plan = getattr(module.config, \"base_model_tp_plan\", {})\n+            model._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n+\n+    if model._tp_plan is not None and is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n+        for v in model._tp_plan.values():\n+            if v not in ALL_PARALLEL_STYLES:\n+                raise ValueError(f\"Unsupported tensor parallel style {v}. Supported styles are {ALL_PARALLEL_STYLES}\")\n+        for name, module in model.named_modules():\n+            if not getattr(module, \"_is_hooked\", False):\n+                from transformers.integrations.tensor_parallel import add_tensor_parallel_hooks_to_module\n+\n+                plan = _get_parameter_tp_plan(parameter_name=name, tp_plan=model._tp_plan, is_weight=False)\n+                add_tensor_parallel_hooks_to_module(\n+                    model=model,\n+                    module=module,\n+                    tp_plan=model._tp_plan,\n+                    layer_name=\"\",\n+                    current_module_plan=plan,\n+                    device_mesh=device_mesh,\n+                )\n+            module._is_hooked = True\n+    return model"
        },
        {
            "sha": "374b308d19d656ca0b072bd0fc558d42325fad9b",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 16,
            "deletions": 34,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/300d42a43eb3804002b841a389637ceb99a081bb/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/300d42a43eb3804002b841a389637ceb99a081bb/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=300d42a43eb3804002b841a389637ceb99a081bb",
            "patch": "@@ -63,8 +63,8 @@\n from .integrations.sdpa_attention import sdpa_attention_forward\n from .integrations.sdpa_paged import sdpa_attention_paged_forward\n from .integrations.tensor_parallel import (\n-    ALL_PARALLEL_STYLES,\n     _get_parameter_tp_plan,\n+    distribute_model,\n     initialize_tensor_parallelism,\n     repack_weights,\n     replace_state_dict_local_with_dtensor,\n@@ -2218,6 +2218,9 @@ def post_init(self):\n         \"\"\"\n         A method executed at the end of each Transformer model initialization, to execute code that needs the model's\n         modules properly initialized (such as weight initialization).\n+\n+        This is also used when the user is running distributed code. We add hooks to the modules here, according to\n+        the model's tp_plan!\n         \"\"\"\n         self.init_weights()\n         self._backward_compatibility_gradient_checkpointing()\n@@ -2250,17 +2253,6 @@ def post_init(self):\n \n         # If current model is a base model, attach `base_model_tp_plan` and `base_model_pp_plan` from config\n         self._pp_plan = self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else None\n-        self._tp_plan = self.config.base_model_tp_plan.copy() if self.config.base_model_tp_plan is not None else {}\n-        for name, module in self.named_children():\n-            if plan := getattr(module, \"_tp_plan\", None):\n-                self._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n-\n-        if self._tp_plan is not None and is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n-            for v in self._tp_plan.values():\n-                if v not in ALL_PARALLEL_STYLES:\n-                    raise ValueError(\n-                        f\"Unsupported tensor parallel style {v}. Supported styles are {ALL_PARALLEL_STYLES}\"\n-                    )\n \n     def dequantize(self):\n         \"\"\"\n@@ -4568,6 +4560,7 @@ def from_pretrained(\n         load_in_8bit = kwargs.pop(\"load_in_8bit\", False)\n         load_in_4bit = kwargs.pop(\"load_in_4bit\", False)\n         quantization_config = kwargs.pop(\"quantization_config\", None)\n+        distributed_config = kwargs.pop(\"distributed_config\", None)\n         subfolder = kwargs.pop(\"subfolder\", \"\")\n         commit_hash = kwargs.pop(\"_commit_hash\", None)\n         variant = kwargs.pop(\"variant\", None)\n@@ -4588,6 +4581,9 @@ def from_pretrained(\n         ):\n             key_mapping = cls._checkpoint_conversion_mapping\n \n+        if distributed_config is not None:\n+            tp_plan = \"auto\"\n+\n         # Not used anymore -- remove them from the kwargs\n         _ = kwargs.pop(\"resume_download\", None)\n         _ = kwargs.pop(\"mirror\", None)\n@@ -4619,16 +4615,12 @@ def from_pretrained(\n         # `device_map` pointing to the correct device\n         if tp_plan is not None:\n             if device_mesh is None:\n-                tp_plan, device_map, device_mesh = initialize_tensor_parallelism(tp_plan, tp_size=None)\n+                tp_plan, device_map, device_mesh, tp_size = initialize_tensor_parallelism(tp_plan, tp_size=tp_size)\n             else:\n-                if \"tp\" not in device_mesh.mesh_dim_names:\n-                    raise ValueError(\n-                        \"When using `tp_plan`, the `device_mesh` must contain a 'tp' dimension. \"\n-                        \"Please provide a valid `device_mesh`.\"\n-                    )\n-                device_mesh = device_mesh[\"tp\"]\n-                tp_size = device_mesh[\"tp\"].size()\n-                device_map = torch.device(f\"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}\")\n+                # TODO: make device_mesh support multiple dimensions\n+                if device_mesh.ndim > 1:\n+                    raise ValueError(\"device_mesh must be 1 dimensional and will be used for TP\")\n+                device_map = torch.device(device_mesh.device_type, int(os.environ[\"LOCAL_RANK\"]))\n \n             if tp_size is None:\n                 tp_size = torch.distributed.get_world_size()\n@@ -4928,23 +4920,18 @@ def from_pretrained(\n             )\n \n         config.name_or_path = pretrained_model_name_or_path\n-\n-        # Instantiate model.\n         model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)\n-\n         config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.\n         with ContextManagers(model_init_context):\n             # Let's make sure we don't run the init function of buffer modules\n             model = cls(config, *model_args, **model_kwargs)\n \n+        if _torch_distributed_available and device_mesh is not None:\n+            model = distribute_model(model, distributed_config, device_mesh, tp_size)\n+\n         # Make sure to tie the weights correctly\n         model.tie_weights()\n \n-        # Last check for tp\n-        if device_mesh is not None and not model.supports_tp_plan:\n-            if config.base_model_tp_plan is None and config.get_text_config().base_model_tp_plan is None:\n-                raise NotImplementedError(\"This model does not have a tensor parallel plan.\")\n-\n         # make sure we use the model's config since the __init__ call might have copied it\n         config = model.config\n \n@@ -5025,11 +5012,6 @@ def _assign_original_dtype(module):\n                 key_mapping=key_mapping,\n                 weights_only=weights_only,\n             )\n-\n-        # record tp degree the model sharded to\n-        model._tp_size = tp_size\n-        model._device_mesh = device_mesh\n-\n         # make sure token embedding weights are still tied if needed\n         model.tie_weights()\n "
        },
        {
            "sha": "76162ee25964daff96af246b17b9788419831e9b",
            "filename": "src/transformers/models/llama4/configuration_llama4.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/300d42a43eb3804002b841a389637ceb99a081bb/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/300d42a43eb3804002b841a389637ceb99a081bb/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py?ref=300d42a43eb3804002b841a389637ceb99a081bb",
            "patch": "@@ -265,6 +265,19 @@ class Llama4TextConfig(PretrainedConfig):\n         \"layers.*.feed_forward.down_proj\": \"local_rowwise\",\n         \"layers.*.feed_forward\": \"gather\",\n     }\n+    base_model_ep_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.feed_forward.experts.gate_up_proj\": \"grouped_gemm\",  # row because not linear\n+        \"layers.*.feed_forward.experts.down_proj\": \"grouped_gemm\",  # col because not linear\n+        \"layers.*.feed_forward.experts\": \"gather\",  # all reduce\n+        \"layers.*.feed_forward.gate_proj\": \"local_colwise\",\n+        \"layers.*.feed_forward.up_proj\": \"local_colwise\",\n+        \"layers.*.feed_forward.down_proj\": \"local_rowwise\",\n+        \"layers.*.feed_forward.router\": \"ep_router\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "d04d443ec851c5d6355e23c8a76a59a43fc1316d",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 32,
            "deletions": 85,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/300d42a43eb3804002b841a389637ceb99a081bb/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/300d42a43eb3804002b841a389637ceb99a081bb/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=300d42a43eb3804002b841a389637ceb99a081bb",
            "patch": "@@ -26,7 +26,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations.hub_kernels import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_chunked_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -35,6 +35,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.generic import check_model_inputs\n from .configuration_llama4 import Llama4Config, Llama4TextConfig\n \n \n@@ -65,7 +66,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         Returns:\n             torch.Tensor\n         \"\"\"\n-        hidden_states = hidden_states.view(self.num_experts, -1, self.hidden_size)\n+        hidden_states = hidden_states.view(self.gate_up_proj.shape[0], -1, self.hidden_size)\n         gate_up = torch.bmm(hidden_states, self.gate_up_proj)\n         gate, up = gate_up.chunk(2, dim=-1)  # not supported for DTensors\n         next_states = torch.bmm((up * self.act_fn(gate)), self.down_proj)\n@@ -127,6 +128,20 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n \n \n+class Llama4Router(nn.Linear):\n+    def __init__(self, config):\n+        super().__init__(config.hidden_size, config.num_local_experts, bias=False)\n+        self.num_experts = config.num_local_experts\n+        self.top_k = config.num_experts_per_tok\n+\n+    def forward(self, hidden_states):\n+        router_logits = super().forward(hidden_states)\n+        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=1)\n+        router_scores = torch.full_like(router_logits, float(\"-inf\")).scatter_(1, router_indices, router_top_value)\n+        router_scores = torch.nn.functional.sigmoid(router_scores.float()).to(router_scores.dtype)\n+        return router_scores, router_logits\n+\n+\n @use_kernel_forward_from_hub(\"Llama4TextMoe\")\n class Llama4TextMoe(nn.Module):\n     def __init__(self, config):\n@@ -135,28 +150,18 @@ def __init__(self, config):\n         self.hidden_dim = config.hidden_size\n         self.num_experts = config.num_local_experts\n         self.experts = Llama4TextExperts(config)\n-        self.router = nn.Linear(config.hidden_size, config.num_local_experts, bias=False)\n+        self.router = Llama4Router(config)\n         self.shared_expert = Llama4TextMLP(config)\n \n     def forward(self, hidden_states):\n         hidden_states = hidden_states.reshape(-1, self.hidden_dim)\n-        router_logits = self.router(hidden_states)\n-\n-        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=1)\n-\n-        router_scores = (\n-            torch.full_like(router_logits, float(\"-inf\")).scatter_(1, router_indices, router_top_value).transpose(0, 1)\n-        )\n-        router_scores = torch.sigmoid(router_scores.float()).to(hidden_states.dtype)\n-\n-        routed_in = hidden_states.repeat(self.num_experts, 1)\n+        router_scores, router_logits = self.router(hidden_states)\n+        routed_in = hidden_states.repeat(router_scores.shape[1], 1)\n         routed_in = routed_in * router_scores.reshape(-1, 1)\n         routed_out = self.experts(routed_in)\n-\n         out = self.shared_expert(hidden_states)\n-        out.add_(routed_out.reshape(self.num_experts, -1, self.hidden_dim).sum(dim=0))\n-\n-        return out, router_scores\n+        out.add_(routed_out.reshape(router_scores.shape[1], -1, routed_out.shape[-1]).sum(dim=0))\n+        return out, router_logits\n \n \n class Llama4TextRotaryEmbedding(nn.Module):\n@@ -383,8 +388,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_value: Optional[tuple[torch.Tensor]] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -395,12 +398,11 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        attention_states, self_attn_weights = self.self_attn(\n+        attention_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             past_key_value=past_key_value,\n-            output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n@@ -409,23 +411,12 @@ def forward(\n \n         # Fully Connected\n         residual = hidden_states\n-\n         hidden_states = self.post_attention_layernorm(hidden_states)\n         hidden_states = self.feed_forward(hidden_states)\n         if self.is_moe_layer:\n-            hidden_states, router_logits = hidden_states\n-        else:\n-            router_logits = None\n+            hidden_states, _ = hidden_states\n         hidden_states = residual + hidden_states.view(residual.shape)\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -472,6 +463,11 @@ class Llama4TextModel(Llama4PreTrainedModel):\n     _no_split_modules = [\"Llama4TextDecoderLayer\"]\n     base_model_prefix = \"model\"\n     config: Llama4TextConfig\n+    _can_record_outputs = {\n+        \"attentions\": Llama4TextAttention,\n+        \"hidden_states\": Llama4TextDecoderLayer,\n+        \"router_logits\": Llama4TextMoe,\n+    }\n \n     def __init__(self, config: Llama4TextConfig):\n         super().__init__(config)\n@@ -489,7 +485,7 @@ def __init__(self, config: Llama4TextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -499,28 +495,12 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids.to(self.embed_tokens.weight.device))\n \n@@ -558,42 +538,22 @@ def forward(\n         # create position embeddings to be shared across the decoder layers\n         freq_cis = self.rotary_emb(hidden_states, position_ids)\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            layer_outputs = decoder_layer(\n+            hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=position_ids,\n                 past_key_value=past_key_values,\n-                output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=freq_cis,\n                 **kwargs,\n             )\n-\n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n         hidden_states = self.norm(hidden_states)\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n         )\n \n \n@@ -630,9 +590,6 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -659,23 +616,13 @@ def forward(\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n             cache_position=cache_position,\n             **kwargs,\n         )"
        },
        {
            "sha": "43cb4b88f2cd32a8329d824d90017cec4a33a66b",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/300d42a43eb3804002b841a389637ceb99a081bb/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/300d42a43eb3804002b841a389637ceb99a081bb/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=300d42a43eb3804002b841a389637ceb99a081bb",
            "patch": "@@ -2634,7 +2634,14 @@ def _inner_training_loop(\n \n                         self.control = self.callback_handler.on_pre_optimizer_step(args, self.state, self.control)\n \n-                        self.optimizer.step()\n+                        context = contextlib.nullcontext\n+                        if self.is_tp_enabled:\n+                            from torch.distributed._tensor.experimental import implicit_replication\n+\n+                            context = implicit_replication\n+\n+                        with context():\n+                            self.optimizer.step()\n \n                         self.control = self.callback_handler.on_optimizer_step(args, self.state, self.control)\n "
        },
        {
            "sha": "1904fc8bd1e73563f6c8fa804cbfb60213032347",
            "filename": "tests/tensor_parallel/test_tensor_parallel.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/300d42a43eb3804002b841a389637ceb99a081bb/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/300d42a43eb3804002b841a389637ceb99a081bb/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py?ref=300d42a43eb3804002b841a389637ceb99a081bb",
            "patch": "@@ -109,7 +109,7 @@ def test_model_forward(self):\n \n             assert has_dtensor == 1, \"TP model must has DTensor\"\n \n-            tokenizer = AutoTokenizer.from_pretrained(model_id)\n+            tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n             prompt = \"Can I help\"\n \n             inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)"
        }
    ],
    "stats": {
        "total": 622,
        "additions": 436,
        "deletions": 186
    }
}