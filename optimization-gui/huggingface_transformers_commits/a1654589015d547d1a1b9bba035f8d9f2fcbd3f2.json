{
    "author": "jerryzh168",
    "message": "Add device workaround for int4 weight only quantization after API update (#36980)\n\n* merge\n\n* fix import\n\n* format\n\n* reformat\n\n* reformat\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "a1654589015d547d1a1b9bba035f8d9f2fcbd3f2",
    "files": [
        {
            "sha": "44d83c44e3f5c7cea45702642fe8afe1e524b470",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1654589015d547d1a1b9bba035f8d9f2fcbd3f2/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1654589015d547d1a1b9bba035f8d9f2fcbd3f2/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=a1654589015d547d1a1b9bba035f8d9f2fcbd3f2",
            "patch": "@@ -46,6 +46,12 @@\n from torch.nn import CrossEntropyLoss, Identity\n from torch.utils.checkpoint import checkpoint\n \n+from transformers.utils import is_torchao_available\n+\n+\n+if is_torchao_available():\n+    from torchao.quantization import Int4WeightOnlyConfig\n+\n from .activations import get_activation\n from .configuration_utils import PretrainedConfig\n from .dynamic_module_utils import custom_object_save\n@@ -4840,7 +4846,10 @@ def _load_pretrained_model(\n                 device_map is not None\n                 and hf_quantizer is not None\n                 and hf_quantizer.quantization_config.quant_method == QuantizationMethod.TORCHAO\n-                and hf_quantizer.quantization_config.quant_type in [\"int4_weight_only\", \"autoquant\"]\n+                and (\n+                    hf_quantizer.quantization_config.quant_type in [\"int4_weight_only\", \"autoquant\"]\n+                    or isinstance(hf_quantizer.quantization_config.quant_type, Int4WeightOnlyConfig)\n+                )\n             ):\n                 map_location = torch.device([d for d in device_map.values() if d not in [\"cpu\", \"disk\"]][0])\n "
        },
        {
            "sha": "ac8155f22767dc77d06343fe661b675cc02eb2da",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1654589015d547d1a1b9bba035f8d9f2fcbd3f2/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1654589015d547d1a1b9bba035f8d9f2fcbd3f2/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=a1654589015d547d1a1b9bba035f8d9f2fcbd3f2",
            "patch": "@@ -143,7 +143,7 @@ def adjust_target_dtype(self, torch_dtype: \"torch.dtype\") -> \"torch.dtype\":\n             from accelerate.utils import CustomDtype\n \n             # Import AOBaseConfig directly since we know we have the right version\n-            if self.quantization_config._get_ao_version() >= version.Version(\"0.10.0\"):\n+            if self.quantization_config._get_ao_version() > version.Version(\"0.9.0\"):\n                 from torchao.core.config import AOBaseConfig\n \n                 quant_type = self.quantization_config.quant_type\n@@ -236,7 +236,7 @@ def create_quantized_param(\n         else:\n             assert isinstance(self.quantization_config, TorchAoConfig)\n             module._parameters[tensor_name] = torch.nn.Parameter(param_value).to(device=target_device)\n-            quantize_(module, self.quantization_config.get_apply_tensor_subclass(), set_inductor_config=False)\n+            quantize_(module, self.quantization_config.get_apply_tensor_subclass())\n \n     def _process_model_after_weight_loading(self, model, **kwargs):\n         \"\"\"No process required for torchao quantized model\"\"\""
        },
        {
            "sha": "edf6932eb24e6f5764d4468d272c7e3c2f869123",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1654589015d547d1a1b9bba035f8d9f2fcbd3f2/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1654589015d547d1a1b9bba035f8d9f2fcbd3f2/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=a1654589015d547d1a1b9bba035f8d9f2fcbd3f2",
            "patch": "@@ -1528,7 +1528,7 @@ def post_init(self):\n         # Handle quant_type based on type and version\n         if isinstance(self.quant_type, str):\n             self._validate_string_quant_type()\n-        elif ao_version >= version.parse(\"0.10.0\"):\n+        elif ao_version > version.parse(\"0.9.0\"):\n             from torchao.quantization.quant_api import AOBaseConfig\n \n             if not isinstance(self.quant_type, AOBaseConfig):\n@@ -1537,8 +1537,8 @@ def post_init(self):\n                 )\n         else:\n             raise ValueError(\n-                f\"In torchao < 0.10.0, quant_type must be a string. Got {type(self.quant_type)}. \"\n-                f\"Please upgrade to torchao >= 0.10.0 to use AOBaseConfig instances.\"\n+                f\"In torchao <= 0.9.0, quant_type must be a string. Got {type(self.quant_type)}. \"\n+                f\"Please upgrade to torchao > 0.9.0 to use AOBaseConfig instances.\"\n             )\n \n     def _validate_string_quant_type(self):\n@@ -1624,9 +1624,7 @@ def to_dict(self):\n     def from_dict(cls, config_dict, return_unused_kwargs=False, **kwargs):\n         \"\"\"Create configuration from a dictionary.\"\"\"\n         ao_verison = cls._get_ao_version()\n-        assert ao_verison >= version.parse(\"0.10.0\"), (\n-            \"TorchAoConfig requires torchao >= 0.10.0 for construction from dict\"\n-        )\n+        assert ao_verison > version.parse(\"0.9.0\"), \"TorchAoConfig requires torchao > 0.9.0 for construction from dict\"\n         config_dict = config_dict.copy()\n         quant_type = config_dict.pop(\"quant_type\")\n         # Check if we only have one key which is \"default\""
        },
        {
            "sha": "b6c12ab738b5ef1d1fd69f33a69dab828f7c2192",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1654589015d547d1a1b9bba035f8d9f2fcbd3f2/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1654589015d547d1a1b9bba035f8d9f2fcbd3f2/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=a1654589015d547d1a1b9bba035f8d9f2fcbd3f2",
            "patch": "@@ -322,14 +322,16 @@ class TorchAoSerializationTest(unittest.TestCase):\n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n-        cls.quant_config = TorchAoConfig(cls.quant_scheme, **cls.quant_scheme_kwargs)\n-        cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n-            cls.model_name,\n+        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n+\n+    def setUp(self):\n+        self.quant_config = TorchAoConfig(self.quant_scheme, **self.quant_scheme_kwargs)\n+        self.quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name,\n             torch_dtype=torch.bfloat16,\n-            device_map=cls.device,\n-            quantization_config=cls.quant_config,\n+            device_map=self.device,\n+            quantization_config=self.quant_config,\n         )\n-        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n \n     def tearDown(self):\n         gc.collect()"
        }
    ],
    "stats": {
        "total": 39,
        "additions": 24,
        "deletions": 15
    }
}