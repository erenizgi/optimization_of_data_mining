{
    "author": "zzaebok",
    "message": "Replace tokenizer to processing_class in Seq2SeqTrainer (#35452)",
    "sha": "02ed609285c2448b3b54c31e362f2c389fa952ab",
    "files": [
        {
            "sha": "76b7c1556d8a4786b6cd800152beb69be5acefb3",
            "filename": "src/transformers/trainer_seq2seq.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/02ed609285c2448b3b54c31e362f2c389fa952ab/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/02ed609285c2448b3b54c31e362f2c389fa952ab/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_seq2seq.py?ref=02ed609285c2448b3b54c31e362f2c389fa952ab",
            "patch": "@@ -372,10 +372,12 @@ def prediction_step(\n         return loss, generated_tokens, labels\n \n     def _pad_tensors_to_max_len(self, tensor, max_length):\n-        if self.tokenizer is not None and hasattr(self.tokenizer, \"pad_token_id\"):\n+        if self.processing_class is not None and hasattr(self.processing_class, \"pad_token_id\"):\n             # If PAD token is not defined at least EOS token has to be defined\n             pad_token_id = (\n-                self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n+                self.processing_class.pad_token_id\n+                if self.processing_class.pad_token_id is not None\n+                else self.processing_class.eos_token_id\n             )\n         else:\n             if self.model.config.pad_token_id is not None:"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 4,
        "deletions": 2
    }
}