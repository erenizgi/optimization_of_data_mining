{
    "author": "xuzifei-dmatrix",
    "message": "making gpt2 fx traceable (#34633)\n\n* making gpt2 fx tracable\r\n\r\n* running make fix-copies\r\n\r\n* Revert \"running make fix-copies\"\r\n\r\nThis reverts commit 5a3437cb5b63799243bceae7d21a2aed8d0418c7.",
    "sha": "bfc3556b201ba5c07face45797992d2a7bf552f2",
    "files": [
        {
            "sha": "58143192c2048210aafbfbb12ded3f0e79e1fa6c",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfc3556b201ba5c07face45797992d2a7bf552f2/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfc3556b201ba5c07face45797992d2a7bf552f2/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=bfc3556b201ba5c07face45797992d2a7bf552f2",
            "patch": "@@ -1101,7 +1101,8 @@ def forward(\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n         all_hidden_states = () if output_hidden_states else None\n-        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n+        for i in range(len(self.h)):\n+            block, layer_past = self.h[i], past_key_values[i]\n             # Model parallel\n             if self.model_parallel:\n                 torch.cuda.set_device(hidden_states.device)"
        }
    ],
    "stats": {
        "total": 3,
        "additions": 2,
        "deletions": 1
    }
}