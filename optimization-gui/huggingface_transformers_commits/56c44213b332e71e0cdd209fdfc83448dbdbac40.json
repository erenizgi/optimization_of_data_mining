{
    "author": "materight",
    "message": "[detection] fix attention mask for RT-DETR-based models (#40269)\n\n* Fix get_contrastive_denoising_training_group attention\n\n* Add bool attention_mask conversion",
    "sha": "56c44213b332e71e0cdd209fdfc83448dbdbac40",
    "files": [
        {
            "sha": "154be65b8137b199b974c702551f2008e3bfa396",
            "filename": "examples/modular-transformers/modeling_test_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/56c44213b332e71e0cdd209fdfc83448dbdbac40/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56c44213b332e71e0cdd209fdfc83448dbdbac40/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_test_detr.py?ref=56c44213b332e71e0cdd209fdfc83448dbdbac40",
            "patch": "@@ -561,6 +561,10 @@ def forward(\n                     f\"Attention mask should be of size {(batch_size, 1, target_len, source_len)}, but is\"\n                     f\" {attention_mask.size()}\"\n                 )\n+            if attention_mask.dtype == torch.bool:\n+                attention_mask = torch.zeros_like(attention_mask, dtype=attn_weights.dtype).masked_fill_(\n+                    attention_mask, -torch.inf\n+                )\n             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask\n             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)\n "
        },
        {
            "sha": "16b8f679023b8394f8e844d9ff8c77f06c269200",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=56c44213b332e71e0cdd209fdfc83448dbdbac40",
            "patch": "@@ -537,6 +537,10 @@ def forward(\n                     f\"Attention mask should be of size {(batch_size, 1, target_len, source_len)}, but is\"\n                     f\" {attention_mask.size()}\"\n                 )\n+            if attention_mask.dtype == torch.bool:\n+                attention_mask = torch.zeros_like(attention_mask, dtype=attn_weights.dtype).masked_fill_(\n+                    attention_mask, -torch.inf\n+                )\n             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask\n             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)\n \n@@ -654,6 +658,10 @@ def forward(\n                     f\"Attention mask should be of size {(batch_size, 1, target_len, source_len)}, but is\"\n                     f\" {attention_mask.size()}\"\n                 )\n+            if attention_mask.dtype == torch.bool:\n+                attention_mask = torch.zeros_like(attention_mask, dtype=attn_weights.dtype).masked_fill_(\n+                    attention_mask, -torch.inf\n+                )\n             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask\n             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)\n "
        },
        {
            "sha": "0777f51b44b011d253a2cfefe8e3e089b63ae4e5",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=56c44213b332e71e0cdd209fdfc83448dbdbac40",
            "patch": "@@ -299,6 +299,10 @@ def forward(\n                     f\"Attention mask should be of size {(batch_size, 1, target_len, source_len)}, but is\"\n                     f\" {attention_mask.size()}\"\n                 )\n+            if attention_mask.dtype == torch.bool:\n+                attention_mask = torch.zeros_like(attention_mask, dtype=attn_weights.dtype).masked_fill_(\n+                    attention_mask, -torch.inf\n+                )\n             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask\n             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)\n \n@@ -856,16 +860,16 @@ def get_contrastive_denoising_training_group(\n     input_query_class = class_embed(input_query_class)\n \n     target_size = num_denoising_queries + num_queries\n-    attn_mask = torch.full([target_size, target_size], False, dtype=torch.bool, device=device)\n+    attn_mask = torch.full([target_size, target_size], 0, dtype=torch.float, device=device)\n     # match query cannot see the reconstruction\n-    attn_mask[num_denoising_queries:, :num_denoising_queries] = True\n+    attn_mask[num_denoising_queries:, :num_denoising_queries] = -torch.inf\n \n     # reconstructions cannot see each other\n     for i in range(num_groups_denoising_queries):\n         idx_block_start = max_gt_num * 2 * i\n         idx_block_end = max_gt_num * 2 * (i + 1)\n-        attn_mask[idx_block_start:idx_block_end, :idx_block_start] = True\n-        attn_mask[idx_block_start:idx_block_end, idx_block_end:num_denoising_queries] = True\n+        attn_mask[idx_block_start:idx_block_end, :idx_block_start] = -torch.inf\n+        attn_mask[idx_block_start:idx_block_end, idx_block_end:num_denoising_queries] = -torch.inf\n \n     denoising_meta_values = {\n         \"dn_positive_idx\": denoise_positive_idx,"
        },
        {
            "sha": "5e84bb50aefdfda38fd9a28c6a8a83ad4f3792db",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=56c44213b332e71e0cdd209fdfc83448dbdbac40",
            "patch": "@@ -674,6 +674,10 @@ def forward(\n                     f\"Attention mask should be of size {(batch_size, 1, target_len, source_len)}, but is\"\n                     f\" {attention_mask.size()}\"\n                 )\n+            if attention_mask.dtype == torch.bool:\n+                attention_mask = torch.zeros_like(attention_mask, dtype=attn_weights.dtype).masked_fill_(\n+                    attention_mask, -torch.inf\n+                )\n             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask\n             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)\n "
        },
        {
            "sha": "8110a0d633d79b971132e82231baf1809cefffa3",
            "filename": "src/transformers/models/deprecated/deta/modeling_deta.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py?ref=56c44213b332e71e0cdd209fdfc83448dbdbac40",
            "patch": "@@ -789,6 +789,10 @@ def forward(\n                     f\"Attention mask should be of size {(batch_size, 1, target_len, source_len)}, but is\"\n                     f\" {attention_mask.size()}\"\n                 )\n+            if attention_mask.dtype == torch.bool:\n+                attention_mask = torch.zeros_like(attention_mask, dtype=attn_weights.dtype).masked_fill_(\n+                    attention_mask, -torch.inf\n+                )\n             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask\n             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)\n "
        },
        {
            "sha": "58dc6a2900a9b83e2af5a9ac73fb65d87f608939",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=56c44213b332e71e0cdd209fdfc83448dbdbac40",
            "patch": "@@ -505,6 +505,10 @@ def forward(\n                     f\"Attention mask should be of size {(batch_size, 1, target_len, source_len)}, but is\"\n                     f\" {attention_mask.size()}\"\n                 )\n+            if attention_mask.dtype == torch.bool:\n+                attention_mask = torch.zeros_like(attention_mask, dtype=attn_weights.dtype).masked_fill_(\n+                    attention_mask, -torch.inf\n+                )\n             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask\n             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)\n "
        },
        {
            "sha": "9e1c0072425b516f2c11dea7965483199ad3dd0c",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=56c44213b332e71e0cdd209fdfc83448dbdbac40",
            "patch": "@@ -482,6 +482,10 @@ def forward(\n                     f\"Attention mask should be of size {(batch_size, 1, target_len, source_len)}, but is\"\n                     f\" {attention_mask.size()}\"\n                 )\n+            if attention_mask.dtype == torch.bool:\n+                attention_mask = torch.zeros_like(attention_mask, dtype=attn_weights.dtype).masked_fill_(\n+                    attention_mask, -torch.inf\n+                )\n             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask\n             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)\n "
        },
        {
            "sha": "58c84e3453a658b3035f01243363471f1a027437",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=56c44213b332e71e0cdd209fdfc83448dbdbac40",
            "patch": "@@ -456,16 +456,16 @@ def get_contrastive_denoising_training_group(\n     input_query_class = class_embed(input_query_class)\n \n     target_size = num_denoising_queries + num_queries\n-    attn_mask = torch.full([target_size, target_size], False, dtype=torch.bool, device=device)\n+    attn_mask = torch.full([target_size, target_size], 0, dtype=torch.float, device=device)\n     # match query cannot see the reconstruction\n-    attn_mask[num_denoising_queries:, :num_denoising_queries] = True\n+    attn_mask[num_denoising_queries:, :num_denoising_queries] = -torch.inf\n \n     # reconstructions cannot see each other\n     for i in range(num_groups_denoising_queries):\n         idx_block_start = max_gt_num * 2 * i\n         idx_block_end = max_gt_num * 2 * (i + 1)\n-        attn_mask[idx_block_start:idx_block_end, :idx_block_start] = True\n-        attn_mask[idx_block_start:idx_block_end, idx_block_end:num_denoising_queries] = True\n+        attn_mask[idx_block_start:idx_block_end, :idx_block_start] = -torch.inf\n+        attn_mask[idx_block_start:idx_block_end, idx_block_end:num_denoising_queries] = -torch.inf\n \n     denoising_meta_values = {\n         \"dn_positive_idx\": denoise_positive_idx,\n@@ -854,6 +854,10 @@ def forward(\n                     f\"Attention mask should be of size {(batch_size, 1, target_len, source_len)}, but is\"\n                     f\" {attention_mask.size()}\"\n                 )\n+            if attention_mask.dtype == torch.bool:\n+                attention_mask = torch.zeros_like(attention_mask, dtype=attn_weights.dtype).masked_fill_(\n+                    attention_mask, -torch.inf\n+                )\n             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask\n             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)\n "
        },
        {
            "sha": "588b3dd29473dfb515ec9261462daab0d0ea8d82",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=56c44213b332e71e0cdd209fdfc83448dbdbac40",
            "patch": "@@ -305,6 +305,10 @@ def forward(\n                     f\"Attention mask should be of size {(batch_size, 1, target_len, source_len)}, but is\"\n                     f\" {attention_mask.size()}\"\n                 )\n+            if attention_mask.dtype == torch.bool:\n+                attention_mask = torch.zeros_like(attention_mask, dtype=attn_weights.dtype).masked_fill_(\n+                    attention_mask, -torch.inf\n+                )\n             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask\n             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)\n \n@@ -1188,16 +1192,16 @@ def get_contrastive_denoising_training_group(\n     input_query_class = class_embed(input_query_class)\n \n     target_size = num_denoising_queries + num_queries\n-    attn_mask = torch.full([target_size, target_size], False, dtype=torch.bool, device=device)\n+    attn_mask = torch.full([target_size, target_size], 0, dtype=torch.float, device=device)\n     # match query cannot see the reconstruction\n-    attn_mask[num_denoising_queries:, :num_denoising_queries] = True\n+    attn_mask[num_denoising_queries:, :num_denoising_queries] = -torch.inf\n \n     # reconstructions cannot see each other\n     for i in range(num_groups_denoising_queries):\n         idx_block_start = max_gt_num * 2 * i\n         idx_block_end = max_gt_num * 2 * (i + 1)\n-        attn_mask[idx_block_start:idx_block_end, :idx_block_start] = True\n-        attn_mask[idx_block_start:idx_block_end, idx_block_end:num_denoising_queries] = True\n+        attn_mask[idx_block_start:idx_block_end, :idx_block_start] = -torch.inf\n+        attn_mask[idx_block_start:idx_block_end, idx_block_end:num_denoising_queries] = -torch.inf\n \n     denoising_meta_values = {\n         \"dn_positive_idx\": denoise_positive_idx,"
        },
        {
            "sha": "693f209284e66b79bdaafdc1f1ec37c66cd709f2",
            "filename": "src/transformers/models/table_transformer/modeling_table_transformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56c44213b332e71e0cdd209fdfc83448dbdbac40/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py?ref=56c44213b332e71e0cdd209fdfc83448dbdbac40",
            "patch": "@@ -464,6 +464,10 @@ def forward(\n                     f\"Attention mask should be of size {(batch_size, 1, target_len, source_len)}, but is\"\n                     f\" {attention_mask.size()}\"\n                 )\n+            if attention_mask.dtype == torch.bool:\n+                attention_mask = torch.zeros_like(attention_mask, dtype=attn_weights.dtype).masked_fill_(\n+                    attention_mask, -torch.inf\n+                )\n             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask\n             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)\n "
        }
    ],
    "stats": {
        "total": 68,
        "additions": 56,
        "deletions": 12
    }
}