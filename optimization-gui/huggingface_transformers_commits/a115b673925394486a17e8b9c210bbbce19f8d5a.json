{
    "author": "ArthurZucker",
    "message": "Fix tp cb (#39838)\n\n* fixes\n\n* one more",
    "sha": "a115b673925394486a17e8b9c210bbbce19f8d5a",
    "files": [
        {
            "sha": "482e28bcccb1bc616776d5934da0b76fdf29ac61",
            "filename": "src/transformers/generation/continuous_batching.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a115b673925394486a17e8b9c210bbbce19f8d5a/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a115b673925394486a17e8b9c210bbbce19f8d5a/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py?ref=a115b673925394486a17e8b9c210bbbce19f8d5a",
            "patch": "@@ -1106,7 +1106,8 @@ def __init__(\n             max_queue_size: Maximum size of the request queue (0 = unlimited)\n             streaming: Whether to stream tokens as they are generated\n         \"\"\"\n-        self.model = model\n+        self.model = model.eval()\n+        generation_config = model.generation_config if generation_config is None else generation_config\n         self.generation_config = generation_config\n         self.input_queue = queue.Queue(maxsize=max_queue_size)\n         self.output_queue = queue.Queue()\n@@ -1118,7 +1119,6 @@ def __init__(\n         self._request_lock = threading.Lock()\n         self.model.generation_config.top_p = None\n         self.do_sample = getattr(generation_config, \"do_sample\", True)\n-        generation_config = model.generation_config if generation_config is None else generation_config\n         self.logit_processor = self.model._get_logits_processor(generation_config)\n         self.use_cuda_graph = getattr(generation_config, \"use_cuda_graph\", True)\n         self.profile = getattr(generation_config, \"profile\", False)\n@@ -1242,15 +1242,15 @@ def __iter__(self):\n \n     @traced\n     def warmup(self, batch_processor):\n-        stream = torch.cuda.Stream()\n+        stream = torch.cuda.Stream(device=self.model.device)\n         stream.wait_stream(torch.cuda.current_stream())\n         with torch.cuda.stream(stream):\n             # Warmup the model with a dummy forward pass\n             self._generation_step(batch_processor)\n         torch.cuda.current_stream().wait_stream(stream)\n \n         self.graph = torch.cuda.CUDAGraph()\n-        with torch.cuda.graph(self.graph):\n+        with torch.cuda.graph(self.graph, stream=stream):\n             self._generation_step(batch_processor)\n \n     @traced\n@@ -1326,7 +1326,7 @@ def _run_generation_loop(self):\n             is_first = True\n \n             if self.profile:\n-                tracing_schedule = schedule(skip_first=2, warmup=3, active=200, repeat=100, wait=1)\n+                tracing_schedule = schedule(skip_first=2, warmup=1, active=1, repeat=3, wait=1)\n                 trace_handler = tensorboard_trace_handler(\n                     dir_name=\"/fsx/arthur/transformers\", use_gzip=True, worker_name=\"paged_compile\"\n                 )"
        },
        {
            "sha": "5c917144866c88ba6e40e8e0de52a0f4813db6b3",
            "filename": "src/transformers/integrations/flash_paged.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a115b673925394486a17e8b9c210bbbce19f8d5a/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a115b673925394486a17e8b9c210bbbce19f8d5a/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_paged.py?ref=a115b673925394486a17e8b9c210bbbce19f8d5a",
            "patch": "@@ -63,5 +63,6 @@ def paged_attention_forward(\n         # block_table=block_tables, -> torch.Tensor\n         # **kwargs,\n     )\n-\n+    if isinstance(attn_output, tuple):\n+        attn_output = attn_output[0]\n     return attn_output, None"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 7,
        "deletions": 6
    }
}