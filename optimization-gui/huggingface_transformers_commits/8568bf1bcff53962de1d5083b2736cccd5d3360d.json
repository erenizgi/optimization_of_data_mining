{
    "author": "faaany",
    "message": "[docs] make `empty_cache` device-agnostic (#34774)\n\nmake device-agnostic",
    "sha": "8568bf1bcff53962de1d5083b2736cccd5d3360d",
    "files": [
        {
            "sha": "6fcf6809204b5a9b2c87b43dcedab91966d12ab8",
            "filename": "docs/source/en/training.md",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8568bf1bcff53962de1d5083b2736cccd5d3360d/docs%2Fsource%2Fen%2Ftraining.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8568bf1bcff53962de1d5083b2736cccd5d3360d/docs%2Fsource%2Fen%2Ftraining.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftraining.md?ref=8568bf1bcff53962de1d5083b2736cccd5d3360d",
            "patch": "@@ -287,9 +287,10 @@ model.fit(tf_dataset)\n At this point, you may need to restart your notebook or execute the following code to free some memory:\n \n ```py\n+from accelerate.utils.memory import clear_device_cache\n del model\n del trainer\n-torch.cuda.empty_cache()\n+clear_device_cache()\n ```\n \n Next, manually postprocess `tokenized_dataset` to prepare it for training.\n@@ -364,8 +365,9 @@ Lastly, specify `device` to use a GPU if you have access to one. Otherwise, trai\n \n ```py\n >>> import torch\n+>>> from accelerate.test_utils.testing import get_backend\n \n->>> device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n+>>> device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n >>> model.to(device)\n ```\n "
        }
    ],
    "stats": {
        "total": 6,
        "additions": 4,
        "deletions": 2
    }
}