{
    "author": "SunMarc",
    "message": "fix awq tests due to ipex backend (#34011)\n\nfix awq tests",
    "sha": "1909def2de013deadd67b418b49d24957c53ae87",
    "files": [
        {
            "sha": "c860ea1f53744bba6a4d63bc73ee51a187fc92f8",
            "filename": "src/transformers/integrations/awq.py",
            "status": "modified",
            "additions": 12,
            "deletions": 5,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/1909def2de013deadd67b418b49d24957c53ae87/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1909def2de013deadd67b418b49d24957c53ae87/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fawq.py?ref=1909def2de013deadd67b418b49d24957c53ae87",
            "patch": "@@ -13,9 +13,13 @@\n # limitations under the License.\n \"AWQ (Activation aware Weight Quantization) integration file\"\n \n+import importlib\n+\n+from packaging import version\n+\n from ..activations import ACT2FN\n from ..modeling_utils import PreTrainedModel\n-from ..utils import is_auto_awq_available, is_torch_available, logging\n+from ..utils import is_auto_awq_available, is_ipex_available, is_torch_available, logging\n from ..utils.quantization_config import (\n     AwqBackendPackingMethod,\n     AwqConfig,\n@@ -379,7 +383,7 @@ def _fuse_awq_attention_layers(model, module, modules_to_fuse, current_module_na\n             The `QuantAttentionFused` class as it only supports that class\n             for now.\n     \"\"\"\n-    from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV, WQLinear_IPEX\n+    from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV\n \n     module_has_been_fused = False\n \n@@ -396,9 +400,12 @@ def _fuse_awq_attention_layers(model, module, modules_to_fuse, current_module_na\n         elif isinstance(q_proj, WQLinear_GEMM):\n             linear_target_cls = WQLinear_GEMM\n             cat_dim = 1\n-        elif isinstance(q_proj, WQLinear_IPEX):\n-            linear_target_cls = WQLinear_IPEX\n-            cat_dim = 1\n+        elif is_ipex_available() and version.parse(importlib.metadata.version(\"autoawq\")) > version.parse(\"0.2.6\"):\n+            from awq.modules.linear import WQLinear_IPEX\n+\n+            if isinstance(q_proj, WQLinear_IPEX):\n+                linear_target_cls = WQLinear_IPEX\n+                cat_dim = 1\n         else:\n             raise ValueError(\"Unsupported q_proj type: {type(q_proj)}\")\n "
        },
        {
            "sha": "18b883429c5ec587235b7da5113f129d033d1814",
            "filename": "src/transformers/quantizers/quantizer_awq.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1909def2de013deadd67b418b49d24957c53ae87/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1909def2de013deadd67b418b49d24957c53ae87/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py?ref=1909def2de013deadd67b418b49d24957c53ae87",
            "patch": "@@ -53,6 +53,10 @@ def validate_environment(self, device_map, **kwargs):\n             raise ImportError(\"Loading an AWQ quantized model requires accelerate (`pip install accelerate`)\")\n \n         if self.quantization_config.version == AWQLinearVersion.IPEX:\n+            if version.parse(importlib.metadata.version(\"autoawq\")) < version.parse(\"0.2.6\"):\n+                raise RuntimeError(\n+                    \"To use IPEX backend, you need autoawq>0.6.2. Please install the latest version or from source.\"\n+                )\n             if (\n                 device_map is not None\n                 and isinstance(device_map, dict)"
        }
    ],
    "stats": {
        "total": 21,
        "additions": 16,
        "deletions": 5
    }
}