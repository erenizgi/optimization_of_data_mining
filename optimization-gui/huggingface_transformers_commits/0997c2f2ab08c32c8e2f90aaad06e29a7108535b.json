{
    "author": "shuminghu",
    "message": "Fix doc for PerceptionLMForConditionalGeneration forward. (#40733)\n\n* Fix doc for PerceptionLMForConditionalGeneration forward.\n\n* fix last nit\n\n---------\n\nCo-authored-by: raushan <raushan@huggingface.co>",
    "sha": "0997c2f2ab08c32c8e2f90aaad06e29a7108535b",
    "files": [
        {
            "sha": "d310d44a013614bd164011a36dd8d427c0b7fded",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "modified",
            "additions": 37,
            "deletions": 16,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/0997c2f2ab08c32c8e2f90aaad06e29a7108535b/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0997c2f2ab08c32c8e2f90aaad06e29a7108535b/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=0997c2f2ab08c32c8e2f90aaad06e29a7108535b",
            "patch": "@@ -374,23 +374,44 @@ def forward(\n         Example:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n-        >>> from transformers import AutoProcessor, PerceptionLMForConditionalGeneration\n-\n-        >>> model = PerceptionLMForConditionalGeneration.from_pretrained(\"perception_lm-hf/perception_lm-1.5-7b-hf\")\n-        >>> processor = AutoProcessor.from_pretrained(\"perception_lm-hf/perception_lm-1.5-7b-hf\")\n-\n-        >>> prompt = \"USER: <image>\\nWhat's the content of the image? ASSISTANT:\"\n-        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n-\n-        >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n+        from transformers import AutoProcessor, AutoModelForImageTextToText\n+        from huggingface_hub import hf_hub_download\n+\n+        MODEL_PATH = \"facebook/Perception-LM-1B\"\n+        processor = AutoProcessor.from_pretrained(MODEL_PATH, use_fast=True)\n+        model = AutoModelForImageTextToText.from_pretrained(MODEL_PATH).to(\"cuda\")\n+        test_image_file = hf_hub_download(\n+                    repo_id=\"shumingh/perception_lm_test_images\",\n+                    filename=\"14496_0.PNG\",\n+                    repo_type=\"dataset\",\n+        )\n+        conversation = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": test_image_file,\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the bar plot in the image.\"},\n+                ],\n+            }\n+        ]\n+\n+        inputs = processor.apply_chat_template(\n+            [conversation],\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+        )\n+        inputs = inputs.to(model.device)\n+        generate_ids = model.generate(**inputs, max_new_tokens=256)\n+        input_length = inputs[\"input_ids\"].shape[1]\n+        generate_ids_without_inputs = generate_ids[:, input_length:]\n \n-        >>> # Generate\n-        >>> generate_ids = model.generate(**inputs, max_new_tokens=15)\n-        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"USER:  \\nWhat's the content of the image? ASSISTANT: The image features a busy city street with a stop sign prominently displayed\"\n+        for output in processor.batch_decode(generate_ids_without_inputs, skip_special_tokens=True):\n+            print(output)\n         ```\"\"\"\n         outputs = self.model(\n             input_ids=input_ids,"
        },
        {
            "sha": "b2d6e4090c3a8d06e654eac9b4f670fc739b8b9a",
            "filename": "src/transformers/models/perception_lm/modular_perception_lm.py",
            "status": "modified",
            "additions": 48,
            "deletions": 0,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/0997c2f2ab08c32c8e2f90aaad06e29a7108535b/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0997c2f2ab08c32c8e2f90aaad06e29a7108535b/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py?ref=0997c2f2ab08c32c8e2f90aaad06e29a7108535b",
            "patch": "@@ -335,6 +335,54 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **lm_kwargs,\n     ) -> Union[tuple, PerceptionLMCausalLMOutputWithPast]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        from transformers import AutoProcessor, AutoModelForImageTextToText\n+        from huggingface_hub import hf_hub_download\n+\n+        MODEL_PATH = \"facebook/Perception-LM-1B\"\n+        processor = AutoProcessor.from_pretrained(MODEL_PATH, use_fast=True)\n+        model = AutoModelForImageTextToText.from_pretrained(MODEL_PATH).to(\"cuda\")\n+        test_image_file = hf_hub_download(\n+                    repo_id=\"shumingh/perception_lm_test_images\",\n+                    filename=\"14496_0.PNG\",\n+                    repo_type=\"dataset\",\n+        )\n+        conversation = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": test_image_file,\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the bar plot in the image.\"},\n+                ],\n+            }\n+        ]\n+\n+        inputs = processor.apply_chat_template(\n+            [conversation],\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+        )\n+        inputs = inputs.to(model.device)\n+        generate_ids = model.generate(**inputs, max_new_tokens=256)\n+        input_length = inputs[\"input_ids\"].shape[1]\n+        generate_ids_without_inputs = generate_ids[:, input_length:]\n+\n+        for output in processor.batch_decode(generate_ids_without_inputs, skip_special_tokens=True):\n+            print(output)\n+        ```\"\"\"\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,"
        }
    ],
    "stats": {
        "total": 101,
        "additions": 85,
        "deletions": 16
    }
}