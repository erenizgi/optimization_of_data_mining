{
    "author": "gante",
    "message": "[docs] remove last references to `transformers` TF classes/methods (#40429)\n\n* halfway through tasks\n\n* complete\n\n* Update utils/check_docstrings.py",
    "sha": "1763ef2951cc1f200d84a0f57de759be6b1b8af5",
    "files": [
        {
            "sha": "4fe267b5f645aa86c2798814c85c50f43aab1472",
            "filename": "docs/source/en/add_new_pipeline.md",
            "status": "modified",
            "additions": 7,
            "deletions": 12,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fadd_new_pipeline.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fadd_new_pipeline.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fadd_new_pipeline.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -100,19 +100,18 @@ pipeline(\"This is the best meal I've ever had\")\n \n Register the new task your pipeline supports in the `PIPELINE_REGISTRY`. The registry defines:\n \n-- the machine learning framework the pipeline supports with either `pt_model` or `tf_model` (add both to ensure it works with either frameworks)\n+- The supported Pytorch model class with `pt_model`\n - a default model which should come from a specific revision (branch, or commit hash) where the model works as expected with `default`\n - the expected input with `type`\n \n ```py\n from transformers.pipelines import PIPELINE_REGISTRY\n-from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n+from transformers import AutoModelForSequenceClassification\n \n PIPELINE_REGISTRY.register_pipeline(\n     \"new-task\",\n     pipeline_class=MyPipeline,\n     pt_model=AutoModelForSequenceClassification,\n-    tf_model=TFAutoModelForSequenceClassification,\n     default={\"pt\": (\"user/awesome-model\", \"branch-name\")},\n     type=\"text\",\n )\n@@ -128,7 +127,7 @@ It's faster to upload your pipeline code to the Hub because it doesn't require a\n \n Add your pipeline code to the Hub in a Python file.\n \n-For example, a custom pipeline for sentence pair classification might look like the following code below. The implementation works for PyTorch and TensorFlow models.\n+For example, a custom pipeline for sentence pair classification might look like the following code below.\n \n ```py\n import numpy as np\n@@ -168,13 +167,12 @@ Save the code in a file named `pair_classification.py`, and import and register\n ```py\n from pair_classification import PairClassificationPipeline\n from transformers.pipelines import PIPELINE_REGISTRY\n-from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n+from transformers import AutoModelForSequenceClassification\n \n PIPELINE_REGISTRY.register_pipeline(\n     \"pair-classification\",\n     pipeline_class=PairClassificationPipeline,\n     pt_model=AutoModelForSequenceClassification,\n-    tf_model=TFAutoModelForSequenceClassification,\n )\n ```\n \n@@ -187,9 +185,6 @@ The [register_pipeline](https://github.com/huggingface/transformers/blob/9feae5f\n       \"pt\": [\n         \"AutoModelForSequenceClassification\"\n       ],\n-      \"tf\": [\n-        \"TFAutoModelForSequenceClassification\"\n-      ],\n     }\n   },\n ```\n@@ -219,11 +214,11 @@ Add your pipeline code as a new module to the [pipelines](https://github.com/hug\n \n Next, add a new test for the pipeline in [transformers/tests/pipelines](https://github.com/huggingface/transformers/tree/main/tests/pipelines). You can look at the other tests for examples of how to test your pipeline.\n \n-The [run_pipeline_test](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L186) function should be very generic and run on the models defined in [model_mapping](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L48) and [tf_model_mapping](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L49). This is important for testing future compatibility with new models.\n+The [run_pipeline_test](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L186) function should be very generic and run on the models defined in [model_mapping](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L48). This is important for testing future compatibility with new models.\n \n You'll also notice `ANY` is used throughout the [run_pipeline_test](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L186) function. The models are random, so you can't check the actual values. Using `ANY` allows the test to match the output of the pipeline type instead.\n \n Finally, you should also implement the following 4 tests.\n \n-1. [test_small_model_pt](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L59) and [test_small_model_tf](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L150), use a small model for these pipelines to make sure they return the correct outputs. The results don't have to make sense. Each pipeline should return the same result.\n-1. [test_large_model_pt](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_zero_shot_image_classification.py#L187) nad [test_large_model_tf](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_zero_shot_image_classification.py#L220), use a realistic model for these pipelines to make sure they return meaningful results. These tests are slow and should be marked as slow.\n+1. [test_small_model_pt](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L59), use a small model for these pipelines to make sure they return the correct outputs. The results don't have to make sense. Each pipeline should return the same result.\n+1. [test_large_model_pt](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_zero_shot_image_classification.py#L187), use a realistic model for these pipelines to make sure they return meaningful results. These tests are slow and should be marked as slow."
        },
        {
            "sha": "93d38dbf8ec0bae13373e8f141954cf53d3ffe50",
            "filename": "docs/source/en/community.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fcommunity.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fcommunity.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcommunity.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -17,7 +17,6 @@ This page regroups resources around ðŸ¤— Transformers developed by the community\n | Notebook     |      Description      |      Author      |      |\n |:----------|:-------------|:-------------|------:|\n | [Fine-tune a pre-trained Transformer to generate lyrics](https://github.com/AlekseyKorshuk/huggingartists) | How to generate lyrics in the style of your favorite artist by fine-tuning a GPT-2 model |  [Aleksey Korshuk](https://github.com/AlekseyKorshuk) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb) |\n-| [Train T5 in Tensorflow 2](https://github.com/snapthat/TF-T5-text-to-text) | How to train T5 for any task using Tensorflow 2. This notebook demonstrates a Question & Answer task implemented in Tensorflow 2 using SQUAD | [Muhammad Harris](https://github.com/HarrisDePerceptron) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb) |\n | [Train T5 on TPU](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb)  | How to train T5 on SQUAD with Transformers and Nlp | [Suraj Patil](https://github.com/patil-suraj) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil) |\n | [Fine-tune T5 for Classification and Multiple Choice](https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb)  | How to fine-tune T5 for classification and multiple choice tasks using a text-to-text format with PyTorch Lightning |  [Suraj Patil](https://github.com/patil-suraj) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb) |\n | [Fine-tune DialoGPT on New Datasets and Languages](https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb)  | How to fine-tune the DialoGPT model on a new dataset for open-dialog conversational chatbots |  [Nathan Cooper](https://github.com/ncoop57) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb) |\n@@ -42,7 +41,6 @@ This page regroups resources around ðŸ¤— Transformers developed by the community\n |[Fine-tune ALBERT for sentence-pair classification](https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb) | How to fine-tune an ALBERT model or another BERT-based model for the sentence-pair classification task | [Nadir El Manouzi](https://github.com/NadirEM) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb)|\n |[Fine-tune Roberta for sentiment analysis](https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb) | How to fine-tune a Roberta model for sentiment analysis | [Dhaval Taunk](https://github.com/DhavalTaunk08) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb)|\n |[Evaluating Question Generation Models](https://github.com/flexudy-pipe/qugeev) | How accurate are the answers to questions generated by your seq2seq transformer model? | [Pascal Zoleko](https://github.com/zolekode) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1bpsSqCQU-iw_5nNoRm_crPq6FRuJthq_?usp=sharing)|\n-|[Classify text with DistilBERT and Tensorflow](https://github.com/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb) | How to fine-tune DistilBERT for text classification in TensorFlow | [Peter Bayerle](https://github.com/peterbayerle) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb)|\n |[Leverage BERT for Encoder-Decoder Summarization on CNN/Dailymail](https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb) | How to warm-start a *EncoderDecoderModel* with a *google-bert/bert-base-uncased* checkpoint for summarization on CNN/Dailymail | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)|\n |[Leverage RoBERTa for Encoder-Decoder Summarization on BBC XSum](https://github.com/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb) | How to warm-start a shared *EncoderDecoderModel* with a *FacebookAI/roberta-base* checkpoint for summarization on BBC/XSum | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb)|\n |[Fine-tune TAPAS on Sequential Question Answering (SQA)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) | How to fine-tune *TapasForQuestionAnswering* with a *tapas-base* checkpoint on the Sequential Question Answering (SQA) dataset | [Niels Rogge](https://github.com/nielsrogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)|"
        },
        {
            "sha": "3e9db79cfc7f0eafb461387c89f71c59cb9831dd",
            "filename": "docs/source/en/fast_tokenizers.md",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ffast_tokenizers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ffast_tokenizers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ffast_tokenizers.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -31,7 +31,7 @@ from transformers import AutoTokenizer\n tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library\", return_tensors=\"pt\")\n {'input_ids': tensor([[     2,   1734,    708,   1508,   4915,    577,   1500,    692,    573,\n-         156808, 128149,   9581, 235265]]), \n+         156808, 128149,   9581, 235265]]),\n  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n }\n ```\n@@ -62,7 +62,7 @@ from transformers import AutoTokenizer\n tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\", return_tensors=\"pt\")\n {'input_ids': tensor([[     2,   1734,    708,   1508,   4915,    577,   1500,    692,    573,\n-         156808, 128149,   9581, 235265]]), \n+         156808, 128149,   9581, 235265]]),\n  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n }\n ```\n@@ -112,7 +112,7 @@ tokenizer = GemmaTokenizerFast(vocab_file=\"my_vocab_file.txt\")\n \n ## Multimodal tokenizers\n \n-In addition to text tokens, multimodal tokenizers also holds tokens from other modalities as a part of its attributes for easy access. \n+In addition to text tokens, multimodal tokenizers also holds tokens from other modalities as a part of its attributes for easy access.\n \n To add these special tokens to a tokenizer, pass them as a dictionary to the `extra_special_tokens` parameter in [`~AutoTokenizer.from_pretrained`]. The example below adds the `image_token` to a vision-language model.\n \n@@ -198,7 +198,7 @@ Add the `subfolder` parameter to [`~PreTrainedModel.from_pretrained`] to specify\n ```py\n from transformers import AutoTokenizer\n \n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", subfolder=\"original\") \n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", subfolder=\"original\")\n ```\n \n ### Create a tiktoken tokenizer\n@@ -226,15 +226,15 @@ tokenizer = PreTrainedTokenizerFast.from_pretrained(\"config/save/dir\")\n \n <Youtube id=\"Yffk5aydLzg\"/>\n \n-A Transformers model expects the input to be a PyTorch, TensorFlow, or NumPy tensor. A tokenizers job is to preprocess text into those tensors. Specify the framework tensor type to return with the `return_tensors` parameter.\n+A Transformers model expects the input to be a PyTorch or NumPy tensor. A tokenizers job is to preprocess text into those tensors. Specify the framework tensor type to return with the `return_tensors` parameter.\n \n ```py\n from transformers import AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\", return_tensors=\"pt\")\n {'input_ids': tensor([[     2,   1734,    708,   1508,   4915,    577,   1500,    692,    573,\n-         156808, 128149,   9581, 235265]]), \n+         156808, 128149,   9581, 235265]]),\n  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n }\n ```\n@@ -321,12 +321,12 @@ batch_sentences = [\n encoded_inputs = tokenizer(batch_sentences, return_tensors=\"pt\")\n print(encoded_inputs)\n {\n- 'input_ids': \n-    [[2, 1860, 1212, 1105, 2257, 14457, 235336], \n-     [2, 4454, 235303, 235251, 1742, 693, 9242, 1105, 2257, 14457, 235269, 48782, 235265], \n-     [2, 1841, 1105, 29754, 37453, 235336]], \n- 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], \n-                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n+ 'input_ids':\n+    [[2, 1860, 1212, 1105, 2257, 14457, 235336],\n+     [2, 4454, 235303, 235251, 1742, 693, 9242, 1105, 2257, 14457, 235269, 48782, 235265],\n+     [2, 1841, 1105, 29754, 37453, 235336]],\n+ 'attention_mask': [[1, 1, 1, 1, 1, 1, 1],\n+                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n                     [1, 1, 1, 1, 1, 1]]\n }\n ```"
        },
        {
            "sha": "9e57c3fdc9f8f3b311b98496be52c2c8431d75b3",
            "filename": "docs/source/en/glossary.md",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fglossary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fglossary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fglossary.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -67,7 +67,7 @@ We can see that 0s have been added on the right of the first sentence to make it\n [[101, 1188, 1110, 170, 1603, 4954, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102]]\n ```\n \n-This can then be converted into a tensor in PyTorch or TensorFlow. The attention mask is a binary tensor indicating the\n+This can then be converted into a tensor in PyTorch. The attention mask is a binary tensor indicating the\n position of the padded indices so that the model does not attend to them. For the [`BertTokenizer`], `1` indicates a\n value that should be attended to, while `0` indicates a padded value. This attention mask is in the dictionary returned\n by the tokenizer under the key \"attention_mask\":\n@@ -114,7 +114,7 @@ A type of layer in a neural network where the input matrix is multiplied element\n \n ### DataParallel (DP)\n \n-Parallelism technique for training on multiple GPUs where the same setup is replicated multiple times, with each instance \n+Parallelism technique for training on multiple GPUs where the same setup is replicated multiple times, with each instance\n receiving a distinct data slice. The processing is done in parallel and all setups are synchronized at the end of each training step.\n \n Learn more about how DataParallel works [here](perf_train_gpu_many#dataparallel-vs-distributeddataparallel).\n@@ -295,7 +295,7 @@ These labels are different according to the model head, for example:\n   `class_labels` and `boxes` key where each value of the batch corresponds to the expected label and number of bounding boxes of each individual image.\n - For automatic speech recognition models, ([`Wav2Vec2ForCTC`]), the model expects a tensor of dimension `(batch_size,\n   target_length)` with each value corresponding to the expected label of each individual token.\n-  \n+\n <Tip>\n \n Each model's labels may be different, so be sure to always check the documentation of each model for more information\n@@ -346,8 +346,8 @@ For more details, see [Pipelines for inference](https://huggingface.co/docs/tran\n \n ### PipelineParallel (PP)\n \n-Parallelism technique in which the model is split up vertically (layer-level) across multiple GPUs, so that only one or \n-several layers of the model are placed on a single GPU. Each GPU processes in parallel different stages of the pipeline \n+Parallelism technique in which the model is split up vertically (layer-level) across multiple GPUs, so that only one or\n+several layers of the model are placed on a single GPU. Each GPU processes in parallel different stages of the pipeline\n and working on a small chunk of the batch. Learn more about how PipelineParallel works [here](perf_train_gpu_many#from-naive-model-parallelism-to-pipeline-parallelism).\n \n ### pixel values\n@@ -379,7 +379,7 @@ The task of preparing raw data into a format that can be easily consumed by mach\n A model that has been pretrained on some data (for instance all of Wikipedia). Pretraining methods involve a\n self-supervised objective, which can be reading the text and trying to predict the next word (see [causal language\n modeling](#causal-language-modeling)) or masking some words and trying to predict them (see [masked language\n-modeling](#masked-language-modeling-mlm)). \n+modeling](#masked-language-modeling-mlm)).\n \n Speech and vision models have their own pretraining objectives. For example, Wav2Vec2 is a speech model pretrained on a contrastive task which requires the model to identify the \"true\" speech representation from a set of \"false\" speech representations. On the other hand, BEiT is a vision model pretrained on a masked image modeling task which masks some of the image patches and requires the model to predict the masked patches (similar to the masked language modeling objective).\n \n@@ -403,9 +403,9 @@ A measurement in hertz of the number of samples (the audio signal) taken per sec\n \n Each element of the input finds out which other elements of the input they should attend to.\n \n-### self-supervised learning \n+### self-supervised learning\n \n-A category of machine learning techniques in which a model creates its own learning objective from unlabeled data. It differs from [unsupervised learning](#unsupervised-learning) and [supervised learning](#supervised-learning) in that the learning process is supervised, but not explicitly from the user. \n+A category of machine learning techniques in which a model creates its own learning objective from unlabeled data. It differs from [unsupervised learning](#unsupervised-learning) and [supervised learning](#supervised-learning) in that the learning process is supervised, but not explicitly from the user.\n \n One example of self-supervised learning is [masked language modeling](#masked-language-modeling-mlm), where a model is passed sentences with a proportion of its tokens removed and learns to predict the missing tokens.\n \n@@ -436,9 +436,9 @@ A form of model training that directly uses labeled data to correct and instruct\n \n ### Tensor Parallelism (TP)\n \n-Parallelism technique for training on multiple GPUs in which each tensor is split up into multiple chunks, so instead of \n-having the whole tensor reside on a single GPU, each shard of the tensor resides on its designated GPU. Shards gets \n-processed separately and in parallel on different GPUs and the results are synced at the end of the processing step. \n+Parallelism technique for training on multiple GPUs in which each tensor is split up into multiple chunks, so instead of\n+having the whole tensor reside on a single GPU, each shard of the tensor resides on its designated GPU. Shards gets\n+processed separately and in parallel on different GPUs and the results are synced at the end of the processing step.\n This is what is sometimes called horizontal parallelism, as the splitting happens on horizontal level.\n Learn more about Tensor Parallelism [here](perf_train_gpu_many#tensor-parallelism).\n \n@@ -516,7 +516,7 @@ A form of model training in which data provided to the model is not labeled. Uns\n \n ### Zero Redundancy Optimizer (ZeRO)\n \n-Parallelism technique which performs sharding of the tensors somewhat similar to [TensorParallel](#tensor-parallelism-tp), \n-except the whole tensor gets reconstructed in time for a forward or backward computation, therefore the model doesn't need \n-to be modified. This method also supports various offloading techniques to compensate for limited GPU memory. \n+Parallelism technique which performs sharding of the tensors somewhat similar to [TensorParallel](#tensor-parallelism-tp),\n+except the whole tensor gets reconstructed in time for a forward or backward computation, therefore the model doesn't need\n+to be modified. This method also supports various offloading techniques to compensate for limited GPU memory.\n Learn more about ZeRO [here](perf_train_gpu_many#zero-data-parallelism)."
        },
        {
            "sha": "cec846b6355e3fc6de9dac721c905f50da8e322e",
            "filename": "docs/source/en/hpo_train.md",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fhpo_train.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fhpo_train.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fhpo_train.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -37,7 +37,6 @@ An example `model_init` function is shown below.\n def model_init(trial):\n     return AutoModelForSequenceClassification.from_pretrained(\n         model_args.model_name_or_path,\n-        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n         config=config,\n         cache_dir=model_args.cache_dir,\n         revision=model_args.model_revision,\n@@ -103,7 +102,7 @@ def ray_hp_space(trial):\n         \"per_device_train_batch_size\": tune.choice([16, 32, 64, 128]),\n     }\n \n-best_trials = trainer.hyperparameter_search( \n+best_trials = trainer.hyperparameter_search(\n     direction=[\"minimize\", \"maximize\"],\n     backend=\"ray\",\n     hp_space=ray_hp_space,\n@@ -128,7 +127,7 @@ def sigopt_hp_space(trial):\n         },\n     ]\n \n-best_trials = trainer.hyperparameter_search( \n+best_trials = trainer.hyperparameter_search(\n     direction=[\"minimize\", \"maximize\"],\n     backend=\"sigopt\",\n     hp_space=sigopt_hp_space,\n@@ -153,7 +152,7 @@ def wandb_hp_space(trial):\n         },\n     }\n \n-best_trials = trainer.hyperparameter_search( \n+best_trials = trainer.hyperparameter_search(\n     direction=[\"minimize\", \"maximize\"],\n     backend=\"wandb\",\n     hp_space=wandb_hp_space,"
        },
        {
            "sha": "b29c9e7264ec48aeeb60b28b7b8ed725fe1a6d41",
            "filename": "docs/source/en/main_classes/callback.md",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fmain_classes%2Fcallback.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fmain_classes%2Fcallback.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fcallback.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -17,9 +17,8 @@ rendered properly in your Markdown viewer.\n # Callbacks\n \n Callbacks are objects that can customize the behavior of the training loop in the PyTorch\n-[`Trainer`] (this feature is not yet implemented in TensorFlow) that can inspect the training loop\n-state (for progress reporting, logging on TensorBoard or other ML platforms...) and take decisions (like early\n-stopping).\n+[`Trainer`] that can inspect the training loop state (for progress reporting, logging on TensorBoard or other ML\n+platforms...) and take decisions (like early stopping).\n \n Callbacks are \"read only\" pieces of code, apart from the [`TrainerControl`] object they return, they\n cannot change anything in the training loop. For customizations that require changes in the training loop, you should\n@@ -48,7 +47,7 @@ By default, `TrainingArguments.report_to` is set to `\"all\"`, so a [`Trainer`] wi\n - [`~integrations.DVCLiveCallback`] if [dvclive](https://dvc.org/doc/dvclive) is installed.\n - [`~integrations.SwanLabCallback`] if [swanlab](http://swanlab.cn/) is installed.\n \n-If a package is installed but you don't wish to use the accompanying integration, you can change `TrainingArguments.report_to` to a list of just those integrations you want to use (e.g. `[\"azure_ml\", \"wandb\"]`). \n+If a package is installed but you don't wish to use the accompanying integration, you can change `TrainingArguments.report_to` to a list of just those integrations you want to use (e.g. `[\"azure_ml\", \"wandb\"]`).\n \n The main class that implements callbacks is [`TrainerCallback`]. It gets the\n [`TrainingArguments`] used to instantiate the [`Trainer`], can access that"
        },
        {
            "sha": "fd451a35481a137261e3d45f6f4be131b7e76966",
            "filename": "docs/source/en/main_classes/feature_extractor.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fmain_classes%2Ffeature_extractor.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fmain_classes%2Ffeature_extractor.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Ffeature_extractor.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n \n # Feature Extractor\n \n-A feature extractor is in charge of preparing input features for audio or vision models. This includes feature extraction from sequences, e.g., pre-processing audio files to generate Log-Mel Spectrogram features, feature extraction from images, e.g., cropping image files, but also padding, normalization, and conversion to NumPy, PyTorch, and TensorFlow tensors.\n+A feature extractor is in charge of preparing input features for audio or vision models. This includes feature extraction from sequences, e.g., pre-processing audio files to generate Log-Mel Spectrogram features, feature extraction from images, e.g., cropping image files, but also padding, normalization, and conversion to NumPy and PyTorch tensors.\n \n \n ## FeatureExtractionMixin"
        },
        {
            "sha": "10e78b34a4a6177ed84f007678c9d4d980070053",
            "filename": "docs/source/en/main_classes/image_processor.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fmain_classes%2Fimage_processor.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fmain_classes%2Fimage_processor.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fimage_processor.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n \n # Image Processor\n \n-An image processor is in charge of preparing input features for vision models and post processing their outputs. This includes transformations such as resizing, normalization, and conversion to PyTorch, TensorFlow, Flax and Numpy tensors. It may also include model specific post-processing such as converting logits to segmentation masks.\n+An image processor is in charge of preparing input features for vision models and post processing their outputs. This includes transformations such as resizing, normalization, and conversion to Numpy and PyTorch tensors. It may also include model specific post-processing such as converting logits to segmentation masks.\n \n Fast image processors are available for a few models and more will be added in the future. They are based on the [torchvision](https://pytorch.org/vision/stable/index.html) library and provide a significant speed-up, especially when processing on GPU.\n They have the same API as the base image processors and can be used as drop-in replacements."
        },
        {
            "sha": "303db58fcfb1b8d5a607065af6589eeebc7d39f3",
            "filename": "docs/source/en/main_classes/optimizer_schedules.md",
            "status": "modified",
            "additions": 2,
            "deletions": 18,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fmain_classes%2Foptimizer_schedules.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fmain_classes%2Foptimizer_schedules.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Foptimizer_schedules.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -23,19 +23,13 @@ The `.optimization` module provides:\n - a gradient accumulation class to accumulate the gradients of multiple batches\n \n \n-## AdaFactor (PyTorch)\n+## AdaFactor\n \n [[autodoc]] Adafactor\n \n-## AdamWeightDecay (TensorFlow)\n-\n-[[autodoc]] AdamWeightDecay\n-\n-[[autodoc]] create_optimizer\n-\n ## Schedules\n \n-### Learning Rate Schedules (PyTorch)\n+### Learning Rate Schedules\n \n [[autodoc]] SchedulerType\n \n@@ -64,13 +58,3 @@ The `.optimization` module provides:\n [[autodoc]] get_inverse_sqrt_schedule\n \n [[autodoc]] get_wsd_schedule\n-\n-### Warmup (TensorFlow)\n-\n-[[autodoc]] WarmUp\n-\n-## Gradient Strategies\n-\n-### GradientAccumulator (TensorFlow)\n-\n-[[autodoc]] GradientAccumulator"
        },
        {
            "sha": "70bec48d6ec43d562cbdc3c450b0684db5cf91f9",
            "filename": "docs/source/en/quicktour.md",
            "status": "modified",
            "additions": 2,
            "deletions": 88,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquicktour.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -45,7 +45,7 @@ notebook_login()\n \n </hfoption>\n <hfoption id=\"CLI\">\n-   \n+\n Make sure the [huggingface_hub[cli]](https://huggingface.co/docs/huggingface_hub/guides/cli#getting-started) package is installed and run the command below. Paste your User Access Token when prompted to log in.\n \n ```bash\n@@ -55,25 +55,12 @@ hf auth login\n </hfoption>\n </hfoptions>\n \n-Install a machine learning framework.\n-\n-<hfoptions id=\"installation\">\n-<hfoption id=\"PyTorch\">\n+Install Pytorch.\n \n ```bash\n !pip install torch\n ```\n \n-</hfoption>\n-<hfoption id=\"TensorFlow\">\n-\n-```bash\n-!pip install tensorflow\n-```\n-\n-</hfoption>\n-</hfoptions>\n-\n Then install an up-to-date version of Transformers and some additional libraries from the Hugging Face ecosystem for accessing datasets and vision models, evaluating training, and optimizing training for large models.\n \n ```bash\n@@ -94,9 +81,6 @@ We recommend using the [AutoClass](./model_doc/auto) API to load models and prep\n \n Use [`~PreTrainedModel.from_pretrained`] to load the weights and configuration file from the Hub into the model and preprocessor class.\n \n-<hfoptions id=\"base-classes\">\n-<hfoption id=\"PyTorch\">\n-\n When you load a model, configure the following parameters to ensure the model is optimally loaded.\n \n - `device_map=\"auto\"` automatically allocates the model weights to your fastest device first.\n@@ -125,35 +109,6 @@ tokenizer.batch_decode(generated_ids)[0]\n '<s> The secret to baking a good cake is 100% in the preparation. There are so many recipes out there,'\n ```\n \n-</hfoption>\n-<hfoption id=\"TensorFlow\">\n-\n-```py\n-from transformers import TFAutoModelForCausalLM, AutoTokenizer\n-\n-model = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-xl\")\n-tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-xl\")\n-```\n-\n-Tokenize the text and return TensorFlow tensors with the tokenizer.\n-\n-```py\n-model_inputs = tokenizer([\"The secret to baking a good cake is \"], return_tensors=\"tf\")\n-```\n-\n-The model is now ready for inference or training.\n-\n-For inference, pass the tokenized inputs to [`~GenerationMixin.generate`] to generate text. Decode the token ids back into text with [`~PreTrainedTokenizerBase.batch_decode`].\n-\n-```py\n-generated_ids = model.generate(**model_inputs, max_length=30)\n-tokenizer.batch_decode(generated_ids)[0]\n-'The secret to baking a good cake is \\xa0to use the right ingredients. \\xa0The secret to baking a good cake is to use the right'\n-```\n-\n-</hfoption>\n-</hfoptions>\n-\n > [!TIP]\n > Skip ahead to the [Trainer](#trainer-api) section to learn how to fine-tune a model.\n \n@@ -309,47 +264,6 @@ trainer.push_to_hub()\n \n Congratulations, you just trained your first model with Transformers!\n \n-### TensorFlow\n-\n-> [!WARNING]\n-> Not all pretrained models are available in TensorFlow. Refer to a models API doc to check whether a TensorFlow implementation is supported.\n-\n-[`Trainer`] doesn't work with TensorFlow models, but you can still train a Transformers model implemented in TensorFlow with [Keras](https://keras.io/). Transformers TensorFlow models are a standard [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model), which is compatible with Keras' [compile](https://keras.io/api/models/model_training_apis/#compile-method) and [fit](https://keras.io/api/models/model_training_apis/#fit-method) methods.\n-\n-Load a model, tokenizer, and dataset for training.\n-\n-```py\n-from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n-\n-model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Create a function to tokenize the text and convert it into TensorFlow tensors. Apply this function to the whole dataset with the [`~datasets.Dataset.map`] method.\n-\n-```py\n-def tokenize_dataset(dataset):\n-    return tokenizer(dataset[\"text\"])\n-dataset = dataset.map(tokenize_dataset)\n-```\n-\n-Transformers provides the [`~TFPreTrainedModel.prepare_tf_dataset`] method to collate and batch a dataset.\n-\n-```py\n-tf_dataset = model.prepare_tf_dataset(\n-    dataset[\"train\"], batch_size=16, shuffle=True, tokenizer=tokenizer\n-)\n-```\n-\n-Finally, call [compile](https://keras.io/api/models/model_training_apis/#compile-method) to configure the model for training and [fit](https://keras.io/api/models/model_training_apis/#fit-method) to start.\n-\n-```py\n-from tensorflow.keras.optimizers import Adam\n-\n-model.compile(optimizer=\"adam\")\n-model.fit(tf_dataset)\n-```\n-\n ## Next steps\n \n Now that you have a better understanding of Transformers and what it offers, it's time to keep exploring and learning what interests you the most."
        },
        {
            "sha": "831f163bed18eec269dd206b0ca3db2f12a475a2",
            "filename": "docs/source/en/serialization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fserialization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Fserialization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserialization.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -42,7 +42,7 @@ pip install optimum[exporters]\n > optimum-cli export onnx --help\n > ```\n \n-Set the `--model` argument to export a PyTorch or TensorFlow model from the Hub.\n+Set the `--model` argument to export a PyTorch model from the Hub.\n \n ```bash\n optimum-cli export onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/"
        },
        {
            "sha": "39b013f129cce4aac87253bc9ea40309d6ba0bfd",
            "filename": "docs/source/en/tasks/image_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 238,
            "changes": 239,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Fimage_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Fimage_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_classification.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -108,8 +108,6 @@ The next step is to load a ViT image processor to process the image into a tenso\n >>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n ```\n \n-<frameworkcontent>\n-<pt>\n Apply some image transformations to the images to make the model more robust against overfitting. Here you'll use torchvision's [`transforms`](https://pytorch.org/vision/stable/transforms.html) module, but you can also use any image library you like.\n \n Crop a random part of the image, resize it, and normalize it with the image mean and standard deviation:\n@@ -148,95 +146,6 @@ Now create a batch of examples using [`DefaultDataCollator`]. Unlike other data\n \n >>> data_collator = DefaultDataCollator()\n ```\n-</pt>\n-</frameworkcontent>\n-\n-\n-<frameworkcontent>\n-<tf>\n-\n-To avoid overfitting and to make the model more robust, add some data augmentation to the training part of the dataset.\n-Here we use Keras preprocessing layers to define the transformations for the training data (includes data augmentation),\n-and transformations for the validation data (only center cropping, resizing and normalizing). You can use `tf.image`or\n-any other library you prefer.\n-\n-```py\n->>> from tensorflow import keras\n->>> from tensorflow.keras import layers\n-\n->>> size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n-\n->>> train_data_augmentation = keras.Sequential(\n-...     [\n-...         layers.RandomCrop(size[0], size[1]),\n-...         layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n-...         layers.RandomFlip(\"horizontal\"),\n-...         layers.RandomRotation(factor=0.02),\n-...         layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n-...     ],\n-...     name=\"train_data_augmentation\",\n-... )\n-\n->>> val_data_augmentation = keras.Sequential(\n-...     [\n-...         layers.CenterCrop(size[0], size[1]),\n-...         layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n-...     ],\n-...     name=\"val_data_augmentation\",\n-... )\n-```\n-\n-Next, create functions to apply appropriate transformations to a batch of images, instead of one image at a time.\n-\n-```py\n->>> import numpy as np\n->>> import tensorflow as tf\n->>> from PIL import Image\n-\n-\n->>> def convert_to_tf_tensor(image: Image):\n-...     np_image = np.array(image)\n-...     tf_image = tf.convert_to_tensor(np_image)\n-...     # `expand_dims()` is used to add a batch dimension since\n-...     # the TF augmentation layers operates on batched inputs.\n-...     return tf.expand_dims(tf_image, 0)\n-\n-\n->>> def preprocess_train(example_batch):\n-...     \"\"\"Apply train_transforms across a batch.\"\"\"\n-...     images = [\n-...         train_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n-...     ]\n-...     example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n-...     return example_batch\n-\n-\n-... def preprocess_val(example_batch):\n-...     \"\"\"Apply val_transforms across a batch.\"\"\"\n-...     images = [\n-...         val_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n-...     ]\n-...     example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n-...     return example_batch\n-```\n-\n-Use ðŸ¤— Datasets [`~datasets.Dataset.set_transform`] to apply the transformations on the fly:\n-\n-```py\n-food[\"train\"].set_transform(preprocess_train)\n-food[\"test\"].set_transform(preprocess_val)\n-```\n-\n-As a final preprocessing step, create a batch of examples using `DefaultDataCollator`. Unlike other data collators in ðŸ¤— Transformers, the\n-`DefaultDataCollator` does not apply additional preprocessing, such as padding.\n-\n-```py\n->>> from transformers import DefaultDataCollator\n-\n->>> data_collator = DefaultDataCollator(return_tensors=\"tf\")\n-```\n-</tf>\n-</frameworkcontent>\n \n ## Evaluate\n \n@@ -266,8 +175,7 @@ Your `compute_metrics` function is ready to go now, and you'll return to it when\n \n ## Train\n \n-<frameworkcontent>\n-<pt>\n+\n <Tip>\n \n If you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n@@ -329,116 +237,6 @@ Once training is completed, share your model to the Hub with the [`~transformers\n ```py\n >>> trainer.push_to_hub()\n ```\n-</pt>\n-</frameworkcontent>\n-\n-<frameworkcontent>\n-<tf>\n-\n-<Tip>\n-\n-If you are unfamiliar with fine-tuning a model with Keras, check out the [basic tutorial](./training#train-a-tensorflow-model-with-keras) first!\n-\n-</Tip>\n-\n-To fine-tune a model in TensorFlow, follow these steps:\n-1. Define the training hyperparameters, and set up an optimizer and a learning rate schedule.\n-2. Instantiate a pre-trained model.\n-3. Convert a ðŸ¤— Dataset to a `tf.data.Dataset`.\n-4. Compile your model.\n-5. Add callbacks and use the `fit()` method to run the training.\n-6. Upload your model to ðŸ¤— Hub to share with the community.\n-\n-Start by defining the hyperparameters, optimizer and learning rate schedule:\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_epochs = 5\n->>> num_train_steps = len(food[\"train\"]) * num_epochs\n->>> learning_rate = 3e-5\n->>> weight_decay_rate = 0.01\n-\n->>> optimizer, lr_schedule = create_optimizer(\n-...     init_lr=learning_rate,\n-...     num_train_steps=num_train_steps,\n-...     weight_decay_rate=weight_decay_rate,\n-...     num_warmup_steps=0,\n-... )\n-```\n-\n-Then, load ViT with [`TFAutoModelForImageClassification`] along with the label mappings:\n-\n-```py\n->>> from transformers import TFAutoModelForImageClassification\n-\n->>> model = TFAutoModelForImageClassification.from_pretrained(\n-...     checkpoint,\n-...     id2label=id2label,\n-...     label2id=label2id,\n-... )\n-```\n-\n-Convert your datasets to the `tf.data.Dataset` format using the [`~datasets.Dataset.to_tf_dataset`] and your `data_collator`:\n-\n-```py\n->>> # converting our train dataset to tf.data.Dataset\n->>> tf_train_dataset = food[\"train\"].to_tf_dataset(\n-...     columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n-... )\n-\n->>> # converting our test dataset to tf.data.Dataset\n->>> tf_eval_dataset = food[\"test\"].to_tf_dataset(\n-...     columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n-... )\n-```\n-\n-Configure the model for training with `compile()`:\n-\n-```py\n->>> from tensorflow.keras.losses import SparseCategoricalCrossentropy\n-\n->>> loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n->>> model.compile(optimizer=optimizer, loss=loss)\n-```\n-\n-To compute the accuracy from the predictions and push your model to the ðŸ¤— Hub, use [Keras callbacks](../main_classes/keras_callbacks).\n-Pass your `compute_metrics` function to [KerasMetricCallback](../main_classes/keras_callbacks#transformers.KerasMetricCallback),\n-and use the [PushToHubCallback](../main_classes/keras_callbacks#transformers.PushToHubCallback) to upload the model:\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_eval_dataset)\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"food_classifier\",\n-...     tokenizer=image_processor,\n-...     save_strategy=\"no\",\n-... )\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-Finally, you are ready to train your model! Call `fit()` with your training and validation datasets, the number of epochs,\n-and your callbacks to fine-tune the model:\n-\n-```py\n->>> model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs, callbacks=callbacks)\n-Epoch 1/5\n-250/250 [==============================] - 313s 1s/step - loss: 2.5623 - val_loss: 1.4161 - accuracy: 0.9290\n-Epoch 2/5\n-250/250 [==============================] - 265s 1s/step - loss: 0.9181 - val_loss: 0.6808 - accuracy: 0.9690\n-Epoch 3/5\n-250/250 [==============================] - 252s 1s/step - loss: 0.3910 - val_loss: 0.4303 - accuracy: 0.9820\n-Epoch 4/5\n-250/250 [==============================] - 251s 1s/step - loss: 0.2028 - val_loss: 0.3191 - accuracy: 0.9900\n-Epoch 5/5\n-250/250 [==============================] - 238s 949ms/step - loss: 0.1232 - val_loss: 0.3259 - accuracy: 0.9890\n-```\n-\n-Congratulations! You have fine-tuned your model and shared it on the ðŸ¤— Hub. You can now use it for inference!\n-</tf>\n-</frameworkcontent>\n \n \n <Tip>\n@@ -478,8 +276,6 @@ The simplest way to try out your finetuned model for inference is to use it in a\n \n You can also manually replicate the results of the `pipeline` if you'd like:\n \n-<frameworkcontent>\n-<pt>\n Load an image processor to preprocess the image and return the `input` as PyTorch tensors:\n \n ```py\n@@ -507,36 +303,3 @@ Get the predicted label with the highest probability, and use the model's `id2la\n >>> model.config.id2label[predicted_label]\n 'beignets'\n ```\n-</pt>\n-</frameworkcontent>\n-\n-<frameworkcontent>\n-<tf>\n-Load an image processor to preprocess the image and return the `input` as TensorFlow tensors:\n-\n-```py\n->>> from transformers import AutoImageProcessor\n-\n->>> image_processor = AutoImageProcessor.from_pretrained(\"MariaK/food_classifier\")\n->>> inputs = image_processor(image, return_tensors=\"tf\")\n-```\n-\n-Pass your inputs to the model and return the logits:\n-\n-```py\n->>> from transformers import TFAutoModelForImageClassification\n-\n->>> model = TFAutoModelForImageClassification.from_pretrained(\"MariaK/food_classifier\")\n->>> logits = model(**inputs).logits\n-```\n-\n-Get the predicted label with the highest probability, and use the model's `id2label` mapping to convert it to a label:\n-\n-```py\n->>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n->>> model.config.id2label[predicted_class_id]\n-'beignets'\n-```\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "6ff73af98206d5e308d87bcf804056ca0f871406",
            "filename": "docs/source/en/tasks/language_modeling.md",
            "status": "modified",
            "additions": 1,
            "deletions": 119,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Flanguage_modeling.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Flanguage_modeling.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Flanguage_modeling.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -189,8 +189,6 @@ Apply the `group_texts` function over the entire dataset:\n Now create a batch of examples using [`DataCollatorForLanguageModeling`]. It's more efficient to *dynamically pad* the\n sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n \n-<frameworkcontent>\n-<pt>\n Use the end-of-sequence token as the padding token and set `mlm=False`. This will use the inputs as labels shifted to the right by one element:\n \n ```py\n@@ -200,24 +198,8 @@ Use the end-of-sequence token as the padding token and set `mlm=False`. This wil\n >>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n ```\n \n-</pt>\n-<tf>\n-Use the end-of-sequence token as the padding token and set `mlm=False`. This will use the inputs as labels shifted to the right by one element:\n-\n-```py\n->>> from transformers import DataCollatorForLanguageModeling\n-\n->>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"tf\")\n-```\n-\n-</tf>\n-</frameworkcontent>\n-\n-\n ## Train\n \n-<frameworkcontent>\n-<pt>\n <Tip>\n \n If you aren't familiar with finetuning a model with the [`Trainer`], take a look at the [basic tutorial](../training#train-with-pytorch-trainer)!\n@@ -274,81 +256,11 @@ Then share your model to the Hub with the [`~transformers.Trainer.push_to_hub`]\n ```py\n >>> trainer.push_to_hub()\n ```\n-</pt>\n-<tf>\n-<Tip>\n-\n-If you aren't familiar with finetuning a model with Keras, take a look at the [basic tutorial](../training#train-a-tensorflow-model-with-keras)!\n-\n-</Tip>\n-To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n-\n-```py\n->>> from transformers import create_optimizer, AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-Then you can load DistilGPT2 with [`TFAutoModelForCausalLM`]:\n-\n-```py\n->>> from transformers import TFAutoModelForCausalLM\n-\n->>> model = TFAutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n-```\n-\n-Convert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     lm_dataset[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = model.prepare_tf_dataset(\n-...     lm_dataset[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-This can be done by specifying where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_eli5_clm-model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-Finally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callback to finetune the model:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])\n-```\n-\n-Once training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n-</tf>\n-</frameworkcontent>\n \n <Tip>\n \n For a more in-depth example of how to finetune a model for causal language modeling, take a look at the corresponding\n-[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)\n-or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n+[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n \n </Tip>\n \n@@ -372,8 +284,6 @@ The simplest way to try out your finetuned model for inference is to use it in a\n [{'generated_text': \"Somatic hypermutation allows the immune system to be able to effectively reverse the damage caused by an infection.\\n\\n\\nThe damage caused by an infection is caused by the immune system's ability to perform its own self-correcting tasks.\"}]\n ```\n \n-<frameworkcontent>\n-<pt>\n Tokenize the text and return the `input_ids` as PyTorch tensors:\n \n ```py\n@@ -399,31 +309,3 @@ Decode the generated token ids back into text:\n >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n [\"Somatic hypermutation allows the immune system to react to drugs with the ability to adapt to a different environmental situation. In other words, a system of 'hypermutation' can help the immune system to adapt to a different environmental situation or in some cases even a single life. In contrast, researchers at the University of Massachusetts-Boston have found that 'hypermutation' is much stronger in mice than in humans but can be found in humans, and that it's not completely unknown to the immune system. A study on how the immune system\"]\n ```\n-</pt>\n-<tf>\n-Tokenize the text and return the `input_ids` as TensorFlow tensors:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_eli5_clm-model\")\n->>> inputs = tokenizer(prompt, return_tensors=\"tf\").input_ids\n-```\n-\n-Use the [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] method to create the summarization. For more details about the different text generation strategies and parameters for controlling generation, check out the [Text generation strategies](../generation_strategies) page.\n-\n-```py\n->>> from transformers import TFAutoModelForCausalLM\n-\n->>> model = TFAutoModelForCausalLM.from_pretrained(\"username/my_awesome_eli5_clm-model\")\n->>> outputs = model.generate(input_ids=inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n-```\n-\n-Decode the generated token ids back into text:\n-\n-```py\n->>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-['Somatic hypermutation allows the immune system to detect the presence of other viruses as they become more prevalent. Therefore, researchers have identified a high proportion of human viruses. The proportion of virus-associated viruses in our study increases with age. Therefore, we propose a simple algorithm to detect the presence of these new viruses in our samples as a sign of improved immunity. A first study based on this algorithm, which will be published in Science on Friday, aims to show that this finding could translate into the development of a better vaccine that is more effective for']\n-```\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "3c024739d738b3603fdd65393774100c9e9d55da",
            "filename": "docs/source/en/tasks/masked_language_modeling.md",
            "status": "modified",
            "additions": 2,
            "deletions": 126,
            "changes": 128,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Fmasked_language_modeling.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Fmasked_language_modeling.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmasked_language_modeling.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -151,7 +151,7 @@ This dataset contains the token sequences, but some of these are longer than the\n \n You can now use a second preprocessing function to\n - concatenate all the sequences\n-- split the concatenated sequences into shorter chunks defined by `block_size`, which should be both shorter than the maximum input length and short enough for your GPU RAM. \n+- split the concatenated sequences into shorter chunks defined by `block_size`, which should be both shorter than the maximum input length and short enough for your GPU RAM.\n \n ```py\n >>> block_size = 128\n@@ -181,9 +181,6 @@ Apply the `group_texts` function over the entire dataset:\n \n Now create a batch of examples using [`DataCollatorForLanguageModeling`]. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n \n-<frameworkcontent>\n-<pt>\n-\n Use the end-of-sequence token as the padding token and specify `mlm_probability` to randomly mask tokens each time you iterate over the data:\n \n ```py\n@@ -192,23 +189,9 @@ Use the end-of-sequence token as the padding token and specify `mlm_probability`\n >>> tokenizer.pad_token = tokenizer.eos_token\n >>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n ```\n-</pt>\n-<tf>\n-\n-Use the end-of-sequence token as the padding token and specify `mlm_probability` to randomly mask tokens each time you iterate over the data:\n-\n-```py\n->>> from transformers import DataCollatorForLanguageModeling\n-\n->>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"tf\")\n-```\n-</tf>\n-</frameworkcontent>\n \n ## Train\n \n-<frameworkcontent>\n-<pt>\n <Tip>\n \n If you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n@@ -266,81 +249,11 @@ Then share your model to the Hub with the [`~transformers.Trainer.push_to_hub`]\n ```py\n >>> trainer.push_to_hub()\n ```\n-</pt>\n-<tf>\n-<Tip>\n-\n-If you aren't familiar with finetuning a model with Keras, take a look at the basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!\n-\n-</Tip>\n-To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n-\n-```py\n->>> from transformers import create_optimizer, AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-Then you can load DistilRoBERTa with [`TFAutoModelForMaskedLM`]:\n-\n-```py\n->>> from transformers import TFAutoModelForMaskedLM\n-\n->>> model = TFAutoModelForMaskedLM.from_pretrained(\"distilbert/distilroberta-base\")\n-```\n-\n-Convert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     lm_dataset[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = model.prepare_tf_dataset(\n-...     lm_dataset[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-This can be done by specifying where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_eli5_mlm_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-Finally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callback to finetune the model:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])\n-```\n-\n-Once training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n-</tf>\n-</frameworkcontent>\n \n <Tip>\n \n For a more in-depth example of how to finetune a model for masked language modeling, take a look at the corresponding\n-[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)\n-or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n+[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n \n </Tip>\n \n@@ -375,8 +288,6 @@ The simplest way to try out your finetuned model for inference is to use it in a\n   'sequence': 'The Milky Way is a small galaxy.'}]\n ```\n \n-<frameworkcontent>\n-<pt>\n Tokenize the text and return the `input_ids` as PyTorch tensors. You'll also need to specify the position of the `<mask>` token:\n \n ```py\n@@ -408,38 +319,3 @@ The Milky Way is a spiral galaxy.\n The Milky Way is a massive galaxy.\n The Milky Way is a small galaxy.\n ```\n-</pt>\n-<tf>\n-Tokenize the text and return the `input_ids` as TensorFlow tensors. You'll also need to specify the position of the `<mask>` token:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_eli5_mlm_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\")\n->>> mask_token_index = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n-```\n-\n-Pass your inputs to the model and return the `logits` of the masked token:\n-\n-```py\n->>> from transformers import TFAutoModelForMaskedLM\n-\n->>> model = TFAutoModelForMaskedLM.from_pretrained(\"username/my_awesome_eli5_mlm_model\")\n->>> logits = model(**inputs).logits\n->>> mask_token_logits = logits[0, mask_token_index, :]\n-```\n-\n-Then return the three masked tokens with the highest probability and print them out:\n-\n-```py\n->>> top_3_tokens = tf.math.top_k(mask_token_logits, 3).indices.numpy()\n-\n->>> for token in top_3_tokens:\n-...     print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))\n-The Milky Way is a spiral galaxy.\n-The Milky Way is a massive galaxy.\n-The Milky Way is a small galaxy.\n-```\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "3f4c9d4637fb16c6a847c36a21b0728c2772ba98",
            "filename": "docs/source/en/tasks/multiple_choice.md",
            "status": "modified",
            "additions": 1,
            "deletions": 124,
            "changes": 125,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Fmultiple_choice.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Fmultiple_choice.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmultiple_choice.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -144,8 +144,6 @@ Your `compute_metrics` function is ready to go now, and you'll return to it when\n \n ## Train\n \n-<frameworkcontent>\n-<pt>\n <Tip>\n \n If you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n@@ -198,100 +196,12 @@ Once training is completed, share your model to the Hub with the [`~transformers\n ```py\n >>> trainer.push_to_hub()\n ```\n-</pt>\n-<tf>\n-<Tip>\n-\n-If you aren't familiar with finetuning a model with Keras, take a look at the basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!\n-\n-</Tip>\n-To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_train_epochs = 2\n->>> total_train_steps = (len(tokenized_swag[\"train\"]) // batch_size) * num_train_epochs\n->>> optimizer, schedule = create_optimizer(init_lr=5e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n-```\n-\n-Then you can load BERT with [`TFAutoModelForMultipleChoice`]:\n-\n-```py\n->>> from transformers import TFAutoModelForMultipleChoice\n-\n->>> model = TFAutoModelForMultipleChoice.from_pretrained(\"google-bert/bert-base-uncased\")\n-```\n-\n-Convert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n-\n-```py\n->>> data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_swag[\"train\"],\n-...     shuffle=True,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = model.prepare_tf_dataset(\n-...     tokenized_swag[\"validation\"],\n-...     shuffle=False,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:\n-\n-```py\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-The last two things to setup before you start training is to compute the accuracy from the predictions, and provide a way to push your model to the Hub. Both are done by using [Keras callbacks](../main_classes/keras_callbacks).\n-\n-Pass your `compute_metrics` function to [`~transformers.KerasMetricCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n-```\n-\n-Specify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-Then bundle your callbacks together:\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-Finally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callbacks to finetune the model:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=2, callbacks=callbacks)\n-```\n-\n-Once training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n-</tf>\n-</frameworkcontent>\n \n \n <Tip>\n \n For a more in-depth example of how to finetune a model for multiple choice, take a look at the corresponding\n-[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)\n-or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb).\n+[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb).\n \n </Tip>\n \n@@ -307,8 +217,6 @@ Come up with some text and two candidate answers:\n >>> candidate2 = \"The law applies to baguettes.\"\n ```\n \n-<frameworkcontent>\n-<pt>\n Tokenize each prompt and candidate answer pair and return PyTorch tensors. You should also create some `labels`:\n \n ```py\n@@ -336,34 +244,3 @@ Get the class with the highest probability:\n >>> predicted_class\n 0\n ```\n-</pt>\n-<tf>\n-Tokenize each prompt and candidate answer pair and return TensorFlow tensors:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_swag_model\")\n->>> inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"tf\", padding=True)\n-```\n-\n-Pass your inputs to the model and return the `logits`:\n-\n-```py\n->>> from transformers import TFAutoModelForMultipleChoice\n-\n->>> model = TFAutoModelForMultipleChoice.from_pretrained(\"username/my_awesome_swag_model\")\n->>> inputs = {k: tf.expand_dims(v, 0) for k, v in inputs.items()}\n->>> outputs = model(inputs)\n->>> logits = outputs.logits\n-```\n-\n-Get the class with the highest probability:\n-\n-```py\n->>> predicted_class = int(tf.math.argmax(logits, axis=-1)[0])\n->>> predicted_class\n-0\n-```\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "905c2aeb3ee21f39ea3e477f61753d6c2ca6fb5a",
            "filename": "docs/source/en/tasks/question_answering.md",
            "status": "modified",
            "additions": 1,
            "deletions": 128,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Fquestion_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Fquestion_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fquestion_answering.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -167,27 +167,14 @@ To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [\n \n Now create a batch of examples using [`DefaultDataCollator`]. Unlike other data collators in ðŸ¤— Transformers, the [`DefaultDataCollator`] does not apply any additional preprocessing such as padding.\n \n-<frameworkcontent>\n-<pt>\n ```py\n >>> from transformers import DefaultDataCollator\n \n >>> data_collator = DefaultDataCollator()\n ```\n-</pt>\n-<tf>\n-```py\n->>> from transformers import DefaultDataCollator\n-\n->>> data_collator = DefaultDataCollator(return_tensors=\"tf\")\n-```\n-</tf>\n-</frameworkcontent>\n \n ## Train\n \n-<frameworkcontent>\n-<pt>\n <Tip>\n \n If you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n@@ -237,87 +224,11 @@ Once training is completed, share your model to the Hub with the [`~transformers\n ```py\n >>> trainer.push_to_hub()\n ```\n-</pt>\n-<tf>\n-<Tip>\n-\n-If you aren't familiar with finetuning a model with Keras, take a look at the basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!\n-\n-</Tip>\n-To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_epochs = 2\n->>> total_train_steps = (len(tokenized_squad[\"train\"]) // batch_size) * num_epochs\n->>> optimizer, schedule = create_optimizer(\n-...     init_lr=2e-5,\n-...     num_warmup_steps=0,\n-...     num_train_steps=total_train_steps,\n-... )\n-```\n-\n-Then you can load DistilBERT with [`TFAutoModelForQuestionAnswering`]:\n-\n-```py\n->>> from transformers import TFAutoModelForQuestionAnswering\n-\n->>> model = TFAutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Convert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_squad[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = model.prepare_tf_dataset(\n-...     tokenized_squad[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method):\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)\n-```\n-\n-The last thing to setup before you start training is to provide a way to push your model to the Hub. This can be done by specifying where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_qa_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-Finally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callback to finetune the model:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=[callback])\n-```\n-Once training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n-</tf>\n-</frameworkcontent>\n \n <Tip>\n \n For a more in-depth example of how to finetune a model for question answering, take a look at the corresponding\n-[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)\n-or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).\n+[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb).\n \n </Tip>\n \n@@ -353,8 +264,6 @@ The simplest way to try out your finetuned model for inference is to use it in a\n \n You can also manually replicate the results of the `pipeline` if you'd like:\n \n-<frameworkcontent>\n-<pt>\n Tokenize the text and return PyTorch tensors:\n \n ```py\n@@ -389,39 +298,3 @@ Decode the predicted tokens to get the answer:\n >>> tokenizer.decode(predict_answer_tokens)\n '176 billion parameters and can generate text in 46 languages natural languages and 13'\n ```\n-</pt>\n-<tf>\n-Tokenize the text and return TensorFlow tensors:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_qa_model\")\n->>> inputs = tokenizer(question, context, return_tensors=\"tf\")\n-```\n-\n-Pass your inputs to the model and return the `logits`:\n-\n-```py\n->>> from transformers import TFAutoModelForQuestionAnswering\n-\n->>> model = TFAutoModelForQuestionAnswering.from_pretrained(\"my_awesome_qa_model\")\n->>> outputs = model(**inputs)\n-```\n-\n-Get the highest probability from the model output for the start and end positions:\n-\n-```py\n->>> answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n->>> answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n-```\n-\n-Decode the predicted tokens to get the answer:\n-\n-```py\n->>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n->>> tokenizer.decode(predict_answer_tokens)\n-'176 billion parameters and can generate text in 46 languages natural languages and 13'\n-```\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "5d3c8e70aa1ff5417dee48b774b83e8511e293a1",
            "filename": "docs/source/en/tasks/semantic_segmentation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 251,
            "changes": 252,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -319,9 +319,6 @@ The next step is to load a SegFormer image processor to prepare the images and a\n >>> image_processor = AutoImageProcessor.from_pretrained(checkpoint, do_reduce_labels=True)\n ```\n \n-<frameworkcontent>\n-<pt>\n-\n It is common to apply some data augmentations to an image dataset to make a model more robust against overfitting. In this guide, you'll use the [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html) function from [torchvision](https://pytorch.org/vision/stable/index.html) to randomly change the color properties of an image, but you can also use any image library you like.\n \n ```py\n@@ -354,67 +351,6 @@ To apply the `jitter` over the entire dataset, use the ðŸ¤— Datasets [`~datasets\n >>> test_ds.set_transform(val_transforms)\n ```\n \n-</pt>\n-</frameworkcontent>\n-\n-<frameworkcontent>\n-<tf>\n-It is common to apply some data augmentations to an image dataset to make a model more robust against overfitting.\n-In this guide, you'll use [`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image) to randomly change the color properties of an image, but you can also use any image\n-library you like.\n-Define two separate transformation functions:\n-- training data transformations that include image augmentation\n-- validation data transformations that only transpose the images, since computer vision models in ðŸ¤— Transformers expect channels-first layout\n-\n-```py\n->>> import tensorflow as tf\n-\n-\n->>> def aug_transforms(image):\n-...     image = tf.keras.utils.img_to_array(image)\n-...     image = tf.image.random_brightness(image, 0.25)\n-...     image = tf.image.random_contrast(image, 0.5, 2.0)\n-...     image = tf.image.random_saturation(image, 0.75, 1.25)\n-...     image = tf.image.random_hue(image, 0.1)\n-...     image = tf.transpose(image, (2, 0, 1))\n-...     return image\n-\n-\n->>> def transforms(image):\n-...     image = tf.keras.utils.img_to_array(image)\n-...     image = tf.transpose(image, (2, 0, 1))\n-...     return image\n-```\n-\n-Next, create two preprocessing functions to prepare batches of images and annotations for the model. These functions apply\n-the image transformations and use the earlier loaded `image_processor` to convert the images into `pixel_values` and\n-annotations to `labels`. `ImageProcessor` also takes care of resizing and normalizing the images.\n-\n-```py\n->>> def train_transforms(example_batch):\n-...     images = [aug_transforms(x.convert(\"RGB\")) for x in example_batch[\"image\"]]\n-...     labels = [x for x in example_batch[\"annotation\"]]\n-...     inputs = image_processor(images, labels)\n-...     return inputs\n-\n-\n->>> def val_transforms(example_batch):\n-...     images = [transforms(x.convert(\"RGB\")) for x in example_batch[\"image\"]]\n-...     labels = [x for x in example_batch[\"annotation\"]]\n-...     inputs = image_processor(images, labels)\n-...     return inputs\n-```\n-\n-To apply the preprocessing transformations over the entire dataset, use the ðŸ¤— Datasets [`~datasets.Dataset.set_transform`] function.\n-The transform is applied on the fly which is faster and consumes less disk space:\n-\n-```py\n->>> train_ds.set_transform(train_transforms)\n->>> test_ds.set_transform(val_transforms)\n-```\n-</tf>\n-</frameworkcontent>\n-\n ### Evaluate\n \n Including a metric during training is often helpful for evaluating your model's performance. You can quickly load an evaluation method with the ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [mean Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy) (IoU) metric (see the ðŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):\n@@ -428,9 +364,6 @@ Including a metric during training is often helpful for evaluating your model's\n Then create a function to [`~evaluate.EvaluationModule.compute`] the metrics. Your predictions need to be converted to\n logits first, and then reshaped to match the size of the labels before you can call [`~evaluate.EvaluationModule.compute`]:\n \n-<frameworkcontent>\n-<pt>\n-\n ```py\n >>> import numpy as np\n >>> import torch\n@@ -461,48 +394,10 @@ logits first, and then reshaped to match the size of the labels before you can c\n ...         return metrics\n ```\n \n-</pt>\n-</frameworkcontent>\n-\n-\n-<frameworkcontent>\n-<tf>\n-\n-```py\n->>> def compute_metrics(eval_pred):\n-...     logits, labels = eval_pred\n-...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])\n-...     logits_resized = tf.image.resize(\n-...         logits,\n-...         size=tf.shape(labels)[1:],\n-...         method=\"bilinear\",\n-...     )\n-\n-...     pred_labels = tf.argmax(logits_resized, axis=-1)\n-...     metrics = metric.compute(\n-...         predictions=pred_labels,\n-...         references=labels,\n-...         num_labels=num_labels,\n-...         ignore_index=-1,\n-...         reduce_labels=image_processor.do_reduce_labels,\n-...     )\n-\n-...     per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n-...     per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n-\n-...     metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n-...     metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n-...     return {\"val_\" + k: v for k, v in metrics.items()}\n-```\n-\n-</tf>\n-</frameworkcontent>\n-\n Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training.\n \n ### Train\n-<frameworkcontent>\n-<pt>\n+\n <Tip>\n \n If you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#finetune-with-trainer)!\n@@ -557,111 +452,6 @@ Once training is completed, share your model to the Hub with the [`~transformers\n ```py\n >>> trainer.push_to_hub()\n ```\n-</pt>\n-</frameworkcontent>\n-\n-<frameworkcontent>\n-<tf>\n-<Tip>\n-\n-If you are unfamiliar with fine-tuning a model with Keras, check out the [basic tutorial](./training#train-a-tensorflow-model-with-keras) first!\n-\n-</Tip>\n-\n-To fine-tune a model in TensorFlow, follow these steps:\n-1. Define the training hyperparameters, and set up an optimizer and a learning rate schedule.\n-2. Instantiate a pretrained model.\n-3. Convert a ðŸ¤— Dataset to a `tf.data.Dataset`.\n-4. Compile your model.\n-5. Add callbacks to calculate metrics and upload your model to ðŸ¤— Hub\n-6. Use the `fit()` method to run the training.\n-\n-Start by defining the hyperparameters, optimizer and learning rate schedule:\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 2\n->>> num_epochs = 50\n->>> num_train_steps = len(train_ds) * num_epochs\n->>> learning_rate = 6e-5\n->>> weight_decay_rate = 0.01\n-\n->>> optimizer, lr_schedule = create_optimizer(\n-...     init_lr=learning_rate,\n-...     num_train_steps=num_train_steps,\n-...     weight_decay_rate=weight_decay_rate,\n-...     num_warmup_steps=0,\n-... )\n-```\n-\n-Then, load SegFormer with [`TFAutoModelForSemanticSegmentation`] along with the label mappings, and compile it with the\n-optimizer. Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:\n-\n-```py\n->>> from transformers import TFAutoModelForSemanticSegmentation\n-\n->>> model = TFAutoModelForSemanticSegmentation.from_pretrained(\n-...     checkpoint,\n-...     id2label=id2label,\n-...     label2id=label2id,\n-... )\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-Convert your datasets to the `tf.data.Dataset` format using the [`~datasets.Dataset.to_tf_dataset`] and the [`DefaultDataCollator`]:\n-\n-```py\n->>> from transformers import DefaultDataCollator\n-\n->>> data_collator = DefaultDataCollator(return_tensors=\"tf\")\n-\n->>> tf_train_dataset = train_ds.to_tf_dataset(\n-...     columns=[\"pixel_values\", \"label\"],\n-...     shuffle=True,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_eval_dataset = test_ds.to_tf_dataset(\n-...     columns=[\"pixel_values\", \"label\"],\n-...     shuffle=True,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-To compute the accuracy from the predictions and push your model to the ðŸ¤— Hub, use [Keras callbacks](../main_classes/keras_callbacks).\n-Pass your `compute_metrics` function to [`KerasMetricCallback`],\n-and use the [`PushToHubCallback`] to upload the model:\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n-\n->>> metric_callback = KerasMetricCallback(\n-...     metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=[\"labels\"]\n-... )\n-\n->>> push_to_hub_callback = PushToHubCallback(output_dir=\"scene_segmentation\", tokenizer=image_processor)\n-\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-Finally, you are ready to train your model! Call `fit()` with your training and validation datasets, the number of epochs,\n-and your callbacks to fine-tune the model:\n-\n-```py\n->>> model.fit(\n-...     tf_train_dataset,\n-...     validation_data=tf_eval_dataset,\n-...     callbacks=callbacks,\n-...     epochs=num_epochs,\n-... )\n-```\n-\n-Congratulations! You have fine-tuned your model and shared it on the ðŸ¤— Hub. You can now use it for inference!\n-</tf>\n-</frameworkcontent>\n \n ### Inference\n \n@@ -683,8 +473,6 @@ Reload the dataset and load an image for inference.\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png\" alt=\"Image of bedroom\"/>\n </div>\n \n-<frameworkcontent>\n-<pt>\n \n We will now see how to infer without a pipeline. Process the image with an image processor and place the `pixel_values` on a GPU:\n \n@@ -715,44 +503,6 @@ Next, rescale the logits to the original image size:\n >>> pred_seg = upsampled_logits.argmax(dim=1)[0]\n ```\n \n-</pt>\n-</frameworkcontent>\n-\n-<frameworkcontent>\n-<tf>\n-Load an image processor to preprocess the image and return the input as TensorFlow tensors:\n-\n-```py\n->>> from transformers import AutoImageProcessor\n-\n->>> image_processor = AutoImageProcessor.from_pretrained(\"MariaK/scene_segmentation\")\n->>> inputs = image_processor(image, return_tensors=\"tf\")\n-```\n-\n-Pass your input to the model and return the `logits`:\n-\n-```py\n->>> from transformers import TFAutoModelForSemanticSegmentation\n-\n->>> model = TFAutoModelForSemanticSegmentation.from_pretrained(\"MariaK/scene_segmentation\")\n->>> logits = model(**inputs).logits\n-```\n-\n-Next, rescale the logits to the original image size and apply argmax on the class dimension:\n-```py\n->>> logits = tf.transpose(logits, [0, 2, 3, 1])\n-\n->>> upsampled_logits = tf.image.resize(\n-...     logits,\n-...     # We reverse the shape of `image` because `image.size` returns width and height.\n-...     image.size[::-1],\n-... )\n-\n->>> pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]\n-```\n-\n-</tf>\n-</frameworkcontent>\n \n To visualize the results, load the [dataset color palette](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51) as `ade_palette()` that maps each class to their RGB values.\n "
        },
        {
            "sha": "686871a56229da349bcde3cb5b5c751bce9fd571",
            "filename": "docs/source/en/tasks/sequence_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 138,
            "changes": 139,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Fsequence_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Fsequence_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fsequence_classification.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -97,22 +97,11 @@ tokenized_imdb = imdb.map(preprocess_function, batched=True)\n \n Now create a batch of examples using [`DataCollatorWithPadding`]. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n \n-<frameworkcontent>\n-<pt>\n ```py\n >>> from transformers import DataCollatorWithPadding\n \n >>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n ```\n-</pt>\n-<tf>\n-```py\n->>> from transformers import DataCollatorWithPadding\n-\n->>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n-```\n-</tf>\n-</frameworkcontent>\n \n ## Evaluate\n \n@@ -147,8 +136,6 @@ Before you start training your model, create a map of the expected ids to their\n >>> label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n ```\n \n-<frameworkcontent>\n-<pt>\n <Tip>\n \n If you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n@@ -209,104 +196,11 @@ Once training is completed, share your model to the Hub with the [`~transformers\n ```py\n >>> trainer.push_to_hub()\n ```\n-</pt>\n-<tf>\n-<Tip>\n-\n-If you aren't familiar with finetuning a model with Keras, take a look at the basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!\n-\n-</Tip>\n-To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n-\n-```py\n->>> from transformers import create_optimizer\n->>> import tensorflow as tf\n-\n->>> batch_size = 16\n->>> num_epochs = 5\n->>> batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n->>> total_train_steps = int(batches_per_epoch * num_epochs)\n->>> optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n-```\n-\n-Then you can load DistilBERT with [`TFAutoModelForSequenceClassification`] along with the number of expected labels, and the label mappings:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(\n-...     \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n-... )\n-```\n-\n-Convert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_imdb[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = model.prepare_tf_dataset(\n-...     tokenized_imdb[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-The last two things to setup before you start training is to compute the accuracy from the predictions, and provide a way to push your model to the Hub. Both are done by using [Keras callbacks](../main_classes/keras_callbacks).\n-\n-Pass your `compute_metrics` function to [`~transformers.KerasMetricCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n-```\n-\n-Specify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-Then bundle your callbacks together:\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-Finally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callbacks to finetune the model:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)\n-```\n-\n-Once training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n-</tf>\n-</frameworkcontent>\n \n <Tip>\n \n For a more in-depth example of how to finetune a model for text classification, take a look at the corresponding\n-[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)\n-or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb).\n+[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb).\n \n </Tip>\n \n@@ -332,8 +226,6 @@ The simplest way to try out your finetuned model for inference is to use it in a\n \n You can also manually replicate the results of the `pipeline` if you'd like:\n \n-<frameworkcontent>\n-<pt>\n Tokenize the text and return PyTorch tensors:\n \n ```py\n@@ -360,32 +252,3 @@ Get the class with the highest probability, and use the model's `id2label` mappi\n >>> model.config.id2label[predicted_class_id]\n 'POSITIVE'\n ```\n-</pt>\n-<tf>\n-Tokenize the text and return TensorFlow tensors:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\")\n-```\n-\n-Pass your inputs to the model and return the `logits`:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(\"stevhliu/my_awesome_model\")\n->>> logits = model(**inputs).logits\n-```\n-\n-Get the class with the highest probability, and use the model's `id2label` mapping to convert it to a text label:\n-\n-```py\n->>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n->>> model.config.id2label[predicted_class_id]\n-'POSITIVE'\n-```\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "c57097421fbcc5a5198ef983ff901c42e4807fc6",
            "filename": "docs/source/en/tasks/summarization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 132,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Fsummarization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Fsummarization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fsummarization.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -119,24 +119,11 @@ To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [\n \n Now create a batch of examples using [`DataCollatorForSeq2Seq`]. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n \n-<frameworkcontent>\n-<pt>\n-\n ```py\n >>> from transformers import DataCollatorForSeq2Seq\n \n >>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n ```\n-</pt>\n-<tf>\n-\n-```py\n->>> from transformers import DataCollatorForSeq2Seq\n-\n->>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")\n-```\n-</tf>\n-</frameworkcontent>\n \n ## Evaluate\n \n@@ -172,8 +159,6 @@ Your `compute_metrics` function is ready to go now, and you'll return to it when\n \n ## Train\n \n-<frameworkcontent>\n-<pt>\n <Tip>\n \n If you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n@@ -227,97 +212,12 @@ Once training is completed, share your model to the Hub with the [`~transformers\n ```py\n >>> trainer.push_to_hub()\n ```\n-</pt>\n-<tf>\n-<Tip>\n-\n-If you aren't familiar with finetuning a model with Keras, take a look at the basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!\n-\n-</Tip>\n-To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n-\n-```py\n->>> from transformers import create_optimizer, AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-Then you can load T5 with [`TFAutoModelForSeq2SeqLM`]:\n-\n-```py\n->>> from transformers import TFAutoModelForSeq2SeqLM\n-\n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n-```\n-\n-Convert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_billsum[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = model.prepare_tf_dataset(\n-...     tokenized_billsum[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-The last two things to setup before you start training is to compute the ROUGE score from the predictions, and provide a way to push your model to the Hub. Both are done by using [Keras callbacks](../main_classes/keras_callbacks).\n-\n-Pass your `compute_metrics` function to [`~transformers.KerasMetricCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_test_set)\n-```\n-\n-Specify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_billsum_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-Then bundle your callbacks together:\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n \n-Finally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callbacks to finetune the model:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)\n-```\n-\n-Once training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n-</tf>\n-</frameworkcontent>\n \n <Tip>\n \n For a more in-depth example of how to finetune a model for summarization, take a look at the corresponding\n-[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)\n-or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb).\n+[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb).\n \n </Tip>\n \n@@ -343,9 +243,6 @@ The simplest way to try out your finetuned model for inference is to use it in a\n \n You can also manually replicate the results of the `pipeline` if you'd like:\n \n-\n-<frameworkcontent>\n-<pt>\n Tokenize the text and return the `input_ids` as PyTorch tensors:\n \n ```py\n@@ -370,31 +267,3 @@ Decode the generated token ids back into text:\n >>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n 'the inflation reduction act lowers prescription drug costs, health care costs, and energy costs. it's the most aggressive action on tackling the climate crisis in american history. it will ask the ultra-wealthy and corporations to pay their fair share.'\n ```\n-</pt>\n-<tf>\n-Tokenize the text and return the `input_ids` as TensorFlow tensors:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_billsum_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\").input_ids\n-```\n-\n-Use the [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] method to create the summarization. For more details about the different text generation strategies and parameters for controlling generation, check out the [Text Generation](../main_classes/text_generation) API.\n-\n-```py\n->>> from transformers import TFAutoModelForSeq2SeqLM\n-\n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(\"username/my_awesome_billsum_model\")\n->>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n-```\n-\n-Decode the generated token ids back into text:\n-\n-```py\n->>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n-'the inflation reduction act lowers prescription drug costs, health care costs, and energy costs. it's the most aggressive action on tackling the climate crisis in american history. it will ask the ultra-wealthy and corporations to pay their fair share.'\n-```\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "49b0fcf216b8c60e12bfc862abd7ba9540b9bb76",
            "filename": "docs/source/en/tasks/token_classification.md",
            "status": "modified",
            "additions": 2,
            "deletions": 157,
            "changes": 159,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Ftoken_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Ftoken_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Ftoken_classification.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -157,22 +157,11 @@ To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [\n \n Now create a batch of examples using [`DataCollatorWithPadding`]. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n \n-<frameworkcontent>\n-<pt>\n ```py\n >>> from transformers import DataCollatorForTokenClassification\n \n >>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n ```\n-</pt>\n-<tf>\n-```py\n->>> from transformers import DataCollatorForTokenClassification\n-\n->>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")\n-```\n-</tf>\n-</frameworkcontent>\n \n ## Evaluate\n \n@@ -253,8 +242,7 @@ Before you start training your model, create a map of the expected ids to their\n ... }\n ```\n \n-<frameworkcontent>\n-<pt>\n+\n <Tip>\n \n If you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n@@ -309,107 +297,12 @@ Once training is completed, share your model to the Hub with the [`~transformers\n ```py\n >>> trainer.push_to_hub()\n ```\n-</pt>\n-<tf>\n-<Tip>\n-\n-If you aren't familiar with finetuning a model with Keras, take a look at the basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!\n-\n-</Tip>\n-To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_train_epochs = 3\n->>> num_train_steps = (len(tokenized_wnut[\"train\"]) // batch_size) * num_train_epochs\n->>> optimizer, lr_schedule = create_optimizer(\n-...     init_lr=2e-5,\n-...     num_train_steps=num_train_steps,\n-...     weight_decay_rate=0.01,\n-...     num_warmup_steps=0,\n-... )\n-```\n-\n-Then you can load DistilBERT with [`TFAutoModelForTokenClassification`] along with the number of expected labels, and the label mappings:\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\n-...     \"distilbert/distilbert-base-uncased\", num_labels=13, id2label=id2label, label2id=label2id\n-... )\n-```\n-\n-Convert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_wnut[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = model.prepare_tf_dataset(\n-...     tokenized_wnut[\"validation\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-The last two things to setup before you start training is to compute the seqeval scores from the predictions, and provide a way to push your model to the Hub. Both are done by using [Keras callbacks](../main_classes/keras_callbacks).\n-\n-Pass your `compute_metrics` function to [`~transformers.KerasMetricCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n-```\n-\n-Specify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_wnut_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-Then bundle your callbacks together:\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n \n-Finally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callbacks to finetune the model:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)\n-```\n-\n-Once training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n-</tf>\n-</frameworkcontent>\n \n <Tip>\n \n For a more in-depth example of how to finetune a model for token classification, take a look at the corresponding\n-[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)\n-or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb).\n+[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb).\n \n </Tip>\n \n@@ -464,8 +357,6 @@ The simplest way to try out your finetuned model for inference is to use it in a\n \n You can also manually replicate the results of the `pipeline` if you'd like:\n \n-<frameworkcontent>\n-<pt>\n Tokenize the text and return PyTorch tensors:\n \n ```py\n@@ -509,49 +400,3 @@ Get the class with the highest probability, and use the model's `id2label` mappi\n  'O',\n  'O']\n ```\n-</pt>\n-<tf>\n-Tokenize the text and return TensorFlow tensors:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_wnut_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\")\n-```\n-\n-Pass your inputs to the model and return the `logits`:\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\"stevhliu/my_awesome_wnut_model\")\n->>> logits = model(**inputs).logits\n-```\n-\n-Get the class with the highest probability, and use the model's `id2label` mapping to convert it to a text label:\n-\n-```py\n->>> predicted_token_class_ids = tf.math.argmax(logits, axis=-1)\n->>> predicted_token_class = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]\n->>> predicted_token_class\n-['O',\n- 'O',\n- 'B-location',\n- 'I-location',\n- 'B-group',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'B-location',\n- 'B-location',\n- 'O',\n- 'O']\n-```\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "69246a8c17a9be4dd303246bea1c46267abcb0e9",
            "filename": "docs/source/en/tasks/translation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 131,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Ftranslation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/docs%2Fsource%2Fen%2Ftasks%2Ftranslation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Ftranslation.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -114,23 +114,11 @@ To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [\n \n Now create a batch of examples using [`DataCollatorForSeq2Seq`]. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n \n-<frameworkcontent>\n-<pt>\n ```py\n >>> from transformers import DataCollatorForSeq2Seq\n \n >>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n ```\n-</pt>\n-<tf>\n-\n-```py\n->>> from transformers import DataCollatorForSeq2Seq\n-\n->>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")\n-```\n-</tf>\n-</frameworkcontent>\n \n ## Evaluate\n \n@@ -179,8 +167,6 @@ Your `compute_metrics` function is ready to go now, and you'll return to it when\n \n ## Train\n \n-<frameworkcontent>\n-<pt>\n <Tip>\n \n If you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n@@ -234,97 +220,11 @@ Once training is completed, share your model to the Hub with the [`~transformers\n ```py\n >>> trainer.push_to_hub()\n ```\n-</pt>\n-<tf>\n-<Tip>\n-\n-If you aren't familiar with finetuning a model with Keras, take a look at the basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!\n-\n-</Tip>\n-To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n-\n-```py\n->>> from transformers import AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-Then you can load T5 with [`TFAutoModelForSeq2SeqLM`]:\n-\n-```py\n->>> from transformers import TFAutoModelForSeq2SeqLM\n-\n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n-```\n-\n-Convert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_books[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = model.prepare_tf_dataset(\n-...     tokenized_books[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-The last two things to setup before you start training is to compute the SacreBLEU metric from the predictions, and provide a way to push your model to the Hub. Both are done by using [Keras callbacks](../main_classes/keras_callbacks).\n-\n-Pass your `compute_metrics` function to [`~transformers.KerasMetricCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_test_set)\n-```\n-\n-Specify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_opus_books_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-Then bundle your callbacks together:\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-Finally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callbacks to finetune the model:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)\n-```\n-\n-Once training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n-</tf>\n-</frameworkcontent>\n \n <Tip>\n \n For a more in-depth example of how to finetune a model for translation, take a look at the corresponding\n-[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)\n-or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb).\n+[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb).\n \n </Tip>\n \n@@ -353,8 +253,6 @@ The simplest way to try out your finetuned model for inference is to use it in a\n \n You can also manually replicate the results of the `pipeline` if you'd like:\n \n-<frameworkcontent>\n-<pt>\n Tokenize the text and return the `input_ids` as PyTorch tensors:\n \n ```py\n@@ -379,31 +277,3 @@ Decode the generated token ids back into text:\n >>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n 'Les lignÃ©es partagent des ressources avec des bactÃ©ries enfixant l'azote.'\n ```\n-</pt>\n-<tf>\n-Tokenize the text and return the `input_ids` as TensorFlow tensors:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_opus_books_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\").input_ids\n-```\n-\n-Use the [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] method to create the translation. For more details about the different text generation strategies and parameters for controlling generation, check out the [Text Generation](../main_classes/text_generation) API.\n-\n-```py\n->>> from transformers import TFAutoModelForSeq2SeqLM\n-\n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(\"username/my_awesome_opus_books_model\")\n->>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n-```\n-\n-Decode the generated token ids back into text:\n-\n-```py\n->>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n-'Les lugumes partagent les ressources avec des bactÃ©ries fixatrices d'azote.'\n-```\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "1401e24fcf62dafffc68100093377050f2fdcc21",
            "filename": "notebooks/README.md",
            "status": "modified",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/notebooks%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/notebooks%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/notebooks%2FREADME.md?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -102,40 +102,6 @@ You can open any page of the documentation as a notebook in Colab (there is a bu\n |:----------|:-------------|:-------------|------:|\n | [How to export model to ONNX](https://github.com/huggingface/notebooks/blob/main/examples/onnx-export.ipynb)| Highlight how to export and run inference workloads through ONNX | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/onnx-export.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/onnx-export.ipynb)|\n \n-### TensorFlow Examples\n-\n-#### Natural Language Processing[[tensorflow-nlp]]\n-\n-| Notebook     |      Description      |   |   |\n-|:----------|:-------------|:-------------|------:|\n-| [Train your tokenizer](https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)  | How to train and use your very own tokenizer  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)|\n-| [Train your language model](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)   | How to easily start using transformers  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)|\n-| [How to fine-tune a model on text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)|\n-| [How to fine-tune a model on language modeling](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a causal or masked LM task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)|\n-| [How to fine-tune a model on token classification](https://github.com/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a token classification task (NER, PoS). | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)|\n-| [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SQUAD. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)|\n-| [How to fine-tune a model on multiple choice](https://github.com/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SWAG. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)|\n-| [How to fine-tune a model on translation](https://github.com/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on WMT. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)|\n-| [How to fine-tune a model on summarization](https://github.com/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on XSUM. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)|\n-\n-#### Computer Vision[[tensorflow-cv]]\n-\n-| Notebook                                                                                                                                                 | Description                                                                                         |   |   |\n-|:---------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------|:-------------|------:|\n-| [How to fine-tune a model on image classification](https://github.com/huggingface/notebooks/blob/main/examples/image_classification-tf.ipynb)            | Show how to preprocess the data and fine-tune any pretrained Vision model on Image Classification   | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification-tf.ipynb)|\n-| [How to fine-tune a SegFormer model on semantic segmentation](https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb) | Show how to preprocess the data and fine-tune a pretrained SegFormer model on Semantic Segmentation | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb)|\n-\n-#### Biological Sequences[[tensorflow-bio]]\n-\n-| Notebook     |      Description      |   |   |\n-|:----------|:-------------|:-------------|------:|\n-| [How to fine-tune a pre-trained protein model](https://github.com/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb) | See how to tokenize proteins and fine-tune a large pre-trained protein \"language\" model | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb) |\n-\n-#### Utility notebooks[[tensorflow-utility]]\n-\n-| Notebook     |      Description      |   |                                                                                                                                                                                      |\n-|:----------|:-------------|:-------------|------:|\n-| [How to train TF/Keras models on TPU](https://github.com/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) | See how to train at high speed on Google's TPU hardware | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) |\n \n ### Optimum notebooks\n "
        },
        {
            "sha": "8e05eae99ae013e6bcbe7ccd1ff936ed406055d5",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -54,6 +54,7 @@\n     is_torch_xpu_available,\n     logging,\n )\n+from ..utils.deprecation import deprecate_kwarg\n \n \n GenericTensor = Union[list[\"GenericTensor\"], \"torch.Tensor\", \"tf.Tensor\"]\n@@ -1549,6 +1550,7 @@ def check_task(self, task: str) -> tuple[str, dict, Any]:\n             f\"Unknown task {task}, available tasks are {self.get_supported_tasks() + ['translation_XX_to_YY']}\"\n         )\n \n+    @deprecate_kwarg(old_name=\"tf_model\", version=\"5.0.0\")\n     def register_pipeline(\n         self,\n         task: str,"
        },
        {
            "sha": "259b3fed21b0235ec3238bff6aaf3eed6063bb19",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 14,
            "deletions": 145,
            "changes": 159,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -75,10 +75,10 @@\n # Args that are always overridden in the docstring, for clarity we don't want to remove them from the docstring\n ALWAYS_OVERRIDE = [\"labels\"]\n \n-# This is a temporary list of objects to ignore while we progressively fix them. Do not add anything here, fix the\n+# This is a temporary set of objects to ignore while we progressively fix them. Do not add anything here, fix the\n # docstrings instead. If formatting should be ignored for the docstring, you can put a comment # no-format on the\n # line before the docstring.\n-OBJECTS_TO_IGNORE = [\n+OBJECTS_TO_IGNORE = {\n     \"Mxfp4Config\",\n     \"Exaone4Config\",\n     \"SmolLM3Config\",\n@@ -187,128 +187,6 @@\n     \"ErnieMTokenizer\",\n     \"EsmConfig\",\n     \"EsmModel\",\n-    \"FlaxAlbertForMaskedLM\",\n-    \"FlaxAlbertForMultipleChoice\",\n-    \"FlaxAlbertForPreTraining\",\n-    \"FlaxAlbertForQuestionAnswering\",\n-    \"FlaxAlbertForSequenceClassification\",\n-    \"FlaxAlbertForTokenClassification\",\n-    \"FlaxAlbertModel\",\n-    \"FlaxBartForCausalLM\",\n-    \"FlaxBartForConditionalGeneration\",\n-    \"FlaxBartForQuestionAnswering\",\n-    \"FlaxBartForSequenceClassification\",\n-    \"FlaxBartModel\",\n-    \"FlaxBeitForImageClassification\",\n-    \"FlaxBeitForMaskedImageModeling\",\n-    \"FlaxBeitModel\",\n-    \"FlaxBertForCausalLM\",\n-    \"FlaxBertForMaskedLM\",\n-    \"FlaxBertForMultipleChoice\",\n-    \"FlaxBertForNextSentencePrediction\",\n-    \"FlaxBertForPreTraining\",\n-    \"FlaxBertForQuestionAnswering\",\n-    \"FlaxBertForSequenceClassification\",\n-    \"FlaxBertForTokenClassification\",\n-    \"FlaxBertModel\",\n-    \"FlaxBigBirdForCausalLM\",\n-    \"FlaxBigBirdForMaskedLM\",\n-    \"FlaxBigBirdForMultipleChoice\",\n-    \"FlaxBigBirdForPreTraining\",\n-    \"FlaxBigBirdForQuestionAnswering\",\n-    \"FlaxBigBirdForSequenceClassification\",\n-    \"FlaxBigBirdForTokenClassification\",\n-    \"FlaxBigBirdModel\",\n-    \"FlaxBlenderbotForConditionalGeneration\",\n-    \"FlaxBlenderbotModel\",\n-    \"FlaxBlenderbotSmallForConditionalGeneration\",\n-    \"FlaxBlenderbotSmallModel\",\n-    \"FlaxBloomForCausalLM\",\n-    \"FlaxBloomModel\",\n-    \"FlaxCLIPModel\",\n-    \"FlaxDinov2ForImageClassification\",\n-    \"FlaxDinov2Model\",\n-    \"FlaxDistilBertForMaskedLM\",\n-    \"FlaxDistilBertForMultipleChoice\",\n-    \"FlaxDistilBertForQuestionAnswering\",\n-    \"FlaxDistilBertForSequenceClassification\",\n-    \"FlaxDistilBertForTokenClassification\",\n-    \"FlaxDistilBertModel\",\n-    \"FlaxElectraForCausalLM\",\n-    \"FlaxElectraForMaskedLM\",\n-    \"FlaxElectraForMultipleChoice\",\n-    \"FlaxElectraForPreTraining\",\n-    \"FlaxElectraForQuestionAnswering\",\n-    \"FlaxElectraForSequenceClassification\",\n-    \"FlaxElectraForTokenClassification\",\n-    \"FlaxElectraModel\",\n-    \"FlaxEncoderDecoderModel\",\n-    \"FlaxGPT2LMHeadModel\",\n-    \"FlaxGPT2Model\",\n-    \"FlaxGPTJForCausalLM\",\n-    \"FlaxGPTJModel\",\n-    \"FlaxGPTNeoForCausalLM\",\n-    \"FlaxGPTNeoModel\",\n-    \"FlaxLlamaForCausalLM\",\n-    \"FlaxLlamaModel\",\n-    \"FlaxGemmaForCausalLM\",\n-    \"FlaxGemmaModel\",\n-    \"FlaxMBartForConditionalGeneration\",\n-    \"FlaxMBartForQuestionAnswering\",\n-    \"FlaxMBartForSequenceClassification\",\n-    \"FlaxMBartModel\",\n-    \"FlaxMarianMTModel\",\n-    \"FlaxMarianModel\",\n-    \"FlaxMistralForCausalLM\",\n-    \"FlaxMistralModel\",\n-    \"FlaxOPTForCausalLM\",\n-    \"FlaxPegasusForConditionalGeneration\",\n-    \"FlaxPegasusModel\",\n-    \"FlaxRegNetForImageClassification\",\n-    \"FlaxRegNetModel\",\n-    \"FlaxResNetForImageClassification\",\n-    \"FlaxResNetModel\",\n-    \"FlaxRoFormerForMaskedLM\",\n-    \"FlaxRoFormerForMultipleChoice\",\n-    \"FlaxRoFormerForQuestionAnswering\",\n-    \"FlaxRoFormerForSequenceClassification\",\n-    \"FlaxRoFormerForTokenClassification\",\n-    \"FlaxRoFormerModel\",\n-    \"FlaxRobertaForCausalLM\",\n-    \"FlaxRobertaForMaskedLM\",\n-    \"FlaxRobertaForMultipleChoice\",\n-    \"FlaxRobertaForQuestionAnswering\",\n-    \"FlaxRobertaForSequenceClassification\",\n-    \"FlaxRobertaForTokenClassification\",\n-    \"FlaxRobertaModel\",\n-    \"FlaxRobertaPreLayerNormForCausalLM\",\n-    \"FlaxRobertaPreLayerNormForMaskedLM\",\n-    \"FlaxRobertaPreLayerNormForMultipleChoice\",\n-    \"FlaxRobertaPreLayerNormForQuestionAnswering\",\n-    \"FlaxRobertaPreLayerNormForSequenceClassification\",\n-    \"FlaxRobertaPreLayerNormForTokenClassification\",\n-    \"FlaxRobertaPreLayerNormModel\",\n-    \"FlaxSpeechEncoderDecoderModel\",\n-    \"FlaxViTForImageClassification\",\n-    \"FlaxViTModel\",\n-    \"FlaxVisionEncoderDecoderModel\",\n-    \"FlaxVisionTextDualEncoderModel\",\n-    \"FlaxWav2Vec2ForCTC\",\n-    \"FlaxWav2Vec2ForPreTraining\",\n-    \"FlaxWav2Vec2Model\",\n-    \"FlaxWhisperForAudioClassification\",\n-    \"FlaxWhisperForConditionalGeneration\",\n-    \"FlaxWhisperModel\",\n-    \"FlaxWhisperTimeStampLogitsProcessor\",\n-    \"FlaxXGLMForCausalLM\",\n-    \"FlaxXGLMModel\",\n-    \"FlaxXLMRobertaForCausalLM\",\n-    \"FlaxXLMRobertaForMaskedLM\",\n-    \"FlaxXLMRobertaForMultipleChoice\",\n-    \"FlaxXLMRobertaForQuestionAnswering\",\n-    \"FlaxXLMRobertaForSequenceClassification\",\n-    \"FlaxXLMRobertaForTokenClassification\",\n-    \"FlaxXLMRobertaModel\",\n     \"FNetConfig\",\n     \"FNetModel\",\n     \"FNetTokenizerFast\",\n@@ -517,26 +395,6 @@\n     \"Text2TextGenerationPipeline\",\n     \"TextClassificationPipeline\",\n     \"TextGenerationPipeline\",\n-    \"TFBartForConditionalGeneration\",\n-    \"TFBartForSequenceClassification\",\n-    \"TFBartModel\",\n-    \"TFBertModel\",\n-    \"TFConvNextModel\",\n-    \"TFData2VecVisionModel\",\n-    \"TFDeiTModel\",\n-    \"TFEncoderDecoderModel\",\n-    \"TFEsmModel\",\n-    \"TFMobileViTModel\",\n-    \"TFRagModel\",\n-    \"TFRagSequenceForGeneration\",\n-    \"TFRagTokenForGeneration\",\n-    \"TFRepetitionPenaltyLogitsProcessor\",\n-    \"TFSwinModel\",\n-    \"TFViTModel\",\n-    \"TFVisionEncoderDecoderModel\",\n-    \"TFVisionTextDualEncoderModel\",\n-    \"TFXGLMForCausalLM\",\n-    \"TFXGLMModel\",\n     \"TimeSeriesTransformerConfig\",\n     \"TokenClassificationPipeline\",\n     \"TrOCRConfig\",\n@@ -601,6 +459,13 @@\n     \"ZeroShotImageClassificationPipeline\",\n     \"ZeroShotObjectDetectionPipeline\",\n     \"Llama4TextConfig\",\n+}\n+# In addition to the objects above, we also ignore objects with certain prefixes. If you add an item to the list\n+# below, make sure to add a comment explaining why.\n+OBJECT_TO_IGNORE_PREFIXES = [\n+    \"_\",  # Private objects are not documented\n+    \"TF\",  # TensorFlow objects are scheduled to be removed in the future\n+    \"Flax\",  # Flax objects are scheduled to be removed in the future\n ]\n \n # Supported math operations when interpreting the value of defaults.\n@@ -1542,7 +1407,11 @@ def check_docstrings(overwrite: bool = False, check_all: bool = False):\n     to_clean = []\n     for name in dir(transformers):\n         # Skip objects that are private or not documented.\n-        if name.startswith(\"_\") or ignore_undocumented(name) or name in OBJECTS_TO_IGNORE:\n+        if (\n+            any(name.startswith(prefix) for prefix in OBJECT_TO_IGNORE_PREFIXES)\n+            or ignore_undocumented(name)\n+            or name in OBJECTS_TO_IGNORE\n+        ):\n             continue\n \n         obj = getattr(transformers, name)"
        },
        {
            "sha": "eeec1aec1bc68a5eb0357fe2c18f314acacc42af",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1763ef2951cc1f200d84a0f57de759be6b1b8af5/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1763ef2951cc1f200d84a0f57de759be6b1b8af5/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=1763ef2951cc1f200d84a0f57de759be6b1b8af5",
            "patch": "@@ -1011,12 +1011,14 @@ def find_all_documented_objects() -> list[str]:\n \n # One good reason for not being documented is to be deprecated. Put in this list deprecated objects.\n DEPRECATED_OBJECTS = [\n+    \"AdamWeightDecay\",  # TensorFlow object, support is deprecated\n     \"AutoModelWithLMHead\",\n     \"BartPretrainedModel\",\n     \"DataCollator\",\n     \"DataCollatorForSOP\",\n     \"GlueDataset\",\n     \"GlueDataTrainingArguments\",\n+    \"GradientAccumulator\",  # TensorFlow object, support is deprecated\n     \"LineByLineTextDataset\",\n     \"LineByLineWithRefDataset\",\n     \"LineByLineWithSOPTextDataset\",\n@@ -1034,8 +1036,10 @@ def find_all_documented_objects() -> list[str]:\n     \"TextDataset\",\n     \"TextDatasetForNextSentencePrediction\",\n     \"TFTrainingArguments\",\n+    \"WarmUp\",  # TensorFlow object, support is deprecated\n     \"Wav2Vec2ForMaskedLM\",\n     \"Wav2Vec2Tokenizer\",\n+    \"create_optimizer\",  # TensorFlow object, support is deprecated\n     \"glue_compute_metrics\",\n     \"glue_convert_examples_to_features\",\n     \"glue_output_modes\","
        }
    ],
    "stats": {
        "total": 1958,
        "additions": 78,
        "deletions": 1880
    }
}