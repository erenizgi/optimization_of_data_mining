{
    "author": "tonywu71",
    "message": "Add ColQwen2 to ðŸ¤— transformers (#35778)\n\n* feat: add colqwen2 (wip)\n\n* tests: fix test_attention_outputs\n\n* tests: reduce hidden size to accelerate tests\n\n* tests: fix `test_attention_outputs` ðŸ¥³\n\n* fix: fix wrong parent class for `ColQwen2ForRetrievalOutput`\n\n* fix: minor typing and style changes\n\n* chore: run `make style`\n\n* feat: remove redundant `max_num_visual_tokens` attribute in `ColQwen2Processor`\n\n* tests: tweak comments\n\n* style: apply ruff formatter\n\n* feat: move default values for `visual_prompt_prefix` and `query_prefix`\n\n* docs: update ColQwen2 model card\n\n* docs: tweak model cards\n\n* docs: add required example config checkpoint\n\n* tests: update expected scores in integration test\n\n* docs: tweak quickstart snippets\n\n* fix: address PR comments\n\n* tests: fix colqwen2 tests + tweak comment in colpali test\n\n* tests: unskip useful tests\n\n* fix: fix bug when `visual_prompt_prefix` or `query_prefix` is an empty string\n\n* fix: fix ColPali outputs when `return_dict == False`\n\n* fix: fix issue with PaliGemma output not being a dict\n\n* docs: set default dtype to bfloat16 in quickstart snippets\n\n* fix: fix error when `return_dict=False` in ColPali and ColQwen2\n\n* tests: fix special tokens not being replaced in input_ids\n\n* style: fix lint\n\n* fix: `ColQwen2Processor`'s `padding_side` is now set from `processor_config.json`\n\n* fix: remove unused `padding_side` in ColQwen2 model\n\n* docs: update ColQwen2's model doc\n\n* fix: fix harcoded vlm backbone class in ColQwen2Config\n\n* fix: remove `padding_side` from ColQwen2Processor as should fed from kwargs\n\n* docs: fix typo in model docstring\n\n* docs: add illuin mention in model docs\n\n* fix: let `padding_size` be handled by `tokenizer_config.json`\n\n* docs: add colpali reference url in colqwen2's model doc\n\n* docs: add Hf mention in model docs\n\n* docs: add late interaction mention in model docs\n\n* docs: tweak colqwen2 model doc\n\n* docs: update reference checkpoint for ColPali to v1.3\n\n* docs: simplify quickstart snippets\n\n* docs: remove redundant `.eval()`\n\n* refactor:  use `can_return_tuple` decorator for ColPali and ColQwen2\n\n* docs: fix copyright date\n\n* docs: add missing copyright in tests\n\n* fix: raise error when `initializer_range` is not in config\n\n* docs: remove redundant `.eval()` in colpali doc\n\n* fix: fix `get_text_config` now that Qwen2VL has a proper `text_config` attribute\n\nSee https://github.com/huggingface/transformers/pull/37268 for details about changes in Qwen2VL's config.\n\n* fix: add missing `initializer_range` attribute in `ColQwen2Config`\n\n* fix: use `get_text_config` in `resize_token_embeddings`\n\n* update colwen2 with auto_docstring\n\n* docs: fix wrong copyright year\n\n* chore: remove `raise` as `initializer_range` has a default value in `ColQwen2Config`\n\n* refactor: merge `inner_forward` into `forward`\n\n* Refactor colqwen2 after refactoring of qwen2VL, use modular for modeling code\n\n* protect torch import in modular to protect in processing\n\n* protect torch import in modular to protect in processing\n\n* tests: fix hf model path in ColQwen2 integration test\n\n* docs: clarify `attn_implementation` and add comments\n\n* docs: add fallback snippet for using offline PIL dummy images\n\n* docs: temporarily revert attn_implementation to `None` while sdpa is not fixed\n\n* docs: tweaks in colpali/colqwen2 quick start snippets\n\n* fix: add missing flags to enable SDPA/Flex Attention in ColQwen2 model\n\n* fix: add missing changes in modular file\n\n* fix modeling tests\n\n---------\n\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",
    "sha": "c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
    "files": [
        {
            "sha": "d12644ae9d7feca2da8e535f542a2e0b6a847abc",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -937,6 +937,8 @@\n         title: CLVP\n       - local: model_doc/colpali\n         title: ColPali\n+      - local: model_doc/colqwen2\n+        title: ColQwen2\n       - local: model_doc/data2vec\n         title: Data2Vec\n       - local: model_doc/deplot"
        },
        {
            "sha": "84d0e087b3bf9c6c2b995202d0ecd1996d2a1a15",
            "filename": "docs/source/en/model_doc/colpali.md",
            "status": "modified",
            "additions": 39,
            "deletions": 20,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -20,31 +20,37 @@ rendered properly in your Markdown viewer.\n \n # ColPali\n \n-[ColPali](https://huggingface.co/papers/2407.01449) is a model designed to retrieve documents by analyzing their visual features. Unlike traditional systems that rely heavily on text extraction and OCR, ColPali treats each page as an image. It uses [Paligemma-3B](./paligemma) to capture not only text, but also the layout, tables, charts, and other visual elements to create detailed embeddings. This offers a more comprehensive understanding of documents and enables more efficient and accurate retrieval.\n+[ColPali](https://huggingface.co/papers/2407.01449) is a model designed to retrieve documents by analyzing their visual features. Unlike traditional systems that rely heavily on text extraction and OCR, ColPali treats each page as an image. It uses [Paligemma-3B](./paligemma) to capture not only text, but also the layout, tables, charts, and other visual elements to create detailed multi-vector embeddings that can be used for retrieval by computing pairwise late interaction similarity scores. This offers a more comprehensive understanding of documents and enables more efficient and accurate retrieval.\n \n-You can find all the original ColPali checkpoints under the [ColPali](https://huggingface.co/collections/vidore/hf-native-colvision-models-6755d68fc60a8553acaa96f7) collection.\n+This model was contributed by [@tonywu71](https://huggingface.co/tonywu71) (ILLUIN Technology) and [@yonigozlan](https://huggingface.co/yonigozlan) (HuggingFace).\n+\n+You can find all the original ColPali checkpoints under Vidore's [Hf-native ColVision Models](https://huggingface.co/collections/vidore/hf-native-colvision-models-6755d68fc60a8553acaa96f7) collection.\n \n > [!TIP]\n > Click on the ColPali models in the right sidebar for more examples of how to use ColPali for image retrieval.\n \n <hfoptions id=\"usage\">\n <hfoption id=\"image retrieval\">\n \n-```py\n+```python\n import requests\n import torch\n from PIL import Image\n+\n from transformers import ColPaliForRetrieval, ColPaliProcessor\n \n-# Load model (bfloat16 support is limited; fallback to float32 if needed)\n+\n+# Load the model and the processor\n+model_name = \"vidore/colpali-v1.3-hf\"\n+\n model = ColPaliForRetrieval.from_pretrained(\n-    \"vidore/colpali-v1.2-hf\",\n-    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n+    model_name,\n+    torch_dtype=torch.bfloat16,\n     device_map=\"auto\",  # \"cpu\", \"cuda\", or \"mps\" for Apple Silicon\n-).eval()\n-\n+)\n processor = ColPaliProcessor.from_pretrained(model_name)\n \n+# The document page screenshots from your corpus\n url1 = \"https://upload.wikimedia.org/wikipedia/commons/8/89/US-original-Declaration-1776.jpg\"\n url2 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Romeoandjuliet1597.jpg/500px-Romeoandjuliet1597.jpg\"\n \n@@ -53,38 +59,53 @@ images = [\n     Image.open(requests.get(url2, stream=True).raw),\n ]\n \n+# The queries you want to retrieve documents for\n queries = [\n-    \"Who printed the edition of Romeo and Juliet?\",\n     \"When was the United States Declaration of Independence proclaimed?\",\n+    \"Who printed the edition of Romeo and Juliet?\",\n ]\n \n # Process the inputs\n-inputs_images = processor(images=images, return_tensors=\"pt\").to(model.device)\n-inputs_text = processor(text=queries, return_tensors=\"pt\").to(model.device)\n+inputs_images = processor(images=images).to(model.device)\n+inputs_text = processor(text=queries).to(model.device)\n \n # Forward pass\n with torch.no_grad():\n     image_embeddings = model(**inputs_images).embeddings\n     query_embeddings = model(**inputs_text).embeddings\n \n+# Score the queries against the images\n scores = processor.score_retrieval(query_embeddings, image_embeddings)\n \n print(\"Retrieval scores (query x image):\")\n print(scores)\n ```\n+\n+If you have issue with loading the images with PIL, you can use the following code to create dummy images:\n+\n+```python\n+images = [\n+    Image.new(\"RGB\", (128, 128), color=\"white\"),\n+    Image.new(\"RGB\", (64, 32), color=\"black\"),\n+]\n+```\n+\n </hfoption>\n </hfoptions>\n \n Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n The example below uses [bitsandbytes](../quantization/bitsandbytes.md) to quantize the weights to int4.\n \n-```py\n+```python\n import requests\n import torch\n from PIL import Image\n-from transformers import ColPaliForRetrieval, ColPaliProcessor\n-from transformers import BitsAndBytesConfig\n+\n+from transformers import BitsAndBytesConfig, ColPaliForRetrieval, ColPaliProcessor\n+\n+\n+model_name = \"vidore/colpali-v1.3-hf\"\n \n # 4-bit quantization configuration\n bnb_config = BitsAndBytesConfig(\n@@ -94,14 +115,11 @@ bnb_config = BitsAndBytesConfig(\n     bnb_4bit_compute_dtype=torch.float16,\n )\n \n-model_name = \"vidore/colpali-v1.2-hf\"\n-\n-# Load model \n model = ColPaliForRetrieval.from_pretrained(\n     model_name,\n     quantization_config=bnb_config,\n-    device_map=\"cuda\"\n-).eval()\n+    device_map=\"cuda\",\n+)\n \n processor = ColPaliProcessor.from_pretrained(model_name)\n \n@@ -114,8 +132,8 @@ images = [\n ]\n \n queries = [\n-    \"Who printed the edition of Romeo and Juliet?\",\n     \"When was the United States Declaration of Independence proclaimed?\",\n+    \"Who printed the edition of Romeo and Juliet?\",\n ]\n \n # Process the inputs\n@@ -127,6 +145,7 @@ with torch.no_grad():\n     image_embeddings = model(**inputs_images).embeddings\n     query_embeddings = model(**inputs_text).embeddings\n \n+# Score the queries against the images\n scores = processor.score_retrieval(query_embeddings, image_embeddings)\n \n print(\"Retrieval scores (query x image):\")"
        },
        {
            "sha": "e8c48f08a6aba20b95aca36b02dfac0f0cdbee41",
            "filename": "docs/source/en/model_doc/colqwen2.md",
            "status": "added",
            "additions": 176,
            "deletions": 0,
            "changes": 176,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -0,0 +1,176 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n+# ColQwen2\n+\n+[ColQwen2](https://doi.org/10.48550/arXiv.2407.01449) is a variant of the [ColPali](./colpali) model designed to retrieve documents by analyzing their visual features. Unlike traditional systems that rely heavily on text extraction and OCR, ColQwen2 treats each page as an image. It uses the [Qwen2-VL](./qwen2_vl) backbone to capture not only text, but also the layout, tables, charts, and other visual elements to create detailed multi-vector embeddings that can be used for retrieval by computing pairwise late interaction similarity scores. This offers a more comprehensive understanding of documents and enables more efficient and accurate retrieval.\n+\n+This model was contributed by [@tonywu71](https://huggingface.co/tonywu71) (ILLUIN Technology) and [@yonigozlan](https://huggingface.co/yonigozlan) (HuggingFace).\n+\n+You can find all the original ColPali checkpoints under Vidore's [Hf-native ColVision Models](https://huggingface.co/collections/vidore/hf-native-colvision-models-6755d68fc60a8553acaa96f7) collection.\n+\n+> [!TIP]\n+> Click on the ColQwen2 models in the right sidebar for more examples of how to use ColQwen2 for image retrieval.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"image retrieval\">\n+\n+```python\n+import requests\n+import torch\n+from PIL import Image\n+\n+from transformers import ColQwen2ForRetrieval, ColQwen2Processor\n+from transformers.utils.import_utils import is_flash_attn_2_available\n+\n+\n+# Load the model and the processor\n+model_name = \"vidore/colqwen2-v1.0-hf\"\n+\n+model = ColQwen2ForRetrieval.from_pretrained(\n+    model_name,\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",  # \"cpu\", \"cuda\", or \"mps\" for Apple Silicon\n+    attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else \"sdpa\",\n+)\n+processor = ColQwen2Processor.from_pretrained(model_name)\n+\n+# The document page screenshots from your corpus\n+url1 = \"https://upload.wikimedia.org/wikipedia/commons/8/89/US-original-Declaration-1776.jpg\"\n+url2 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Romeoandjuliet1597.jpg/500px-Romeoandjuliet1597.jpg\"\n+\n+images = [\n+    Image.open(requests.get(url1, stream=True).raw),\n+    Image.open(requests.get(url2, stream=True).raw),\n+]\n+\n+# The queries you want to retrieve documents for\n+queries = [\n+    \"When was the United States Declaration of Independence proclaimed?\",\n+    \"Who printed the edition of Romeo and Juliet?\",\n+]\n+\n+# Process the inputs\n+inputs_images = processor(images=images).to(model.device)\n+inputs_text = processor(text=queries).to(model.device)\n+\n+# Forward pass\n+with torch.no_grad():\n+    image_embeddings = model(**inputs_images).embeddings\n+    query_embeddings = model(**inputs_text).embeddings\n+\n+# Score the queries against the images\n+scores = processor.score_retrieval(query_embeddings, image_embeddings)\n+\n+print(\"Retrieval scores (query x image):\")\n+print(scores)\n+```\n+\n+If you have issue with loading the images with PIL, you can use the following code to create dummy images:\n+\n+```python\n+images = [\n+    Image.new(\"RGB\", (128, 128), color=\"white\"),\n+    Image.new(\"RGB\", (64, 32), color=\"black\"),\n+]\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\n+The example below uses [bitsandbytes](../quantization/bitsandbytes.md) to quantize the weights to int4.\n+\n+```python\n+import requests\n+import torch\n+from PIL import Image\n+\n+from transformers import BitsAndBytesConfig, ColQwen2ForRetrieval, ColQwen2Processor\n+\n+\n+model_name = \"vidore/colqwen2-v1.0-hf\"\n+\n+# 4-bit quantization configuration\n+bnb_config = BitsAndBytesConfig(\n+    load_in_4bit=True,\n+    bnb_4bit_use_double_quant=True,\n+    bnb_4bit_quant_type=\"nf4\",\n+    bnb_4bit_compute_dtype=torch.float16,\n+)\n+\n+model = ColQwen2ForRetrieval.from_pretrained(\n+    model_name,\n+    quantization_config=bnb_config,\n+    device_map=\"cuda\",\n+).eval()\n+\n+processor = ColQwen2Processor.from_pretrained(model_name)\n+\n+url1 = \"https://upload.wikimedia.org/wikipedia/commons/8/89/US-original-Declaration-1776.jpg\"\n+url2 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Romeoandjuliet1597.jpg/500px-Romeoandjuliet1597.jpg\"\n+\n+images = [\n+    Image.open(requests.get(url1, stream=True).raw),\n+    Image.open(requests.get(url2, stream=True).raw),\n+]\n+\n+queries = [\n+    \"When was the United States Declaration of Independence proclaimed?\",\n+    \"Who printed the edition of Romeo and Juliet?\",\n+]\n+\n+# Process the inputs\n+inputs_images = processor(images=images, return_tensors=\"pt\").to(model.device)\n+inputs_text = processor(text=queries, return_tensors=\"pt\").to(model.device)\n+\n+# Forward pass\n+with torch.no_grad():\n+    image_embeddings = model(**inputs_images).embeddings\n+    query_embeddings = model(**inputs_text).embeddings\n+\n+# Score the queries against the images\n+scores = processor.score_retrieval(query_embeddings, image_embeddings)\n+\n+print(\"Retrieval scores (query x image):\")\n+print(scores)\n+```\n+\n+## Notes\n+\n+- [`~ColQwen2Processor.score_retrieval`] returns a 2D tensor where the first dimension is the number of queries and the second dimension is the number of images. A higher score indicates more similarity between the query and image.\n+- Unlike ColPali, ColQwen2 supports arbitrary image resolutions and aspect ratios, which means images are not resized into fixed-size squares. This preserves more of the original input signal.\n+- Larger input images generate longer multi-vector embeddings, allowing users to adjust image resolution to balance performance and memory usage.\n+\n+## ColQwen2Config\n+\n+[[autodoc]] ColQwen2Config\n+\n+## ColQwen2Processor\n+\n+[[autodoc]] ColQwen2Processor\n+\n+## ColQwen2ForRetrieval\n+\n+[[autodoc]] ColQwen2ForRetrieval\n+    - forward"
        },
        {
            "sha": "151eb0844d0d0e67a00a0336f9426d0e27ea15ad",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -62,6 +62,7 @@\n     from .cohere import *\n     from .cohere2 import *\n     from .colpali import *\n+    from .colqwen2 import *\n     from .conditional_detr import *\n     from .convbert import *\n     from .convnext import *"
        },
        {
            "sha": "056516e7318a39edf4bdde64e03d96235cc47e2f",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -79,6 +79,7 @@\n         (\"cohere\", \"CohereConfig\"),\n         (\"cohere2\", \"Cohere2Config\"),\n         (\"colpali\", \"ColPaliConfig\"),\n+        (\"colqwen2\", \"ColQwen2Config\"),\n         (\"conditional_detr\", \"ConditionalDetrConfig\"),\n         (\"convbert\", \"ConvBertConfig\"),\n         (\"convnext\", \"ConvNextConfig\"),\n@@ -437,6 +438,7 @@\n         (\"cohere\", \"Cohere\"),\n         (\"cohere2\", \"Cohere2\"),\n         (\"colpali\", \"ColPali\"),\n+        (\"colqwen2\", \"ColQwen2\"),\n         (\"conditional_detr\", \"Conditional DETR\"),\n         (\"convbert\", \"ConvBERT\"),\n         (\"convnext\", \"ConvNeXT\"),"
        },
        {
            "sha": "8ea758c1d72b93a43db80f0dd118c5946e011e88",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -365,6 +365,7 @@\n         (\"bloom\", \"BloomForCausalLM\"),\n         (\"camembert\", \"CamembertForMaskedLM\"),\n         (\"colpali\", \"ColPaliForRetrieval\"),\n+        (\"colqwen2\", \"ColQwen2ForRetrieval\"),\n         (\"ctrl\", \"CTRLLMHeadModel\"),\n         (\"data2vec-text\", \"Data2VecTextForMaskedLM\"),\n         (\"deberta\", \"DebertaForMaskedLM\"),"
        },
        {
            "sha": "d67b2c3982e69560e4b4b54b8b4a6550a4516caa",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -66,6 +66,7 @@\n         (\"clipseg\", \"CLIPSegProcessor\"),\n         (\"clvp\", \"ClvpProcessor\"),\n         (\"colpali\", \"ColPaliProcessor\"),\n+        (\"colqwen2\", \"ColQwen2Processor\"),\n         (\"emu3\", \"Emu3Processor\"),\n         (\"flava\", \"FlavaProcessor\"),\n         (\"fuyu\", \"FuyuProcessor\"),"
        },
        {
            "sha": "f2344daba026b2198fad758aa3c4fc50f1612db2",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -147,6 +147,7 @@\n         (\"cohere\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"cohere2\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"colpali\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"colqwen2\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n         (\"convbert\", (\"ConvBertTokenizer\", \"ConvBertTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n             \"cpm\","
        },
        {
            "sha": "c7a1d01d14640bad45dc1fe31404dea2d7bcd3c3",
            "filename": "src/transformers/models/colpali/configuration_colpali.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconfiguration_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconfiguration_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconfiguration_colpali.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -33,8 +33,6 @@ class ColPaliConfig(PretrainedConfig):\n     Creating a configuration with the default settings will result in a configuration where the VLM backbone is set to the\n     default PaliGemma configuration, i.e the one from [vidore/colpali-v1.2](https://huggingface.co/vidore/colpali-v1.2).\n \n-    The ColPali config is very similar to [`PaligemmaConfig`], but with an extra attribute defining the embedding dimension.\n-\n     Note that contrarily to what the class name suggests (actually the name refers to the ColPali **methodology**), you can\n     use a different VLM backbone model than PaliGemma by passing the corresponding VLM configuration to the class constructor.\n \n@@ -93,7 +91,7 @@ def __init__(\n             )\n \n         self.vlm_config = vlm_config\n-        self.text_config = text_config = text_config if text_config is not None else vlm_config.text_config\n+        self.text_config = text_config if text_config is not None else vlm_config.text_config\n         if isinstance(self.text_config, dict):\n             text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"gemma\"\n             self.text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)"
        },
        {
            "sha": "69ba6e6643db2c1cba19fb85adb2ec3730b23deb",
            "filename": "src/transformers/models/colpali/modeling_colpali.py",
            "status": "modified",
            "additions": 19,
            "deletions": 27,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -24,16 +24,10 @@\n \n from ...cache_utils import Cache\n from ...modeling_utils import PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple\n from .configuration_colpali import ColPaliConfig\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The bare ColPali model outputting raw hidden-states without any specific head on top.\n-    \"\"\"\n-)\n-@auto_docstring\n class ColPaliPreTrainedModel(PreTrainedModel):\n     config_class = ColPaliConfig\n     base_model_prefix = \"model\"\n@@ -98,13 +92,16 @@ class ColPaliForRetrievalOutput(ModelOutput):\n \n @auto_docstring(\n     custom_intro=\"\"\"\n-    In our proposed ColPali approach, we leverage VLMs to construct efficient multi-vector embeddings directly\n-    from document images (â€œscreenshotsâ€) for document retrieval. We train the model to maximize the similarity\n+    The ColPali architecture leverages VLMs to construct efficient multi-vector embeddings directly\n+    from document images (â€œscreenshotsâ€) for document retrieval. The model is trained to maximize the similarity\n     between these document embeddings and the corresponding query embeddings, using the late interaction method\n     introduced in ColBERT.\n \n     Using ColPali removes the need for potentially complex and brittle layout recognition and OCR pipelines with a\n     single model that can take into account both the textual and visual content (layout, charts, etc.) of a document.\n+\n+    ColPali is part of the ColVision model family, which was first introduced in the following paper:\n+    [*ColPali: Efficient Document Retrieval with Vision Language Models*](https://arxiv.org/abs/2407.01449).\n     \"\"\"\n )\n class ColPaliForRetrieval(ColPaliPreTrainedModel):\n@@ -126,6 +123,7 @@ def __init__(self, config: ColPaliConfig):\n \n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -136,48 +134,42 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[Tuple, ColPaliForRetrievalOutput]:\n-        if \"pixel_values\" in kwargs:\n-            kwargs[\"pixel_values\"] = kwargs[\"pixel_values\"].to(dtype=self.dtype)\n+    ) -> ColPaliForRetrievalOutput:\n+        if pixel_values is not None:\n+            pixel_values = pixel_values.to(dtype=self.dtype)\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n \n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.vlm(\n+        vlm_output = self.vlm(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             pixel_values=pixel_values,\n             output_hidden_states=True,\n-            return_dict=return_dict,\n+            return_dict=True,\n             output_attentions=output_attentions,\n             **kwargs,\n         )\n+        vlm_hidden_states = vlm_output.hidden_states if output_hidden_states else None\n+        vlm_image_hidden_states = vlm_output.image_hidden_states if pixel_values is not None else None\n \n-        last_hidden_states = outputs.hidden_states[-1]  # (batch_size, sequence_length, hidden_size)\n+        last_hidden_states = vlm_output.hidden_states[-1]  # (batch_size, sequence_length, hidden_size)\n         embeddings = self.embedding_proj_layer(last_hidden_states)  # (batch_size, sequence_length, dim)\n \n         # L2 normalization\n         embeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)  # (batch_size, sequence_length, dim)\n \n         embeddings = embeddings * attention_mask.unsqueeze(-1)  # (batch_size, sequence_length, dim)\n \n-        loss = None\n-        if not return_dict:\n-            output = (embeddings,) + outputs[2:]\n-            output[2] = output[2] if output_hidden_states is not None else None\n-            output[-1] = (outputs.image_hidden_states if pixel_values is not None else None,)\n-            return (loss,) + output if loss is not None else output\n-\n         return ColPaliForRetrievalOutput(\n-            loss=loss,\n             embeddings=embeddings,\n-            past_key_values=outputs.past_key_values,\n-            hidden_states=outputs.hidden_states if output_hidden_states else None,\n-            attentions=outputs.attentions,\n-            image_hidden_states=outputs.image_hidden_states if pixel_values is not None else None,\n+            past_key_values=vlm_output.past_key_values,\n+            hidden_states=vlm_hidden_states,\n+            attentions=vlm_output.attentions,\n+            image_hidden_states=vlm_image_hidden_states,\n         )\n \n     def get_input_embeddings(self):"
        },
        {
            "sha": "513af889228ff610b392d5191848d10d00d5db50",
            "filename": "src/transformers/models/colpali/modular_colpali.py",
            "status": "modified",
            "additions": 25,
            "deletions": 27,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -14,28 +14,15 @@\n # limitations under the License.\n \n \n-from typing import ClassVar, List, Optional, Union\n+from typing import List, Optional, Union\n \n-from transformers.models.paligemma.processing_paligemma import (\n-    IMAGE_TOKEN,\n-    PaliGemmaProcessor,\n-    build_string_from_input,\n-)\n+from transformers.models.paligemma.processing_paligemma import IMAGE_TOKEN, PaliGemmaProcessor, build_string_from_input\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image, make_flat_list_of_images\n-from ...processing_utils import (\n-    ProcessingKwargs,\n-    Unpack,\n-)\n-from ...tokenization_utils_base import (\n-    PreTokenizedInput,\n-    TextInput,\n-)\n-from ...utils import (\n-    is_torch_available,\n-    logging,\n-)\n+from ...processing_utils import ProcessingKwargs, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import is_torch_available, logging\n \n \n if is_torch_available():\n@@ -73,10 +60,23 @@ class ColPaliProcessor(PaliGemmaProcessor):\n             The tokenizer is a required input.\n         chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n             in a chat into a tokenizable string.\n+        visual_prompt_prefix (`str`, *optional*, defaults to `\"Describe the image.\"`):\n+            A string that gets tokenized and prepended to the image tokens.\n+        query_prefix (`str`, *optional*, defaults to `\"Question: \"`):\n+            A prefix to be used for the query.\n     \"\"\"\n \n-    visual_prompt_prefix: ClassVar[str] = \"Describe the image.\"\n-    query_prefix: ClassVar[str] = \"Question: \"\n+    def __init__(\n+        self,\n+        image_processor=None,\n+        tokenizer=None,\n+        chat_template=None,\n+        visual_prompt_prefix: str = \"Describe the image.\",\n+        query_prefix: str = \"Question: \",\n+    ):\n+        super().__init__(image_processor=image_processor, tokenizer=tokenizer, chat_template=chat_template)\n+        self.visual_prompt_prefix = visual_prompt_prefix\n+        self.query_prefix = query_prefix\n \n     @property\n     def query_augmentation_token(self) -> str:\n@@ -96,7 +96,7 @@ def __call__(\n         **kwargs: Unpack[ColPaliProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n-        Main method to prepare for the model either (1) one or several texts, either (2) one or several image(s). This method is custom\n+        Main method to prepare for the model either (1) one or several texts, either (2) one or several image(s). This method is a custom\n         wrapper around the PaliGemmaProcessor's [`~PaliGemmaProcessor.__call__`] method adapted for the ColPali model. It cannot process\n         both text and images at the same time.\n \n@@ -196,12 +196,10 @@ def __call__(\n \n             if suffix is None:\n                 suffix = self.query_augmentation_token * 10\n-            texts_query: List[str] = []\n \n+            texts_query: List[str] = []\n             for query in text:\n-                query = self.tokenizer.bos_token + self.query_prefix + query\n-                query += suffix  # add suffix (pad tokens)\n-                query += \"\\n\"  # make input ISO to PaliGemma's processor\n+                query = self.tokenizer.bos_token + self.query_prefix + query + suffix + \"\\n\"\n                 texts_query.append(query)\n \n             output_kwargs[\"text_kwargs\"][\"max_length\"] = output_kwargs[\"text_kwargs\"].get(\"max_length\", 50)\n@@ -223,7 +221,7 @@ def process_images(\n         Prepare for the model one or several image(s). This method is a wrapper around the `__call__` method of the ColPaliProcessor's\n         [`ColPaliProcessor.__call__`].\n \n-        This method forwards the `images` and `kwargs` arguments to SiglipImageProcessor's [`~SiglipImageProcessor.__call__`].\n+        This method forwards the `images` and `kwargs` arguments to the image processor.\n \n         Args:\n             images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n@@ -258,7 +256,7 @@ def process_queries(\n         Prepare for the model one or several texts. This method is a wrapper around the `__call__` method of the ColPaliProcessor's\n         [`ColPaliProcessor.__call__`].\n \n-        This method forwards the `text` and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`].\n+        This method forwards the `text` and `kwargs` arguments to the tokenizer.\n \n         Args:\n             text (`str`, `List[str]`, `List[List[str]]`):"
        },
        {
            "sha": "a0f1c124d6342c56b1b3955bd729b820925cc40d",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 15,
            "deletions": 14,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -20,7 +20,7 @@\n # limitations under the License.\n \n \n-from typing import ClassVar, List, Optional, Union\n+from typing import List, Optional, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image, make_flat_list_of_images\n@@ -87,22 +87,25 @@ class ColPaliProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n         chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n             in a chat into a tokenizable string.\n+        visual_prompt_prefix (`str`, *optional*, defaults to `\"Describe the image.\"`):\n+            A string that gets tokenized and prepended to the image tokens.\n+        query_prefix (`str`, *optional*, defaults to `\"Question: \"`):\n+            A prefix to be used for the query.\n     \"\"\"\n \n     attributes = [\"image_processor\", \"tokenizer\"]\n     image_processor_class = (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")\n     tokenizer_class = (\"GemmaTokenizer\", \"GemmaTokenizerFast\")\n \n-    visual_prompt_prefix: ClassVar[str] = \"Describe the image.\"\n-    query_prefix: ClassVar[str] = \"Question: \"\n-\n     def __init__(\n         self,\n         image_processor=None,\n         tokenizer=None,\n         chat_template=None,\n-        **kwargs,\n+        visual_prompt_prefix: str = \"Describe the image.\",\n+        query_prefix: str = \"Question: \",\n     ):\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n         if image_processor is None:\n             raise ValueError(\"You need to specify an `image_processor`.\")\n         if tokenizer is None:\n@@ -125,8 +128,8 @@ def __init__(\n         tokenizer.add_tokens(EXTRA_TOKENS)\n         tokenizer.add_bos_token = False\n         tokenizer.add_eos_token = False\n-\n-        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n+        self.visual_prompt_prefix = visual_prompt_prefix\n+        self.query_prefix = query_prefix\n \n     def __call__(\n         self,\n@@ -137,7 +140,7 @@ def __call__(\n         **kwargs: Unpack[ColPaliProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n-        Main method to prepare for the model either (1) one or several texts, either (2) one or several image(s). This method is custom\n+        Main method to prepare for the model either (1) one or several texts, either (2) one or several image(s). This method is a custom\n         wrapper around the PaliGemmaProcessor's [`~PaliGemmaProcessor.__call__`] method adapted for the ColPali model. It cannot process\n         both text and images at the same time.\n \n@@ -237,12 +240,10 @@ def __call__(\n \n             if suffix is None:\n                 suffix = self.query_augmentation_token * 10\n-            texts_query: List[str] = []\n \n+            texts_query: List[str] = []\n             for query in text:\n-                query = self.tokenizer.bos_token + self.query_prefix + query\n-                query += suffix  # add suffix (pad tokens)\n-                query += \"\\n\"  # make input ISO to PaliGemma's processor\n+                query = self.tokenizer.bos_token + self.query_prefix + query + suffix + \"\\n\"\n                 texts_query.append(query)\n \n             output_kwargs[\"text_kwargs\"][\"max_length\"] = output_kwargs[\"text_kwargs\"].get(\"max_length\", 50)\n@@ -312,7 +313,7 @@ def process_images(\n         Prepare for the model one or several image(s). This method is a wrapper around the `__call__` method of the ColPaliProcessor's\n         [`ColPaliProcessor.__call__`].\n \n-        This method forwards the `images` and `kwargs` arguments to SiglipImageProcessor's [`~SiglipImageProcessor.__call__`].\n+        This method forwards the `images` and `kwargs` arguments to the image processor.\n \n         Args:\n             images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n@@ -347,7 +348,7 @@ def process_queries(\n         Prepare for the model one or several texts. This method is a wrapper around the `__call__` method of the ColPaliProcessor's\n         [`ColPaliProcessor.__call__`].\n \n-        This method forwards the `text` and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`].\n+        This method forwards the `text` and `kwargs` arguments to the tokenizer.\n \n         Args:\n             text (`str`, `List[str]`, `List[List[str]]`):"
        },
        {
            "sha": "d313293c0a0a7b5c56ac5bec1e54e8eb1e5a6e4a",
            "filename": "src/transformers/models/colqwen2/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolqwen2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolqwen2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2F__init__.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_colqwen2 import *\n+    from .modeling_colqwen2 import *\n+    from .processing_colqwen2 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "39035d87736593a13267fad32c40efcfaa47e84a",
            "filename": "src/transformers/models/colqwen2/configuration_colqwen2.py",
            "status": "added",
            "additions": 94,
            "deletions": 0,
            "changes": 94,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconfiguration_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconfiguration_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconfiguration_colqwen2.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -0,0 +1,94 @@\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from copy import deepcopy\n+from typing import Any, Dict\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ..auto import CONFIG_MAPPING\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class ColQwen2Config(PretrainedConfig):\n+    r\"\"\"\n+    Configuration class to store the configuration of a [`ColQ2en2ForRetrieval`]. It is used to instantiate an instance\n+    of `ColQwen2ForRetrieval` according to the specified arguments, defining the model architecture following the methodology\n+    from the \"ColPali: Efficient Document Retrieval with Vision Language Models\" paper.\n+\n+    Instantiating a configuration with the defaults will yield a similar configuration to the vision encoder used by the pre-trained\n+    ColQwen2-v1.0 model, e.g. [vidore/colqwen2-v1.0-hf](https://huggingface.co/vidore/colqwen2-v1.0-hf).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vlm_config (`PretrainedConfig`, *optional*):\n+            Configuration of the VLM backbone model.\n+        embedding_dim (`int`, *optional*, defaults to 128):\n+            Dimension of the multi-vector embeddings produced by the model.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+    Example:\n+\n+    ```python\n+    from transformers.models.colqwen2 import ColQwen2Config, ColQwen2ForRetrieval\n+\n+    config = ColQwen2Config()\n+    model = ColQwen2ForRetrieval(config)\n+    ```\n+    \"\"\"\n+\n+    model_type = \"colqwen2\"\n+    sub_configs: Dict[str, Any] = {\"vlm_config\": PretrainedConfig}\n+\n+    def __init__(\n+        self,\n+        vlm_config=None,\n+        embedding_dim: int = 128,\n+        initializer_range: float = 0.02,\n+        **kwargs,\n+    ):\n+        if vlm_config is None:\n+            vlm_config = CONFIG_MAPPING[\"qwen2_vl\"]()\n+            logger.info(\n+                \"`vlm_config` is `None`. Initializing `vlm_config` with the `Qwen2VLConfig` with default values.\"\n+            )\n+        elif isinstance(vlm_config, dict):\n+            vlm_config = deepcopy(vlm_config)\n+            if \"model_type\" not in vlm_config:\n+                raise KeyError(\n+                    \"The `model_type` key is missing in the `vlm_config` dictionary. Please provide the model type.\"\n+                )\n+            vlm_config = CONFIG_MAPPING[vlm_config[\"model_type\"]](**vlm_config)\n+        elif isinstance(vlm_config, PretrainedConfig):\n+            vlm_config = vlm_config\n+        else:\n+            raise TypeError(\n+                f\"Invalid type for `vlm_config`. Expected `PretrainedConfig`, `dict`, or `None`, but got {type(vlm_config)}.\"\n+            )\n+\n+        self.vlm_config = vlm_config\n+        self.embedding_dim = embedding_dim\n+        self.initializer_range = initializer_range\n+        super().__init__(**kwargs)\n+\n+    def get_text_config(self, decoder=False) -> PretrainedConfig:\n+        return self.vlm_config.get_text_config(decoder=decoder)\n+\n+\n+__all__ = [\"ColQwen2Config\"]"
        },
        {
            "sha": "3b652470ccdb8cc658c0174120dfe89722498c2d",
            "filename": "src/transformers/models/colqwen2/convert_colqwen2_weights_to_hf.py",
            "status": "added",
            "additions": 212,
            "deletions": 0,
            "changes": 212,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconvert_colqwen2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconvert_colqwen2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconvert_colqwen2_weights_to_hf.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -0,0 +1,212 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"\n+Convert ColQwen2 weights from the original repository to the HF model format.\n+\n+Don't forget to manually upload the processor-related files to the HF model repository\n+after running this script.\n+\n+Original repository: https://github.com/illuin-tech/colqwen2.\n+\n+NOTE: This script was originally run using `torch==2.5.1` and with:\n+\n+```bash\n+python src/transformers/models/colqwen2/convert_colqwen2_weights_to_hf.py \\\n+    --model_id vidore/colqwen2-v1.0-merged \\\n+    --revision eeccbae1d44bdcb0c83b1788127a2b2cad7d718e \\\n+    --original_vlm_name_or_path Qwen/Qwen2-VL-2B-Instruct \\\n+    --output_dir vidore/colqwen2-v1.0-hf-internal \\\n+    --push_to_hub\n+```\n+\"\"\"\n+\n+import argparse\n+import glob\n+from pathlib import Path\n+from typing import Any, Dict, Optional\n+\n+import torch\n+from huggingface_hub import snapshot_download\n+from safetensors import safe_open\n+\n+from transformers import AutoConfig\n+from transformers.models.colqwen2 import ColQwen2ForRetrieval\n+from transformers.models.colqwen2.configuration_colqwen2 import ColQwen2Config\n+from transformers.utils import logging\n+\n+\n+logging.set_verbosity_info()\n+logger = logging.get_logger(__name__)\n+\n+\n+ORIGINAL_DTYPE = torch.bfloat16\n+\n+\n+def load_original_state_dict(model_id: str, revision: Optional[str] = None) -> Dict[str, torch.Tensor]:\n+    directory_path = snapshot_download(\n+        repo_id=model_id,\n+        revision=revision,\n+        allow_patterns=[\"*.safetensors\"],\n+    )\n+\n+    original_state_dict = {}\n+    for path in glob.glob(f\"{directory_path}/*\"):\n+        if path.endswith(\".safetensors\"):\n+            with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n+                for key in f.keys():\n+                    original_state_dict[key] = f.get_tensor(key)\n+\n+    # Some weights are tied, so `lm.head`` is not saved. Let's clone to load state dict.\n+    if \"lm_head.weight\" not in original_state_dict:\n+        original_state_dict[\"lm_head.weight\"] = original_state_dict[\"model.embed_tokens.weight\"].clone()\n+\n+    return original_state_dict\n+\n+\n+def rename_state_dict_keys(state_dict: Dict[str, Any]) -> Dict[str, Any]:\n+    new_state_dict: Dict[str, Any] = {}\n+    for key, value in state_dict.items():\n+        if key.startswith(\"custom_text_proj\"):\n+            new_key = key.replace(\"custom_text_proj\", \"embedding_proj_layer\")\n+        else:\n+            # The original ColQwen2 inherits from Qwen2VL, so we simply need to add the `vlm.` prefix\n+            # to all remaining keys.\n+            if key.startswith(\"model.\"):\n+                key = key.replace(\"model.\", \"model.language_model.\")\n+            if key.startswith(\"visual.\"):\n+                key = key.replace(\"visual.\", \"model.visual.\")\n+            new_key = \"vlm.\" + key\n+        new_state_dict[new_key] = value\n+    return new_state_dict\n+\n+\n+@torch.no_grad()\n+def convert_colqwen2_weights_to_hf(\n+    model_id: str,\n+    output_dir: str,\n+    push_to_hub: bool,\n+    revision: Optional[str] = None,\n+    original_vlm_name_or_path: Optional[str] = None,\n+):\n+    # Load the original model data\n+    original_config = AutoConfig.from_pretrained(\n+        model_id,\n+        revision=revision,\n+    )\n+    if original_vlm_name_or_path is not None:\n+        original_config._name_or_path = original_vlm_name_or_path\n+    if hasattr(original_config, \"architectures\"):\n+        delattr(original_config, \"architectures\")\n+\n+    original_state_dict = load_original_state_dict(model_id, revision=revision)\n+\n+    # Format the state_dict keys\n+    original_state_dict = rename_state_dict_keys(original_state_dict)\n+\n+    # Create the new config\n+    config = ColQwen2Config(\n+        vlm_config=original_config,\n+        embedding_dim=128,  # hardcoded in the original model\n+    )\n+    config.model_type = \"colqwen2\"\n+    config.is_composition = False\n+\n+    # Load the untrained model\n+    model = ColQwen2ForRetrieval(config=config).to(\"cpu\").eval()\n+    print(\"Created model with new config and randomly initialized weights\")\n+\n+    # NOTE: The new model was initialized with float32 weights. We need to convert it to the desired precision.\n+    # There are two ways to set the model's dtype:\n+    # - Using `model.from_pretrained(..., torch_dtype=dtype_precision)` doesn't convert the hyperparameters to the desired precision.\n+    # - Using `model.to(dtype_precision)` converts all values - including the hyperparameters - to the desired precision.\n+    # The following snippet allows a fine-grained control over the model's dtype, making sure that all\n+    # the new weights' dtypes match the original model.\n+    for param in model.parameters():\n+        param.data = param.data.to(ORIGINAL_DTYPE)\n+    print(f\"Converted the new model weights to `{ORIGINAL_DTYPE}`\")\n+\n+    # Load the original weights\n+    model.load_state_dict(original_state_dict)\n+    print(\"Loaded original model weights\")\n+\n+    # # Sanity check: ensure all keys are the same\n+    state_dict_keys_old = set(original_state_dict.keys())\n+    state_dict_keys_new = set(model.state_dict().keys())\n+    disjoint_keys = state_dict_keys_old.symmetric_difference(state_dict_keys_new)\n+    if disjoint_keys:\n+        raise ValueError(f\"Incompatible keys: {disjoint_keys}\")\n+\n+    # Save the model\n+    if push_to_hub:\n+        model.push_to_hub(output_dir, private=True)\n+        print(f\"Model pushed to the hub at `{output_dir}`\")\n+    else:\n+        Path(output_dir).mkdir(exist_ok=True, parents=True)\n+        model.save_pretrained(output_dir)\n+        print(f\"Model saved to `{output_dir}`\")\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser(\n+        description=\"\"\"\n+        This script converts the original ColQwen2 model to the HF model format.\n+\n+        Don't forget to manually upload the processor-related files to the HF model repository\n+        after running this script.\n+\n+        Example usage:\n+        ```bash\n+        python src/transformers/models/colqwen2/convert_colqwen2_weights_to_hf.py \\\n+            --model_id vidore/colqwen2-v1.0-merged \\\n+            --revision eeccbae1d44bdcb0c83b1788127a2b2cad7d718e \\\n+            --original_vlm_name_or_path Qwen/Qwen2-VL-2B-Instruct \\\n+            --output_dir vidore/colqwen2-v1.0-hf-internal \\\n+            --push_to_hub\n+        ```\n+        \"\"\"\n+    )\n+    parser.add_argument(\n+        \"--model_id\",\n+        help=\"Model ID of the original model to convert\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        help=\"Location to write HF model and tokenizer\",\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        help=\"Whether or not to push the model to the hub at `output_dir` instead of saving it locally\",\n+        action=\"store_true\",\n+        default=False,\n+    )\n+    parser.add_argument(\n+        \"--revision\",\n+        help=\"Revision of the model to download\",\n+        default=None,\n+    )\n+    parser.add_argument(\n+        \"--original_vlm_name_or_path\",\n+        help=\"Name or path of the original VLM backbone model\",\n+        default=None,\n+    )\n+    args = parser.parse_args()\n+\n+    convert_colqwen2_weights_to_hf(\n+        model_id=args.model_id,\n+        output_dir=args.output_dir,\n+        push_to_hub=args.push_to_hub,\n+        revision=args.revision,\n+        original_vlm_name_or_path=args.original_vlm_name_or_path,\n+    )"
        },
        {
            "sha": "c4fdc63567bea959d4ceabb399d2b4cf9d638a60",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "added",
            "additions": 268,
            "deletions": 0,
            "changes": 268,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -0,0 +1,268 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/colqwen2/modular_colqwen2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_colqwen2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from dataclasses import dataclass\n+from typing import List, Optional, Tuple, Union\n+\n+from torch import nn\n+\n+from transformers import AutoModelForImageTextToText\n+\n+from ...cache_utils import Cache\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, is_torch_available\n+from .configuration_colqwen2 import ColQwen2Config\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class ColQwen2PreTrainedModel(PreTrainedModel):\n+    config_class = ColQwen2Config\n+    base_model_prefix = \"model\"\n+    _no_split_modules = []\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_cache_class = True\n+\n+    def _init_weights(self, module):\n+        std = (\n+            self.config.initializer_range\n+            if hasattr(self.config, \"initializer_range\")\n+            else self.config.vlm_config.text_config.initializer_range\n+        )\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+@dataclass\n+class ColQwen2ForRetrievalOutput(ModelOutput):\n+    \"\"\"\n+    Base class for ColQwen2 embeddings output.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction).\n+        embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            The embeddings of the model.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    embeddings: Optional[torch.Tensor] = None\n+    past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Following the ColPali approach, ColQwen2 leverages VLMs to construct efficient multi-vector embeddings directly\n+    from document images (â€œscreenshotsâ€) for document retrieval. The model is trained to maximize the similarity\n+    between these document embeddings and the corresponding query embeddings, using the late interaction method\n+    introduced in ColBERT.\n+\n+    Using ColQwen2 removes the need for potentially complex and brittle layout recognition and OCR pipelines with\n+    a single model that can take into account both the textual and visual content (layout, charts, ...) of a document.\n+\n+    ColQwen2 is part of the ColVision model family, which was introduced with ColPali in the following paper:\n+    [*ColPali: Efficient Document Retrieval with Vision Language Models*](https://arxiv.org/abs/2407.01449).\n+    \"\"\"\n+)\n+class ColQwen2ForRetrieval(ColQwen2PreTrainedModel):\n+    def __init__(self, config: ColQwen2Config):\n+        super().__init__(config)\n+        self.config = config\n+        self.vocab_size = config.vlm_config.text_config.vocab_size\n+\n+        vlm = AutoModelForImageTextToText.from_config(config.vlm_config)\n+        if vlm._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"vlm.{k}\" for k in vlm._tied_weights_keys]\n+        self.vlm = vlm\n+\n+        self.embedding_dim = self.config.embedding_dim\n+        self.embedding_proj_layer = nn.Linear(\n+            self.config.vlm_config.text_config.hidden_size,\n+            self.embedding_dim,\n+        )\n+\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> ColQwen2ForRetrievalOutput:\n+        r\"\"\"\n+        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each image in LLM.\n+        \"\"\"\n+        if pixel_values is not None:\n+            pixel_values = pixel_values.to(dtype=self.dtype)  # (batch_size, max_num_patches, pixel_values)\n+\n+        # Handle the custom \"pixel_values\" input obtained with `ColQwen2Processor` through unpadding\n+        if pixel_values is not None and image_grid_thw is not None:\n+            # NOTE: image_grid_thw: (batch_size, 3) where image_grid_thw[i] = (num_patches_h, num_patches_w, temporal_patch_size)\n+            offsets = image_grid_thw[:, 1] * image_grid_thw[:, 2]  # (num_patches_h, num_patches_w)\n+            pixel_values = torch.cat(\n+                [pixel_sequence[:offset] for pixel_sequence, offset in zip(pixel_values, offsets)],\n+                dim=0,\n+            )  # (num_patches_h * num_patches_w, pixel_values)\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        position_ids, rope_deltas = self.vlm.model.get_rope_index(\n+            input_ids=input_ids,\n+            image_grid_thw=image_grid_thw,\n+            video_grid_thw=None,\n+            attention_mask=attention_mask,\n+        )\n+\n+        # Custom data preparation to fix an issue with the gradient flow when training with multiple GPUs.\n+        if inputs_embeds is None:\n+            inputs_embeds = self.vlm.model.language_model.embed_tokens(input_ids)\n+\n+            if pixel_values is not None:\n+                pixel_values = pixel_values.type(self.vlm.visual.get_dtype())\n+                image_embeds = self.vlm.visual(pixel_values, grid_thw=image_grid_thw)\n+                image_mask = (\n+                    (input_ids == self.config.vlm_config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n+                )\n+                image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+                inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n+\n+            if attention_mask is not None:\n+                attention_mask = attention_mask.to(inputs_embeds.device)\n+\n+        vlm_output = self.vlm.model(\n+            input_ids=None,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+        )\n+\n+        vlm_hidden_states = vlm_output.hidden_states if output_hidden_states else None\n+\n+        last_hidden_states = vlm_output[0]  # (batch_size, sequence_length, hidden_size)\n+        embeddings = self.embedding_proj_layer(last_hidden_states)  # (batch_size, sequence_length, dim)\n+\n+        # L2 normalization\n+        embeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)  # (batch_size, sequence_length, dim)\n+        if attention_mask is not None:\n+            embeddings = embeddings * attention_mask.unsqueeze(-1)  # (batch_size, sequence_length, dim)\n+\n+        return ColQwen2ForRetrievalOutput(\n+            embeddings=embeddings,\n+            past_key_values=vlm_output.past_key_values,\n+            hidden_states=vlm_hidden_states,\n+            attentions=vlm_output.attentions,\n+        )\n+\n+    def get_input_embeddings(self):\n+        return self.vlm.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.vlm.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.vlm.get_output_embeddings()\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.vlm.set_output_embeddings(new_embeddings)\n+\n+    def set_decoder(self, decoder):\n+        self.vlm.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.vlm.get_decoder()\n+\n+    def tie_weights(self):\n+        return self.vlm.tie_weights()\n+\n+    def resize_token_embeddings(\n+        self,\n+        new_num_tokens: Optional[int] = None,\n+        pad_to_multiple_of: Optional[int] = None,\n+        mean_resizing: bool = True,\n+    ) -> nn.Embedding:\n+        model_embeds = self.vlm.resize_token_embeddings(\n+            new_num_tokens=new_num_tokens,\n+            pad_to_multiple_of=pad_to_multiple_of,\n+            mean_resizing=mean_resizing,\n+        )\n+\n+        self.config.vlm_config.text_config.vocab_size = model_embeds.num_embeddings\n+        self.config.vlm_config.vocab_size = model_embeds.num_embeddings\n+        self.vlm.vocab_size = model_embeds.num_embeddings\n+        self.vocab_size = model_embeds.num_embeddings\n+\n+        return model_embeds\n+\n+\n+__all__ = [\"ColQwen2ForRetrieval\", \"ColQwen2PreTrainedModel\"]"
        },
        {
            "sha": "43c4cc5308dd643ccac03ee52a29df2932473770",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "added",
            "additions": 383,
            "deletions": 0,
            "changes": 383,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -0,0 +1,383 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from dataclasses import dataclass\n+from typing import List, Optional, Tuple, Union\n+\n+from transformers.models.colpali.modeling_colpali import ColPaliForRetrieval, ColPaliPreTrainedModel\n+from transformers.models.colpali.processing_colpali import ColPaliProcessor\n+\n+from ...cache_utils import Cache\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ImageInput, is_valid_image\n+from ...processing_utils import ProcessingKwargs, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, is_torch_available, logging\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class ColQwen2ProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": \"longest\",\n+        },\n+        \"images_kwargs\": {\n+            \"data_format\": \"channels_first\",\n+            \"do_convert_rgb\": True,\n+        },\n+        \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+    }\n+\n+\n+class ColQwen2Processor(ColPaliProcessor):\n+    r\"\"\"\n+    Constructs a ColQwen2 processor which wraps a Qwen2VLProcessor and special methods to process images and queries, as\n+    well as to compute the late-interaction retrieval score.\n+\n+    [`ColQwen2Processor`] offers all the functionalities of [`Qwen2VLProcessor`]. See the [`~Qwen2VLProcessor.__call__`]\n+    for more information.\n+\n+    Args:\n+        image_processor ([`Qwen2VLImageProcessor`], *optional*):\n+            The image processor is a required input.\n+        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n+            The tokenizer is a required input.\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+        visual_prompt_prefix (`str`, *optional*): A string that gets tokenized and prepended to the image tokens.\n+        query_prefix (`str`, *optional*): A prefix to be used for the query.\n+    \"\"\"\n+\n+    image_processor_class = \"Qwen2VLImageProcessor\"\n+    tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n+\n+    def __init__(\n+        self,\n+        image_processor=None,\n+        tokenizer=None,\n+        chat_template=None,\n+        visual_prompt_prefix: Optional[str] = None,\n+        query_prefix: Optional[str] = None,\n+        **kwargs,\n+    ):\n+        ColPaliProcessor().__init__(image_processor, tokenizer, chat_template=chat_template)\n+        self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n+        self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n+\n+        if visual_prompt_prefix is None:\n+            visual_prompt_prefix = \"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe the image.<|im_end|><|endoftext|>\"\n+        self.visual_prompt_prefix = visual_prompt_prefix\n+\n+        if query_prefix is None:\n+            query_prefix = \"Query: \"\n+        self.query_prefix = query_prefix\n+\n+    def __call__(\n+        self,\n+        images: ImageInput = None,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[ColQwen2ProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model either (1) one or several texts, either (2) one or several image(s). This method is a custom\n+        wrapper around the Qwen2VLProcessor's [`~Qwen2VLProcessor.__call__`] method adapted for the ColQwen2 model. It cannot process\n+        both text and images at the same time.\n+\n+        When preparing the the text(s), this method forwards the `text` and `kwargs` arguments to Qwen2TokenizerFast's\n+        [`~Qwen2TokenizerFast.__call__`].\n+        When preparing the the image(s), this method forwards the `images` and `kwargs` arguments to Qwen2VLImageProcessor's\n+        [`~Qwen2VLImageProcessor.__call__`].\n+        Please refer to the doctsring of the above two methods for more information.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n+                number of channels, H and W are image height and width.\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            ColQwen2ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+        suffix = output_kwargs[\"text_kwargs\"].pop(\"suffix\", None)\n+\n+        return_token_type_ids = True if suffix is not None else False\n+\n+        if text is None and images is None:\n+            raise ValueError(\"Either text or images must be provided\")\n+        if text is not None and images is not None:\n+            raise ValueError(\"Only one of text or images can be processed at a time\")\n+\n+        if images is not None:\n+            if is_valid_image(images):\n+                images = [images]\n+            elif isinstance(images, list) and is_valid_image(images[0]):\n+                pass\n+            elif not (isinstance(images, list) and isinstance(images[0], list) and is_valid_image(images[0][0])):\n+                raise ValueError(\"images must be an image, list of images or list of list of images\")\n+\n+            texts_doc = [self.visual_prompt_prefix] * len(images)\n+\n+            image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n+            image_grid_thw = image_inputs[\"image_grid_thw\"]\n+\n+            if image_grid_thw is not None:\n+                merge_length = self.image_processor.merge_size**2\n+                index = 0\n+                for i in range(len(texts_doc)):\n+                    while self.image_token in texts_doc[i]:\n+                        texts_doc[i] = texts_doc[i].replace(\n+                            self.image_token, \"<|placeholder|>\" * (image_grid_thw[index].prod() // merge_length), 1\n+                        )\n+                        index += 1\n+                    texts_doc[i] = texts_doc[i].replace(\"<|placeholder|>\", self.image_token)\n+\n+            text_inputs = self.tokenizer(\n+                texts_doc,\n+                return_token_type_ids=False,\n+                **output_kwargs[\"text_kwargs\"],\n+            )\n+\n+            return_data = BatchFeature(data={**text_inputs, **image_inputs})\n+\n+            # NOTE: The following adjustment ensures correct behavior with DDP on multiple GPUs.\n+            offsets = return_data[\"image_grid_thw\"][:, 1] * return_data[\"image_grid_thw\"][:, 2]  # (batch_size,)\n+\n+            # Split the pixel_values tensor into a list of tensors, one per image\n+            pixel_values = list(\n+                torch.split(return_data[\"pixel_values\"], offsets.tolist())\n+            )  # [(num_patches_image_0, pixel_values), ..., (num_patches_image_n, pixel_values)]\n+\n+            # Pad the list of pixel_value tensors to the same length along the sequence dimension\n+            return_data[\"pixel_values\"] = torch.nn.utils.rnn.pad_sequence(\n+                pixel_values, batch_first=True\n+            )  # (batch_size, max_num_patches, pixel_values)\n+\n+            if return_token_type_ids:\n+                labels = return_data[\"input_ids\"].masked_fill(return_data[\"token_type_ids\"] == 0, -100)\n+                return_data.update({\"labels\": labels})\n+\n+            return return_data\n+\n+        elif text is not None:\n+            if isinstance(text, str):\n+                text = [text]\n+            elif not (isinstance(text, list) and isinstance(text[0], str)):\n+                raise ValueError(\"Text must be a string or a list of strings\")\n+\n+            if suffix is None:\n+                suffix = self.query_augmentation_token * 10\n+\n+            texts_query: List[str] = []\n+\n+            for query in text:\n+                augmented_query = self.query_prefix + query + suffix\n+                texts_query.append(augmented_query)\n+\n+            batch_query = self.tokenizer(\n+                texts_query,\n+                return_token_type_ids=False,\n+                **output_kwargs[\"text_kwargs\"],\n+            )\n+\n+            return batch_query\n+\n+\n+class ColQwen2PreTrainedModel(ColPaliPreTrainedModel):\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_cache_class = True\n+\n+\n+@dataclass\n+class ColQwen2ForRetrievalOutput(ModelOutput):\n+    \"\"\"\n+    Base class for ColQwen2 embeddings output.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction).\n+        embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            The embeddings of the model.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    embeddings: Optional[torch.Tensor] = None\n+    past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Following the ColPali approach, ColQwen2 leverages VLMs to construct efficient multi-vector embeddings directly\n+    from document images (â€œscreenshotsâ€) for document retrieval. The model is trained to maximize the similarity\n+    between these document embeddings and the corresponding query embeddings, using the late interaction method\n+    introduced in ColBERT.\n+\n+    Using ColQwen2 removes the need for potentially complex and brittle layout recognition and OCR pipelines with\n+    a single model that can take into account both the textual and visual content (layout, charts, ...) of a document.\n+\n+    ColQwen2 is part of the ColVision model family, which was introduced with ColPali in the following paper:\n+    [*ColPali: Efficient Document Retrieval with Vision Language Models*](https://arxiv.org/abs/2407.01449).\n+    \"\"\"\n+)\n+class ColQwen2ForRetrieval(ColPaliForRetrieval):\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> ColQwen2ForRetrievalOutput:\n+        r\"\"\"\n+        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each image in LLM.\n+        \"\"\"\n+        if pixel_values is not None:\n+            pixel_values = pixel_values.to(dtype=self.dtype)  # (batch_size, max_num_patches, pixel_values)\n+\n+        # Handle the custom \"pixel_values\" input obtained with `ColQwen2Processor` through unpadding\n+        if pixel_values is not None and image_grid_thw is not None:\n+            # NOTE: image_grid_thw: (batch_size, 3) where image_grid_thw[i] = (num_patches_h, num_patches_w, temporal_patch_size)\n+            offsets = image_grid_thw[:, 1] * image_grid_thw[:, 2]  # (num_patches_h, num_patches_w)\n+            pixel_values = torch.cat(\n+                [pixel_sequence[:offset] for pixel_sequence, offset in zip(pixel_values, offsets)],\n+                dim=0,\n+            )  # (num_patches_h * num_patches_w, pixel_values)\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        position_ids, rope_deltas = self.vlm.model.get_rope_index(\n+            input_ids=input_ids,\n+            image_grid_thw=image_grid_thw,\n+            video_grid_thw=None,\n+            attention_mask=attention_mask,\n+        )\n+\n+        # Custom data preparation to fix an issue with the gradient flow when training with multiple GPUs.\n+        if inputs_embeds is None:\n+            inputs_embeds = self.vlm.model.language_model.embed_tokens(input_ids)\n+\n+            if pixel_values is not None:\n+                pixel_values = pixel_values.type(self.vlm.visual.get_dtype())\n+                image_embeds = self.vlm.visual(pixel_values, grid_thw=image_grid_thw)\n+                image_mask = (\n+                    (input_ids == self.config.vlm_config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n+                )\n+                image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+                inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n+\n+            if attention_mask is not None:\n+                attention_mask = attention_mask.to(inputs_embeds.device)\n+\n+        vlm_output = self.vlm.model(\n+            input_ids=None,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+        )\n+\n+        vlm_hidden_states = vlm_output.hidden_states if output_hidden_states else None\n+\n+        last_hidden_states = vlm_output[0]  # (batch_size, sequence_length, hidden_size)\n+        embeddings = self.embedding_proj_layer(last_hidden_states)  # (batch_size, sequence_length, dim)\n+\n+        # L2 normalization\n+        embeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)  # (batch_size, sequence_length, dim)\n+        if attention_mask is not None:\n+            embeddings = embeddings * attention_mask.unsqueeze(-1)  # (batch_size, sequence_length, dim)\n+\n+        return ColQwen2ForRetrievalOutput(\n+            embeddings=embeddings,\n+            past_key_values=vlm_output.past_key_values,\n+            hidden_states=vlm_hidden_states,\n+            attentions=vlm_output.attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"ColQwen2ForRetrieval\",\n+    \"ColQwen2PreTrainedModel\",\n+    \"ColQwen2Processor\",\n+]"
        },
        {
            "sha": "30e2d8882e9b8fe4e9ba6adc86105dee41122474",
            "filename": "src/transformers/models/colqwen2/processing_colqwen2.py",
            "status": "added",
            "additions": 408,
            "deletions": 0,
            "changes": 408,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -0,0 +1,408 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/colqwen2/modular_colqwen2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_colqwen2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import List, Optional, Union\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ImageInput, is_valid_image\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class ColQwen2ProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": \"longest\",\n+        },\n+        \"images_kwargs\": {\n+            \"data_format\": \"channels_first\",\n+            \"do_convert_rgb\": True,\n+        },\n+        \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+    }\n+\n+\n+class ColQwen2Processor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a ColQwen2 processor which wraps a Qwen2VLProcessor and special methods to process images and queries, as\n+    well as to compute the late-interaction retrieval score.\n+\n+    [`ColQwen2Processor`] offers all the functionalities of [`Qwen2VLProcessor`]. See the [`~Qwen2VLProcessor.__call__`]\n+    for more information.\n+\n+    Args:\n+        image_processor ([`Qwen2VLImageProcessor`], *optional*):\n+            The image processor is a required input.\n+        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n+            The tokenizer is a required input.\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+        visual_prompt_prefix (`str`, *optional*): A string that gets tokenized and prepended to the image tokens.\n+        query_prefix (`str`, *optional*): A prefix to be used for the query.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+\n+    image_processor_class = \"Qwen2VLImageProcessor\"\n+    tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n+\n+    def __init__(\n+        self,\n+        image_processor=None,\n+        tokenizer=None,\n+        chat_template=None,\n+        visual_prompt_prefix: Optional[str] = None,\n+        query_prefix: Optional[str] = None,\n+        **kwargs,\n+    ):\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n+        self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n+        self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n+\n+        if visual_prompt_prefix is None:\n+            visual_prompt_prefix = \"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe the image.<|im_end|><|endoftext|>\"\n+        self.visual_prompt_prefix = visual_prompt_prefix\n+\n+        if query_prefix is None:\n+            query_prefix = \"Query: \"\n+        self.query_prefix = query_prefix\n+\n+    def __call__(\n+        self,\n+        images: ImageInput = None,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[ColQwen2ProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model either (1) one or several texts, either (2) one or several image(s). This method is a custom\n+        wrapper around the Qwen2VLProcessor's [`~Qwen2VLProcessor.__call__`] method adapted for the ColQwen2 model. It cannot process\n+        both text and images at the same time.\n+\n+        When preparing the the text(s), this method forwards the `text` and `kwargs` arguments to Qwen2TokenizerFast's\n+        [`~Qwen2TokenizerFast.__call__`].\n+        When preparing the the image(s), this method forwards the `images` and `kwargs` arguments to Qwen2VLImageProcessor's\n+        [`~Qwen2VLImageProcessor.__call__`].\n+        Please refer to the doctsring of the above two methods for more information.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n+                number of channels, H and W are image height and width.\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            ColQwen2ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+        suffix = output_kwargs[\"text_kwargs\"].pop(\"suffix\", None)\n+\n+        return_token_type_ids = True if suffix is not None else False\n+\n+        if text is None and images is None:\n+            raise ValueError(\"Either text or images must be provided\")\n+        if text is not None and images is not None:\n+            raise ValueError(\"Only one of text or images can be processed at a time\")\n+\n+        if images is not None:\n+            if is_valid_image(images):\n+                images = [images]\n+            elif isinstance(images, list) and is_valid_image(images[0]):\n+                pass\n+            elif not (isinstance(images, list) and isinstance(images[0], list) and is_valid_image(images[0][0])):\n+                raise ValueError(\"images must be an image, list of images or list of list of images\")\n+\n+            texts_doc = [self.visual_prompt_prefix] * len(images)\n+\n+            image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n+            image_grid_thw = image_inputs[\"image_grid_thw\"]\n+\n+            if image_grid_thw is not None:\n+                merge_length = self.image_processor.merge_size**2\n+                index = 0\n+                for i in range(len(texts_doc)):\n+                    while self.image_token in texts_doc[i]:\n+                        texts_doc[i] = texts_doc[i].replace(\n+                            self.image_token, \"<|placeholder|>\" * (image_grid_thw[index].prod() // merge_length), 1\n+                        )\n+                        index += 1\n+                    texts_doc[i] = texts_doc[i].replace(\"<|placeholder|>\", self.image_token)\n+\n+            text_inputs = self.tokenizer(\n+                texts_doc,\n+                return_token_type_ids=False,\n+                **output_kwargs[\"text_kwargs\"],\n+            )\n+\n+            return_data = BatchFeature(data={**text_inputs, **image_inputs})\n+\n+            # NOTE: The following adjustment ensures correct behavior with DDP on multiple GPUs.\n+            offsets = return_data[\"image_grid_thw\"][:, 1] * return_data[\"image_grid_thw\"][:, 2]  # (batch_size,)\n+\n+            # Split the pixel_values tensor into a list of tensors, one per image\n+            pixel_values = list(\n+                torch.split(return_data[\"pixel_values\"], offsets.tolist())\n+            )  # [(num_patches_image_0, pixel_values), ..., (num_patches_image_n, pixel_values)]\n+\n+            # Pad the list of pixel_value tensors to the same length along the sequence dimension\n+            return_data[\"pixel_values\"] = torch.nn.utils.rnn.pad_sequence(\n+                pixel_values, batch_first=True\n+            )  # (batch_size, max_num_patches, pixel_values)\n+\n+            if return_token_type_ids:\n+                labels = return_data[\"input_ids\"].masked_fill(return_data[\"token_type_ids\"] == 0, -100)\n+                return_data.update({\"labels\": labels})\n+\n+            return return_data\n+\n+        elif text is not None:\n+            if isinstance(text, str):\n+                text = [text]\n+            elif not (isinstance(text, list) and isinstance(text[0], str)):\n+                raise ValueError(\"Text must be a string or a list of strings\")\n+\n+            if suffix is None:\n+                suffix = self.query_augmentation_token * 10\n+\n+            texts_query: List[str] = []\n+\n+            for query in text:\n+                augmented_query = self.query_prefix + query + suffix\n+                texts_query.append(augmented_query)\n+\n+            batch_query = self.tokenizer(\n+                texts_query,\n+                return_token_type_ids=False,\n+                **output_kwargs[\"text_kwargs\"],\n+            )\n+\n+            return batch_query\n+\n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+\n+        Args:\n+            image_sizes (List[List[str]], *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+        Returns:\n+            Dict[str, List[int]]: A dictionary mapping each modality (\"image\", \"video\", \"audio\")\n+            to a list containing the number of placeholder tokens required. If the model doesn't accept\n+            a certain modality or no input sizes are provided, the dict value is set to an empty list.\n+        \"\"\"\n+        vision_data = {}\n+        if image_sizes is not None:\n+            num_image_tokens = [self.image_seq_length] * len(image_sizes)\n+            num_image_patches = [1] * len(image_sizes)\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+        return MultiModalData(**vision_data)\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+    @property\n+    def query_augmentation_token(self) -> str:\n+        \"\"\"\n+        Return the query augmentation token.\n+\n+        Query augmentation buffers are used as reasoning buffers during inference.\n+        \"\"\"\n+        return self.tokenizer.pad_token\n+\n+    def process_images(\n+        self,\n+        images: ImageInput = None,\n+        **kwargs: Unpack[ColQwen2ProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Prepare for the model one or several image(s). This method is a wrapper around the `__call__` method of the ColQwen2Processor's\n+        [`ColQwen2Processor.__call__`].\n+\n+        This method forwards the `images` and `kwargs` arguments to the image processor.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n+                number of channels, H and W are image height and width.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+        return self.__call__(images=images, **kwargs)\n+\n+    def process_queries(\n+        self,\n+        text: Union[TextInput, List[TextInput]],\n+        **kwargs: Unpack[ColQwen2ProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Prepare for the model one or several texts. This method is a wrapper around the `__call__` method of the ColQwen2Processor's\n+        [`ColQwen2Processor.__call__`].\n+\n+        This method forwards the `text` and `kwargs` arguments to the tokenizer.\n+\n+        Args:\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+        \"\"\"\n+        return self.__call__(text=text, **kwargs)\n+\n+    def score_retrieval(\n+        self,\n+        query_embeddings: Union[\"torch.Tensor\", List[\"torch.Tensor\"]],\n+        passage_embeddings: Union[\"torch.Tensor\", List[\"torch.Tensor\"]],\n+        batch_size: int = 128,\n+        output_dtype: Optional[\"torch.dtype\"] = None,\n+        output_device: Union[\"torch.device\", str] = \"cpu\",\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Compute the late-interaction/MaxSim score (ColBERT-like) for the given multi-vector\n+        query embeddings (`qs`) and passage embeddings (`ps`). For ColQwen2, a passage is the\n+        image of a document page.\n+\n+        Because the embedding tensors are multi-vector and can thus have different shapes, they\n+        should be fed as:\n+        (1) a list of tensors, where the i-th tensor is of shape (sequence_length_i, embedding_dim)\n+        (2) a single tensor of shape (n_passages, max_sequence_length, embedding_dim) -> usually\n+            obtained by padding the list of tensors.\n+\n+        Args:\n+            query_embeddings (`Union[torch.Tensor, List[torch.Tensor]`): Query embeddings.\n+            passage_embeddings (`Union[torch.Tensor, List[torch.Tensor]`): Passage embeddings.\n+            batch_size (`int`, *optional*, defaults to 128): Batch size for computing scores.\n+            output_dtype (`torch.dtype`, *optional*, defaults to `torch.float32`): The dtype of the output tensor.\n+                If `None`, the dtype of the input embeddings is used.\n+            output_device (`torch.device` or `str`, *optional*, defaults to \"cpu\"): The device of the output tensor.\n+\n+        Returns:\n+            `torch.Tensor`: A tensor of shape `(n_queries, n_passages)` containing the scores. The score\n+            tensor is saved on the \"cpu\" device.\n+        \"\"\"\n+\n+        if len(query_embeddings) == 0:\n+            raise ValueError(\"No queries provided\")\n+        if len(passage_embeddings) == 0:\n+            raise ValueError(\"No passages provided\")\n+\n+        if query_embeddings[0].device != passage_embeddings[0].device:\n+            raise ValueError(\"Queries and passages must be on the same device\")\n+\n+        if query_embeddings[0].dtype != passage_embeddings[0].dtype:\n+            raise ValueError(\"Queries and passages must have the same dtype\")\n+\n+        if output_dtype is None:\n+            output_dtype = query_embeddings[0].dtype\n+\n+        scores: List[torch.Tensor] = []\n+\n+        for i in range(0, len(query_embeddings), batch_size):\n+            batch_scores: List[torch.Tensor] = []\n+            batch_queries = torch.nn.utils.rnn.pad_sequence(\n+                query_embeddings[i : i + batch_size], batch_first=True, padding_value=0\n+            )\n+            for j in range(0, len(passage_embeddings), batch_size):\n+                batch_passages = torch.nn.utils.rnn.pad_sequence(\n+                    passage_embeddings[j : j + batch_size], batch_first=True, padding_value=0\n+                )\n+                batch_scores.append(\n+                    torch.einsum(\"bnd,csd->bcns\", batch_queries, batch_passages).max(dim=3)[0].sum(dim=2)\n+                )\n+            scores.append(torch.cat(batch_scores, dim=1).to(output_dtype).to(output_device))\n+\n+        return torch.cat(scores, dim=0)\n+\n+\n+__all__ = [\"ColQwen2Processor\"]"
        },
        {
            "sha": "82844e3fd374d80f3ae1dc97d855a5808772a409",
            "filename": "tests/models/colpali/test_modeling_colpali.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -168,7 +168,6 @@ def prepare_config_and_inputs_for_common(self):\n             \"input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n             \"labels\": input_ids,\n-            \"token_type_ids\": torch.zeros_like(input_ids),\n         }\n         return config, inputs_dict\n \n@@ -333,7 +332,7 @@ def test_model_integration_test(self):\n         scores = self.processor.score_retrieval(\n             query_embeddings=query_embeddings,\n             passage_embeddings=image_embeddings,\n-        )  # (len(qs), len(ps))\n+        )  # (num_queries, num_passages)\n \n         assert scores.ndim == 2, f\"Expected 2D tensor, got {scores.ndim}\"\n         assert scores.shape == (len(ds), len(ds)), f\"Expected shape {(len(ds), len(ds))}, got {scores.shape}\""
        },
        {
            "sha": "7a517861587a2546a581edecb106c51d29e05783",
            "filename": "tests/models/colpali/test_processing_colpali.py",
            "status": "modified",
            "additions": 16,
            "deletions": 1,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -1,3 +1,18 @@\n+# Copyright 2024 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the ColPali processor.\"\"\"\n+\n import shutil\n import tempfile\n import unittest\n@@ -89,7 +104,7 @@ def test_process_queries(self):\n         self.assertIsInstance(batch_feature[\"input_ids\"], torch.Tensor)\n         self.assertEqual(batch_feature[\"input_ids\"].shape[0], len(queries))\n \n-        # The following tests are overwritten as ColPaliProcessor can only take one of images or text as input at a time\n+    # The following tests override the parent tests because ColPaliProcessor can only take one of images or text as input at a time.\n \n     def test_tokenizer_defaults_preserved_by_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/colqwen2/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/tests%2Fmodels%2Fcolqwen2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/tests%2Fmodels%2Fcolqwen2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2F__init__.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5"
        },
        {
            "sha": "302f468cd150387cdbfe23666c945da74ad79e45",
            "filename": "tests/models/colqwen2/test_modeling_colqwen2.py",
            "status": "added",
            "additions": 333,
            "deletions": 0,
            "changes": 333,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -0,0 +1,333 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch ColQwen2 model.\"\"\"\n+\n+import gc\n+import unittest\n+from typing import ClassVar\n+\n+import torch\n+from datasets import load_dataset\n+\n+from tests.test_configuration_common import ConfigTester\n+from tests.test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from transformers import is_torch_available\n+from transformers.models.colqwen2.configuration_colqwen2 import ColQwen2Config\n+from transformers.models.colqwen2.modeling_colqwen2 import ColQwen2ForRetrieval, ColQwen2ForRetrievalOutput\n+from transformers.models.colqwen2.processing_colqwen2 import ColQwen2Processor\n+from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class ColQwen2ForRetrievalModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        ignore_index=-100,\n+        pad_token_id=2,\n+        projector_hidden_act=\"gelu\",\n+        seq_length=11,\n+        vision_feature_select_strategy=\"default\",\n+        vision_feature_layer=-1,\n+        projection_dim=32,\n+        is_training=False,\n+        use_cache=False,\n+        vlm_config={\n+            \"_name_or_path\": \"Qwen/Qwen2-VL-2B-Instruct\",\n+            \"bos_token_id\": 0,\n+            \"eos_token_id\": 1,\n+            \"vision_start_token_id\": 3,\n+            \"image_token_id\": 4,\n+            \"video_token_id\": 5,\n+            \"hidden_size\": 64,\n+            \"intermediate_size\": 2,\n+            \"max_window_layers\": 2,\n+            \"model_type\": \"qwen2_vl\",\n+            \"num_attention_heads\": 2,\n+            \"num_hidden_layers\": 2,\n+            \"num_key_value_heads\": 2,\n+            \"rms_norm_eps\": 1e-06,\n+            \"rope_scaling\": {\"mrope_section\": [4, 6, 6], \"rope_type\": \"default\", \"type\": \"default\"},\n+            \"sliding_window\": 32768,\n+            \"tie_word_embeddings\": True,\n+            \"vision_config\": {\n+                \"depth\": 2,\n+                \"embed_dim\": 32,\n+                \"hidden_act\": \"quick_gelu\",\n+                \"hidden_size\": 64,\n+                \"mlp_ratio\": 4,\n+                \"num_heads\": 4,\n+                \"patch_size\": 14,\n+                \"in_chans\": 3,\n+                \"spatial_merge_size\": 1,\n+                \"temporal_patch_size\": 2,\n+            },\n+            \"vision_end_token_id\": 151653,\n+            \"vision_token_id\": 151654,\n+            \"vocab_size\": 99,\n+        },\n+        embedding_dim=32,\n+        initializer_range=0.02,\n+    ):\n+        self.parent = parent\n+        self.ignore_index = ignore_index\n+        self.pad_token_id = pad_token_id\n+\n+        # `image_token_index` is set to 0 to pass \"resize_embeddings\" test, do not modify\n+        self.image_token_index = 0\n+\n+        self.image_token_id = vlm_config[\"image_token_id\"]\n+        self.video_token_id = vlm_config[\"video_token_id\"]\n+        self.pad_token_id = vlm_config[\"eos_token_id\"]\n+        self.vision_start_token_id = vlm_config[\"vision_start_token_id\"]\n+        self.projector_hidden_act = projector_hidden_act\n+        self.vision_feature_select_strategy = vision_feature_select_strategy\n+        self.vision_feature_layer = vision_feature_layer\n+\n+        self.image_size = 56\n+        self.num_image_tokens = 4\n+\n+        self.seq_length = seq_length + self.num_image_tokens\n+        self.projection_dim = projection_dim\n+\n+        self.num_hidden_layers = vlm_config[\"num_hidden_layers\"]\n+        self.vocab_size = vlm_config[\"vocab_size\"]\n+        self.hidden_size = vlm_config[\"hidden_size\"]\n+        self.num_attention_heads = vlm_config[\"num_attention_heads\"]\n+        self.is_training = is_training\n+\n+        self.batch_size = 3\n+        self.num_channels = vlm_config[\"vision_config\"][\"in_chans\"]\n+\n+        self.encoder_seq_length = self.seq_length\n+        self.use_cache = use_cache\n+\n+        self.vlm_config = vlm_config\n+        self.embedding_dim = embedding_dim\n+        self.initializer_range = initializer_range\n+\n+    def get_config(self):\n+        return ColQwen2Config(\n+            vlm_config=self.vlm_config,\n+            embedding_dim=self.embedding_dim,\n+            initializer_range=self.initializer_range,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+        patch_size = config.vlm_config.vision_config.patch_size\n+        temporal_patch_size = config.vlm_config.vision_config.temporal_patch_size\n+\n+        # NOTE: Assume all inputs are square images of the same size.\n+        num_patches = (self.image_size // patch_size) ** 2\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size * num_patches,\n+                self.num_channels * (patch_size**2) * temporal_patch_size,\n+            ]\n+        )\n+\n+        # Hardcoded image grid size: do not change unless you modified image size or patch size!\n+        image_grid_thw = torch.tensor([1, 4, 4]).repeat(self.batch_size, 1)\n+\n+        # NOTE: The following adjustment ensures correct behavior with DDP on multiple GPUs.\n+        # Line is copied from `src/transformers/models/colqwen2/processing_colqwen2.py`\n+        offsets = image_grid_thw[:, 1] * image_grid_thw[:, 2]  # (batch_size,)\n+        pixel_values = list(\n+            torch.split(pixel_values, offsets.tolist())\n+        )  # [(num_patches_image_0, pixel_values), ..., (num_patches_image_n, pixel_values)]\n+        pixel_values = torch.nn.utils.rnn.pad_sequence(\n+            pixel_values, batch_first=True\n+        )  # (batch_size, max_num_patches, pixel_values)\n+\n+        return config, pixel_values, image_grid_thw\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values, image_grid_thw = config_and_inputs\n+        input_ids = (\n+            ids_tensor(\n+                shape=[self.batch_size, self.seq_length],\n+                vocab_size=config.vlm_config.vocab_size - 1,\n+            )\n+            + 1\n+        )\n+        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n+\n+        input_ids[:, -1] = self.pad_token_id\n+        input_ids[:, : self.num_image_tokens] = self.image_token_id\n+        input_ids[input_ids == self.video_token_id] = self.pad_token_id\n+        input_ids[input_ids == self.image_token_id] = self.pad_token_id\n+        input_ids[input_ids == self.vision_start_token_id] = self.pad_token_id\n+\n+        inputs_dict = {\n+            \"input_ids\": input_ids,\n+            \"pixel_values\": pixel_values,\n+            \"image_grid_thw\": image_grid_thw,\n+            \"attention_mask\": attention_mask,\n+            \"labels\": input_ids,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class ColQwen2ForRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Model tester for `ColQwen2ForRetrieval`.\n+    \"\"\"\n+\n+    all_model_classes = (ColQwen2ForRetrieval,) if is_torch_available() else ()\n+    fx_compatible = False\n+    test_torchscript = False\n+    test_pruning = False\n+    test_resize_embeddings = True\n+    test_head_masking = False\n+\n+    def setUp(self):\n+        self.model_tester = ColQwen2ForRetrievalModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=ColQwen2Config, has_text_modality=False)\n+\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            wte = model.get_input_embeddings()\n+            inputs[\"inputs_embeds\"] = wte(input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    # while some other models require pixel_values to be present\n+    def test_inputs_embeds_matches_input_ids(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+\n+            with torch.no_grad():\n+                out_ids = model(input_ids=input_ids, **inputs)[0]\n+                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n+            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+\n+    @slow\n+    @require_vision\n+    def test_colqwen2_forward_inputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            with torch.no_grad():\n+                outputs = model(**inputs, return_dict=True)\n+\n+            self.assertIsInstance(outputs, ColQwen2ForRetrievalOutput)\n+\n+    @unittest.skip(reason=\"Some undefined behavior encountered with test versions of Qwen2-VL. Skip for now.\")\n+    def test_model_parallelism(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Pass because ColQwen2 requires `attention_mask is not None`\")\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Pass because ColQwen2 requires `attention_mask is not None`\")\n+    def test_sdpa_can_compile_dynamic(self):\n+        pass\n+\n+\n+@require_torch\n+class ColQwen2ModelIntegrationTest(unittest.TestCase):\n+    model_name: ClassVar[str] = \"vidore/colqwen2-v1.0-hf\"\n+\n+    def setUp(self):\n+        self.processor = ColQwen2Processor.from_pretrained(self.model_name)\n+\n+    def tearDown(self):\n+        gc.collect()\n+        torch.cuda.empty_cache()\n+\n+    @slow\n+    def test_model_integration_test(self):\n+        \"\"\"\n+        Test if the model is able to retrieve the correct pages for a small and easy dataset.\n+        \"\"\"\n+        model = ColQwen2ForRetrieval.from_pretrained(\n+            self.model_name,\n+            torch_dtype=torch.bfloat16,\n+            device_map=torch_device,\n+        ).eval()\n+\n+        # Load the test dataset\n+        ds = load_dataset(\"hf-internal-testing/document-visual-retrieval-test\", split=\"test\")\n+\n+        # Preprocess the examples\n+        batch_images = self.processor(images=ds[\"image\"]).to(torch_device)\n+        batch_queries = self.processor(text=ds[\"query\"]).to(torch_device)\n+\n+        # Run inference\n+        with torch.inference_mode():\n+            image_embeddings = model(**batch_images).embeddings\n+            query_embeddings = model(**batch_queries).embeddings\n+\n+        # Compute retrieval scores\n+        scores = self.processor.score_retrieval(\n+            query_embeddings=query_embeddings,\n+            passage_embeddings=image_embeddings,\n+        )  # (num_queries, num_passages)\n+\n+        assert scores.ndim == 2, f\"Expected 2D tensor, got {scores.ndim}\"\n+        assert scores.shape == (len(ds), len(ds)), f\"Expected shape {(len(ds), len(ds))}, got {scores.shape}\"\n+\n+        # Check if the maximum scores per row are in the diagonal of the matrix score\n+        self.assertTrue((scores.argmax(axis=1) == torch.arange(len(ds), device=scores.device)).all())\n+\n+        # Further validation: fine-grained check, with a hardcoded score from the original Hf implementation.\n+        expected_scores = torch.tensor(\n+            [\n+                [16.2500, 7.8750, 14.6875],\n+                [9.5000, 17.1250, 10.5000],\n+                [14.9375, 10.9375, 20.0000],\n+            ],\n+            dtype=scores.dtype,\n+        )\n+\n+        assert torch.allclose(scores, expected_scores, atol=1e-3), f\"Expected scores {expected_scores}, got {scores}\""
        },
        {
            "sha": "0da0ce86b42dddcf2a23b17c1738c20a45bbb70a",
            "filename": "tests/models/colqwen2/test_processing_colqwen2.py",
            "status": "added",
            "additions": 262,
            "deletions": 0,
            "changes": 262,
            "blob_url": "https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py?ref=c72ba6944171e2e6dd4f4a93d61b24fa52b718f5",
            "patch": "@@ -0,0 +1,262 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the ColQwen2 processor.\"\"\"\n+\n+import shutil\n+import tempfile\n+import unittest\n+\n+import torch\n+\n+from transformers import AutoProcessor, Qwen2VLProcessor\n+from transformers.models.colqwen2.processing_colqwen2 import ColQwen2Processor\n+from transformers.testing_utils import get_tests_dir, require_torch, require_vision\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import (\n+        ColQwen2Processor,\n+    )\n+\n+SAMPLE_VOCAB = get_tests_dir(\"fixtures/test_sentencepiece.model\")\n+\n+\n+@require_torch\n+@require_vision\n+class ColQwen2ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = ColQwen2Processor\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.tmpdirname = tempfile.mkdtemp()\n+        processor = Qwen2VLProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n+        processor.save_pretrained(cls.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        shutil.rmtree(cls.tmpdirname)\n+\n+    def test_process_images(self):\n+        # Processor configuration\n+        image_input = self.prepare_image_inputs()\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", max_length=112, padding=\"max_length\")\n+        image_processor.image_seq_length = 14\n+\n+        # Get the processor\n+        processor = self.processor_class(\n+            tokenizer=tokenizer,\n+            image_processor=image_processor,\n+        )\n+\n+        # Process the image\n+        batch_feature = processor.process_images(images=image_input, return_tensors=\"pt\")\n+\n+        # Assertions\n+        self.assertIn(\"pixel_values\", batch_feature)\n+        self.assertEqual(batch_feature[\"pixel_values\"].shape, torch.Size([1, 56, 1176]))\n+\n+    def test_process_queries(self):\n+        # Inputs\n+        queries = [\n+            \"Is attention really all you need?\",\n+            \"Are Benjamin, Antoine, Merve, and Jo best friends?\",\n+        ]\n+\n+        # Processor configuration\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", max_length=112, padding=\"max_length\")\n+        image_processor.image_seq_length = 14\n+\n+        # Get the processor\n+        processor = self.processor_class(\n+            tokenizer=tokenizer,\n+            image_processor=image_processor,\n+        )\n+\n+        # Process the image\n+        batch_feature = processor.process_queries(text=queries, return_tensors=\"pt\")\n+\n+        # Assertions\n+        self.assertIn(\"input_ids\", batch_feature)\n+        self.assertIsInstance(batch_feature[\"input_ids\"], torch.Tensor)\n+        self.assertEqual(batch_feature[\"input_ids\"].shape[0], len(queries))\n+\n+    # The following tests override the parent tests because ColQwen2Processor can only take one of images or text as input at a time.\n+\n+    def test_tokenizer_defaults_preserved_by_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(**processor_components)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = self.prepare_text_inputs()\n+        inputs = processor(text=input_str, return_tensors=\"pt\")\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 117)\n+\n+    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n+        \"\"\"\n+        We use do_rescale=True, rescale_factor=-1 to ensure that image_processor kwargs are preserved in the processor.\n+        We then check that the mean of the pixel_values is less than or equal to 0 after processing.\n+        Since the original pixel_values are in [0, 255], this is a good indicator that the rescale_factor is indeed applied.\n+        \"\"\"\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor_components[\"image_processor\"] = self.get_component(\n+            \"image_processor\", do_rescale=True, rescale_factor=-1\n+        )\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(**processor_components)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(images=image_input, return_tensors=\"pt\")\n+        self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n+\n+    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", padding=\"longest\")\n+\n+        processor = self.processor_class(**processor_components)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = self.prepare_text_inputs()\n+        inputs = processor(text=input_str, return_tensors=\"pt\", max_length=112, padding=\"max_length\")\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 112)\n+\n+    def test_kwargs_overrides_default_image_processor_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor_components[\"image_processor\"] = self.get_component(\n+            \"image_processor\", do_rescale=True, rescale_factor=1\n+        )\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(**processor_components)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(images=image_input, do_rescale=True, rescale_factor=-1, return_tensors=\"pt\")\n+        self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n+\n+    def test_unstructured_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor = self.processor_class(**processor_components)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs()\n+        inputs = processor(\n+            text=input_str,\n+            return_tensors=\"pt\",\n+            do_rescale=True,\n+            rescale_factor=-1,\n+            padding=\"max_length\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n+\n+    def test_unstructured_kwargs_batched(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor = self.processor_class(**processor_components)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        image_input = self.prepare_image_inputs(batch_size=2)\n+        inputs = processor(\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            do_rescale=True,\n+            rescale_factor=-1,\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+\n+        self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n+\n+    def test_doubly_passed_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor = self.processor_class(**processor_components)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        image_input = self.prepare_image_inputs()\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                images=image_input,\n+                images_kwargs={\"do_rescale\": True, \"rescale_factor\": -1},\n+                do_rescale=True,\n+                return_tensors=\"pt\",\n+            )\n+\n+    def test_structured_kwargs_nested(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor = self.processor_class(**processor_components)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, **all_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n+\n+    def test_structured_kwargs_nested_from_dict(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor = self.processor_class(**processor_components)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(images=image_input, **all_kwargs)\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 76)"
        }
    ],
    "stats": {
        "total": 2382,
        "additions": 2288,
        "deletions": 94
    }
}