{
    "author": "Cyrilvallez",
    "message": "Allow easy registration of custom attention functions (#36889)\n\n* Update modeling_utils.py\n\n* style\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* add to init\n\n* Update modeling_utils.py\n\n* style\n\n* update\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* style\n\n* Add some doc\n\n* Update _toctree.yml\n\n* readd it for tgi/vllm compat\n\n* CIs\n\n* CIs",
    "sha": "788e1092e90eacfd1afb14166a68574ef58cafa9",
    "files": [
        {
            "sha": "034ba00abcd3cecc2e0bab582d7a25d1b4f526e0",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/788e1092e90eacfd1afb14166a68574ef58cafa9/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/788e1092e90eacfd1afb14166a68574ef58cafa9/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=788e1092e90eacfd1afb14166a68574ef58cafa9",
            "patch": "@@ -29,6 +29,8 @@\n       title: The Transformer model family\n     - local: attention\n       title: Attention mechanisms\n+    - local: attention_interface\n+      title: Customizing attention function\n     title: Models\n   - sections:\n     - local: fast_tokenizers"
        },
        {
            "sha": "275a3d88e3aad95b498094cb8de62469fe0af79f",
            "filename": "docs/source/en/attention_interface.md",
            "status": "added",
            "additions": 106,
            "deletions": 0,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/788e1092e90eacfd1afb14166a68574ef58cafa9/docs%2Fsource%2Fen%2Fattention_interface.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/788e1092e90eacfd1afb14166a68574ef58cafa9/docs%2Fsource%2Fen%2Fattention_interface.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fattention_interface.md?ref=788e1092e90eacfd1afb14166a68574ef58cafa9",
            "patch": "@@ -0,0 +1,106 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Attention Interface\n+\n+This page describes how to use the `AttentionInterface` in order to register custom attention functions to use with\n+supported models.\n+\n+## Customizing attention function\n+\n+Most recent models can now switch from one attention function used in the Attention layer to the other, thanks to a simple mapping.\n+By default, we provide the implementation for [`sdpa`](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html),\n+[`flash_attention_2`](https://github.com/Dao-AILab/flash-attention) and [`flex_attention`](https://pytorch.org/docs/stable/nn.attention.flex_attention.html#module-torch.nn.attention.flex_attention)\n+as well as `eager`, which is simple matrix multiplication without any optimization on top.  \n+This is the setting you can usually choose when instantiating a model:\n+\n+```python\n+from transformers import AutoModelForCausalLM\n+\n+model_id = \"meta-llama/Llama-3.2-1B\n+\n+# Here, using flash attention as an example\n+model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=\"flash_attention_2\")\n+```\n+\n+But what if you wanted to create your own attention function? Or simply play around with existing ones, adding\n+a few statements here and there? You can now do so with the `AttentionInterface`! Here is an example:\n+\n+```python\n+from transformers import AutoModelForCausalLM, AttentionInterface\n+from transformers.integrations.sdpa_attention import sdpa_attention_forward\n+import torch\n+\n+model_id = \"meta-llama/Llama-3.2-1B\n+\n+def my_new_sdpa(*args, **kwargs):\n+    print(\"I just entered the attention computation\")\n+    return sdpa_attention_forward(*args, **kwargs)\n+\n+AttentionInterface.register(\"my_new_sdpa\", my_new_sdpa)\n+\n+model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=\"my_new_sdpa\")\n+# Try running the forward with the new attention function\n+model(torch.ones(1, 5, dtype=int))\n+```\n+\n+You will see it prints \"I just entered the attention computation\" as many times as there are layers in the model (with this example, 16 times.\n+\n+## Dynamically switching attention function\n+\n+You could dynamically change the model's attention function as well, by overriding the `config._attn_implementation` field:\n+\n+```python\n+# Back to use original sdpa implementation\n+model.config._attn_implementation = \"sdpa\"\n+\n+model(torch.ones(1, 5, dtype=int))\n+```\n+\n+and it will stop printing the statements, as it now uses the `sdpa` attention.  \n+This allows to quickly change attention function, without needing to reload the model!\n+\n+## What about new args needed in my custom function?\n+\n+But indeed, what if the new function requires a new arg to be properly used? It's no issue! Models supporting the\n+`AttentionInterface` propagates kwargs all the way to the Attention layers, and to the attention function used. That way,\n+you can simply pass the arg (as a kwargs, i.e. you need to qualify the name of the arg) in the model's forward, and it will be correctly used in the attention. However, custom attention functions have some limitations. In particular, it must follow the signature and return format of other attention functions, i.e.\n+\n+```python\n+from transformers import AutoModelForCausalLM, AttentionInterface\n+from transformers.integrations.sdpa_attention import sdpa_attention_forward\n+import torch\n+\n+def custom_attention(\n+    module: torch.nn.Module,  # required arg\n+    query: torch.Tensor,  # required arg\n+    key: torch.Tensor,  # required arg\n+    value: torch.Tensor,  # required arg\n+    attention_mask: Optional[torch.Tensor],  # required arg\n+    a_new_kwargs = None,  # You can now add as many kwargs as you need\n+    another_new_kwargs = None,  # You can now add as many kwargs as you need\n+    **kwargs,  # You need to accept **kwargs as models will pass other args\n+) -> Tuple[torch.Tensor, Optional[torch.Tensor]]\n+    ...  # do your magic!\n+    return attn_output, attn_weights  # attn_weights are optional here\n+\n+AttentionInterface.register(\"custom\", custom_attention)\n+\n+model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=\"custom\")\n+# Forward pass with the new kwargs\n+model(torch.ones(1, 5, dtype=int), a_new_kwargs=..., another_new_kwargs=...)\n+```\n+\n+If in doubt about what args/kwargs a given model sends to the attention function, simply check that model's modeling code on [GitHub](https://github.com/huggingface/transformers/tree/main/src/transformers/models)!\n\\ No newline at end of file"
        },
        {
            "sha": "2fccc772cfa1a1b76542f817c8f0739b0290597e",
            "filename": "docs/source/en/internal/modeling_utils.md",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/788e1092e90eacfd1afb14166a68574ef58cafa9/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/788e1092e90eacfd1afb14166a68574ef58cafa9/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md?ref=788e1092e90eacfd1afb14166a68574ef58cafa9",
            "patch": "@@ -16,10 +16,14 @@ rendered properly in your Markdown viewer.\n \n # Custom Layers and Utilities\n \n-This page lists all the custom layers used by the library, as well as the utility functions it provides for modeling.\n+This page lists all the custom layers used by the library, as well as the utility functions and classes it provides for modeling.\n \n Most of those are only useful if you are studying the code of the models in the library.\n \n+## Attention Functions\n+\n+[[autodoc]] AttentionInterface\n+    - register\n \n ## Pytorch custom modules\n "
        },
        {
            "sha": "2db51e4770717b68ac36d55bd13730fd831e9ec1",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/788e1092e90eacfd1afb14166a68574ef58cafa9/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/788e1092e90eacfd1afb14166a68574ef58cafa9/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=788e1092e90eacfd1afb14166a68574ef58cafa9",
            "patch": "@@ -1482,7 +1482,7 @@\n     _import_structure[\"modeling_flash_attention_utils\"] = []\n     _import_structure[\"modeling_outputs\"] = []\n     _import_structure[\"modeling_rope_utils\"] = [\"ROPE_INIT_FUNCTIONS\"]\n-    _import_structure[\"modeling_utils\"] = [\"PreTrainedModel\"]\n+    _import_structure[\"modeling_utils\"] = [\"PreTrainedModel\", \"AttentionInterface\"]\n \n     # PyTorch models structure\n \n@@ -6727,7 +6727,7 @@\n             model_addition_debugger_context,\n         )\n         from .modeling_rope_utils import ROPE_INIT_FUNCTIONS\n-        from .modeling_utils import PreTrainedModel\n+        from .modeling_utils import AttentionInterface, PreTrainedModel\n         from .models.albert import (\n             AlbertForMaskedLM,\n             AlbertForMultipleChoice,"
        },
        {
            "sha": "f9b3faf480f009f9a2e64a4ab2bcc3109e6d10fa",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 49,
            "deletions": 8,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/788e1092e90eacfd1afb14166a68574ef58cafa9/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/788e1092e90eacfd1afb14166a68574ef58cafa9/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=788e1092e90eacfd1afb14166a68574ef58cafa9",
            "patch": "@@ -28,6 +28,7 @@\n import tempfile\n import warnings\n from collections import defaultdict\n+from collections.abc import MutableMapping\n from contextlib import contextmanager\n from dataclasses import dataclass\n from enum import Enum\n@@ -2081,9 +2082,10 @@ def _autoset_attn_implementation(\n                     ' We recommend to just use `attn_implementation=\"flash_attention_2\"` when loading the model.'\n                 )\n \n-            if not isinstance(config._attn_implementation, dict) and config._attn_implementation not in [\n-                \"eager\"\n-            ] + list(ALL_ATTENTION_FUNCTIONS.keys()):\n+            if (\n+                not isinstance(config._attn_implementation, dict)\n+                and config._attn_implementation not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys()\n+            ):\n                 message = f'Specified `attn_implementation=\"{config._attn_implementation}\"` is not supported. The only possible arguments are `attn_implementation=\"eager\"` (manual attention implementation)'\n                 if cls._supports_flash_attn_2:\n                     message += ', `\"attn_implementation=flash_attention_2\"` (implementation using flash attention 2)'\n@@ -2148,7 +2150,7 @@ def _autoset_attn_implementation(\n                     \"Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\"\n                 )\n                 torch.backends.cuda.enable_flash_sdp(False)\n-        elif requested_attn_implementation in list(ALL_ATTENTION_FUNCTIONS.keys()):\n+        elif requested_attn_implementation in ALL_ATTENTION_FUNCTIONS.valid_keys():\n             config._attn_implementation = requested_attn_implementation\n         elif isinstance(requested_attn_implementation, dict):\n             config._attn_implementation = None\n@@ -5891,12 +5893,51 @@ def get_disk_only_shard_files(device_map, weight_map):\n     return [fname for fname, devices in files_content.items() if set(devices) == {\"disk\"}]\n \n \n-ALL_ATTENTION_FUNCTIONS: Dict[str, Callable] = {}\n+class AttentionInterface(MutableMapping):\n+    \"\"\"\n+    Dict-like object keeping track of allowed attention functions. You can easily add a new attention function\n+    with a call to `register()`. If a model needs to locally overwrite an existing attention function, say `sdpa`,\n+    it needs to declare a new instance of this class inside the `modeling.py`, and declare it on that instance.\n+    \"\"\"\n \n-ALL_ATTENTION_FUNCTIONS.update(\n-    {\n+    # Class instance object, so that a call to `register` can be reflected into all other files correctly, even if\n+    # a new instance is created (in order to locally override a given function)\n+    _global_mapping = {\n         \"flash_attention_2\": flash_attention_forward,\n         \"flex_attention\": flex_attention_forward,\n         \"sdpa\": sdpa_attention_forward,\n     }\n-)\n+\n+    def __init__(self):\n+        self._local_mapping = {}\n+\n+    def __getitem__(self, key):\n+        # First check if instance has a local override\n+        if key in self._local_mapping:\n+            return self._local_mapping[key]\n+        return self._global_mapping[key]\n+\n+    def __setitem__(self, key, value):\n+        # Allow local update of the default functions without impacting other instances\n+        self._local_mapping.update({key: value})\n+\n+    def __delitem__(self, key):\n+        del self._local_mapping[key]\n+\n+    def __iter__(self):\n+        # Ensure we use all keys, with the overwritten ones on top\n+        return iter(self._global_mapping.update(self._local_mapping))\n+\n+    def __len__(self):\n+        return len(self._global_mapping.keys() | self._local_mapping.keys())\n+\n+    @classmethod\n+    def register(cls, key: str, value: Callable):\n+        cls._global_mapping.update({key: value})\n+\n+    def valid_keys(self) -> List[str]:\n+        return list(self._global_mapping.keys() | self._local_mapping.keys())\n+\n+\n+# Global AttentionInterface shared by all models which do not need to overwrite any of the existing ones\n+ALL_ATTENTION_FUNCTIONS: AttentionInterface = AttentionInterface()"
        },
        {
            "sha": "744a3161610c10758014f059fc9f1b026627c528",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/788e1092e90eacfd1afb14166a68574ef58cafa9/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/788e1092e90eacfd1afb14166a68574ef58cafa9/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=788e1092e90eacfd1afb14166a68574ef58cafa9",
            "patch": "@@ -549,6 +549,13 @@ def model_addition_debugger_context(*args, **kwargs):\n ROPE_INIT_FUNCTIONS = None\n \n \n+class AttentionInterface(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class PreTrainedModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        }
    ],
    "stats": {
        "total": 182,
        "additions": 171,
        "deletions": 11
    }
}