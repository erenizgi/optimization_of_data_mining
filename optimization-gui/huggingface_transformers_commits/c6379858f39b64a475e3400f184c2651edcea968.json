{
    "author": "ArthurZucker",
    "message": "bump tokenizers, fix added tokens fast (#32535)\n\n* update based on tokenizers release\r\n\r\n* update\r\n\r\n* nits\r\n\r\n* update\r\n\r\n* revert re addition\r\n\r\n* don't break that yet\r\n\r\n* fmt\r\n\r\n* revert unwanted\r\n\r\n* update tokenizers version\r\n\r\n* update dep table\r\n\r\n* update\r\n\r\n* update in conversion script as well\r\n\r\n* some fix\r\n\r\n* revert\r\n\r\n* fully revert\r\n\r\n* fix training\r\n\r\n* remove set trace\r\n\r\n* fixup\r\n\r\n* update\r\n\r\n* update",
    "sha": "c6379858f39b64a475e3400f184c2651edcea968",
    "files": [
        {
            "sha": "b563b11b6226871a457d2bdf341a2bd99d733a39",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6379858f39b64a475e3400f184c2651edcea968/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6379858f39b64a475e3400f184c2651edcea968/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=c6379858f39b64a475e3400f184c2651edcea968",
            "patch": "@@ -181,7 +181,7 @@\n     \"timeout-decorator\",\n     \"tiktoken\",\n     \"timm<=0.9.16\",\n-    \"tokenizers>=0.19,<0.20\",\n+    \"tokenizers>=0.20,<0.21\",\n     \"torch\",\n     \"torchaudio\",\n     \"torchvision\","
        },
        {
            "sha": "21876c7f61d895459f94f71f7dc5e7bfb152a34b",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 27,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6379858f39b64a475e3400f184c2651edcea968/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6379858f39b64a475e3400f184c2651edcea968/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=c6379858f39b64a475e3400f184c2651edcea968",
            "patch": "@@ -609,33 +609,12 @@ def tokenizer(self, proto):\n             for id, p in enumerate(proto.pieces)\n             if p.type in [3, 4]\n         ]\n-        tokens_to_add = [\n-            AddedToken(token, normalized=False, special=special)\n-            for id, token, special in sorted(spm_added_tokens, key=lambda x: x[0])\n-        ]\n-\n-        if len(tokens_to_add) > 0:\n-            # super hack: if a token.special is set, tokenizer ignores it for now so FIXME @ArthurZ\n-            # Accumulate added tokens into batches of special/non-special tokens, because calling add_tokens() for\n-            # individual tokens would repeatedly rebuild a trie, which can be slow.\n-            is_last_special = None\n-            tokens = []\n-            for token in tokens_to_add:\n-                is_special = token.special\n-                if is_last_special is None or is_last_special == is_special:\n-                    tokens.append(token)\n-                else:\n-                    if is_last_special:\n-                        tokenizer.add_special_tokens(tokens)\n-                    else:\n-                        tokenizer.add_tokens(tokens)\n-                    tokens = [token]\n-                is_last_special = is_special\n-            if tokens:\n-                if is_last_special:\n-                    tokenizer.add_special_tokens(tokens)\n-                else:\n-                    tokenizer.add_tokens(tokens)\n+        tokenizer.add_tokens(\n+            [\n+                AddedToken(token, normalized=False, special=special)\n+                for id, token, special in sorted(spm_added_tokens, key=lambda x: x[0])\n+            ]\n+        )\n \n         return tokenizer\n "
        },
        {
            "sha": "6564e0790336345fe228eee02e5114d027d1dc5e",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6379858f39b64a475e3400f184c2651edcea968/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6379858f39b64a475e3400f184c2651edcea968/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=c6379858f39b64a475e3400f184c2651edcea968",
            "patch": "@@ -86,7 +86,7 @@\n     \"timeout-decorator\": \"timeout-decorator\",\n     \"tiktoken\": \"tiktoken\",\n     \"timm\": \"timm<=0.9.16\",\n-    \"tokenizers\": \"tokenizers>=0.19,<0.20\",\n+    \"tokenizers\": \"tokenizers>=0.20,<0.21\",\n     \"torch\": \"torch\",\n     \"torchaudio\": \"torchaudio\",\n     \"torchvision\": \"torchvision\","
        },
        {
            "sha": "cec91e038dd054dc74bb61c0e97e75d61be5108f",
            "filename": "src/transformers/tokenization_utils_fast.py",
            "status": "modified",
            "additions": 16,
            "deletions": 17,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6379858f39b64a475e3400f184c2651edcea968/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6379858f39b64a475e3400f184c2651edcea968/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_fast.py?ref=c6379858f39b64a475e3400f184c2651edcea968",
            "patch": "@@ -175,15 +175,8 @@ def __init__(self, *args, **kwargs):\n \n         # We call this after having initialized the backend tokenizer because we update it.\n         super().__init__(**kwargs)\n-\n-        # Set the splitting mode for special tokens for the tokenizer to be used throughout the class.\n         self._tokenizer.encode_special_tokens = self.split_special_tokens\n \n-        # The following logic will be replace with a single add_tokens once a fix is pushed to tokenizers\n-        # allows converting a slow -> fast, non-legacy: if the `tokenizer.json` does not have all the added tokens\n-        # uses the information stored in `added_tokens_decoder`.\n-        # this is costly for fast tokenizers as we re-compute the regex again. But not all tokens are added tokens\n-        # Use hash to speed up the very slow operation `token not in added_tokens_decoder`.\n         added_tokens_decoder_hash = {hash(repr(token)) for token in self.added_tokens_decoder}\n         tokens_to_add = [\n             token\n@@ -197,10 +190,6 @@ def __init__(self, *args, **kwargs):\n         ]\n \n         if len(tokens_to_add) > 0:\n-            # super hack: if a token.special is set, tokenizer ignores it for now so FIXME @ArthurZ\n-            # Accumulate added tokens into batches of special/non-special tokens, because calling add_tokens() for\n-            # individual tokens would repeatedly rebuild a trie, which can be slow.\n-            is_last_special = None\n             tokens = []\n             special_tokens = self.all_special_tokens\n             for token in tokens_to_add:\n@@ -209,14 +198,13 @@ def __init__(self, *args, **kwargs):\n                     if isinstance(token, AddedToken)\n                     else str(token) in special_tokens\n                 )\n-                if is_last_special is None or is_last_special == is_special:\n-                    tokens.append(token)\n+                if isinstance(token, str):\n+                    token = AddedToken(token, special=is_special)\n                 else:\n-                    self._add_tokens(tokens, special_tokens=is_last_special)\n-                    tokens = [token]\n-                is_last_special = is_special\n+                    token.special = is_special\n+                tokens.append(token)\n             if tokens:\n-                self._add_tokens(tokens, special_tokens=is_last_special)\n+                self.add_tokens(tokens)\n \n     @property\n     def is_fast(self) -> bool:\n@@ -849,6 +837,13 @@ def train_new_from_iterator(\n                     if special_tokens_map is not None:\n                         tokens = [special_tokens_map.get(token, token) for token in tokens]\n                     post_processor[\"special_tokens\"][key][\"tokens\"] = tokens\n+                    for token in tokens:\n+                        token_id = tokenizer.token_to_id(token)\n+                        if token_id is None:\n+                            raise ValueError(\n+                                \"Attempted to set a token in the post processor that does not exist in the mapping\"\n+                            )\n+\n                     post_processor[\"special_tokens\"][key][\"ids\"] = [tokenizer.token_to_id(token) for token in tokens]\n \n             for special_token in [\"cls\", \"sep\"]:\n@@ -857,6 +852,10 @@ def train_new_from_iterator(\n                     if special_tokens_map is not None and token in special_tokens_map:\n                         token = special_tokens_map[token]\n                     token_id = tokenizer.token_to_id(token)\n+                    if token_id is None:\n+                        raise ValueError(\n+                            \"Attempted to set a token in the post processor that does not exist in the mapping\"\n+                        )\n                     post_processor[special_token] = [token, token_id]\n \n             trained_tokenizer_json[\"post_processor\"] = post_processor"
        }
    ],
    "stats": {
        "total": 70,
        "additions": 24,
        "deletions": 46
    }
}