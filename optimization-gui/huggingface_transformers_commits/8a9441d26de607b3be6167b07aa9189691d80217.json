{
    "author": "zucchini-nlp",
    "message": "[chat template] separate jinja logic from tokenizers  (#37602)\n\n* split oit jinja\n\n* raise error",
    "sha": "8a9441d26de607b3be6167b07aa9189691d80217",
    "files": [
        {
            "sha": "403569ae89c237285df82b6f8296731c1be40d63",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 29,
            "deletions": 5,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a9441d26de607b3be6167b07aa9189691d80217/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a9441d26de607b3be6167b07aa9189691d80217/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=8a9441d26de607b3be6167b07aa9189691d80217",
            "patch": "@@ -41,6 +41,7 @@\n     load_image,\n     load_video,\n )\n+from .utils.chat_template_utils import render_jinja_template\n \n \n if is_vision_available():\n@@ -1426,6 +1427,14 @@ def apply_chat_template(\n                 # It's a template string, render it directly\n                 chat_template = chat_template\n \n+        if kwargs.get(\"continue_final_message\", False):\n+            if kwargs.get(\"add_generation_prompt\", False):\n+                raise ValueError(\n+                    \"continue_final_message and add_generation_prompt are not compatible. Use continue_final_message when you want the model to continue the final message, and add_generation_prompt when you want to add a header that will prompt it to start a new assistant message instead.\"\n+                )\n+            if kwargs.get(\"return_assistant_tokens_mask\", False):\n+                raise ValueError(\"continue_final_message is not compatible with return_assistant_tokens_mask.\")\n+\n         # Fill sets of kwargs that should be used by different parts of template\n         processed_kwargs = {\n             \"mm_load_kwargs\": {},\n@@ -1534,12 +1543,11 @@ def apply_chat_template(\n                 **processed_kwargs[\"mm_load_kwargs\"],\n             )\n \n-        prompt = self.tokenizer.apply_chat_template(\n-            conversations,\n+        prompt, generation_indices = render_jinja_template(\n+            conversations=conversations,\n             chat_template=chat_template,\n-            tokenize=False,\n-            return_dict=False,\n-            **processed_kwargs[\"template_kwargs\"],\n+            **processed_kwargs[\"template_kwargs\"],  # different flags such as `return_assistant_mask`\n+            **self.tokenizer.special_tokens_map,  # tokenizer special tokens are used by some templates\n         )\n \n         if not is_batched:\n@@ -1564,6 +1572,22 @@ def apply_chat_template(\n                 **kwargs,\n             )\n             if return_dict:\n+                if processed_kwargs[\"template_kwargs\"].get(\"return_assistant_tokens_mask\", False):\n+                    assistant_masks = []\n+                    input_ids = out[\"input_ids\"]\n+                    for i in range(len(input_ids)):\n+                        current_mask = [0] * len(input_ids[i])\n+                        for assistant_start_char, assistant_end_char in generation_indices[i]:\n+                            start_token = out.char_to_token(i, assistant_start_char)\n+                            end_token = out.char_to_token(i, assistant_end_char - 1)\n+                            if start_token is None:\n+                                # start_token is out of bounds maybe due to truncation.\n+                                break\n+                            for token_id in range(start_token, end_token + 1 if end_token else len(input_ids[i])):\n+                                current_mask[token_id] = 1\n+                        assistant_masks.append(current_mask)\n+                    out[\"assistant_masks\"] = assistant_masks\n+                    out.convert_to_tensors(tensor_type=kwargs.get(\"return_tensors\", None))\n                 return out\n             else:\n                 return out[\"input_ids\"]"
        },
        {
            "sha": "62e4ff6d709a0b2551f2fb2166e74593283ae869",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 15,
            "deletions": 88,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a9441d26de607b3be6167b07aa9189691d80217/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a9441d26de607b3be6167b07aa9189691d80217/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=8a9441d26de607b3be6167b07aa9189691d80217",
            "patch": "@@ -27,7 +27,6 @@\n from collections.abc import Mapping, Sized\n from contextlib import contextmanager\n from dataclasses import dataclass\n-from inspect import isfunction\n from pathlib import Path\n from typing import TYPE_CHECKING, Any, Callable, Dict, List, NamedTuple, Optional, Sequence, Tuple, Union\n \n@@ -50,7 +49,6 @@\n     copy_func,\n     download_url,\n     extract_commit_hash,\n-    get_json_schema,\n     is_flax_available,\n     is_jax_tensor,\n     is_mlx_available,\n@@ -69,7 +67,7 @@\n     requires_backends,\n     to_py_obj,\n )\n-from .utils.chat_template_utils import _compile_jinja_template, _render_with_assistant_indices\n+from .utils.chat_template_utils import render_jinja_template\n from .utils.import_utils import PROTOBUF_IMPORT_ERROR\n \n \n@@ -1633,14 +1631,6 @@ def apply_chat_template(\n \n         chat_template = self.get_chat_template(chat_template, tools)\n \n-        if return_assistant_tokens_mask and not re.search(r\"\\{\\%-?\\s*generation\\s*-?\\%\\}\", chat_template):\n-            logger.warning_once(\n-                \"return_assistant_tokens_mask==True but chat template does not contain `{% generation %}` keyword.\"\n-            )\n-\n-        # Compilation function uses a cache to avoid recompiling the same template\n-        compiled_template = _compile_jinja_template(chat_template)\n-\n         if isinstance(conversation, (list, tuple)) and (\n             isinstance(conversation[0], (list, tuple)) or hasattr(conversation[0], \"messages\")\n         ):\n@@ -1658,87 +1648,24 @@ def apply_chat_template(\n             if return_assistant_tokens_mask:\n                 raise ValueError(\"continue_final_message is not compatible with return_assistant_tokens_mask.\")\n \n-        # We accept either JSON schemas or functions for tools. If we get functions, we convert them to schemas\n-        if tools is not None:\n-            tool_schemas = []\n-            for tool in tools:\n-                if isinstance(tool, dict):\n-                    tool_schemas.append(tool)\n-                elif isfunction(tool):\n-                    tool_schemas.append(get_json_schema(tool))\n-                else:\n-                    raise ValueError(\n-                        \"Tools should either be a JSON schema, or a callable function with type hints \"\n-                        \"and a docstring suitable for auto-conversion to a schema.\"\n-                    )\n-        else:\n-            tool_schemas = None\n-\n-        if documents is not None:\n-            for document in documents:\n-                if not isinstance(document, dict):\n-                    raise TypeError(\"Documents should be a list of dicts with 'title' and 'text' keys!\")\n-\n-        rendered = []\n-        all_generation_indices = []\n         template_kwargs = {**self.special_tokens_map, **kwargs}  # kwargs overwrite special tokens if both are present\n-        for chat in conversations:\n-            if hasattr(chat, \"messages\"):\n-                # Indicates it's a Conversation object\n-                chat = chat.messages\n-            if return_assistant_tokens_mask:\n-                rendered_chat, generation_indices = _render_with_assistant_indices(\n-                    compiled_template=compiled_template,\n-                    messages=chat,\n-                    tools=tool_schemas,\n-                    documents=documents,\n-                    add_generation_prompt=add_generation_prompt,\n-                    **template_kwargs,\n-                )\n-                all_generation_indices.append(generation_indices)\n-            else:\n-                rendered_chat = compiled_template.render(\n-                    messages=chat,\n-                    tools=tool_schemas,\n-                    documents=documents,\n-                    add_generation_prompt=add_generation_prompt,\n-                    **template_kwargs,\n-                )\n-            if continue_final_message:\n-                final_message = chat[-1][\"content\"]\n-                if isinstance(final_message, (list, tuple)):\n-                    for content_block in reversed(final_message):\n-                        if \"text\" in content_block:\n-                            # Pick the last text block in the message (the first one we hit while iterating in reverse)\n-                            final_message = content_block[\"text\"]\n-                            break\n-                    else:\n-                        raise ValueError(\n-                            \"continue_final_message is set but we could not find any text to continue\"\n-                            \"in the final message!\"\n-                        )\n-                if final_message.strip() not in rendered_chat:\n-                    raise ValueError(\n-                        \"continue_final_message is set but the final message does not appear in the chat after \"\n-                        \"applying the chat template! This can happen if the chat template deletes portions of \"\n-                        \"the final message. Please verify the chat template and final message in your chat to \"\n-                        \"ensure they are compatible.\"\n-                    )\n-                final_msg_loc = rendered_chat.rindex(final_message.strip())\n-                if rendered_chat[final_msg_loc : final_msg_loc + len(final_message.lstrip())] == final_message:\n-                    # The template preserves spacing or the message doesn't have trailing spacing, so things are simple\n-                    rendered_chat = rendered_chat[: final_msg_loc + len(final_message.lstrip())]\n-                else:\n-                    # The message has trailing spacing that was trimmed, so we must be more cautious\n-                    rendered_chat = rendered_chat[: final_msg_loc + len(final_message.strip())]\n-            rendered.append(rendered_chat)\n+        rendered_chat, generation_indices = render_jinja_template(\n+            conversations=conversations,\n+            tools=tools,\n+            documents=documents,\n+            chat_template=chat_template,\n+            return_assistant_tokens_mask=return_assistant_tokens_mask,\n+            continue_final_message=continue_final_message,\n+            add_generation_prompt=add_generation_prompt,\n+            **template_kwargs,\n+        )\n \n         if not is_batched:\n-            rendered = rendered[0]\n+            rendered_chat = rendered_chat[0]\n \n         if tokenize:\n             out = self(\n-                rendered,\n+                rendered_chat,\n                 padding=padding,\n                 truncation=truncation,\n                 max_length=max_length,\n@@ -1755,7 +1682,7 @@ def apply_chat_template(\n                         input_ids = [out[\"input_ids\"]]\n                     for i in range(len(input_ids)):\n                         current_mask = [0] * len(input_ids[i])\n-                        for assistant_start_char, assistant_end_char in all_generation_indices[i]:\n+                        for assistant_start_char, assistant_end_char in generation_indices[i]:\n                             start_token = out.char_to_token(i, assistant_start_char)\n                             end_token = out.char_to_token(i, assistant_end_char - 1)\n                             if start_token is None:\n@@ -1777,7 +1704,7 @@ def apply_chat_template(\n             else:\n                 return out[\"input_ids\"]\n         else:\n-            return rendered\n+            return rendered_chat\n \n     def get_chat_template(self, chat_template: Optional[str] = None, tools: Optional[List[Dict]] = None) -> str:\n         \"\"\""
        },
        {
            "sha": "d91dcace910ab5233d7c640baa2fdb76038cba3c",
            "filename": "src/transformers/utils/chat_template_utils.py",
            "status": "modified",
            "additions": 98,
            "deletions": 0,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a9441d26de607b3be6167b07aa9189691d80217/src%2Ftransformers%2Futils%2Fchat_template_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a9441d26de607b3be6167b07aa9189691d80217/src%2Ftransformers%2Futils%2Fchat_template_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fchat_template_utils.py?ref=8a9441d26de607b3be6167b07aa9189691d80217",
            "patch": "@@ -19,13 +19,17 @@\n from contextlib import contextmanager\n from datetime import datetime\n from functools import lru_cache\n+from inspect import isfunction\n from typing import Any, Callable, Optional, Union, get_args, get_origin, get_type_hints\n \n from packaging import version\n \n+from . import logging\n from .import_utils import is_jinja_available, is_torch_available, is_vision_available\n \n \n+logger = logging.get_logger(__name__)\n+\n if is_jinja_available():\n     import jinja2\n     from jinja2.ext import Extension\n@@ -433,3 +437,97 @@ def strftime_now(format):\n     jinja_env.globals[\"raise_exception\"] = raise_exception\n     jinja_env.globals[\"strftime_now\"] = strftime_now\n     return jinja_env.from_string(chat_template)\n+\n+\n+def render_jinja_template(\n+    conversations: list[list[dict[str, str]]],\n+    tools: Optional[list[Union[dict, Callable]]] = None,\n+    documents: Optional[list[dict[str, str]]] = None,\n+    chat_template: Optional[str] = None,\n+    return_assistant_tokens_mask: Optional[bool] = False,\n+    continue_final_message: Optional[bool] = False,\n+    add_generation_prompt: Optional[bool] = False,\n+    **kwargs,\n+) -> str:\n+    if return_assistant_tokens_mask and not re.search(r\"\\{\\%-?\\s*generation\\s*-?\\%\\}\", chat_template):\n+        logger.warning_once(\n+            \"return_assistant_tokens_mask==True but chat template does not contain `{% generation %}` keyword.\"\n+        )\n+\n+    # Compilation function uses a cache to avoid recompiling the same template\n+    compiled_template = _compile_jinja_template(chat_template)\n+\n+    # We accept either JSON schemas or functions for tools. If we get functions, we convert them to schemas\n+    if tools is not None:\n+        tool_schemas = []\n+        for tool in tools:\n+            if isinstance(tool, dict):\n+                tool_schemas.append(tool)\n+            elif isfunction(tool):\n+                tool_schemas.append(get_json_schema(tool))\n+            else:\n+                raise ValueError(\n+                    \"Tools should either be a JSON schema, or a callable function with type hints \"\n+                    \"and a docstring suitable for auto-conversion to a schema.\"\n+                )\n+    else:\n+        tool_schemas = None\n+\n+    if documents is not None:\n+        for document in documents:\n+            if not isinstance(document, dict):\n+                raise TypeError(\"Documents should be a list of dicts with 'title' and 'text' keys!\")\n+\n+    rendered = []\n+    all_generation_indices = []\n+    for chat in conversations:\n+        if hasattr(chat, \"messages\"):\n+            # Indicates it's a Conversation object\n+            chat = chat.messages\n+        if return_assistant_tokens_mask:\n+            rendered_chat, generation_indices = _render_with_assistant_indices(\n+                compiled_template=compiled_template,\n+                messages=chat,\n+                tools=tool_schemas,\n+                documents=documents,\n+                add_generation_prompt=add_generation_prompt,\n+                **kwargs,\n+            )\n+            all_generation_indices.append(generation_indices)\n+        else:\n+            rendered_chat = compiled_template.render(\n+                messages=chat,\n+                tools=tool_schemas,\n+                documents=documents,\n+                add_generation_prompt=add_generation_prompt,\n+                **kwargs,\n+            )\n+        if continue_final_message:\n+            final_message = chat[-1][\"content\"]\n+            if isinstance(final_message, (list, tuple)):\n+                for content_block in reversed(final_message):\n+                    if \"text\" in content_block:\n+                        # Pick the last text block in the message (the first one we hit while iterating in reverse)\n+                        final_message = content_block[\"text\"]\n+                        break\n+                else:\n+                    raise ValueError(\n+                        \"continue_final_message is set but we could not find any text to continuein the final message!\"\n+                    )\n+            if final_message.strip() not in rendered_chat:\n+                raise ValueError(\n+                    \"continue_final_message is set but the final message does not appear in the chat after \"\n+                    \"applying the chat template! This can happen if the chat template deletes portions of \"\n+                    \"the final message. Please verify the chat template and final message in your chat to \"\n+                    \"ensure they are compatible.\"\n+                )\n+            final_msg_loc = rendered_chat.rindex(final_message.strip())\n+            if rendered_chat[final_msg_loc : final_msg_loc + len(final_message.lstrip())] == final_message:\n+                # The template preserves spacing or the message doesn't have trailing spacing, so things are simple\n+                rendered_chat = rendered_chat[: final_msg_loc + len(final_message.lstrip())]\n+            else:\n+                # The message has trailing spacing that was trimmed, so we must be more cautious\n+                rendered_chat = rendered_chat[: final_msg_loc + len(final_message.strip())]\n+        rendered.append(rendered_chat)\n+\n+    return rendered, all_generation_indices"
        }
    ],
    "stats": {
        "total": 235,
        "additions": 142,
        "deletions": 93
    }
}