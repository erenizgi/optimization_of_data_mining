{
    "author": "MekkCyber",
    "message": "Clean up the compressed-tensors integration (#37349)\n\nclean up",
    "sha": "8bbcdf54096efbbb8de6f1a4e73c6517edf344a8",
    "files": [
        {
            "sha": "752227914d98b40e919ab34e26e88868a9334c61",
            "filename": "src/transformers/integrations/compressed_tensors.py",
            "status": "removed",
            "additions": 0,
            "deletions": 54,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a826a45cab1ca84d3e2a5b9212b883bdb4ea74a/src%2Ftransformers%2Fintegrations%2Fcompressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a826a45cab1ca84d3e2a5b9212b883bdb4ea74a/src%2Ftransformers%2Fintegrations%2Fcompressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fcompressed_tensors.py?ref=3a826a45cab1ca84d3e2a5b9212b883bdb4ea74a",
            "patch": "@@ -1,54 +0,0 @@\n-# Copyright 2025 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from transformers.utils import is_torch_available\n-\n-\n-if is_torch_available():\n-    import torch\n-    import torch.nn as nn\n-\n-from transformers.models.llama4.modeling_llama4 import Llama4TextMLP\n-\n-\n-def skip(*args, **kwargs):\n-    pass\n-\n-\n-class CompressedExpertsLinear(nn.Module):\n-    \"\"\"\n-    A module that implements a compressed version of a list of expert modules.\n-    This is specifically designed to work with Llama4TextExperts in MoE layers.\n-    \"\"\"\n-\n-    def __init__(self, config):\n-        # Skip random weight initialization for experts. Otherwise,\n-        # the init of this module would take over minutes. For a model\n-        # with tens of layers of experts, it would easily take over 20 minutes.\n-        nn.init.kaiming_uniform_ = skip\n-        nn.init.uniform_ = skip\n-        nn.init.normal_ = skip\n-        super().__init__()\n-        self.num_experts = config.num_local_experts\n-        self.expert_modules = nn.ModuleList([Llama4TextMLP(config) for _ in range(self.num_experts)])\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-    ) -> torch.Tensor:\n-        hidden_states = hidden_states.reshape(self.num_experts, -1, hidden_states.shape[-1])\n-        expert_routed_out_list = []\n-        for expert_idx in range(self.num_experts):\n-            expert_routed_out_list.append(self.expert_modules[expert_idx](hidden_states[expert_idx]))\n-        routed_out = torch.cat(expert_routed_out_list, dim=0)\n-        return routed_out"
        }
    ],
    "stats": {
        "total": 54,
        "additions": 0,
        "deletions": 54
    }
}