{
    "author": "MekkCyber",
    "message": "Refactor bitsandbytes doc (#37668)\n\n* doc\n\n* torch ops\n\n* fix\n\n* nits\n\n* Update docs/source/en/quantization/bitsandbytes.md\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "de182ba2690fe6c3466f6463c7f4b3a61694b885",
    "files": [
        {
            "sha": "3ffdd89f2fd563f41237432d3d41f20b2eed47a2",
            "filename": "docs/source/en/quantization/bitsandbytes.md",
            "status": "modified",
            "additions": 60,
            "deletions": 8,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/de182ba2690fe6c3466f6463c7f4b3a61694b885/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de182ba2690fe6c3466f6463c7f4b3a61694b885/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md?ref=de182ba2690fe6c3466f6463c7f4b3a61694b885",
            "patch": "@@ -14,13 +14,21 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# bitsandbytes\n+# Bitsandbytes\n \n-[bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes) features the LLM.int8 and QLoRA quantization to enable accessible large language model inference and training.\n+The [bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes) library provides quantization tools for LLMs through a lightweight Python wrapper around CUDA functions. It enables working with large models using limited computational resources by reducing their memory footprint.\n \n-[LLM.int8()](https://hf.co/papers/2208.07339) is a quantization method that aims to make large language model inference more accessible without significant degradation. Unlike naive 8-bit quantization, which can result in loss of critical information and accuracy, LLM.int8() dynamically adapts to ensure sensitive components of the computation retain higher precision when needed.\n+At its core, bitsandbytes provides:\n \n-QLoRA, or 4-bit quantization, compresses a model even further to 4-bits and inserts a small set of trainable low-rank adaptation (LoRA) weights to allowing training. \n+- **Quantized Linear Layers**: `Linear8bitLt` and `Linear4bit` layers that replace standard PyTorch linear layers with memory-efficient quantized alternatives\n+- **Optimized Optimizers**: 8-bit versions of common optimizers through its `optim` module, enabling training of large models with reduced memory requirements\n+- **Matrix Multiplication**: Optimized matrix multiplication operations that leverage the quantized format\n+\n+bitsandbytes offers two main quantization features:\n+\n+1. **LLM.int8()** - An 8-bit quantization method that makes inference more accessible without significant performance degradation. Unlike naive quantization, [LLM.int8()](https://hf.co/papers/2208.07339) dynamically preserves higher precision for critical computations, preventing information loss in sensitive parts of the model.\n+\n+2. **QLoRA** - A 4-bit quantization technique that compresses models even further while maintaining trainability by inserting a small set of trainable low-rank adaptation (LoRA) weights.\n \n > **Note:** For a user-friendly quantization experience, you can use the `bitsandbytes` [community space](https://huggingface.co/spaces/bnb-community/bnb-my-repo).\n \n@@ -30,12 +38,38 @@ Run the command below to install bitsandbytes.\n ```bash\n pip install --upgrade transformers accelerate bitsandbytes\n ```\n+To compile from source, follow the instructions in the [bitsandbytes installation guide](https://huggingface.co/docs/bitsandbytes/main/en/installation).\n+\n+## Hardware Compatibility\n+bitsandbytes is currently only supported on CUDA GPUs for CUDA versions 11.0 - 12.8. However, there's an ongoing multi-backend effort under development, which is currently in alpha. If you're interested in providing feedback or testing, check out the [bitsandbytes repository](https://github.com/bitsandbytes-foundation/bitsandbytes) for more information.\n+\n+### CUDA\n+\n+| Feature | Minimum Hardware Requirement |\n+|---------|-------------------------------|\n+| 8-bit optimizers | NVIDIA Maxwell (GTX 900 series, TITAN X, M40) or newer GPUs * |\n+| LLM.int8() | NVIDIA Turing (RTX 20 series, T4) or newer GPUs |\n+| NF4/FP4 quantization | NVIDIA Maxwell (GTX 900 series, TITAN X, M40) or newer GPUs * |\n+\n+### Multi-backend\n+\n+| Backend | Supported Versions | Python versions | Architecture Support | Status |\n+|---------|-------------------|----------------|---------------------|---------|\n+| AMD ROCm | 6.1+ | 3.10+ | minimum CDNA - gfx90a, RDNA - gfx1100 | Alpha |\n+| Apple Silicon (MPS) | WIP | 3.10+ | M1/M2 chips | Planned |\n+| Intel CPU | v2.4.0+ (ipex) | 3.10+ | Intel CPU | Alpha |\n+| Intel GPU | v2.4.0+ (ipex) | 3.10+ | Intel GPU | Experimental |\n+| Ascend NPU | 2.1.0+ (torch_npu) | 3.10+ | Ascend NPU | Experimental |\n+\n+> **Note:** Bitsandbytes is moving away from the multi-backend approach towards using [Pytorch Custom Operators](https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html), as the main mechanism for supporting new hardware, and dispatching to the correct backend.\n+\n+## Quantization Examples\n \n Quantize a model by passing a [`BitsAndBytesConfig`] to [`~PreTrainedModel.from_pretrained`]. This works for any model in any modality, as long as it supports [Accelerate](https://huggingface.co/docs/accelerate/index) and contains [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers.\n \n <hfoptions id=\"bnb\">\n <hfoption id=\"8-bit\">\n-\n+<div class=\"bnb-container\" style=\"border: 1px solid #ddd; border-radius: 8px; padding: 20px; margin: 20px 0\">\n Quantizing a model in 8-bit halves the memory-usage, and for large models, set `device_map=\"auto\"` to efficiently distribute the weights across all available GPUs.\n \n ```py\n@@ -45,6 +79,7 @@ quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n \n model_8bit = AutoModelForCausalLM.from_pretrained(\n     \"bigscience/bloom-1b7\", \n+    device_map=\"auto\",\n     quantization_config=quantization_config\n )\n ```\n@@ -59,6 +94,7 @@ quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n \n model_8bit = AutoModelForCausalLM.from_pretrained(\n     \"facebook/opt-350m\", \n+    device_map=\"auto\",\n     quantization_config=quantization_config, \n     torch_dtype=\"auto\"\n )\n@@ -74,16 +110,16 @@ quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n \n model = AutoModelForCausalLM.from_pretrained(\n     \"bigscience/bloom-560m\", \n+    device_map=\"auto\",\n     quantization_config=quantization_config\n )\n-tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n \n model.push_to_hub(\"bloom-560m-8bit\")\n ```\n-\n+</div>\n </hfoption>\n <hfoption id=\"4-bit\">\n-\n+<div class=\"bnb-container\" style=\"border: 1px solid #ddd; border-radius: 8px; padding: 20px; margin: 20px 0\">\n Quantizing a model in 4-bit reduces your memory-usage by 4x, and for large models, set `device_map=\"auto\"` to efficiently distribute the weights across all available GPUs.\n \n ```py\n@@ -93,6 +129,7 @@ quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n \n model_4bit = AutoModelForCausalLM.from_pretrained(\n     \"bigscience/bloom-1b7\",\n+    device_map=\"auto\",\n     quantization_config=quantization_config\n )\n ```\n@@ -107,6 +144,7 @@ quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n \n model_4bit = AutoModelForCausalLM.from_pretrained(\n     \"facebook/opt-350m\",\n+    device_map=\"auto\",\n     quantization_config=quantization_config, \n     torch_dtype=\"auto\"\n )\n@@ -115,6 +153,20 @@ model_4bit.model.decoder.layers[-1].final_layer_norm.weight.dtype\n \n Make sure you have the latest bitsandbytes version so you can serialize 4-bit models and push them to the Hub with [`~PreTrainedModel.push_to_hub`]. Use [`~PreTrainedModel.save_pretrained`] to save the 4-bit model locally.  \n \n+```py\n+from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n+\n+quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"bigscience/bloom-560m\", \n+    device_map=\"auto\",\n+    quantization_config=quantization_config\n+)\n+\n+model.push_to_hub(\"bloom-560m-4bit\")\n+```\n+</div>\n </hfoption>\n </hfoptions>\n "
        }
    ],
    "stats": {
        "total": 68,
        "additions": 60,
        "deletions": 8
    }
}