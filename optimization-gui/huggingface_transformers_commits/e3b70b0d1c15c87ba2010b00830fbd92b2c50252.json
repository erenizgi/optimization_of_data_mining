{
    "author": "yonigozlan",
    "message": "Refactor image processor phi4 (#36976)\n\n* refactor image processor phi4\n\n* nits fast image proc\n\n* add image tests phi4\n\n* Fix image processing tests\n\n* update integration tests\n\n* remove revision and add comment in integration tests",
    "sha": "e3b70b0d1c15c87ba2010b00830fbd92b2c50252",
    "files": [
        {
            "sha": "7141cd2e9eb33ee98ae45c5d28199eeb04e9cbbc",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3b70b0d1c15c87ba2010b00830fbd92b2c50252/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3b70b0d1c15c87ba2010b00830fbd92b2c50252/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=e3b70b0d1c15c87ba2010b00830fbd92b2c50252",
            "patch": "@@ -128,7 +128,7 @@\n             (\"owlvit\", (\"OwlViTImageProcessor\", \"OwlViTImageProcessorFast\")),\n             (\"paligemma\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"perceiver\", (\"PerceiverImageProcessor\", \"PerceiverImageProcessorFast\")),\n-            (\"phi4_multimodal\", \"Phi4MultimodalImageProcessorFast\"),\n+            (\"phi4_multimodal\", (\"Phi4MultimodalImageProcessorFast\",)),\n             (\"pix2struct\", (\"Pix2StructImageProcessor\",)),\n             (\"pixtral\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n             (\"poolformer\", (\"PoolFormerImageProcessor\", \"PoolFormerImageProcessorFast\")),"
        },
        {
            "sha": "2273293b4c4f7e093df6f10c87837620e68c0070",
            "filename": "src/transformers/models/phi4_multimodal/image_processing_phi4_multimodal_fast.py",
            "status": "modified",
            "additions": 123,
            "deletions": 119,
            "changes": 242,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3b70b0d1c15c87ba2010b00830fbd92b2c50252/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3b70b0d1c15c87ba2010b00830fbd92b2c50252/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py?ref=e3b70b0d1c15c87ba2010b00830fbd92b2c50252",
            "patch": "@@ -12,53 +12,70 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-\"\"\"\n-Processor class for Phi4Multimodal\n-\"\"\"\n-\n import math\n from typing import List, Optional, Union\n \n import torch\n-from torchvision.transforms import functional as F\n \n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n     BatchFeature,\n     DefaultFastImageProcessorKwargs,\n     Unpack,\n-    convert_to_rgb,\n )\n-from ...image_utils import ImageInput, make_flat_list_of_images, valid_images\n-from ...utils import TensorType, logging\n+from ...image_utils import ImageInput, SizeDict\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    is_vision_available,\n+    logging,\n+)\n+\n \n+if is_vision_available():\n+    from ...image_utils import PILImageResampling\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n \n logger = logging.get_logger(__name__)\n \n \n class Phi4MultimodalFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    image_size: Optional[int]\n+    r\"\"\"\n+    patch_size (`int`, *optional*):\n+        The size of the patch.\n+    dynamic_hd (`int`, *optional*):\n+        The maximum number of crops per image.\n+    \"\"\"\n+\n     patch_size: Optional[int]\n     dynamic_hd: Optional[int]\n \n \n+@auto_docstring\n class Phi4MultimodalImageProcessorFast(BaseImageProcessorFast):\n-    r\"\"\"\n-    Constructs a Phi4Multimodal image processor.\n-    \"\"\"\n-\n-    image_size = 448\n+    resample = PILImageResampling.BICUBIC\n+    size = {\"height\": 448, \"width\": 448}\n     patch_size = 14\n     dynamic_hd = 36\n     image_mean = [0.5, 0.5, 0.5]\n     image_std = [0.5, 0.5, 0.5]\n-    valid_init_kwargs = Phi4MultimodalFastImageProcessorKwargs\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+    valid_kwargs = Phi4MultimodalFastImageProcessorKwargs\n     model_input_names = [\"image_pixel_values\", \"image_sizes\", \"image_attention_mask\"]\n \n     def __init__(self, **kwargs: Unpack[Phi4MultimodalFastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n-    def find_closest_aspect_ratio(self, aspect_ratio, target_ratios, width, height):\n+    def find_closest_aspect_ratio(self, aspect_ratio, target_ratios, width, height, image_size):\n         best_ratio_diff = float(\"inf\")\n         best_ratio = (1, 1)\n         area = width * height\n@@ -69,15 +86,12 @@ def find_closest_aspect_ratio(self, aspect_ratio, target_ratios, width, height):\n                 best_ratio_diff = ratio_diff\n                 best_ratio = ratio\n             elif ratio_diff == best_ratio_diff:\n-                if area > 0.5 * self.image_size * self.image_size * ratio[0] * ratio[1]:\n+                if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n                     best_ratio = ratio\n         return best_ratio\n \n-    def dynamic_preprocess(self, image, max_num=36, min_num=1):\n-        image_size = self.image_size\n-        patch_size = self.patch_size\n-        mask_size = image_size // patch_size\n-        orig_width, orig_height = image.size\n+    def dynamic_preprocess(self, image, image_size, patch_size, mask_size, max_num=36, min_num=1):\n+        orig_height, orig_width = image.shape[-2:]\n \n         w_crop_num = math.ceil(orig_width / float(image_size))\n         h_crop_num = math.ceil(orig_height / float(image_size))\n@@ -95,7 +109,9 @@ def dynamic_preprocess(self, image, max_num=36, min_num=1):\n             target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n \n             # find the closest aspect ratio to the target\n-            target_aspect_ratio = self.find_closest_aspect_ratio(aspect_ratio, target_ratios, orig_width, orig_height)\n+            target_aspect_ratio = self.find_closest_aspect_ratio(\n+                aspect_ratio, target_ratios, orig_width, orig_height, image_size\n+            )\n \n             # calculate the target width and height\n             target_width = image_size * target_aspect_ratio[0]\n@@ -148,113 +164,101 @@ def pad_mask_to_max_num_crops(self, masks, max_crops=5):\n             masks = torch.cat([masks, pad], dim=0)\n         return masks\n \n+    @auto_docstring\n     def preprocess(\n         self,\n         images: ImageInput,\n+        **kwargs: Unpack[Phi4MultimodalFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images: List[\"torch.Tensor\"],\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        patch_size: int,\n+        dynamic_hd: int,\n+        do_rescale: bool,\n+        rescale_factor: Optional[float],\n+        do_normalize: bool,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n+        **kwargs,\n     ):\n-        \"\"\"\n-        Args:\n-            images (`ImageInput`):\n-                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n-                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n-                Mean to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n-            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n-                Standard deviation to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return. Can be one of:\n-                - Unset: Return a list of `np.ndarray`.\n-                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n-                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n-                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n-                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n-        \"\"\"\n-        image_mean = image_mean if image_mean is not None else self.image_mean\n-        image_std = image_std if image_std is not None else self.image_std\n-\n-        images = make_flat_list_of_images(images)\n-        if not valid_images(images):\n-            raise ValueError(\n-                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n-                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+        if size.height != size.width:\n+            raise ValueError(\"Phi4MultimodalFastImageProcessor only supports square sizes.\")\n+        mask_size = size.height // patch_size\n+        images_transformed = []\n+        masks_transformed = []\n+        images_tokens = []\n+        image_sizes = []\n+        for image in images:\n+            resized_image, attention_mask = self.dynamic_preprocess(\n+                image, size.height, patch_size, mask_size, max_num=dynamic_hd\n+            )\n+            processed_image = self.rescale_and_normalize(\n+                resized_image, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n             )\n-        images = [convert_to_rgb(image) for image in images]\n-\n-        image_size = self.image_size\n-        patch_size = self.patch_size\n-        mask_size = image_size // patch_size\n-        imgs_and_masks = [self.dynamic_preprocess(image, max_num=self.dynamic_hd) for image in images]\n-        images, image_attention_masks = [x[0] for x in imgs_and_masks], [x[1] for x in imgs_and_masks]\n-\n-        images = [F.to_tensor(image) for image in images]\n-        hd_images = [F.normalize(image, image_mean, image_std) for image in images]\n-        global_image = [\n-            torch.nn.functional.interpolate(\n-                image.unsqueeze(0).float(),\n-                size=(image_size, image_size),\n-                mode=\"bicubic\",\n-            ).to(image.dtype)\n-            for image in hd_images\n-        ]\n-\n-        shapes = [[image.size(1), image.size(2)] for image in hd_images]\n-        mask_shapes = [[mask.size(0), mask.size(1)] for mask in image_attention_masks]\n-        global_attention_mask = [torch.ones((1, mask_size, mask_size)) for _ in hd_images]\n-\n-        hd_images_reshape = []\n-        for im, (h, w) in zip(hd_images, shapes):\n-            im = im.reshape(1, 3, h // image_size, image_size, w // image_size, image_size)\n-            im = im.permute(0, 2, 4, 1, 3, 5)\n-            im = im.reshape(-1, 3, image_size, image_size)\n-            hd_images_reshape.append(im.contiguous())\n-\n-        attention_masks_reshape = []\n-        for mask, (h, w) in zip(image_attention_masks, mask_shapes):\n-            mask = mask.reshape(h // mask_size, mask_size, w // mask_size, mask_size)\n-            mask = mask.transpose(1, 2)\n-            mask = mask.reshape(-1, mask_size, mask_size)\n-            attention_masks_reshape.append(mask.contiguous())\n-\n-        downsample_attention_masks = []\n-        for mask, (h, w) in zip(attention_masks_reshape, mask_shapes):\n-            mask = mask[:, 0::2, 0::2]\n-            mask = mask.reshape(\n-                h // mask_size, w // mask_size, mask_size // 2 + mask_size % 2, mask_size // 2 + mask_size % 2\n+            global_image = self.resize(processed_image, size, interpolation=interpolation, antialias=False)\n+            height, width = processed_image.shape[-2:]\n+            mask_height, mask_width = attention_mask.shape[-2:]\n+            global_attention_mask = torch.ones((1, mask_size, mask_size))\n+\n+            hd_image_reshape = processed_image.reshape(\n+                1, 3, height // size.height, size.height, width // size.width, size.width\n+            )\n+            hd_image_reshape = hd_image_reshape.permute(0, 2, 4, 1, 3, 5)\n+            hd_image_reshape = hd_image_reshape.reshape(-1, 3, size.height, size.width).contiguous()\n+\n+            attention_mask_reshape = attention_mask.reshape(\n+                mask_height // mask_size, mask_size, mask_width // mask_size, mask_size\n             )\n-            mask = mask.transpose(1, 2)\n-            mask = mask.reshape(mask.size(0) * mask.size(1), mask.size(2) * mask.size(3))\n-            downsample_attention_masks.append(mask)\n-\n-        num_img_tokens = [\n-            256 + 1 + int(mask.sum().item()) + int(mask[:, 0].sum().item()) + 16 for mask in downsample_attention_masks\n-        ]\n-\n-        hd_images_reshape = [\n-            torch.cat([_global_image] + [_im], dim=0) for _global_image, _im in zip(global_image, hd_images_reshape)\n-        ]\n-        hd_masks_reshape = [\n-            torch.cat([_global_mask] + [_mask], dim=0)\n-            for _global_mask, _mask in zip(global_attention_mask, attention_masks_reshape)\n-        ]\n-        max_crops = max([img.size(0) for img in hd_images_reshape])\n-        image_transformed = [self.pad_to_max_num_crops(im, max_crops) for im in hd_images_reshape]\n-        image_transformed = torch.stack(image_transformed, dim=0)\n-        mask_transformed = [self.pad_mask_to_max_num_crops(mask, max_crops) for mask in hd_masks_reshape]\n-        mask_transformed = torch.stack(mask_transformed, dim=0)\n-\n-        returned_input_image_embeds = image_transformed\n-        returned_image_sizes = torch.tensor(shapes, dtype=torch.long)\n-        returned_image_attention_mask = mask_transformed\n-        returned_num_img_tokens = num_img_tokens\n+            attention_mask_reshape = attention_mask_reshape.transpose(1, 2)\n+            attention_mask_reshape = attention_mask_reshape.reshape(-1, mask_size, mask_size).contiguous()\n+\n+            downsample_attention_mask = attention_mask_reshape[:, 0::2, 0::2]\n+            downsample_attention_mask = downsample_attention_mask.reshape(\n+                mask_height // mask_size,\n+                mask_width // mask_size,\n+                mask_size // 2 + mask_size % 2,\n+                mask_size // 2 + mask_size % 2,\n+            )\n+            downsample_attention_mask = downsample_attention_mask.transpose(1, 2)\n+            downsample_attention_mask = downsample_attention_mask.reshape(\n+                downsample_attention_mask.size(0) * downsample_attention_mask.size(1),\n+                downsample_attention_mask.size(2) * downsample_attention_mask.size(3),\n+            )\n+\n+            num_img_tokens = (\n+                256\n+                + 1\n+                + int(downsample_attention_mask.sum().item())\n+                + int(downsample_attention_mask[:, 0].sum().item())\n+                + 16\n+            )\n+\n+            hd_image_reshape = torch.cat([global_image.unsqueeze(0), hd_image_reshape], dim=0)\n+            hd_attention_mask_reshape = torch.cat([global_attention_mask, attention_mask_reshape], dim=0)\n+\n+            images_transformed.append(hd_image_reshape)\n+            masks_transformed.append(hd_attention_mask_reshape)\n+            images_tokens.append(num_img_tokens)\n+            image_sizes.append([height, width])\n+            max_crops = hd_image_reshape.size(0)\n+        max_crops = max([img.size(0) for img in images_transformed])\n+        images_transformed = [self.pad_to_max_num_crops(im, max_crops) for im in images_transformed]\n+        images_transformed = torch.stack(images_transformed, dim=0)\n+        masks_transformed = [self.pad_mask_to_max_num_crops(mask, max_crops) for mask in masks_transformed]\n+        masks_transformed = torch.stack(masks_transformed, dim=0)\n+        image_sizes = torch.tensor(image_sizes, dtype=torch.long)\n \n         data = {\n-            \"image_pixel_values\": returned_input_image_embeds,\n-            \"image_sizes\": returned_image_sizes,\n-            \"image_attention_mask\": returned_image_attention_mask,\n-            \"num_img_tokens\": returned_num_img_tokens,\n+            \"image_pixel_values\": images_transformed,\n+            \"image_sizes\": image_sizes,\n+            \"image_attention_mask\": masks_transformed,\n+            \"num_img_tokens\": images_tokens,\n         }\n \n         return BatchFeature(data=data, tensor_type=return_tensors)"
        },
        {
            "sha": "3ad87b5780db7c1f4bcd06b3745adc15629c8136",
            "filename": "tests/models/phi4_multimodal/test_image_processing_phi4_multimodal.py",
            "status": "added",
            "additions": 307,
            "deletions": 0,
            "changes": 307,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3b70b0d1c15c87ba2010b00830fbd92b2c50252/tests%2Fmodels%2Fphi4_multimodal%2Ftest_image_processing_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3b70b0d1c15c87ba2010b00830fbd92b2c50252/tests%2Fmodels%2Fphi4_multimodal%2Ftest_image_processing_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2Ftest_image_processing_phi4_multimodal.py?ref=e3b70b0d1c15c87ba2010b00830fbd92b2c50252",
            "patch": "@@ -0,0 +1,307 @@\n+# coding=utf-8\n+# Copyright 2021 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import inspect\n+import math\n+import unittest\n+import warnings\n+\n+import numpy as np\n+from packaging import version\n+\n+from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    if is_torchvision_available():\n+        from transformers import Phi4MultimodalImageProcessorFast\n+\n+\n+class Phi4MultimodalImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=100,\n+        min_resolution=30,\n+        max_resolution=400,\n+        dynamic_hd=36,\n+        do_resize=True,\n+        size=None,\n+        patch_size=14,\n+        do_normalize=True,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+        do_convert_rgb=True,\n+    ):\n+        super().__init__()\n+        size = size if size is not None else {\"height\": 100, \"width\": 100}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.dynamic_hd = dynamic_hd\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.patch_size = patch_size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"patch_size\": self.patch_size,\n+            \"dynamic_hd\": self.dynamic_hd,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        max_num_patches = 0\n+        for image in images:\n+            if isinstance(image, Image.Image):\n+                width, height = image.size\n+            elif isinstance(image, np.ndarray):\n+                height, width = image.shape[:2]\n+            elif isinstance(image, torch.Tensor):\n+                height, width = image.shape[-2:]\n+            w_crop_num = math.ceil(width / float(self.size[\"width\"]))\n+            h_crop_num = math.ceil(height / float(self.size[\"height\"]))\n+            num_patches = min(w_crop_num * h_crop_num + 1, self.dynamic_hd)\n+            max_num_patches = max(max_num_patches, num_patches)\n+        num_patches = max_num_patches\n+        return num_patches, self.num_channels, self.size[\"height\"], self.size[\"width\"]\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+class Phi4MultimodalImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    fast_image_processing_class = Phi4MultimodalImageProcessorFast if is_torchvision_available() else None\n+    test_slow_image_processor = False\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = Phi4MultimodalImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 100, \"width\": 100})\n+\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+\n+    @unittest.skip(reason=\"Phi4MultimodalImageProcessorFast doesn't treat 4 channel PIL and numpy consistently yet\")\n+    def test_call_numpy_4_channels(self):\n+        pass\n+\n+    def test_cast_dtype_device(self):\n+        for image_processing_class in self.image_processor_list:\n+            if self.test_cast_dtype is not None:\n+                # Initialize image_processor\n+                image_processor = image_processing_class(**self.image_processor_dict)\n+\n+                # create random PyTorch tensors\n+                image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+\n+                encoding = image_processor(image_inputs, return_tensors=\"pt\")\n+                # for layoutLM compatibility\n+                self.assertEqual(encoding.image_pixel_values.device, torch.device(\"cpu\"))\n+                self.assertEqual(encoding.image_pixel_values.dtype, torch.float32)\n+\n+                encoding = image_processor(image_inputs, return_tensors=\"pt\").to(torch.float16)\n+                self.assertEqual(encoding.image_pixel_values.device, torch.device(\"cpu\"))\n+                self.assertEqual(encoding.image_pixel_values.dtype, torch.float16)\n+\n+                encoding = image_processor(image_inputs, return_tensors=\"pt\").to(\"cpu\", torch.bfloat16)\n+                self.assertEqual(encoding.image_pixel_values.device, torch.device(\"cpu\"))\n+                self.assertEqual(encoding.image_pixel_values.dtype, torch.bfloat16)\n+\n+                with self.assertRaises(TypeError):\n+                    _ = image_processor(image_inputs, return_tensors=\"pt\").to(torch.bfloat16, \"cpu\")\n+\n+                # Try with text + image feature\n+                encoding = image_processor(image_inputs, return_tensors=\"pt\")\n+                encoding.update({\"input_ids\": torch.LongTensor([[1, 2, 3], [4, 5, 6]])})\n+                encoding = encoding.to(torch.float16)\n+\n+                self.assertEqual(encoding.image_pixel_values.device, torch.device(\"cpu\"))\n+                self.assertEqual(encoding.image_pixel_values.dtype, torch.float16)\n+                self.assertEqual(encoding.input_ids.dtype, torch.long)\n+\n+    def test_call_pil(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").image_pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").image_pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(\n+                tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n+            )\n+\n+    def test_call_numpy(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").image_pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").image_pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(\n+                tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n+            )\n+\n+    def test_call_pytorch(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").image_pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").image_pixel_values\n+            self.assertEqual(\n+                tuple(encoded_images.shape),\n+                (self.image_processor_tester.batch_size, *expected_output_image_shape),\n+            )\n+\n+    def test_image_processor_preprocess_arguments(self):\n+        is_tested = False\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+\n+            # validation done by _valid_processor_keys attribute\n+            if hasattr(image_processor, \"_valid_processor_keys\") and hasattr(image_processor, \"preprocess\"):\n+                preprocess_parameter_names = inspect.getfullargspec(image_processor.preprocess).args\n+                preprocess_parameter_names.remove(\"self\")\n+                preprocess_parameter_names.sort()\n+                valid_processor_keys = image_processor._valid_processor_keys\n+                valid_processor_keys.sort()\n+                self.assertEqual(preprocess_parameter_names, valid_processor_keys)\n+                is_tested = True\n+\n+            # validation done by @filter_out_non_signature_kwargs decorator\n+            if hasattr(image_processor.preprocess, \"_filter_out_non_signature_kwargs\"):\n+                if hasattr(self.image_processor_tester, \"prepare_image_inputs\"):\n+                    inputs = self.image_processor_tester.prepare_image_inputs()\n+                elif hasattr(self.image_processor_tester, \"prepare_video_inputs\"):\n+                    inputs = self.image_processor_tester.prepare_video_inputs()\n+                else:\n+                    self.skipTest(reason=\"No valid input preparation method found\")\n+\n+                with warnings.catch_warnings(record=True) as raised_warnings:\n+                    warnings.simplefilter(\"always\")\n+                    image_processor(inputs, extra_argument=True)\n+\n+                messages = \" \".join([str(w.message) for w in raised_warnings])\n+                self.assertGreaterEqual(len(raised_warnings), 1)\n+                self.assertIn(\"extra_argument\", messages)\n+                is_tested = True\n+\n+        if not is_tested:\n+            self.skipTest(reason=\"No validation found for `preprocess` method\")\n+\n+    @slow\n+    def test_can_compile_fast_image_processor(self):\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        torch.compiler.reset()\n+        input_image = torch.randint(0, 255, (3, 224, 224), dtype=torch.uint8)\n+        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n+        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+\n+        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n+        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+\n+        torch.testing.assert_close(\n+            output_eager.image_pixel_values, output_compiled.image_pixel_values, rtol=1e-4, atol=1e-4\n+        )"
        },
        {
            "sha": "4dd51cc34e6c384a61348a4ef729372444c4a482",
            "filename": "tests/models/phi4_multimodal/test_modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3b70b0d1c15c87ba2010b00830fbd92b2c50252/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3b70b0d1c15c87ba2010b00830fbd92b2c50252/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py?ref=e3b70b0d1c15c87ba2010b00830fbd92b2c50252",
            "patch": "@@ -31,12 +31,7 @@\n     is_torch_available,\n     is_vision_available,\n )\n-from transformers.testing_utils import (\n-    require_soundfile,\n-    require_torch,\n-    slow,\n-    torch_device,\n-)\n+from transformers.testing_utils import require_soundfile, require_torch, slow, torch_device\n from transformers.utils import is_soundfile_available\n \n from ...generation.test_utils import GenerationTesterMixin\n@@ -285,6 +280,8 @@ class Phi4MultimodalIntegrationTest(unittest.TestCase):\n     audio_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"\n \n     def setUp(self):\n+        # Currently, the Phi-4 checkpoint on the hub is not working with the latest Phi-4 code, so the slow integration tests\n+        # won't pass without using the correct revision (refs/pr/70)\n         self.processor = AutoProcessor.from_pretrained(self.checkpoint_path)\n         self.generation_config = GenerationConfig(max_new_tokens=20, do_sample=False)\n         self.user_token = \"<|user|>\"\n@@ -325,7 +322,7 @@ def test_vision_text_generation(self):\n             self.checkpoint_path, torch_dtype=torch.float16, device_map=torch_device\n         )\n \n-        prompt = f\"{self.user_token}<|image_1|>What is shown in this image?{self.end_token}{self.assistant_token}\"\n+        prompt = f\"{self.user_token}<|image|>What is shown in this image?{self.end_token}{self.assistant_token}\"\n         inputs = self.processor(prompt, images=self.image, return_tensors=\"pt\").to(torch_device)\n \n         output = model.generate(\n@@ -349,7 +346,7 @@ def test_multi_image_vision_text_generation(self):\n         for i in range(1, 5):\n             url = f\"https://image.slidesharecdn.com/azureintroduction-191206101932/75/Introduction-to-Microsoft-Azure-Cloud-{i}-2048.jpg\"\n             images.append(Image.open(requests.get(url, stream=True).raw))\n-            placeholder += f\"<|image_{i}|>\"\n+            placeholder += \"<|image|>\"\n \n         prompt = f\"{self.user_token}{placeholder}Summarize the deck of slides.{self.end_token}{self.assistant_token}\"\n         inputs = self.processor(prompt, images, return_tensors=\"pt\").to(torch_device)\n@@ -371,8 +368,8 @@ def test_audio_text_generation(self):\n             self.checkpoint_path, torch_dtype=torch.float16, device_map=torch_device\n         )\n \n-        prompt = f\"{self.user_token}<|audio_1|>What is happening in this audio?{self.end_token}{self.assistant_token}\"\n-        inputs = self.processor(prompt, audios=self.audio, sampling_rate=self.sampling_rate, return_tensors=\"pt\").to(\n+        prompt = f\"{self.user_token}<|audio|>What is happening in this audio?{self.end_token}{self.assistant_token}\"\n+        inputs = self.processor(prompt, audio=self.audio, sampling_rate=self.sampling_rate, return_tensors=\"pt\").to(\n             torch_device\n         )\n "
        },
        {
            "sha": "f70adea169c6d01a484e662f883b08264a75db9b",
            "filename": "tests/test_image_processing_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3b70b0d1c15c87ba2010b00830fbd92b2c50252/tests%2Ftest_image_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3b70b0d1c15c87ba2010b00830fbd92b2c50252/tests%2Ftest_image_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_image_processing_common.py?ref=e3b70b0d1c15c87ba2010b00830fbd92b2c50252",
            "patch": "@@ -279,7 +279,7 @@ def test_image_processor_save_load_with_autoimageprocessor(self):\n                 saved_file = image_processor_first.save_pretrained(tmpdirname)[0]\n                 check_json_file_has_correct_format(saved_file)\n \n-                use_fast = i == 1\n+                use_fast = i == 1 or not self.test_slow_image_processor\n                 image_processor_second = AutoImageProcessor.from_pretrained(tmpdirname, use_fast=use_fast)\n \n             self.assertEqual(image_processor_second.to_dict(), image_processor_first.to_dict())"
        }
    ],
    "stats": {
        "total": 570,
        "additions": 439,
        "deletions": 131
    }
}