{
    "author": "ArthurZucker",
    "message": "Multiple llama4 fixe (#37353)\n\n* update for fixes\n\n* more fixes\n\n* fuxix dynamic cache?\n\n* style\n\n* fix both traiining and generating. Eager seems alright\n\n* dynamic does not work\n\n* fix most cases, use_cache or not, eager or not, no default cache (ex: not training but you want to get cache states)\n\n* should be final fixes\n\n* fix more stuff no cat\n\n* style\n\n* fix\n\n* style\n\n* final sytle\n\n* qualityeioiwhjfaopsejdpofqsdjkfjha;wesdhgfkjlqsw.denghjkaswednkgs\n\n* fix\n\n* revert",
    "sha": "2da82e432dbc08f9e497b353cdccfee7e84bd6a8",
    "files": [
        {
            "sha": "760e676d96b89ddf69c0372d165376d227429e8c",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 32,
            "deletions": 28,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/2da82e432dbc08f9e497b353cdccfee7e84bd6a8/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2da82e432dbc08f9e497b353cdccfee7e84bd6a8/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=2da82e432dbc08f9e497b353cdccfee7e84bd6a8",
            "patch": "@@ -1857,7 +1857,7 @@ class HybridChunkedCache(Cache):\n \n     # TODO (joao): dive deeper into gemma2 and paligemma -- there are reports of speed loss with compilation. Revert\n     # ALL changes from the PR that commented the line below when reactivating it.\n-    # is_compileable = True\n+    is_compileable = True\n \n     def __init__(\n         self,\n@@ -1912,26 +1912,37 @@ def initialise_cache_layer(self, layer_idx, key_states):\n         self.value_cache.append(new_layer_value_cache)\n \n     def _sliding_update(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len):\n-        cumulative_length = self.cumulative_length[layer_idx]\n-        is_full = cumulative_length >= max_cache_len\n-        if is_full:\n-            full_key_states = torch.cat((k_out[:, :, 1:, :], key_states), dim=-2)\n-            full_value_states = torch.cat((v_out[:, :, 1:, :], value_states), dim=-2)\n-        elif not is_full and cumulative_length + key_states.shape[2] > max_cache_len:\n-            full_key_states = torch.cat((k_out[:, :, :cumulative_length, :], key_states), dim=-2)\n-            full_value_states = torch.cat((v_out[:, :, :cumulative_length, :], value_states), dim=-2)\n-        else:\n-            self.key_cache[layer_idx].index_copy_(2, cache_position, key_states)\n-            self.value_cache[layer_idx].index_copy_(2, cache_position, value_states)\n-            self.cumulative_length[layer_idx] += key_states.shape[-2]\n-            return self.key_cache[layer_idx], self.value_cache[layer_idx]\n-\n-        self.key_cache[layer_idx].copy_(full_key_states[:, :, -max_cache_len:, :])\n-        self.value_cache[layer_idx].copy_(full_value_states[:, :, -max_cache_len:, :])\n-        self.cumulative_length[layer_idx] += key_states.shape[-2]\n-        # we should return the whole states instead of k_out, v_out to take the whole prompt\n-        # into consideration when building kv cache instead of just throwing away tokens outside of the window\n-        return full_key_states, full_value_states\n+        if cache_position.shape[0] > max_cache_len:\n+            cache_position = cache_position.clamp(0, max_cache_len - 1)\n+            k_out = key_states[:, :, -max_cache_len:, :]\n+            v_out = value_states[:, :, -max_cache_len:, :]\n+            # Assumption: caches are all zeros at this point, `+=` is equivalent to `=` but compile-friendly\n+            self.key_cache[layer_idx].zero_()\n+            self.value_cache[layer_idx].zero_()\n+\n+            self.key_cache[layer_idx] += k_out\n+            self.value_cache[layer_idx] += v_out\n+            # we should return the whole states instead of k_out, v_out to take the whole prompt\n+            # into consideration when building kv cache instead of just throwing away tokens outside of the window\n+            return key_states, value_states\n+\n+        # otherwise we are decoding. Most efficient way to cat 1 token\n+        slicing = torch.ones(max_cache_len, dtype=torch.long, device=value_states.device).cumsum(0)\n+        cache_position = cache_position.clamp(0, max_cache_len - 1)\n+        to_shift = cache_position >= max_cache_len - 1\n+        indices = (slicing + to_shift[-1].int() - 1) % max_cache_len\n+        k_out = k_out[:, :, indices]\n+        v_out = v_out[:, :, indices]\n+\n+        k_out[:, :, cache_position] = key_states\n+        v_out[:, :, cache_position] = value_states\n+        # `_.zero()` followed by `+=` is equivalent `=`, but compile-friendly (without graph breaks due to assignment)\n+        self.key_cache[layer_idx].zero_()\n+        self.value_cache[layer_idx].zero_()\n+\n+        self.key_cache[layer_idx] += k_out\n+        self.value_cache[layer_idx] += v_out\n+        return k_out, v_out\n \n     def _static_update(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len):\n         k_out[:, :, cache_position] = key_states\n@@ -1953,13 +1964,6 @@ def update(\n         cache_position = cache_kwargs.get(\"cache_position\")\n         self.initialise_cache_layer(layer_idx, key_states)\n \n-        # These two `if` blocks are only reached in multigpu and if `layer_device_map` is not passed. They are used\n-        # when the cache is initialized in the forward pass (e.g. Gemma2)\n-        if self.key_cache[layer_idx].device != key_states.device:\n-            self.key_cache[layer_idx] = self.key_cache[layer_idx].to(key_states.device)\n-        if self.value_cache[layer_idx].device != value_states.device:\n-            self.value_cache[layer_idx] = self.value_cache[layer_idx].to(value_states.device)\n-\n         k_out = self.key_cache[layer_idx]\n         v_out = self.value_cache[layer_idx]\n         key_states = key_states.to(k_out.dtype)"
        },
        {
            "sha": "b68de89f66afb64ecc2d8d979e2ce315f70f76b8",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2da82e432dbc08f9e497b353cdccfee7e84bd6a8/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2da82e432dbc08f9e497b353cdccfee7e84bd6a8/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=2da82e432dbc08f9e497b353cdccfee7e84bd6a8",
            "patch": "@@ -1961,6 +1961,9 @@ def _prepare_cache_for_generation(\n             )\n             generation_config.cache_implementation = None\n \n+        generation_config.cache_implementation = generation_config.cache_implementation or getattr(\n+            self.config.get_text_config(), \"cache_implementation\", None\n+        )\n         if generation_config.cache_implementation is not None:\n             if generation_config.cache_implementation in NEED_SETUP_CACHE_CLASSES_MAPPING:\n                 if generation_config.cache_implementation == \"static\" and not self._supports_static_cache:"
        },
        {
            "sha": "ec4ebcb22d284196126a5af04eaab2213d1bb4ca",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/2da82e432dbc08f9e497b353cdccfee7e84bd6a8/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2da82e432dbc08f9e497b353cdccfee7e84bd6a8/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=2da82e432dbc08f9e497b353cdccfee7e84bd6a8",
            "patch": "@@ -57,15 +57,15 @@ def __new__(cls, *args, **kwargs):\n         return cls._instance\n \n     @torch.compiler.disable(recursive=False)\n-    def __init__(self):\n+    def __init__(self, training):\n         \"\"\"\n         Initialize or update the singleton instance.\n         \"\"\"\n         if not self._is_flex_compiled:\n             # In PyTorch 2.6.0, there's a known issue with flex attention compilation which may\n             # cause errors. The suggested fix is to compile with \"max-autotune-no-cudagraphs\"\n             # see https://github.com/pytorch/pytorch/issues/146260 for training\n-            if _torch_version == \"2.6.0\":\n+            if _torch_version == \"2.6.0\" and training:\n                 self._compiled_flex_attention = torch.compile(\n                     flex_attention, dynamic=False, mode=\"max-autotune-no-cudagraphs\"\n                 )\n@@ -167,10 +167,11 @@ def compile_friendly_flex_attention(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n+    training=False,\n     **kwargs,\n ) -> torch.Tensor:\n     # First call initialise singleton wrapper object, second call invokes the object method to return compiled flex attention\n-    flex_attention_compiled = WrappedFlexAttention()()\n+    flex_attention_compiled = WrappedFlexAttention(training)()\n     return flex_attention_compiled(\n         query,\n         key,\n@@ -243,6 +244,7 @@ def score_mod(score, batch_idx, head_idx, q_idx, kv_idx):\n         # Last time checked on PyTorch == 2.5.1: Flex Attention always computes the lse regardless.\n         # For simplification, we thus always return it as no additional computations are introduced.\n         return_lse=True,\n+        training=module.training,\n     )\n     # lse is returned in float32\n     attention_weights = attention_weights.to(value.dtype)"
        },
        {
            "sha": "0013f6b333873f46f0d97610bc0ddc90b4830b16",
            "filename": "src/transformers/models/llama4/configuration_llama4.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2da82e432dbc08f9e497b353cdccfee7e84bd6a8/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2da82e432dbc08f9e497b353cdccfee7e84bd6a8/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py?ref=2da82e432dbc08f9e497b353cdccfee7e84bd6a8",
            "patch": "@@ -231,6 +231,7 @@ class Llama4TextConfig(PretrainedConfig):\n         attn_temperature_tuning (`int`, *optional*, defaults to 4): TODO\n         floor_scale (`int`, *optional*, defaults to 8192): TODO\n         attn_scale (`int`, *optional*, defaults to 0.1): TODO\n+        cache_implementation (`<fill_type>`, *optional*, defaults to `\"hybrid\"`): <fill_docstring>\n \n     Example:\n     \"\"\"\n@@ -293,6 +294,7 @@ def __init__(\n         attn_temperature_tuning=4,\n         floor_scale=8192,\n         attn_scale=0.1,\n+        cache_implementation=\"hybrid\",\n         **kwargs,\n     ):\n         super().__init__(\n@@ -314,7 +316,7 @@ def __init__(\n         self.num_attention_heads = num_attention_heads\n         self.rope_scaling = rope_scaling\n         self.attention_bias = False\n-\n+        self.cache_implementation = cache_implementation\n         # for backward compatibility\n         if num_key_value_heads is None:\n             num_key_value_heads = num_attention_heads\n@@ -417,7 +419,6 @@ def __init__(\n         self.boi_token_index = boi_token_index\n         self.eoi_token_index = eoi_token_index\n         self.image_token_index = image_token_index\n-\n         if text_config is None:\n             self.text_config = Llama4TextConfig()\n             logger.info(\"text_config is None, using default llama4 text config\")"
        },
        {
            "sha": "ac466f6de917c0ff1d8fb9ee5dc9ec97084d57cb",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 21,
            "deletions": 16,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/2da82e432dbc08f9e497b353cdccfee7e84bd6a8/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2da82e432dbc08f9e497b353cdccfee7e84bd6a8/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=2da82e432dbc08f9e497b353cdccfee7e84bd6a8",
            "patch": "@@ -25,7 +25,7 @@\n from transformers.models.llama4.configuration_llama4 import Llama4VisionConfig\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache\n+from ...cache_utils import Cache, HybridChunkedCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -655,7 +655,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids.to(self.embed_tokens.weight.device))\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = HybridChunkedCache(self.config, inputs_embeds.shape[0], inputs_embeds.shape[1])\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -667,7 +667,7 @@ def forward(\n             position_ids = cache_position.unsqueeze(0)\n \n         causal_mask, chunk_causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions, use_cache=use_cache\n         )\n \n         hidden_states = inputs_embeds\n@@ -730,7 +730,7 @@ def forward(\n         )\n         return output if return_dict else output.to_tuple()\n \n-    @torch.compiler.disable  # the operations in this method are not compilable\n+    @torch.compiler.disable(recursive=False)  # the operations in this method are not compilable\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -739,6 +739,7 @@ def _update_causal_mask(\n         past_key_values: Cache,\n         output_attentions: bool = False,\n         chunked_attention_mask=None,\n+        use_cache=True,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():\n@@ -755,23 +756,27 @@ def _update_causal_mask(\n         first_cache_position = cache_position[0]\n         last_cache_position = cache_position[-1]\n \n+        if past_key_values is not None:\n+            full_cache_length = past_key_values.get_max_cache_shape() or sequence_length\n+        else:\n+            full_cache_length = attention_mask.shape[-1] if attention_mask is not None else sequence_length\n+\n         # to avoid graph break, we introduce this hack\n         cond1 = first_cache_position >= attention_chunk_size\n         cond2 = (first_cache_position < attention_chunk_size) & (\n             first_cache_position + sequence_length > attention_chunk_size\n         )\n \n-        key_length = torch.where(\n-            cond1,\n-            attention_chunk_size + sequence_length - 1,\n-            torch.where(cond2, first_cache_position + sequence_length, attention_chunk_size),\n+        key_length = (\n+            torch.where(\n+                cond1,\n+                attention_chunk_size + sequence_length - 1,\n+                torch.where(cond2, first_cache_position + sequence_length, attention_chunk_size),\n+            )\n+            if use_cache\n+            else full_cache_length\n         )\n \n-        if past_key_values is not None and past_key_values.is_compileable:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = attention_mask.shape[-1] if attention_mask is not None else sequence_length\n-\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 offsets = (first_cache_position, max(last_cache_position - key_length, 0))\n@@ -781,7 +786,7 @@ def _update_causal_mask(\n                 attention_mask = make_flex_block_causal_mask(\n                     attention_mask,\n                     query_length=sequence_length,\n-                    key_length=target_length,\n+                    key_length=full_cache_length,\n                     offsets=None if sequence_length != 1 else (first_cache_position, 0),\n                 )\n                 return attention_mask, chunked_attention_mask\n@@ -793,13 +798,13 @@ def _update_causal_mask(\n         causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n             attention_mask,\n             sequence_length=sequence_length,\n-            target_length=target_length,\n+            target_length=full_cache_length,\n             dtype=dtype,\n             device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n-        if target_length > self.config.attention_chunk_size:\n+        if full_cache_length > self.config.attention_chunk_size:\n             chunked_attention_mask = self.create_chunked_attention_mask(\n                 self.config.attention_chunk_size,\n                 start=first_cache_position,"
        },
        {
            "sha": "7b9744f4a9098c1bbe9235430fa2cd232bf4677e",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2da82e432dbc08f9e497b353cdccfee7e84bd6a8/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2da82e432dbc08f9e497b353cdccfee7e84bd6a8/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=2da82e432dbc08f9e497b353cdccfee7e84bd6a8",
            "patch": "@@ -244,6 +244,7 @@\n         \"output_router_logits\",\n         \"router_aux_loss_coef\",\n         \"router_jitter_noise\",\n+        \"cache_implementation\",\n     ],\n     \"Llama4VisionConfig\": [\"multi_modal_projector_bias\", \"norm_eps\"],\n }"
        },
        {
            "sha": "b8a4406c08a43ccc46bb9f16cc3a242ccd61a054",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2da82e432dbc08f9e497b353cdccfee7e84bd6a8/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2da82e432dbc08f9e497b353cdccfee7e84bd6a8/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=2da82e432dbc08f9e497b353cdccfee7e84bd6a8",
            "patch": "@@ -580,6 +580,7 @@\n     \"ZeroShotClassificationPipeline\",\n     \"ZeroShotImageClassificationPipeline\",\n     \"ZeroShotObjectDetectionPipeline\",\n+    \"Llama4TextConfig\",\n ]\n \n # Supported math operations when interpreting the value of defaults."
        }
    ],
    "stats": {
        "total": 115,
        "additions": 66,
        "deletions": 49
    }
}