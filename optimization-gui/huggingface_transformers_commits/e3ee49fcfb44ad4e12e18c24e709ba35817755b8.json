{
    "author": "keyboardAnt",
    "message": "Refactoring `AssistedCandidateGenerator` for Improved Modularity and Reusability (#35009)\n\n* move `TestAssistedCandidateGeneratorDifferentTokenizers` into a new testing file\r\n\r\n* refactor\r\n\r\n* NOTHING. add space to rerun github actions tests\r\n\r\n* remove it...\r\n\r\n* NOTHING. add space to rerun github actions tests\r\n\r\n* remove it...\r\n\r\n* replace: `self.prev_tokens` -> `self.prev_assistant_ids`\r\n\r\n* NOTHING. rerun CI tests\r\n\r\n* remove it\r\n\r\n* introduce `self.prev_target_ids_len`\r\n\r\n* fix style\r\n\r\n* fix style\r\n\r\n---------\r\n\r\nCo-authored-by: Jonathan Mamou <jonathan.mamou@intel.com>",
    "sha": "e3ee49fcfb44ad4e12e18c24e709ba35817755b8",
    "files": [
        {
            "sha": "ba5d0f0005a6798a4a8abd83e4610d391ad0cfc7",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 96,
            "deletions": 100,
            "changes": 196,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3ee49fcfb44ad4e12e18c24e709ba35817755b8/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3ee49fcfb44ad4e12e18c24e709ba35817755b8/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=e3ee49fcfb44ad4e12e18c24e709ba35817755b8",
            "patch": "@@ -208,56 +208,15 @@ def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor,\n             vocabulary_size)` containing the logits associated to each candidate.\n         \"\"\"\n         input_ids = input_ids.to(self.assistant_model.device)\n-\n-        # Don't generate more than `max_length - 1` candidates since the target model generates one extra token.\n-        new_cur_len = input_ids.shape[-1]\n-        max_new_tokens = min(int(self.num_assistant_tokens), self.generation_config.max_length - new_cur_len - 1)\n-        min_new_tokens = max(min(max_new_tokens, self.main_model_min_length - new_cur_len), 0)\n+        # Calculate new tokens to generate\n+        min_new_tokens, max_new_tokens = self._calculate_new_tokens(input_ids)\n         if max_new_tokens == 0:\n             return input_ids, None\n-\n-        # 1. If it is not the first round of candidate generation, prepare the inputs based on the input_ids length\n-        # (which implicitly contains the number of accepted candidates from the previous round)\n-        has_past_key_values = self.assistant_kwargs.get(\"past_key_values\", None) is not None\n-        if has_past_key_values:\n-            new_cache_size = new_cur_len - 1\n-            self.assistant_kwargs[\"past_key_values\"] = _crop_past_key_values(\n-                self.assistant_model, self.assistant_kwargs[\"past_key_values\"], new_cache_size - 1\n-            )  # the assistant does not have the token after the last match, hence the -1\n-\n-            self.assistant_kwargs = _prepare_attention_mask(\n-                self.assistant_kwargs, new_cur_len, self.assistant_model.config.is_encoder_decoder\n-            )\n-            self.assistant_kwargs = _prepare_token_type_ids(self.assistant_kwargs, new_cur_len)\n-\n-        # 2. Forecast next N tokens using the assistant model.\n-        assistant_generation_kwargs = {\n-            self.input_ids_key: input_ids,\n-            \"min_new_tokens\": min_new_tokens,\n-            \"max_new_tokens\": max_new_tokens,\n-            \"generation_config\": self.generation_config,\n-            \"logits_processor\": self.logits_processor,\n-        }\n-\n-        assistant_output = self.assistant_model.generate(**assistant_generation_kwargs, **self.assistant_kwargs)\n-\n-        # 3. Update variables for the next round of candidate generation\n-        self.assistant_kwargs[\"past_key_values\"] = assistant_output.past_key_values\n-\n-        if (\n-            is_sklearn_available()\n-            and self.assistant_model.generation_config.assistant_confidence_threshold\n-            and type(self) is AssistedCandidateGenerator\n-        ):\n-            scores_tensor = torch.cat(assistant_output.scores, dim=0)\n-            scores_softmax = torch.softmax(scores_tensor, dim=-1)\n-            ids = assistant_output.sequences[-1, -len(assistant_output.scores) :]\n-            p = scores_softmax[range(len(ids)), ids]\n-            self.probs.extend(p.tolist())\n-\n-        # 4. Prepare variables for output\n-        candidate_logits = torch.stack(assistant_output.scores, dim=1)\n-        candidate_ids = assistant_output.sequences\n+        # Update past key values and masks\n+        self._update_past_and_masks(input_ids)\n+        # Generate candidates\n+        generation_args = self._prepare_generation_args(input_ids, min_new_tokens, max_new_tokens)\n+        candidate_ids, candidate_logits = self._generate_candidates(generation_args)\n         return candidate_ids, candidate_logits\n \n     def update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, num_matches: int):\n@@ -318,6 +277,55 @@ def update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.F\n \n                 self.assistant_model.generation_config.assistant_confidence_threshold = best_threshold\n \n+    def _calculate_new_tokens(self, input_ids: torch.LongTensor) -> Tuple[int, int]:\n+        \"\"\"Calculate the minimum and maximum number of new tokens to generate.\"\"\"\n+        new_cur_len = input_ids.shape[-1]\n+        max_new_tokens = min(int(self.num_assistant_tokens), self.generation_config.max_length - new_cur_len - 1)\n+        min_new_tokens = max(min(max_new_tokens, self.main_model_min_length - new_cur_len), 0)\n+        return min_new_tokens, max_new_tokens\n+\n+    def _update_past_and_masks(self, input_ids: torch.LongTensor, remove_from_pkv: int = 0) -> bool:\n+        \"\"\"Update past key values and attention masks for subsequent generation rounds.\"\"\"\n+        has_past_key_values = self.assistant_kwargs.get(\"past_key_values\", None) is not None\n+        if has_past_key_values:\n+            new_cache_size = input_ids.shape[-1] - 1 - remove_from_pkv\n+            self.assistant_kwargs[\"past_key_values\"] = _crop_past_key_values(\n+                self.assistant_model, self.assistant_kwargs[\"past_key_values\"], new_cache_size - 1\n+            )\n+            self.assistant_kwargs = _prepare_attention_mask(\n+                self.assistant_kwargs, input_ids.shape[-1], self.assistant_model.config.is_encoder_decoder\n+            )\n+            self.assistant_kwargs = _prepare_token_type_ids(self.assistant_kwargs, input_ids.shape[-1])\n+        return has_past_key_values\n+\n+    def _prepare_generation_args(self, input_ids: torch.LongTensor, min_new_tokens: int, max_new_tokens: int) -> Dict:\n+        \"\"\"Prepare arguments for the generation call.\"\"\"\n+        return {\n+            self.input_ids_key: input_ids,\n+            \"min_new_tokens\": min_new_tokens,\n+            \"max_new_tokens\": max_new_tokens,\n+            \"generation_config\": self.generation_config,\n+            \"logits_processor\": self.logits_processor,\n+        }\n+\n+    def _generate_candidates(self, generation_args: Dict) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n+        \"\"\"Generate candidate sequences using the assistant model.\"\"\"\n+        assistant_output = self.assistant_model.generate(**generation_args, **self.assistant_kwargs)\n+        self.assistant_kwargs[\"past_key_values\"] = assistant_output.past_key_values\n+        if (\n+            is_sklearn_available()\n+            and self.assistant_model.generation_config.assistant_confidence_threshold\n+            and type(self) is AssistedCandidateGenerator\n+        ):\n+            scores_tensor = torch.cat(assistant_output.scores, dim=0)\n+            scores_softmax = torch.softmax(scores_tensor, dim=-1)\n+            ids = assistant_output.sequences[-1, -len(assistant_output.scores) :]\n+            p = scores_softmax[range(len(ids)), ids]\n+            self.probs.extend(p.tolist())\n+        candidate_logits = torch.stack(assistant_output.scores, dim=1)\n+        candidate_ids = assistant_output.sequences\n+        return candidate_ids, candidate_logits\n+\n \n class AssistedCandidateGeneratorDifferentTokenizers(AssistedCandidateGenerator):\n     \"\"\"\n@@ -367,6 +375,7 @@ def __init__(\n \n         self.target_tokenizer = target_tokenizer\n         self.assistant_tokenizer = assistant_tokenizer\n+        self.prev_target_ids_len: Optional[int] = None\n         self.prev_assistant_ids = None\n         self.target_lookbehind = assistant_model.generation_config.target_lookbehind\n         self.assistant_lookbehind = assistant_model.generation_config.assistant_lookbehind\n@@ -497,27 +506,50 @@ def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor,\n             return input_ids, None\n \n         input_ids = input_ids.to(self.assistant_model.device)\n+        remove_from_pkv = 0\n+\n+        assistant_input_ids, remove_from_pkv = self._prepare_assistant_input_ids(input_ids)\n+        self.prev_assistant_ids = assistant_input_ids\n+\n+        min_new_tokens = max(min(max_new_tokens, self.main_model_min_length - assistant_input_ids.shape[-1]), 0)\n+\n+        self._update_past_and_masks(assistant_input_ids, remove_from_pkv)\n+        generation_args = self._prepare_generation_args(assistant_input_ids, min_new_tokens, max_new_tokens)\n+        self.assistant_kwargs.pop(\"attention_mask\", None)\n+\n+        assistant_output = self.assistant_model.generate(**generation_args, **self.assistant_kwargs)\n+        new_target_ids = self._process_assistant_outputs(input_ids, assistant_output.sequences, assistant_input_ids)\n+\n+        # Update state\n+        self.prev_target_ids_len = input_ids.shape[1]\n+        self.assistant_kwargs[\"past_key_values\"] = assistant_output.past_key_values\n+        self.prev_assistant_ids = assistant_output.sequences\n+\n+        if self.prev_target_ids_len >= new_target_ids.shape[1]:\n+            return input_ids, None\n+\n+        return new_target_ids, None\n+\n+    def _prepare_assistant_input_ids(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, int]:\n+        \"\"\"Converts target input IDs to assistant input IDs, handling discrepancies.\"\"\"\n         convert_kwargs = {\n             \"source_tokenizer\": self.target_tokenizer,\n             \"destination_tokenizer\": self.assistant_tokenizer,\n         }\n         remove_from_pkv = 0\n \n-        # Since re-encoding the tokens may result in tokenization discrepancies, we use 2 look behind values\n-        # (one for each conversion) which mark where to start looking for the overlap between the\n-        # source and target encodings, to ensure the new tokens include the correct prompt suffix.\n-        if self.prev_assistant_ids is not None and input_ids.shape[1] > self.target_lookbehind:\n+        if self.prev_assistant_ids is not None and self.prev_target_ids_len > self.target_lookbehind:\n             # input_ids contains all target prompt input ids and some new target input ids\n-            start_index_in_target_window = input_ids.shape[1] - self.target_lookbehind\n+            start_index_in_target_window = self.prev_target_ids_len - self.target_lookbehind\n \n             new_assistant_ids = self.convert_source_tokens_to_target_tokens(\n                 input_ids[:, start_index_in_target_window:], **convert_kwargs\n             )\n             prompt_use_length = new_assistant_ids.shape[1]\n             prompt_use = self.prev_assistant_ids[:, -prompt_use_length:]\n \n-            discrepancy_length, new_tokens_only, discrepancy_only = (\n-                AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(prompt_use, new_assistant_ids)\n+            discrepancy_length, new_tokens_only, discrepancy_only = self._get_tokens_diag(\n+                prompt_use, new_assistant_ids\n             )\n             assistant_input_ids = self.prev_assistant_ids\n \n@@ -538,58 +570,29 @@ def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor,\n             else:\n                 # edge case: in case of no intersection between prompt and new_assistant_ids\n                 assistant_input_ids = torch.cat([assistant_input_ids, new_assistant_ids], dim=-1)\n-\n         else:\n             assistant_input_ids = self.convert_source_tokens_to_target_tokens(input_ids, **convert_kwargs)\n+            self.prev_target_ids_len = input_ids.shape[1]\n \n-        self.prev_assistant_ids = assistant_input_ids\n-        new_cur_len = assistant_input_ids.shape[-1]\n-        min_new_tokens = max(min(max_new_tokens, self.main_model_min_length - new_cur_len), 0)\n-\n-        # 1. If it is not the first round of candidate generation, prepare the inputs based on the input_ids length\n-        # (which implicitly contains the number of accepted candidates from the previous round)\n-        has_past_key_values = self.assistant_kwargs.get(\"past_key_values\", None) is not None\n-        if has_past_key_values:\n-            new_cache_size = new_cur_len - 1 - remove_from_pkv\n-            self.assistant_kwargs[\"past_key_values\"] = _crop_past_key_values(\n-                self.assistant_model, self.assistant_kwargs[\"past_key_values\"], new_cache_size - 1\n-            )  # the assistant does not have the token after the last match, hence the -1\n-\n-            self.assistant_kwargs = _prepare_attention_mask(\n-                self.assistant_kwargs, new_cur_len, self.assistant_model.config.is_encoder_decoder\n-            )\n-            self.assistant_kwargs = _prepare_token_type_ids(self.assistant_kwargs, new_cur_len)\n-\n-        # 2. Forecast next N tokens using the assistant model.\n-        assistant_generation_kwargs = {\n-            self.input_ids_key: assistant_input_ids,\n-            \"min_new_tokens\": min_new_tokens,\n-            \"max_new_tokens\": max_new_tokens,\n-            \"generation_config\": self.generation_config,\n-            \"logits_processor\": self.logits_processor,\n-        }\n-\n-        self.assistant_kwargs.pop(\"attention_mask\", None)\n-\n-        assistant_output = self.assistant_model.generate(**assistant_generation_kwargs, **self.assistant_kwargs)\n+        return assistant_input_ids, remove_from_pkv\n \n+    def _process_assistant_outputs(\n+        self, input_ids: torch.LongTensor, assistant_sequences: torch.LongTensor, assistant_input_ids: torch.LongTensor\n+    ) -> torch.LongTensor:\n+        \"\"\"Processes assistant outputs to obtain target input IDs.\"\"\"\n         num_prev_assistant = self.prev_assistant_ids.shape[1]\n         start_assistant_look_index = num_prev_assistant - self.assistant_lookbehind\n-        if start_assistant_look_index < 0:\n-            start_assistant_look_index = 0\n \n         new_target_ids_from_window = self.convert_source_tokens_to_target_tokens(\n-            assistant_output.sequences[:, start_assistant_look_index:],\n+            assistant_sequences[:, start_assistant_look_index:],\n             source_tokenizer=self.assistant_tokenizer,\n             destination_tokenizer=self.target_tokenizer,\n         )\n         target_prompt_use_length = new_target_ids_from_window.shape[1]\n \n         target_prompt_use = input_ids[:, -target_prompt_use_length:]\n \n-        _, target_new_tokens_only, _ = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(\n-            target_prompt_use, new_target_ids_from_window\n-        )\n+        _, target_new_tokens_only, _ = self._get_tokens_diag(target_prompt_use, new_target_ids_from_window)\n \n         new_target_ids = input_ids\n \n@@ -603,14 +606,7 @@ def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor,\n         if hasattr(self.generation_config, \"max_length\"):\n             new_target_ids = new_target_ids[:, : self.generation_config.max_length]\n \n-        # 3. Update variables for the next round of candidate generation\n-        self.assistant_kwargs[\"past_key_values\"] = assistant_output.past_key_values\n-\n-        # 4. Prepare variables for output\n-        if input_ids.shape[1] >= new_target_ids.shape[1]:\n-            return input_ids, None\n-\n-        return new_target_ids, None\n+        return new_target_ids\n \n \n class PromptLookupCandidateGenerator(CandidateGenerator):"
        },
        {
            "sha": "03fd51324b022f175bfb5b3ed33fa4510c83ff72",
            "filename": "tests/generation/test_candidate_generator.py",
            "status": "added",
            "additions": 43,
            "deletions": 0,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3ee49fcfb44ad4e12e18c24e709ba35817755b8/tests%2Fgeneration%2Ftest_candidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3ee49fcfb44ad4e12e18c24e709ba35817755b8/tests%2Fgeneration%2Ftest_candidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_candidate_generator.py?ref=e3ee49fcfb44ad4e12e18c24e709ba35817755b8",
            "patch": "@@ -0,0 +1,43 @@\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.generation.candidate_generator import AssistedCandidateGeneratorDifferentTokenizers\n+\n+\n+class TestAssistedCandidateGeneratorDifferentTokenizers(unittest.TestCase):\n+    def test_no_intersection(self):\n+        prompt = np.array([[1, 2, 3]])\n+        prompt_plus_new_tokens = np.array([[4, 5, 6]])\n+        result = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(prompt, prompt_plus_new_tokens)\n+        self.assertEqual(result, (None, None, None))\n+\n+    def test_complete_overlap(self):\n+        prompt = np.array([[1, 2, 3]])\n+        prompt_plus_new_tokens = np.array([[1, 2, 3, 4, 5]])\n+        discrep_length, new_tokens_only, discrep_only = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(\n+            prompt, prompt_plus_new_tokens\n+        )\n+        self.assertEqual(discrep_length, 0)\n+        np.testing.assert_array_equal(new_tokens_only, np.array([[4, 5]]))\n+        np.testing.assert_array_equal(discrep_only, np.array([[]]))\n+\n+    def test_partial_overlap(self):\n+        prompt = np.array([[1, 2, 3]])\n+        prompt_plus_new_tokens = np.array([[2, 3, 4, 5]])\n+        discrep_length, new_tokens_only, discrep_only = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(\n+            prompt, prompt_plus_new_tokens\n+        )\n+        self.assertEqual(discrep_length, 0)\n+        np.testing.assert_array_equal(new_tokens_only, np.array([[4, 5]]))\n+        np.testing.assert_array_equal(discrep_only, np.array([[]]))\n+\n+    def test_no_new_tokens(self):\n+        prompt = np.array([[1, 2, 3]])\n+        prompt_plus_new_tokens = np.array([[1, 2, 3]])\n+        discrep_length, new_tokens_only, discrep_only = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(\n+            prompt, prompt_plus_new_tokens\n+        )\n+        self.assertEqual(discrep_length, 0)\n+        np.testing.assert_array_equal(new_tokens_only, np.array([[]]))\n+        np.testing.assert_array_equal(discrep_only, np.array([[]]))"
        }
    ],
    "stats": {
        "total": 239,
        "additions": 139,
        "deletions": 100
    }
}