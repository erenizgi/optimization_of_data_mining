{
    "author": "Cyrilvallez",
    "message": "Fix jamba (#41309)\n\n* reactivate tests\n\n* first pass\n\n* fix\n\n* fix bias\n\n* fix and simplify\n\n* finally fix this stupid bug\n\n* add skips\n\n* remove bad stuff\n\n* fix copies\n\n* simplify",
    "sha": "c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7",
    "files": [
        {
            "sha": "97e911351c806f2b588f1de5c43f45b2a366515b",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7",
            "patch": "@@ -140,6 +140,12 @@ def __init__(self, config: BambaConfig, batch_size, dtype=torch.float16, device=\n         self.key_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n         self.value_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n \n+    def __len__(self):\n+        return len(self.key_cache)\n+\n+    def __getitem__(self, layer_idx):\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n     def update(\n         self,\n         key_states: torch.Tensor,\n@@ -182,7 +188,7 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n         # take any layer that contains cache and not empty tensor\n         layer_idx = self.transformer_layers[0] if layer_idx not in self.transformer_layers else layer_idx\n-        if len(self.key_cache) <= layer_idx:\n+        if len(self.key_cache) <= layer_idx or self.key_cache[layer_idx].shape[-1] == 0:\n             return 0\n         return self.key_cache[layer_idx].shape[-2]\n "
        },
        {
            "sha": "62797f2ecc631bd5983782d3f22713d0127b6b16",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7",
            "patch": "@@ -124,6 +124,12 @@ def __init__(\n         self.key_cache: list[torch.Tensor] = []\n         self.value_cache: list[torch.Tensor] = []\n \n+    def __len__(self):\n+        return len(self.key_cache)\n+\n+    def __getitem__(self, layer_idx):\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n     def update(\n         self,\n         key_states: torch.Tensor,\n@@ -189,7 +195,7 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n         # take any layer that contains cache and not empty tensor\n         layer_idx = self.transformer_layers[0] if layer_idx not in self.transformer_layers else layer_idx\n-        if len(self.key_cache) <= layer_idx:\n+        if len(self.key_cache) <= layer_idx or self.key_cache[layer_idx].shape[-1] == 0:\n             return 0\n         return self.key_cache[layer_idx].shape[-2]\n "
        },
        {
            "sha": "cc623b0f1d28093787b0893afb8e35d997a5ea98",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7",
            "patch": "@@ -216,6 +216,12 @@ def __init__(self, config: GraniteMoeHybridConfig, batch_size, dtype=torch.float\n         self.key_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n         self.value_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n \n+    def __len__(self):\n+        return len(self.key_cache)\n+\n+    def __getitem__(self, layer_idx):\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n     def update(\n         self,\n         key_states: torch.Tensor,\n@@ -258,7 +264,7 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n         # take any layer that contains cache and not empty tensor\n         layer_idx = self.transformer_layers[0] if layer_idx not in self.transformer_layers else layer_idx\n-        if len(self.key_cache) <= layer_idx:\n+        if len(self.key_cache) <= layer_idx or self.key_cache[layer_idx].shape[-1] == 0:\n             return 0\n         return self.key_cache[layer_idx].shape[-2]\n "
        },
        {
            "sha": "f0ab0589dcd75953c2847007d6d55adefd04cb48",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 83,
            "deletions": 129,
            "changes": 212,
            "blob_url": "https://github.com/huggingface/transformers/blob/c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7",
            "patch": "@@ -4,17 +4,31 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_jamba.py file directly. One of our CI enforces this.\n #                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n-# This is an automatically generated file by the modularization script. Please do not make any manual changes\n-# to this file as they will be overwritten.\n-\n-import math\n+# coding=utf-8\n+# Copyright 2024 AI21 Labs Ltd. and the HuggingFace Inc. team. All rights reserved.\n+#\n+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n+# and OPT implementations in this library. It has been modified from its\n+# original forms to accommodate minor architectural differences compared\n+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n from typing import Any, Callable, Optional, Union\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask\n@@ -23,6 +37,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available\n from .configuration_jamba import JambaConfig\n@@ -106,6 +121,12 @@ def __init__(self, config, batch_size, dtype=torch.float16, device=None):\n         self.key_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n         self.value_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n \n+    def __len__(self):\n+        return len(self.key_cache)\n+\n+    def __getitem__(self, layer_idx):\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n     def update(\n         self,\n         key_states: torch.Tensor,\n@@ -148,7 +169,7 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n         # take any layer that contains cache and not empty tensor\n         layer_idx = self.transformer_layers[0] if layer_idx not in self.transformer_layers else layer_idx\n-        if len(self.key_cache) <= layer_idx:\n+        if len(self.key_cache) <= layer_idx or self.key_cache[layer_idx].shape[-1] == 0:\n             return 0\n         return self.key_cache[layer_idx].shape[-2]\n \n@@ -192,48 +213,31 @@ def eager_attention_forward(\n \n \n class JambaAttention(nn.Module):\n-    \"\"\"\n-    Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n-    and \"Generating Long Sequences with Sparse Transformers\".\n-    \"\"\"\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: JambaConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: JambaConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.is_causal = True\n-        self.attention_dropout = config.attention_dropout\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.scaling = self.head_dim**-0.5\n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n-    def forward(  # FIME: this is also the classic attention NOPE\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[HybridMambaAttentionDynamicCache]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -242,7 +246,9 @@ def forward(  # FIME: this is also the classic attention NOPE\n         value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         if past_key_values is not None:\n-            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx)\n+            key_states, value_states = past_key_values.update(\n+                key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+            )\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -285,9 +291,7 @@ def __init__(self, config: JambaConfig, layer_idx):\n         self.ssm_state_size = config.mamba_d_state\n         self.conv_kernel_size = config.mamba_d_conv\n         self.intermediate_size = config.mamba_expand * config.hidden_size\n-        self.time_step_rank = (\n-            math.ceil(self.hidden_size / 16) if config.mamba_dt_rank == \"auto\" else config.mamba_dt_rank\n-        )\n+        self.time_step_rank = config.mamba_dt_rank\n         self.use_conv_bias = config.mamba_conv_bias\n         self.use_bias = config.mamba_proj_bias\n         self.conv1d = nn.Conv1d(\n@@ -325,7 +329,7 @@ def __init__(self, config: JambaConfig, layer_idx):\n         self.c_layernorm = JambaRMSNorm(self.ssm_state_size, eps=config.rms_norm_eps)\n \n         if not is_fast_path_available:\n-            logger.warning(\n+            logger.warning_once(\n                 \"The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\"\n                 \" is None. To install follow https://github.com/state-spaces/mamba/#installation and\"\n                 \" https://github.com/Dao-AILab/causal-conv1d. If you want to use the naive implementation, set `use_mamba_kernels=False` in the model config\"\n@@ -338,16 +342,14 @@ def cuda_kernels_forward(\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n         batch_size, seq_len, _ = hidden_states.shape\n-        if cache_params is None:\n-            use_precomputed_states = False\n-        else:\n-            use_precomputed_states = (\n-                cache_params.has_previous_state\n-                and seq_len == 1\n-                and cache_params.conv_states[self.layer_idx].shape[0]\n-                == cache_params.ssm_states[self.layer_idx].shape[0]\n-                == batch_size\n-            )\n+        use_precomputed_states = (\n+            cache_params is not None\n+            and cache_params.has_previous_state\n+            and seq_len == 1\n+            and cache_params.conv_states[self.layer_idx].shape[0]\n+            == cache_params.ssm_states[self.layer_idx].shape[0]\n+            == batch_size\n+        )\n         # 1. Gated MLP's linear projection\n         projected_states = self.in_proj(hidden_states).transpose(1, 2)\n \n@@ -361,8 +363,6 @@ def cuda_kernels_forward(\n         # 2. Convolution sequence transformation\n         conv_weights = self.conv1d.weight.view(self.conv1d.weight.size(0), self.conv1d.weight.size(2))\n         if use_precomputed_states:\n-            if causal_conv1d_update is None:\n-                raise ImportError(\"causal_conv1d_update is not available\")\n             hidden_states = causal_conv1d_update(\n                 hidden_states.squeeze(-1),\n                 cache_params.conv_states[self.layer_idx],\n@@ -375,8 +375,6 @@ def cuda_kernels_forward(\n             if cache_params is not None:\n                 conv_states = nn.functional.pad(hidden_states, (self.conv_kernel_size - hidden_states.shape[-1], 0))\n                 cache_params.conv_states[self.layer_idx].copy_(conv_states)\n-            if causal_conv1d_fn is None:\n-                raise ImportError(\"causal_conv1d_fn is not available\")\n             hidden_states = causal_conv1d_fn(hidden_states, conv_weights, self.conv1d.bias, activation=self.activation)\n \n         if attention_mask is not None:\n@@ -410,8 +408,6 @@ def cuda_kernels_forward(\n         # 3.c perform the recurrence y ‚Üê SSM(A, B, C)(x)\n         time_proj_bias = time_proj_bias.float() if time_proj_bias is not None else None\n         if use_precomputed_states:\n-            if selective_state_update is None:\n-                raise ImportError(\"selective_state_update is not available\")\n             scan_outputs = selective_state_update(\n                 cache_params.ssm_states[self.layer_idx],\n                 hidden_states[..., 0],\n@@ -425,8 +421,6 @@ def cuda_kernels_forward(\n                 dt_softplus=True,\n             ).unsqueeze(-1)\n         else:\n-            if selective_scan_fn is None:\n-                raise ImportError(\"selective_scan_fn is not available\")\n             scan_outputs, ssm_state = selective_scan_fn(\n                 hidden_states,\n                 discrete_time_step,\n@@ -452,15 +446,15 @@ def slow_forward(self, input_states, cache_params: Optional[HybridMambaAttention\n         batch_size, seq_len, _ = input_states.shape\n         dtype = input_states.dtype\n         # 1. Gated MLP's linear projection\n-        projected_states = self.in_proj(input_states).transpose(1, 2)                   # [batch, 2 * intermediate_size, seq_len]\n+        projected_states = self.in_proj(input_states).transpose(1, 2)\n         hidden_states, gate = projected_states.chunk(2, dim=1)\n \n         if attention_mask is not None:\n             hidden_states = hidden_states * attention_mask.unsqueeze(1)\n \n         use_cache = isinstance(cache_params, HybridMambaAttentionDynamicCache)\n         # 2. Convolution sequence transformation\n-        if use_cache and cache_params is not None and cache_params.ssm_states[self.layer_idx].shape[0] == batch_size:\n+        if use_cache and cache_params.ssm_states[self.layer_idx].shape[0] == batch_size:\n             if self.training:\n                 # In training mode, we don't want to perform in-place operations on ssm_state so we can compute the backwards pass\n                 ssm_state = cache_params.ssm_states[self.layer_idx].clone()\n@@ -471,27 +465,27 @@ def slow_forward(self, input_states, cache_params: Optional[HybridMambaAttention\n \n             if cache_params.has_previous_state and seq_len == 1 and \\\n                     cache_params.conv_states[self.layer_idx].shape[0] == batch_size:\n-                conv_state = cache_params.conv_states[self.layer_idx]                   # [batch, intermediate_size, conv_kernel_size]\n+                conv_state = cache_params.conv_states[self.layer_idx]\n                 conv_state = torch.roll(conv_state, shifts=-1, dims=-1)\n                 conv_state[:, :, -1] = hidden_states[:, :, 0]\n                 cache_params.conv_states[self.layer_idx] = conv_state\n                 hidden_states = torch.sum(conv_state * self.conv1d.weight[:, 0, :], dim=-1)\n                 if self.use_conv_bias:\n                     hidden_states += self.conv1d.bias\n-                hidden_states = self.act(hidden_states).to(dtype).unsqueeze(-1)         # [batch, intermediate_size, 1] : decoding\n+                hidden_states = self.act(hidden_states).to(dtype).unsqueeze(-1)\n             else:\n                 conv_state = nn.functional.pad(\n                     hidden_states,\n                     (self.conv_kernel_size - hidden_states.shape[-1], 0)\n                 )\n                 cache_params.conv_states[self.layer_idx] = conv_state\n-                hidden_states = self.act(self.conv1d(hidden_states)[..., :seq_len])     # [batch, intermediate_size, seq_len]\n+                hidden_states = self.act(self.conv1d(hidden_states)[..., :seq_len])\n         else:\n             ssm_state = torch.zeros(\n                 (batch_size, self.intermediate_size, self.ssm_state_size),\n                 device=hidden_states.device, dtype=dtype\n             )\n-            hidden_states = self.act(self.conv1d(hidden_states)[..., :seq_len])         # [batch, intermediate_size, seq_len]\n+            hidden_states = self.act(self.conv1d(hidden_states)[..., :seq_len])\n \n         if attention_mask is not None:\n             hidden_states = hidden_states * attention_mask.unsqueeze(1)\n@@ -507,29 +501,29 @@ def slow_forward(self, input_states, cache_params: Optional[HybridMambaAttention\n         B = self.b_layernorm(B)\n         C = self.c_layernorm(C)\n \n-        discrete_time_step = self.dt_proj(time_step)                                    # [batch, seq_len, intermediate_size]\n-        discrete_time_step = nn.functional.softplus(discrete_time_step).transpose(1, 2) # [batch, intermediate_size, seq_len]\n+        discrete_time_step = self.dt_proj(time_step)\n+        discrete_time_step = nn.functional.softplus(discrete_time_step).transpose(1, 2)\n \n         # 3.b. Discretization: B and C to [batch, seq_len, intermediate_size, ssm_state_size] (SRAM)\n-        A = -torch.exp(self.A_log.float())                                              # [intermediate_size, ssm_state_size]\n-        discrete_A = torch.exp(A[None, :, None, :] * discrete_time_step[:, :, :, None]) # [batch, intermediate_size, seq_len, ssm_state_size]\n-        discrete_B = discrete_time_step[:, :, :, None] * B[:, None, :, :].float()       # [batch, intermediate_size, seq_len, ssm_state_size]\n+        A = -torch.exp(self.A_log.float())\n+        discrete_A = torch.exp(A[None, :, None, :] * discrete_time_step[:, :, :, None])\n+        discrete_B = discrete_time_step[:, :, :, None] * B[:, None, :, :].float()\n         deltaB_u = discrete_B * hidden_states[:, :, :, None].float()\n         # 3.c perform the recurrence y ‚Üê SSM(A, B, C)(x)\n         scan_outputs = []\n         for i in range(seq_len):\n-            ssm_state = discrete_A[:, :, i, :] * ssm_state + deltaB_u[:, :, i, :]      # [batch, intermediate_size, ssm_state]\n-            scan_output = torch.matmul(ssm_state.to(dtype), C[:, i, :].unsqueeze(-1))  # [batch, intermediate_size, 1]\n+            ssm_state = discrete_A[:, :, i, :] * ssm_state + deltaB_u[:, :, i, :]\n+            scan_output = torch.matmul(ssm_state.to(dtype), C[:, i, :].unsqueeze(-1))\n             scan_outputs.append(scan_output[:, :, 0])\n-        scan_output = torch.stack(scan_outputs, dim=-1)                                # [batch, intermediate_size, seq_len]\n+        scan_output = torch.stack(scan_outputs, dim=-1)\n         scan_output = scan_output + (hidden_states * self.D[None, :, None])\n         scan_output = (scan_output * self.act(gate))\n \n         if use_cache:\n             cache_params.ssm_states[self.layer_idx] = ssm_state\n \n         # 4. Final linear projection\n-        contextualized_states = self.out_proj(scan_output.transpose(1, 2))  # [batch, seq_len, hidden_size]\n+        contextualized_states = self.out_proj(scan_output.transpose(1, 2))\n         return contextualized_states\n     # fmt: on\n \n@@ -554,9 +548,9 @@ def __init__(self, config):\n         self.config = config\n         self.hidden_size = config.hidden_size\n         self.intermediate_size = config.intermediate_size\n-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size)\n-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size)\n-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size)\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n         self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(self, x):\n@@ -628,7 +622,7 @@ def route_tokens_to_experts(self, hidden_states, router_logits):\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n-        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n         router_logits = self.router(hidden_states)\n         top_k_index, top_k_weights = self.route_tokens_to_experts(hidden_states, router_logits)\n         hidden_states = self.experts(hidden_states, top_k_index, top_k_weights)\n@@ -656,16 +650,17 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[torch.Tensor], Optional[HybridMambaAttentionDynamicCache]]:\n-        hidden_states = self.input_layernorm(hidden_states)\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n         hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n         residual = hidden_states\n@@ -692,7 +687,7 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[torch.Tensor], Optional[HybridMambaAttentionDynamicCache]]:\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         hidden_states = self.mamba(\n@@ -716,7 +711,6 @@ class JambaPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    # Note: only supports HybridMambaAttentionDynamicCache\n     _is_stateful = True\n     _can_record_outputs = {\n         \"hidden_states\": [JambaAttentionDecoderLayer, JambaMambaDecoderLayer],\n@@ -725,18 +719,8 @@ class JambaPreTrainedModel(PreTrainedModel):\n     }\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, JambaRMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, JambaMambaMixer):\n+        super()._init_weights(module)\n+        if isinstance(module, JambaMambaMixer):\n             A = torch.arange(1, module.ssm_state_size + 1)[None, :]\n             A = A.expand(module.intermediate_size, -1).contiguous()\n             module.A_log.data.copy_(torch.log(A))\n@@ -760,7 +744,6 @@ def __init__(self, config: JambaConfig):\n             decoder_layers.append(layer_class(config, layer_idx=i))\n         self.layers = nn.ModuleList(decoder_layers)\n \n-        self._attn_implementation = config._attn_implementation\n         self.final_layernorm = JambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n         self.gradient_checkpointing = False\n@@ -796,7 +779,7 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.LongTensor = torch.arange(\n+            cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n \n@@ -823,6 +806,7 @@ def forward(\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n+                **kwargs,\n             )\n \n         hidden_states = self.final_layernorm(hidden_states)\n@@ -956,7 +940,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -976,8 +960,8 @@ def forward(\n         ```python\n         >>> from transformers import AutoTokenizer, JambaForCausalLM\n \n-        >>> model = JambaForCausalLM.from_pretrained(\"mistralai/Jamba-8x7B-v0.1\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Jamba-8x7B-v0.1\")\n+        >>> model = JambaForCausalLM.from_pretrained(\"ai21labs/Jamba-v0.1\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"ai21labs/Jamba-v0.1\")\n \n         >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n@@ -1035,36 +1019,6 @@ def forward(\n             router_logits=outputs.router_logits,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        inputs_embeds=None,\n-        cache_position=None,\n-        **kwargs,\n-    ):\n-        # create cache if necessary\n-        if past_key_values is None:\n-            past_key_values = HybridMambaAttentionDynamicCache(\n-                self.config, input_ids.shape[0], self.dtype, device=self.device\n-            )\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and past_key_values.get_seq_length() == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids}\n-\n-        model_inputs.update(\n-            {\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": kwargs.get(\"use_cache\"),\n-                \"attention_mask\": attention_mask,\n-                \"cache_position\": cache_position,\n-            }\n-        )\n-        return model_inputs\n-\n \n class JambaForSequenceClassification(GenericForSequenceClassification, JambaPreTrainedModel):\n     pass"
        },
        {
            "sha": "7b48926da008973602f122da44201a5e1ee672ab",
            "filename": "src/transformers/models/jamba/modular_jamba.py",
            "status": "modified",
            "additions": 126,
            "deletions": 148,
            "changes": 274,
            "blob_url": "https://github.com/huggingface/transformers/blob/c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py?ref=c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7",
            "patch": "@@ -1,29 +1,38 @@\n-# This is an automatically generated file by the modularization script. Please do not make any manual changes\n-# to this file as they will be overwritten.\n-\n-import math\n-from typing import Any, Callable, Optional\n+# coding=utf-8\n+# Copyright 2024 AI21 Labs Ltd. and the HuggingFace Inc. team. All rights reserved.\n+#\n+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n+# and OPT implementations in this library. It has been modified from its\n+# original forms to accommodate minor architectural differences compared\n+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Any, Callable, Optional, Union\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n from ...masking_utils import create_causal_mask\n-from ...modeling_layers import (\n-    GenericForSequenceClassification,\n-    GradientCheckpointingLayer,\n-)\n-from ...modeling_outputs import MoeModelOutputWithPast\n+from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    TransformersKwargs,\n-    auto_docstring,\n-    logging,\n-)\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.generic import OutputRecorder, check_model_inputs\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available\n-from ..llama.modeling_llama import LlamaMLP, LlamaRMSNorm, eager_attention_forward\n+from ..llama.modeling_llama import LlamaAttention, LlamaRMSNorm, eager_attention_forward\n+from ..mistral.modeling_mistral import MistralMLP\n from ..mixtral.modeling_mixtral import MixtralExperts, MixtralForCausalLM\n from .configuration_jamba import JambaConfig\n \n@@ -93,6 +102,12 @@ def __init__(self, config, batch_size, dtype=torch.float16, device=None):\n         self.key_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n         self.value_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n \n+    def __len__(self):\n+        return len(self.key_cache)\n+\n+    def __getitem__(self, layer_idx):\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n     def update(\n         self,\n         key_states: torch.Tensor,\n@@ -135,54 +150,27 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n         # take any layer that contains cache and not empty tensor\n         layer_idx = self.transformer_layers[0] if layer_idx not in self.transformer_layers else layer_idx\n-        if len(self.key_cache) <= layer_idx:\n+        if len(self.key_cache) <= layer_idx or self.key_cache[layer_idx].shape[-1] == 0:\n             return 0\n         return self.key_cache[layer_idx].shape[-2]\n \n \n-class JambaAttention(nn.Module):\n-    \"\"\"\n-    Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n-    and \"Generating Long Sequences with Sparse Transformers\".\n-    \"\"\"\n-\n-    def __init__(self, config: JambaConfig, layer_idx: Optional[int] = None):\n-        super().__init__()\n-        self.config = config\n-        self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.is_causal = True\n-        self.attention_dropout = config.attention_dropout\n-        self.scaling = self.head_dim**-0.5\n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n+class JambaAttention(LlamaAttention):\n+    def __init__(self, config: JambaConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n-    def forward(  # FIME: this is also the classic attention NOPE\n+    def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[HybridMambaAttentionDynamicCache]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -191,7 +179,9 @@ def forward(  # FIME: this is also the classic attention NOPE\n         value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         if past_key_values is not None:\n-            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx)\n+            key_states, value_states = past_key_values.update(\n+                key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+            )\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -229,9 +219,7 @@ def __init__(self, config: JambaConfig, layer_idx):\n         self.ssm_state_size = config.mamba_d_state\n         self.conv_kernel_size = config.mamba_d_conv\n         self.intermediate_size = config.mamba_expand * config.hidden_size\n-        self.time_step_rank = (\n-            math.ceil(self.hidden_size / 16) if config.mamba_dt_rank == \"auto\" else config.mamba_dt_rank\n-        )\n+        self.time_step_rank = config.mamba_dt_rank\n         self.use_conv_bias = config.mamba_conv_bias\n         self.use_bias = config.mamba_proj_bias\n         self.conv1d = nn.Conv1d(\n@@ -269,7 +257,7 @@ def __init__(self, config: JambaConfig, layer_idx):\n         self.c_layernorm = JambaRMSNorm(self.ssm_state_size, eps=config.rms_norm_eps)\n \n         if not is_fast_path_available:\n-            logger.warning(\n+            logger.warning_once(\n                 \"The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\"\n                 \" is None. To install follow https://github.com/state-spaces/mamba/#installation and\"\n                 \" https://github.com/Dao-AILab/causal-conv1d. If you want to use the naive implementation, set `use_mamba_kernels=False` in the model config\"\n@@ -282,16 +270,14 @@ def cuda_kernels_forward(\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n         batch_size, seq_len, _ = hidden_states.shape\n-        if cache_params is None:\n-            use_precomputed_states = False\n-        else:\n-            use_precomputed_states = (\n-                cache_params.has_previous_state\n-                and seq_len == 1\n-                and cache_params.conv_states[self.layer_idx].shape[0]\n-                == cache_params.ssm_states[self.layer_idx].shape[0]\n-                == batch_size\n-            )\n+        use_precomputed_states = (\n+            cache_params is not None\n+            and cache_params.has_previous_state\n+            and seq_len == 1\n+            and cache_params.conv_states[self.layer_idx].shape[0]\n+            == cache_params.ssm_states[self.layer_idx].shape[0]\n+            == batch_size\n+        )\n         # 1. Gated MLP's linear projection\n         projected_states = self.in_proj(hidden_states).transpose(1, 2)\n \n@@ -305,8 +291,6 @@ def cuda_kernels_forward(\n         # 2. Convolution sequence transformation\n         conv_weights = self.conv1d.weight.view(self.conv1d.weight.size(0), self.conv1d.weight.size(2))\n         if use_precomputed_states:\n-            if causal_conv1d_update is None:\n-                raise ImportError(\"causal_conv1d_update is not available\")\n             hidden_states = causal_conv1d_update(\n                 hidden_states.squeeze(-1),\n                 cache_params.conv_states[self.layer_idx],\n@@ -319,8 +303,6 @@ def cuda_kernels_forward(\n             if cache_params is not None:\n                 conv_states = nn.functional.pad(hidden_states, (self.conv_kernel_size - hidden_states.shape[-1], 0))\n                 cache_params.conv_states[self.layer_idx].copy_(conv_states)\n-            if causal_conv1d_fn is None:\n-                raise ImportError(\"causal_conv1d_fn is not available\")\n             hidden_states = causal_conv1d_fn(hidden_states, conv_weights, self.conv1d.bias, activation=self.activation)\n \n         if attention_mask is not None:\n@@ -354,8 +336,6 @@ def cuda_kernels_forward(\n         # 3.c perform the recurrence y ‚Üê SSM(A, B, C)(x)\n         time_proj_bias = time_proj_bias.float() if time_proj_bias is not None else None\n         if use_precomputed_states:\n-            if selective_state_update is None:\n-                raise ImportError(\"selective_state_update is not available\")\n             scan_outputs = selective_state_update(\n                 cache_params.ssm_states[self.layer_idx],\n                 hidden_states[..., 0],\n@@ -369,8 +349,6 @@ def cuda_kernels_forward(\n                 dt_softplus=True,\n             ).unsqueeze(-1)\n         else:\n-            if selective_scan_fn is None:\n-                raise ImportError(\"selective_scan_fn is not available\")\n             scan_outputs, ssm_state = selective_scan_fn(\n                 hidden_states,\n                 discrete_time_step,\n@@ -396,15 +374,15 @@ def slow_forward(self, input_states, cache_params: Optional[HybridMambaAttention\n         batch_size, seq_len, _ = input_states.shape\n         dtype = input_states.dtype\n         # 1. Gated MLP's linear projection\n-        projected_states = self.in_proj(input_states).transpose(1, 2)                   # [batch, 2 * intermediate_size, seq_len]\n+        projected_states = self.in_proj(input_states).transpose(1, 2)\n         hidden_states, gate = projected_states.chunk(2, dim=1)\n \n         if attention_mask is not None:\n             hidden_states = hidden_states * attention_mask.unsqueeze(1)\n \n         use_cache = isinstance(cache_params, HybridMambaAttentionDynamicCache)\n         # 2. Convolution sequence transformation\n-        if use_cache and cache_params is not None and cache_params.ssm_states[self.layer_idx].shape[0] == batch_size:\n+        if use_cache and cache_params.ssm_states[self.layer_idx].shape[0] == batch_size:\n             if self.training:\n                 # In training mode, we don't want to perform in-place operations on ssm_state so we can compute the backwards pass\n                 ssm_state = cache_params.ssm_states[self.layer_idx].clone()\n@@ -415,27 +393,27 @@ def slow_forward(self, input_states, cache_params: Optional[HybridMambaAttention\n \n             if cache_params.has_previous_state and seq_len == 1 and \\\n                     cache_params.conv_states[self.layer_idx].shape[0] == batch_size:\n-                conv_state = cache_params.conv_states[self.layer_idx]                   # [batch, intermediate_size, conv_kernel_size]\n+                conv_state = cache_params.conv_states[self.layer_idx]\n                 conv_state = torch.roll(conv_state, shifts=-1, dims=-1)\n                 conv_state[:, :, -1] = hidden_states[:, :, 0]\n                 cache_params.conv_states[self.layer_idx] = conv_state\n                 hidden_states = torch.sum(conv_state * self.conv1d.weight[:, 0, :], dim=-1)\n                 if self.use_conv_bias:\n                     hidden_states += self.conv1d.bias\n-                hidden_states = self.act(hidden_states).to(dtype).unsqueeze(-1)         # [batch, intermediate_size, 1] : decoding\n+                hidden_states = self.act(hidden_states).to(dtype).unsqueeze(-1)\n             else:\n                 conv_state = nn.functional.pad(\n                     hidden_states,\n                     (self.conv_kernel_size - hidden_states.shape[-1], 0)\n                 )\n                 cache_params.conv_states[self.layer_idx] = conv_state\n-                hidden_states = self.act(self.conv1d(hidden_states)[..., :seq_len])     # [batch, intermediate_size, seq_len]\n+                hidden_states = self.act(self.conv1d(hidden_states)[..., :seq_len])\n         else:\n             ssm_state = torch.zeros(\n                 (batch_size, self.intermediate_size, self.ssm_state_size),\n                 device=hidden_states.device, dtype=dtype\n             )\n-            hidden_states = self.act(self.conv1d(hidden_states)[..., :seq_len])         # [batch, intermediate_size, seq_len]\n+            hidden_states = self.act(self.conv1d(hidden_states)[..., :seq_len])\n \n         if attention_mask is not None:\n             hidden_states = hidden_states * attention_mask.unsqueeze(1)\n@@ -451,29 +429,29 @@ def slow_forward(self, input_states, cache_params: Optional[HybridMambaAttention\n         B = self.b_layernorm(B)\n         C = self.c_layernorm(C)\n \n-        discrete_time_step = self.dt_proj(time_step)                                    # [batch, seq_len, intermediate_size]\n-        discrete_time_step = nn.functional.softplus(discrete_time_step).transpose(1, 2) # [batch, intermediate_size, seq_len]\n+        discrete_time_step = self.dt_proj(time_step)\n+        discrete_time_step = nn.functional.softplus(discrete_time_step).transpose(1, 2)\n \n         # 3.b. Discretization: B and C to [batch, seq_len, intermediate_size, ssm_state_size] (SRAM)\n-        A = -torch.exp(self.A_log.float())                                              # [intermediate_size, ssm_state_size]\n-        discrete_A = torch.exp(A[None, :, None, :] * discrete_time_step[:, :, :, None]) # [batch, intermediate_size, seq_len, ssm_state_size]\n-        discrete_B = discrete_time_step[:, :, :, None] * B[:, None, :, :].float()       # [batch, intermediate_size, seq_len, ssm_state_size]\n+        A = -torch.exp(self.A_log.float())\n+        discrete_A = torch.exp(A[None, :, None, :] * discrete_time_step[:, :, :, None])\n+        discrete_B = discrete_time_step[:, :, :, None] * B[:, None, :, :].float()\n         deltaB_u = discrete_B * hidden_states[:, :, :, None].float()\n         # 3.c perform the recurrence y ‚Üê SSM(A, B, C)(x)\n         scan_outputs = []\n         for i in range(seq_len):\n-            ssm_state = discrete_A[:, :, i, :] * ssm_state + deltaB_u[:, :, i, :]      # [batch, intermediate_size, ssm_state]\n-            scan_output = torch.matmul(ssm_state.to(dtype), C[:, i, :].unsqueeze(-1))  # [batch, intermediate_size, 1]\n+            ssm_state = discrete_A[:, :, i, :] * ssm_state + deltaB_u[:, :, i, :]\n+            scan_output = torch.matmul(ssm_state.to(dtype), C[:, i, :].unsqueeze(-1))\n             scan_outputs.append(scan_output[:, :, 0])\n-        scan_output = torch.stack(scan_outputs, dim=-1)                                # [batch, intermediate_size, seq_len]\n+        scan_output = torch.stack(scan_outputs, dim=-1)\n         scan_output = scan_output + (hidden_states * self.D[None, :, None])\n         scan_output = (scan_output * self.act(gate))\n \n         if use_cache:\n             cache_params.ssm_states[self.layer_idx] = ssm_state\n \n         # 4. Final linear projection\n-        contextualized_states = self.out_proj(scan_output.transpose(1, 2))  # [batch, seq_len, hidden_size]\n+        contextualized_states = self.out_proj(scan_output.transpose(1, 2))\n         return contextualized_states\n     # fmt: on\n \n@@ -492,12 +470,8 @@ def forward(\n         return self.slow_forward(hidden_states, cache_params, attention_mask)\n \n \n-class JambaMLP(LlamaMLP):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size)\n-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size)\n-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size)\n+class JambaMLP(MistralMLP):\n+    pass\n \n \n class JambaExperts(MixtralExperts):\n@@ -533,7 +507,7 @@ def route_tokens_to_experts(self, hidden_states, router_logits):\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n-        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n         router_logits = self.router(hidden_states)\n         top_k_index, top_k_weights = self.route_tokens_to_experts(hidden_states, router_logits)\n         hidden_states = self.experts(hidden_states, top_k_index, top_k_weights)\n@@ -561,16 +535,17 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[torch.Tensor], Optional[HybridMambaAttentionDynamicCache]]:\n-        hidden_states = self.input_layernorm(hidden_states)\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n         hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n         residual = hidden_states\n@@ -597,7 +572,7 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[torch.Tensor], Optional[HybridMambaAttentionDynamicCache]]:\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         hidden_states = self.mamba(\n@@ -624,7 +599,6 @@ class JambaPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    # Note: only supports HybridMambaAttentionDynamicCache\n     _is_stateful = True\n     _can_record_outputs = {\n         \"hidden_states\": [JambaAttentionDecoderLayer, JambaMambaDecoderLayer],\n@@ -633,18 +607,8 @@ class JambaPreTrainedModel(PreTrainedModel):\n     }\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, JambaRMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, JambaMambaMixer):\n+        super()._init_weights(module)\n+        if isinstance(module, JambaMambaMixer):\n             A = torch.arange(1, module.ssm_state_size + 1)[None, :]\n             A = A.expand(module.intermediate_size, -1).contiguous()\n             module.A_log.data.copy_(torch.log(A))\n@@ -665,7 +629,6 @@ def __init__(self, config: JambaConfig):\n             decoder_layers.append(layer_class(config, layer_idx=i))\n         self.layers = nn.ModuleList(decoder_layers)\n \n-        self._attn_implementation = config._attn_implementation\n         self.final_layernorm = JambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n         self.gradient_checkpointing = False\n@@ -701,7 +664,7 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.LongTensor = torch.arange(\n+            cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n \n@@ -728,6 +691,7 @@ def forward(\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n+                **kwargs,\n             )\n \n         hidden_states = self.final_layernorm(hidden_states)\n@@ -755,44 +719,58 @@ def _update_mamba_mask(self, attention_mask, cache_position):\n \n \n class JambaForCausalLM(MixtralForCausalLM):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n-\n     def __init__(self, config: JambaConfig):\n         super().__init__(config)\n-        self.model = JambaModel(config)\n-        self.router_aux_loss_coef = config.router_aux_loss_coef\n         self.num_experts = config.num_experts\n-        self.num_experts_per_tok = config.num_experts_per_tok\n \n-    def prepare_inputs_for_generation(\n+    def forward(\n         self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        inputs_embeds=None,\n-        cache_position=None,\n-        **kwargs,\n-    ):\n-        # create cache if necessary\n-        if past_key_values is None:\n-            past_key_values = HybridMambaAttentionDynamicCache(\n-                self.config, input_ids.shape[0], self.dtype, device=self.device\n-            )\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and past_key_values.get_seq_length() == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids}\n-\n-        model_inputs.update(\n-            {\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": kwargs.get(\"use_cache\"),\n-                \"attention_mask\": attention_mask,\n-                \"cache_position\": cache_position,\n-            }\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_router_logits: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeCausalLMOutputWithPast:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, JambaForCausalLM\n+\n+        >>> model = JambaForCausalLM.from_pretrained(\"ai21labs/Jamba-v0.1\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"ai21labs/Jamba-v0.1\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        return super().forward(\n+            input_ids,\n+            attention_mask,\n+            position_ids,\n+            past_key_values,\n+            inputs_embeds,\n+            labels,\n+            use_cache,\n+            cache_position,\n+            logits_to_keep,\n+            **kwargs,\n         )\n-        return model_inputs\n \n \n class JambaForSequenceClassification(GenericForSequenceClassification, JambaPreTrainedModel):"
        },
        {
            "sha": "9057b323d1084d755284ea71a9d2138e7649f723",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7",
            "patch": "@@ -185,7 +185,7 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n         # take any layer that contains cache and not empty tensor\n         layer_idx = self.transformer_layers[0] if layer_idx not in self.transformer_layers else layer_idx\n-        if len(self.key_cache) <= layer_idx:\n+        if len(self.key_cache) <= layer_idx or self.key_cache[layer_idx].shape[-1] == 0:\n             return 0\n         return self.key_cache[layer_idx].shape[-2]\n "
        },
        {
            "sha": "e0001b8afdc7684b61afec83be38acda3841e2f2",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 7,
            "deletions": 51,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=c2b3cc3e64b9fdb6259d3b170f0ac8101fa751e7",
            "patch": "@@ -33,6 +33,7 @@\n     torch_device,\n )\n \n+from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -319,7 +320,7 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class JambaModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class JambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (\n         (\n             JambaModel,\n@@ -532,12 +533,12 @@ def test_flash_attn_2_fp32_ln(self):\n                 # with attention mask\n                 _ = model(dummy_input, attention_mask=dummy_attention_mask)\n \n-    @unittest.skip(\"TODO, jamba is annoying, needs another refactor, too tired for that now\")\n-    def test_generation_tester_mixin_inheritance(self):\n+    @unittest.skip(\"Jamba has a non standard cache format (mamba cache)\")\n+    def test_past_key_values_format(self):\n         pass\n \n-    @unittest.skip(\"TODO, jamba is annoying, needs another refactor, too tired for that now\")\n-    def test_batching_equivalence(self):\n+    @unittest.skip(\"Jamba has a non standard cache which is not compatible with dp/ddp\")\n+    def test_multi_gpu_data_parallel_forward(self):\n         pass\n \n \n@@ -555,6 +556,7 @@ def setUpClass(cls):\n         cls.model = JambaForCausalLM.from_pretrained(\n             model_id,\n             dtype=torch.bfloat16,\n+            use_mamba_kernels=False,\n         )\n         cls.tokenizer = AutoTokenizer.from_pretrained(model_id)\n         cls.device_properties = get_device_properties()\n@@ -584,23 +586,6 @@ def test_simple_generate(self):\n         output_sentence = self.tokenizer.decode(out[0, :])\n         self.assertEqual(output_sentence, expected_sentence)\n \n-        # TODO: there are significant differences in the logits across major cuda versions, which shouldn't exist\n-        if self.device_properties[0] == \"cuda\" and self.device_properties[1] == 8:\n-            with torch.no_grad():\n-                logits = self.model(input_ids=input_ids).logits\n-\n-            EXPECTED_LOGITS_NO_GRAD = torch.tensor(\n-                [\n-                    -7.6875, -7.6562,  8.9375, -7.7812, -7.4062, -7.9688, -8.3125, -7.4062,\n-                    -7.8125, -8.1250, -7.8125, -7.3750, -7.8438, -7.5000, -8.0625, -8.0625,\n-                    -7.5938, -7.9688, -8.2500, -7.5625, -7.7500, -7.7500, -7.6562, -7.6250,\n-                    -8.1250, -8.0625, -8.1250, -7.8750, -8.1875, -8.2500, -7.5938, -8.0000,\n-                    -7.5000, -7.7500, -7.9375, -7.4688, -8.0625, -7.3438, -8.0000, -7.5000\n-                ]\n-                , dtype=torch.float32)  # fmt: skip\n-\n-            torch.testing.assert_close(logits[0, -1, :40].cpu(), EXPECTED_LOGITS_NO_GRAD, rtol=1e-3, atol=1e-3)\n-\n     @slow\n     def test_simple_batched_generate_with_padding(self):\n         # (\"cuda\", 8) for A100/A10, and (\"cuda\", 7) for T4.\n@@ -626,32 +611,3 @@ def test_simple_batched_generate_with_padding(self):\n         output_sentences = self.tokenizer.batch_decode(out)\n         self.assertEqual(output_sentences[0], expected_sentences[0])\n         self.assertEqual(output_sentences[1], expected_sentences[1])\n-\n-        # TODO: there are significant differences in the logits across major cuda versions, which shouldn't exist\n-        if self.device_properties[0] == \"cuda\" and self.device_properties[1] == 8:\n-            with torch.no_grad():\n-                logits = self.model(input_ids=inputs[\"input_ids\"]).logits\n-\n-            # TODO fix logits\n-            EXPECTED_LOGITS_NO_GRAD_0 = torch.tensor(\n-                [\n-                    -7.7188, -7.6875,  8.8750, -7.8125, -7.4062, -8.0000, -8.3125, -7.4375,\n-                    -7.8125, -8.1250, -7.8125, -7.4062, -7.8438, -7.5312, -8.0625, -8.0625,\n-                    -7.6250, -8.0000, -8.3125, -7.5938, -7.7500, -7.7500, -7.6562, -7.6562,\n-                    -8.1250, -8.0625, -8.1250, -7.8750, -8.1875, -8.2500, -7.5938, -8.0625,\n-                     -7.5000, -7.7812, -7.9375, -7.4688, -8.0625, -7.3750, -8.0000, -7.50003\n-                ]\n-                , dtype=torch.float32)  # fmt: skip\n-\n-            EXPECTED_LOGITS_NO_GRAD_1 = torch.tensor(\n-                [\n-                    -3.5469, -4.0625,  8.5000, -3.8125, -3.6406, -3.7969, -3.8125, -3.3594,\n-                     -3.7188, -3.7500, -3.7656, -3.5469, -3.7969, -4.0000, -3.5625, -3.6406,\n-                    -3.7188, -3.6094, -4.0938, -3.6719, -3.8906, -3.9844, -3.8594, -3.4219,\n-                    -3.2031, -3.4375, -3.7500, -3.6562, -3.9688, -4.1250, -3.6406, -3.57811,\n-                    -3.0312, -3.4844, -3.6094, -3.5938, -3.7656, -3.8125, -3.7500, -3.8594\n-                ]\n-                , dtype=torch.float32)  # fmt: skip\n-\n-            torch.testing.assert_close(logits[0, -1, :40].cpu(), EXPECTED_LOGITS_NO_GRAD_0, rtol=1e-3, atol=1e-3)\n-            torch.testing.assert_close(logits[1, -1, :40].cpu(), EXPECTED_LOGITS_NO_GRAD_1, rtol=1e-3, atol=1e-3)"
        }
    ],
    "stats": {
        "total": 570,
        "additions": 238,
        "deletions": 332
    }
}