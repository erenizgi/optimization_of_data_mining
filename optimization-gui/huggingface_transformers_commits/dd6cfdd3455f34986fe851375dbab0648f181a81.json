{
    "author": "Furkan-rgb",
    "message": "Add SDPA support for PatchTST model (#42465)\n\n* Add SDPA and Flash Attention support for PatchTST model\n\n- Add _supports_sdpa = True and _supports_flash_attn = True to PatchTSTPreTrainedModel\n- The existing PatchTSTAttention class already uses ALL_ATTENTION_FUNCTIONS\n  to select the attention implementation based on config._attn_implementation\n- Fix test_modeling_patchtst.py _prepare_for_class for dynamic batch sizes\n\n* Guard PatchTST positional init under ZeRO-3\n\n* Force SDPA in PatchTST regression integration test\n\n* Use sdpa attn in PatchTST regression test\n\n* fixups re tests\n\n---------\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\nCo-authored-by: vasqu <antonprogamer@gmail.com>\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "dd6cfdd3455f34986fe851375dbab0648f181a81",
    "files": [
        {
            "sha": "c0bbdc748eeaf8d763dcbec43ab390c3ba2d2e69",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 14,
            "deletions": 2,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd6cfdd3455f34986fe851375dbab0648f181a81/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd6cfdd3455f34986fe851375dbab0648f181a81/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=dd6cfdd3455f34986fe851375dbab0648f181a81",
            "patch": "@@ -24,6 +24,7 @@\n \n from ... import initialization as init\n from ...activations import ACT2CLS\n+from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -418,7 +419,7 @@ def __init__(self, config: PatchTSTConfig):\n         super().__init__()\n \n         self.channel_attention = config.channel_attention\n-        # Multi-Head attention\n+\n         self.self_attn = PatchTSTAttention(\n             embed_dim=config.d_model,\n             num_heads=config.num_attention_heads,\n@@ -555,6 +556,9 @@ class PatchTSTPreTrainedModel(PreTrainedModel):\n     main_input_name = \"past_values\"\n     input_modalities = (\"time\",)\n     supports_gradient_checkpointing = False\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n \n     @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n@@ -571,7 +575,15 @@ def _init_weights(self, module: nn.Module):\n                 init.normal_(module.cls_token, std=0.02)\n                 num_patches += 1\n             # initialize positional encoding\n-            init.copy_(module.position_enc, module._init_pe(self.config, num_patches))\n+            position_enc = module._init_pe(self.config, num_patches)\n+            if is_deepspeed_zero3_enabled():\n+                import deepspeed\n+\n+                with deepspeed.zero.GatheredParameters(module.position_enc, modifier_rank=None):\n+                    if module.position_enc.numel() > 0:\n+                        init.copy_(module.position_enc, position_enc)\n+            else:\n+                init.copy_(module.position_enc, position_enc)\n         elif isinstance(module, nn.LayerNorm):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)"
        },
        {
            "sha": "72ac0f8087d2cf98af8dbdf1964fb9daa152a609",
            "filename": "tests/models/patchtst/test_modeling_patchtst.py",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd6cfdd3455f34986fe851375dbab0648f181a81/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd6cfdd3455f34986fe851375dbab0648f181a81/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py?ref=dd6cfdd3455f34986fe851375dbab0648f181a81",
            "patch": "@@ -22,7 +22,7 @@\n \n from transformers import is_torch_available\n from transformers.models.auto import get_values\n-from transformers.testing_utils import is_flaky, require_torch, slow, torch_device\n+from transformers.testing_utils import is_flaky, require_read_token, require_torch, slow, torch_device\n from transformers.utils import check_torch_load_is_safe\n \n from ...test_configuration_common import ConfigTester\n@@ -184,20 +184,23 @@ def test_config(self):\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n \n+        # Get the actual batch size from the inputs (may differ from model_tester.batch_size in some tests)\n+        batch_size = inputs_dict[\"past_values\"].shape[0]\n+\n         #  if PatchTSTForPretraining\n         if model_class == PatchTSTForPretraining:\n-            inputs_dict.pop(\"future_values\")\n+            inputs_dict.pop(\"future_values\", None)\n         # else if classification model:\n         elif model_class in get_values(MODEL_FOR_TIME_SERIES_CLASSIFICATION_MAPPING):\n             rng = random.Random(self.model_tester.seed)\n-            labels = ids_tensor([self.model_tester.batch_size], self.model_tester.num_targets, rng=rng)\n+            labels = ids_tensor([batch_size], self.model_tester.num_targets, rng=rng)\n             inputs_dict[\"target_values\"] = labels\n-            inputs_dict.pop(\"future_values\")\n+            inputs_dict.pop(\"future_values\", None)\n         elif model_class in get_values(MODEL_FOR_TIME_SERIES_REGRESSION_MAPPING):\n             rng = random.Random(self.model_tester.seed)\n-            target_values = floats_tensor([self.model_tester.batch_size, self.model_tester.num_targets], rng=rng)\n+            target_values = floats_tensor([batch_size, self.model_tester.num_targets], rng=rng)\n             inputs_dict[\"target_values\"] = target_values\n-            inputs_dict.pop(\"future_values\")\n+            inputs_dict.pop(\"future_values\", None)\n         return inputs_dict\n \n     def test_save_load_strict(self):\n@@ -329,7 +332,7 @@ def test_pretrain_head(self):\n         )\n         torch.testing.assert_close(output[0, :7, :1, :1], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n-    # Publishing of pretrained weights are under internal review. Pretrained model is not yet downloadable.\n+    @require_read_token\n     def test_prediction_head(self):\n         model = PatchTSTForPrediction.from_pretrained(\"namctin/patchtst_etth1_forecast\").to(torch_device)\n         batch = prepare_batch(file=\"test-batch.pt\")\n@@ -349,6 +352,7 @@ def test_prediction_head(self):\n         )\n         torch.testing.assert_close(output[0, :1, :7], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n+    @require_read_token\n     def test_prediction_generation(self):\n         model = PatchTSTForPrediction.from_pretrained(\"namctin/patchtst_etth1_forecast\").to(torch_device)\n         batch = prepare_batch(file=\"test-batch.pt\")"
        }
    ],
    "stats": {
        "total": 34,
        "additions": 25,
        "deletions": 9
    }
}