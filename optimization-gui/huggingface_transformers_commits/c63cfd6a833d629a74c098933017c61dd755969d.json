{
    "author": "RyanMullins",
    "message": "Gemma 3n (#39059)\n\n* Gemma 3n\n\n* initial commit of Gemma 3n scaffold\n\n* Fixing param pass through on Gemm3p5RMSNorm\n\n* Adds Einsum layer to Gemma 3n\n\n* Updating EinsumLayer API\n\n* Undoing erroneous force push\n\n* Reverting RMSNorm to with_scale by default\n\n* Adds LAuReL to Gemma 3n\n\n* Adds AltUp to Gemma 3n\n\n* Adding Gemma3p5 overall and text config with vision and audio config placeholders (#3)\n\n* Adding gemma3p5 text configs\n\n* Adding audio config placeholders\n\n* Adding a placeholder for vision configs\n\n* Updating MobileNetVisionConfig, inheriting TimmWrapperConfig\n\n* Updating text configs\n\n* Update src/transformers/models/gemma3p5/modular_gemma3p5.py\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Removing altup configs to accept the suggested configs\n\n* Update src/transformers/models/gemma3p5/modular_gemma3p5.py\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Updating altup config\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Addressing review comments and updating text configs\n\n* Adding a config for activation sparsity\n\n* Updating configs to pass through options to super class init and adjust some name prefixes\n\n* Updating laurel and altup with corrected config values\n\n* Normalizing sub_config initializers\n\n---------\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Updating MLP with activation sparsity (#2)\n\n* Updating DecoderBlock for Gemma 3n (#3)\n\n* Initial Gemm3nTextModel (#4)\n\nNOTE: This implementation WILL CHANGE in the coming weeks, however, changes will be strictly additive and this will remain a suitable baseline for downstream implementations to reference.\n\n* Adding KV Cache Sharing\n\n* Adds Einsum layer to Gemma 3n\n\n* Updating EinsumLayer API\n\n* Refactored kv cache sharing in attention\n\n* Adding KVStore for cache sharing\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update src/transformers/cache_utils.py\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Undoing erroneous force push\n\n* Reverting RMSNorm to with_scale by default\n\n* Adds LAuReL to Gemma 3n\n\n* Updating KV Cache Sharing implementation\n\n* Updating the q and k norm definitions in the attention module\n\n* Fixing name error for q,k,v RMS norm to use the right 3n module\n\n* Updating MLP with activation sparsity\n\n* Updating DecoderBlock for Gemma 3.5\n\n* Updating kv cache sharing implementation with the use of a cache buffer and refactoring some lines of code\n\n* Isolating KV Cache logic to relevant components\n\n* Fixing logic error in Gemma3nAttention.forward\n\n* Refactoring caching contributions and fixing kv_store initialization\n\n* Simplifying Configs\n\n* Remove errant self from super init call\n\n* Bug fix in the Attention module - changing self.head_dim to config.head_dim\n\n* Bug fixes in the LaurelBlock and RMS Norm super init call\n\n* removing redundant code from a merge\n\n* Adding per_layer_inputs to TextModel\n\n* Adding preprocess embeddings with altup\n\n* Adds per-layer-to-single output and a host of TODOs\n\n* Integrating altup predict with the model workflow and other minor bug fixes\n\n* Using nn.Embedding temporarily for text model\n\n* It goes forward\n\n* Minor refactor of attention sparsity and RoPE initialization\n\n* Fixing duplicate rope_scaling param bug when loading from pretrained\n\n---------\n\nCo-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>\nCo-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>\n\n* Normalizing on altup_num_inputs config option\n\n* regenerating modeling file after syncing to HEAD\n\n* Use torch.std(..., unbiased=False) for activation sparsity (#8)\n\n* Refactoring to a single QVK Norm (#13)\n\n* AltUp: support scale_corrected_output (#14)\n\n* Converts einsums to nn.Linear (#7)\n\n* Converts einsums to nn.Linear\n\n* Removing unused variables\n\n* Aligning SharedKVCache with HybridCache (#11)\n\n* Alinging SharedKVStore with HybridCache\n\n* Remove KVStore. Refactor apply_rotary_pos_emb for sharing\n\n* Addressing review comments\n\n* Supporting split modality embeddings in Gemma3n (#10)\n\n* Adding the Embedder class\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryan@ryanmullins.org>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryan@ryanmullins.org>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryan@ryanmullins.org>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryan@ryanmullins.org>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryan@ryanmullins.org>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryan@ryanmullins.org>\n\n* Addressing review comments, adding audio embedding layers, integrating embedder with the remaining architecture, adding a forward method for conditional generation\n\n* Apply suggestions from code review\n\nCo-authored-by: Ryan Mullins <ryan@ryanmullins.org>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryan@ryanmullins.org>\n\n* Addressing review comments, prop drilling audio and vision configs to the text config\n\n* Removing TODO's that have been addressed\n\n* Simplify Embedder init and add audio embeddings\n\n* Embeddings refactor. Adds Gemma3nAudioEmbedder and Gemma3nVisionEmbedder\n\n* Refactoring vision and audio embeddings into ConditionalGeneration model\n\n---------\n\nCo-authored-by: Ryan Mullins <ryan@ryanmullins.org>\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Updating attention mask for Gemma 3.5 (#15)\n\n* xxx_token_index to xxx_token_id\n\n* remvoing deprecated last_cache_position\n\n* Removing references to SigLIP\n\n* Always init per-layer inputs\n\n* Using torch.finfo().min for epsilon_tensor\n\n* Gemma3nDecoderLayer inherits from Gemma3DecoderLayer. Remove gating lambdas\n\n* fix modular GEMMA3N_INPUTS_DOCSTRING\n\n* Gemma3nAttention inherits from Gemma3Attention\n\n* Modular inheritance fixes\n\n* CausalLM conversion script for 4B model (#16)\n\n* Add Gemma3n Audio Encoder (#6)\n\n* initial commit of Gemma 3.5 scaffold\n\n* Fixing param pass through on Gemm3nRMSNorm\n\n* Adds Einsum layer to Gemma 3.5\n\n* Updating EinsumLayer API\n\n* Undoing erroneous force push\n\n* Reverting RMSNorm to with_scale by default\n\n* Adds LAuReL to Gemma 3n\n\n* Adds AltUp to Gemma 3n\n\n* Adding Gemma3n overall and text config with vision and audio config placeholders (#3)\n\n* Adding gemma3n text configs\n\n* Adding audio config placeholders\n\n* Adding a placeholder for vision configs\n\n* Updating MobileNetVisionConfig, inheriting TimmWrapperConfig\n\n* Updating text configs\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Removing altup configs to accept the suggested configs\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Updating altup config\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Addressing review comments and updating text configs\n\n* Adding a config for activation sparsity\n\n* Updating configs to pass through options to super class init and adjust some name prefixes\n\n* Updating laurel and altup with corrected config values\n\n* Normalizing sub_config initializers\n\n---------\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Updating MLP with activation sparsity (#2)\n\n* Updating DecoderBlock for Gemma 3.5 (#3)\n\n* Initial Gemm3nTextModel (#4)\n\nNOTE: This implementation WILL CHANGE in the coming weeks, however, changes will be strictly additive and this will remain a suitable baseline for downstream implementations to reference.\n\n* Adding KV Cache Sharing\n\n* Adds Einsum layer to Gemma 3.5\n\n* Updating EinsumLayer API\n\n* Refactored kv cache sharing in attention\n\n* Adding KVStore for cache sharing\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update src/transformers/cache_utils.py\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Undoing erroneous force push\n\n* Reverting RMSNorm to with_scale by default\n\n* Adds LAuReL to Gemma 3n\n\n* Updating KV Cache Sharing implementation\n\n* Updating the q and k norm definitions in the attention module\n\n* Fixing name error for q,k,v RMS norm to use the right Gemma 3n module\n\n* Updating MLP with activation sparsity\n\n* Updating DecoderBlock for Gemma 3.5\n\n* Updating kv cache sharing implementation with the use of a cache buffer and refactoring some lines of code\n\n* Isolating KV Cache logic to relevant components\n\n* Fixing logic error in Gemma3nAttention.forward\n\n* Refactoring caching contributions and fixing kv_store initialization\n\n* Simplifying Configs\n\n* Remove errant self from super init call\n\n* Bug fix in the Attention module - changing self.head_dim to config.head_dim\n\n* Bug fixes in the LaurelBlock and RMS Norm super init call\n\n* removing redundant code from a merge\n\n* Adding per_layer_inputs to TextModel\n\n* Adding preprocess embeddings with altup\n\n* Adds per-layer-to-single output and a host of TODOs\n\n* Integrating altup predict with the model workflow and other minor bug fixes\n\n* Using nn.Embedding temporarily for text model\n\n* It goes forward\n\n* Minor refactor of attention sparsity and RoPE initialization\n\n* Fixing duplicate rope_scaling param bug when loading from pretrained\n\n---------\n\nCo-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>\nCo-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>\n\n* Normalizing on altup_num_inputs config option\n\n* Adding audio encoder config\n\n* Adds high-level components for Audio Encoder\n\n* Implement uniform reducer for Audio Encoder\n\n* Adding placeholders for Conformer components in Audio Encoder\n\n* Adding placeholders for SubSampleConvProjection components in Audio Encoder\n\n* Adding SequenceLayer component placeholders\n\n* Implementing Gemma3nAudioEncoder with nn.Sequential\n\n* Implementing Gemma3nAudioSubSampleConvProjection with nn.Sequential\n\n* Implementing Conformer model with SequenceLayers\n\n* Use OrderedDict in nn.Sequential initializers\n\n* Implements sl.Residual in Torch with nn.Sequential and OrderedDict\n\n* Adopting a base SequenceLayer class with default forward() method\n\n* Implementing sl.GatedLinearUnit in Torch\n\n* Implementing sl.Swish in Torch\n\n* Implementing sl.ReLU in Torch\n\n* Implementing sl.Scale in Torch\n\n* Removing sl.Dropout after tree-shaking\n\n* Implementing sl.RMSNorm in Torch with fake shape\n\n* Implementing sl.GroupNorm in Torch\n\n* Implementing sl.Conv2d in Torch\n\n* Implementing sl.Dense in Torch\n\n* Removing sl.Delay layers, which act as pass-throughs\n\n* Connecting shapes to configs in initializers\n\n* Removing sl.Emit\n\n* Implementing sl.ExpandDims in Torch\n\n* Adding sl.GradientClipping to Torch\n\n* Implementing sl.DenseShaped in Torch\n\n* Implementing sl.LDPA in Torch\n\n* Removing unused sl.CombinedQKVProj class\n\n* Fixing erroneous type hint\n\n* Implemnenting sl.DepthwiseConv1D in Torch\n\n* Implementing sl.MaskInvalid in Torch\n\n* Fixes for initialization\n\n* Fixes for saving weights\n\n* Removing einsums per feedback from HF staff\n\n* Removing Sequence Layers idioms from audio encoder\n\n* Fixes for reviewer comments\n\n* CausalLM conversion script for 4B model\n\n* inv_timescales to non-persistent buffer\n\n* Addressing audio encoder Attention feedback\n\n* Addressing Gemma3nAudioSSCPConvBlock feedback\n\n* Addressing Gemma3nAudioConformerAttention feedback\n\n* Addressing padding feedback\n\n* Weights conversion loads audio state dict\n\n* Always use vision_config so saving works\n\n* Token id updates for configs\n\n* Stubs for interleaving audio embs\n\n* Addressing reviewer feedback\n\n---------\n\nCo-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>\nCo-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>\n\n* Fixing cache access error\n\n* Removing duplicate code from a bad merge\n\n* Gemma 3n Text + Vision Part 1 (#17)\n\n* testing utilities for numerics comparisons\n\n* Corrected einsum to nn.Linear weights conversion\n\n* Inherit scaled word embs from Gemma3 not Bart\n\n* Fixing transposes for collapsed linears\n\n* More transpose fixes\n\n* numpy api fix\n\n* RMSNorm: Explicit kwargs, scale_shift=0.0 when with_scale=True\n\n* Force AltUp  to float32\n\n* Updating debugging script for AudioEncoder debugging\n\n* Support divide_weight_by_sqrt_fan_in from JAX for per-layer inputs\n\n* Correcting attention einsum conversions\n\n* RMSNorm in type of x\n\n* Fixing douplicate laurel norm/gating\n\n* KV sharing using the right previous indices\n\n* Refactor kv shared index computation. Correct frac_shared_layers\n\n* Use num_shared_layers instead of inferring from a fraction\n\n* fixing a bug for logging\n\n* Fix shared data_ptrs in altup inits\n\n* rope: adjust proj -> norm -> rope to preserve computation (#20)\n\n* rope: adjust proj -> norm -> rope to preserve computation\n\n* Removing some breaking language model fluff in ConditionalGeneration\n\n* Consolidate query_states transforms\n\n---------\n\nCo-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com>\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Vectorize the loops in AltUp (#19)\n\n* Vectorize the loops in AltUp\n\n* fix typo\n\n* Expanding to support batched inputs\n\n* remove extra debug script\n\n* Fix AltUp.forward\n\n---------\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Add 'scale_shift=0.0, with_scale=True' to the final norm in TextModel\n\n* Convert norm to 1/sqrt (#21)\n\n* Convert norm to 1/sqrt\n\n* Scale shift change per Phil's rec\n\n* Adding default activation sparsity\n\n* Fixing 2B config in weights conversion script\n\n* Fixing RMSNorm parameters - adding scale_shift and with_scale\n\n* Correcting query pre-attention scaling\n\n* Adding query_rescale_scalar to text config\n\n* Adding layer_idx to MLP\n\n* Permafix for input_layernorm\n\n* Use 1/sqrt instead of rsqrt in DecoderLayer\n\n* Fix o_proj conversion\n\n* Conversion script update for vision encoder\n\n* Removing logging for debugging timm model\n\n* Fixing bugs in Gemma3nForConditionalGeneration for text generation\n\n* Generating the modeling_gemma3n.py file\n\n* Removing the addition of an erroneous line in the modeling file\n\n* Adding gemma3n text model to modeling_auto\n\n* Bugfix: Updating the interleaving of inputs_embeds and vision_embeds\n\n* Updating the modeling file with the latest bugfix changes\n\n* Updating models/auto for Gemma 3n\n\n* using AutoTokenizer in forward test\n\n* Adding processing_gemma3n.py\n\n* Gemma 3n configured for AutoModel. Conversion script updated.\n\n* Removing errant merge artifacts\n\n---------\n\nCo-authored-by: Mayank Chaturvedi <imayank@google.com>\nCo-authored-by: Douglas Reid <douglas-reid@users.noreply.github.com>\nCo-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com>\nCo-authored-by: Xuan-Son Nguyen <thichthat@gmail.com>\nCo-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>\n\n* Removing errant debugging statements from Gemma 3\n\n* Gemma3n audio model (#18)\n\n* testing utilities for numerics comparisons\n\n* Implement CumulativeGroupNorm and add to SubSampleConvProjection and SSCPConvBlock\n\n* Add audio version of forward script based on RyanMullins' implementation\n\n* Updating to match encoder tests. WIP: config question needs resolving\n\n* Updates to audio classes to enable end-to-end running\n\n* Removing vestigial classes, cleaning up print statements\n\n* Adding SiLU / Swish to audio conformer feed forward block\n\n* Shifted Gemma3p5Audio naming prefix to Gemma3NanoAudio\n\n* Adding outputs to audio test\n\n* Fixes to padding in SSCP and 1D convolution, align RMS Norm with wider model\n\n* Update forward test to load from local weights\n\n* Update conversion to process / output audio layers\n\n* Update __all__ to export audio encoder\n\n* AutoModel registration for Gemma 3n Audio\n\n* Use AutoModel for ConditionalGeneration.audio_tower\n\n* Fixing input_proj_linear transpose\n\n* Fixing Gemma3NanoAudioConformerAttention.post conversion\n\n* Fixing Gemma3NanoAudioSSCPConvBlock.conv weights conversion\n\n* Correcting indentation issue on Gemma3p5RMSNorm\n\n---------\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Text + Vision Part 2 (#23)\n\n* Updates for ConditionalGeneration.get_image_features\n\n* Adding a WIP draft of image_processing_gemma3p5.py\n\n* Update src/transformers/models/gemma3p5/modular_gemma3p5.py\n\nCo-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>\n\n* Modular conversion after github suggested change\n\n* Text + image gives good results\n\n* Fixing image size preset\n\n* Updating configs for the 2B variant in the conversion script\n\n* Using final generation config in conversion script\n\n---------\n\nCo-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>\nCo-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>\n\n* Audio Integration (#12)\n\n* initial commit of Gemma 3n scaffold\n\n* Fixing param pass through on Gemm3nRMSNorm\n\n* Adds Einsum layer to Gemma 3n\n\n* Updating EinsumLayer API\n\n* Undoing erroneous force push\n\n* Reverting RMSNorm to with_scale by default\n\n* Adds LAuReL to Gemma 3n\n\n* Adds AltUp to Gemma 3n\n\n* Adding Gemma 3n overall and text config with vision and audio config placeholders (#3)\n\n* Adding Gemma 3n text configs\n\n* Adding audio config placeholders\n\n* Adding a placeholder for vision configs\n\n* Updating MobileNetVisionConfig, inheriting TimmWrapperConfig\n\n* Updating text configs\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Removing altup configs to accept the suggested configs\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Updating altup config\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Addressing review comments and updating text configs\n\n* Adding a config for activation sparsity\n\n* Updating configs to pass through options to super class init and adjust some name prefixes\n\n* Updating laurel and altup with corrected config values\n\n* Normalizing sub_config initializers\n\n---------\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Updating MLP with activation sparsity (#2)\n\n* Updating DecoderBlock for Gemma 3n (#3)\n\n* Initial Gemma3nTextModel (#4)\n\nNOTE: This implementation WILL CHANGE in the coming weeks, however, changes will be strictly additive and this will remain a suitable baseline for downstream implementations to reference.\n\n* Adding KV Cache Sharing\n\n* Adds Einsum layer to Gemma 3n\n\n* Updating EinsumLayer API\n\n* Refactored kv cache sharing in attention\n\n* Adding KVStore for cache sharing\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update modular\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Update src/transformers/cache_utils.py\n\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\n\n* Undoing erroneous force push\n\n* Reverting RMSNorm to with_scale by default\n\n* Adds LAuReL to Gemma 3n\n\n* Updating KV Cache Sharing implementation\n\n* Updating the q and k norm definitions in the attention module\n\n* Fixing name error for q,k,v RMS norm to use the right 3n module\n\n* Updating MLP with activation sparsity\n\n* Updating DecoderBlock for Gemma 3n\n\n* Updating kv cache sharing implementation with the use of a cache buffer and refactoring some lines of code\n\n* Isolating KV Cache logic to relevant components\n\n* Fixing logic error in Gemma3nAttention.forward\n\n* Refactoring caching contributions and fixing kv_store initialization\n\n* Simplifying Configs\n\n* Remove errant self from super init call\n\n* Bug fix in the Attention module - changing self.head_dim to config.head_dim\n\n* Bug fixes in the LaurelBlock and RMS Norm super init call\n\n* removing redundant code from a merge\n\n* Adding per_layer_inputs to TextModel\n\n* Adding preprocess embeddings with altup\n\n* Adds per-layer-to-single output and a host of TODOs\n\n* Integrating altup predict with the model workflow and other minor bug fixes\n\n* Using nn.Embedding temporarily for text model\n\n* It goes forward\n\n* Minor refactor of attention sparsity and RoPE initialization\n\n* Fixing duplicate rope_scaling param bug when loading from pretrained\n\n---------\n\nCo-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>\nCo-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>\n\n* Normalizing on altup_num_inputs config option\n\n* Adding audio encoder config\n\n* Adds high-level components for Audio Encoder\n\n* Implement uniform reducer for Audio Encoder\n\n* Adding placeholders for Conformer components in Audio Encoder\n\n* Adding placeholders for SubSampleConvProjection components in Audio Encoder\n\n* Adding SequenceLayer component placeholders\n\n* Implementing Gemma3nAudioEncoder with nn.Sequential\n\n* Implementing Gemma3nAudioSubSampleConvProjection with nn.Sequential\n\n* Implementing Conformer model with SequenceLayers\n\n* Use OrderedDict in nn.Sequential initializers\n\n* Implements sl.Residual in Torch with nn.Sequential and OrderedDict\n\n* Adopting a base SequenceLayer class with default forward() method\n\n* Implementing sl.GatedLinearUnit in Torch\n\n* Implementing sl.Swish in Torch\n\n* Implementing sl.ReLU in Torch\n\n* Implementing sl.Scale in Torch\n\n* Removing sl.Dropout after tree-shaking\n\n* Implementing sl.RMSNorm in Torch with fake shape\n\n* Implementing sl.GroupNorm in Torch\n\n* Implementing sl.Conv2d in Torch\n\n* Implementing sl.Dense in Torch\n\n* Removing sl.Delay layers, which act as pass-throughs\n\n* Connecting shapes to configs in initializers\n\n* Removing sl.Emit\n\n* Implementing sl.ExpandDims in Torch\n\n* Adding sl.GradientClipping to Torch\n\n* Implementing sl.DenseShaped in Torch\n\n* Implementing sl.LDPA in Torch\n\n* Removing unused sl.CombinedQKVProj class\n\n* Fixing erroneous type hint\n\n* Implemnenting sl.DepthwiseConv1D in Torch\n\n* Implementing sl.MaskInvalid in Torch\n\n* Fixes for initialization\n\n* Fixes for saving weights\n\n* Removing einsums per feedback from HF staff\n\n* Removing Sequence Layers idioms from audio encoder\n\n* Fixes for reviewer comments\n\n* Converting sl.Frontend to FeatureExtractor\n\n* Updates for ConditionalGeneration.get_image_features\n\n* Adding a WIP draft of image_processing_gemma3n.py\n\n* Update modular\n\nCo-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>\n\n* Modular conversion after github suggested change\n\n* Text + image gives good results\n\n* Fixing image size preset\n\n* Draft of audio data in chat template\n\n* Removing image processing. Using SigLIP instead.\n\n* Audio input going end-to-end\n\n* Fixing dtype issues in audio encoder\n\n* x-lib formatting consistency\n\n* Adding example data\n\n* Save preprocessor_config.json from conversion script\n\n* Instrumentaiton for debugging\n\n* Additional instrumentation for preprocessing debugging\n\n* Updates to preprocessor, padding; produces correct end-to-end results on sample\n\n* Tackling configuraiton TODOs\n\n* Start of feature extractor refatcor\n\n* Adds Numpy version of USM extractor, removes Torch version and dependencies\n\n* Fixing AltUp.correct coef permute\n\n* Supporting batches of single audio segment inputs\n\n* Docstrings updates for config\n\n* In-lining audio feature extraction\n\n* Adjustments to conversion script and smoke test script\n\n---------\n\nCo-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>\nCo-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>\nCo-authored-by: pculliton <phillipculliton@gmail.com>\n\n* Gemma 3n renaming\n\n* Removing test data and utilities\n\n* Renaming test files\n\n* Gemma 3n refactor\n\n* Fix tokenizer config in conversion script\n\n* Address reviewer feedback\n\n* FeatureExtractor returns float32 by default\n\n* Adding basic tests for audio, and input name for audio encoder\n\n* Audio integration test, updates to model_id for other integration tests\n\n* Use scales for q and k norms (#26)\n\n* Update audio integration test to use HF dataset\n\n* Reviewer feedback\n\n* Expand embedding table to full vocab size in weights conversion\n\n* Mix-n-match MatFormers for Gemma 3n (#25)\n\n* Remove in-place operations (#30)\n\n* chore: removing inplace ops\n\n* remove [tensor] * n pattern\n\n* chore: reviewer feedback in AudioEncoder and AltUp\n\n* More grad clipping\n\n* Dynamo compatibility\n\n* fix: cache slicing error\n\n* chore: simplify shared kv cache slicing\n\n* chore: vision encoder rename in timm\n\n* fix: image processor do_normalize=False\n\n* fixup: style\n\n* chore: model_doc\n\n* fix: docs for code quality\n\n* chore: repo consistency\n\n* fix: RMSNorm in float as in prior Gemmas\n\n* fix: per_layer_inputs = None\n\n* chore: Gemma3nForCausalLM from Gemma3nForConditionalGeneration checkpoint\n\n* chore: repo consistency\n\n* Add initial unit tests for Gemma3nAudioFeatureExtractor (#27)\n\n* Add initial unit tests for Gemma3nAudioFeatureExtractor\n\n* Add basic unit tests for Gemma3nProcessor (#28)\n\nCo-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com>\n\n* parameterize tests\n\n---------\n\nCo-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com>\n\n* chore: code style\n\n* fix: test cases\n\n* style and consistency\n\n* fix config in the test to be coherent with layer cache sharing\n\n* fix hidden states in tests and code\n\n* inits and mappings\n\n* fix modality prefixes\n\n* test order and prefixes\n\n* fix test exception\n\n* fix class order and reduce model size for faster tests\n\n* restore _checkpoint_conversion_mapping to load Caual from Conditional\n\n* fix config mapping!\n\n* fix: reviewer feedback\n\n---------\n\nCo-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>\nCo-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>\nCo-authored-by: raushan <raushan@huggingface.co>\nCo-authored-by: Mayank Chaturvedi <imayank@google.com>\nCo-authored-by: Douglas Reid <douglas-reid@users.noreply.github.com>\nCo-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com>\nCo-authored-by: Xuan-Son Nguyen <thichthat@gmail.com>\nCo-authored-by: pculliton <phillipculliton@gmail.com>\nCo-authored-by: Aritra Roy Gosthipaty <aritra.born2fly@gmail.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\n\n* fix import test\n\n* add model args\n\n* auto_docstring\n\n* replace test path\n\n* consistency\n\n* skip tests for now\n\n* fix docstring for doc builder\n\n* skip unused attr\n\n---------\n\nCo-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>\nCo-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>\nCo-authored-by: raushan <raushan@huggingface.co>\nCo-authored-by: Mayank Chaturvedi <imayank@google.com>\nCo-authored-by: Douglas Reid <douglas-reid@users.noreply.github.com>\nCo-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com>\nCo-authored-by: Xuan-Son Nguyen <thichthat@gmail.com>\nCo-authored-by: pculliton <phillipculliton@gmail.com>\nCo-authored-by: Aritra Roy Gosthipaty <aritra.born2fly@gmail.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\nCo-authored-by: Arthur <arthur.zucker@gmail.com>",
    "sha": "c63cfd6a833d629a74c098933017c61dd755969d",
    "files": [
        {
            "sha": "7508f0968863386931289d52617829c647b3869d",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -959,6 +959,8 @@\n         title: FLAVA\n       - local: model_doc/gemma3\n         title: Gemma3\n+      - local: model_doc/gemma3n\n+        title: Gemma3n\n       - local: model_doc/git\n         title: GIT\n       - local: model_doc/glm4v"
        },
        {
            "sha": "7f38c3b18c92866abba05de5aa0730abd31303fe",
            "filename": "docs/source/en/model_doc/gemma3n.md",
            "status": "added",
            "additions": 204,
            "deletions": 0,
            "changes": 204,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -0,0 +1,204 @@\n+\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n+# Gemma3n\n+\n+## Overview\n+\n+Gemma3n is a multimodal model with pretrained and instruction-tuned variants, available in E4B and E2B sizes. While\n+large portions of the language model architecture are shared with prior Gemma releases, there are many new additions in\n+this model, including [Alternating Updates][altup] (AltUp), [Learned Augmented Residual Layer][laurel] (LAuReL),\n+[MatFormer][matformer], Per-Layer Embeddings (PLE), activation sparsity, and KV cache sharing. The language model uses\n+a similar attention pattern to [Gemma 3](./gemma3.md) with alternating 4 local sliding window self-attention layers for\n+every global self-attention layer with a maximum context length of 32k tokens. Gemma 3n introduces\n+[MobileNet v5][mobilenetv5] as the vision encoder, using a default resolution of 768x768 pixels, and adds a\n+[Universal Speech Model][usm] (USM) as the audio encoder.\n+\n+The instruction-tuned variant was post-trained with knowledge distillation and reinforcement learning.\n+\n+You can find all the original Gemma 3n checkpoints under the [Gemma 3n][gemma3n-collection] release.\n+\n+> [!TIP]\n+> Click on the Gemma 3n models in the right sidebar for more examples of how to apply Gemma to different vision, audio,\n+> and language tasks.\n+\n+The example below demonstrates how to generate text based on an image with [`Pipeline`] or the [`AutoModel`] class.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    task=\"image-text-to-text\",\n+    model=\"google/gemma-3n-e4b\",\n+    device=0,\n+    torch_dtype=torch.bfloat16\n+)\n+pipeline(\n+    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+    text=\"<start_of_image> What is shown in this image?\"\n+)\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoProcessor, Gemma3nForConditionalGeneration\n+\n+model = Gemma3nForConditionalGeneration.from_pretrained(\n+    \"google/gemma-3n-e4b-it\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+processor = AutoProcessor.from_pretrained(\n+    \"google/gemma-3n-e4b-it\",\n+    padding_side=\"left\"\n+)\n+\n+messages = [\n+    {\n+        \"role\": \"system\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}\n+        ]\n+    },\n+    {\n+        \"role\": \"user\", \"content\": [\n+            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"},\n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+        ]\n+    },\n+]\n+inputs = processor.apply_chat_template(\n+    messages,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+    add_generation_prompt=True,\n+).to(\"cuda\")\n+\n+output = model.generate(**inputs, max_new_tokens=50, cache_implementation=\"static\")\n+print(processor.decode(output[0], skip_special_tokens=True))\n+```\n+\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n+\n+```bash\n+echo -e \"Plants create energy through a process known as\" | transformers run --task text-generation --model google/gemma-3n-e2b --device 0\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## Notes\n+\n+-   Use [`Gemma3nForConditionalGeneration`] for image-audio-and-text, image-and-text, image-and-audio, audio-and-text,\n+    image-only and aduio-only inputs.\n+-   Gemma 3n supports multiple images per input, but make sure the images are correctly batched before passing them to\n+    the processor. Each batch should be a list of one or more images.\n+\n+    ```py\n+    url_cow = \"https://media.istockphoto.com/id/1192867753/photo/cow-in-berchida-beach-siniscola.jpg?s=612x612&w=0&k=20&c=v0hjjniwsMNfJSuKWZuIn8pssmD5h5bSN1peBd1CmH4=\"\n+    url_cat = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+\n+    messages =[\n+        {\n+            \"role\": \"system\",\n+            \"content\": [\n+                {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}\n+            ]\n+        },\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"image\", \"url\": url_cow},\n+                {\"type\": \"image\", \"url\": url_cat},\n+                {\"type\": \"text\", \"text\": \"Which image is cuter?\"},\n+            ]\n+        },\n+    ]\n+    ```\n+-   Text passed to the processor should have a `<image_soft_token>` token wherever an image should be inserted.\n+-   Gemma 3n accept at most one target audio clip per input, though multiple audio clips can be provided in few-shot\n+    prompts, for example.\n+-   Text passed to the processor should have a `<audio_soft_token>` token wherever an audio clip should be inserted.\n+-   The processor has its own [`~ProcessorMixin.apply_chat_template`] method to convert chat messages to model inputs.\n+\n+## Gemma3nAudioFeatureExtractor\n+\n+[[autodoc]] Gemma3nAudioFeatureExtractor\n+\n+## Gemma3nProcessor\n+\n+[[autodoc]] Gemma3nProcessor\n+\n+## Gemma3nTextConfig\n+\n+[[autodoc]] Gemma3nTextConfig\n+\n+## Gemma3nVisionConfig\n+\n+[[autodoc]] Gemma3nVisionConfig\n+\n+## Gemma3nAudioConfig\n+\n+[[autodoc]] Gemma3nAudioConfig\n+\n+## Gemma3nConfig\n+\n+[[autodoc]] Gemma3nConfig\n+\n+## Gemma3nTextModel\n+\n+[[autodoc]] Gemma3nTextModel\n+    - forward\n+\n+## Gemma3nModel\n+\n+[[autodoc]] Gemma3nModel\n+    - forward\n+\n+## Gemma3nForCausalLM\n+\n+[[autodoc]] Gemma3nForCausalLM\n+    - forward\n+\n+## Gemma3nForConditionalGeneration\n+\n+[[autodoc]] Gemma3nForConditionalGeneration\n+    - forward\n+\n+[altup]: https://proceedings.neurips.cc/paper_files/paper/2023/hash/f2059277ac6ce66e7e5543001afa8bb5-Abstract-Conference.html\n+[attention-mask-viz]: https://github.com/huggingface/transformers/blob/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/src/transformers/utils/attention_visualizer.py#L139\n+[gemma3n-collection]: https://huggingface.co/collections/google/gemma-3n\n+[laurel]: https://arxiv.org/abs/2411.07501\n+[matformer]: https://arxiv.org/abs/2310.07707\n+[usm]: https://arxiv.org/abs/2303.01037"
        },
        {
            "sha": "3b9e3e65df6fb82c14d691d96e0078ec25e15192",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -140,6 +140,10 @@\n         (\"gemma2\", \"Gemma2Config\"),\n         (\"gemma3\", \"Gemma3Config\"),\n         (\"gemma3_text\", \"Gemma3TextConfig\"),\n+        (\"gemma3n\", \"Gemma3nConfig\"),\n+        (\"gemma3n_audio\", \"Gemma3nAudioConfig\"),\n+        (\"gemma3n_text\", \"Gemma3nTextConfig\"),\n+        (\"gemma3n_vision\", \"Gemma3nVisionConfig\"),\n         (\"git\", \"GitConfig\"),\n         (\"glm\", \"GlmConfig\"),\n         (\"glm4\", \"Glm4Config\"),\n@@ -518,6 +522,10 @@\n         (\"gemma2\", \"Gemma2\"),\n         (\"gemma3\", \"Gemma3ForConditionalGeneration\"),\n         (\"gemma3_text\", \"Gemma3ForCausalLM\"),\n+        (\"gemma3n\", \"Gemma3nForConditionalGeneration\"),\n+        (\"gemma3n_audio\", \"Gemma3nAudioEncoder\"),\n+        (\"gemma3n_text\", \"Gemma3nForCausalLM\"),\n+        (\"gemma3n_vision\", \"TimmWrapperModel\"),\n         (\"git\", \"GIT\"),\n         (\"glm\", \"GLM\"),\n         (\"glm4\", \"GLM4\"),\n@@ -839,6 +847,9 @@\n         (\"clip_text_model\", \"clip\"),\n         (\"aria_text\", \"aria\"),\n         (\"gemma3_text\", \"gemma3\"),\n+        (\"gemma3n_audio\", \"gemma3n\"),\n+        (\"gemma3n_text\", \"gemma3n\"),\n+        (\"gemma3n_vision\", \"gemma3n\"),\n         (\"glm4v_text\", \"glm4v\"),\n         (\"idefics3_vision\", \"idefics3\"),\n         (\"siglip_vision_model\", \"siglip\"),"
        },
        {
            "sha": "3595de53bbda0e56441d9975bda5d2ec5adef17f",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -61,6 +61,7 @@\n         (\"dpt\", \"DPTFeatureExtractor\"),\n         (\"encodec\", \"EncodecFeatureExtractor\"),\n         (\"flava\", \"FlavaFeatureExtractor\"),\n+        (\"gemma3n\", \"Gemma3nAudioFeatureExtractor\"),\n         (\"glpn\", \"GLPNFeatureExtractor\"),\n         (\"granite_speech\", \"GraniteSpeechFeatureExtractor\"),\n         (\"groupvit\", \"CLIPFeatureExtractor\"),"
        },
        {
            "sha": "bee0335338cc708c47b88c506b5bbb86b785896a",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -88,6 +88,7 @@\n             (\"focalnet\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n             (\"fuyu\", (\"FuyuImageProcessor\",)),\n             (\"gemma3\", (\"Gemma3ImageProcessor\", \"Gemma3ImageProcessorFast\")),\n+            (\"gemma3n\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"git\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"glm4v\", (\"Glm4vImageProcessor\", \"Glm4vImageProcessorFast\")),\n             (\"glpn\", (\"GLPNImageProcessor\",)),"
        },
        {
            "sha": "08b91dc1ea5fc23fc97042e3e3760be0c68a3db7",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -132,6 +132,10 @@\n         (\"gemma2\", \"Gemma2Model\"),\n         (\"gemma3\", \"Gemma3Model\"),\n         (\"gemma3_text\", \"Gemma3TextModel\"),\n+        (\"gemma3n\", \"Gemma3nModel\"),\n+        (\"gemma3n_audio\", \"Gemma3nAudioEncoder\"),\n+        (\"gemma3n_text\", \"Gemma3nTextModel\"),\n+        (\"gemma3n_vision\", \"TimmWrapperModel\"),\n         (\"git\", \"GitModel\"),\n         (\"glm\", \"GlmModel\"),\n         (\"glm4\", \"Glm4Model\"),\n@@ -583,6 +587,8 @@\n         (\"gemma2\", \"Gemma2ForCausalLM\"),\n         (\"gemma3\", \"Gemma3ForConditionalGeneration\"),\n         (\"gemma3_text\", \"Gemma3ForCausalLM\"),\n+        (\"gemma3n\", \"Gemma3nForConditionalGeneration\"),\n+        (\"gemma3n_text\", \"Gemma3nForCausalLM\"),\n         (\"git\", \"GitForCausalLM\"),\n         (\"glm\", \"GlmForCausalLM\"),\n         (\"glm4\", \"Glm4ForCausalLM\"),\n@@ -906,6 +912,7 @@\n         (\"emu3\", \"Emu3ForConditionalGeneration\"),\n         (\"fuyu\", \"FuyuForCausalLM\"),\n         (\"gemma3\", \"Gemma3ForConditionalGeneration\"),\n+        (\"gemma3n\", \"Gemma3nForConditionalGeneration\"),\n         (\"git\", \"GitForCausalLM\"),\n         (\"glm4v\", \"Glm4vForConditionalGeneration\"),\n         (\"got_ocr2\", \"GotOcr2ForConditionalGeneration\"),"
        },
        {
            "sha": "e5bd673f6390cba46aaf3843011275fae183b8c1",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -66,6 +66,7 @@\n         (\"flava\", \"FlavaProcessor\"),\n         (\"fuyu\", \"FuyuProcessor\"),\n         (\"gemma3\", \"Gemma3Processor\"),\n+        (\"gemma3n\", \"Gemma3nProcessor\"),\n         (\"git\", \"GitProcessor\"),\n         (\"glm4v\", \"Glm4vProcessor\"),\n         (\"got_ocr2\", \"GotOcr2Processor\"),"
        },
        {
            "sha": "c8656a710746d97657b8b8a5109e6fa0dc12f7bb",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -236,6 +236,20 @@\n                 \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n             ),\n         ),\n+        (\n+            \"gemma3n\",\n+            (\n+                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n+                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"gemma3n_text\",\n+            (\n+                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n+                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n         (\"git\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"glm\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"glm4\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),"
        },
        {
            "sha": "229e91827036d0830593ea9294e232cffefbac7b",
            "filename": "src/transformers/models/gemma3n/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fgemma3n%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fgemma3n%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2F__init__.py?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_gemma3n import *\n+    from .feature_extraction_gemma3n import *\n+    from .modeling_gemma3n import *\n+    from .processing_gemma3n import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "ca1a06717748448310a0a5b4b623968855fdc646",
            "filename": "src/transformers/models/gemma3n/configuration_gemma3n.py",
            "status": "added",
            "additions": 680,
            "deletions": 0,
            "changes": 680,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -0,0 +1,680 @@\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+#           This file was automatically generated from src/transformers/models/gemma3n/modular_gemma3n.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_gemma3n.py file directly. One of our CI enforces this.\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+# coding=utf-8\n+# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from collections.abc import Sequence\n+from typing import Any, Optional, Union\n+\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...modeling_rope_utils import rope_config_validation\n+from ...utils import is_timm_available, logging, requires_backends\n+\n+\n+if is_timm_available():\n+    from timm.data import ImageNetInfo, infer_imagenet_subset\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Gemma3nTextConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Gemma3nTextModel`]. It is used to instantiate an\n+    Gemma3nTextModel model according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the Gemma 3n E4B, e.g.\n+    [google/gemma-3n-E4B](https://huggingface.co/google/gemma-3n-E4B).\n+\n+    Configuration objects that inherit from [`Gemma3nTextConfig`] and can be used to control the model outputs. Read\n+    the documentation from [`Gemma3nTextConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 262400):\n+            Vocabulary size of the Gemma3nText model. Defines the number of different tokens that can be represented by\n+            the `inputs_ids` passed when calling [`Gemma3nTextModel`]\n+        vocab_size_per_layer_input (`int`, *optional*, defaults to 262144):\n+            Vocabulary size of the per-layer text embeddings that augment the standard embeddings.\n+        hidden_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the hidden representations.\n+        hidden_size_per_layer_input (`int`, *optional*, defaults to 256):\n+            Dimension of the hidden representations for per-layer emebeddings.\n+        intermediate_size (`int` or `Sequence[int]`, *optional*, defaults to 16384):\n+            Dimension of the MLP representations. MatFormer configurations may wish to provide a sequence of integers\n+            to account for vairable intermediate_size values across layers. In such cases,\n+            `len(intermediate_size) == num_hidden_layers`.\n+        num_hidden_layers (`int`, *optional*, defaults to 35):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 2):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout this\n+            [paper](https://arxiv.org/pdf/2305.13245.pdf). If not specified, will default to `num_attention_heads`.\n+        head_dim (`int`, *optional*, defaults to 256):\n+            The attention head dimension.\n+        hidden_activation (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+            The non-linear activation function (function or string) in the decoder. Will default to\n+            `\"gelu_pytorch_tanh\"` if not specified. `\"gelu_pytorch_tanh\"` uses an approximation of the `\"gelu\"`\n+            activation function.\n+        max_position_embeddings (`int`, *optional*, defaults to 32768):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        eos_token_id (`int`, *optional*, defaults to 1):\n+            End of stream token id.\n+        bos_token_id (`int`, *optional*, defaults to 2):\n+            Beginning of stream token id.\n+        rope_theta (`float`, *optional*, defaults to 1000000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings used in gloabl attention.\n+            NOTE: if you apply new rope type and you expect the model to work on longer `max_position_embeddings`, we\n+            recommend you to update this value accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_local_base_freq (float, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings for local attention.\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        sliding_window (`int`, *optional*, defaults to 512):\n+            This is the size of the sliding window used by local attention layers.\n+        layer_types (`Optional`, *optional*):\n+            A sequence of strings defining the attention type for that layer as either \"sliding_attention\" or\n+            \"full_attention\". If not provided, `layer_types` will de inferred from `num_hidden_layers` using a pattern\n+            of four \"sliding_attention\" layers followed one \"full_attention\". The last layer in the model should always\n+            be a \"full_attention\" layer.\n+        final_logit_softcapping (`float`, *optional*, defaults to 30.0):\n+            Scaling factor when applying tanh softcapping on the logits.\n+        altup_active_idx (`int`, *optional*, defaults to 0):\n+            The index of the prediction from which AltUp will compute additional predictions or correct\n+        altup_coef_clip (`float`, *optional*, defaults to 120.0):\n+            The maximum amplitude of an AltUp prediction or correction coeficient weight.\n+        altup_correct_scale (`bool`, *optional*, defaults to `True`):\n+            If True, apply the `AltUp.correct_output_scale` to the corrected prediction at `altup_active_idx`.\n+        altup_num_inputs (`int`, *optional*, defaults to 4):\n+            The number of predictions that AltUp should be make given the input sequence.\n+        num_kv_shared_layers (`int`, *optional*, defaults to 15):\n+            The number of layer that share KV cache values. During the forward pass, the last `num_kv_shared_layers`\n+            layers in the model \"share\" the KV values in that each local and global layer in this range uses the KV\n+            cache values computed for the last local or global layer, respectively, before entering this range. The\n+            value should be `num_kv_shared_layers` should be a scalar of `sliding_window_pattern`.\n+        laurel_rank (int, *optional*, defaults to 64):\n+            The intermediate size for the linear projections in the Learned Augmented Residual Layer.\n+        activation_sparsity_pattern (Sequence[float], *optional*, defaults to `(0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)`):\n+            The sparsity factor used to extract the top-k activations for a given layer. The provided Sequence must\n+            explicitly provide a sparsity value for each layer in the model.\n+\n+    ```python\n+    >>> from transformers import Gemma3nTextModel, Gemma3nTextConfig\n+\n+    >>> # Initializing a Gemma3nText gemma3n_text-E4B style configuration\n+    >>> configuration = Gemma3nTextConfig()\n+\n+    >>> # Initializing a model from the gemma3n_text-E4B style configuration\n+    >>> model = Gemma3nTextModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\n+    \"\"\"\n+\n+    model_type = \"gemma3n_text\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size: int = 262_400,\n+        vocab_size_per_layer_input: int = 262_144,\n+        hidden_size: int = 2048,\n+        hidden_size_per_layer_input: int = 256,\n+        intermediate_size: Union[int, Sequence[int]] = 16_384,\n+        num_hidden_layers: int = 35,\n+        num_attention_heads: int = 8,\n+        num_key_value_heads: int = 2,\n+        head_dim: int = 256,\n+        hidden_activation: str = \"gelu_pytorch_tanh\",\n+        max_position_embeddings: int = 32_768,\n+        initializer_range: float = 0.02,\n+        rms_norm_eps: float = 1e-6,\n+        use_cache: bool = True,\n+        pad_token_id: int = 0,\n+        eos_token_id: int = 1,\n+        bos_token_id: int = 2,\n+        rope_theta: float = 1_000_000.0,\n+        rope_scaling: Optional[dict[str, Any]] = None,\n+        rope_local_base_freq: float = 10_000.0,\n+        attention_bias: bool = False,\n+        attention_dropout: float = 0.0,\n+        sliding_window: int = 512,\n+        layer_types: Optional[Sequence[str]] = None,\n+        final_logit_softcapping: float = 30.0,\n+        altup_active_idx: int = 0,\n+        altup_coef_clip: float = 120.0,\n+        altup_correct_scale: bool = True,\n+        altup_num_inputs: int = 4,\n+        num_kv_shared_layers: int = 15,\n+        laurel_rank: int = 64,\n+        activation_sparsity_pattern: Optional[Union[float, Sequence[float]]] = (0.95,) * 10 + (0.0,) * 25,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            **kwargs,\n+        )\n+\n+        if isinstance(intermediate_size, Sequence) and (intsize_len := len(intermediate_size)) != num_hidden_layers:\n+            raise ValueError(\n+                \"intermediate_size must have an explicit intermediate size for every layer or one for all layers. \"\n+                f\"Expected {num_hidden_layers} values but got {intsize_len}.\"\n+            )\n+        elif not isinstance(intermediate_size, Sequence):\n+            intermediate_size = [intermediate_size] * num_hidden_layers\n+\n+        self.vocab_size = vocab_size\n+        self.vocab_size_per_layer_input = vocab_size_per_layer_input\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.head_dim = head_dim\n+        self.num_key_value_heads = num_key_value_heads\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.hidden_activation = hidden_activation\n+        self.sliding_window = sliding_window\n+        self.final_logit_softcapping = final_logit_softcapping\n+        self.layer_types = layer_types\n+\n+        self.rope_local_base_freq = rope_local_base_freq\n+        self.rope_scaling = rope_scaling\n+        rope_config_validation(self)\n+\n+        if layer_types is None:\n+            self.layer_types = [\n+                \"full_attention\" if i % 5 == 0 else \"sliding_attention\" for i in range(self.num_hidden_layers)\n+            ]\n+        else:\n+            self.layer_types = layer_types\n+\n+        layer_type_validation(self.layer_types)\n+\n+        self.hidden_size_per_layer_input = hidden_size_per_layer_input\n+        self.num_kv_shared_layers = num_kv_shared_layers\n+\n+        self.altup_active_idx = altup_active_idx\n+        self.altup_coef_clip = altup_coef_clip\n+        self.altup_correct_scale = altup_correct_scale\n+        self.altup_num_inputs = altup_num_inputs\n+\n+        self.laurel_rank = laurel_rank\n+\n+        if activation_sparsity_pattern is None:\n+            activation_sparsity_pattern = [0.0] * num_hidden_layers\n+\n+        if (len_asp := len(activation_sparsity_pattern)) != num_hidden_layers:\n+            raise ValueError(\n+                \"activation_sparsity_pattern must have an explicit activation sparsity value for every layer.\"\n+                f\"Expected {num_hidden_layers} values but got {len_asp}.\"\n+            )\n+        self.activation_sparsity_pattern = activation_sparsity_pattern\n+\n+\n+class Gemma3nAudioConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Gemma3nAudioEncoder`], based on Gogole's\n+    [Universal Speech Model](). It is used to instantiate an Gemma3nAudioEncoder model according to the specified\n+    arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar\n+    configuration to that of the Gemma 3n E4B, e.g. [google/gemma-3n-E4B](https://huggingface.co/google/gemma-3n-E4B).\n+\n+    Configuration objects that inherit from [`Gemma3nAudioConfig`] and can be used to control the model outputs. Read\n+    the documentation from [`Gemma3nAudioConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 128):\n+            Vocabulary size of the additional hard-token embeddings for audio model. These augment the embeddings\n+            included in the `Gemma3nTextModel` to provide, e.g., the end of audio and audio soft token placeholder\n+            tokens when converting `input_ids` to embeddings in the `Gemma3nForConditionalGeneration` model.\n+        vocab_offset (`int`, *optional*, defaults to 262272):\n+            Offset between the tokenizer vocab index for the token ids embedded by `Gemma3nMultimodalEmbedder` and the\n+            0-indexed `Gemma3nMultimodalEmbedder.embedding` table.\n+        input_feat_size (`int`, *optional*, defaults to 128):\n+            The number of channels in each mel-spectrogram frame.\n+        hidden_size (`int`, *optional*, defaults to 1536):\n+            Dimension of the hidden representations.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        gradient_clipping (`float`, *optional*, defaults to 10000000000.0):\n+            Clipping value used to stablize extremely large gradient values.\n+        conf_attention_chunk_size (`int`, *optional*, defaults to 12):\n+            The sub-sequence size for local attention processing inside the Conformer (\"conf\") section of the\n+            Universal Speech Model.\n+        conf_attention_context_left (`int`, *optional*, defaults to 13):\n+            The left context size of the local attention inside the Conformer (\"conf\") section of the\n+            Universal Speech Model.\n+        conf_attention_context_right (`int`, *optional*, defaults to 0):\n+            The right context size of the local attention inside the Conformer (\"conf\") section of the\n+            Universal Speech Model.\n+        conf_attention_logit_cap (`float`, *optional*, defaults to 50.0):\n+            Logit cap applied during local attention inside the Conformer (\"conf\") section of the\n+            Universal Speech Model.\n+        conf_num_attention_heads (`int`, *optional*, defaults to 8):\n+            The number of attention heads in local attention inside the Conformer (\"conf\") section of the\n+            Universal Speech Model.\n+        conf_num_hidden_layers (`int`, *optional*, defaults to 12):\n+            The number of layers that use local attention inside the Conformer (\"conf\") section of the\n+            Universal Speech Model.\n+        conf_conv_kernel_size (`int`, *optional*, defaults to 5):\n+            Convolution kernel size for the conformer block inside the Conformer (\"conf\") section of the\n+            Universal Speech Model.\n+        conf_reduction_factor (`int`, *optional*, defaults to 4):\n+            Reduction factor used in the conformer block inside the Conformer (\"conf\") section of the\n+            Universal Speech Model.\n+        conf_residual_weight (`float`, *optional*, defaults to 0.5):\n+            Residual connection weight inside the Conformer (\"conf\") section of the\n+            Universal Speech Model.\n+        sscp_conv_channel_size (`tuple(int, int)`, *optional*, defaults to `(128, 32)`):\n+            The channel sizes for the first and second convolutional layers in the Sub-sample Convolution Projection\n+            (\"sscp\") section of the Universal Speech Model.\n+        sscp_conv_group_norm_eps (`float`, *optional*, defaults to 0.001):\n+            Epsilon used in group normalization in the subsample convolution projection in the Sub-sample Convolution\n+            Projection (\"sscp\") section of the Universal Speech Model.\n+        sscp_conv_kernel_size (`tuple(tuple(int, int), tuple(int, int))`, *optional*, defaults to `((3, 3), (3, 3))`):\n+            Kernel sizes of the two convolutional layers in the subsample convolution projection  in the Sub-sample\n+            Convolution Projection (\"sscp\") section of the Universal Speech Model. The kernel sizes are specified as a\n+            tuple of height and width for each layer, where the height corresponds to the time dimension and the width\n+            corresponds to the frequency dimension.\n+        sscp_conv_stride_size (`tuple(tuple(int, int), tuple(int, int))`, *optional*, defaults to `((2, 2), (2, 2))`):\n+            Stride sizes of the two convolutional layers in the subsample convolution projection in the Sub-sample\n+            Convolution Projection (\"sscp\") section of the Universal Speech Model. The stride sizes are specified as a\n+            tuple of height and width for each layer, where the height corresponds to the time dimension and the width\n+            corresponds to the frequency dimension.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Gemma3nAudioConfig, Gemma3nAudioEncoder\n+\n+    >>> # Initializing a Gemma3nAudioEncoder gemma3n_audio-E4B-style configuration\n+    >>> configuration = Gemma3nAudioConfig()\n+\n+    >>> # Initializing a model from the gemma3n_audio-E4B style configuration\n+    >>> model = Gemma3nAudioEncoder(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\n+    \"\"\"\n+\n+    model_type = \"gemma3n_audio\"\n+\n+    def __init__(\n+        self,\n+        vocab_size: int = 128,\n+        vocab_offset: int = 262_144 + 128,  # text vocab size + vision vocab size\n+        input_feat_size: int = 128,\n+        hidden_size: int = 1536,\n+        rms_norm_eps: float = 1e-6,\n+        gradient_clipping: float = 10_000_000_000.0,\n+        conf_attention_chunk_size: int = 12,\n+        conf_attention_context_left: int = 13,\n+        conf_attention_context_right: int = 0,\n+        conf_attention_logit_cap: float = 50.0,\n+        conf_num_attention_heads: int = 8,\n+        conf_num_hidden_layers: int = 12,\n+        conf_conv_kernel_size: int = 5,\n+        conf_reduction_factor: int = 4,\n+        conf_residual_weight: float = 0.5,\n+        sscp_conv_channel_size: tuple[int, int] = (128, 32),\n+        sscp_conv_group_norm_eps: float = 1e-3,\n+        sscp_conv_kernel_size: tuple[tuple[int, int], tuple[int, int]] = (\n+            (3, 3),\n+            (3, 3),\n+        ),\n+        sscp_conv_stride_size: tuple[tuple[int, int], tuple[int, int]] = (\n+            (2, 2),\n+            (2, 2),\n+        ),\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.input_feat_size = input_feat_size\n+        self.hidden_size = hidden_size\n+        self.rms_norm_eps = rms_norm_eps\n+        self.vocab_size = vocab_size\n+        self.vocab_offset = vocab_offset\n+        self.gradient_clipping = gradient_clipping\n+        self.conf_attention_chunk_size = conf_attention_chunk_size\n+        self.conf_attention_context_left = conf_attention_context_left\n+        self.conf_attention_context_right = conf_attention_context_right\n+        self.conf_attention_logit_cap = conf_attention_logit_cap\n+        self.conf_num_attention_heads = conf_num_attention_heads\n+        self.conf_num_hidden_layers = conf_num_hidden_layers\n+        self.conf_conv_kernel_size = conf_conv_kernel_size\n+        self.conf_reduction_factor = conf_reduction_factor\n+        self.conf_residual_weight = conf_residual_weight\n+        self.sscp_conv_channel_size = sscp_conv_channel_size\n+        self.sscp_conv_group_norm_eps = sscp_conv_group_norm_eps\n+        self.sscp_conv_kernel_size = sscp_conv_kernel_size\n+        self.sscp_conv_stride_size = sscp_conv_stride_size\n+\n+\n+class Gemma3nVisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration for a timm backbone [`TimmWrapper`]. It is used to\n+    instantiate an timm model model according to the specified arguments, defining the model architecture.\n+    Instantiating a configuration with the defaults will yield a similar configuration to that of the Gemma 3n E4B\n+    vision tower, e.g. [google/gemma-3n-E4B](https://huggingface.co/google/gemma-3n-E4B).\n+\n+    Configuration objects inherit from [`Gemma3nVisionConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`Gemma3nVisionConfig`] for more information.\n+\n+    Config loads imagenet label descriptions and stores them in `id2label` attribute, `label2id` attribute for default\n+    imagenet models is set to `None` due to occlusions in the label descriptions.\n+\n+    Args:\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        do_pooling (`bool`, *optional*, defaults to `False`):\n+            Whether to do pooling for the last_hidden_state in `TimmWrapper` or not.\n+        architecture (`str`, *optional*, defaults to `\"mobilenetv5_300m_enc\"`):\n+            Determines vision architecture for TimmWrapper.\n+        hidden_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the hidden representations.\n+        vocab_size (`int`, *optional*, defaults to 128):\n+            Vocabulary size of the additional hard-token embeddings for vision model.\n+        vocab_offset (`int`, *optional*, defaults to 262144):\n+            Offset between the tokenizer vocab index for the token ids embedded by `Gemma3nMultimodalEmbedder` and the\n+            0-indexed `Gemma3nMultimodalEmbedder.embedding` table.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+\n+    Example:\n+    ```python\n+    >>> from transformers import Gemma3nVisionConfig, TimmWrapper\n+\n+    >>> # Initializing a TimmWrapper gemma3n_vision-E4B-style configuration\n+    >>> configuration = Gemma3nVisionConfig()\n+\n+    >>> # Initializing a gemma3n_vision-E4B-style TimmWrapper from the configuration\n+    >>> model = TimmWrapper(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\n+    \"\"\"\n+\n+    model_type = \"gemma3n_vision\"\n+\n+    def __init__(\n+        self,\n+        initializer_range: float = 0.02,\n+        do_pooling: bool = False,\n+        architecture: str = \"mobilenetv5_300m_enc\",\n+        hidden_size: int = 2048,\n+        vocab_size: int = 128,\n+        vocab_offset: int = 262_144,\n+        rms_norm_eps: float = 1e-06,\n+        model_args: Optional[dict] = None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.initializer_range = initializer_range\n+        self.do_pooling = do_pooling\n+        self.model_args = model_args  # named \"model_args\" for BC with timm\n+        self.architecture = architecture\n+        self.hidden_size = hidden_size\n+        self.vocab_size = vocab_size\n+        self.vocab_offset = vocab_offset\n+        self.rms_norm_eps = rms_norm_eps\n+\n+    @classmethod\n+    def from_dict(cls, config_dict: dict[str, Any], **kwargs):\n+        label_names = config_dict.get(\"label_names\", None)\n+        is_custom_model = \"num_labels\" in kwargs or \"id2label\" in kwargs\n+\n+        # if no labels added to config, use imagenet labeller in timm\n+        if label_names is None and not is_custom_model:\n+            requires_backends(cls, [\"timm\"])\n+            imagenet_subset = infer_imagenet_subset(config_dict)\n+            if imagenet_subset:\n+                dataset_info = ImageNetInfo(imagenet_subset)\n+                synsets = dataset_info.label_names()\n+                label_descriptions = dataset_info.label_descriptions(as_dict=True)\n+                label_names = [label_descriptions[synset] for synset in synsets]\n+\n+        if label_names is not None and not is_custom_model:\n+            kwargs[\"id2label\"] = dict(enumerate(label_names))\n+\n+            # if all label names are unique, create label2id mapping as well\n+            if len(set(label_names)) == len(label_names):\n+                kwargs[\"label2id\"] = {name: i for i, name in enumerate(label_names)}\n+            else:\n+                kwargs[\"label2id\"] = None\n+\n+        # timm config stores the `num_classes` attribute in both the root of config and in the \"pretrained_cfg\" dict.\n+        # We are removing these attributes in order to have the native `transformers` num_labels attribute in config\n+        # and to avoid duplicate attributes\n+        num_labels_in_kwargs = kwargs.pop(\"num_labels\", None)\n+        num_labels_in_dict = config_dict.pop(\"num_classes\", None)\n+\n+        # passed num_labels has priority over num_classes in config_dict\n+        kwargs[\"num_labels\"] = num_labels_in_kwargs or num_labels_in_dict\n+\n+        # pop num_classes from \"pretrained_cfg\",\n+        # it is not necessary to have it, only root one is used in timm\n+        if \"pretrained_cfg\" in config_dict and \"num_classes\" in config_dict[\"pretrained_cfg\"]:\n+            config_dict[\"pretrained_cfg\"].pop(\"num_classes\", None)\n+\n+        return super().from_dict(config_dict, **kwargs)\n+\n+    def to_dict(self) -> dict[str, Any]:\n+        output = super().to_dict()\n+        output[\"num_classes\"] = self.num_labels\n+        output[\"label_names\"] = list(self.id2label.values())\n+        output.pop(\"id2label\", None)\n+        output.pop(\"label2id\", None)\n+        return output\n+\n+\n+class Gemma3nConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Gemma3nForConditionalGeneration`]. It is used to\n+    instantiate a Gemma3nForConditionalGeneration according to the specified arguments, defining the model\n+    architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of\n+    Gemma3n-E4B.\n+\n+    e.g. [google/gemma-3n-E4B](https://huggingface.co/google/gemma-3n-E4B)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`Union[Gemma3nTextConfig, dict]`, *optional*):\n+            The config object of the text backbone.\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*):\n+            Custom vision config or dict.\n+        audio_config (`Union[AutoConfig, dict]`,  *optional*):\n+            Custom audio config or dict.\n+        audio_soft_tokens_per_image (`int`, *optional*, defaults to 188):\n+            The number of soft tokens per audio clip.\n+        vision_soft_tokens_per_image (`int`, *optional*, defaults to 256):\n+            The number of soft tokens per image.\n+        boi_token_id (`int`, *optional*, defaults to 255999):\n+            The begin-of-image token index to wrap the image prompt.\n+        eoi_token_id (`int`, *optional*, defaults to 262144):\n+            The end-of-image token index to wrap the image prompt.\n+        image_token_id (`int`, *optional*, defaults to 262145):\n+            The image token index to encode the image prompt.\n+        boa_token_id (`int`, *optional*, defaults to 256000):\n+            The begin-of-audio token index to wrap the audio prompt.\n+        eoa_token_id (`int`, *optional*, defaults to 262272):\n+            The end-of-audio token index to wrap the audio prompt.\n+        audio_token_id (`int`, *optional*, defaults to 262273):\n+            The audio token index to encode the audio prompt.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Gemma3nForConditionalGeneration, Gemma3nConfig, Gemma3nTextConfig\n+\n+    >>> # Initializing a MobileNet vision config, which is loaded from TIMM\n+    >>> vision_config = Gemma3nVisionConfig()\n+\n+    >>> # Initializing a Gemma3n Audio config\n+    >>> audio_config = Gemma3nAudioConfig()\n+\n+    >>> # Initializing a Gemma3n Text config\n+    >>> text_config = Gemma3nTextConfig()\n+\n+    >>> # Initializing a Gemma3n gemma-3-4b style configuration\n+    >>> configuration = Gemma3nConfig(text_config, vision_config, audio_config)\n+\n+    >>> # Initializing a model from the gemma-3-4b style configuration\n+    >>> model = Gemma3nTextConfig(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"gemma3n\"\n+    sub_configs = {\n+        \"text_config\": Gemma3nTextConfig,\n+        \"vision_config\": Gemma3nVisionConfig,\n+        \"audio_config\": Gemma3nAudioConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        text_config: Optional[Union[Gemma3nTextConfig, dict[str, Any]]] = None,\n+        vision_config: Optional[Union[Gemma3nVisionConfig, dict[str, Any]]] = None,\n+        audio_config: Optional[Union[Gemma3nAudioConfig, dict[str, Any]]] = None,\n+        audio_soft_tokens_per_image: int = 188,\n+        vision_soft_tokens_per_image: int = 256,\n+        boi_token_id: int = 255_999,\n+        eoi_token_id: int = 262_144,\n+        image_token_id: int = 262_145,\n+        boa_token_id: int = 256_000,\n+        eoa_token_id: int = 262_272,\n+        audio_token_id: int = 262_273,\n+        initializer_range: float = 0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        if isinstance(text_config, dict):\n+            text_config = Gemma3nTextConfig(**text_config)\n+        elif text_config is None:\n+            text_config = Gemma3nTextConfig()\n+            logger.info(\"text_config is None. Using default Gemma3nTextConfig.\")\n+\n+        if isinstance(vision_config, dict):\n+            vision_config = Gemma3nVisionConfig(**vision_config)\n+        elif vision_config is None:\n+            vision_config = Gemma3nVisionConfig()\n+            logger.info(\"vision_config is None. Using default Gemma3nVisionConfig.\")\n+\n+        if isinstance(audio_config, dict):\n+            audio_config = Gemma3nAudioConfig(**audio_config)\n+        elif audio_config is None:\n+            audio_config = Gemma3nAudioConfig()\n+            logger.info(\"audio_config is None. Using default Gemma3nAudioConfig.\")\n+\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.audio_config = audio_config\n+\n+        self.audio_soft_tokens_per_image = audio_soft_tokens_per_image\n+        self.vision_soft_tokens_per_image = vision_soft_tokens_per_image\n+        self.boi_token_id = boi_token_id\n+        self.eoi_token_id = eoi_token_id\n+        self.image_token_id = image_token_id\n+        self.boa_token_id = boa_token_id\n+        self.eoa_token_id = eoa_token_id\n+        self.audio_token_id = audio_token_id\n+        self.initializer_range = initializer_range\n+\n+\n+__all__ = [\"Gemma3nAudioConfig\", \"Gemma3nConfig\", \"Gemma3nTextConfig\", \"Gemma3nVisionConfig\"]"
        },
        {
            "sha": "2f25ca56d4653b45eda7c46dd2539129efbfe39a",
            "filename": "src/transformers/models/gemma3n/convert_gemma3n_weights.py",
            "status": "added",
            "additions": 807,
            "deletions": 0,
            "changes": 807,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconvert_gemma3n_weights.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconvert_gemma3n_weights.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconvert_gemma3n_weights.py?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -0,0 +1,807 @@\n+# coding=utf-8\n+# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+r\"\"\"Utility to convert Gemma models from Orbax to HF Transformers checkpoint.\n+\n+python src/transformers/models/gemma3n/convert_gemma3n_weights.py \\\n+    --variant='gemma3n_e4b' \\\n+    --tokenizer_path=\"$HOME/nano3/checkpoints/tokenizer/gemma-3n-tokenizer.model\" \\\n+    --checkpoint_path=\"$HOME/nano3/checkpoints/g251_orbax/\" \\\n+    --output_path=\"$HOME/nano3/checkpoints/g251_vision_encoder/\"\n+\"\"\"\n+\n+import json\n+import os\n+import re\n+from collections.abc import Iterable, Mapping\n+from typing import Any\n+\n+import accelerate\n+import numpy as np\n+import torch\n+import tree\n+from absl import app, flags, logging\n+from orbax import checkpoint as obc\n+\n+from transformers import (\n+    Gemma3nAudioConfig,\n+    Gemma3nAudioFeatureExtractor,\n+    Gemma3nConfig,\n+    Gemma3nForConditionalGeneration,\n+    Gemma3nProcessor,\n+    Gemma3nTextConfig,\n+    Gemma3nVisionConfig,\n+    GemmaTokenizerFast,\n+    GenerationConfig,\n+    SiglipImageProcessorFast,\n+)\n+from transformers.image_utils import PILImageResampling\n+\n+\n+# ==== Internal Constants and Classes ====\n+\n+\n+_CHAT_TEMPLATE = \"\"\"{{ bos_token }}\n+{%- if messages[0]['role'] == 'system' -%}\n+    {%- if messages[0]['content'] is string -%}\n+        {%- set first_user_prefix = messages[0]['content'] + '\\n\\n' -%}\n+    {%- else -%}\n+        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\\n\\n' -%}\n+    {%- endif -%}\n+    {%- set loop_messages = messages[1:] -%}\n+{%- else -%}\n+    {%- set first_user_prefix = \"\" -%}\n+    {%- set loop_messages = messages -%}\n+{%- endif -%}\n+{%- for message in loop_messages -%}\n+    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n+        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n+    {%- endif -%}\n+    {%- if (message['role'] == 'assistant') -%}\n+        {%- set role = \"model\" -%}\n+    {%- else -%}\n+        {%- set role = message['role'] -%}\n+    {%- endif -%}\n+    {{ '<start_of_turn>' + role + '\\n' + (first_user_prefix if loop.first else \"\") }}\n+    {%- if message['content'] is string -%}\n+        {{ message['content'] | trim }}\n+    {%- elif message['content'] is iterable -%}\n+        {%- for item in message['content'] -%}\n+            {%- if item['type'] == 'audio' -%}\n+                {{ '<audio_soft_token>' }}\n+            {%- elif item['type'] == 'image' -%}\n+                {{ '<image_soft_token>' }}\n+            {%- elif item['type'] == 'text' -%}\n+                {{ item['text'] | trim }}\n+            {%- endif -%}\n+        {%- endfor -%}\n+    {%- else -%}\n+        {{ raise_exception(\"Invalid content type\") }}\n+    {%- endif -%}\n+    {{ '<end_of_turn>\\n' }}\n+{%- endfor -%}\n+{%- if add_generation_prompt -%}\n+    {{'<start_of_turn>model\\n'}}\n+{%- endif -%}\n+\"\"\"\n+\n+_DTYPES = {\"float32\", \"bfloat16\", \"float16\"}\n+\n+_SLIDING_WINDOW_PATTERN = 5\n+\n+_AUDIO_ENCODER_PARAMETER = \"AudioEncoder/encoder\"\n+_AUDIO_ENCODER_CONFORMER = f\"{_AUDIO_ENCODER_PARAMETER}/conformer/stacked_layers\"\n+_AUDIO_ENCODER_SSCP = f\"{_AUDIO_ENCODER_PARAMETER}/feature\"\n+\n+_TRANSFORMER_PARAMETER = \"transformer\"\n+_TRANSFORMER_ALTUP_PROJ = f\"{_TRANSFORMER_PARAMETER}/altup_projection_\"\n+_TRANSFORMER_ALTUP_UNEMB = f\"{_TRANSFORMER_PARAMETER}/altup_unembed_projection_\"\n+_TRANSFORMER_DECODER_BLOCK = f\"{_TRANSFORMER_PARAMETER}/stacked_layers/attention_type_\"\n+_TRANSFORMER_DECODER_BLOCK_LEN = len(_TRANSFORMER_DECODER_BLOCK)\n+_TRANSFORMER_EMBEDDER = f\"{_TRANSFORMER_PARAMETER}/embedder\"\n+_TRANSFORMER_FINAL_NORM = \"transformer/final_norm\"\n+_TRANSFORMER_POST_TRAINING_PREFIX = \"rlx_networks/policy_network/\"\n+_TRANSFORMER_POST_TRAINING_PREFIX_LEN = len(_TRANSFORMER_POST_TRAINING_PREFIX)\n+\n+# _MOBILE_NET_CONFIG = Gemma3nVisionConfig.from_pretrained(\"\")\n+\n+_MOBILE_NET_PREFIX = \"mobilenet\"\n+_MOBILE_NET_TIMM_SUMMED_BLOCK_SIZES = [3, 8, 45, 84]\n+_MOBILE_NET_CONV = \"block_group_conv2d_\"\n+_MOBILE_NET_FIB = \"block_group_fused_ib_\"\n+_MOBILE_NET_MQA = \"block_group_mmqa_\"\n+_MOBILE_NET_MSFA = \"block_adapter_\"\n+_MOBILE_NET_UIB = \"block_group_uib_\"\n+_MOBILE_NET_UIB_HAS_DW_START = {\n+    (1, 0),\n+    (1, 1),\n+    (1, 2),\n+    (1, 3),\n+    (1, 4),\n+    (2, 0),\n+    (2, 1),\n+    (2, 2),\n+    (2, 3),\n+    (2, 4),\n+    (2, 5),\n+    (2, 6),\n+    (2, 7),\n+    (3, 0),\n+}\n+_MOBILE_NET_UIB_HAS_DW_MID = {\n+    (1, 0),\n+    (2, 0),\n+    (3, 0),\n+}\n+\n+_VARIANT_GEMMA_3_2B = \"gemma3n_e2b\"\n+_VARIANT_GEMMA_3_4B = \"gemma3n_e4b\"\n+_VARIANTS: Mapping[str, Gemma3nConfig] = {\n+    _VARIANT_GEMMA_3_2B: Gemma3nConfig(\n+        text_config=Gemma3nTextConfig(\n+            intermediate_size=2048 * 4,\n+            num_hidden_layers=30,\n+            activation_sparsity_pattern=(0.95,) * 10 + (0.0,) * 20,\n+            num_kv_shared_layers=10,\n+        ),\n+        vision_config=Gemma3nVisionConfig(),\n+        audio_config=Gemma3nAudioConfig(),\n+    ),\n+    _VARIANT_GEMMA_3_4B: Gemma3nConfig(\n+        text_config=Gemma3nTextConfig(),\n+        vision_config=Gemma3nVisionConfig(),\n+        audio_config=Gemma3nAudioConfig(),\n+    ),\n+}\n+\n+\n+# ==== Flags ====\n+\n+_AUDIO_DTYPE = flags.DEFINE_enum(\n+    name=\"audio_dtype\",\n+    default=\"bfloat16\",\n+    help=\"The floating point precision (aka dtype) of the model.\",\n+    enum_values=_DTYPES,\n+)\n+\n+_CHECKPOINT_PATH = flags.DEFINE_string(\n+    name=\"checkpoint_path\",\n+    default=None,\n+    help=\"Path to the Orbax checkpoint.\",\n+    required=True,\n+)\n+\n+_INCLUDE_CHAT_TEMPLATE = flags.DEFINE_bool(\n+    name=\"include_chat_template\", default=False, help=\"If true, will save the default chat template with the tokenizer\"\n+)\n+\n+_OUTPUT_PATH = flags.DEFINE_string(\n+    name=\"output_path\",\n+    default=None,\n+    help=\"Path to store the HF checkpoint.\",\n+    required=True,\n+)\n+\n+_TRANSFORMER_DTYPE = flags.DEFINE_enum(\n+    name=\"text_dtype\",\n+    default=\"bfloat16\",\n+    help=\"The floating point precision (aka dtype) of the model.\",\n+    enum_values=_DTYPES,\n+)\n+\n+_TOKENIZER_PATH = flags.DEFINE_string(\n+    name=\"tokenizer_path\",\n+    default=None,\n+    help=\"Path to the SentencePiece model file.\",\n+    required=True,\n+)\n+\n+_VARIANT = flags.DEFINE_enum(\n+    name=\"variant\",\n+    default=_VARIANT_GEMMA_3_4B,\n+    help=\"The model variant to convert.\",\n+    enum_values=set(_VARIANTS.keys()),\n+)\n+\n+_VERBOSE = flags.DEFINE_bool(\n+    name=\"verbose\",\n+    default=False,\n+    help=\"If true, log the path, shape, and dtype of every converted layer.\",\n+)\n+\n+_VISION_DTYPE = flags.DEFINE_enum(\n+    name=\"vision_dtype\",\n+    default=\"bfloat16\",\n+    help=\"The floating point precision (aka dtype) of the model.\",\n+    enum_values=_DTYPES,\n+)\n+\n+\n+def convert_audio_encoder_weights(\n+    config: Gemma3nAudioConfig,\n+    path: str,\n+    param: str,\n+    weights: np.ndarray,\n+) -> Iterable[tuple[str, np.ndarray]]:\n+    converted_paths: list[str] = []\n+    converted_weights: list[Any] = []\n+\n+    if path.startswith(_AUDIO_ENCODER_CONFORMER):\n+        assert weights.shape[0] == config.conf_num_hidden_layers\n+\n+        for i, matrix in enumerate(weights):\n+            if \"fflayer_end\" in path:\n+                base = f\"conformer.{i}.ffw_layer_end\"\n+\n+                if path.endswith(\"ffn_layer1\"):\n+                    converted_paths.append(f\"{base}.ffw_layer_1.weight\")\n+                    converted_weights.append(matrix.transpose())\n+                elif path.endswith(\"ffn_layer2\"):\n+                    converted_paths.append(f\"{base}.ffw_layer_2.weight\")\n+                    converted_weights.append(matrix.transpose())\n+                elif path.endswith(\"post_layer_norm\"):\n+                    converted_paths.append(f\"{base}.post_layer_norm.weight\")\n+                    converted_weights.append(matrix)\n+                elif path.endswith(\"pre_layer_norm\"):\n+                    converted_paths.append(f\"{base}.pre_layer_norm.weight\")\n+                    converted_weights.append(matrix)\n+            elif \"fflayer_start\" in path:\n+                base = f\"conformer.{i}.ffw_layer_start\"\n+\n+                if path.endswith(\"ffn_layer1\"):\n+                    converted_paths.append(f\"{base}.ffw_layer_1.weight\")\n+                    converted_weights.append(matrix.transpose())\n+                elif path.endswith(\"ffn_layer2\"):\n+                    converted_paths.append(f\"{base}.ffw_layer_2.weight\")\n+                    converted_weights.append(matrix.transpose())\n+                elif path.endswith(\"post_layer_norm\"):\n+                    converted_paths.append(f\"{base}.post_layer_norm.weight\")\n+                    converted_weights.append(matrix)\n+                elif path.endswith(\"pre_layer_norm\"):\n+                    converted_paths.append(f\"{base}.pre_layer_norm.weight\")\n+                    converted_weights.append(matrix)\n+            elif path.endswith(\"final_ln\"):\n+                converted_paths.append(f\"conformer.{i}.norm.weight\")\n+                converted_weights.append(matrix)\n+            elif \"lconv\" in path:\n+                base = f\"conformer.{i}.lconv1d\"\n+\n+                if path.endswith(\"conv_norm\"):\n+                    converted_paths.append(f\"{base}.conv_norm.weight\")\n+                    converted_weights.append(matrix)\n+                elif path.endswith(\"depthwise_conv1d\"):\n+                    converted_paths.append(f\"{base}.depthwise_conv1d.weight\")\n+                    converted_weights.append(matrix.transpose())\n+                elif path.endswith(\"linear_end\"):\n+                    converted_paths.append(f\"{base}.linear_end.weight\")\n+                    converted_weights.append(matrix.transpose())\n+                elif path.endswith(\"linear_start\"):\n+                    converted_paths.append(f\"{base}.linear_start.weight\")\n+                    converted_weights.append(matrix.transpose())\n+                elif path.endswith(\"ln\"):\n+                    converted_paths.append(f\"{base}.pre_layer_norm.weight\")\n+                    converted_weights.append(matrix)\n+            elif \"trans_atten\" in path:\n+                base = f\"conformer.{i}.attention\"\n+\n+                if param == \"per_dim_scale\":\n+                    converted_paths.append(f\"{base}.attn.per_dim_scale\")\n+                    converted_weights.append(matrix)\n+\n+                if path.endswith(\"query_key_value_projection\"):\n+                    converted_paths.extend(\n+                        [f\"{base}.attn.q_proj.weight\", f\"{base}.attn.k_proj.weight\", f\"{base}.attn.v_proj.weight\"]\n+                    )\n+                    converted_weights.extend(\n+                        [\n+                            m.reshape(config.hidden_size, config.hidden_size).transpose()\n+                            for m in matrix.transpose(1, 0, 2, 3)\n+                        ]\n+                    )\n+                elif path.endswith(\"pos_proj\"):\n+                    converted_paths.append(f\"{base}.attn.relative_position_embedding.pos_proj.weight\")\n+                    converted_weights.append(matrix.reshape(config.hidden_size, config.hidden_size).transpose())\n+                elif path.endswith(\"post\"):\n+                    converted_paths.append(f\"{base}.post.weight\")\n+                    converted_weights.append(matrix.transpose(2, 0, 1).reshape(config.hidden_size, config.hidden_size))\n+                elif path.endswith(\"post_norm\"):\n+                    converted_paths.append(f\"{base}.post_norm.weight\")\n+                    converted_weights.append(matrix)\n+                elif path.endswith(\"pre_norm\"):\n+                    converted_paths.append(f\"{base}.pre_attn_norm.weight\")\n+                    converted_weights.append(matrix)\n+    elif path.startswith(_AUDIO_ENCODER_SSCP):\n+        if path.endswith(\"input_proj\"):\n+            converted_paths.append(\"subsample_conv_projection.input_proj_linear.weight\")\n+            converted_weights.append(\n+                weights.transpose(2, 0, 1).reshape(config.hidden_size, config.sscp_conv_channel_size[1] ** 2)\n+            )\n+        elif \"norm_\" in path:\n+            index = int(path[-1])\n+            converted_paths.append(f\"subsample_conv_projection.conv_{index}.norm.weight\")\n+            converted_weights.append(weights)\n+        elif \"subsampling_\" in path:\n+            index = int(path[-1])\n+            converted_paths.append(f\"subsample_conv_projection.conv_{index}.conv.weight\")\n+            converted_weights.append(weights.transpose(3, 2, 0, 1))\n+\n+    if (cpl := len(converted_paths)) != (cwl := len(converted_weights)):\n+        raise ValueError(\n+            \"The `converted_paths` and `converted_weights` should be the same \"\n+            f\"length. Got {cpl} and {cwl}, respectively, for {path}.\"\n+        )\n+\n+    return zip(converted_paths, converted_weights)\n+\n+\n+def convert_transformer_weights(\n+    config: Gemma3nTextConfig,\n+    path: str,\n+    param: str,\n+    weights: np.ndarray,\n+) -> Iterable[tuple[str, np.ndarray]]:\n+    if path.startswith(_TRANSFORMER_POST_TRAINING_PREFIX):\n+        path = path[_TRANSFORMER_POST_TRAINING_PREFIX_LEN:]\n+\n+    converted_paths: list[str] = []\n+    converted_weights: list[Any] = []\n+\n+    if path.startswith(_TRANSFORMER_ALTUP_PROJ):\n+        index = int(path[-1])\n+        converted_paths.append(f\"altup_projections.{index}.weight\")\n+        converted_weights.append(weights.transpose())\n+    elif path.startswith(_TRANSFORMER_ALTUP_UNEMB):\n+        index = int(path[-1])\n+        converted_paths.append(f\"altup_unembed_projections.{index}.weight\")\n+        converted_weights.append(weights.transpose())\n+    elif path.startswith(_TRANSFORMER_DECODER_BLOCK):\n+        attention_type_index = int(path[_TRANSFORMER_DECODER_BLOCK_LEN])\n+        assert weights.shape[0] == config.num_hidden_layers / _SLIDING_WINDOW_PATTERN\n+\n+        for i, matrix in enumerate(weights):\n+            layer_idx = _SLIDING_WINDOW_PATTERN * i + attention_type_index\n+            base_path = f\"layers.{layer_idx}\"\n+\n+            if \"altup\" in path:\n+                altup_path = f\"{base_path}.altup\"\n+\n+                if param == \"correct_output_scale\":\n+                    converted_paths.append(f\"{altup_path}.correct_output_scale\")\n+                    converted_weights.append(matrix)\n+                elif param == \"correction_coefs\":\n+                    converted_paths.append(f\"{altup_path}.correction_coefs.weight\")\n+                    converted_weights.append(matrix.transpose())\n+                elif param == \"prediction_coefs\":\n+                    converted_paths.append(f\"{altup_path}.prediction_coefs.weight\")\n+                    converted_weights.append(\n+                        np.clip(\n+                            matrix.reshape(config.altup_num_inputs, config.altup_num_inputs**2).transpose(),\n+                            -config.altup_coef_clip,\n+                            config.altup_coef_clip,\n+                        )\n+                    )\n+\n+                if path.endswith(\"modality_router\"):\n+                    converted_paths.append(f\"{altup_path}.modality_router.weight\")\n+                    converted_weights.append(matrix.transpose())\n+                elif path.endswith(\"router_norm_layer\"):\n+                    converted_paths.append(f\"{altup_path}.router_norm.weight\")\n+                    converted_weights.append(matrix)\n+            elif path.endswith(\"attn/attn_vec_einsum\"):\n+                converted_paths.append(f\"{base_path}.self_attn.o_proj.weight\")\n+                converted_weights.append(\n+                    matrix.transpose(2, 0, 1).reshape(config.hidden_size, config.num_attention_heads * config.head_dim)\n+                )\n+            elif path.endswith(\"attn/kv_einsum\"):\n+                converted_paths.extend(\n+                    [\n+                        f\"{base_path}.self_attn.k_proj.weight\",\n+                        f\"{base_path}.self_attn.v_proj.weight\",\n+                    ]\n+                )\n+                k_proj_weights, v_proj_weights = matrix.transpose(0, 2, 1, 3)\n+                kv_proj_shape = (config.hidden_size, config.num_key_value_heads * config.head_dim)\n+                converted_weights.extend(\n+                    [\n+                        k_proj_weights.reshape(kv_proj_shape).transpose(),\n+                        v_proj_weights.reshape(kv_proj_shape).transpose(),\n+                    ]\n+                )\n+            elif path.endswith(\"attn/q_einsum\"):\n+                converted_paths.append(f\"{base_path}.self_attn.q_proj.weight\")\n+                converted_weights.append(\n+                    matrix.transpose(1, 0, 2)\n+                    .reshape(config.hidden_size, config.num_attention_heads * config.head_dim)\n+                    .transpose()\n+                )\n+            elif path.endswith(\"attn/query_norm\"):\n+                converted_paths.append(f\"{base_path}.self_attn.q_norm.weight\")\n+                converted_weights.append(matrix)\n+            elif path.endswith(\"attn/key_norm\"):\n+                converted_paths.append(f\"{base_path}.self_attn.k_norm.weight\")\n+                converted_weights.append(matrix)\n+            elif path.endswith(\"laurel_block/linear_left\"):\n+                converted_paths.append(f\"{base_path}.laurel.linear_left.weight\")\n+                converted_weights.append(matrix.transpose())\n+            elif path.endswith(\"laurel_block/linear_right\"):\n+                converted_paths.append(f\"{base_path}.laurel.linear_right.weight\")\n+                converted_weights.append(matrix.transpose())\n+            elif path.endswith(\"mlp/gating_einsum\"):\n+                converted_paths.extend([f\"{base_path}.mlp.gate_proj.weight\", f\"{base_path}.mlp.up_proj.weight\"])\n+                gate_proj_weight, up_proj_weight = matrix\n+                converted_weights.extend([gate_proj_weight, up_proj_weight])\n+            elif path.endswith(\"mlp/linear\"):\n+                converted_paths.append(f\"{base_path}.mlp.down_proj.weight\")\n+                converted_weights.append(matrix.transpose())\n+            elif path.endswith(\"per_layer_input_gate\"):\n+                converted_paths.append(f\"{base_path}.per_layer_input_gate.weight\")\n+                converted_weights.append(matrix.transpose())\n+            elif path.endswith(\"per_layer_projection\"):\n+                converted_paths.append(f\"{base_path}.per_layer_projection.weight\")\n+                converted_weights.append(matrix.transpose())\n+            elif path.endswith(\"post_attention_norm\"):\n+                converted_paths.append(f\"{base_path}.post_attention_layernorm.weight\")\n+                converted_weights.append(matrix)\n+            elif path.endswith(\"post_ffw_norm\"):\n+                converted_paths.append(f\"{base_path}.post_feedforward_layernorm.weight\")\n+                converted_weights.append(matrix)\n+            elif path.endswith(\"post_laurel_norm\"):\n+                converted_paths.append(f\"{base_path}.laurel.post_laurel_norm.weight\")\n+                converted_weights.append(matrix)\n+            elif path.endswith(\"post_per_layer_input_norm\"):\n+                converted_paths.append(f\"{base_path}.post_per_layer_input_norm.weight\")\n+                converted_weights.append(matrix)\n+            elif path.endswith(\"pre_attention_norm\"):\n+                converted_paths.append(f\"{base_path}.input_layernorm.weight\")\n+                converted_weights.append(matrix)\n+            elif path.endswith(\"pre_ffw_norm\"):\n+                converted_paths.append(f\"{base_path}.pre_feedforward_layernorm.weight\")\n+                converted_weights.append(matrix)\n+    elif path == _TRANSFORMER_EMBEDDER:\n+        if param == \"input_embedding\":\n+            converted_paths.append(\"embed_tokens.weight\")\n+            # Gemma 3n model doesn't have soft tokens or \"end of\" tokens for images and audio in its input and output\n+            # embeddings, so we resize to avoid bugs observed with Mllama\n+            pre_expansion_embeddings = weights\n+            pad_token_slice = slice(config.pad_token_id, config.pad_token_id + 1)\n+            new_embeddings = np.repeat(pre_expansion_embeddings[pad_token_slice], 256, axis=0)\n+            weights = np.vstack([pre_expansion_embeddings, new_embeddings])\n+            converted_weights.append(weights)\n+        elif param == \"per_layer_embeddings\":\n+            converted_paths.append(\"embed_tokens_per_layer.weight\")\n+            converted_weights.append(\n+                weights.reshape(\n+                    config.vocab_size_per_layer_input, config.num_hidden_layers * config.hidden_size_per_layer_input\n+                )\n+            )\n+    elif path.startswith(_TRANSFORMER_EMBEDDER):\n+        # TODO: ryanmullins - support multimodal norms and projections\n+        if path.endswith(\"per_layer_model_projection\"):\n+            converted_paths.append(\"per_layer_model_projection.weight\")\n+            converted_weights.append(\n+                weights.reshape(\n+                    config.hidden_size, config.num_hidden_layers * config.hidden_size_per_layer_input\n+                ).transpose()\n+            )\n+        elif path.endswith(\"per_layer_projection_norm\"):\n+            converted_paths.append(\"per_layer_projection_norm.weight\")\n+            converted_weights.append(weights)\n+    elif path == _TRANSFORMER_FINAL_NORM:\n+        converted_paths = [\"norm.weight\"]\n+        converted_weights = [weights]\n+\n+    if (cpl := len(converted_paths)) != (cwl := len(converted_weights)):\n+        raise ValueError(\n+            \"The `converted_paths` and `converted_weights` should be the same \"\n+            f\"length. Got {cpl} and {cwl}, respectively, for {path}.\"\n+        )\n+\n+    return zip(converted_paths, converted_weights)\n+\n+\n+def convert_vision_weights(\n+    config: Gemma3nVisionConfig,\n+    path: str,\n+    param: str,\n+    weights: np.ndarray,\n+) -> Iterable[tuple[str, np.ndarray]]:\n+    def generate_base_path(path: str, block_type: str) -> tuple[str, tuple[int, int]]:\n+        re_str = r\"{}(\\d+)/\".format(block_type)\n+        re_pattern = re.compile(re_str)\n+        match = re.search(re_pattern, path).group(1)\n+        idx = abs(int(match)) - 1\n+\n+        for block_idx, v in enumerate(_MOBILE_NET_TIMM_SUMMED_BLOCK_SIZES):\n+            if v > idx:\n+                offset = _MOBILE_NET_TIMM_SUMMED_BLOCK_SIZES[block_idx - 1] if block_idx > 0 else 0\n+                layer_idx = idx - offset\n+                return f\"blocks.{block_idx}.{layer_idx}\", (block_idx, layer_idx)\n+\n+        raise ValueError(f\"could not extract a base path from {path}\")\n+\n+    if _MOBILE_NET_MSFA in path:\n+        converted_path = \"msfa\"\n+\n+        if \"ffn/Normalize_0\" in path:\n+            converted_path += \".ffn.pw_exp.bn.weight\"\n+            converted_weight = weights\n+        elif \"ffn/Normalize_1\" in path:\n+            converted_path += \".ffn.pw_proj.bn.weight\"\n+            converted_weight = weights\n+        elif \"ffn/expand\" in path:\n+            converted_path += \".ffn.pw_exp.conv.weight\"\n+            converted_weight = weights.transpose()[:, :, None, None]\n+        elif \"ffn/project\" in path:\n+            converted_path += \".ffn.pw_proj.conv.weight\"\n+            converted_weight = weights.transpose()[:, :, None, None]\n+        elif \"Normalize_0\" in path:\n+            converted_path += \".norm.weight\"\n+            converted_weight = weights\n+    elif _MOBILE_NET_CONV in path:\n+        if \"Conv_0\" in path:\n+            converted_path = \"conv_stem.conv.weight\"\n+            converted_weight = weights.transpose(3, 2, 1, 0)\n+        elif \"Normalize_0\" in path:\n+            converted_path = \"conv_stem.bn.weight\"\n+            converted_weight = weights\n+    elif _MOBILE_NET_FIB in path:\n+        converted_path, _ = generate_base_path(path, _MOBILE_NET_FIB)\n+        if \"Normalize_0\" in path:\n+            converted_path += \".bn1.weight\"\n+            converted_weight = weights\n+        elif \"Normalize_1\" in path:\n+            converted_path += \".bn2.weight\"\n+            converted_weight = weights\n+        elif \"expand_conv\" in path:\n+            converted_path += \".conv_exp.weight\"\n+            converted_weight = weights.transpose(3, 2, 1, 0)\n+        else:\n+            converted_path += \".conv_pwl.weight\"\n+            converted_weight = weights.transpose()[:, :, None, None]\n+    elif _MOBILE_NET_MQA in path:\n+        converted_path, _ = generate_base_path(path, _MOBILE_NET_MQA)\n+\n+        if \"LayerScale_0\" in path:\n+            converted_path += \".layer_scale.gamma\"\n+            converted_weight = weights\n+        elif \"Normalize_0\" in path:\n+            converted_path += \".norm.weight\"\n+            converted_weight = weights\n+        elif \"Normalize_1\" in path:\n+            converted_path += \".attn.key.norm.weight\"\n+            converted_weight = weights\n+        elif \"Normalize_2\" in path:\n+            converted_path += \".attn.value.norm.weight\"\n+            converted_weight = weights\n+        elif \"key_dwconv\" in path:\n+            converted_path += \".attn.key.down_conv.weight\"\n+            converted_weight = weights.transpose()\n+        elif \"key_proj\" in path:\n+            converted_path += \".attn.key.proj.weight\"\n+            converted_weight = weights.transpose()[:, :, None, None]\n+        elif \"output_proj\" in path:\n+            converted_path += \".attn.output.proj.weight\"\n+            converted_weight = weights.transpose()[:, :, None, None]\n+        elif \"query_proj\" in path:\n+            converted_path += \".attn.query.proj.weight\"\n+            converted_weight = weights.transpose()[:, :, None, None]\n+        elif \"value_dwconv\" in path:\n+            converted_path += \".attn.value.down_conv.weight\"\n+            converted_weight = weights.transpose()\n+        elif \"value_proj\" in path:\n+            converted_path += \".attn.value.proj.weight\"\n+            converted_weight = weights.transpose()[:, :, None, None]\n+    elif _MOBILE_NET_UIB in path:\n+        converted_path, idx_key = generate_base_path(path, _MOBILE_NET_UIB)\n+\n+        has_dw_start = idx_key in _MOBILE_NET_UIB_HAS_DW_START\n+        has_dw_mid = idx_key in _MOBILE_NET_UIB_HAS_DW_MID\n+\n+        if \"LayerScale_0\" in path:\n+            converted_path += \".layer_scale.gamma\"\n+            converted_weight = weights\n+        elif \"Normalize_0\" in path:\n+            converted_path += \".dw_start.bn.weight\" if has_dw_start else \".pw_exp.bn.weight\"\n+            converted_weight = weights\n+        elif \"Normalize_1\" in path:\n+            converted_path += \".pw_exp.bn.weight\" if has_dw_start else \".pw_proj.bn.weight\"\n+            converted_weight = weights\n+        elif \"Normalize_2\" in path:\n+            converted_path += \".dw_mid.bn.weight\" if has_dw_mid else \".pw_proj.bn.weight\"\n+            converted_weight = weights\n+        elif \"Normalize_3\" in path:\n+            converted_path += \".pw_proj.bn.weight\"\n+            converted_weight = weights\n+        elif \"expand\" in path:\n+            converted_path += \".pw_exp.conv.weight\"\n+            converted_weight = weights.transpose()[:, :, None, None]\n+        elif \"middle_dwconv\" in path:\n+            converted_path += \".dw_mid.conv.weight\"\n+            converted_weight = weights.transpose(3, 2, 1, 0)\n+        elif \"project\" in path:\n+            converted_path += \".pw_proj.conv.weight\"\n+            converted_weight = weights.transpose()[:, :, None, None]\n+        elif \"start_dwconv\" in path:\n+            converted_path += \".dw_start.conv.weight\"\n+            converted_weight = weights.transpose(3, 2, 1, 0)\n+\n+    return [(converted_path, converted_weight)]\n+\n+\n+def convert(checkpoint_path: str, config: Gemma3nConfig) -> dict[str, torch.Tensor]:\n+    \"\"\"Loads Orbax checkpoint from `input_path` and converts it to HF tree.\"\"\"\n+    checkpointer = obc.PyTreeCheckpointer()\n+    ckpt = checkpointer.restore(checkpoint_path)\n+    hf_tree: dict[str, torch.Tensor] = {}\n+\n+    def update_tree(path: str, weights: np.ndarray, target_dtype: torch.dtype) -> None:\n+        hf_tree[path] = torch.from_numpy(weights.astype(\"float32\")).type(target_dtype)\n+        if _VERBOSE.value:\n+            logging.info(\n+                \"%s converted shape=%s with dtype=%s\",\n+                path,\n+                weights.shape,\n+                target_dtype,\n+            )\n+\n+    for (path, param), value in tree.flatten_with_path(ckpt):\n+        if param == \"audio_input_embedding_extra\":\n+            update_tree(\"model.embed_audio.embedding.weight\", value, config.audio_config.torch_dtype)\n+        elif path.endswith(\"audio_embedding_norm\"):\n+            update_tree(\"model.embed_audio.hard_embedding_norm.weight\", value, config.audio_config.torch_dtype)\n+        elif path.endswith(\"audio_input_projection\"):\n+            update_tree(\n+                \"model.embed_audio.embedding_projection.weight\", value.transpose(), config.audio_config.torch_dtype\n+            )\n+        elif path.endswith(\"audio_soft_embedding_norm\"):\n+            update_tree(\"model.embed_audio.soft_embedding_norm.weight\", value, config.audio_config.torch_dtype)\n+        elif param == \"mm_input_embedding_extra\":\n+            update_tree(\"model.embed_vision.embedding.weight\", value, config.vision_config.torch_dtype)\n+        elif path.endswith(\"mm_hard_embedding_norm\"):\n+            update_tree(\"model.embed_vision.hard_embedding_norm.weight\", value, config.vision_config.torch_dtype)\n+        elif path.endswith(\"mm_input_projection\"):\n+            update_tree(\n+                \"model.embed_vision.embedding_projection.weight\", value.transpose(), config.vision_config.torch_dtype\n+            )\n+        elif path.endswith(\"mm_soft_embedding_norm\"):\n+            update_tree(\"model.embed_vision.soft_embedding_norm.weight\", value, config.vision_config.torch_dtype)\n+        elif path.startswith(_TRANSFORMER_PARAMETER):\n+            for path, weights in convert_transformer_weights(config.text_config, path, param, value):\n+                update_tree(f\"model.language_model.{path}\", weights, config.text_config.torch_dtype)\n+        elif _MOBILE_NET_PREFIX in path:\n+            mobilenet_prefix_idx = path.index(_MOBILE_NET_PREFIX)\n+            path = path[mobilenet_prefix_idx:]\n+            for path, weights in convert_vision_weights(config.vision_config, path, param, value):\n+                update_tree(f\"model.vision_tower.timm_model.{path}\", weights, config.vision_config.torch_dtype)\n+        elif path.startswith(_AUDIO_ENCODER_PARAMETER):\n+            for path, weights in convert_audio_encoder_weights(config.audio_config, path, param, value):\n+                update_tree(f\"model.audio_tower.{path}\", weights, config.audio_config.torch_dtype)\n+\n+    hf_tree[\"lm_head.weight\"] = hf_tree[\"model.language_model.embed_tokens.weight\"]\n+\n+    return hf_tree\n+\n+\n+def main(*args):\n+    del args\n+\n+    output_path = _OUTPUT_PATH.value\n+    variant = _VARIANT.value\n+\n+    config = _VARIANTS[variant]\n+    config.audio_config.torch_dtype = getattr(torch, _AUDIO_DTYPE.value)\n+    config.text_config.torch_dtype = getattr(torch, _TRANSFORMER_DTYPE.value)\n+    config.vision_config.torch_dtype = getattr(torch, _VISION_DTYPE.value)\n+    if _INCLUDE_CHAT_TEMPLATE.value:\n+        # Chat template is included for instruction tuned models, which treat\n+        # both \"<eos>\" and \"<end_of_turn>\" as generation stoppers.\n+        config.eos_token_id = [1, 106]\n+\n+    logging.info(\n+        \"Converting Gemma 3 (%s) @ %s (language) and %s (vision)\",\n+        variant,\n+        _TRANSFORMER_DTYPE.value,\n+        _VISION_DTYPE.value,\n+    )\n+    state_tree = convert(_CHECKPOINT_PATH.value, config)\n+    logging.info(\"Converted Gemma 3 (%s) state tree from Orbax to Hugging Face.\", variant)\n+\n+    with accelerate.init_empty_weights():\n+        model = Gemma3nForConditionalGeneration(config=config)\n+\n+    model.load_state_dict(state_tree, assign=True, strict=True)\n+    logging.info(\n+        \"Loaded Gemma 3 (%s) in Hugging Face Transformers as a %s instance.\",\n+        variant,\n+        type(model).__name__,\n+    )\n+    model.save_pretrained(output_path, state_dict=state_tree, safe_serialization=True)\n+    logging.info(\n+        \"Saved Gemma 3 (%s) to SafeTensors in %s using %s\",\n+        variant,\n+        output_path,\n+        type(model).__name__,\n+    )\n+    del model\n+    del state_tree\n+\n+    chat_template_kwargs = {\"chat_template\": _CHAT_TEMPLATE} if _INCLUDE_CHAT_TEMPLATE.value else {}\n+\n+    tokenizer = GemmaTokenizerFast(\n+        _TOKENIZER_PATH.value,\n+        add_bos_token=True,\n+        extra_special_tokens={\n+            \"image_token\": \"<image_soft_token>\",  # Should be ID=262_145\n+            \"boi_token\": \"<start_of_image>\",  # Should be ID=255_999\n+            \"eoi_token\": \"<end_of_image>\",  # Should be ID=262_144\n+            \"audio_token\": \"<audio_soft_token>\",  # Should be ID=262_273\n+            \"boa_token\": \"<start_of_audio>\",  # Should be ID=256_000\n+            \"eoa_token\": \"<end_of_audio>\",  # Should be ID=262_272\n+        },\n+        **chat_template_kwargs,\n+    )\n+    tokenizer.save_pretrained(output_path)\n+    logging.info(\"Saved GemmaTokenizer for %s to %s\", variant, output_path)\n+\n+    feature_extractor = Gemma3nAudioFeatureExtractor()\n+    image_processor = SiglipImageProcessorFast(\n+        image_seq_length=256,\n+        image_mean=(0.5,) * 3,\n+        image_std=(0.5,) * 3,\n+        size={\"height\": 768, \"width\": 768},\n+        resample=PILImageResampling.BILINEAR,\n+        do_normalize=False,\n+    )\n+    processor = Gemma3nProcessor(\n+        feature_extractor=feature_extractor,\n+        image_processor=image_processor,\n+        tokenizer=tokenizer,\n+        **chat_template_kwargs,\n+    )\n+    processor.save_pretrained(output_path)\n+\n+    logging.info(\"Saved Gemma3nProcessor for %s to %s\", variant, output_path)\n+\n+    # NOTE: feature_extractor and image_processor both use the same filename, preprocessor_config.json, when saved to\n+    # disk, but the files are overwritten by processor.save_pretrained(). However, the configs can be unioned, saved,\n+    # and loaded from the same preprocessor_config.json file, so we do that explicitly here.\n+    feature_extractor_config = json.loads(feature_extractor.to_json_string())\n+    image_processor_config = json.loads(image_processor.to_json_string())\n+    preprocessor_config = {**feature_extractor_config, **image_processor_config}\n+    with open(os.path.join(output_path, \"preprocessor_config.json\"), \"w\", encoding=\"utf-8\") as writer:\n+        writer.write(json.dumps(preprocessor_config, indent=2, sort_keys=True) + \"\\n\")\n+\n+    logging.info(\"Saved joint preprocessor_config.json for %s to %s\", variant, output_path)\n+\n+    del feature_extractor, image_processor, processor, tokenizer\n+\n+    generation_config = GenerationConfig(\n+        pad_token_id=config.text_config.pad_token_id,\n+        bos_token_id=config.text_config.bos_token_id,\n+        eos_token_id=(\n+            [config.text_config.eos_token_id, 106] if _INCLUDE_CHAT_TEMPLATE.value else config.text_config.eos_token_id\n+        ),\n+        cache_implementation=\"hybrid\",\n+        temperature=1.0,\n+        do_sample=True,\n+        top_k=64,\n+        top_p=0.95,\n+    )\n+    generation_config.save_pretrained(output_path)\n+\n+\n+if __name__ == \"__main__\":\n+    app.run(main)"
        },
        {
            "sha": "63598926af2b5f614a1697fee8af2170db029559",
            "filename": "src/transformers/models/gemma3n/feature_extraction_gemma3n.py",
            "status": "added",
            "additions": 338,
            "deletions": 0,
            "changes": 338,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Ffeature_extraction_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Ffeature_extraction_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Ffeature_extraction_gemma3n.py?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -0,0 +1,338 @@\n+# coding=utf-8\n+# Copyright 2025 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from collections.abc import Sequence\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...feature_extraction_sequence_utils import SequenceFeatureExtractor\n+from ...feature_extraction_utils import BatchFeature\n+from ...utils import PaddingStrategy, TensorType, logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def create_fb_matrix(\n+    n_freqs: int,\n+    f_min: float,\n+    f_max: float,\n+    n_mels: int,\n+    sample_rate: int,\n+    fft_length: int,\n+    norm: Optional[str] = None,\n+) -> np.ndarray:\n+    r\"\"\"Create a frequency bin conversion matrix (NumPy version).\n+\n+    Args:\n+        n_freqs (int): Number of frequencies to highlight/apply\n+        f_min (float): Minimum frequency (Hz)\n+        f_max (float): Maximum frequency (Hz)\n+        n_mels (int): Number of mel filterbanks\n+        sample_rate (int): Sample rate of the audio waveform\n+        fft_length (int): FFT length\n+        norm (Optional[str]): If 'slaney', divide the triangular mel weights by\n+          the width of the mel band (area normalization). (Default: ``None``)\n+\n+    Returns:\n+        np.ndarray: Triangular filter banks (fb matrix) of size (``n_freqs``,\n+        ``n_mels``)\n+        meaning number of frequencies to highlight/apply to x the number of\n+        filterbanks.\n+        Each column is a filterbank so that assuming there is a matrix A of\n+        size (..., ``n_freqs``), the applied result would be\n+        ``A @ create_fb_matrix_numpy(A.shape[-1], ...)``.\n+    \"\"\"\n+\n+    if norm is not None and norm != \"slaney\":\n+        raise ValueError(\"norm must be one of None or 'slaney'\")\n+\n+    # freq bins\n+    all_freqs = np.arange(n_freqs, dtype=np.float32) * (sample_rate / fft_length)\n+\n+    # calculate mel freq bins\n+    # hertz to mel(f) is 2595. * math.log10(1. + (f / 700.))\n+    m_min = 2595.0 * math.log10(1.0 + (f_min / 700.0))\n+    m_max = 2595.0 * math.log10(1.0 + (f_max / 700.0))\n+    m_pts = np.linspace(m_min, m_max, n_mels + 2)\n+    # mel to hertz(mel) is 700. * (10**(mel / 2595.) - 1.)\n+    f_pts = 700.0 * (10 ** (m_pts / 2595.0) - 1.0)\n+    # calculate difference between each mel point and each stft freq point in Hz\n+    f_diff = f_pts[1:] - f_pts[:-1]  # (n_mels + 1)\n+    slopes = np.expand_dims(f_pts, 0) - np.expand_dims(all_freqs, 1)  # (n_freqs, n_mels + 2)\n+    # create overlapping triangles\n+    zero = np.zeros(1, dtype=np.float32)\n+    down_slopes = (-1.0 * slopes[:, :-2]) / f_diff[:-1]  # (n_freqs, n_mels)\n+    up_slopes = slopes[:, 2:] / f_diff[1:]  # (n_freqs, n_mels)\n+    fb = np.maximum(zero, np.minimum(down_slopes, up_slopes))\n+\n+    if norm is not None and norm == \"slaney\":\n+        # Slaney-style mel is scaled to be approx constant energy per channel\n+        enorm = 2.0 / (f_pts[2 : n_mels + 2] - f_pts[:n_mels])\n+        fb *= np.expand_dims(enorm, 0)\n+\n+    return fb\n+\n+\n+def _unfold(array: np.ndarray, dimension: int, size: int, step: int) -> np.ndarray:\n+    \"\"\"A basic NumPy equivalent of PyTorch's unfold for 2D arrays along the last dim.\"\"\"\n+    if array.ndim != 2:\n+        raise ValueError(\"This unfold implementation currently supports 2D arrays (batch, time).\")\n+    if dimension != -1 and dimension != array.ndim - 1:\n+        raise ValueError(\"This unfold implementation only supports unfolding the last dimension.\")\n+\n+    batch_size, original_length = array.shape\n+    num_frames = (original_length - size) // step + 1\n+\n+    if num_frames <= 0:\n+        return np.zeros((batch_size, 0, size), dtype=array.dtype)\n+\n+    output_shape = (batch_size, num_frames, size)\n+    output_strides = (array.strides[0], array.strides[1] * step, array.strides[1])\n+\n+    return np.lib.stride_tricks.as_strided(array, shape=output_shape, strides=output_strides)\n+\n+\n+class Gemma3nAudioFeatureExtractor(SequenceFeatureExtractor):\n+    \"\"\"An audio feature extractor Universal Speech Models https://arxiv.org/abs/2303.01037.\n+\n+    Args:\n+        feature_size (`int`, *optional*, defaults to 128):\n+            The feature dimension of the extracted features.\n+        sampling_rate (`int`, *optional*, defaults to 16000):\n+            The sampling rate at which the audio files should be digitalized expressed in hertz (Hz).\n+        padding_value (`float`, *optional*, defaults to 0.0):\n+            Padding value used to pad the audio. Should correspond to silences.\n+        return_attention_mask (`bool`, *optional*, defaults to `True`):\n+            Whether to return the attention mask for the generated MEL spectrograms.\n+        frame_length_ms (`float`, *optional*, defaults to 32.0):\n+            The length of a frame in milliseconds.\n+        hop_length_ms (`float`, *optional*, defaults to 10.0):\n+            Length of the overlapping windows for the STFT used to obtain the Mel Frequency coefficients.\n+        min_frequency (`float`, *optional*, defaults to 125.0):\n+            The minimum frequency (in Hz) for the Mel filterbank.\n+        max_frequency (`float`, *optional*, defaults to 7600.0):\n+            The maximum frequency (in Hz) for the Mel filterbank.\n+        preemphasis (`float`, *optional*, defaults to 0.97):\n+            The preemphasis coefficient.\n+        preemphasis_htk_flavor (`bool`, *optional*, defaults to `True`):\n+            Whether to use HTK-style preemphasis.\n+        fft_overdrive (`bool`, *optional*, defaults to `True`):\n+            Whether to use FFT overdrive.\n+        dither (`float`, *optional*, defaults to 0.0):\n+            Adds dithering. In other words, adds a small Gaussian noise to each frame.\n+            E.g. use 0.0001 to add dithering with a normal distribution centered\n+            around 0.0 with standard deviation 0.0001 (assuming [-1,+1] range of raw_speech).\n+            The value 0.0 means no dithering.\n+            Dithering has similar effect as `spectrogram(mel_floor=...)`. It reduces\n+            the high log_mel_fbank values for signals with hard-zero sections,\n+            when VAD cutoff is present in the signal.\n+        input_scale_factor (`float`, *optional*, defaults to 1.0):\n+            Scaling factor applied to the input waveform.\n+        mel_floor (`float`, *optional*, defaults to 1e-05):\n+            Minimum value for Mel spectrograms to avoid log(0).\n+        per_bin_mean (`Optional[Sequence[float]]`, *optional*):\n+            Mean values for per-bin normalization.\n+        per_bin_stddev (`Optional[Sequence[float]]`, *optional*):\n+            Standard deviation values for per-bin normalization.\n+    \"\"\"\n+\n+    model_input_names = [\"input_features\", \"input_features_mask\"]\n+\n+    def __init__(\n+        self,\n+        feature_size: int = 128,\n+        sampling_rate: int = 16_000,\n+        padding_value: float = 0.0,\n+        return_attention_mask: bool = True,\n+        frame_length_ms: float = 32.0,\n+        hop_length_ms: float = 10.0,\n+        min_frequency: float = 125.0,\n+        max_frequency: float = 7600.0,\n+        preemphasis: float = 0.97,\n+        preemphasis_htk_flavor: bool = True,\n+        fft_overdrive: bool = True,\n+        dither: float = 0.0,\n+        input_scale_factor: float = 1.0,\n+        mel_floor: float = 1e-5,\n+        per_bin_mean: Optional[Sequence[float]] = None,\n+        per_bin_stddev: Optional[Sequence[float]] = None,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            feature_size=feature_size,\n+            sampling_rate=sampling_rate,\n+            padding_value=padding_value,\n+            return_attention_mask=return_attention_mask,\n+            **kwargs,\n+        )\n+\n+        self.min_frequency = min_frequency\n+        self.max_frequency = max_frequency\n+        self.preemphasis = preemphasis\n+        self.preemphasis_htk_flavor = preemphasis_htk_flavor\n+        self.fft_overdrive = fft_overdrive\n+        self.dither = dither\n+        self.input_scale_factor = input_scale_factor\n+        self.frame_length = int(round(sampling_rate * frame_length_ms / 1000.0))\n+        self.hop_length = int(round(sampling_rate * hop_length_ms / 1000.0))\n+        self.mel_floor = np.array(mel_floor, dtype=np.float64)\n+\n+        fft_length = 2 ** math.ceil(math.log2(self.frame_length))\n+        if self.fft_overdrive:\n+            fft_length *= 2\n+        self.fft_length = fft_length\n+\n+        hann_arange = np.arange(self.frame_length, dtype=np.float32)\n+        window = 0.5 * (1 - np.cos(2 * np.pi * hann_arange / self.frame_length))\n+        self.window = window.astype(np.float32)\n+\n+        self.mel_filters = create_fb_matrix(\n+            n_freqs=self.fft_length // 2 + 1,\n+            f_min=min_frequency,\n+            f_max=max_frequency,\n+            n_mels=feature_size,\n+            sample_rate=self.sampling_rate,\n+            norm=None,\n+            fft_length=fft_length,\n+        )\n+\n+        if per_bin_mean is not None:\n+            self.per_bin_mean = np.array(per_bin_mean).reshape(1, 1, feature_size)\n+        else:\n+            self.per_bin_mean = None\n+\n+        if per_bin_stddev is not None:\n+            self.per_bin_stddev = np.array(per_bin_stddev).reshape(1, 1, feature_size)\n+        else:\n+            self.per_bin_stddev = None\n+\n+    def _extract_spectrogram(self, waveform: np.ndarray, attention_mask: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n+        \"\"\"\"\"\"\n+        if waveform.ndim == 1:  # If single waveform, add batch dimension\n+            waveform = np.expand_dims(waveform, axis=0)\n+\n+        if self.dither > 0.0:\n+            waveform = waveform + self.dither * np.random.randn(*waveform.shape).astype(waveform.dtype)\n+\n+        if self.input_scale_factor != 1.0:\n+            waveform = waveform * self.input_scale_factor\n+\n+        frame_size_for_unfold = self.frame_length + 1\n+\n+        # NumPy equivalent of unfold for [B, NumFrames, frame_size_for_unfold]\n+        frames_to_process = _unfold(waveform, dimension=-1, size=frame_size_for_unfold, step=self.hop_length)\n+\n+        if self.preemphasis > 0.0:\n+            if self.preemphasis_htk_flavor:\n+                first_in_frame = frames_to_process[..., :1] * (1.0 - self.preemphasis)\n+                rest_in_frame = frames_to_process[..., 1:-1] - self.preemphasis * frames_to_process[..., :-2]\n+                frames = np.concatenate([first_in_frame, rest_in_frame], axis=-1)\n+            else:\n+                frames = frames_to_process[..., 1:] - self.preemphasis * frames_to_process[..., :-1]\n+        else:\n+            frames = frames_to_process[..., :-1]\n+\n+        frames = frames * self.window  # Broadcasting window\n+        stft = np.fft.rfft(frames, n=self.fft_length, axis=-1)\n+\n+        magnitude_spec = np.abs(stft)\n+\n+        mel_spec = np.matmul(magnitude_spec, self.mel_filters)\n+        log_mel_spec = np.log(np.maximum(mel_spec, self.mel_floor))\n+\n+        if self.per_bin_mean is not None:\n+            log_mel_spec = log_mel_spec - self.per_bin_mean  # Broadcasting\n+\n+        if self.per_bin_stddev is not None:\n+            log_mel_spec = log_mel_spec / self.per_bin_stddev  # Broadcasting\n+\n+        mel_spectrogram = log_mel_spec.squeeze()\n+        mask = attention_mask[:: self.hop_length].astype(bool)\n+        # TODO: The filtered mask is always exactly 3 elements longer than the mel_spectrogram. Why???\n+        return mel_spectrogram, mask[: mel_spectrogram.shape[0]]\n+\n+    def __call__(\n+        self,\n+        raw_speech: Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]],\n+        padding: Union[bool, str, PaddingStrategy] = \"longest\",\n+        max_length: Optional[int] = 480_000,\n+        truncation: bool = True,\n+        pad_to_multiple_of: Optional[int] = 128,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_attention_mask: Optional[bool] = True,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"Creates a batch of MEL spectrograms from the provided raw speech.\n+\n+        This implementation uses a different algorithm for windowing and preemphasis compared to the built-in\n+        `transformers.audio_utils.spectrogram()` function that _will_ result in different outputs. Consider this\n+        carefully when selecting an audio feature extactor, especially with pre-trained models.\n+\n+        Args:\n+            raw_speech:\n+                The audio for which MEL spectrograms are created.\n+            padding (`Union[bool, str, PaddingStrategy]`, *optional*, defaults to `\"longest\"`):\n+                The padding strategy to use for batches of audio with different lengths.\n+            max_length (`int`, *optional*, defaults to 480000):\n+                If provided, defines the maximum length of the audio to allow. Audio longer than this will be\n+                truncated if `truncation=True`.\n+            truncation (`bool`, *optional*, defaults to `True`):\n+                Whether or not to truncate audio above `max_length`.\n+            pad_to_multiple_of (`int`, *optional*, defaults to 128):\n+                When padding, pad to a multiple of this value. The default value is defined for optimal TPU support.\n+            return_tensors (`Union[str, TensorType]`, *optional*, defaults to `None`):\n+                The type of tensors to return (e.g., NumPy, Torch, JAX, TensorFlow).\n+            return_attention_mask (`bool`, *optional*, defaults to `True`):\n+                Whether to return the attention mask for the generated MEL spectrograms.\n+        \"\"\"\n+\n+        is_batched_numpy = isinstance(raw_speech, np.ndarray) and len(raw_speech.shape) > 1\n+        is_batched_sequence = isinstance(raw_speech, Sequence) and isinstance(raw_speech[0], (np.ndarray, Sequence))\n+        is_batched = is_batched_numpy or is_batched_sequence\n+\n+        if is_batched:\n+            raw_speech = [np.asarray([rs]).T for rs in raw_speech]\n+        elif not is_batched and not isinstance(raw_speech, np.ndarray):\n+            raw_speech = np.asarray(raw_speech)\n+\n+        if not is_batched:  # always return a batch\n+            raw_speech = [np.asarray([raw_speech])]\n+\n+        batched_speech = self.pad(\n+            BatchFeature({\"input_features\": raw_speech}),\n+            padding=padding,\n+            max_length=max_length,\n+            truncation=truncation,\n+            pad_to_multiple_of=pad_to_multiple_of,\n+            return_attention_mask=return_attention_mask,\n+        )\n+\n+        prepared_speech = []\n+        prepared_speech_mask = []\n+        for speech, mask in zip(batched_speech.input_features, batched_speech.attention_mask):\n+            speech, mask = self._extract_spectrogram(speech.T, mask)\n+            prepared_speech.append(speech.astype(np.float32))\n+            prepared_speech_mask.append(mask)\n+\n+        return BatchFeature(\n+            {\"input_features\": prepared_speech, \"input_features_mask\": prepared_speech_mask},\n+            tensor_type=return_tensors,\n+        )\n+\n+\n+__all__ = [\"Gemma3nAudioFeatureExtractor\"]"
        },
        {
            "sha": "0817e16451ac5db5a822c3111678beb758fb7bb1",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "added",
            "additions": 2422,
            "deletions": 0,
            "changes": 2422,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=c63cfd6a833d629a74c098933017c61dd755969d"
        },
        {
            "sha": "a3ffa710d84254bae1e9711ba86262b3a5385fd8",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "added",
            "additions": 2664,
            "deletions": 0,
            "changes": 2664,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=c63cfd6a833d629a74c098933017c61dd755969d"
        },
        {
            "sha": "45e953b5c5d217d9eb2524bebd74869877480c78",
            "filename": "src/transformers/models/gemma3n/processing_gemma3n.py",
            "status": "added",
            "additions": 191,
            "deletions": 0,
            "changes": 191,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -0,0 +1,191 @@\n+# coding=utf-8\n+# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ImageInput, make_nested_list_of_images\n+from ...processing_utils import AudioKwargs, ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+\n+\n+class Gemma3nImagesKwargs(ImagesKwargs):\n+    do_pan_and_scan: Optional[bool]\n+    pan_and_scan_min_crop_size: Optional[int]\n+    pan_and_scan_max_num_crops: Optional[int]\n+    pan_and_scan_min_ratio_to_activate: Optional[float]\n+    do_convert_rgb: Optional[bool]\n+\n+\n+class Gemma3nProcessorKwargs(ProcessingKwargs, total=False):\n+    audio_kwargs: AudioKwargs\n+    images_kwargs: Gemma3nImagesKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+        },\n+    }\n+\n+\n+class Gemma3nProcessor(ProcessorMixin):\n+    \"\"\"\n+    A processor for Gemma 3n, wrapping the full capabilities of a feature extractor, image processor, and tokenizer\n+    into a single processor.\n+\n+    Args:\n+        feature_extractor (`Gemma3nAudioFeatureExtractor`):\n+            Feature extractor that converts raw audio waveforms into MEL spectrograms for the audio encoder. This\n+            should return a `BatchFeature` with `input_features` and `input_features_mask` features.\n+        image_processor (`SiglipImageProcessorFast`):\n+            Image processor that prepares batches of images for the vision encoder. This should return a `BatchFeature`\n+            with a `pixel_values` feature.\n+        tokenizer (`GemmaTokenizerFast`):\n+            The text tokenizer for the model.\n+        chat_template (`string`, *optional*):\n+            A Jinja template for generating text prompts from a set of messages.\n+        audio_seq_length (int, *optional*, defaults to 188):\n+            The number of audio soft tokens that will be added to the text prompt\n+        image_seq_length (int, *optional*, defaults to 256):\n+            The number of image soft tokens that should be added to\n+    \"\"\"\n+\n+    attributes = [\"feature_extractor\", \"image_processor\", \"tokenizer\"]\n+    feature_extractor_class = \"AutoFeatureExtractor\"\n+    image_processor_class = \"AutoImageProcessor\"\n+    tokenizer_class = \"AutoTokenizer\"\n+\n+    def __init__(\n+        self,\n+        feature_extractor,\n+        image_processor,\n+        tokenizer,\n+        chat_template=None,\n+        audio_seq_length: int = 188,\n+        image_seq_length: int = 256,\n+        **kwargs,\n+    ):\n+        self.audio_seq_length = audio_seq_length\n+        self.audio_token_id = tokenizer.audio_token_id\n+        self.boa_token = tokenizer.boa_token\n+        self.audio_token = tokenizer.audio_token\n+        audio_tokens_expanded = \"\".join([tokenizer.audio_token] * audio_seq_length)\n+        self.full_audio_sequence = f\"\\n\\n{tokenizer.boa_token}{audio_tokens_expanded}{tokenizer.eoa_token}\\n\\n\"\n+\n+        self.image_seq_length = image_seq_length\n+        self.image_token_id = tokenizer.image_token_id\n+        self.boi_token = tokenizer.boi_token\n+        self.image_token = tokenizer.image_token\n+        image_tokens_expanded = \"\".join([tokenizer.image_token] * image_seq_length)\n+        self.full_image_sequence = f\"\\n\\n{tokenizer.boi_token}{image_tokens_expanded}{tokenizer.eoi_token}\\n\\n\"\n+\n+        super().__init__(\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            tokenizer=tokenizer,\n+            chat_template=chat_template,\n+            **kwargs,\n+        )\n+\n+    def __call__(\n+        self,\n+        images: ImageInput = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        audio: Optional[Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]]] = None,\n+        videos=None,\n+        **kwargs: Unpack[Gemma3nProcessorKwargs],\n+    ) -> BatchFeature:\n+        if text is None and images is None and audio is None:\n+            raise ValueError(\"Provide at least one of `text`, `images`, or `audio`.\")\n+\n+        output_kwargs = self._merge_kwargs(\n+            Gemma3nProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        if isinstance(text, str):\n+            text = [text]\n+        elif not isinstance(text, list) and not isinstance(text[0], str):\n+            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n+        if audio is not None:\n+            audio_inputs = self.feature_extractor(audio, **output_kwargs[\"audio_kwargs\"])\n+\n+            if not text:\n+                text = [self.audio_token for _ in audio]\n+\n+            # Expand placeholder audio tokens to the full audio token sequence\n+            text = [prompt.replace(self.audio_token, self.full_audio_sequence) for prompt in text]\n+        else:\n+            audio_inputs = {}\n+\n+        if images is not None:\n+            batched_images = make_nested_list_of_images(images)\n+            image_inputs = self.image_processor(batched_images, **output_kwargs[\"images_kwargs\"])\n+\n+            # Create empty text to be replaced with placeholders\n+            if not text:\n+                text = [\" \".join([self.image_token] * len(images)) for images in batched_images]\n+\n+            if len(batched_images) != len(text):\n+                raise ValueError(\n+                    f\"Received inconsistently sized batches of images ({len(batched_images)}) and text ({len(text)}).\"\n+                )\n+\n+            # Expand placeholder image tokens to the full image token sequence\n+            text = [prompt.replace(self.image_token, self.full_image_sequence) for prompt in text]\n+        else:\n+            image_inputs = {}\n+\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        text_inputs = self.tokenizer(text=text, **output_kwargs[\"text_kwargs\"], return_tensors=\"np\")\n+        self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\"])\n+\n+        # Add token type ids manually, as tokenizer can't do arbitrary position token types\n+        array_ids = text_inputs[\"input_ids\"]\n+        token_type_ids = np.zeros_like(array_ids)\n+        token_type_ids[array_ids == self.image_token_id] = 1\n+        token_type_ids[array_ids == self.audio_token_id] = 3\n+        text_inputs = {k: v.tolist() for k, v in text_inputs.items()}  # in case user requested list inputs\n+        text_inputs[\"token_type_ids\"] = token_type_ids.tolist()\n+        return BatchFeature(data={**text_inputs, **image_inputs, **audio_inputs}, tensor_type=return_tensors)\n+\n+    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Gemma\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.decode with CLIP->Gemma\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names + [\"token_type_ids\"]\n+        image_processor_input_names = self.image_processor.model_input_names\n+        feature_extactor_input_names = self.feature_extractor.model_input_names\n+        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names + feature_extactor_input_names))\n+\n+\n+__all__ = [\"Gemma3nProcessor\"]"
        },
        {
            "sha": "3b9cb2c5201567f41c2fef5b1e58a48e5fec4d14",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -1642,6 +1642,7 @@ def set_model_tester_for_less_flaky_test(test_case):\n         \"AriaVisionText2TextModelTester\",\n         \"GPTNeoModelTester\",\n         \"DPTModelTester\",\n+        \"Gemma3nTextModelTester\",  # cannot have a single layer combined with the cache sharing config attrs in the tester\n     ]\n     if test_case.model_tester.__class__.__name__ in exceptional_classes:\n         target_num_hidden_layers = None"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/gemma3n/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/tests%2Fmodels%2Fgemma3n%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/tests%2Fmodels%2Fgemma3n%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2F__init__.py?ref=c63cfd6a833d629a74c098933017c61dd755969d"
        },
        {
            "sha": "d2b10315bd6e6de98ddcd114d3014bbdb669a5c6",
            "filename": "tests/models/gemma3n/test_feature_extraction_gemma3n.py",
            "status": "added",
            "additions": 277,
            "deletions": 0,
            "changes": 277,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/tests%2Fmodels%2Fgemma3n%2Ftest_feature_extraction_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/tests%2Fmodels%2Fgemma3n%2Ftest_feature_extraction_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_feature_extraction_gemma3n.py?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -0,0 +1,277 @@\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import itertools\n+import os\n+import random\n+import tempfile\n+import unittest\n+from typing import Optional, Sequence\n+\n+import numpy as np\n+from parameterized import parameterized\n+\n+from transformers.models.gemma3n import Gemma3nAudioFeatureExtractor\n+from transformers.testing_utils import (\n+    check_json_file_has_correct_format,\n+    require_torch,\n+)\n+from transformers.utils.import_utils import is_torch_available\n+\n+from ...test_sequence_feature_extraction_common import SequenceFeatureExtractionTestMixin\n+\n+\n+if is_torch_available():\n+    pass\n+\n+global_rng = random.Random()\n+\n+MAX_LENGTH_FOR_TESTING = 512\n+\n+\n+def floats_list(shape, scale=1.0, rng=None):\n+    \"\"\"Creates a random float32 tensor\"\"\"\n+    if rng is None:\n+        rng = global_rng\n+\n+    values = []\n+    for _ in range(shape[0]):\n+        values.append([])\n+        for _ in range(shape[1]):\n+            values[-1].append(rng.random() * scale)\n+\n+    return values\n+\n+\n+class Gemma3nAudioFeatureExtractionTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        min_seq_length=400,\n+        max_seq_length=2000,\n+        feature_size: int = 128,\n+        sampling_rate: int = 16_000,\n+        padding_value: float = 0.0,\n+        return_attention_mask: bool = False,\n+        # ignore hop_length / frame_length for now, as ms -> length conversion causes issues with serialization tests\n+        # frame_length_ms: float = 32.0,\n+        # hop_length: float = 10.0,\n+        min_frequency: float = 125.0,\n+        max_frequency: float = 7600.0,\n+        preemphasis: float = 0.97,\n+        preemphasis_htk_flavor: bool = True,\n+        fft_overdrive: bool = True,\n+        dither: float = 0.0,\n+        input_scale_factor: float = 1.0,\n+        mel_floor: float = 1e-5,\n+        per_bin_mean: Optional[Sequence[float]] = None,\n+        per_bin_stddev: Optional[Sequence[float]] = None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.min_seq_length = min_seq_length\n+        self.max_seq_length = max_seq_length\n+        self.seq_length_diff = (self.max_seq_length - self.min_seq_length) // (self.batch_size - 1)\n+        self.feature_size = feature_size\n+        self.sampling_rate = sampling_rate\n+        self.padding_value = padding_value\n+        self.return_attention_mask = return_attention_mask\n+        # ignore hop_length / frame_length for now, as ms -> length conversion causes issues with serialization tests\n+        # self.frame_length_ms = frame_length_ms\n+        # self.hop_length = hop_length\n+        self.min_frequency = min_frequency\n+        self.max_frequency = max_frequency\n+        self.preemphasis = preemphasis\n+        self.preemphasis_htk_flavor = preemphasis_htk_flavor\n+        self.fft_overdrive = fft_overdrive\n+        self.dither = dither\n+        self.input_scale_factor = input_scale_factor\n+        self.mel_floor = mel_floor\n+        self.per_bin_mean = per_bin_mean\n+        self.per_bin_stddev = per_bin_stddev\n+\n+    def prepare_feat_extract_dict(self):\n+        return {\n+            \"feature_size\": self.feature_size,\n+            \"sampling_rate\": self.sampling_rate,\n+            \"padding_value\": self.padding_value,\n+            \"return_attention_mask\": self.return_attention_mask,\n+            \"min_frequency\": self.min_frequency,\n+            \"max_frequency\": self.max_frequency,\n+            \"preemphasis\": self.preemphasis,\n+            \"preemphasis_htk_flavor\": self.preemphasis_htk_flavor,\n+            \"fft_overdrive\": self.fft_overdrive,\n+            \"dither\": self.dither,\n+            \"input_scale_factor\": self.input_scale_factor,\n+            \"mel_floor\": self.mel_floor,\n+            \"per_bin_mean\": self.per_bin_mean,\n+            \"per_bin_stddev\": self.per_bin_stddev,\n+        }\n+\n+    def prepare_inputs_for_common(self, equal_length=False, numpify=False):\n+        def _flatten(list_of_lists):\n+            return list(itertools.chain(*list_of_lists))\n+\n+        if equal_length:\n+            speech_inputs = [floats_list((self.max_seq_length, self.feature_size)) for _ in range(self.batch_size)]\n+        else:\n+            # make sure that inputs increase in size\n+            speech_inputs = [\n+                floats_list((x, self.feature_size))\n+                for x in range(self.min_seq_length, self.max_seq_length, self.seq_length_diff)\n+            ]\n+        if numpify:\n+            speech_inputs = [np.asarray(x) for x in speech_inputs]\n+        return speech_inputs\n+\n+\n+class Gemma3nAudioFeatureExtractionTest(SequenceFeatureExtractionTestMixin, unittest.TestCase):\n+    feature_extraction_class = Gemma3nAudioFeatureExtractor\n+\n+    def setUp(self):\n+        self.feat_extract_tester = Gemma3nAudioFeatureExtractionTester(self)\n+\n+    def test_feat_extract_from_and_save_pretrained(self):\n+        feat_extract_first = self.feature_extraction_class(**self.feat_extract_dict)\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            saved_file = feat_extract_first.save_pretrained(tmpdirname)[0]\n+            check_json_file_has_correct_format(saved_file)\n+            feat_extract_second = self.feature_extraction_class.from_pretrained(tmpdirname)\n+\n+        dict_first = feat_extract_first.to_dict()\n+        dict_second = feat_extract_second.to_dict()\n+        mel_1 = feat_extract_first.mel_filters\n+        mel_2 = feat_extract_second.mel_filters\n+        self.assertTrue(np.allclose(mel_1, mel_2))\n+        self.assertEqual(dict_first, dict_second)\n+\n+    def test_feat_extract_to_json_file(self):\n+        feat_extract_first = self.feature_extraction_class(**self.feat_extract_dict)\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            json_file_path = os.path.join(tmpdirname, \"feat_extract.json\")\n+            feat_extract_first.to_json_file(json_file_path)\n+            feat_extract_second = self.feature_extraction_class.from_json_file(json_file_path)\n+\n+        dict_first = feat_extract_first.to_dict()\n+        dict_second = feat_extract_second.to_dict()\n+        mel_1 = feat_extract_first.mel_filters\n+        mel_2 = feat_extract_second.mel_filters\n+        self.assertTrue(np.allclose(mel_1, mel_2))\n+        self.assertEqual(dict_first, dict_second)\n+\n+    def test_feat_extract_from_pretrained_kwargs(self):\n+        feat_extract_first = self.feature_extraction_class(**self.feat_extract_dict)\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            saved_file = feat_extract_first.save_pretrained(tmpdirname)[0]\n+            check_json_file_has_correct_format(saved_file)\n+            feat_extract_second = self.feature_extraction_class.from_pretrained(\n+                tmpdirname, feature_size=2 * self.feat_extract_dict[\"feature_size\"]\n+            )\n+\n+        mel_1 = feat_extract_first.mel_filters\n+        mel_2 = feat_extract_second.mel_filters\n+        self.assertTrue(2 * mel_1.shape[1] == mel_2.shape[1])\n+\n+    @parameterized.expand(\n+        [\n+            ([floats_list((1, x))[0] for x in range(800, 1400, 200)],),\n+            ([floats_list((1, x))[0] for x in (800, 800, 800)],),\n+            ([floats_list((1, x))[0] for x in range(200, (MAX_LENGTH_FOR_TESTING + 500), 200)], True),\n+        ]\n+    )\n+    def test_call(self, audio_inputs, test_truncation=False):\n+        feature_extractor = self.feature_extraction_class(**self.feat_extract_tester.prepare_feat_extract_dict())\n+        np_audio_inputs = [np.asarray(audio_input) for audio_input in audio_inputs]\n+\n+        input_features = feature_extractor(np_audio_inputs, padding=\"max_length\", return_tensors=\"np\").input_features\n+        self.assertTrue(input_features.ndim == 3)\n+        # input_features.shape should be (batch, num_frames, n_mels) ~= (batch, num_frames, feature_size)\n+        # 480_000 is the max_length that inputs are padded to. we use that to calculate num_frames\n+        expected_num_frames = (480_000 - feature_extractor.frame_length) // (feature_extractor.hop_length) + 1\n+        self.assertTrue(\n+            input_features.shape[-2] == expected_num_frames,\n+            f\"no match: {input_features.shape[-1]} vs {expected_num_frames}\",\n+        )\n+        self.assertTrue(input_features.shape[-1] == feature_extractor.feature_size)\n+\n+        encoded_sequences_1 = feature_extractor(audio_inputs, return_tensors=\"np\").input_features\n+        encoded_sequences_2 = feature_extractor(np_audio_inputs, return_tensors=\"np\").input_features\n+        for enc_seq_1, enc_seq_2 in zip(encoded_sequences_1, encoded_sequences_2):\n+            self.assertTrue(np.allclose(enc_seq_1, enc_seq_2, atol=1e-3))\n+\n+        if test_truncation:\n+            audio_inputs_truncated = [x[:MAX_LENGTH_FOR_TESTING] for x in audio_inputs]\n+            np_audio_inputs_truncated = [np.asarray(audio_input) for audio_input in audio_inputs_truncated]\n+\n+            encoded_sequences_1 = feature_extractor(\n+                audio_inputs_truncated, max_length=MAX_LENGTH_FOR_TESTING, return_tensors=\"np\"\n+            ).input_features\n+            encoded_sequences_2 = feature_extractor(\n+                np_audio_inputs_truncated, max_length=MAX_LENGTH_FOR_TESTING, return_tensors=\"np\"\n+            ).input_features\n+            for enc_seq_1, enc_seq_2 in zip(encoded_sequences_1, encoded_sequences_2):\n+                self.assertTrue(np.allclose(enc_seq_1, enc_seq_2, atol=1e-3))\n+\n+    def test_dither(self):\n+        np.random.seed(42)  # seed the dithering randn()\n+\n+        # Tests that features with and without little dithering are similar, but not the same\n+        dict_no_dither = self.feat_extract_tester.prepare_feat_extract_dict()\n+        dict_no_dither[\"dither\"] = 0.0\n+\n+        dict_dither = self.feat_extract_tester.prepare_feat_extract_dict()\n+        dict_dither[\"dither\"] = 0.00003  # approx. 1/32k\n+\n+        feature_extractor_no_dither = self.feature_extraction_class(**dict_no_dither)\n+        feature_extractor_dither = self.feature_extraction_class(**dict_dither)\n+\n+        # create three inputs of length 800, 1000, and 1200\n+        speech_inputs = [floats_list((1, x))[0] for x in range(800, 1400, 200)]\n+        np_speech_inputs = [np.asarray(speech_input) for speech_input in speech_inputs]\n+\n+        # compute features\n+        input_features_no_dither = feature_extractor_no_dither(\n+            np_speech_inputs, padding=True, return_tensors=\"np\", sampling_rate=dict_no_dither[\"sampling_rate\"]\n+        ).input_features\n+        input_features_dither = feature_extractor_dither(\n+            np_speech_inputs, padding=True, return_tensors=\"np\", sampling_rate=dict_dither[\"sampling_rate\"]\n+        ).input_features\n+\n+        # test there is a difference between features (there's added noise to input signal)\n+        diff = input_features_dither - input_features_no_dither\n+\n+        # features are not identical\n+        self.assertTrue(np.abs(diff).mean() > 1e-6)\n+        # features are not too different\n+        self.assertTrue(np.abs(diff).mean() <= 1e-4)\n+        self.assertTrue(np.abs(diff).max() <= 5e-3)\n+\n+    @require_torch\n+    def test_double_precision_pad(self):\n+        import torch\n+\n+        feature_extractor = self.feature_extraction_class(**self.feat_extract_tester.prepare_feat_extract_dict())\n+        np_speech_inputs = np.random.rand(100, 32).astype(np.float64)\n+        py_speech_inputs = np_speech_inputs.tolist()\n+\n+        for inputs in [py_speech_inputs, np_speech_inputs]:\n+            np_processed = feature_extractor.pad([{\"input_features\": inputs}], return_tensors=\"np\")\n+            self.assertTrue(np_processed.input_features.dtype == np.float32)\n+            pt_processed = feature_extractor.pad([{\"input_features\": inputs}], return_tensors=\"pt\")\n+            self.assertTrue(pt_processed.input_features.dtype == torch.float32)"
        },
        {
            "sha": "2f546e19e49c35c3499d6d1ddfa02a36ee20eb9d",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "added",
            "additions": 886,
            "deletions": 0,
            "changes": 886,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -0,0 +1,886 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Gemma3n model.\"\"\"\n+\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+import pytest\n+from datasets import load_dataset\n+from parameterized import parameterized\n+\n+from transformers import (\n+    AutoModelForCausalLM,\n+    AutoProcessor,\n+    AutoTokenizer,\n+    Gemma3nAudioConfig,\n+    Gemma3nAudioFeatureExtractor,\n+    Gemma3nConfig,\n+    Gemma3nTextConfig,\n+    GenerationConfig,\n+    is_torch_available,\n+)\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_flash_attn,\n+    require_read_token,\n+    require_torch,\n+    require_torch_gpu,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ..gemma.test_modeling_gemma import GemmaModelTester\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        Gemma3nAudioEncoder,\n+        Gemma3nForCausalLM,\n+        Gemma3nForConditionalGeneration,\n+        Gemma3nModel,\n+        Gemma3nTextModel,\n+    )\n+\n+\n+class Gemma3nAudioModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=2,\n+        num_channels=32,  # feature_size / input_feat_size\n+        sampling_rate=16_000,\n+        raw_audio_length=8_000,\n+        is_training=True,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.sampling_rate = sampling_rate\n+        self.raw_audio_length = raw_audio_length\n+        self.is_training = is_training\n+\n+    def get_feature_extractor_config(self):\n+        return {\n+            \"feature_size\": self.num_channels,\n+            \"sampling_rate\": self.sampling_rate,\n+            \"padding_value\": 0.0,\n+            \"return_attention_mask\": True,\n+            \"frame_length_ms\": 32.0,\n+            \"hop_length_ms\": 10.0,\n+            \"dither\": 0.0,  # Important for determinism\n+        }\n+\n+    def get_audio_encoder_config(self):\n+        return Gemma3nAudioConfig(\n+            input_feat_size=self.num_channels,\n+            hidden_size=32,\n+            conf_num_attention_heads=4,\n+            conf_num_hidden_layers=2,\n+            sscp_conv_channel_size=(16, 8),\n+            conf_conv_kernel_size=3,\n+            conf_attention_chunk_size=4,\n+            conf_attention_context_left=5,\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        # Prepare inputs for the audio encoder\n+        feature_extractor_config = self.get_feature_extractor_config()\n+        audio_encoder_config = self.get_audio_encoder_config()\n+\n+        np.random.seed(0)\n+        raw_speech_1 = np.sin(2 * np.pi * 440 * np.linspace(0, 1, self.raw_audio_length)).astype(np.float32)\n+        raw_speech_2 = np.random.randn(self.raw_audio_length // 2).astype(np.float32)\n+        raw_speech = [raw_speech_1, raw_speech_2]\n+\n+        feature_extractor = Gemma3nAudioFeatureExtractor(**feature_extractor_config)\n+        audio_inputs = feature_extractor(raw_speech, return_tensors=\"pt\")\n+\n+        input_features = audio_inputs[\"input_features\"]\n+        # The encoder expects a padding mask (True for padding), while the feature extractor\n+        # returns an attention mask (True for valid tokens). We must invert it.\n+        input_features_mask = ~audio_inputs[\"input_features_mask\"].to(torch.bool)\n+\n+        inputs_dict = {\n+            \"audio_mel\": input_features,\n+            \"audio_mel_mask\": input_features_mask,\n+        }\n+        return audio_encoder_config, inputs_dict\n+\n+\n+@unittest.skip(\"Skipped for now!\")\n+@require_torch\n+class Gemma3nAudioModelTest(ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (Gemma3nAudioEncoder,) if is_torch_available() else ()\n+    test_pruning = False\n+    test_head_masking = False\n+    test_missing_keys = False\n+    is_generative = False\n+    _is_stateful = True\n+    main_input_name = \"audio_mel\"\n+    test_initialization = False\n+    test_can_init_all_missing_weights = False\n+\n+    def setUp(self):\n+        self.model_tester = Gemma3nAudioModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=Gemma3nAudioConfig, hidden_size=37)\n+        torch.manual_seed(0)\n+\n+        # The following values are golden outputs from a deterministic run of the components.\n+        # They are used to ensure that changes to the code do not alter the numerical output.\n+        # Generated with seeds np.random.seed(0) and torch.manual_seed(0).\n+        self.expected_input_features_shape = (2, 48, 32)\n+        self.expected_input_features_slice = np.array([-5.733152, -5.337127, -4.916284, -4.378989, -3.7622747])\n+        self.expected_input_features_mask_shape = (2, 48)\n+        self.expected_input_features_mask_slice = np.array([True, True, True, True, False])\n+\n+        self.expected_encoder_output_shape = (2, 3, 32)\n+        self.expected_encoder_output_slice = torch.tensor([-0.4159, 0.6459, 0.6305, 2.2902, 0.9683])\n+        self.expected_encoder_mask_shape = (2, 3)\n+        self.expected_encoder_mask_slice = torch.tensor([False, False, True])\n+\n+        # Prepare a shared feature extractor and raw audio for the tests\n+        self.feature_extractor = Gemma3nAudioFeatureExtractor(**self.model_tester.get_feature_extractor_config())\n+        np.random.seed(0)\n+        raw_speech_1 = np.sin(2 * np.pi * 440 * np.linspace(0, 1, self.model_tester.raw_audio_length)).astype(\n+            np.float32\n+        )\n+        raw_speech_2 = np.random.randn(self.model_tester.raw_audio_length // 2).astype(np.float32)\n+        self.raw_speech = [raw_speech_1, raw_speech_2]\n+\n+    @unittest.skip(\"Audio encoder does not support attention output\")\n+    def test_attention_outputs(self):\n+        pass\n+\n+    @unittest.skip(\"Audio encoder does not support hidden state output\")\n+    def test_hidden_states_output(self):\n+        pass\n+\n+    @unittest.skip(\"Audio encoder returns a tuple, not a ModelOutput object, skipping equivalence test.\")\n+    def test_model_outputs_equivalence(self):\n+        pass\n+\n+    @unittest.skip(\"Audio encoder does not support retaining gradients on hidden states/attentions.\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(\"Audio encoder does not have a concept of token embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(\"Audio encoder does not have a concept of token embeddings\")\n+    def test_resize_tokens_embeddings(self):\n+        pass\n+\n+    @unittest.skip(\"This model has a complex downsampling scheme that is hard to test with the generic batching test.\")\n+    def test_batching_equivalence(self):\n+        pass\n+\n+    def test_feature_extractor(self):\n+        \"\"\"\n+        Tests the feature extractor's output against pre-computed golden values.\n+        This ensures the NumPy-based audio preprocessing is correct and consistent.\n+        \"\"\"\n+        audio_inputs = self.feature_extractor(\n+            self.raw_speech, padding=\"longest\", pad_to_multiple_of=128, return_tensors=\"np\"\n+        )\n+\n+        input_features = audio_inputs[\"input_features\"]\n+        self.assertEqual(input_features.shape, self.expected_input_features_shape)\n+        np.testing.assert_allclose(input_features[0, 0, :5], self.expected_input_features_slice, rtol=1e-5, atol=1e-5)\n+\n+        print(input_features[0, 0, :5])\n+\n+        input_features_mask = audio_inputs[\"input_features_mask\"]\n+        self.assertEqual(input_features_mask.shape, self.expected_input_features_mask_shape)\n+        # The second audio sample is shorter (22 frames vs 48), so its mask should become False at index 22\n+        np.testing.assert_array_equal(input_features_mask[1, 21:26], self.expected_input_features_mask_slice)\n+\n+    def test_audio_encoder(self):\n+        \"\"\"\n+        Tests the audio encoder's forward pass against pre-computed golden values.\n+        This ensures the PyTorch-based audio encoding model is correct and consistent.\n+        \"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        model = Gemma3nAudioEncoder(config).to(torch_device).eval()\n+\n+        with torch.no_grad():\n+            encoder_output, encoder_mask = model(**inputs_dict)\n+\n+        print(encoder_output[0, 0, :5])\n+\n+        # Check output encodings\n+        self.assertEqual(encoder_output.shape, self.expected_encoder_output_shape)\n+        torch.testing.assert_close(\n+            encoder_output[0, 0, :5], self.expected_encoder_output_slice.to(torch_device), rtol=1e-4, atol=1e-4\n+        )\n+\n+        # Check output mask (True means padded)\n+        # Second sample has 22 feature frames. After downsampling by 4 (conv) -> 5 frames. After downsampling by 4 (reduction) -> 1 frame.\n+        # So the mask should be [False, True, True]\n+        self.assertEqual(encoder_mask.shape, self.expected_encoder_mask_shape)\n+        torch.testing.assert_close(encoder_mask[1, :], self.expected_encoder_mask_slice.to(torch_device))\n+\n+\n+class Gemma3nTextModelTester(GemmaModelTester):\n+    activation_sparsity_pattern = None\n+    forced_config_args = [\"activation_sparsity_pattern\"]\n+\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        seq_length=7,\n+        is_training=True,\n+        use_input_mask=True,\n+        use_token_type_ids=False,\n+        use_labels=True,\n+        vocab_size=99,\n+        vocab_size_per_layer_input=99,\n+        hidden_size=16,\n+        num_hidden_layers=4,  # override to correctly test sharing cache pattern\n+        num_kv_shared_layers=2,  # important to override\n+        layer_types=[\n+            \"full_attention\",\n+            \"sliding_attention\",\n+            \"full_attention\",\n+            \"sliding_attention\",\n+        ],  # similarly we want to test sharing on both types\n+        num_attention_heads=2,\n+        num_key_value_heads=2,\n+        altup_num_inputs=2,\n+        intermediate_size=21,\n+        hidden_activation=\"gelu_pytorch_tanh\",\n+        max_position_embeddings=512,\n+        type_vocab_size=16,\n+        type_sequence_label_size=2,\n+        initializer_range=0.02,\n+        num_labels=3,\n+        num_choices=4,\n+        pad_token_id=0,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        is_decoder=False,\n+    ):\n+        self._verify_model_attributes()\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.is_training = is_training\n+        self.use_input_mask = use_input_mask\n+        self.use_token_type_ids = use_token_type_ids\n+        self.use_labels = use_labels\n+        self.vocab_size = vocab_size\n+        self.vocab_size_per_layer_input = vocab_size_per_layer_input\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_kv_shared_layers = num_kv_shared_layers\n+        self.layer_types = layer_types\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.altup_num_inputs = altup_num_inputs\n+        self.intermediate_size = intermediate_size\n+        self.hidden_activation = hidden_activation\n+        self.max_position_embeddings = max_position_embeddings\n+        self.type_vocab_size = type_vocab_size\n+        self.type_sequence_label_size = type_sequence_label_size\n+        self.initializer_range = initializer_range\n+        self.num_labels = num_labels\n+        self.num_choices = num_choices\n+        self.pad_token_id = pad_token_id\n+        self.bos_token_id = bos_token_id\n+        self.eos_token_id = eos_token_id\n+        self.head_dim = self.hidden_size // self.num_attention_heads\n+        self.is_decoder = is_decoder\n+\n+    if is_torch_available():\n+        config_class = Gemma3nTextConfig\n+        model_class = Gemma3nTextModel\n+        for_causal_lm_class = Gemma3nForCausalLM\n+\n+\n+@unittest.skip(\"Skipped for now!\")\n+@require_torch\n+class Gemma3nTextModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    all_model_classes = (Gemma3nTextModel, Gemma3nForCausalLM) if is_torch_available() else ()\n+    all_generative_model_classes = (Gemma3nForCausalLM,) if is_torch_available() else ()\n+    test_headmasking = False\n+    test_pruning = False\n+    _is_stateful = True\n+    model_split_percents = [0.5, 0.6]\n+\n+    def setUp(self):\n+        self.model_tester = Gemma3nTextModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=Gemma3nConfig,\n+            hidden_size=37,\n+            text_config={\"activation_sparsity_pattern\": None},\n+        )\n+\n+    def _check_hidden_states_for_generate(\n+        self, batch_size, hidden_states, prompt_length, output_length, config, use_cache=False\n+    ):\n+        \"Gemma3n has special hidden states shape with 1 additional dim (which is then reduced with projections)\"\n+\n+        self.assertIsInstance(hidden_states, tuple)\n+        self.assertListEqual(\n+            [isinstance(iter_hidden_states, tuple) for iter_hidden_states in hidden_states],\n+            [True] * len(hidden_states),\n+        )\n+        self.assertEqual(len(hidden_states), (output_length - prompt_length))\n+\n+        # When `output_hidden_states=True`, each iteration of generate appends the hidden states corresponding to the\n+        # new token(s)\n+        # NOTE: `HybridCache` may have different lengths on different layers, if this test starts failing add more\n+        # elaborate checks\n+        for generated_length, iter_hidden_states in enumerate(hidden_states):\n+            # regardless of using cache, the first forward pass will have the full prompt as input\n+            if use_cache and generated_length > 0:\n+                model_input_length = 1\n+            else:\n+                model_input_length = prompt_length + generated_length\n+            expected_shape = (config.altup_num_inputs, batch_size, model_input_length, config.hidden_size)\n+            # check hidden size\n+            self.assertListEqual(\n+                [layer_hidden_states.shape for layer_hidden_states in iter_hidden_states],\n+                [expected_shape] * len(iter_hidden_states),\n+            )\n+\n+\n+class Gemma3nVision2TextModelTester:\n+    text_config = {\"activation_sparsity_pattern\": None}\n+    forced_config_args = [\"text_config\"]\n+\n+    def __init__(\n+        self,\n+        parent,\n+        mm_tokens_per_image=2,\n+        image_token_index=1,\n+        boi_token_index=2,\n+        eoi_token_index=3,\n+        seq_length=25,\n+        is_training=True,\n+        vision_config={\n+            \"use_labels\": True,\n+            \"image_size\": 20,\n+            \"patch_size\": 5,\n+            \"num_channels\": 3,\n+            \"is_training\": True,\n+            \"hidden_size\": 32,\n+            \"num_key_value_heads\": 1,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"intermediate_size\": 37,\n+            \"dropout\": 0.1,\n+            \"attention_dropout\": 0.1,\n+            \"initializer_range\": 0.02,\n+        },\n+        use_cache=False,\n+    ):\n+        self.parent = parent\n+        # `image_token_index` is set to 0 to pass \"resize_embeddings\" test, do not modify\n+        self.mm_tokens_per_image = mm_tokens_per_image\n+        self.image_token_index = image_token_index\n+        self.boi_token_index = boi_token_index\n+        self.eoi_token_index = eoi_token_index\n+        self.llm_tester = Gemma3nTextModelTester(self.parent)\n+        self.text_config = self.llm_tester.get_config()\n+        self.vision_config = vision_config\n+        self.seq_length = seq_length\n+        self.pad_token_id = self.text_config.pad_token_id\n+\n+        self.num_hidden_layers = self.text_config.num_hidden_layers\n+        self.vocab_size = self.text_config.vocab_size\n+        self.hidden_size = self.text_config.hidden_size\n+        self.num_attention_heads = self.text_config.num_attention_heads\n+        self.is_training = is_training\n+\n+        self.batch_size = 3\n+        self.num_channels = vision_config[\"num_channels\"]\n+        self.image_size = vision_config[\"image_size\"]\n+        self.encoder_seq_length = seq_length\n+        self.use_cache = use_cache\n+\n+    def get_config(self):\n+        return Gemma3nConfig(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            image_token_index=self.image_token_index,\n+            boi_token_index=self.boi_token_index,\n+            eoi_token_index=self.eoi_token_index,\n+            mm_tokens_per_image=self.mm_tokens_per_image,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                self.vision_config[\"num_channels\"],\n+                self.vision_config[\"image_size\"],\n+                self.vision_config[\"image_size\"],\n+            ]\n+        )\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 1) + 1\n+        attention_mask = input_ids.ne(self.pad_token_id).to(torch_device)\n+\n+        # set the 3 first tokens to be image, and ensure that no other tokens are image tokens\n+        # do not change this unless you modified image size or patch size\n+        input_ids[input_ids == config.image_token_index] = self.pad_token_id\n+        input_ids[:, :1] = config.image_token_index\n+\n+        token_type_ids = torch.zeros_like(input_ids)\n+        token_type_ids[input_ids == config.image_token_index] = 1\n+\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"token_type_ids\": token_type_ids,\n+        }\n+        return config, inputs_dict\n+\n+\n+@unittest.skip(\"Skipped for now!\")\n+@require_torch\n+class Gemma3nVision2TextModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    all_model_classes = (Gemma3nModel, Gemma3nForConditionalGeneration) if is_torch_available() else ()\n+    all_generative_model_classes = (Gemma3nForConditionalGeneration,) if is_torch_available() else ()\n+    test_headmasking = False\n+    test_pruning = False\n+    test_missing_keys = False\n+    _is_stateful = True\n+    model_split_percents = [0.5, 0.6]\n+\n+    # MP works but offload doesn't work when the SigLIP MultiheadAttention is offloaded\n+    # TODO: One potential solution would be to add to set preload_module_classes = [\"SiglipMultiheadAttentionPoolingHead\"]\n+    # in the dispatch_model function\n+    test_cpu_offload = False\n+    test_disk_offload_safetensors = False\n+    test_disk_offload_bin = False\n+\n+    def setUp(self):\n+        self.model_tester = Gemma3nVision2TextModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=Gemma3nConfig,\n+            hidden_size=37,\n+            text_config={\"activation_sparsity_pattern\": None},\n+        )\n+\n+    @unittest.skip(reason=\"SiglipVisionModel (vision backbone) does not support standalone training\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SiglipVisionModel (vision backbone) does not support standalone training\")\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SiglipVisionModel (vision backbone) does not support standalone training\")\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"HybridCache can't be gathered because it is not iterable. Adding a simple iter and dumping `distributed_iterator`\"\n+        \" as in Dynamic Cache doesnt work. NOTE: @gante all cache objects would need better compatibility with multi gpu setting\"\n+    )\n+    def test_multi_gpu_data_parallel_forward(self):\n+        pass\n+\n+    @unittest.skip(\"Failing because of unique cache (HybridCache)\")\n+    def test_model_outputs_equivalence(self, **kwargs):\n+        pass\n+\n+    @parameterized.expand([(\"random\",), (\"same\",)])\n+    @pytest.mark.generate\n+    @unittest.skip(\"Gemma3n has HybridCache which is not compatible with assisted decoding\")\n+    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n+        pass\n+\n+    @unittest.skip(\"Gemma3n has HybridCache which is not compatible with assisted decoding\")\n+    def test_prompt_lookup_decoding_matches_greedy_search(self, assistant_type):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(\"Gemma3n has HybridCache which is not compatible with assisted decoding\")\n+    def test_assisted_decoding_sample(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma3n has HybridCache which is not compatible with dola decoding\")\n+    def test_dola_decoding_sample(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma3n has HybridCache and doesn't support continue from past kv\")\n+    def test_generate_continue_from_past_key_values(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma3n has HybridCache and doesn't support low_memory generation\")\n+    def test_beam_search_low_memory(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma3n has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma3n has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma3n has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate_low_memory(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma3n has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n+    def test_generate_with_static_cache(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma3n has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n+    def test_generate_from_inputs_embeds_with_static_cache(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"Siglip (vision backbone) uses the same initialization scheme as the Flax original implementation\"\n+    )\n+    def test_initialization(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"Siglip has no FLEX attention, and we don't have a proper way to set/test attn in VLMs. TODO @raushan\"\n+    )\n+    def test_flex_attention_with_grads(self):\n+        pass\n+\n+    def test_automodelforcausallm(self):\n+        \"\"\"\n+        Regression test for #36741 -- make sure `AutoModelForCausalLM` works with a Gemma3n config, i.e. that\n+        `AutoModelForCausalLM.from_pretrained` pulls the text config before loading the model\n+        \"\"\"\n+        config = self.model_tester.get_config()\n+        model = Gemma3nForConditionalGeneration(config)\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            model.save_pretrained(tmp_dir)\n+            for_causal_lm = AutoModelForCausalLM.from_pretrained(tmp_dir)\n+            self.assertIsInstance(for_causal_lm, Gemma3nForCausalLM)\n+\n+\n+@unittest.skip(\"Skipped for now!\")\n+@slow\n+@require_torch_gpu\n+@require_read_token\n+class Gemma3nIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.processor = AutoProcessor.from_pretrained(\"Google/gemma-3n-E4B-it\", padding_side=\"left\")\n+\n+        url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/cow_beach_1.png\"\n+        self.messages = [\n+            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\", \"url\": url},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+\n+        audio_ds = load_dataset(\n+            \"etechgrid/28.5k_wavfiles_dataset\", \"default\", data_files=\"wav_dataset/103-1240-0000.wav\"\n+        )\n+        self.audio_file_path = audio_ds[\"train\"][0][\"audio\"][\"path\"]\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def test_model_4b_bf16(self):\n+        model_id = \"Google/gemma-3n-E4B-it\"\n+\n+        model = Gemma3nForConditionalGeneration.from_pretrained(\n+            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n+        ).to(torch_device)\n+\n+        inputs = self.processor.apply_chat_template(\n+            self.messages,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            add_generation_prompt=True,\n+        ).to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n+\n+        EXPECTED_TEXTS = ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear blue water and a blue sky in the background. It looks like']  # fmt: skip\n+        self.assertEqual(output_text, EXPECTED_TEXTS)\n+\n+    def test_model_with_audio(self):\n+        \"\"\"\n+        Tests the full model pipeline with batched audio inputs provided as file paths.\n+        This ensures the processor correctly loads and processes audio files.\n+        \"\"\"\n+\n+        model_id = \"Google/gemma-3n-E4B-it\"\n+\n+        model = Gemma3nForConditionalGeneration.from_pretrained(\n+            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n+        ).to(torch_device)\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"Transcribe the following speech segment in English:\"},\n+                        {\"type\": \"audio\", \"audio\": str(self.audio_file_path)},\n+                    ],\n+                }\n+            ],\n+        ]\n+\n+        inputs = self.processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            padding=True,\n+            return_tensors=\"pt\",\n+        ).to(torch_device, dtype=model.dtype)\n+\n+        input_len = inputs[\"input_ids\"].shape[-1]\n+\n+        output = model.generate(**inputs, max_new_tokens=16, do_sample=False)\n+        output = output[:, input_len:]\n+        output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n+\n+        EXPECTED_TEXTS = [\"Chapter 1. Mrs. Rachel Lind is surprised.\\n\\nMrs. Rachel Lind\"]\n+        self.assertEqual(output_text, EXPECTED_TEXTS)\n+\n+    def test_model_4b_batch(self):\n+        model_id = \"Google/gemma-3n-E4B-it\"\n+\n+        model = Gemma3nForConditionalGeneration.from_pretrained(\n+            model_id, low_cpu_mem_usage=False, torch_dtype=torch.bfloat16\n+        ).to(torch_device)\n+\n+        messages_2 = [\n+            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/cow_beach_1.png\",\n+                    },\n+                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\"type\": \"text\", \"text\": \"Are these images identical?\"},\n+                ],\n+            },\n+        ]\n+\n+        inputs = self.processor.apply_chat_template(\n+            [self.messages, messages_2],\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+            add_generation_prompt=True,\n+        ).to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n+\n+        EXPECTED_TEXTS = [\n+            'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear turquoise water and a blue sky in the background. It looks like',\n+            \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, these images are not identical. \\n\\nHere's a breakdown of the differences:\\n\\n*   **Image 1:** Shows a cow\"\n+        ]  # fmt: skip\n+        self.assertEqual(output_text, EXPECTED_TEXTS)\n+\n+    def test_model_4b_crops(self):\n+        model_id = \"Google/gemma-3n-E4B-it\"\n+\n+        model = Gemma3nForConditionalGeneration.from_pretrained(\n+            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n+        ).to(torch_device)\n+\n+        crop_config = {\n+            \"images_kwargs\": {\n+                \"do_pan_and_scan\": True,\n+                \"pan_and_scan_max_num_crops\": 448,\n+                \"pan_and_scan_min_crop_size\": 32,\n+                \"pan_and_scan_min_ratio_to_activate\": 0.3,\n+            }\n+        }\n+\n+        inputs = self.processor.apply_chat_template(\n+            self.messages,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            add_generation_prompt=True,\n+            **crop_config,\n+        ).to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n+\n+        EXPECTED_NUM_IMAGES = 3  # one for the origin image and two crops of images\n+        EXPECTED_TEXTS = ['user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a beach with a turquoise ocean and blue sky in the background.']  # fmt: skip\n+        self.assertEqual(len(inputs[\"pixel_values\"]), EXPECTED_NUM_IMAGES)\n+        self.assertEqual(output_text, EXPECTED_TEXTS)\n+\n+    def test_model_4b_multiimage(self):\n+        model_id = \"Google/gemma-3n-E4B-it\"\n+\n+        model = Gemma3nForConditionalGeneration.from_pretrained(\n+            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n+        ).to(torch_device)\n+\n+        messages = [\n+            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\"type\": \"text\", \"text\": \"What do you see here?\"},\n+                ],\n+            },\n+        ]\n+\n+        inputs = self.processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+            add_generation_prompt=True,\n+        ).to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n+\n+        EXPECTED_TEXTS = [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image:\\n\\n**Overall Scene:**\\n\\nIt looks like a street scene in a vibrant,\"]  # fmt: skip\n+        self.assertEqual(output_text, EXPECTED_TEXTS)\n+\n+    def test_model_1b_text_only(self):\n+        model_id = \"google/gemma-3-1b-it\"\n+\n+        model = Gemma3nForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\n+            torch_device\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n+        inputs = tokenizer(\"Write a poem about Machine Learning.\", return_tensors=\"pt\").to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n+\n+        EXPECTED_TEXTS = ['Write a poem about Machine Learning.\\n\\n---\\n\\nThe data flows, a river deep,\\nWith patterns hidden, secrets sleep.\\nA neural net, a watchful eye,\\nLearning']  # fmt: skip\n+        self.assertEqual(output_text, EXPECTED_TEXTS)\n+\n+    # TODO: raushan FA2 generates gibberish for no reason, check later\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n+    def test_model_4b_flash_attn(self):\n+        model_id = \"Google/gemma-3n-E4B-it\"\n+\n+        model = Gemma3nForConditionalGeneration.from_pretrained(\n+            model_id, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+        ).to(torch_device)\n+\n+        inputs = self.processor.apply_chat_template(\n+            self.messages,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            add_generation_prompt=True,\n+        ).to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n+\n+        EXPECTED_TEXTS = ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach next to a turquoise ocean. It looks like a very sunny and']  # fmt: skip\n+        self.assertEqual(output_text, EXPECTED_TEXTS)\n+\n+    @parameterized.expand([(\"flash_attention_2\",), (\"sdpa\",), (\"eager\",)])\n+    def test_generation_beyond_sliding_window(self, attn_implementation: str):\n+        \"\"\"Test that we can correctly generate beyond the sliding window. This is non trivial as\n+        we need to correctly slice the attention mask in all cases (because we use a HybridCache).\n+        Outputs for every attention functions should be coherent and identical.\n+        \"\"\"\n+        model_id = \"google/gemma-3-1b-it\"\n+\n+        input_text = [\n+            \"This is a nice place. \" * 800 + \"I really enjoy the scenery,\",  # This is larger than 4096 tokens\n+            \"A list of colors: red, blue\",  # This will almost all be padding tokens\n+        ]\n+        tokenizer = AutoTokenizer.from_pretrained(model_id, padding=\"left\")\n+        inputs = tokenizer(input_text, padding=True, return_tensors=\"pt\").to(torch_device)\n+\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id, attn_implementation=attn_implementation, torch_dtype=torch.float16\n+        ).to(torch_device)\n+\n+        # Make sure prefill is larger than sliding window\n+        input_size = inputs.input_ids.shape[-1]\n+        self.assertTrue(input_size > model.config.sliding_window)\n+\n+        out = model.generate(**inputs, max_new_tokens=20)[:, input_size:]\n+        output_text = tokenizer.batch_decode(out)\n+\n+        EXPECTED_COMPLETIONS = [\" and I'm going to take a walk.\\n\\nI really enjoy the scenery, and I'\", \", green, yellow, orange, purple, brown, black, white, gray.\\n\\nI'\"]  # fmt: skip\n+        self.assertEqual(output_text, EXPECTED_COMPLETIONS)\n+\n+    def test_generation_beyond_sliding_window_with_generation_config(self):\n+        \"\"\"\n+        Same as `test_generation_beyond_sliding_window`, but passing a GenerationConfig. Regression test for #36684 --\n+        ensures `cache_implementation='hybrid'` is correctly inherited from the base `model.generation_config`.\n+        \"\"\"\n+        model_id = \"google/gemma-3-1b-it\"\n+        attn_implementation = \"sdpa\"\n+\n+        input_text = [\n+            \"This is a nice place. \" * 800 + \"I really enjoy the scenery,\",  # This is larger than 4096 tokens\n+            \"A list of colors: red, blue\",  # This will almost all be padding tokens\n+        ]\n+        tokenizer = AutoTokenizer.from_pretrained(model_id, padding=\"left\")\n+        inputs = tokenizer(input_text, padding=True, return_tensors=\"pt\").to(torch_device)\n+\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id, attn_implementation=attn_implementation, torch_dtype=torch.float16\n+        ).to(torch_device)\n+\n+        # Make sure prefill is larger than sliding window\n+        input_size = inputs.input_ids.shape[-1]\n+        self.assertTrue(input_size > model.config.sliding_window)\n+\n+        generation_config = GenerationConfig(max_new_tokens=20)\n+\n+        out = model.generate(**inputs, generation_config=generation_config)[:, input_size:]\n+        output_text = tokenizer.batch_decode(out)\n+\n+        EXPECTED_COMPLETIONS = [\" and I'm going to take a walk.\\n\\nI really enjoy the scenery, and I'\", \", green, yellow, orange, purple, brown, black, white, gray.\\n\\nI'\"]  # fmt: skip\n+        self.assertEqual(output_text, EXPECTED_COMPLETIONS)"
        },
        {
            "sha": "1d30a80c4896a2375a670bfadf4ccde6cb7c1fc6",
            "filename": "tests/models/gemma3n/test_processing_gemma3n.py",
            "status": "added",
            "additions": 185,
            "deletions": 0,
            "changes": 185,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/tests%2Fmodels%2Fgemma3n%2Ftest_processing_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/tests%2Fmodels%2Fgemma3n%2Ftest_processing_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_processing_gemma3n.py?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -0,0 +1,185 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import shutil\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+from parameterized import parameterized\n+\n+from transformers import GemmaTokenizerFast, SiglipImageProcessorFast, is_speech_available\n+from transformers.testing_utils import require_sentencepiece, require_torch, require_torchaudio, require_vision\n+\n+from .test_feature_extraction_gemma3n import floats_list\n+\n+\n+if is_speech_available():\n+    from transformers.models.gemma3n import Gemma3nAudioFeatureExtractor, Gemma3nProcessor\n+\n+\n+@require_torch\n+@require_torchaudio\n+@require_vision\n+@require_sentencepiece\n+class Gemma3nProcessorTest(unittest.TestCase):\n+    def setUp(self):\n+        # TODO: update to google?\n+        self.model_id = \"Google/gemma-3n-E4B-it\"\n+        self.tmpdirname = tempfile.mkdtemp(suffix=\"gemma3n\")\n+        self.maxDiff = None\n+\n+    def get_tokenizer(self, **kwargs):\n+        return GemmaTokenizerFast.from_pretrained(self.model_id, **kwargs)\n+\n+    def get_feature_extractor(self, **kwargs):\n+        return Gemma3nAudioFeatureExtractor.from_pretrained(self.model_id, **kwargs)\n+\n+    def get_image_processor(self, **kwargs):\n+        return SiglipImageProcessorFast.from_pretrained(self.model_id, **kwargs)\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    def test_save_load_pretrained_default(self):\n+        # NOTE: feature_extractor and image_processor both use the same filename, preprocessor_config.json, when saved to\n+        # disk, but the files are overwritten by processor.save_pretrained(). This test does not attempt to address\n+        # this potential issue, and as such, does not guarantee content accuracy.\n+\n+        tokenizer = self.get_tokenizer()\n+        feature_extractor = self.get_feature_extractor()\n+        image_processor = self.get_image_processor()\n+\n+        processor = Gemma3nProcessor(\n+            tokenizer=tokenizer, feature_extractor=feature_extractor, image_processor=image_processor\n+        )\n+\n+        processor.save_pretrained(self.tmpdirname)\n+        processor = Gemma3nProcessor.from_pretrained(self.tmpdirname)\n+\n+        self.assertIsInstance(processor.tokenizer, GemmaTokenizerFast)\n+        self.assertEqual(processor.tokenizer.get_vocab(), tokenizer.get_vocab())\n+\n+        self.assertIsInstance(processor.feature_extractor, Gemma3nAudioFeatureExtractor)\n+        self.assertEqual(processor.feature_extractor.to_json_string(), feature_extractor.to_json_string())\n+\n+    def test_save_load_pretrained_additional_features(self):\n+        tokenizer = self.get_tokenizer()\n+        feature_extractor = self.get_feature_extractor()\n+        image_processor = self.get_image_processor()\n+\n+        processor = Gemma3nProcessor(\n+            tokenizer=tokenizer, feature_extractor=feature_extractor, image_processor=image_processor\n+        )\n+        processor.save_pretrained(self.tmpdirname)\n+\n+        tokenizer_add_kwargs = self.get_tokenizer(bos_token=\"(BOS-BOS)\", eos_token=\"(EOS-EOS)\")\n+        feature_extractor_add_kwargs = self.get_feature_extractor(dither=5.0, padding_value=1.0)\n+\n+        processor = Gemma3nProcessor.from_pretrained(\n+            self.tmpdirname, bos_token=\"(BOS-BOS)\", eos_token=\"(EOS-EOS)\", dither=5.0, padding_value=1.0\n+        )\n+\n+        self.assertEqual(processor.tokenizer.get_vocab(), tokenizer_add_kwargs.get_vocab())\n+        self.assertIsInstance(processor.tokenizer, GemmaTokenizerFast)\n+\n+        self.assertEqual(processor.feature_extractor.to_json_string(), feature_extractor_add_kwargs.to_json_string())\n+        self.assertIsInstance(processor.feature_extractor, Gemma3nAudioFeatureExtractor)\n+\n+    @parameterized.expand([256, 512, 768, 1024])\n+    def test_image_processor(self, image_size: int):\n+        feature_extractor = self.get_feature_extractor()\n+        tokenizer = self.get_tokenizer()\n+        image_processor = self.get_image_processor()\n+        processor = Gemma3nProcessor(\n+            tokenizer=tokenizer, feature_extractor=feature_extractor, image_processor=image_processor\n+        )\n+\n+        raw_image = np.random.randint(0, 256, size=(image_size, image_size, 3), dtype=np.uint8)\n+        input_image_processor = image_processor(raw_image, return_tensors=\"pt\")\n+        input_processor = processor(text=\"Describe:\", images=raw_image, return_tensors=\"pt\")\n+\n+        for key in input_image_processor.keys():\n+            self.assertAlmostEqual(input_image_processor[key].sum(), input_processor[key].sum(), delta=1e-2)\n+            if \"pixel_values\" in key:\n+                # NOTE: all images should be re-scaled to 768x768\n+                self.assertEqual(input_image_processor[key].shape, (1, 3, 768, 768))\n+                self.assertEqual(input_processor[key].shape, (1, 3, 768, 768))\n+\n+    def test_audio_feature_extractor(self):\n+        feature_extractor = self.get_feature_extractor()\n+        tokenizer = self.get_tokenizer()\n+        image_processor = self.get_image_processor()\n+        processor = Gemma3nProcessor(\n+            tokenizer=tokenizer, feature_extractor=feature_extractor, image_processor=image_processor\n+        )\n+\n+        raw_speech = floats_list((3, 1000))\n+        input_feat_extract = feature_extractor(raw_speech, return_tensors=\"pt\")\n+        input_processor = processor(text=\"Transcribe:\", audio=raw_speech, return_tensors=\"pt\")\n+\n+        for key in input_feat_extract.keys():\n+            self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)\n+\n+    def test_tokenizer(self):\n+        feature_extractor = self.get_feature_extractor()\n+        tokenizer = self.get_tokenizer()\n+        image_processor = self.get_image_processor()\n+        processor = Gemma3nProcessor(\n+            tokenizer=tokenizer, feature_extractor=feature_extractor, image_processor=image_processor\n+        )\n+\n+        input_str = \"This is a test string\"\n+\n+        encoded_processor = processor(text=input_str)\n+\n+        encoded_tok = tokenizer(input_str)\n+\n+        for key in encoded_tok.keys():\n+            self.assertListEqual(encoded_tok[key], encoded_processor[key][0])\n+\n+    def test_tokenizer_decode(self):\n+        feature_extractor = self.get_feature_extractor()\n+        tokenizer = self.get_tokenizer()\n+        image_processor = self.get_image_processor()\n+        processor = Gemma3nProcessor(\n+            tokenizer=tokenizer, feature_extractor=feature_extractor, image_processor=image_processor\n+        )\n+\n+        predicted_ids = [[1, 4, 5, 8, 1, 0, 8], [3, 4, 3, 1, 1, 8, 9]]\n+\n+        decoded_processor = processor.batch_decode(predicted_ids)\n+        decoded_tok = tokenizer.batch_decode(predicted_ids)\n+\n+        self.assertListEqual(decoded_tok, decoded_processor)\n+\n+    def test_model_input_names(self):\n+        feature_extractor = self.get_feature_extractor()\n+        tokenizer = self.get_tokenizer()\n+        image_processor = self.get_image_processor()\n+        processor = Gemma3nProcessor(\n+            tokenizer=tokenizer, feature_extractor=feature_extractor, image_processor=image_processor\n+        )\n+\n+        for key in feature_extractor.model_input_names:\n+            self.assertIn(\n+                key,\n+                processor.model_input_names,\n+            )\n+\n+        for key in image_processor.model_input_names:\n+            self.assertIn(\n+                key,\n+                processor.model_input_names,\n+            )"
        },
        {
            "sha": "04fb04a647388345c47f56bc7675b54c368489d3",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -277,6 +277,7 @@\n     ],\n     \"Llama4VisionConfig\": [\"multi_modal_projector_bias\", \"norm_eps\"],\n     \"SmolLM3Config\": [\"no_rope_layer_interval\"],\n+    \"Gemma3nVisionConfig\": [\"architecture\", \"do_pooling\", \"model_args\"],  # this is for use in `timm`\n }\n \n "
        },
        {
            "sha": "bc247b2b6011383bd8d812e8cf4cb601ba5dd3af",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c63cfd6a833d629a74c098933017c61dd755969d/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c63cfd6a833d629a74c098933017c61dd755969d/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=c63cfd6a833d629a74c098933017c61dd755969d",
            "patch": "@@ -79,6 +79,7 @@\n # docstrings instead. If formatting should be ignored for the docstring, you can put a comment # no-format on the\n # line before the docstring.\n OBJECTS_TO_IGNORE = [\n+    \"Gemma3nVisionConfig\",\n     \"Llama4Processor\",\n     # Deprecated\n     \"InputExample\","
        }
    ],
    "stats": {
        "total": 8723,
        "additions": 8723,
        "deletions": 0
    }
}