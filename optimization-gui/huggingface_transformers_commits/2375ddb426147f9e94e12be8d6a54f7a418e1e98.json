{
    "author": "bzhangGo",
    "message": "T5gemma2 (#41834)\n\n* Fix small bug in T5Gemma 1 in __init__\n\n* Add t5gemma2 model & configurations.\n\n* Add auto support\n\n* Add test case.\n\n* Add doctree.\n\n* Update positional embeddings to match latest update.\n\n* Style fix & add use of final_logit_softcapping for attributes check.\n\n* Update tests and embedding design.\n\n* Add t5gemma2 to image-text-to-text category.\n\n* Add T5Gemma2 doc.\n\n* remove unused imports.\n\n* minor update following comments.\n\n* minor style fixes.\n\n* fix config.\n\n* Update T5Gemma2 following Anton's comments:\n1. Override _prepare_cache_for_generation to take care of cross-attention cache.\n2. Move vision preprocessing from main model to encoder.\n3. Clean and fix bugs in modular model.\n\n* Add T5Gemma2VisionConfig.\n\n* Minor updates.\n\n* fix style\n\n* re-structure vision encoder and minor fixes.\n\n* fix parameter tying.\n\n* remove several unnecessary codes and fix small bugs.\n\n* update and fix init.\n\n* Update weight tying and other minor changes.\n\n* Skip `tie_word_embeddings` in config attributes check in T5Gemma2.\n\n* minor fix.\n\n* fix the inherence of t5gemma2decoderlayer\n\n* sync to head.\n\n* update decorator usage\n\n* disable FA and Flex due to merged module behavior\n\n* style\n\n---------\n\nCo-authored-by: vasqu <antonprogamer@gmail.com>",
    "sha": "2375ddb426147f9e94e12be8d6a54f7a418e1e98",
    "files": [
        {
            "sha": "9d06bfc8ca0496282ffc3b714f5cc2e3c8d72237",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2375ddb426147f9e94e12be8d6a54f7a418e1e98/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/2375ddb426147f9e94e12be8d6a54f7a418e1e98/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=2375ddb426147f9e94e12be8d6a54f7a418e1e98",
            "patch": "@@ -704,6 +704,8 @@\n         title: T5\n       - local: model_doc/t5gemma\n         title: T5Gemma\n+      - local: model_doc/t5gemma2\n+        title: T5Gemma2\n       - local: model_doc/t5v1.1\n         title: T5v1.1\n       - local: model_doc/ul2"
        },
        {
            "sha": "7cf306069a7f98609ccda789e92ae3b6dc655b86",
            "filename": "docs/source/en/model_doc/t5gemma2.md",
            "status": "added",
            "additions": 116,
            "deletions": 0,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/2375ddb426147f9e94e12be8d6a54f7a418e1e98/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2375ddb426147f9e94e12be8d6a54f7a418e1e98/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma2.md?ref=2375ddb426147f9e94e12be8d6a54f7a418e1e98",
            "patch": "@@ -0,0 +1,116 @@\n+\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n+# T5Gemma 2\n+\n+T5Gemma 2 is a family of pretrained encoder-decoder large language models with strong multilingual, multimodal and long-context capability, available in 270M-270M, 1B-1B and 4B-4B parameters. Following T5Gemma, it is built via model adaptation (based on Gemma 3) using UL2. The architecture is similar to T5Gemma and Gemma 3, enhanced with tied word embeddings and merged self- and cross-attention to save model parameters.\n+\n+> [!TIP]\n+> Click on the T5Gemma 2 models in the right sidebar for more examples of how to apply T5Gemma 2 to different language tasks.\n+\n+The example below demonstrates how to chat with the model with [`Pipeline`] or the [`AutoModel`] class, and from the command line.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```python\n+import torch\n+from transformers import pipeline\n+\n+generator = pipeline(\n+    \"image-text-to-text\",\n+    model=\"google/t5gemma-2-270m-270m\",\n+    dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+)\n+\n+generator(\n+    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\",\n+    text=\"<start_of_image> in this image, there is\",\n+    generate_kwargs={\"do_sample\": False, \"max_new_tokens\": 50},\n+)\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```python\n+import torch\n+import requests\n+from PIL import Image\n+from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n+\n+processor = AutoProcessor.from_pretrained(\"google/t5gemma-2-270m-270m\")\n+model = AutoModelForSeq2SeqLM.from_pretrained(\n+    \"google/t5gemma-2-270m-270m\",\n+    device_map=\"auto\",\n+    dtype=torch.bfloat16,\n+)\n+\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+prompt = \"<start_of_image> in this image, there is\"\n+\n+model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n+generation = model.generate(**model_inputs, max_new_tokens=20, do_sample=False)\n+print(processor.decode(generation[0]))\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## T5Gemma2Config\n+\n+[[autodoc]] T5Gemma2Config\n+\n+## T5Gemma2TextConfig\n+\n+[[autodoc]] T5Gemma2TextConfig\n+\n+## T5Gemma2EncoderConfig\n+\n+[[autodoc]] T5Gemma2EncoderConfig\n+\n+## T5Gemma2DecoderConfig\n+[[autodoc]] T5Gemma2DecoderConfig\n+\n+## T5Gemma2Model\n+\n+[[autodoc]] T5Gemma2Model\n+    - forward\n+\n+## T5Gemma2ForConditionalGeneration\n+\n+[[autodoc]] T5Gemma2ForConditionalGeneration\n+    - forward\n+\n+## T5Gemma2ForSequenceClassification\n+\n+[[autodoc]] T5Gemma2ForSequenceClassification\n+    - forward\n+\n+## T5Gemma2ForTokenClassification\n+\n+[[autodoc]] T5Gemma2ForTokenClassification\n+    - forward"
        },
        {
            "sha": "4cca929ce6ce75a065c50f2e2e2cb17e46cf9717",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=2375ddb426147f9e94e12be8d6a54f7a418e1e98",
            "patch": "@@ -342,6 +342,7 @@\n     from .switch_transformers import *\n     from .t5 import *\n     from .t5gemma import *\n+    from .t5gemma2 import *\n     from .table_transformer import *\n     from .tapas import *\n     from .textnet import *"
        },
        {
            "sha": "f5b5f316a8b139e090fbb10c48e6f25eba6c31db",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=2375ddb426147f9e94e12be8d6a54f7a418e1e98",
            "patch": "@@ -395,6 +395,7 @@\n         (\"switch_transformers\", \"SwitchTransformersConfig\"),\n         (\"t5\", \"T5Config\"),\n         (\"t5gemma\", \"T5GemmaConfig\"),\n+        (\"t5gemma2\", \"T5Gemma2Config\"),\n         (\"table-transformer\", \"TableTransformerConfig\"),\n         (\"tapas\", \"TapasConfig\"),\n         (\"textnet\", \"TextNetConfig\"),\n@@ -845,6 +846,7 @@\n         (\"switch_transformers\", \"SwitchTransformers\"),\n         (\"t5\", \"T5\"),\n         (\"t5gemma\", \"T5Gemma\"),\n+        (\"t5gemma2\", \"T5Gemma2\"),\n         (\"t5v1.1\", \"T5v1.1\"),\n         (\"table-transformer\", \"Table Transformer\"),\n         (\"tapas\", \"TAPAS\"),"
        },
        {
            "sha": "88949a23b2d0b1f4e49be83501230b4227b43860",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=2375ddb426147f9e94e12be8d6a54f7a418e1e98",
            "patch": "@@ -189,6 +189,7 @@\n             (\"swin\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"swin2sr\", (\"Swin2SRImageProcessor\", \"Swin2SRImageProcessorFast\")),\n             (\"swinv2\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n+            (\"t5gemma2\", (\"Gemma3ImageProcessor\", \"Gemma3ImageProcessorFast\")),\n             (\"table-transformer\", (\"DetrImageProcessor\", \"DetrImageProcessorFast\")),\n             (\"textnet\", (\"TextNetImageProcessor\", \"TextNetImageProcessorFast\")),\n             (\"timesformer\", (\"VideoMAEImageProcessor\", None)),"
        },
        {
            "sha": "f83a8e8c470c2bddf0af67c87407993092674230",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=2375ddb426147f9e94e12be8d6a54f7a418e1e98",
            "patch": "@@ -383,6 +383,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"switch_transformers\", \"SwitchTransformersModel\"),\n         (\"t5\", \"T5Model\"),\n         (\"t5gemma\", \"T5GemmaModel\"),\n+        (\"t5gemma2\", \"T5Gemma2Model\"),\n         (\"table-transformer\", \"TableTransformerModel\"),\n         (\"tapas\", \"TapasModel\"),\n         (\"textnet\", \"TextNetModel\"),\n@@ -506,6 +507,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"switch_transformers\", \"SwitchTransformersForConditionalGeneration\"),\n         (\"t5\", \"T5ForConditionalGeneration\"),\n         (\"t5gemma\", \"T5GemmaForConditionalGeneration\"),\n+        (\"t5gemma2\", \"T5Gemma2ForConditionalGeneration\"),\n         (\"tapas\", \"TapasForMaskedLM\"),\n         (\"unispeech\", \"UniSpeechForPreTraining\"),\n         (\"unispeech-sat\", \"UniSpeechSatForPreTraining\"),\n@@ -599,6 +601,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"switch_transformers\", \"SwitchTransformersForConditionalGeneration\"),\n         (\"t5\", \"T5ForConditionalGeneration\"),\n         (\"t5gemma\", \"T5GemmaForConditionalGeneration\"),\n+        (\"t5gemma2\", \"T5Gemma2ForConditionalGeneration\"),\n         (\"tapas\", \"TapasForMaskedLM\"),\n         (\"wav2vec2\", \"Wav2Vec2ForMaskedLM\"),\n         (\"whisper\", \"WhisperForConditionalGeneration\"),\n@@ -1027,6 +1030,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"qwen3_vl_moe\", \"Qwen3VLMoeForConditionalGeneration\"),\n         (\"shieldgemma2\", \"Gemma3ForConditionalGeneration\"),\n         (\"smolvlm\", \"SmolVLMForConditionalGeneration\"),\n+        (\"t5gemma2\", \"T5Gemma2ForConditionalGeneration\"),\n         (\"udop\", \"UdopForConditionalGeneration\"),\n         (\"video_llama_3\", \"VideoLlama3ForConditionalGeneration\"),\n         (\"vipllava\", \"VipLlavaForConditionalGeneration\"),\n@@ -1164,6 +1168,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"switch_transformers\", \"SwitchTransformersForConditionalGeneration\"),\n         (\"t5\", \"T5ForConditionalGeneration\"),\n         (\"t5gemma\", \"T5GemmaForConditionalGeneration\"),\n+        (\"t5gemma2\", \"T5Gemma2ForConditionalGeneration\"),\n         (\"umt5\", \"UMT5ForConditionalGeneration\"),\n         (\"voxtral\", \"VoxtralForConditionalGeneration\"),\n     ]\n@@ -1287,6 +1292,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"starcoder2\", \"Starcoder2ForSequenceClassification\"),\n         (\"t5\", \"T5ForSequenceClassification\"),\n         (\"t5gemma\", \"T5GemmaForSequenceClassification\"),\n+        (\"t5gemma2\", \"T5Gemma2ForSequenceClassification\"),\n         (\"tapas\", \"TapasForSequenceClassification\"),\n         (\"umt5\", \"UMT5ForSequenceClassification\"),\n         (\"xlm\", \"XLMForSequenceClassification\"),\n@@ -1485,6 +1491,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"starcoder2\", \"Starcoder2ForTokenClassification\"),\n         (\"t5\", \"T5ForTokenClassification\"),\n         (\"t5gemma\", \"T5GemmaForTokenClassification\"),\n+        (\"t5gemma2\", \"T5Gemma2ForTokenClassification\"),\n         (\"umt5\", \"UMT5ForTokenClassification\"),\n         (\"xlm\", \"XLMForTokenClassification\"),\n         (\"xlm-roberta\", \"XLMRobertaForTokenClassification\"),"
        },
        {
            "sha": "023868aa83f6dbfcb2fe0a9439fc9903f8257214",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=2375ddb426147f9e94e12be8d6a54f7a418e1e98",
            "patch": "@@ -140,6 +140,7 @@\n         (\"smolvlm\", \"SmolVLMProcessor\"),\n         (\"speech_to_text\", \"Speech2TextProcessor\"),\n         (\"speecht5\", \"SpeechT5Processor\"),\n+        (\"t5gemma2\", \"Gemma3Processor\"),\n         (\"trocr\", \"TrOCRProcessor\"),\n         (\"tvp\", \"TvpProcessor\"),\n         (\"udop\", \"UdopProcessor\"),"
        },
        {
            "sha": "0688bdb54cbedec4ae03e3a68331117656ec666f",
            "filename": "src/transformers/models/t5gemma/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Ft5gemma%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Ft5gemma%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2F__init__.py?ref=2375ddb426147f9e94e12be8d6a54f7a418e1e98",
            "patch": "@@ -18,8 +18,8 @@\n \n \n if TYPE_CHECKING:\n-    from .configuration_encdecgemma2 import *\n-    from .modeling_encdecgemma2 import *\n+    from .configuration_t5gemma import *\n+    from .modeling_t5gemma import *\n else:\n     import sys\n "
        },
        {
            "sha": "7d018bfe722ab8bfbda0c38f681159d0182b6672",
            "filename": "src/transformers/models/t5gemma2/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Ft5gemma2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Ft5gemma2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma2%2F__init__.py?ref=2375ddb426147f9e94e12be8d6a54f7a418e1e98",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_t5gemma2 import *\n+    from .modeling_t5gemma2 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "7b2c50943adf05968865aa7ca70890995dfdbd2c",
            "filename": "src/transformers/models/t5gemma2/configuration_t5gemma2.py",
            "status": "added",
            "additions": 663,
            "deletions": 0,
            "changes": 663,
            "blob_url": "https://github.com/huggingface/transformers/blob/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fconfiguration_t5gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fconfiguration_t5gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fconfiguration_t5gemma2.py?ref=2375ddb426147f9e94e12be8d6a54f7a418e1e98",
            "patch": "@@ -0,0 +1,663 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/t5gemma2/modular_t5gemma2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_t5gemma2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Any, Optional, Union\n+\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n+from ...modeling_rope_utils import RopeParameters\n+from ...utils import logging\n+from ..siglip import SiglipVisionConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class T5Gemma2TextConfig(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`T5Gemma2TextModel`]. It is used to instantiate an T5Gemma2Text\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the T5Gemma2Text-7B.\n+    e.g. [google/t5gemma2_text-7b](https://huggingface.co/google/t5gemma2_text-7b)\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 262208):\n+            Vocabulary size of the T5Gemma2Text model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`T5Gemma2TextModel`]\n+        hidden_size (`int`, *optional*, defaults to 2304):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 9216):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 26):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 4):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        head_dim (`int`, *optional*, defaults to 256):\n+            The attention head dimension.\n+        hidden_activation (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+            The non-linear activation function (function or string) in the decoder. Will default to `\"gelu_pytorch_tanh\"`\n+            if not specified. `\"gelu_pytorch_tanh\"` uses an approximation of the `\"gelu\"` activation function.\n+        max_position_embeddings (`int`, *optional*, defaults to 131072):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        eos_token_id (`int`, *optional*, defaults to 1):\n+            End of stream token id.\n+        bos_token_id (`int`, *optional*, defaults to 2):\n+            Beginning of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to tie weight embeddings\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        query_pre_attn_scalar (`float`, *optional*, defaults to 256):\n+            Scaling factor used on the attention scores\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            In T5Gemma2Text, every other layer uses sliding window attention. This is the size of the sliding window.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n+        final_logit_softcapping (`float`, *optional*):\n+            Scaling factor when applying tanh softcapping on the logits.\n+        attn_logit_softcapping (`float`, *optional*):\n+            Scaling factor when applying tanh softcapping on the attention scores.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n+        use_bidirectional_attention (`bool`, *optional*, defaults to `False`):\n+            If True, the model will attend to all text tokens instead of using a causal mask. This does not change\n+            behavior for vision tokens.\n+\n+    ```python\n+    >>> from transformers import T5Gemma2TextModel, T5Gemma2TextConfig\n+    >>> # Initializing a T5Gemma2Text t5gemma2_text-7b style configuration\n+    >>> configuration = T5Gemma2TextConfig()\n+    >>> # Initializing a model from the t5gemma2_text-7b style configuration\n+    >>> model = T5Gemma2TextModel(configuration)\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\n+    \"\"\"\n+\n+    model_type = \"t5gemma2_text\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+    default_theta = {\"global\": 1_000_000.0, \"local\": 10_000.0}\n+\n+    def __init__(\n+        self,\n+        vocab_size: Optional[int] = 262_208,\n+        hidden_size: Optional[int] = 2304,\n+        intermediate_size: Optional[int] = 9216,\n+        num_hidden_layers: Optional[int] = 26,\n+        num_attention_heads: Optional[int] = 8,\n+        num_key_value_heads: Optional[int] = 4,\n+        head_dim: Optional[int] = 256,\n+        hidden_activation: Optional[str] = \"gelu_pytorch_tanh\",\n+        max_position_embeddings: Optional[int] = 131_072,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 0,\n+        eos_token_id: Optional[int] = 1,\n+        bos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = True,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        query_pre_attn_scalar: Optional[int] = 256,\n+        sliding_window: Optional[int] = 4096,\n+        layer_types: Optional[list[str]] = None,\n+        final_logit_softcapping: Optional[float] = None,\n+        attn_logit_softcapping: Optional[float] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n+        use_bidirectional_attention: Optional[bool] = False,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.head_dim = head_dim\n+        self.num_key_value_heads = num_key_value_heads\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.hidden_activation = hidden_activation\n+        self.query_pre_attn_scalar = query_pre_attn_scalar\n+        self.sliding_window = sliding_window\n+        self.final_logit_softcapping = final_logit_softcapping\n+        self.attn_logit_softcapping = attn_logit_softcapping\n+        self.layer_types = layer_types\n+\n+        self.use_bidirectional_attention = use_bidirectional_attention\n+        if use_bidirectional_attention:\n+            self.sliding_window = (self.sliding_window // 2) + 1  # due to fa we set exclusive bounds\n+\n+        # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n+        self._sliding_window_pattern = kwargs.get(\"sliding_window_pattern\", 6)\n+\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\" if bool((i + 1) % self._sliding_window_pattern) else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n+\n+        self.rope_parameters = rope_parameters\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation=None, **kwargs):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`. If we find `rope_parameters`\n+        # as arg in the inputs, we can safely assume that it is in the new format. New naming used -> new format\n+        default_rope_params = {\n+            \"sliding_attention\": {\"rope_type\": \"default\"},\n+            \"full_attention\": {\"rope_type\": \"default\"},\n+        }\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else default_rope_params\n+        if rope_scaling is not None:\n+            self.rope_parameters[\"full_attention\"].update(rope_scaling)\n+        self.rope_parameters[\"full_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"rope_theta\", self.default_theta[\"global\"])\n+        )\n+        self.rope_parameters[\"sliding_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"rope_local_base_freq\", self.default_theta[\"local\"])\n+        )\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        self.standardize_rope_params()\n+        self.validate_rope(ignore_keys=ignore_keys_at_rope_validation)\n+        return kwargs\n+\n+\n+class T5Gemma2EncoderConfig(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`T5Gemma2EncoderForConditionalGeneration`]. It is used to instantiate an\n+    T5Gemma2EncoderForConditionalGeneration according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the PaliGemma-2B.\n+\n+    e.g. [google/gemma-3-4b](https://huggingface.co/google/gemma-3-4b)\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`Union[T5Gemma2EncoderTextConfig, dict]`, *optional*):\n+            The config object of the text backbone.\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*):\n+            Custom vision config or dict.\n+        mm_tokens_per_image (`int`, *optional*, defaults to 256):\n+            The number of tokens per image embedding.\n+        boi_token_index (`int`, *optional*, defaults to 255999):\n+            The begin-of-image token index to wrap the image prompt.\n+        eoi_token_index (`int`, *optional*, defaults to 256000):\n+            The end-of-image token index to wrap the image prompt.\n+        image_token_index (`int`, *optional*, defaults to 262144):\n+            The image token index to encode the image prompt.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import T5Gemma2EncoderForConditionalGeneration, T5Gemma2EncoderConfig, SiglipVisionConfig, T5Gemma2EncoderTextConfig\n+\n+    >>> # Initializing a Siglip-like vision config\n+    >>> vision_config = SiglipVisionConfig()\n+\n+    >>> # Initializing a T5Gemma2Encoder Text config\n+    >>> text_config = T5Gemma2EncoderTextConfig()\n+\n+    >>> # Initializing a T5Gemma2Encoder gemma-3-4b style configuration\n+    >>> configuration = T5Gemma2EncoderConfig(vision_config, text_config)\n+\n+    >>> # Initializing a model from the gemma-3-4b style configuration\n+    >>> model = T5Gemma2EncoderTextConfig(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"t5gemma2_encoder\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+        \"boi_token_id\": \"boi_token_index\",\n+        \"eoi_token_id\": \"eoi_token_index\",\n+    }\n+\n+    sub_configs = {\n+        \"text_config\": T5Gemma2TextConfig,\n+        \"vision_config\": SiglipVisionConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        text_config: Optional[Union[T5Gemma2TextConfig, dict[str, Any]]] = None,\n+        vision_config: Optional[Union[SiglipVisionConfig, dict[str, Any]]] = None,\n+        mm_tokens_per_image: int = 256,\n+        boi_token_index: int = 255_999,\n+        eoi_token_index: int = 256_000,\n+        image_token_index: int = 262_144,\n+        initializer_range: float = 0.02,\n+        **kwargs,\n+    ):\n+        if text_config is None:\n+            text_config = T5Gemma2TextConfig()\n+            logger.info(\"text_config is None, using default T5Gemma2EncoderTextConfig text config.\")\n+        elif isinstance(text_config, dict):\n+            text_config = T5Gemma2TextConfig(**text_config)\n+\n+        if isinstance(vision_config, dict):\n+            vision_config = SiglipVisionConfig(**vision_config)\n+        elif vision_config is None:\n+            vision_config = SiglipVisionConfig()\n+            logger.info(\"vision_config is None, using default SiglipVisionConfig vision config.\")\n+\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.mm_tokens_per_image = mm_tokens_per_image\n+        self.boi_token_index = boi_token_index\n+        self.eoi_token_index = eoi_token_index\n+        self.image_token_index = image_token_index\n+        self.initializer_range = initializer_range\n+\n+        super().__init__(**kwargs)\n+\n+\n+class T5Gemma2DecoderConfig(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`T5Gemma2DecoderModel`]. It is used to instantiate an T5Gemma2Decoder\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the T5Gemma2Decoder-7B.\n+    e.g. [google/t5gemma2_text-7b](https://huggingface.co/google/t5gemma2_text-7b)\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 262208):\n+            Vocabulary size of the T5Gemma2Decoder model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`T5Gemma2DecoderModel`]\n+        hidden_size (`int`, *optional*, defaults to 2304):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 9216):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 26):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 4):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        head_dim (`int`, *optional*, defaults to 256):\n+            The attention head dimension.\n+        hidden_activation (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+            The non-linear activation function (function or string) in the decoder. Will default to `\"gelu_pytorch_tanh\"`\n+            if not specified. `\"gelu_pytorch_tanh\"` uses an approximation of the `\"gelu\"` activation function.\n+        max_position_embeddings (`int`, *optional*, defaults to 131072):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        eos_token_id (`int`, *optional*, defaults to 1):\n+            End of stream token id.\n+        bos_token_id (`int`, *optional*, defaults to 2):\n+            Beginning of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to tie weight embeddings\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        query_pre_attn_scalar (`float`, *optional*, defaults to 256):\n+            Scaling factor used on the attention scores\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            In T5Gemma2Decoder, every other layer uses sliding window attention. This is the size of the sliding window.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n+        final_logit_softcapping (`float`, *optional*):\n+            Scaling factor when applying tanh softcapping on the logits.\n+        attn_logit_softcapping (`float`, *optional*):\n+            Scaling factor when applying tanh softcapping on the attention scores.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n+        use_bidirectional_attention (`bool`, *optional*, defaults to `False`):\n+            If True, the model will attend to all text tokens instead of using a causal mask. This does not change\n+            behavior for vision tokens.\n+\n+    ```python\n+    >>> from transformers import T5Gemma2DecoderModel, T5Gemma2DecoderConfig\n+    >>> # Initializing a T5Gemma2Decoder t5gemma2_text-7b style configuration\n+    >>> configuration = T5Gemma2DecoderConfig()\n+    >>> # Initializing a model from the t5gemma2_text-7b style configuration\n+    >>> model = T5Gemma2DecoderModel(configuration)\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\n+    \"\"\"\n+\n+    model_type = \"t5gemma2_decoder\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+    default_theta = {\"global\": 1_000_000.0, \"local\": 10_000.0}\n+\n+    def __init__(\n+        self,\n+        vocab_size: Optional[int] = 262_208,\n+        hidden_size: Optional[int] = 2304,\n+        intermediate_size: Optional[int] = 9216,\n+        num_hidden_layers: Optional[int] = 26,\n+        num_attention_heads: Optional[int] = 8,\n+        num_key_value_heads: Optional[int] = 4,\n+        head_dim: Optional[int] = 256,\n+        hidden_activation: Optional[str] = \"gelu_pytorch_tanh\",\n+        max_position_embeddings: Optional[int] = 131_072,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 0,\n+        eos_token_id: Optional[int] = 1,\n+        bos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = True,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        query_pre_attn_scalar: Optional[int] = 256,\n+        sliding_window: Optional[int] = 4096,\n+        layer_types: Optional[list[str]] = None,\n+        final_logit_softcapping: Optional[float] = None,\n+        attn_logit_softcapping: Optional[float] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n+        use_bidirectional_attention: Optional[bool] = False,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.head_dim = head_dim\n+        self.num_key_value_heads = num_key_value_heads\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.hidden_activation = hidden_activation\n+        self.query_pre_attn_scalar = query_pre_attn_scalar\n+        self.sliding_window = sliding_window\n+        self.final_logit_softcapping = final_logit_softcapping\n+        self.attn_logit_softcapping = attn_logit_softcapping\n+        self.layer_types = layer_types\n+\n+        self.use_bidirectional_attention = use_bidirectional_attention\n+        if use_bidirectional_attention:\n+            self.sliding_window = (self.sliding_window // 2) + 1  # due to fa we set exclusive bounds\n+\n+        # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n+        self._sliding_window_pattern = kwargs.get(\"sliding_window_pattern\", 6)\n+\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\" if bool((i + 1) % self._sliding_window_pattern) else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n+\n+        self.rope_parameters = rope_parameters\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation=None, **kwargs):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`. If we find `rope_parameters`\n+        # as arg in the inputs, we can safely assume that it is in the new format. New naming used -> new format\n+        default_rope_params = {\n+            \"sliding_attention\": {\"rope_type\": \"default\"},\n+            \"full_attention\": {\"rope_type\": \"default\"},\n+        }\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else default_rope_params\n+        if rope_scaling is not None:\n+            self.rope_parameters[\"full_attention\"].update(rope_scaling)\n+        self.rope_parameters[\"full_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"rope_theta\", self.default_theta[\"global\"])\n+        )\n+        self.rope_parameters[\"sliding_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"rope_local_base_freq\", self.default_theta[\"local\"])\n+        )\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        self.standardize_rope_params()\n+        self.validate_rope(ignore_keys=ignore_keys_at_rope_validation)\n+        return kwargs\n+\n+\n+class T5Gemma2Config(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`T5Gemma2Model`]. It is used to instantiate an T5Gemma2\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to a hypothetical balanced Gemma3 encoder-decoder model.\n+    e.g. [google/t5gemma-2-270m-270m](https://huggingface.co/google/t5gemma-2-270m-270m)\n+    Configuration objects inherit from [PreTrainedConfig] and can be used to control the model outputs. Read the\n+    documentation from [PreTrainedConfig] for more information.\n+\n+    Args:\n+        encoder (`Union[T5Gemma2EncoderConfig, dict]`, optional, *optional*):\n+            Configuration for the encoder.\n+        decoder (`Union[T5Gemma2DecoderConfig, dict]`, optional, *optional*):\n+            Configuration for the decoder.\n+        is_encoder_decoder (bool, optional, *optional*, defaults to `True`):\n+            Whether the model is used as an encoder/decoder or not.\n+        dropout_rate (`float`, *optional*, defaults to 0.0):\n+            The ratio for all dropout layers (following T5).\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for attention.\n+        classifier_dropout_rate (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for classifier (following T5).\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        image_token_index (`int`, *optional*, defaults to 256001):\n+            The image token index to encode the image prompt. Defaults to 256001, which is right after the eoi_token_index.\n+            Note this is different from Gemma 3.\n+    ```python\n+    >>> from transformers import T5Gemma2Config, T5Gemma2Model\n+    >>> t5gemma2_config = T5Gemma2Config.from_pretrained(\"google/t5gemma-270m-270m\")\n+    >>> model = T5Gemma2Model(t5gemma2_config)\n+    ```\n+    \"\"\"\n+\n+    model_type = \"t5gemma2\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    sub_configs = {\n+        \"encoder\": T5Gemma2EncoderConfig,\n+        \"decoder\": T5Gemma2DecoderConfig,\n+    }\n+\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+        \"eoi_token_id\": \"eoi_token_index\",\n+    }\n+\n+    def __init__(\n+        self,\n+        encoder: Optional[Union[T5Gemma2EncoderConfig, dict[str, Any]]] = None,\n+        decoder: Optional[Union[T5Gemma2DecoderConfig, dict[str, Any]]] = None,\n+        is_encoder_decoder: bool = True,\n+        dropout_rate: float = 0.0,\n+        attention_dropout: float = 0.0,\n+        classifier_dropout_rate: float = 0.0,\n+        initializer_range: float = 0.02,\n+        image_token_index: int = 256_001,\n+        **kwargs,\n+    ):\n+        if isinstance(encoder, dict):\n+            encoder = T5Gemma2EncoderConfig(**encoder)\n+        elif encoder is None:\n+            encoder = T5Gemma2EncoderConfig()\n+            logger.info(\"encoder is None, using default T5Gemma2EncoderConfig encoder config.\")\n+        else:\n+            if not isinstance(encoder, T5Gemma2EncoderConfig):\n+                raise ValueError(f\"{type(encoder)} is not supported.\")\n+\n+        if isinstance(decoder, dict):\n+            decoder = T5Gemma2DecoderConfig(**decoder)\n+        elif decoder is None:\n+            decoder = T5Gemma2DecoderConfig()\n+            logger.info(\"decoder is None, using default T5Gemma2DecoderConfig decoder config.\")\n+        else:\n+            if not isinstance(decoder, T5Gemma2DecoderConfig):\n+                raise ValueError(f\"{type(decoder)} is not supported.\")\n+\n+        if encoder.text_config.hidden_size != decoder.hidden_size:\n+            raise ValueError(\n+                \"Imbalanced encoder-decoder is not supported in T5Gemma2: \"\n+                f\"encoder ({encoder.text_config.hidden_size}) vs decoder ({decoder.hidden_size}).\"\n+            )\n+\n+        if not is_encoder_decoder:\n+            raise ValueError(\"T5Gemma2Model only support encoder-decoder modeling.\")\n+\n+        if encoder.text_config.vocab_size != decoder.vocab_size:\n+            raise ValueError(\n+                \"Imbalanced encoder-decoder vocabulary size is not supported in T5Gemma2: \"\n+                f\"encoder ({encoder.text_config.vocab_size}) vs decoder ({decoder.vocab_size}).\"\n+            )\n+\n+        # Encoder.\n+        encoder.text_config.dropout_rate = dropout_rate\n+        encoder.text_config.attention_dropout = attention_dropout\n+        encoder.vision_config.attention_dropout = attention_dropout\n+        encoder.image_token_index = image_token_index\n+        self.encoder = encoder\n+\n+        # Decoder.\n+        decoder.dropout_rate = dropout_rate\n+        decoder.attention_dropout = attention_dropout\n+        self.decoder = decoder\n+\n+        for special_token_key in [\"bos_token_id\", \"pad_token_id\", \"eos_token_id\", \"vocab_size\"]:\n+            if special_token_key not in kwargs:\n+                kwargs[special_token_key] = getattr(decoder, special_token_key)\n+\n+        super().__init__(**kwargs)\n+\n+        self.is_encoder_decoder = is_encoder_decoder\n+        self.dropout_rate = dropout_rate\n+        self.attention_dropout = attention_dropout\n+        self.classifier_dropout_rate = classifier_dropout_rate\n+        self.initializer_range = initializer_range\n+        self.eoi_token_index = encoder.eoi_token_index\n+        self.image_token_index = image_token_index\n+\n+    def __setattr__(self, key, value):\n+        shared_attr_with_submodules = [\n+            \"output_hidden_states\",\n+            \"output_attentions\",\n+            \"_attn_implementation_internal\",\n+            \"dropout_rate\",\n+            \"attention_dropout\",\n+            \"vocab_size\",\n+            \"dtype\",\n+        ]\n+\n+        if key in shared_attr_with_submodules:\n+            setattr(self.encoder.text_config, key, value)\n+            setattr(self.encoder.vision_config, key, value)\n+            setattr(self.decoder, key, value)\n+            setattr(self.encoder, key, value)\n+        super().__setattr__(key, value)\n+\n+\n+__all__ = [\"T5Gemma2Config\", \"T5Gemma2TextConfig\", \"T5Gemma2EncoderConfig\", \"T5Gemma2DecoderConfig\"]"
        },
        {
            "sha": "cdbee5cb2ea7ee8e0c78136d0113500ceb58e762",
            "filename": "src/transformers/models/t5gemma2/modeling_t5gemma2.py",
            "status": "added",
            "additions": 1572,
            "deletions": 0,
            "changes": 1572,
            "blob_url": "https://github.com/huggingface/transformers/blob/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py?ref=2375ddb426147f9e94e12be8d6a54f7a418e1e98",
            "patch": "@@ -0,0 +1,1572 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/t5gemma2/modular_t5gemma2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_t5gemma2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import copy\n+from collections.abc import Callable\n+from typing import Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ... import initialization as init\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n+from ...generation import GenerationConfig, GenerationMixin, GenerationMode\n+from ...integrations import use_kernel_func_from_hub\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+    BaseModelOutputWithPastAndCrossAttentions,\n+    Seq2SeqLMOutput,\n+    Seq2SeqModelOutput,\n+    SequenceClassifierOutput,\n+    TokenClassifierOutput,\n+)\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import OutputRecorder, check_model_inputs\n+from ..auto import AutoModel\n+from .configuration_t5gemma2 import T5Gemma2Config, T5Gemma2DecoderConfig, T5Gemma2EncoderConfig, T5Gemma2TextConfig\n+\n+\n+class T5Gemma2RMSNorm(nn.Module):\n+    def __init__(self, dim: int, eps: float = 1e-6):\n+        super().__init__()\n+        self.eps = eps\n+        self.weight = nn.Parameter(torch.zeros(dim))\n+\n+    def _norm(self, x):\n+        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n+\n+    def forward(self, x):\n+        output = self._norm(x.float())\n+        # Llama does x.to(float16) * w whilst T5Gemma2 is (x * w).to(float16)\n+        # See https://github.com/huggingface/transformers/pull/29402\n+        output = output * (1.0 + self.weight.float())\n+        return output.type_as(x)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n+\n+\n+class T5Gemma2MLP(nn.Module):\n+    def __init__(self, config: T5Gemma2TextConfig):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_activation]\n+        self.dropout = nn.Dropout(config.dropout_rate)\n+\n+    def forward(self, x):\n+        hidden_states = self.act_fn(self.gate_proj(x)) * self.up_proj(x)\n+        hidden_states = self.dropout(hidden_states)\n+        down_proj = self.down_proj(hidden_states)\n+        return down_proj\n+\n+\n+class T5Gemma2RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: T5Gemma2TextConfig, device=None):\n+        super().__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.layer_types = list(set(config.layer_types))\n+        self.rope_type = {}\n+        for layer_type in self.layer_types:\n+            rope_params = self.config.rope_parameters[layer_type]\n+            if rope_params is None:\n+                continue\n+\n+            self.rope_type[layer_type] = rope_params[\"rope_type\"]\n+            rope_init_fn: Callable = self.compute_default_rope_parameters\n+            if self.rope_type[layer_type] != \"default\":\n+                rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type[layer_type]]\n+            curr_inv_freq, curr_attention_scaling = rope_init_fn(self.config, device, layer_type=layer_type)\n+            self.register_buffer(f\"{layer_type}_inv_freq\", curr_inv_freq, persistent=False)\n+            setattr(self, f\"{layer_type}_original_inv_freq\", curr_inv_freq)\n+            setattr(self, f\"{layer_type}_attention_scaling\", curr_attention_scaling)\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[T5Gemma2TextConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+        layer_type: Optional[str] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+            layer_type (`str`, *optional*):\n+                The current layer type if the model has different RoPE parameters per type.\n+                Should not be used unless `config.layer_types is not None`\n+\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n+        base = config.rope_parameters[layer_type][\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids, layer_type=None):\n+        inv_freq = getattr(self, f\"{layer_type}_inv_freq\")\n+        attention_scaling = getattr(self, f\"{layer_type}_attention_scaling\")\n+\n+        inv_freq_expanded = inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * attention_scaling\n+            sin = emb.sin() * attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    dropout: float = 0.0,\n+    scaling: Optional[float] = None,\n+    softcap: Optional[float] = None,\n+    **kwargs,\n+) -> tuple[torch.Tensor, torch.Tensor]:\n+    if scaling is None:\n+        scaling = module.head_dim**-0.5\n+\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+\n+    if softcap is not None:\n+        attn_weights = attn_weights / softcap\n+        attn_weights = torch.tanh(attn_weights)\n+        attn_weights = attn_weights * softcap\n+    if attention_mask is not None:  # no matter the length, we just slice it\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    # upcast attention to fp32\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    return attn_output, attn_weights\n+\n+\n+class T5Gemma2SelfAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: T5Gemma2TextConfig, layer_idx: int):\n+        super().__init__()\n+        self.layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = config.query_pre_attn_scalar**-0.5\n+        self.attention_dropout = self.config.attention_dropout\n+        self.is_causal = not self.config.use_bidirectional_attention\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.rotary_fn = apply_rotary_pos_emb\n+        self.attn_logit_softcapping = self.config.attn_logit_softcapping\n+        self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n+        self.is_sliding = self.layer_type == \"sliding_attention\"\n+\n+        self.q_norm = T5Gemma2RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n+        self.k_norm = T5Gemma2RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: torch.Tensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        query_states = self.q_norm(query_states)\n+        key_states = self.k_norm(key_states)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=self.attention_dropout if self.training else 0.0,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class T5Gemma2MergedAttention(nn.Module):\n+    \"\"\"Merged self-attention and cross-attention for decoder.\"\"\"\n+\n+    def __init__(self, config: T5Gemma2TextConfig, layer_idx: int):\n+        super().__init__()\n+        self.layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = config.query_pre_attn_scalar**-0.5\n+        self.attention_dropout = self.config.attention_dropout\n+        self.is_causal = not self.config.use_bidirectional_attention\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.rotary_fn = apply_rotary_pos_emb\n+        self.attn_logit_softcapping = self.config.attn_logit_softcapping\n+        self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n+        self.is_sliding = self.layer_type == \"sliding_attention\"\n+\n+        self.q_norm = T5Gemma2RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n+        self.k_norm = T5Gemma2RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        # decoder self-attention inputs\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        merged_attention_mask: Optional[torch.Tensor],\n+        # cross-attention inputs\n+        encoder_hidden_states: torch.Tensor,\n+        # cache inputs\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        # others\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        # attention shapes.\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+        cross_input_shape = encoder_hidden_states.shape[:-1]\n+        cross_hidden_shape = (*cross_input_shape, -1, self.head_dim)\n+\n+        # self-attention.\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        query_states = self.q_norm(query_states)\n+        key_states = self.k_norm(key_states)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            # self-attention.\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            self_attention_cache = past_key_values.self_attention_cache\n+            key_states, value_states = self_attention_cache.update(\n+                key_states, value_states, self.layer_idx, cache_kwargs\n+            )\n+\n+            # cross-attention.\n+            is_updated = past_key_values.is_updated.get(self.layer_idx)\n+            cross_attention_cache = past_key_values.cross_attention_cache\n+\n+        if past_key_values is None or not is_updated:\n+            cross_key_states = self.k_proj(encoder_hidden_states).view(cross_hidden_shape).transpose(1, 2)\n+            cross_value_states = self.v_proj(encoder_hidden_states).view(cross_hidden_shape).transpose(1, 2)\n+\n+            cross_key_states = self.k_norm(cross_key_states)\n+\n+            if past_key_values is not None:\n+                cross_key_states, cross_value_states = cross_attention_cache.update(\n+                    cross_key_states, cross_value_states, self.layer_idx\n+                )\n+                past_key_values.is_updated[self.layer_idx] = True\n+        else:\n+            cross_key_states = cross_attention_cache.layers[self.layer_idx].keys\n+            cross_value_states = cross_attention_cache.layers[self.layer_idx].values\n+\n+        # merged attention.\n+        query_states = query_states\n+        cross_key_size = cross_input_shape[1]\n+        key_states = torch.cat([key_states, cross_key_states], dim=2)\n+        value_states = torch.cat([value_states, cross_value_states], dim=2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            merged_attention_mask,\n+            dropout=self.attention_dropout if self.training else 0.0,\n+            scaling=self.scaling,\n+            is_causal=False,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+\n+        # decompose merged attention weights into self & cross attention weights\n+        if attn_weights is not None:\n+            self_attn_weights = attn_weights[..., :-cross_key_size]\n+            cross_attn_weights = attn_weights[..., -cross_key_size:]\n+        else:\n+            self_attn_weights, cross_attn_weights = None, None\n+        return attn_output, self_attn_weights, cross_attn_weights\n+\n+\n+class T5Gemma2EncoderLayer(GradientCheckpointingLayer):\n+    \"\"\"Encoder sub-layer.\"\"\"\n+\n+    def __init__(self, config, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.attention_type = config.layer_types[layer_idx]\n+\n+        self.self_attn = T5Gemma2SelfAttention(\n+            config=config,\n+            layer_idx=layer_idx,\n+        )\n+        self.pre_self_attn_layernorm = T5Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_self_attn_layernorm = T5Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        self.mlp = T5Gemma2MLP(config)\n+        self.pre_feedforward_layernorm = T5Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_feedforward_layernorm = T5Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        self.dropout = nn.Dropout(config.dropout_rate)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.FloatTensor,]:\n+        residual = hidden_states\n+        hidden_states = self.pre_self_attn_layernorm(hidden_states)\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=None,\n+            **kwargs,\n+        )\n+        hidden_states = self.post_self_attn_layernorm(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+\n+        residual = hidden_states\n+        hidden_states = self.pre_feedforward_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = self.post_feedforward_layernorm(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+        return hidden_states\n+\n+\n+class T5Gemma2DecoderLayer(GradientCheckpointingLayer):\n+    \"\"\"Decoder sub-layer: merged attention instead of vanilla self-attention.\"\"\"\n+\n+    def __init__(self, config, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.attention_type = config.layer_types[layer_idx]\n+\n+        # replace vanilla self-attention with merged attention to support joint cross-attention.\n+        self.self_attn = T5Gemma2MergedAttention(\n+            config=config,\n+            layer_idx=layer_idx,\n+        )\n+        self.pre_self_attn_layernorm = T5Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_self_attn_layernorm = T5Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        self.mlp = T5Gemma2MLP(config)\n+        self.pre_feedforward_layernorm = T5Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_feedforward_layernorm = T5Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        self.dropout = nn.Dropout(config.dropout_rate)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        merged_attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> torch.FloatTensor:\n+        residual = hidden_states\n+        hidden_states = self.pre_self_attn_layernorm(hidden_states)\n+\n+        hidden_states, _, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            merged_attention_mask=merged_attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            encoder_hidden_states=encoder_hidden_states,\n+            **kwargs,\n+        )\n+        hidden_states = self.post_self_attn_layernorm(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+\n+        residual = hidden_states\n+        hidden_states = self.pre_feedforward_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = self.post_feedforward_layernorm(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+        return hidden_states\n+\n+\n+class T5Gemma2LMHead(nn.Module):\n+    \"\"\"Head for language modeling (generation) tasks.\"\"\"\n+\n+    def __init__(self, hidden_size: int, vocab_size: int, bias: bool = False):\n+        super().__init__()\n+        self.out_proj = nn.Linear(hidden_size, vocab_size, bias=bias)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        logits = self.out_proj(hidden_states)\n+        return logits\n+\n+\n+class T5Gemma2ClassificationHead(nn.Module):\n+    \"\"\"Head for sentence-level classification tasks.\"\"\"\n+\n+    def __init__(self, hidden_size: int, num_labels: int, classifier_dropout_rate: float = 0.0):\n+        super().__init__()\n+        self.dropout = nn.Dropout(p=classifier_dropout_rate)\n+        self.out_proj = nn.Linear(hidden_size, num_labels)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = self.out_proj(hidden_states)\n+        return hidden_states\n+\n+\n+class T5Gemma2MultiModalProjector(nn.Module):\n+    def __init__(self, config: T5Gemma2EncoderConfig):\n+        super().__init__()\n+\n+        self.mm_input_projection_weight = nn.Parameter(\n+            torch.zeros(config.vision_config.hidden_size, config.text_config.hidden_size)\n+        )\n+\n+        self.mm_soft_emb_norm = T5Gemma2RMSNorm(\n+            config.vision_config.hidden_size, eps=config.vision_config.layer_norm_eps\n+        )\n+\n+        self.patches_per_image = int(config.vision_config.image_size // config.vision_config.patch_size)\n+        self.tokens_per_side = int(config.mm_tokens_per_image**0.5)\n+        self.kernel_size = self.patches_per_image // self.tokens_per_side\n+        self.avg_pool = nn.AvgPool2d(kernel_size=self.kernel_size, stride=self.kernel_size)\n+\n+    def forward(self, vision_outputs: torch.Tensor):\n+        batch_size, _, seq_length = vision_outputs.shape\n+\n+        reshaped_vision_outputs = vision_outputs.transpose(1, 2)\n+        reshaped_vision_outputs = reshaped_vision_outputs.reshape(\n+            batch_size, seq_length, self.patches_per_image, self.patches_per_image\n+        )\n+        reshaped_vision_outputs = reshaped_vision_outputs.contiguous()\n+\n+        pooled_vision_outputs = self.avg_pool(reshaped_vision_outputs)\n+        pooled_vision_outputs = pooled_vision_outputs.flatten(2)\n+        pooled_vision_outputs = pooled_vision_outputs.transpose(1, 2)\n+\n+        normed_vision_outputs = self.mm_soft_emb_norm(pooled_vision_outputs)\n+\n+        projected_vision_outputs = torch.matmul(normed_vision_outputs, self.mm_input_projection_weight)\n+        return projected_vision_outputs.type_as(vision_outputs)\n+\n+\n+class T5Gemma2TextScaledWordEmbedding(nn.Embedding):\n+    \"\"\"T5Gemma2 Embedding: override to add eoi token embedding separately.\"\"\"\n+\n+    def __init__(\n+        self,\n+        num_embeddings: int,\n+        embedding_dim: int,\n+        padding_idx: int,\n+        embed_scale: float = 1.0,\n+        eoi_token_index: int = 256_000,\n+    ):\n+        super().__init__(num_embeddings, embedding_dim, padding_idx)\n+        self.register_buffer(\"embed_scale\", torch.tensor(embed_scale), persistent=False)\n+        self.eoi_token_index = eoi_token_index\n+        self.eoi_embedding = nn.Parameter(torch.zeros(self.embedding_dim))\n+\n+    def forward(self, input_ids: torch.Tensor):\n+        input_embeddings = super().forward(input_ids) * self.embed_scale.to(self.weight.dtype)\n+        input_embeddings[input_ids == self.eoi_token_index] = self.eoi_embedding.to(input_embeddings.dtype)\n+        return input_embeddings\n+\n+\n+@auto_docstring\n+class T5Gemma2PreTrainedModel(PreTrainedModel):\n+    config: T5Gemma2Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+\n+    _no_split_modules = [\n+        \"T5Gemma2EncoderLayer\",\n+        \"T5Gemma2DecoderLayer\",\n+        \"SiglipVisionEmbeddings\",\n+        \"SiglipEncoderLayer\",\n+        \"SiglipMultiheadAttentionPoolingHead\",\n+    ]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+\n+    # Mask creation is incompatible\n+    # FA due to non-default creation / SWA\n+    _supports_flash_attn = False\n+    _supports_sdpa = True\n+    # Flex due to custom masks not compatible to be merged after creation\n+    _supports_flex_attn = False\n+\n+    _can_compile_fullgraph = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": [T5Gemma2EncoderLayer, T5Gemma2DecoderLayer],\n+        \"attentions\": [\n+            OutputRecorder(T5Gemma2SelfAttention, index=1, layer_name=\"self_attn\"),\n+            OutputRecorder(T5Gemma2MergedAttention, index=1, layer_name=\"self_attn\"),\n+            OutputRecorder(T5Gemma2MergedAttention, index=2, layer_name=\"cross_attn\"),\n+        ],\n+    }\n+    input_modalities = (\"image\", \"text\")\n+\n+    @torch.no_grad()\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, T5Gemma2MultiModalProjector):\n+            init.zeros_(module.mm_input_projection_weight)\n+        elif isinstance(module, T5Gemma2TextScaledWordEmbedding):\n+            init.zeros_(module.eoi_embedding)\n+        elif isinstance(module, T5Gemma2ClassificationHead):\n+            scale = module.out_proj.weight.shape[0] ** -0.5\n+            init.normal_(module.out_proj.weight, mean=0.0, std=self.config.initializer_range * scale)\n+            if hasattr(module.out_proj, \"bias\") and module.out_proj.bias is not None:\n+                init.zeros_(module.out_proj.bias)\n+        # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n+        elif \"RMSNorm\" in module.__class__.__name__:\n+            init.zeros_(module.weight)\n+\n+    def prepare_decoder_input_ids_from_labels(self, input_ids):\n+        \"\"\"\n+        Shifts input_ids to the right, prepends the decoder_start_token_id, and handles\n+        pad_token_id replacement for labels that were -100.\n+        This is a common preparation step for decoder inputs in sequence-to-sequence models.\n+        \"\"\"\n+        decoder_config = self.config.decoder\n+        decoder_start_token_id = decoder_config.bos_token_id\n+        pad_token_id = decoder_config.pad_token_id\n+\n+        if decoder_start_token_id is None:\n+            raise ValueError(\"self.model.config.decoder.bos_token_id has to be defined. \")\n+\n+        # shift inputs to the right\n+        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n+        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n+        shifted_input_ids[..., 0] = decoder_start_token_id\n+\n+        if pad_token_id is None:\n+            raise ValueError(\"self.model.config.decoder.pad_token_id has to be defined.\")\n+\n+        # Is this T5 specific?\n+        # replace possible -100 values in labels by `pad_token_id`\n+        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n+\n+        return shifted_input_ids\n+\n+\n+def sliding_window_mask_function(sliding_window: int, is_causal=True) -> Callable:\n+    \"\"\"\n+    This creates uni/bidirectional attention mask with sliding window.\n+    \"\"\"\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        if is_causal:\n+            left_window_size, right_window_size = sliding_window, 0\n+        else:\n+            left_window_size, right_window_size = ((sliding_window + 1) // 2, (sliding_window) // 2 + 1)\n+\n+        dist = q_idx - kv_idx\n+        left_mask = (dist >= 0) & (dist < left_window_size)\n+        right_mask = (dist < 0) & (-dist < right_window_size)\n+        return left_mask | right_mask\n+\n+    return inner_mask\n+\n+\n+class T5Gemma2Encoder(T5Gemma2PreTrainedModel):\n+    config: T5Gemma2EncoderConfig\n+    _can_record_outputs = {\n+        \"attentions\": T5Gemma2SelfAttention,\n+        \"hidden_states\": T5Gemma2EncoderLayer,\n+    }\n+\n+    def __init__(\n+        self,\n+        config: T5Gemma2EncoderConfig,\n+        eoi_token_index: int = 256_000,\n+    ):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.text_config.vocab_size\n+\n+        vision_config = config.vision_config\n+        text_config = config.text_config\n+\n+        # setup vision tower\n+        self.vision_tower = AutoModel.from_config(config=vision_config)\n+        self.multi_modal_projector = T5Gemma2MultiModalProjector(config)\n+\n+        self.embed_tokens = T5Gemma2TextScaledWordEmbedding(\n+            text_config.vocab_size,\n+            text_config.hidden_size,\n+            self.padding_idx,\n+            embed_scale=text_config.hidden_size**0.5,\n+            eoi_token_index=eoi_token_index,\n+        )\n+        self.norm = T5Gemma2RMSNorm(text_config.hidden_size, eps=text_config.rms_norm_eps)\n+        self.gradient_checkpointing = False\n+\n+        self.layers = nn.ModuleList(\n+            [T5Gemma2EncoderLayer(text_config, layer_idx) for layer_idx in range(text_config.num_hidden_layers)]\n+        )\n+        self.dropout = nn.Dropout(text_config.dropout_rate)\n+        self.rotary_emb = T5Gemma2RotaryEmbedding(text_config)\n+\n+        self.text_config = text_config\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n+        \"\"\"Convert pixel image to image features via the encoder and projector.\"\"\"\n+        # pixel_values: (batch_size, channels, height, width)\n+        # image_features: Image feature tensor of shape (num_images, image_length, embed_dim).\n+        vision_outputs = self.vision_tower(pixel_values=pixel_values).last_hidden_state\n+        image_features = self.multi_modal_projector(vision_outputs)\n+        return image_features\n+\n+    def get_image_placeholder_mask(\n+        self,\n+        input_ids: Optional[torch.LongTensor],\n+        inputs_embeds: Optional[torch.FloatTensor],\n+        image_features: torch.FloatTensor,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        image_token_id = self.config.image_token_id\n+        if input_ids is None:\n+            if inputs_embeds is None:\n+                raise ValueError(\"Either `input_ids` or `inputs_embeds` has to be provided.\")\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n+    def preprocess_image_features(\n+        self,\n+        pixel_values: torch.Tensor,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+    ):\n+        \"\"\"Convert pixel images to image features and merge into input embeds.\"\"\"\n+        image_features = self.get_image_features(pixel_values)\n+        image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+\n+        image_mask = self.get_image_placeholder_mask(\n+            input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+        )\n+\n+        inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_features)\n+        return inputs_embeds\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        # Unused for processor compatibility kept in signature.\n+        token_type_ids: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutput:\n+        del token_type_ids\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        # As we want to pass `past_key_values=None` explicitly everywhere, we need to pop them from kwargs if present\n+        kwargs.pop(\"past_key_values\", None)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if pixel_values is not None:\n+            inputs_embeds = self.preprocess_image_features(\n+                pixel_values, input_ids=input_ids, inputs_embeds=inputs_embeds\n+            )\n+\n+        if position_ids is None:\n+            position_ids = torch.arange(0, inputs_embeds.shape[1], device=inputs_embeds.device).unsqueeze(0)\n+\n+        if not isinstance(self_attn_mask_mapping := attention_mask, dict):\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+            }\n+            self_attn_mask_mapping = {\n+                \"full_attention\": create_bidirectional_mask(**mask_kwargs),\n+                \"sliding_attention\": create_bidirectional_mask(\n+                    **mask_kwargs,\n+                    and_mask_function=sliding_window_mask_function(self.text_config.sliding_window, is_causal=False),\n+                ),\n+            }\n+\n+        # input layer\n+        hidden_states = inputs_embeds\n+\n+        # global and local position embeddings\n+        position_embeddings = {}\n+        for layer_type in self.text_config.layer_types:\n+            position_embeddings[layer_type] = self.rotary_emb(hidden_states, position_ids, layer_type)\n+\n+        # dropout\n+        hidden_states = self.dropout(hidden_states)\n+\n+        for layer_module in self.layers[: self.text_config.num_hidden_layers]:\n+            hidden_states = layer_module(\n+                hidden_states,\n+                position_embeddings[layer_module.attention_type],\n+                self_attn_mask_mapping[layer_module.attention_type],\n+                position_ids,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+        )\n+\n+\n+def bidirectional_mask_function(attention_mask: Optional[torch.Tensor]) -> Callable:\n+    \"\"\"\n+    This creates bidirectional attention mask.\n+    \"\"\"\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        if attention_mask is None:\n+            return torch.ones((), dtype=torch.bool)\n+        return attention_mask[batch_idx, kv_idx].to(torch.bool)\n+\n+    return inner_mask\n+\n+\n+class T5Gemma2Decoder(T5Gemma2PreTrainedModel):\n+    config: T5Gemma2DecoderConfig\n+    _can_record_outputs = {\n+        \"attentions\": OutputRecorder(T5Gemma2MergedAttention, index=1),\n+        \"cross_attentions\": OutputRecorder(T5Gemma2MergedAttention, index=2),\n+        \"hidden_states\": T5Gemma2DecoderLayer,\n+    }\n+\n+    def __init__(self, config: T5Gemma2DecoderConfig, eoi_token_index: int = 256_000):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = T5Gemma2TextScaledWordEmbedding(\n+            config.vocab_size,\n+            config.hidden_size,\n+            config.pad_token_id,\n+            embed_scale=config.hidden_size**0.5,\n+            eoi_token_index=eoi_token_index,\n+        )\n+        self.norm = T5Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.gradient_checkpointing = False\n+\n+        self.layers = nn.ModuleList(\n+            [T5Gemma2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.dropout = nn.Dropout(config.dropout_rate)\n+        self.rotary_emb = T5Gemma2RotaryEmbedding(config)\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPastAndCrossAttentions:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+        if encoder_hidden_states is None:\n+            raise ValueError(\"`encoder_hidden_states` must be given in decoder\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if not self.training and use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache())\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        if not isinstance(self_attn_mask_mapping := attention_mask, dict):\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values.self_attention_cache if past_key_values is not None else None,\n+                \"position_ids\": position_ids,\n+            }\n+            # this masking function did nothing to masking but forces `allow_is_causal_skip` to be False\n+            # as we always need a mask during decoding for merged attention.\n+            mask_kwargs[\"and_mask_function\"] = lambda *args: torch.tensor(True, dtype=torch.bool)\n+            self_attn_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n+\n+        if not isinstance(cross_attn_mask_mapping := encoder_attention_mask, dict):\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": encoder_hidden_states,\n+                \"attention_mask\": encoder_attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": None,\n+                \"position_ids\": None,\n+            }\n+            cross_attn_mask_mapping = {\n+                \"full_attention\": create_causal_mask(\n+                    **mask_kwargs,\n+                    or_mask_function=bidirectional_mask_function(encoder_attention_mask),\n+                ),\n+            }\n+\n+        merged_attn_mask_mapping = {\n+            \"full_attention\": torch.cat(\n+                [self_attn_mask_mapping[\"full_attention\"], cross_attn_mask_mapping[\"full_attention\"]], dim=-1\n+            ),\n+            \"sliding_attention\": torch.cat(\n+                [self_attn_mask_mapping[\"sliding_attention\"], cross_attn_mask_mapping[\"full_attention\"]], dim=-1\n+            ),\n+        }\n+\n+        # input layer\n+        hidden_states = inputs_embeds\n+\n+        # global and local position embeddings\n+        position_embeddings = {}\n+        for layer_type in self.config.layer_types:\n+            position_embeddings[layer_type] = self.rotary_emb(hidden_states, position_ids, layer_type)\n+\n+        # dropout\n+        hidden_states = self.dropout(hidden_states)\n+\n+        for layer_module in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = layer_module(\n+                hidden_states,\n+                position_embeddings[layer_module.attention_type],\n+                merged_attn_mask_mapping[layer_module.attention_type],\n+                position_ids,\n+                past_key_values,\n+                use_cache,\n+                cache_position,\n+                encoder_hidden_states,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        return BaseModelOutputWithPastAndCrossAttentions(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+@auto_docstring\n+class T5Gemma2Model(T5Gemma2PreTrainedModel):\n+    _tied_weights_keys = {\n+        \"decoder.embed_tokens.weight\": \"encoder.embed_tokens.weight\",\n+        \"decoder.embed_tokens.eoi_embedding\": \"encoder.embed_tokens.eoi_embedding\",\n+    }\n+\n+    def __init__(self, config: T5Gemma2Config):\n+        super().__init__(config)\n+\n+        # setup encoder and decoder\n+        self.encoder = T5Gemma2Encoder(config.encoder, config.eoi_token_index)\n+        self.decoder = T5Gemma2Decoder(config.decoder, config.eoi_token_index)\n+\n+        self.post_init()\n+\n+    def get_encoder(self):\n+        return self.encoder\n+\n+    def get_decoder(self):\n+        return self.decoder\n+\n+    def get_input_embeddings(self):\n+        return self.encoder.get_input_embeddings()\n+\n+    def set_input_embeddings(self, new_embeddings):\n+        return self.encoder.set_input_embeddings(new_embeddings)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        # encoder inputs\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        # decoder inputs\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        # others (mainly inference or cache related)\n+        encoder_outputs: Optional[BaseModelOutput] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        decoder_inputs_embeds: Optional[torch.Tensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Seq2SeqModelOutput:\n+        r\"\"\"\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n+            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        \"\"\"\n+        # encoder\n+        if encoder_outputs is None:\n+            encoder_outputs = self.encoder(\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                inputs_embeds=inputs_embeds,\n+                pixel_values=pixel_values,\n+                return_dict=True,\n+                **kwargs,\n+            )\n+\n+        encoder_hidden_states = encoder_outputs.last_hidden_state\n+\n+        # decoder\n+        decoder_outputs = self.decoder(\n+            input_ids=decoder_input_ids,\n+            attention_mask=decoder_attention_mask,\n+            position_ids=decoder_position_ids,\n+            inputs_embeds=decoder_inputs_embeds,\n+            past_key_values=past_key_values,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=attention_mask,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        return Seq2SeqModelOutput(\n+            last_hidden_state=decoder_outputs.last_hidden_state,\n+            past_key_values=decoder_outputs.past_key_values,\n+            decoder_hidden_states=decoder_outputs.hidden_states,\n+            decoder_attentions=decoder_outputs.attentions,\n+            cross_attentions=decoder_outputs.cross_attentions,\n+            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n+            encoder_hidden_states=encoder_outputs.hidden_states,\n+            encoder_attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+class T5Gemma2ForConditionalGeneration(T5Gemma2PreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = {\n+        \"lm_head.out_proj.weight\": \"model.encoder.embed_tokens.weight\",\n+    }\n+    _tp_plan = {\"lm_head.out_proj\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head.out_proj\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config: T5Gemma2Config):\n+        super().__init__(config)\n+\n+        self.model = T5Gemma2Model(config)\n+        self.vocab_size = config.decoder.vocab_size\n+        self.lm_head = T5Gemma2LMHead(config.decoder.hidden_size, self.vocab_size)\n+        self.loss_type = \"ForMaskedLM\"\n+\n+        self.post_init()\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head.out_proj = new_embeddings\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head.out_proj\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_encoder(self):\n+        return self.model.get_encoder()\n+\n+    def get_decoder(self):\n+        return self.model.get_decoder()\n+\n+    def get_image_features(self, pixel_values):\n+        return self.get_encoder().get_image_features(pixel_values)\n+\n+    @property\n+    def vision_tower(self):\n+        return self.get_encoder().vision_tower\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        # encoder inputs\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        # decoder inputs\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        # others (mainly inference or cache related)\n+        encoder_outputs: Optional[BaseModelOutput] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n+        r\"\"\"\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n+            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        \"\"\"\n+\n+        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n+            # get decoder inputs from shifting lm labels to the right\n+            decoder_input_ids = self.prepare_decoder_input_ids_from_labels(labels)\n+\n+        decoder_outputs: Seq2SeqModelOutput = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            decoder_input_ids=decoder_input_ids,\n+            decoder_attention_mask=decoder_attention_mask,\n+            decoder_position_ids=decoder_position_ids,\n+            encoder_outputs=encoder_outputs,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            decoder_inputs_embeds=decoder_inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = decoder_outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        decoder_config = self.config.decoder\n+        if decoder_config.final_logit_softcapping is not None:\n+            logits = logits / decoder_config.final_logit_softcapping\n+            logits = torch.tanh(logits)\n+            logits = logits * decoder_config.final_logit_softcapping\n+\n+        loss = None\n+        if labels is not None:\n+            # Input has right-shifted so we directly perform masked lm loss\n+            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n+\n+        return Seq2SeqLMOutput(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=decoder_outputs.past_key_values,\n+            decoder_hidden_states=decoder_outputs.decoder_hidden_states,\n+            decoder_attentions=decoder_outputs.decoder_attentions,\n+            cross_attentions=decoder_outputs.cross_attentions,\n+            encoder_last_hidden_state=decoder_outputs.encoder_last_hidden_state,\n+            encoder_hidden_states=decoder_outputs.encoder_hidden_states,\n+            encoder_attentions=decoder_outputs.encoder_attentions,\n+        )\n+\n+    def _prepare_cache_for_generation(\n+        self,\n+        generation_config: GenerationConfig,\n+        model_kwargs: dict,\n+        generation_mode: GenerationMode,\n+        batch_size: int,\n+        max_cache_length: int,\n+    ) -> bool:\n+        \"\"\"Override cache preparation to support T5Gemma2-specific EncoderDecoder Cache.\"\"\"\n+\n+        # Build cache and past_key_values structure first and then override as needed.\n+        super()._prepare_cache_for_generation(\n+            generation_config,\n+            model_kwargs,\n+            generation_mode,\n+            batch_size,\n+            max_cache_length,\n+        )\n+\n+        # If use_cache is False, do not prepare the cache.\n+        if generation_config.use_cache is False:\n+            return\n+\n+        cache_implementation = generation_config.cache_implementation\n+        if cache_implementation is None:\n+            offload_cache = False\n+        else:\n+            offload_cache = \"offloaded\" in generation_config.cache_implementation\n+\n+        # Main change: use full cache for cross-attention.\n+        cross_attn_config = copy.deepcopy(self.config.get_text_config(decoder=True))\n+\n+        # cross-attention does not use sliding window\n+        del cross_attn_config.sliding_window\n+        del cross_attn_config.layer_types\n+\n+        cross_attn_cache_kwargs = {\n+            \"config\": cross_attn_config,\n+            \"offloading\": offload_cache,\n+        }\n+\n+        past_key_values = model_kwargs.get(\"past_key_values\")\n+        if past_key_values is not None:\n+            if not isinstance(past_key_values, EncoderDecoderCache):\n+                raise ValueError(\n+                    \"The `past_key_values` in `model_kwargs` must be of type `EncoderDecoderCache` for T5Gemma2 model.\"\n+                )\n+\n+            # Cache already established, no need to re-initialize.\n+            if len(past_key_values.is_updated) > 0 and past_key_values.is_updated.get(0):\n+                return\n+\n+            cross_attn_cls = type(past_key_values.cross_attention_cache)\n+            if cross_attn_cls == StaticCache:\n+                cross_attn_cache_kwargs[\"max_cache_len\"] = model_kwargs[\"encoder_outputs\"][0].shape[1]\n+            # Update cross-attention cache only (switch from sliding_window to full).\n+            past_key_values.cross_attention_cache = cross_attn_cls(**cross_attn_cache_kwargs)\n+        else:\n+            # Initialize new cache.\n+            model_kwargs[\"past_key_values\"] = EncoderDecoderCache(\n+                DynamicCache(\n+                    **{\n+                        \"config\": self.config.get_text_config(decoder=True),\n+                        \"offloading\": offload_cache,\n+                    }\n+                ),  # self-attention cache\n+                DynamicCache(),  # cross-attention cache\n+            )\n+\n+        if hasattr(self, \"_cache\") and self._cache is not None:\n+            if not isinstance(self._cache, EncoderDecoderCache):\n+                raise ValueError(\"The internal cache must be of type `EncoderDecoderCache` for T5Gemma2 model.\")\n+\n+            self._cache = model_kwargs[\"past_key_values\"]\n+\n+\n+@auto_docstring\n+class T5Gemma2ForSequenceClassification(T5Gemma2PreTrainedModel):\n+    def __init__(self, config: T5Gemma2Config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        self.hidden_size = config.decoder.hidden_size\n+\n+        self.model = T5Gemma2Model(config)\n+\n+        classifier_dropout = getattr(config, \"classifier_dropout_rate\", 0.1)\n+        self.score = T5Gemma2ClassificationHead(self.hidden_size, self.num_labels, classifier_dropout)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.Tensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        encoder_outputs: Optional[BaseModelOutput] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> SequenceClassifierOutput:\n+        r\"\"\"\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n+            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        if inputs_embeds is not None or decoder_inputs_embeds is not None:\n+            raise NotImplementedError(\n+                f\"Passing input embeddings is currently not supported for {self.__class__.__name__}.\"\n+            )\n+\n+        if input_ids is None:\n+            raise ValueError(\"You have to specify input_ids\")\n+\n+        if decoder_input_ids is None:\n+            decoder_input_ids = self.prepare_decoder_input_ids_from_labels(input_ids)\n+\n+        outputs: Seq2SeqModelOutput = self.model(\n+            input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            decoder_input_ids=decoder_input_ids,\n+            decoder_attention_mask=decoder_attention_mask,\n+            decoder_position_ids=decoder_position_ids,\n+            encoder_outputs=encoder_outputs,\n+            inputs_embeds=inputs_embeds,\n+            decoder_inputs_embeds=decoder_inputs_embeds,\n+            use_cache=False,\n+            **kwargs,\n+        )\n+\n+        last_hidden_state = outputs.last_hidden_state\n+        hidden_states = outputs.decoder_hidden_states\n+        attentions = outputs.decoder_attentions\n+\n+        logits = self.score(last_hidden_state)\n+\n+        batch_size = input_ids.shape[0]\n+        # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+        non_pad_mask = (decoder_input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+        token_indices = torch.arange(decoder_input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n+        last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n+        last_non_pad_token = torch.clamp(last_non_pad_token, max=decoder_input_ids.shape[-1] - 1)\n+\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n+        return SequenceClassifierOutput(\n+            loss=loss,\n+            logits=pooled_logits,\n+            hidden_states=hidden_states,\n+            attentions=attentions,\n+        )\n+\n+\n+@auto_docstring\n+class T5Gemma2ForTokenClassification(T5Gemma2PreTrainedModel):\n+    def __init__(self, config: T5Gemma2Config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        self.hidden_size = config.decoder.hidden_size\n+\n+        self.model = T5Gemma2Model(config)\n+\n+        classifier_dropout = getattr(config, \"classifier_dropout_rate\", 0.1)\n+        self.score = T5Gemma2ClassificationHead(self.hidden_size, self.num_labels, classifier_dropout)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.Tensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        encoder_outputs: Optional[BaseModelOutput] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> TokenClassifierOutput:\n+        r\"\"\"\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n+            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        if inputs_embeds is not None or decoder_inputs_embeds is not None:\n+            raise NotImplementedError(\n+                f\"Passing input embeddings is currently not supported for {self.__class__.__name__}.\"\n+            )\n+\n+        if input_ids is None:\n+            raise ValueError(\"You have to specify input_ids\")\n+\n+        if decoder_input_ids is None:\n+            decoder_input_ids = self.prepare_decoder_input_ids_from_labels(input_ids)\n+\n+        outputs: Seq2SeqModelOutput = self.model(\n+            input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            decoder_input_ids=decoder_input_ids,\n+            decoder_attention_mask=decoder_attention_mask,\n+            decoder_position_ids=decoder_position_ids,\n+            encoder_outputs=encoder_outputs,\n+            inputs_embeds=inputs_embeds,\n+            decoder_inputs_embeds=decoder_inputs_embeds,\n+            use_cache=False,\n+            **kwargs,\n+        )\n+        last_hidden_state = outputs.last_hidden_state\n+        hidden_states = outputs.decoder_hidden_states\n+        attentions = outputs.decoder_attentions\n+\n+        logits = self.score(last_hidden_state)\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, self.config)\n+\n+        return TokenClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=hidden_states,\n+            attentions=attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"T5Gemma2ForConditionalGeneration\",\n+    \"T5Gemma2Model\",\n+    \"T5Gemma2PreTrainedModel\",\n+    \"T5Gemma2ForSequenceClassification\",\n+    \"T5Gemma2ForTokenClassification\",\n+]"
        },
        {
            "sha": "67c749f584277e4d7051cdf4cf50adbc7553e459",
            "filename": "src/transformers/models/t5gemma2/modular_t5gemma2.py",
            "status": "added",
            "additions": 1342,
            "deletions": 0,
            "changes": 1342,
            "blob_url": "https://github.com/huggingface/transformers/blob/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodular_t5gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2375ddb426147f9e94e12be8d6a54f7a418e1e98/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodular_t5gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodular_t5gemma2.py?ref=2375ddb426147f9e94e12be8d6a54f7a418e1e98",
            "patch": "@@ -0,0 +1,1342 @@\n+# coding=utf-8\n+# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import copy\n+from collections.abc import Callable\n+from typing import Any, Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ... import initialization as init\n+from ...cache_utils import DynamicCache, EncoderDecoderCache, StaticCache\n+from ...configuration_utils import PreTrainedConfig\n+from ...generation import GenerationConfig, GenerationMixin, GenerationMode\n+from ...masking_utils import create_bidirectional_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+    BaseModelOutputWithPastAndCrossAttentions,\n+    Seq2SeqLMOutput,\n+    Seq2SeqModelOutput,\n+    SequenceClassifierOutput,\n+    TokenClassifierOutput,\n+)\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TransformersKwargs,\n+    auto_docstring,\n+    can_return_tuple,\n+    logging,\n+)\n+from ...utils.generic import OutputRecorder, check_model_inputs\n+from ..auto import AutoModel\n+from ..gemma3.configuration_gemma3 import Gemma3Config, Gemma3TextConfig\n+from ..gemma3.modeling_gemma3 import (\n+    Gemma3Attention,\n+    Gemma3MLP,\n+    Gemma3MultiModalProjector,\n+    Gemma3PreTrainedModel,\n+    Gemma3RMSNorm,\n+    Gemma3RotaryEmbedding,\n+    Gemma3TextScaledWordEmbedding,\n+    apply_rotary_pos_emb,\n+    create_causal_mask,\n+    create_sliding_window_causal_mask,\n+    eager_attention_forward,\n+)\n+from ..siglip import SiglipVisionConfig\n+from ..t5gemma.modeling_t5gemma import (\n+    T5GemmaClassificationHead,\n+    T5GemmaEncoderLayer,\n+    T5GemmaLMHead,\n+    bidirectional_mask_function,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class T5Gemma2TextConfig(Gemma3TextConfig):\n+    model_type = \"t5gemma2_text\"\n+\n+\n+class T5Gemma2EncoderConfig(Gemma3Config):\n+    model_type = \"t5gemma2_encoder\"\n+\n+    sub_configs = {\n+        \"text_config\": T5Gemma2TextConfig,\n+        \"vision_config\": SiglipVisionConfig,\n+    }\n+\n+\n+class T5Gemma2DecoderConfig(Gemma3TextConfig):\n+    model_type = \"t5gemma2_decoder\"\n+\n+\n+class T5Gemma2Config(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`T5Gemma2Model`]. It is used to instantiate an T5Gemma2\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to a hypothetical balanced Gemma3 encoder-decoder model.\n+    e.g. [google/t5gemma-2-270m-270m](https://huggingface.co/google/t5gemma-2-270m-270m)\n+    Configuration objects inherit from [PreTrainedConfig] and can be used to control the model outputs. Read the\n+    documentation from [PreTrainedConfig] for more information.\n+\n+    Args:\n+        encoder (`Union[T5Gemma2EncoderConfig, dict]`, optional, *optional*):\n+            Configuration for the encoder.\n+        decoder (`Union[T5Gemma2DecoderConfig, dict]`, optional, *optional*):\n+            Configuration for the decoder.\n+        is_encoder_decoder (bool, optional, *optional*, defaults to `True`):\n+            Whether the model is used as an encoder/decoder or not.\n+        dropout_rate (`float`, *optional*, defaults to 0.0):\n+            The ratio for all dropout layers (following T5).\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for attention.\n+        classifier_dropout_rate (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for classifier (following T5).\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        image_token_index (`int`, *optional*, defaults to 256001):\n+            The image token index to encode the image prompt. Defaults to 256001, which is right after the eoi_token_index.\n+            Note this is different from Gemma 3.\n+    ```python\n+    >>> from transformers import T5Gemma2Config, T5Gemma2Model\n+    >>> t5gemma2_config = T5Gemma2Config.from_pretrained(\"google/t5gemma-270m-270m\")\n+    >>> model = T5Gemma2Model(t5gemma2_config)\n+    ```\n+    \"\"\"\n+\n+    model_type = \"t5gemma2\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    sub_configs = {\n+        \"encoder\": T5Gemma2EncoderConfig,\n+        \"decoder\": T5Gemma2DecoderConfig,\n+    }\n+\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+        \"eoi_token_id\": \"eoi_token_index\",\n+    }\n+\n+    def __init__(\n+        self,\n+        encoder: Optional[Union[T5Gemma2EncoderConfig, dict[str, Any]]] = None,\n+        decoder: Optional[Union[T5Gemma2DecoderConfig, dict[str, Any]]] = None,\n+        is_encoder_decoder: bool = True,\n+        dropout_rate: float = 0.0,\n+        attention_dropout: float = 0.0,\n+        classifier_dropout_rate: float = 0.0,\n+        initializer_range: float = 0.02,\n+        image_token_index: int = 256_001,\n+        **kwargs,\n+    ):\n+        if isinstance(encoder, dict):\n+            encoder = T5Gemma2EncoderConfig(**encoder)\n+        elif encoder is None:\n+            encoder = T5Gemma2EncoderConfig()\n+            logger.info(\"encoder is None, using default T5Gemma2EncoderConfig encoder config.\")\n+        else:\n+            if not isinstance(encoder, T5Gemma2EncoderConfig):\n+                raise ValueError(f\"{type(encoder)} is not supported.\")\n+\n+        if isinstance(decoder, dict):\n+            decoder = T5Gemma2DecoderConfig(**decoder)\n+        elif decoder is None:\n+            decoder = T5Gemma2DecoderConfig()\n+            logger.info(\"decoder is None, using default T5Gemma2DecoderConfig decoder config.\")\n+        else:\n+            if not isinstance(decoder, T5Gemma2DecoderConfig):\n+                raise ValueError(f\"{type(decoder)} is not supported.\")\n+\n+        if encoder.text_config.hidden_size != decoder.hidden_size:\n+            raise ValueError(\n+                \"Imbalanced encoder-decoder is not supported in T5Gemma2: \"\n+                f\"encoder ({encoder.text_config.hidden_size}) vs decoder ({decoder.hidden_size}).\"\n+            )\n+\n+        if not is_encoder_decoder:\n+            raise ValueError(\"T5Gemma2Model only support encoder-decoder modeling.\")\n+\n+        if encoder.text_config.vocab_size != decoder.vocab_size:\n+            raise ValueError(\n+                \"Imbalanced encoder-decoder vocabulary size is not supported in T5Gemma2: \"\n+                f\"encoder ({encoder.text_config.vocab_size}) vs decoder ({decoder.vocab_size}).\"\n+            )\n+\n+        # Encoder.\n+        encoder.text_config.dropout_rate = dropout_rate\n+        encoder.text_config.attention_dropout = attention_dropout\n+        encoder.vision_config.attention_dropout = attention_dropout\n+        encoder.image_token_index = image_token_index\n+        self.encoder = encoder\n+\n+        # Decoder.\n+        decoder.dropout_rate = dropout_rate\n+        decoder.attention_dropout = attention_dropout\n+        self.decoder = decoder\n+\n+        for special_token_key in [\"bos_token_id\", \"pad_token_id\", \"eos_token_id\", \"vocab_size\"]:\n+            if special_token_key not in kwargs:\n+                kwargs[special_token_key] = getattr(decoder, special_token_key)\n+\n+        super().__init__(**kwargs)\n+\n+        self.is_encoder_decoder = is_encoder_decoder\n+        self.dropout_rate = dropout_rate\n+        self.attention_dropout = attention_dropout\n+        self.classifier_dropout_rate = classifier_dropout_rate\n+        self.initializer_range = initializer_range\n+        self.eoi_token_index = encoder.eoi_token_index\n+        self.image_token_index = image_token_index\n+\n+    def __setattr__(self, key, value):\n+        shared_attr_with_submodules = [\n+            \"output_hidden_states\",\n+            \"output_attentions\",\n+            \"_attn_implementation_internal\",\n+            \"dropout_rate\",\n+            \"attention_dropout\",\n+            \"vocab_size\",\n+            \"dtype\",\n+        ]\n+\n+        if key in shared_attr_with_submodules:\n+            setattr(self.encoder.text_config, key, value)\n+            setattr(self.encoder.vision_config, key, value)\n+            setattr(self.decoder, key, value)\n+            setattr(self.encoder, key, value)\n+        super().__setattr__(key, value)\n+\n+\n+class T5Gemma2RMSNorm(Gemma3RMSNorm):\n+    pass\n+\n+\n+class T5Gemma2MLP(Gemma3MLP):\n+    def __init__(self, config: T5Gemma2TextConfig):\n+        super().__init__(config)\n+        self.dropout = nn.Dropout(config.dropout_rate)\n+\n+    def forward(self, x):\n+        hidden_states = self.act_fn(self.gate_proj(x)) * self.up_proj(x)\n+        hidden_states = self.dropout(hidden_states)\n+        down_proj = self.down_proj(hidden_states)\n+        return down_proj\n+\n+\n+class T5Gemma2RotaryEmbedding(Gemma3RotaryEmbedding):\n+    def __init__(self, config: T5Gemma2TextConfig, device=None):\n+        super().__init__(config, device)\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[T5Gemma2TextConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+        layer_type: Optional[str] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        return super().compute_default_rope_parameters(config, device, seq_len, layer_type)\n+\n+\n+class T5Gemma2SelfAttention(Gemma3Attention):\n+    def __init__(self, config: T5Gemma2TextConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+\n+\n+class T5Gemma2MergedAttention(Gemma3Attention):\n+    \"\"\"Merged self-attention and cross-attention for decoder.\"\"\"\n+\n+    def __init__(self, config: T5Gemma2TextConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+\n+    def forward(\n+        self,\n+        # decoder self-attention inputs\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        merged_attention_mask: Optional[torch.Tensor],\n+        # cross-attention inputs\n+        encoder_hidden_states: torch.Tensor,\n+        # cache inputs\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        # others\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        # attention shapes.\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+        cross_input_shape = encoder_hidden_states.shape[:-1]\n+        cross_hidden_shape = (*cross_input_shape, -1, self.head_dim)\n+\n+        # self-attention.\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        query_states = self.q_norm(query_states)\n+        key_states = self.k_norm(key_states)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            # self-attention.\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            self_attention_cache = past_key_values.self_attention_cache\n+            key_states, value_states = self_attention_cache.update(\n+                key_states, value_states, self.layer_idx, cache_kwargs\n+            )\n+\n+            # cross-attention.\n+            is_updated = past_key_values.is_updated.get(self.layer_idx)\n+            cross_attention_cache = past_key_values.cross_attention_cache\n+\n+        if past_key_values is None or not is_updated:\n+            cross_key_states = self.k_proj(encoder_hidden_states).view(cross_hidden_shape).transpose(1, 2)\n+            cross_value_states = self.v_proj(encoder_hidden_states).view(cross_hidden_shape).transpose(1, 2)\n+\n+            cross_key_states = self.k_norm(cross_key_states)\n+\n+            if past_key_values is not None:\n+                cross_key_states, cross_value_states = cross_attention_cache.update(\n+                    cross_key_states, cross_value_states, self.layer_idx\n+                )\n+                past_key_values.is_updated[self.layer_idx] = True\n+        else:\n+            cross_key_states = cross_attention_cache.layers[self.layer_idx].keys\n+            cross_value_states = cross_attention_cache.layers[self.layer_idx].values\n+\n+        # merged attention.\n+        query_states = query_states\n+        cross_key_size = cross_input_shape[1]\n+        key_states = torch.cat([key_states, cross_key_states], dim=2)\n+        value_states = torch.cat([value_states, cross_value_states], dim=2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            merged_attention_mask,\n+            dropout=self.attention_dropout if self.training else 0.0,\n+            scaling=self.scaling,\n+            is_causal=False,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+\n+        # decompose merged attention weights into self & cross attention weights\n+        if attn_weights is not None:\n+            self_attn_weights = attn_weights[..., :-cross_key_size]\n+            cross_attn_weights = attn_weights[..., -cross_key_size:]\n+        else:\n+            self_attn_weights, cross_attn_weights = None, None\n+        return attn_output, self_attn_weights, cross_attn_weights\n+\n+\n+def sliding_window_mask_function(sliding_window: int, is_causal=True) -> Callable:\n+    \"\"\"\n+    This creates uni/bidirectional attention mask with sliding window.\n+    \"\"\"\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        if is_causal:\n+            left_window_size, right_window_size = sliding_window, 0\n+        else:\n+            left_window_size, right_window_size = ((sliding_window + 1) // 2, (sliding_window) // 2 + 1)\n+\n+        dist = q_idx - kv_idx\n+        left_mask = (dist >= 0) & (dist < left_window_size)\n+        right_mask = (dist < 0) & (-dist < right_window_size)\n+        return left_mask | right_mask\n+\n+    return inner_mask\n+\n+\n+class T5Gemma2EncoderLayer(T5GemmaEncoderLayer):\n+    pass\n+\n+\n+class T5Gemma2DecoderLayer(T5GemmaEncoderLayer):\n+    \"\"\"Decoder sub-layer: merged attention instead of vanilla self-attention.\"\"\"\n+\n+    def __init__(self, config, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+\n+        # replace vanilla self-attention with merged attention to support joint cross-attention.\n+        self.self_attn = T5Gemma2MergedAttention(\n+            config=config,\n+            layer_idx=layer_idx,\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        merged_attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> torch.FloatTensor:\n+        residual = hidden_states\n+        hidden_states = self.pre_self_attn_layernorm(hidden_states)\n+\n+        hidden_states, _, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            merged_attention_mask=merged_attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            encoder_hidden_states=encoder_hidden_states,\n+            **kwargs,\n+        )\n+        hidden_states = self.post_self_attn_layernorm(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+\n+        residual = hidden_states\n+        hidden_states = self.pre_feedforward_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = self.post_feedforward_layernorm(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+        return hidden_states\n+\n+\n+class T5Gemma2LMHead(T5GemmaLMHead):\n+    pass\n+\n+\n+class T5Gemma2ClassificationHead(T5GemmaClassificationHead):\n+    pass\n+\n+\n+class T5Gemma2MultiModalProjector(Gemma3MultiModalProjector):\n+    def __init__(self, config: T5Gemma2EncoderConfig):\n+        super().__init__(config)\n+\n+\n+class T5Gemma2TextScaledWordEmbedding(Gemma3TextScaledWordEmbedding):\n+    \"\"\"T5Gemma2 Embedding: override to add eoi token embedding separately.\"\"\"\n+\n+    def __init__(\n+        self,\n+        num_embeddings: int,\n+        embedding_dim: int,\n+        padding_idx: int,\n+        embed_scale: float = 1.0,\n+        eoi_token_index: int = 256_000,\n+    ):\n+        super().__init__(num_embeddings, embedding_dim, padding_idx, embed_scale)\n+        self.eoi_token_index = eoi_token_index\n+        self.eoi_embedding = nn.Parameter(torch.zeros(self.embedding_dim))\n+\n+    def forward(self, input_ids: torch.Tensor):\n+        input_embeddings = super().forward(input_ids) * self.embed_scale.to(self.weight.dtype)\n+        input_embeddings[input_ids == self.eoi_token_index] = self.eoi_embedding.to(input_embeddings.dtype)\n+        return input_embeddings\n+\n+\n+@auto_docstring\n+class T5Gemma2PreTrainedModel(Gemma3PreTrainedModel):\n+    config: T5Gemma2Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+\n+    # Mask creation is incompatible\n+    # FA due to non-default creation / SWA\n+    _supports_flash_attn = False\n+    # Flex due to custom masks not compatible to be merged after creation\n+    _supports_flex_attn = False\n+\n+    _no_split_modules = [\n+        \"T5Gemma2EncoderLayer\",\n+        \"T5Gemma2DecoderLayer\",\n+        \"SiglipVisionEmbeddings\",\n+        \"SiglipEncoderLayer\",\n+        \"SiglipMultiheadAttentionPoolingHead\",\n+    ]\n+    _can_record_outputs = {\n+        \"hidden_states\": [T5Gemma2EncoderLayer, T5Gemma2DecoderLayer],\n+        \"attentions\": [\n+            OutputRecorder(T5Gemma2SelfAttention, index=1, layer_name=\"self_attn\"),\n+            OutputRecorder(T5Gemma2MergedAttention, index=1, layer_name=\"self_attn\"),\n+            OutputRecorder(T5Gemma2MergedAttention, index=2, layer_name=\"cross_attn\"),\n+        ],\n+    }\n+\n+    def _init_weights(self, module):\n+        PreTrainedModel._init_weights(self, module)\n+        if isinstance(module, T5Gemma2MultiModalProjector):\n+            init.zeros_(module.mm_input_projection_weight)\n+        elif isinstance(module, T5Gemma2TextScaledWordEmbedding):\n+            init.zeros_(module.eoi_embedding)\n+        elif isinstance(module, T5Gemma2ClassificationHead):\n+            scale = module.out_proj.weight.shape[0] ** -0.5\n+            init.normal_(module.out_proj.weight, mean=0.0, std=self.config.initializer_range * scale)\n+            if hasattr(module.out_proj, \"bias\") and module.out_proj.bias is not None:\n+                init.zeros_(module.out_proj.bias)\n+        # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n+        elif \"RMSNorm\" in module.__class__.__name__:\n+            init.zeros_(module.weight)\n+\n+    def prepare_decoder_input_ids_from_labels(self, input_ids):\n+        \"\"\"\n+        Shifts input_ids to the right, prepends the decoder_start_token_id, and handles\n+        pad_token_id replacement for labels that were -100.\n+        This is a common preparation step for decoder inputs in sequence-to-sequence models.\n+        \"\"\"\n+        decoder_config = self.config.decoder\n+        decoder_start_token_id = decoder_config.bos_token_id\n+        pad_token_id = decoder_config.pad_token_id\n+\n+        if decoder_start_token_id is None:\n+            raise ValueError(\"self.model.config.decoder.bos_token_id has to be defined. \")\n+\n+        # shift inputs to the right\n+        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n+        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n+        shifted_input_ids[..., 0] = decoder_start_token_id\n+\n+        if pad_token_id is None:\n+            raise ValueError(\"self.model.config.decoder.pad_token_id has to be defined.\")\n+\n+        # Is this T5 specific?\n+        # replace possible -100 values in labels by `pad_token_id`\n+        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n+\n+        return shifted_input_ids\n+\n+\n+class T5Gemma2Encoder(T5Gemma2PreTrainedModel):\n+    config: T5Gemma2EncoderConfig\n+    _can_record_outputs = {\n+        \"attentions\": T5Gemma2SelfAttention,\n+        \"hidden_states\": T5Gemma2EncoderLayer,\n+    }\n+\n+    def __init__(\n+        self,\n+        config: T5Gemma2EncoderConfig,\n+        eoi_token_index: int = 256_000,\n+    ):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.text_config.vocab_size\n+\n+        vision_config = config.vision_config\n+        text_config = config.text_config\n+\n+        # setup vision tower\n+        self.vision_tower = AutoModel.from_config(config=vision_config)\n+        self.multi_modal_projector = T5Gemma2MultiModalProjector(config)\n+\n+        self.embed_tokens = T5Gemma2TextScaledWordEmbedding(\n+            text_config.vocab_size,\n+            text_config.hidden_size,\n+            self.padding_idx,\n+            embed_scale=text_config.hidden_size**0.5,\n+            eoi_token_index=eoi_token_index,\n+        )\n+        self.norm = T5Gemma2RMSNorm(text_config.hidden_size, eps=text_config.rms_norm_eps)\n+        self.gradient_checkpointing = False\n+\n+        self.layers = nn.ModuleList(\n+            [T5Gemma2EncoderLayer(text_config, layer_idx) for layer_idx in range(text_config.num_hidden_layers)]\n+        )\n+        self.dropout = nn.Dropout(text_config.dropout_rate)\n+        self.rotary_emb = T5Gemma2RotaryEmbedding(text_config)\n+\n+        self.text_config = text_config\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n+        \"\"\"Convert pixel image to image features via the encoder and projector.\"\"\"\n+        # pixel_values: (batch_size, channels, height, width)\n+        # image_features: Image feature tensor of shape (num_images, image_length, embed_dim).\n+        vision_outputs = self.vision_tower(pixel_values=pixel_values).last_hidden_state\n+        image_features = self.multi_modal_projector(vision_outputs)\n+        return image_features\n+\n+    def get_image_placeholder_mask(\n+        self,\n+        input_ids: Optional[torch.LongTensor],\n+        inputs_embeds: Optional[torch.FloatTensor],\n+        image_features: torch.FloatTensor,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        image_token_id = self.config.image_token_id\n+        if input_ids is None:\n+            if inputs_embeds is None:\n+                raise ValueError(\"Either `input_ids` or `inputs_embeds` has to be provided.\")\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n+    def preprocess_image_features(\n+        self,\n+        pixel_values: torch.Tensor,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+    ):\n+        \"\"\"Convert pixel images to image features and merge into input embeds.\"\"\"\n+        image_features = self.get_image_features(pixel_values)\n+        image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+\n+        image_mask = self.get_image_placeholder_mask(\n+            input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+        )\n+\n+        inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_features)\n+        return inputs_embeds\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        # Unused for processor compatibility kept in signature.\n+        token_type_ids: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutput:\n+        del token_type_ids\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        # As we want to pass `past_key_values=None` explicitly everywhere, we need to pop them from kwargs if present\n+        kwargs.pop(\"past_key_values\", None)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if pixel_values is not None:\n+            inputs_embeds = self.preprocess_image_features(\n+                pixel_values, input_ids=input_ids, inputs_embeds=inputs_embeds\n+            )\n+\n+        if position_ids is None:\n+            position_ids = torch.arange(0, inputs_embeds.shape[1], device=inputs_embeds.device).unsqueeze(0)\n+\n+        if not isinstance(self_attn_mask_mapping := attention_mask, dict):\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+            }\n+            self_attn_mask_mapping = {\n+                \"full_attention\": create_bidirectional_mask(**mask_kwargs),\n+                \"sliding_attention\": create_bidirectional_mask(\n+                    **mask_kwargs,\n+                    and_mask_function=sliding_window_mask_function(self.text_config.sliding_window, is_causal=False),\n+                ),\n+            }\n+\n+        # input layer\n+        hidden_states = inputs_embeds\n+\n+        # global and local position embeddings\n+        position_embeddings = {}\n+        for layer_type in self.text_config.layer_types:\n+            position_embeddings[layer_type] = self.rotary_emb(hidden_states, position_ids, layer_type)\n+\n+        # dropout\n+        hidden_states = self.dropout(hidden_states)\n+\n+        for layer_module in self.layers[: self.text_config.num_hidden_layers]:\n+            hidden_states = layer_module(\n+                hidden_states,\n+                position_embeddings[layer_module.attention_type],\n+                self_attn_mask_mapping[layer_module.attention_type],\n+                position_ids,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+        )\n+\n+\n+class T5Gemma2Decoder(T5Gemma2PreTrainedModel):\n+    config: T5Gemma2DecoderConfig\n+    _can_record_outputs = {\n+        \"attentions\": OutputRecorder(T5Gemma2MergedAttention, index=1),\n+        \"cross_attentions\": OutputRecorder(T5Gemma2MergedAttention, index=2),\n+        \"hidden_states\": T5Gemma2DecoderLayer,\n+    }\n+\n+    def __init__(self, config: T5Gemma2DecoderConfig, eoi_token_index: int = 256_000):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = T5Gemma2TextScaledWordEmbedding(\n+            config.vocab_size,\n+            config.hidden_size,\n+            config.pad_token_id,\n+            embed_scale=config.hidden_size**0.5,\n+            eoi_token_index=eoi_token_index,\n+        )\n+        self.norm = T5Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.gradient_checkpointing = False\n+\n+        self.layers = nn.ModuleList(\n+            [T5Gemma2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.dropout = nn.Dropout(config.dropout_rate)\n+        self.rotary_emb = T5Gemma2RotaryEmbedding(config)\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPastAndCrossAttentions:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+        if encoder_hidden_states is None:\n+            raise ValueError(\"`encoder_hidden_states` must be given in decoder\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if not self.training and use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache())\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        if not isinstance(self_attn_mask_mapping := attention_mask, dict):\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values.self_attention_cache if past_key_values is not None else None,\n+                \"position_ids\": position_ids,\n+            }\n+            # this masking function did nothing to masking but forces `allow_is_causal_skip` to be False\n+            # as we always need a mask during decoding for merged attention.\n+            mask_kwargs[\"and_mask_function\"] = lambda *args: torch.tensor(True, dtype=torch.bool)\n+            self_attn_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n+\n+        if not isinstance(cross_attn_mask_mapping := encoder_attention_mask, dict):\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": encoder_hidden_states,\n+                \"attention_mask\": encoder_attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": None,\n+                \"position_ids\": None,\n+            }\n+            cross_attn_mask_mapping = {\n+                \"full_attention\": create_causal_mask(\n+                    **mask_kwargs,\n+                    or_mask_function=bidirectional_mask_function(encoder_attention_mask),\n+                ),\n+            }\n+\n+        merged_attn_mask_mapping = {\n+            \"full_attention\": torch.cat(\n+                [self_attn_mask_mapping[\"full_attention\"], cross_attn_mask_mapping[\"full_attention\"]], dim=-1\n+            ),\n+            \"sliding_attention\": torch.cat(\n+                [self_attn_mask_mapping[\"sliding_attention\"], cross_attn_mask_mapping[\"full_attention\"]], dim=-1\n+            ),\n+        }\n+\n+        # input layer\n+        hidden_states = inputs_embeds\n+\n+        # global and local position embeddings\n+        position_embeddings = {}\n+        for layer_type in self.config.layer_types:\n+            position_embeddings[layer_type] = self.rotary_emb(hidden_states, position_ids, layer_type)\n+\n+        # dropout\n+        hidden_states = self.dropout(hidden_states)\n+\n+        for layer_module in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = layer_module(\n+                hidden_states,\n+                position_embeddings[layer_module.attention_type],\n+                merged_attn_mask_mapping[layer_module.attention_type],\n+                position_ids,\n+                past_key_values,\n+                use_cache,\n+                cache_position,\n+                encoder_hidden_states,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        return BaseModelOutputWithPastAndCrossAttentions(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+@auto_docstring\n+class T5Gemma2Model(T5Gemma2PreTrainedModel):\n+    _tied_weights_keys = {\n+        \"decoder.embed_tokens.weight\": \"encoder.embed_tokens.weight\",\n+        \"decoder.embed_tokens.eoi_embedding\": \"encoder.embed_tokens.eoi_embedding\",\n+    }\n+\n+    def __init__(self, config: T5Gemma2Config):\n+        super().__init__(config)\n+\n+        # setup encoder and decoder\n+        self.encoder = T5Gemma2Encoder(config.encoder, config.eoi_token_index)\n+        self.decoder = T5Gemma2Decoder(config.decoder, config.eoi_token_index)\n+\n+        self.post_init()\n+\n+    def get_encoder(self):\n+        return self.encoder\n+\n+    def get_decoder(self):\n+        return self.decoder\n+\n+    def get_input_embeddings(self):\n+        return self.encoder.get_input_embeddings()\n+\n+    def set_input_embeddings(self, new_embeddings):\n+        return self.encoder.set_input_embeddings(new_embeddings)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        # encoder inputs\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        # decoder inputs\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        # others (mainly inference or cache related)\n+        encoder_outputs: Optional[BaseModelOutput] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        decoder_inputs_embeds: Optional[torch.Tensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Seq2SeqModelOutput:\n+        r\"\"\"\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n+            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        \"\"\"\n+        # encoder\n+        if encoder_outputs is None:\n+            encoder_outputs = self.encoder(\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                inputs_embeds=inputs_embeds,\n+                pixel_values=pixel_values,\n+                return_dict=True,\n+                **kwargs,\n+            )\n+\n+        encoder_hidden_states = encoder_outputs.last_hidden_state\n+\n+        # decoder\n+        decoder_outputs = self.decoder(\n+            input_ids=decoder_input_ids,\n+            attention_mask=decoder_attention_mask,\n+            position_ids=decoder_position_ids,\n+            inputs_embeds=decoder_inputs_embeds,\n+            past_key_values=past_key_values,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=attention_mask,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        return Seq2SeqModelOutput(\n+            last_hidden_state=decoder_outputs.last_hidden_state,\n+            past_key_values=decoder_outputs.past_key_values,\n+            decoder_hidden_states=decoder_outputs.hidden_states,\n+            decoder_attentions=decoder_outputs.attentions,\n+            cross_attentions=decoder_outputs.cross_attentions,\n+            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n+            encoder_hidden_states=encoder_outputs.hidden_states,\n+            encoder_attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+class T5Gemma2ForConditionalGeneration(T5Gemma2PreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = {\n+        \"lm_head.out_proj.weight\": \"model.encoder.embed_tokens.weight\",\n+    }\n+    _tp_plan = {\"lm_head.out_proj\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head.out_proj\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config: T5Gemma2Config):\n+        super().__init__(config)\n+\n+        self.model = T5Gemma2Model(config)\n+        self.vocab_size = config.decoder.vocab_size\n+        self.lm_head = T5Gemma2LMHead(config.decoder.hidden_size, self.vocab_size)\n+        self.loss_type = \"ForMaskedLM\"\n+\n+        self.post_init()\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head.out_proj = new_embeddings\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head.out_proj\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_encoder(self):\n+        return self.model.get_encoder()\n+\n+    def get_decoder(self):\n+        return self.model.get_decoder()\n+\n+    def get_image_features(self, pixel_values):\n+        return self.get_encoder().get_image_features(pixel_values)\n+\n+    @property\n+    def vision_tower(self):\n+        return self.get_encoder().vision_tower\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        # encoder inputs\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        # decoder inputs\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        # others (mainly inference or cache related)\n+        encoder_outputs: Optional[BaseModelOutput] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n+        r\"\"\"\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n+            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        \"\"\"\n+\n+        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n+            # get decoder inputs from shifting lm labels to the right\n+            decoder_input_ids = self.prepare_decoder_input_ids_from_labels(labels)\n+\n+        decoder_outputs: Seq2SeqModelOutput = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            decoder_input_ids=decoder_input_ids,\n+            decoder_attention_mask=decoder_attention_mask,\n+            decoder_position_ids=decoder_position_ids,\n+            encoder_outputs=encoder_outputs,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            decoder_inputs_embeds=decoder_inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = decoder_outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        decoder_config = self.config.decoder\n+        if decoder_config.final_logit_softcapping is not None:\n+            logits = logits / decoder_config.final_logit_softcapping\n+            logits = torch.tanh(logits)\n+            logits = logits * decoder_config.final_logit_softcapping\n+\n+        loss = None\n+        if labels is not None:\n+            # Input has right-shifted so we directly perform masked lm loss\n+            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n+\n+        return Seq2SeqLMOutput(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=decoder_outputs.past_key_values,\n+            decoder_hidden_states=decoder_outputs.decoder_hidden_states,\n+            decoder_attentions=decoder_outputs.decoder_attentions,\n+            cross_attentions=decoder_outputs.cross_attentions,\n+            encoder_last_hidden_state=decoder_outputs.encoder_last_hidden_state,\n+            encoder_hidden_states=decoder_outputs.encoder_hidden_states,\n+            encoder_attentions=decoder_outputs.encoder_attentions,\n+        )\n+\n+    def _prepare_cache_for_generation(\n+        self,\n+        generation_config: GenerationConfig,\n+        model_kwargs: dict,\n+        generation_mode: GenerationMode,\n+        batch_size: int,\n+        max_cache_length: int,\n+    ) -> bool:\n+        \"\"\"Override cache preparation to support T5Gemma2-specific EncoderDecoder Cache.\"\"\"\n+\n+        # Build cache and past_key_values structure first and then override as needed.\n+        super()._prepare_cache_for_generation(\n+            generation_config,\n+            model_kwargs,\n+            generation_mode,\n+            batch_size,\n+            max_cache_length,\n+        )\n+\n+        # If use_cache is False, do not prepare the cache.\n+        if generation_config.use_cache is False:\n+            return\n+\n+        cache_implementation = generation_config.cache_implementation\n+        if cache_implementation is None:\n+            offload_cache = False\n+        else:\n+            offload_cache = \"offloaded\" in generation_config.cache_implementation\n+\n+        # Main change: use full cache for cross-attention.\n+        cross_attn_config = copy.deepcopy(self.config.get_text_config(decoder=True))\n+\n+        # cross-attention does not use sliding window\n+        del cross_attn_config.sliding_window\n+        del cross_attn_config.layer_types\n+\n+        cross_attn_cache_kwargs = {\n+            \"config\": cross_attn_config,\n+            \"offloading\": offload_cache,\n+        }\n+\n+        past_key_values = model_kwargs.get(\"past_key_values\")\n+        if past_key_values is not None:\n+            if not isinstance(past_key_values, EncoderDecoderCache):\n+                raise ValueError(\n+                    \"The `past_key_values` in `model_kwargs` must be of type `EncoderDecoderCache` for T5Gemma2 model.\"\n+                )\n+\n+            # Cache already established, no need to re-initialize.\n+            if len(past_key_values.is_updated) > 0 and past_key_values.is_updated.get(0):\n+                return\n+\n+            cross_attn_cls = type(past_key_values.cross_attention_cache)\n+            if cross_attn_cls == StaticCache:\n+                cross_attn_cache_kwargs[\"max_cache_len\"] = model_kwargs[\"encoder_outputs\"][0].shape[1]\n+            # Update cross-attention cache only (switch from sliding_window to full).\n+            past_key_values.cross_attention_cache = cross_attn_cls(**cross_attn_cache_kwargs)\n+        else:\n+            # Initialize new cache.\n+            model_kwargs[\"past_key_values\"] = EncoderDecoderCache(\n+                DynamicCache(\n+                    **{\n+                        \"config\": self.config.get_text_config(decoder=True),\n+                        \"offloading\": offload_cache,\n+                    }\n+                ),  # self-attention cache\n+                DynamicCache(),  # cross-attention cache\n+            )\n+\n+        if hasattr(self, \"_cache\") and self._cache is not None:\n+            if not isinstance(self._cache, EncoderDecoderCache):\n+                raise ValueError(\"The internal cache must be of type `EncoderDecoderCache` for T5Gemma2 model.\")\n+\n+            self._cache = model_kwargs[\"past_key_values\"]\n+\n+\n+@auto_docstring\n+class T5Gemma2ForSequenceClassification(T5Gemma2PreTrainedModel):\n+    def __init__(self, config: T5Gemma2Config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        self.hidden_size = config.decoder.hidden_size\n+\n+        self.model = T5Gemma2Model(config)\n+\n+        classifier_dropout = getattr(config, \"classifier_dropout_rate\", 0.1)\n+        self.score = T5Gemma2ClassificationHead(self.hidden_size, self.num_labels, classifier_dropout)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.Tensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        encoder_outputs: Optional[BaseModelOutput] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> SequenceClassifierOutput:\n+        r\"\"\"\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n+            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        if inputs_embeds is not None or decoder_inputs_embeds is not None:\n+            raise NotImplementedError(\n+                f\"Passing input embeddings is currently not supported for {self.__class__.__name__}.\"\n+            )\n+\n+        if input_ids is None:\n+            raise ValueError(\"You have to specify input_ids\")\n+\n+        if decoder_input_ids is None:\n+            decoder_input_ids = self.prepare_decoder_input_ids_from_labels(input_ids)\n+\n+        outputs: Seq2SeqModelOutput = self.model(\n+            input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            decoder_input_ids=decoder_input_ids,\n+            decoder_attention_mask=decoder_attention_mask,\n+            decoder_position_ids=decoder_position_ids,\n+            encoder_outputs=encoder_outputs,\n+            inputs_embeds=inputs_embeds,\n+            decoder_inputs_embeds=decoder_inputs_embeds,\n+            use_cache=False,\n+            **kwargs,\n+        )\n+\n+        last_hidden_state = outputs.last_hidden_state\n+        hidden_states = outputs.decoder_hidden_states\n+        attentions = outputs.decoder_attentions\n+\n+        logits = self.score(last_hidden_state)\n+\n+        batch_size = input_ids.shape[0]\n+        # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+        non_pad_mask = (decoder_input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+        token_indices = torch.arange(decoder_input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n+        last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n+        last_non_pad_token = torch.clamp(last_non_pad_token, max=decoder_input_ids.shape[-1] - 1)\n+\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n+        return SequenceClassifierOutput(\n+            loss=loss,\n+            logits=pooled_logits,\n+            hidden_states=hidden_states,\n+            attentions=attentions,\n+        )\n+\n+\n+@auto_docstring\n+class T5Gemma2ForTokenClassification(T5Gemma2PreTrainedModel):\n+    def __init__(self, config: T5Gemma2Config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        self.hidden_size = config.decoder.hidden_size\n+\n+        self.model = T5Gemma2Model(config)\n+\n+        classifier_dropout = getattr(config, \"classifier_dropout_rate\", 0.1)\n+        self.score = T5Gemma2ClassificationHead(self.hidden_size, self.num_labels, classifier_dropout)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.Tensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        encoder_outputs: Optional[BaseModelOutput] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> TokenClassifierOutput:\n+        r\"\"\"\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n+            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        if inputs_embeds is not None or decoder_inputs_embeds is not None:\n+            raise NotImplementedError(\n+                f\"Passing input embeddings is currently not supported for {self.__class__.__name__}.\"\n+            )\n+\n+        if input_ids is None:\n+            raise ValueError(\"You have to specify input_ids\")\n+\n+        if decoder_input_ids is None:\n+            decoder_input_ids = self.prepare_decoder_input_ids_from_labels(input_ids)\n+\n+        outputs: Seq2SeqModelOutput = self.model(\n+            input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            decoder_input_ids=decoder_input_ids,\n+            decoder_attention_mask=decoder_attention_mask,\n+            decoder_position_ids=decoder_position_ids,\n+            encoder_outputs=encoder_outputs,\n+            inputs_embeds=inputs_embeds,\n+            decoder_inputs_embeds=decoder_inputs_embeds,\n+            use_cache=False,\n+            **kwargs,\n+        )\n+        last_hidden_state = outputs.last_hidden_state\n+        hidden_states = outputs.decoder_hidden_states\n+        attentions = outputs.decoder_attentions\n+\n+        logits = self.score(last_hidden_state)\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, self.config)\n+\n+        return TokenClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=hidden_states,\n+            attentions=attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"T5Gemma2Config\",\n+    \"T5Gemma2TextConfig\",\n+    \"T5Gemma2EncoderConfig\",\n+    \"T5Gemma2DecoderConfig\",\n+    \"T5Gemma2ForConditionalGeneration\",\n+    \"T5Gemma2Model\",\n+    \"T5Gemma2PreTrainedModel\",\n+    \"T5Gemma2ForSequenceClassification\",\n+    \"T5Gemma2ForTokenClassification\",\n+]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/t5gemma2/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/2375ddb426147f9e94e12be8d6a54f7a418e1e98/tests%2Fmodels%2Ft5gemma2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2375ddb426147f9e94e12be8d6a54f7a418e1e98/tests%2Fmodels%2Ft5gemma2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma2%2F__init__.py?ref=2375ddb426147f9e94e12be8d6a54f7a418e1e98"
        },
        {
            "sha": "ce02c0936cc3d6c3efc63dcea9e39d080c64decf",
            "filename": "tests/models/t5gemma2/test_modeling_t5gemma2.py",
            "status": "added",
            "additions": 967,
            "deletions": 0,
            "changes": 967,
            "blob_url": "https://github.com/huggingface/transformers/blob/2375ddb426147f9e94e12be8d6a54f7a418e1e98/tests%2Fmodels%2Ft5gemma2%2Ftest_modeling_t5gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2375ddb426147f9e94e12be8d6a54f7a418e1e98/tests%2Fmodels%2Ft5gemma2%2Ftest_modeling_t5gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma2%2Ftest_modeling_t5gemma2.py?ref=2375ddb426147f9e94e12be8d6a54f7a418e1e98",
            "patch": "@@ -0,0 +1,967 @@\n+# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch T5Gemma2 model.\"\"\"\n+\n+import copy\n+import unittest\n+\n+import pytest\n+\n+from transformers import (\n+    T5Gemma2Config,\n+    T5Gemma2DecoderConfig,\n+    T5Gemma2EncoderConfig,\n+    T5Gemma2TextConfig,\n+    is_torch_available,\n+)\n+from transformers.testing_utils import (\n+    require_torch,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin, has_similar_generate_outputs\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+    import torch.nn.functional as F\n+\n+    from transformers import (\n+        T5Gemma2ForConditionalGeneration,\n+        T5Gemma2ForSequenceClassification,\n+        T5Gemma2ForTokenClassification,\n+        T5Gemma2Model,\n+    )\n+\n+\n+class T5Gemma2ModelTester:\n+    config_class = T5Gemma2Config\n+    text_config_class = T5Gemma2TextConfig\n+    encoder_config_class = T5Gemma2EncoderConfig\n+    decoder_config_class = T5Gemma2DecoderConfig\n+\n+    if is_torch_available():\n+        model_class = T5Gemma2Model\n+        causal_lm_class = T5Gemma2ForConditionalGeneration\n+        sequence_classification_class = T5Gemma2ForSequenceClassification\n+        token_classification_class = T5Gemma2ForTokenClassification\n+\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        is_training=True,\n+        use_attention_mask=True,\n+        use_labels=True,\n+        vocab_size=99,\n+        # decoder-specific\n+        seq_length=7,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        num_key_value_heads=2,\n+        intermediate_size=37,\n+        # encoder-specific\n+        encoder_seq_length=7,\n+        encoder_hidden_size=32,\n+        encoder_num_hidden_layers=2,\n+        encoder_num_attention_heads=4,\n+        encoder_num_key_value_heads=2,\n+        encoder_intermediate_size=37,\n+        # vision-specific\n+        mm_tokens_per_image=2,\n+        image_token_index=4,\n+        boi_token_index=5,\n+        eoi_token_index=6,\n+        siglip_config={\n+            \"use_labels\": True,\n+            \"image_size\": 20,\n+            \"patch_size\": 5,\n+            \"num_channels\": 3,\n+            \"is_training\": True,\n+            \"hidden_size\": 32,\n+            \"num_key_value_heads\": 1,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"intermediate_size\": 37,\n+            \"dropout\": 0.1,\n+            \"attention_dropout\": 0.1,\n+            \"initializer_range\": 0.02,\n+        },\n+        # common\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.1,\n+        attention_probs_dropout_prob=0.1,\n+        max_position_embeddings=512,\n+        type_vocab_size=16,\n+        type_sequence_label_size=2,\n+        initializer_range=0.02,\n+        num_labels=3,\n+        num_choices=4,\n+        scope=None,\n+        # special ids\n+        eos_token_id=1,\n+        pad_token_id=0,\n+        bos_token_id=2,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.is_training = is_training\n+        self.use_attention_mask = use_attention_mask\n+        self.use_labels = use_labels\n+        self.vocab_size = vocab_size\n+        # decoder\n+        self.seq_length = seq_length\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.intermediate_size = intermediate_size\n+        # encoder\n+        self.encoder_seq_length = encoder_seq_length\n+        self.encoder_hidden_size = encoder_hidden_size\n+        self.encoder_num_hidden_layers = encoder_num_hidden_layers\n+        self.encoder_num_attention_heads = encoder_num_attention_heads\n+        self.encoder_num_key_value_heads = encoder_num_key_value_heads\n+        self.encoder_intermediate_size = encoder_intermediate_size\n+        # vision\n+        self.mm_tokens_per_image = mm_tokens_per_image\n+        self.image_token_index = image_token_index\n+        self.boi_token_index = boi_token_index\n+        self.eoi_token_index = eoi_token_index\n+        self.siglip_config = siglip_config\n+        self.num_channels = siglip_config[\"num_channels\"]\n+        self.image_size = siglip_config[\"image_size\"]\n+        # common\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.max_position_embeddings = max_position_embeddings\n+        self.type_vocab_size = type_vocab_size\n+        self.type_sequence_label_size = type_sequence_label_size\n+        self.initializer_range = initializer_range\n+        self.num_labels = num_labels\n+        self.num_choices = num_choices\n+        self.scope = scope\n+        self.head_dim = self.hidden_size // self.num_attention_heads\n+        # special ids\n+        self.eos_token_id = eos_token_id\n+        self.pad_token_id = pad_token_id\n+        self.bos_token_id = bos_token_id\n+\n+    def get_encoder_config(self):\n+        return self.encoder_config_class(\n+            text_config=self.text_config_class(\n+                vocab_size=self.vocab_size,\n+                hidden_size=self.encoder_hidden_size,\n+                num_hidden_layers=self.encoder_num_hidden_layers,\n+                num_attention_heads=self.encoder_num_attention_heads,\n+                num_key_value_heads=self.encoder_num_key_value_heads,\n+                intermediate_size=self.encoder_intermediate_size,\n+                hidden_act=self.hidden_act,\n+                hidden_dropout_prob=self.hidden_dropout_prob,\n+                attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n+                max_position_embeddings=self.max_position_embeddings,\n+                type_vocab_size=self.type_vocab_size,\n+                is_decoder=False,\n+                initializer_range=self.initializer_range,\n+                head_dim=self.head_dim,\n+                bos_token_id=self.bos_token_id,\n+                eos_token_id=self.eos_token_id,\n+                pad_token_id=self.pad_token_id,\n+            ),\n+            # vision.\n+            vision_config=self.siglip_config,\n+            image_token_index=self.image_token_index,\n+            boi_token_index=self.boi_token_index,\n+            eoi_token_index=self.eoi_token_index,\n+            mm_tokens_per_image=self.mm_tokens_per_image,\n+            hidden_size=self.encoder_hidden_size,\n+        )\n+\n+    def get_decoder_config(self):\n+        return self.decoder_config_class(\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            num_key_value_heads=self.num_key_value_heads,\n+            intermediate_size=self.intermediate_size,\n+            cross_attention_hidden_size=self.encoder_hidden_size,\n+            hidden_act=self.hidden_act,\n+            hidden_dropout_prob=self.hidden_dropout_prob,\n+            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n+            max_position_embeddings=self.max_position_embeddings,\n+            type_vocab_size=self.type_vocab_size,\n+            is_decoder=True,\n+            initializer_range=self.initializer_range,\n+            head_dim=self.head_dim,\n+            bos_token_id=self.bos_token_id,\n+            eos_token_id=self.eos_token_id,\n+            pad_token_id=self.pad_token_id,\n+        )\n+\n+    def get_config(self, is_encoder_decoder=True):\n+        return self.config_class(\n+            encoder=self.get_encoder_config(),\n+            decoder=self.get_decoder_config(),\n+            is_encoder_decoder=is_encoder_decoder,\n+            # vision.\n+            image_token_index=self.image_token_index,\n+            # Used for generation test.\n+            num_attention_heads=self.num_attention_heads,\n+            num_key_value_heads=self.num_key_value_heads,\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+\n+        input_ids = ids_tensor([self.batch_size, self.encoder_seq_length], self.vocab_size - 1) + 1\n+        decoder_input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size - 1) + 1\n+        # Vision inputs.\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                self.siglip_config[\"num_channels\"],\n+                self.siglip_config[\"image_size\"],\n+                self.siglip_config[\"image_size\"],\n+            ]\n+        )\n+\n+        # Remove BOS symbols from inputs.\n+        input_ids = torch.where(input_ids == self.bos_token_id, 42, input_ids)\n+        decoder_input_ids = torch.where(decoder_input_ids == self.bos_token_id, 42, decoder_input_ids)\n+\n+        # Avoid leading PAD tokens from inputs.\n+        decoder_input_ids[:, 0] = self.pad_token_id + 1\n+\n+        # set the 3 first tokens to be image, and ensure that no other tokens are image tokens\n+        # do not change this unless you modified image size or patch size\n+        input_ids[input_ids == config.encoder.image_token_index] = self.pad_token_id\n+        input_ids[:, :1] = config.encoder.image_token_index\n+\n+        attention_mask = None\n+        decoder_attention_mask = None\n+        if self.use_attention_mask:\n+            attention_mask = ids_tensor([self.batch_size, self.encoder_seq_length], vocab_size=2)\n+            decoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n+\n+        lm_labels = None\n+        if self.use_labels:\n+            lm_labels = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+\n+        return (\n+            config,\n+            input_ids,\n+            decoder_input_ids,\n+            attention_mask,\n+            decoder_attention_mask,\n+            lm_labels,\n+            pixel_values,\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (\n+            config,\n+            input_ids,\n+            decoder_input_ids,\n+            attention_mask,\n+            decoder_attention_mask,\n+            lm_labels,\n+            pixel_values,\n+        ) = config_and_inputs\n+\n+        inputs_dict = {\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"decoder_input_ids\": decoder_input_ids,\n+            \"decoder_attention_mask\": decoder_attention_mask,\n+            \"pixel_values\": pixel_values,\n+        }\n+        return config, inputs_dict\n+\n+    def create_and_check_model(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+        pixel_values,\n+    ):\n+        model = self.model_class(config=config).to(torch_device).eval()\n+\n+        result = model(\n+            input_ids=input_ids,\n+            decoder_input_ids=decoder_input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            decoder_attention_mask=decoder_attention_mask,\n+        )\n+\n+        decoder_output = result.last_hidden_state\n+        decoder_past = result.past_key_values\n+        encoder_output = result.encoder_last_hidden_state\n+\n+        self.parent.assertEqual(\n+            encoder_output.size(), (self.batch_size, self.encoder_seq_length, self.encoder_hidden_size)\n+        )\n+        self.parent.assertEqual(decoder_output.size(), (self.batch_size, self.seq_length, self.hidden_size))\n+        self.parent.assertIsNotNone(decoder_past)\n+        self.parent.assertEqual(len(decoder_past.self_attention_cache), config.decoder.num_hidden_layers)\n+        self.parent.assertEqual(len(decoder_past.cross_attention_cache), config.decoder.num_hidden_layers)\n+\n+    def check_prepare_lm_labels_via_shift_left(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+        pixel_values,\n+    ):\n+        model = self.model_class(config=config).to(torch_device).eval()\n+\n+        # _shift_right should be called on labels\n+        shifted_labels = model.prepare_decoder_input_ids_from_labels(lm_labels)\n+\n+        # first token should be decoder_start_token_id\n+        self.parent.assertTrue(torch.all(shifted_labels[:, 0] == config.decoder.bos_token_id))\n+\n+        # the rest should be the labels shifted by one, with -100 replaced by pad_token_id\n+        labels_without_ignore_index = lm_labels.masked_fill(lm_labels == -100, config.decoder.pad_token_id)\n+        self.parent.assertTrue(torch.all(shifted_labels[:, 1:] == labels_without_ignore_index[:, :-1]))\n+\n+    def create_and_check_with_lm_head(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+        pixel_values,\n+    ):\n+        model = self.causal_lm_class(config=config).to(torch_device).eval()\n+        outputs = model(\n+            input_ids=input_ids,\n+            decoder_input_ids=decoder_input_ids,\n+            attention_mask=attention_mask,\n+            decoder_attention_mask=decoder_attention_mask,\n+            labels=lm_labels,\n+            pixel_values=pixel_values,\n+        )\n+        self.parent.assertEqual(len(outputs), 4)\n+        self.parent.assertEqual(outputs[\"logits\"].size(), (self.batch_size, self.seq_length, self.vocab_size))\n+        self.parent.assertEqual(outputs[\"loss\"].size(), ())\n+\n+    def create_and_check_with_sequence_classification_head(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+        pixel_values,\n+    ):\n+        labels = torch.tensor([1] * self.batch_size, dtype=torch.long, device=torch_device)\n+        model = self.sequence_classification_class(config=config).to(torch_device).eval()\n+        outputs = model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            decoder_input_ids=decoder_input_ids,\n+            labels=labels,\n+        )\n+        self.parent.assertEqual(outputs[\"logits\"].size(), (self.batch_size, config.num_labels))\n+        self.parent.assertEqual(outputs[\"loss\"].size(), ())\n+\n+    def create_and_check_with_token_classification_head(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+        pixel_values,\n+    ):\n+        labels = torch.tensor([1] * self.seq_length * self.batch_size, dtype=torch.long, device=torch_device)\n+        model = self.token_classification_class(config=config)\n+        model = model.to(torch_device).eval()\n+        outputs = model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            decoder_input_ids=decoder_input_ids,\n+            labels=labels,\n+        )\n+\n+        self.parent.assertEqual(outputs[\"logits\"].size(), (self.batch_size, self.seq_length, config.num_labels))\n+        self.parent.assertEqual(outputs[\"loss\"].size(), ())\n+\n+    def create_and_check_decoder_model_past(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+        pixel_values,\n+    ):\n+        model = self.model_class(config=config).get_decoder().to(torch_device).eval()\n+        encoder_hidden_states = torch.ones(\n+            (self.batch_size, self.encoder_seq_length, self.encoder_hidden_size), dtype=torch.float32\n+        ).to(torch_device)\n+\n+        # first forward pass\n+        outputs = model(decoder_input_ids, encoder_hidden_states=encoder_hidden_states, use_cache=True)\n+        outputs_use_cache_conf = model(decoder_input_ids, encoder_hidden_states=encoder_hidden_states)\n+        outputs_no_past = model(decoder_input_ids, encoder_hidden_states=encoder_hidden_states, use_cache=False)\n+\n+        self.parent.assertTrue(len(outputs) == len(outputs_use_cache_conf))\n+        self.parent.assertTrue(len(outputs) == len(outputs_no_past) + 1)\n+\n+        output, past_key_values = outputs.to_tuple()\n+\n+        # create hypothetical next token and extent to next_input_ids\n+        next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size)\n+\n+        # append to next input_ids and\n+        next_input_ids = torch.cat([decoder_input_ids, next_tokens], dim=-1)\n+\n+        output_from_no_past = model(next_input_ids, encoder_hidden_states=encoder_hidden_states)[\"last_hidden_state\"]\n+        output_from_past = model(\n+            next_tokens, encoder_hidden_states=encoder_hidden_states, past_key_values=past_key_values\n+        )[\"last_hidden_state\"]\n+\n+        # select random slice\n+        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n+        output_from_no_past_slice = output_from_no_past[:, -1, random_slice_idx].detach()\n+        output_from_past_slice = output_from_past[:, 0, random_slice_idx].detach()\n+\n+        # test that outputs are equal for slice\n+        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n+\n+    def create_and_check_decoder_model_attention_mask_past(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+        pixel_values,\n+    ):\n+        model = self.model_class(config=config).get_decoder().to(torch_device).eval()\n+        encoder_hidden_states = torch.ones(\n+            (self.batch_size, self.encoder_seq_length, self.encoder_hidden_size), dtype=torch.float32\n+        ).to(torch_device)\n+\n+        # create attention mask\n+        attn_mask = torch.ones(decoder_input_ids.shape, dtype=torch.long, device=torch_device)\n+\n+        half_seq_length = decoder_input_ids.shape[-1] // 2\n+        attn_mask[:, half_seq_length:] = 0\n+\n+        # first forward pass\n+        output, past_key_values = model(\n+            decoder_input_ids, encoder_hidden_states=encoder_hidden_states, attention_mask=attn_mask, use_cache=True\n+        ).to_tuple()\n+\n+        # create hypothetical next token and extent to next_input_ids\n+        next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size)\n+\n+        # change a random masked slice from input_ids\n+        random_seq_idx_to_change = ids_tensor((1,), half_seq_length).item() + 1\n+        random_other_next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size).squeeze(-1)\n+        decoder_input_ids[:, -random_seq_idx_to_change] = random_other_next_tokens\n+\n+        # append to next input_ids and attn_mask\n+        next_input_ids = torch.cat([decoder_input_ids, next_tokens], dim=-1)\n+        attn_mask = torch.cat(\n+            [attn_mask, torch.ones((attn_mask.shape[0], 1), dtype=torch.long, device=torch_device)],\n+            dim=1,\n+        )\n+\n+        # get two different outputs\n+        output_from_no_past = model(\n+            next_input_ids, encoder_hidden_states=encoder_hidden_states, attention_mask=attn_mask\n+        )[\"last_hidden_state\"]\n+        output_from_past = model(\n+            next_tokens,\n+            encoder_hidden_states=encoder_hidden_states,\n+            past_key_values=past_key_values,\n+            attention_mask=attn_mask,\n+        )[\"last_hidden_state\"]\n+\n+        # select random slice\n+        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n+        output_from_no_past_slice = output_from_no_past[:, -1, random_slice_idx].detach()\n+        output_from_past_slice = output_from_past[:, 0, random_slice_idx].detach()\n+\n+        # test that outputs are equal for slice\n+        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n+\n+    def create_and_check_decoder_model_past_large_inputs(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+        pixel_values,\n+    ):\n+        model = self.model_class(config=config).get_decoder().to(torch_device).eval()\n+        encoder_hidden_states = torch.ones(\n+            (self.batch_size, self.encoder_seq_length, self.encoder_hidden_size), dtype=torch.float32\n+        ).to(torch_device)\n+\n+        # first forward pass\n+        outputs = model(\n+            decoder_input_ids,\n+            encoder_hidden_states=encoder_hidden_states,\n+            attention_mask=attention_mask,\n+            use_cache=True,\n+        )\n+\n+        output, past_key_values = outputs.to_tuple()\n+\n+        # create hypothetical multiple next token and extent to next_input_ids\n+        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n+        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n+\n+        # append to next input_ids and\n+        next_input_ids = torch.cat([decoder_input_ids, next_tokens], dim=-1)\n+        next_attention_mask = torch.cat([attention_mask, next_mask], dim=-1)\n+\n+        output_from_no_past = model(\n+            next_input_ids, encoder_hidden_states=encoder_hidden_states, attention_mask=next_attention_mask\n+        )[\"last_hidden_state\"]\n+        output_from_past = model(\n+            next_tokens,\n+            encoder_hidden_states=encoder_hidden_states,\n+            attention_mask=next_attention_mask,\n+            past_key_values=past_key_values,\n+        )[\"last_hidden_state\"]\n+\n+        # select random slice\n+        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n+        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n+        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n+\n+        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n+\n+        # test that outputs are equal for slice\n+        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n+\n+    def create_and_check_generate_with_past_key_values(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+        pixel_values,\n+    ):\n+        model = self.causal_lm_class(config=config).to(torch_device).eval()\n+        torch.manual_seed(0)\n+        output_without_past_cache = model.generate(\n+            input_ids, pixel_values=pixel_values, num_beams=2, max_length=5, do_sample=True, use_cache=False\n+        )\n+        torch.manual_seed(0)\n+        output_with_past_cache = model.generate(\n+            input_ids, pixel_values=pixel_values, num_beams=2, max_length=5, do_sample=True\n+        )\n+        self.parent.assertTrue(torch.all(output_with_past_cache == output_without_past_cache))\n+\n+    def create_and_check_model_fp16_forward(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+        pixel_values,\n+    ):\n+        model = self.model_class(config=config).to(torch_device).half().eval()\n+        output = model(\n+            input_ids,\n+            pixel_values=pixel_values,\n+            decoder_input_ids=decoder_input_ids,\n+            attention_mask=attention_mask,\n+            decoder_attention_mask=decoder_attention_mask,\n+        )[\"last_hidden_state\"]\n+        self.parent.assertFalse(torch.isnan(output).any().item())\n+\n+\n+@require_torch\n+class T5Gemma2ModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    all_model_classes = (\n+        (\n+            T5Gemma2Model,\n+            T5Gemma2ForConditionalGeneration,\n+            T5Gemma2ForSequenceClassification,\n+            T5Gemma2ForTokenClassification,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+\n+    _is_stateful = True\n+    is_encoder_decoder = True\n+\n+    # MP works but offload doesn't work when the SigLIP MultiheadAttention is offloaded\n+    test_cpu_offload = False\n+    test_disk_offload_safetensors = False\n+    test_disk_offload_bin = False\n+\n+    def setUp(self):\n+        self.model_tester = T5Gemma2ModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=T5Gemma2Config,\n+            # For faking the testing.\n+            hidden_size=37,\n+            vocab_size=self.model_tester.vocab_size,\n+            num_attention_heads=self.model_tester.num_attention_heads,\n+            num_hidden_layers=self.model_tester.num_hidden_layers,\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_shift_right(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.check_prepare_lm_labels_via_shift_left(*config_and_inputs)\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    # Based on tests.models.t5.test_modeling_t5.T5ModelTest.test_inputs_embeds\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in (T5Gemma2Model, T5Gemma2ForConditionalGeneration):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))\n+\n+            if not self.is_encoder_decoder:\n+                input_ids = inputs[\"input_ids\"]\n+                del inputs[\"input_ids\"]\n+            else:\n+                encoder_input_ids = inputs[\"input_ids\"]\n+                decoder_input_ids = inputs.get(\"decoder_input_ids\", encoder_input_ids)\n+                del inputs[\"input_ids\"]\n+                inputs.pop(\"decoder_input_ids\", None)\n+\n+            wte = model.get_input_embeddings()\n+            if not self.is_encoder_decoder:\n+                inputs[\"inputs_embeds\"] = wte(input_ids)\n+            else:\n+                inputs[\"inputs_embeds\"] = wte(encoder_input_ids)\n+                inputs[\"decoder_inputs_embeds\"] = wte(decoder_input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)[0]\n+\n+    def test_with_lm_head(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_with_lm_head(*config_and_inputs)\n+\n+    def test_with_sequence_classification_head(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_with_sequence_classification_head(*config_and_inputs)\n+\n+    def test_with_token_classification_head(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_with_token_classification_head(*config_and_inputs)\n+\n+    def test_decoder_model_past(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_decoder_model_past(*config_and_inputs)\n+\n+    def test_decoder_model_past_with_attn_mask(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_decoder_model_attention_mask_past(*config_and_inputs)\n+\n+    def test_decoder_model_past_with_large_inputs(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n+\n+    def test_generate_with_past_key_values(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_generate_with_past_key_values(*config_and_inputs)\n+\n+    @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n+    def test_model_fp16_forward(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model_fp16_forward(*config_and_inputs)\n+\n+    # Based on tests.models.gemma.test_modeling_gemma.GemmaModelTest.test_Gemma_sequence_classification_model with Gemma -> T5Gemma2 (Add is_encoder_decoder option)\n+    def test_T5Gemma2_sequence_classification_model(self):\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_labels = 3\n+        input_ids = input_dict[\"input_ids\"]\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n+\n+        for pixel_values in [None, input_dict[\"pixel_values\"]]:\n+            model = self.model_tester.sequence_classification_class(config).to(torch_device).eval()\n+            result = model(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, labels=sequence_labels)\n+            self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n+\n+    # Based on tests.models.gemma.test_modeling_gemma.GemmaModelTest.test_Gemma_sequence_classification_model_for_single_label with Gemma -> T5Gemma2 (Add is_encoder_decoder option)\n+    def test_T5Gemma2_sequence_classification_model_for_single_label(self):\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_labels = 3\n+        config.problem_type = \"single_label_classification\"\n+        input_ids = input_dict[\"input_ids\"]\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n+\n+        for pixel_values in [None, input_dict[\"pixel_values\"]]:\n+            model = self.model_tester.sequence_classification_class(config).to(torch_device).eval()\n+            result = model(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, labels=sequence_labels)\n+            self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n+\n+    # Based on tests.models.gemma.test_modeling_gemma.GemmaModelTest.test_Gemma_sequence_classification_model_for_multi_label with Gemma -> T5Gemma2 (Add is_encoder_decoder option)\n+    def test_T5Gemma2_sequence_classification_model_for_multi_label(self):\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_labels = 3\n+        config.problem_type = \"multi_label_classification\"\n+        input_ids = input_dict[\"input_ids\"]\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        sequence_labels = ids_tensor(\n+            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n+        ).to(torch.float)\n+\n+        for pixel_values in [None, input_dict[\"pixel_values\"]]:\n+            model = self.model_tester.sequence_classification_class(config).to(torch_device).eval()\n+            result = model(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, labels=sequence_labels)\n+            self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n+\n+    # Based on tests.models.gemma.test_modeling_gemma.GemmaModelTest.test_Gemma_token_classification_model with Gemma -> T5Gemma2 (Add is_encoder_decoder option)\n+    def test_T5Gemma2_token_classification_model(self):\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_labels = 3\n+        input_ids = input_dict[\"input_ids\"]\n+        decoder_input_ids = input_dict[\"decoder_input_ids\"]\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        token_labels = ids_tensor([self.model_tester.batch_size, self.model_tester.seq_length], config.num_labels)\n+\n+        for pixel_values in [None, input_dict[\"pixel_values\"]]:\n+            model = self.model_tester.token_classification_class(config).to(torch_device).eval()\n+\n+            result = model(\n+                input_ids,\n+                decoder_input_ids=decoder_input_ids,\n+                pixel_values=pixel_values,\n+                attention_mask=attention_mask,\n+                labels=token_labels,\n+            )\n+            self.assertEqual(\n+                result.logits.shape,\n+                (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n+            )\n+\n+    @unittest.skip(\"This was not properly written, submodules need the attribute to be overwritten\")\n+    def test_attention_outputs(self):\n+        pass\n+\n+    @unittest.skip(\"Mismatch issue doesn't exist in T5Gemma2.\")\n+    def test_load_with_mismatched_shapes(self):\n+        pass\n+\n+    # Based on tests.generation.test_utils.GenerationTesterMixin.test_generate_continue_from_past_key_values\n+    # Updated decoder_attention_mask to consider the appended bos token\n+    @pytest.mark.generate\n+    def test_generate_continue_from_past_key_values(self):\n+        # Tests that we can continue generating from past key values, returned from a previous `generate` call\n+        for model_class in self.all_generative_model_classes:\n+            if model_class == self.model_tester.token_classification_class:\n+                continue\n+            if any(model_name in model_class.__name__.lower() for model_name in [\"imagegpt\", \"mllama\"]):\n+                self.skipTest(reason=\"Won't fix: old model with unique inputs/caches/other\")\n+            if any(model_name in model_class.__name__.lower() for model_name in [\"umt5\"]):\n+                self.skipTest(reason=\"TODO: needs modeling or test input preparation fixes for compatibility\")\n+\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            if not hasattr(config.get_text_config(), \"use_cache\"):\n+                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n+\n+            # Let's make it always:\n+            # 1. use cache (for obvious reasons)\n+            # 2. generate to max length (which can be achieved by setting the eos token to an invalid value), which\n+            #    would make the test flaky (e.g. EOS is generated on iteration 1 on both generations, but the\n+            #    continuation would force it to generate beyond an EOS token)\n+            # 3. ignore `token_type_ids` for simplicity\n+            # 4. ignore `forced_eos_token_id`, which requires further manipulation of the continuation inputs and is\n+            #    active by default on some models\n+            # 5. ignore `encoder_no_repeat_ngram_size`, which is set by default in some encoder-decoder models. When\n+            #    we use their decoder as a stand-alone model, `encoder_no_repeat_ngram_size` actually prevents\n+            #    repetition exclusively from the prompt. This test relies on comparing one call vs 2 calls\n+            #    with cache, what is considered a prompt is different in the two cases.\n+\n+            if \"token_type_ids\" in inputs:\n+                del inputs[\"token_type_ids\"]\n+\n+            model = model_class(config).to(torch_device)\n+            model.eval()\n+\n+            # If \"past_key_values\" is not returned, skip the test (e.g. RWKV uses a different cache name and format)\n+            outputs = model(**inputs)\n+            if \"past_key_values\" not in outputs:\n+                self.skipTest(reason=\"This model doesn't return `past_key_values`\")\n+\n+            generate_kwargs = {\n+                \"pad_token_id\": -1,\n+                \"eos_token_id\": -1,\n+                \"forced_eos_token_id\": None,\n+                \"encoder_no_repeat_ngram_size\": 0,\n+                \"use_cache\": True,\n+                \"do_sample\": False,\n+                \"return_dict_in_generate\": True,\n+                \"output_scores\": True,\n+            }\n+\n+            # Traditional way of generating text, with `return_dict_in_generate` to return the past key values\n+            outputs = model.generate(**inputs, **generate_kwargs, max_new_tokens=4)\n+\n+            # Let's generate again, but passing the past key values in between (3 + 1 = 4 tokens). Note that the\n+            # inputs may need to be tweaked across `generate` calls (like the attention mask).\n+            outputs_cached = model.generate(**inputs, **generate_kwargs, max_new_tokens=3)\n+\n+            # Continue from the tokens generated above, preparing the inputs accordingly\n+            inputs[\"past_key_values\"] = outputs_cached.past_key_values\n+            new_attention_len = outputs_cached.sequences.shape[-1]\n+\n+            # It must be encoder-decoder models\n+            self.assertTrue(config.is_encoder_decoder)\n+\n+            inputs[\"decoder_input_ids\"] = outputs_cached.sequences\n+            if \"decoder_attention_mask\" in inputs:\n+                decoder_attention_mask = inputs[\"decoder_attention_mask\"]\n+\n+                # Add BOS mask: the new sequence comes with a new BOS token, which is not included in the original inputs\n+                padding_tensor = torch.ones_like(decoder_attention_mask[:, :1])\n+                decoder_attention_mask = torch.cat([padding_tensor, decoder_attention_mask], dim=1)\n+\n+                inputs[\"decoder_attention_mask\"] = torch.nn.functional.pad(\n+                    decoder_attention_mask,\n+                    (0, new_attention_len - decoder_attention_mask.shape[1]),\n+                    mode=\"constant\",\n+                    value=1,\n+                )\n+\n+            first_caches_scores = outputs_cached.scores\n+            outputs_cached = model.generate(**inputs, **generate_kwargs, max_new_tokens=1)\n+            full_cached_scores = first_caches_scores + outputs_cached.scores\n+            outputs_cached.scores = full_cached_scores\n+\n+            # The two sets of generated text and past kv should be equal to each other\n+            self.assertTrue(has_similar_generate_outputs(outputs, outputs_cached))\n+            self._check_caches_are_equal(outputs.past_key_values, outputs_cached.past_key_values)\n+\n+    @unittest.skip(\"T5Gemma 2 only support final layer hidden states.\")\n+    def test_hidden_states_output(self):\n+        pass\n+\n+    # Based on tests.models.t5.test_modeling_t5.T5ModelTest.test_custom_4d_attention_mask\n+    # Excluding the final token from input_ids\n+    def test_custom_4d_attention_mask(self):\n+        for model_class in self.all_generative_model_classes:\n+            config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n+\n+            (\n+                input_ids,\n+                position_ids,\n+                input_ids_shared_prefix,\n+                mask_shared_prefix,\n+                position_ids_shared_prefix,\n+            ) = self._get_custom_4d_mask_test_data()\n+            mask_shared_prefix = mask_shared_prefix == 0.0\n+\n+            outputs = model.forward(\n+                decoder_input_ids=input_ids,\n+                input_ids=input_ids[:, :-1],\n+                decoder_position_ids=position_ids,\n+            )\n+            logits = outputs.logits\n+            # logits.shape == torch.Size([3, 4, ...])\n+\n+            outputs_shared_prefix = model(\n+                input_ids=input_ids[:1, :-1],\n+                decoder_input_ids=input_ids_shared_prefix,\n+                decoder_attention_mask=mask_shared_prefix,\n+                decoder_position_ids=position_ids_shared_prefix,\n+            )\n+            logits_shared_prefix = outputs_shared_prefix.logits\n+            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n+\n+            torch.testing.assert_close(\n+                outputs.encoder_last_hidden_state[0], outputs_shared_prefix.encoder_last_hidden_state[0]\n+            )\n+\n+            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n+            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n+\n+            # comparing softmax-normalized logits:\n+            normalized_0 = F.softmax(out_last_tokens)\n+            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n+            torch.testing.assert_close(normalized_0[2], normalized_1[2], rtol=1e-3, atol=1e-4)\n+            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)\n+\n+    @unittest.skip(reason=\"SiglipVisionModel (vision backbone) does not support standalone training\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SiglipVisionModel (vision backbone) does not support standalone training\")\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SiglipVisionModel (vision backbone) does not support standalone training\")\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SiglipVisionModel (vision backbone) does not support standalone training\")\n+    def test_torch_compile_for_training(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Self&cross attention are splited after the merged attention\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"Merged attention module will always require a mask which is incompatible with the FA backend\"\n+    )\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        pass"
        },
        {
            "sha": "43cfd1ad72d39675025e28d3e9f0a7c564d30411",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2375ddb426147f9e94e12be8d6a54f7a418e1e98/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2375ddb426147f9e94e12be8d6a54f7a418e1e98/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=2375ddb426147f9e94e12be8d6a54f7a418e1e98",
            "patch": "@@ -3372,8 +3372,10 @@ def test_flash_attn_2_can_compile_with_attention_mask_None_without_graph_break(s\n \n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n         cls = self._torch_compile_train_cls  # e.g. LlamaFroCausalLM\n-        model = cls._from_config(config, attn_implementation=\"flash_attention_2\").to(device=torch_device, dtype=dtype)\n+        if not cls._supports_flash_attn:\n+            self.skipTest(f\"{cls.__name__} does not support Flash Attention 2\")\n \n+        model = cls._from_config(config, attn_implementation=\"flash_attention_2\").to(device=torch_device, dtype=dtype)\n         inputs = {\n             \"input_ids\": torch.randint(low=1, high=model.config.vocab_size, size=(2, 10), device=torch_device),\n             \"labels\": torch.randint(low=1, high=model.config.vocab_size, size=(2, 10), device=torch_device),"
        },
        {
            "sha": "ac379f6188234077f02db9cc55926ce968276e34",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2375ddb426147f9e94e12be8d6a54f7a418e1e98/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2375ddb426147f9e94e12be8d6a54f7a418e1e98/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=2375ddb426147f9e94e12be8d6a54f7a418e1e98",
            "patch": "@@ -279,6 +279,8 @@\n     \"GPTNeoXConfig\": [\"rotary_emb_base\"],\n     \"Gemma3Config\": [\"boi_token_index\", \"eoi_token_index\"],\n     \"Gemma3TextConfig\": [\"cache_implementation\", \"tie_word_embeddings\"],\n+    \"T5Gemma2TextConfig\": [\"tie_word_embeddings\"],\n+    \"T5Gemma2DecoderConfig\": [\"tie_word_embeddings\"],\n     \"ShieldGemma2Config\": [\n         \"boi_token_index\",\n         \"eoi_token_index\","
        }
    ],
    "stats": {
        "total": 4711,
        "additions": 4708,
        "deletions": 3
    }
}