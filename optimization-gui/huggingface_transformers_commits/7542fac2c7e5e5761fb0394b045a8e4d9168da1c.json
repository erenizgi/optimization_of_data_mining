{
    "author": "gante",
    "message": "Pipeline: no side-effects on `model.config` and `model.generation_config` ðŸ”«  (#33480)",
    "sha": "7542fac2c7e5e5761fb0394b045a8e4d9168da1c",
    "files": [
        {
            "sha": "5e9ac835c19d6d602759b5d85e76eda1a9e9a32c",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=7542fac2c7e5e5761fb0394b045a8e4d9168da1c",
            "patch": "@@ -1229,6 +1229,10 @@ def from_model_config(cls, model_config: PretrainedConfig) -> \"GenerationConfig\"\n         \"\"\"\n         config_dict = model_config.to_dict()\n         config_dict.pop(\"_from_model_config\", None)\n+\n+        # Removes all `None` from the model config dict -- this lets the generation config defaults to take hold\n+        config_dict = {key: value for key, value in config_dict.items() if value is not None}\n+\n         generation_config = cls.from_dict(config_dict, return_unused_kwargs=False, _from_model_config=True)\n \n         # Special case: some models have generation attributes set in the decoder. Use them if still unset in the"
        },
        {
            "sha": "d8896f91267d7b4d9d2f60616bac68c30889035e",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=7542fac2c7e5e5761fb0394b045a8e4d9168da1c",
            "patch": "@@ -1334,23 +1334,26 @@ def _prepare_generation_config(\n             # the following conditions must be met\n             # 1) the generation config must have been created from the model config (`_from_model_config` field);\n             # 2) the generation config must have seen no modification since its creation (the hash is the same);\n-            # 3) the user must have set generation parameters in the model config.\n+            # 3) there are non-default generation parameters in the model config.\n+            # 4) the user must have set new generation parameters in the model config.\n             # NOTE: `torch.compile` can't compile `hash`, this legacy support is disabled with compilation.\n             if (\n                 not is_torchdynamo_compiling()\n                 and self.generation_config._from_model_config  # 1)\n                 and self.generation_config._original_object_hash == hash(self.generation_config)  # 2)\n+                and len(self.config._get_non_default_generation_parameters()) > 0  # 3)\n             ):\n                 new_generation_config = GenerationConfig.from_model_config(self.config)\n-                if new_generation_config != self.generation_config:  # 3)\n+                if new_generation_config != self.generation_config:  # 4)\n                     warnings.warn(\n                         \"You have modified the pretrained model configuration to control generation. This is a\"\n-                        \" deprecated strategy to control generation and will be removed soon, in a future version.\"\n+                        \" deprecated strategy to control generation and will be removed in v5.\"\n                         \" Please use and modify the model generation configuration (see\"\n-                        \" https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\"\n+                        \" https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\",\n+                        UserWarning,\n                     )\n                     self.generation_config = new_generation_config\n-            using_model_generation_config = True\n+\n             generation_config = self.generation_config\n             using_model_generation_config = True\n "
        },
        {
            "sha": "7c122bed5437cc67a2b6191c6dc1d0f07e5a840a",
            "filename": "src/transformers/pipelines/automatic_speech_recognition.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py?ref=7542fac2c7e5e5761fb0394b045a8e4d9168da1c",
            "patch": "@@ -501,6 +501,10 @@ def _forward(self, model_inputs, return_timestamps=False, **generate_kwargs):\n                     else:\n                         generate_kwargs[\"num_frames\"] = num_frames\n \n+            # User-defined `generation_config` passed to the pipeline call take precedence\n+            if \"generation_config\" not in generate_kwargs:\n+                generate_kwargs[\"generation_config\"] = self.generation_config\n+\n             tokens = self.model.generate(\n                 inputs=inputs,\n                 attention_mask=attention_mask,"
        },
        {
            "sha": "40a91a0d484b8e84cd91103035f8634e6730d8cf",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 21,
            "deletions": 16,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=7542fac2c7e5e5761fb0394b045a8e4d9168da1c",
            "patch": "@@ -13,6 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import collections\n+import copy\n import csv\n import importlib\n import json\n@@ -899,22 +900,26 @@ def __init__(\n         ):\n             self.model.to(self.device)\n \n-        # Update config and generation_config with task specific parameters\n-        task_specific_params = self.model.config.task_specific_params\n-        if task_specific_params is not None and task in task_specific_params:\n-            self.model.config.update(task_specific_params.get(task))\n-            if self.model.can_generate():\n-                self.model.generation_config.update(**task_specific_params.get(task))\n-\n-        # Pipelines calling `generate`: if the tokenizer has a pad token but the model doesn't, set it in the\n-        # forward params so that `generate` is aware of the pad token.\n-        if (\n-            self.tokenizer is not None\n-            and self.model.can_generate()\n-            and self.tokenizer.pad_token_id is not None\n-            and self.model.generation_config.pad_token_id is None\n-        ):\n-            self.model.generation_config.pad_token_id = self.tokenizer.pad_token_id\n+        # If the model can generate, create a local generation config. This is done to avoid side-effects on the model\n+        # as we apply local tweaks to the generation config.\n+        if self.model.can_generate():\n+            self.prefix = self.model.config.prefix if hasattr(self.model.config, \"prefix\") else None\n+            self.generation_config = copy.deepcopy(self.model.generation_config)\n+            # Update the generation config with task specific params if they exist\n+            # NOTE: `prefix` is pipeline-specific and doesn't exist in the generation config.\n+            task_specific_params = self.model.config.task_specific_params\n+            if task_specific_params is not None and task in task_specific_params:\n+                this_task_params = task_specific_params.get(task)\n+                if \"prefix\" in this_task_params:\n+                    self.prefix = this_task_params.pop(\"prefix\")\n+                self.generation_config.update(**this_task_params)\n+            # If the tokenizer has a pad token but the model doesn't, set it so that `generate` is aware of it.\n+            if (\n+                self.tokenizer is not None\n+                and self.tokenizer.pad_token_id is not None\n+                and self.generation_config.pad_token_id is None\n+            ):\n+                self.generation_config.pad_token_id = self.tokenizer.pad_token_id\n \n         self.call_count = 0\n         self._batch_size = kwargs.pop(\"batch_size\", None)"
        },
        {
            "sha": "9198f43226382294357d2bd189fa22c6f841d694",
            "filename": "src/transformers/pipelines/document_question_answering.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py?ref=7542fac2c7e5e5761fb0394b045a8e4d9168da1c",
            "patch": "@@ -429,6 +429,10 @@ def _forward(self, model_inputs, **generate_kwargs):\n         is_last = model_inputs.pop(\"is_last\", False)\n \n         if self.model_type == ModelType.VisionEncoderDecoder:\n+            # User-defined `generation_config` passed to the pipeline call take precedence\n+            if \"generation_config\" not in generate_kwargs:\n+                generate_kwargs[\"generation_config\"] = self.generation_config\n+\n             model_outputs = self.model.generate(**model_inputs, **generate_kwargs)\n         else:\n             model_outputs = self.model(**model_inputs)"
        },
        {
            "sha": "91d44c46d25c10bea259ce048132a43f03e0e6cc",
            "filename": "src/transformers/pipelines/image_to_text.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py?ref=7542fac2c7e5e5761fb0394b045a8e4d9168da1c",
            "patch": "@@ -181,6 +181,10 @@ def _forward(self, model_inputs, **generate_kwargs):\n         ):\n             model_inputs[\"input_ids\"] = None\n \n+        # User-defined `generation_config` passed to the pipeline call take precedence\n+        if \"generation_config\" not in generate_kwargs:\n+            generate_kwargs[\"generation_config\"] = self.generation_config\n+\n         # FIXME: We need to pop here due to a difference in how `generation.py` and `generation.tf_utils.py`\n         #  parse inputs. In the Tensorflow version, `generate` raises an error if we don't use `input_ids` whereas\n         #  the PyTorch version matches it with `self.model.main_input_name` or `self.model.encoder.main_input_name`"
        },
        {
            "sha": "77c95432c7218f311674b280cc43b52bc0e1be4a",
            "filename": "src/transformers/pipelines/table_question_answering.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py?ref=7542fac2c7e5e5761fb0394b045a8e4d9168da1c",
            "patch": "@@ -385,6 +385,10 @@ def _forward(self, model_inputs, sequential=False, **generate_kwargs):\n             else:\n                 outputs = self.batch_inference(**model_inputs)\n         else:\n+            # User-defined `generation_config` passed to the pipeline call take precedence\n+            if \"generation_config\" not in generate_kwargs:\n+                generate_kwargs[\"generation_config\"] = self.generation_config\n+\n             outputs = self.model.generate(**model_inputs, **generate_kwargs)\n         model_outputs = {\"model_inputs\": model_inputs, \"table\": table, \"outputs\": outputs}\n         return model_outputs"
        },
        {
            "sha": "75ded8ac085ca54cd0765d06ac02dd847a6d3d06",
            "filename": "src/transformers/pipelines/text2text_generation.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py?ref=7542fac2c7e5e5761fb0394b045a8e4d9168da1c",
            "patch": "@@ -115,7 +115,7 @@ def check_inputs(self, input_length: int, min_length: int, max_length: int):\n         return True\n \n     def _parse_and_tokenize(self, *args, truncation):\n-        prefix = self.model.config.prefix if self.model.config.prefix is not None else \"\"\n+        prefix = self.prefix if self.prefix is not None else \"\"\n         if isinstance(args[0], list):\n             if self.tokenizer.pad_token_id is None:\n                 raise ValueError(\"Please make sure that the tokenizer has a pad_token_id when using a batch input\")\n@@ -185,9 +185,14 @@ def _forward(self, model_inputs, **generate_kwargs):\n \n         self.check_inputs(\n             input_length,\n-            generate_kwargs.get(\"min_length\", self.model.config.min_length),\n-            generate_kwargs.get(\"max_length\", self.model.config.max_length),\n+            generate_kwargs.get(\"min_length\", self.generation_config.min_length),\n+            generate_kwargs.get(\"max_length\", self.generation_config.max_length),\n         )\n+\n+        # User-defined `generation_config` passed to the pipeline call take precedence\n+        if \"generation_config\" not in generate_kwargs:\n+            generate_kwargs[\"generation_config\"] = self.generation_config\n+\n         output_ids = self.model.generate(**model_inputs, **generate_kwargs)\n         out_b = output_ids.shape[0]\n         if self.framework == \"pt\":"
        },
        {
            "sha": "9bffca522d5f2e598f6094dfcbe01aa608cf4c15",
            "filename": "src/transformers/pipelines/text_generation.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_generation.py?ref=7542fac2c7e5e5761fb0394b045a8e4d9168da1c",
            "patch": "@@ -103,8 +103,8 @@ def __init__(self, *args, **kwargs):\n             # It also defines both some preprocess_kwargs and generate_kwargs\n             # which is why we cannot put them in their respective methods.\n             prefix = None\n-            if self.model.config.prefix is not None:\n-                prefix = self.model.config.prefix\n+            if self.prefix is not None:\n+                prefix = self.prefix\n             if prefix is None and self.model.__class__.__name__ in [\n                 \"XLNetLMHeadModel\",\n                 \"TransfoXLLMHeadModel\",\n@@ -316,7 +316,7 @@ def preprocess(\n             if \"max_new_tokens\" in generate_kwargs:\n                 new_tokens = generate_kwargs[\"max_new_tokens\"]\n             else:\n-                new_tokens = generate_kwargs.get(\"max_length\", self.model.config.max_length) - cur_len\n+                new_tokens = generate_kwargs.get(\"max_length\", self.generation_config.max_length) - cur_len\n                 if new_tokens < 0:\n                     raise ValueError(\"We cannot infer how many new tokens are expected\")\n             if cur_len + new_tokens > self.tokenizer.model_max_length:\n@@ -354,7 +354,7 @@ def _forward(self, model_inputs, **generate_kwargs):\n                 and generate_kwargs[\"generation_config\"].max_new_tokens is not None\n             )\n             if not has_max_new_tokens:\n-                generate_kwargs[\"max_length\"] = generate_kwargs.get(\"max_length\") or self.model.config.max_length\n+                generate_kwargs[\"max_length\"] = generate_kwargs.get(\"max_length\") or self.generation_config.max_length\n                 generate_kwargs[\"max_length\"] += prefix_length\n             has_min_new_tokens = \"min_new_tokens\" in generate_kwargs or (\n                 \"generation_config\" in generate_kwargs\n@@ -363,7 +363,10 @@ def _forward(self, model_inputs, **generate_kwargs):\n             if not has_min_new_tokens and \"min_length\" in generate_kwargs:\n                 generate_kwargs[\"min_length\"] += prefix_length\n \n-        # BS x SL\n+        # User-defined `generation_config` passed to the pipeline call take precedence\n+        if \"generation_config\" not in generate_kwargs:\n+            generate_kwargs[\"generation_config\"] = self.generation_config\n+\n         generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n         out_b = generated_sequence.shape[0]\n         if self.framework == \"pt\":"
        },
        {
            "sha": "d17d18205920b0a1187238b7605395ca4a74e03a",
            "filename": "src/transformers/pipelines/text_to_audio.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py?ref=7542fac2c7e5e5761fb0394b045a8e4d9168da1c",
            "patch": "@@ -111,7 +111,7 @@ def preprocess(self, text, **kwargs):\n         if self.model.config.model_type == \"bark\":\n             # bark Tokenizer is called with BarkProcessor which uses those kwargs\n             new_kwargs = {\n-                \"max_length\": self.model.generation_config.semantic_config.get(\"max_input_semantic_length\", 256),\n+                \"max_length\": self.generation_config.semantic_config.get(\"max_input_semantic_length\", 256),\n                 \"add_special_tokens\": False,\n                 \"return_attention_mask\": True,\n                 \"return_token_type_ids\": False,\n@@ -137,6 +137,10 @@ def _forward(self, model_inputs, **kwargs):\n             # we expect some kwargs to be additional tensors which need to be on the right device\n             generate_kwargs = self._ensure_tensor_on_device(generate_kwargs, device=self.device)\n \n+            # User-defined `generation_config` passed to the pipeline call take precedence\n+            if \"generation_config\" not in generate_kwargs:\n+                generate_kwargs[\"generation_config\"] = self.generation_config\n+\n             # generate_kwargs get priority over forward_params\n             forward_params.update(generate_kwargs)\n "
        },
        {
            "sha": "89988c0cba2b1b065066aa50b0c941aa3476aae2",
            "filename": "src/transformers/pipelines/visual_question_answering.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Fvisual_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/src%2Ftransformers%2Fpipelines%2Fvisual_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fvisual_question_answering.py?ref=7542fac2c7e5e5761fb0394b045a8e4d9168da1c",
            "patch": "@@ -162,6 +162,10 @@ def preprocess(self, inputs, padding=False, truncation=False, timeout=None):\n \n     def _forward(self, model_inputs, **generate_kwargs):\n         if self.model.can_generate():\n+            # User-defined `generation_config` passed to the pipeline call take precedence\n+            if \"generation_config\" not in generate_kwargs:\n+                generate_kwargs[\"generation_config\"] = self.generation_config\n+\n             model_outputs = self.model.generate(**model_inputs, **generate_kwargs)\n         else:\n             model_outputs = self.model(**model_inputs)"
        },
        {
            "sha": "1fec4be3d95ca0504d75b24d590a162092fadca5",
            "filename": "tests/pipelines/test_pipelines_common.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/tests%2Fpipelines%2Ftest_pipelines_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/tests%2Fpipelines%2Ftest_pipelines_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_common.py?ref=7542fac2c7e5e5761fb0394b045a8e4d9168da1c",
            "patch": "@@ -31,6 +31,7 @@\n     AutoTokenizer,\n     DistilBertForSequenceClassification,\n     MaskGenerationPipeline,\n+    T5ForConditionalGeneration,\n     TextClassificationPipeline,\n     TextGenerationPipeline,\n     TFAutoModelForSequenceClassification,\n@@ -234,6 +235,31 @@ def test_auto_model_pipeline_registration_from_local_dir(self):\n \n             self.assertIsInstance(pipe, TextGenerationPipeline)  # Assert successful load\n \n+    @require_torch\n+    def test_pipeline_with_task_parameters_no_side_effects(self):\n+        \"\"\"\n+        Regression test: certain pipeline flags, like `task`, modified the model configuration, causing unexpected\n+        side-effects\n+        \"\"\"\n+        # This checkpoint has task-specific parameters that will modify the behavior of the pipeline\n+        model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n+        self.assertTrue(model.config.num_beams == 1)\n+\n+        # The task-specific parameters used to cause side-effects on `model.config` -- not anymore\n+        pipe = pipeline(model=model, tokenizer=AutoTokenizer.from_pretrained(\"t5-small\"), task=\"translation_en_to_de\")\n+        self.assertTrue(model.config.num_beams == 1)\n+        self.assertTrue(model.generation_config.num_beams == 1)\n+\n+        # Under the hood: we now store a generation config in the pipeline. This generation config stores the\n+        # task-specific paremeters.\n+        self.assertTrue(pipe.generation_config.num_beams == 4)\n+\n+        # We can confirm that the task-specific parameters have an effect. (In this case, the default is `num_beams=1`,\n+        # which would crash when `num_return_sequences=4` is passed.)\n+        pipe(\"Hugging Face doesn't sell hugs.\", num_return_sequences=4)\n+        with self.assertRaises(ValueError):\n+            pipe(\"Hugging Face doesn't sell hugs.\", num_return_sequences=4, num_beams=1)\n+\n \n @is_pipeline_test\n class PipelineScikitCompatTest(unittest.TestCase):"
        },
        {
            "sha": "2130ed4b7c887f71f7662b55e1aaa2c47a4c48f5",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7542fac2c7e5e5761fb0394b045a8e4d9168da1c/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=7542fac2c7e5e5761fb0394b045a8e4d9168da1c",
            "patch": "@@ -1715,6 +1715,38 @@ def test_isin_mps_friendly(self):\n             torch.equal(torch.isin(random_ids, random_test_tensor), isin_mps_friendly(random_ids, random_test_tensor))\n         )\n \n+    def test_save_and_load_config_with_custom_generation(self):\n+        \"\"\"\n+        Regression test for the ability to save and load a config with a custom generation kwarg (i.e. a parameter\n+        that gets moved to the generation config and reset on the model config)\n+        \"\"\"\n+        model = T5ForConditionalGeneration.from_pretrained(TINY_T5)\n+\n+        # The default for `num_beams` is 1 and `early_stopping` is False\n+        self.assertTrue(model.config.num_beams == 1)\n+        self.assertTrue(model.config.early_stopping is False)\n+\n+        # When we save the model, this custom parameter should be moved to the generation config AND the model\n+        # config should contain `None`\n+        model.config.num_beams = 2\n+        model.config.early_stopping = True\n+        self.assertTrue(model.generation_config.num_beams == 1)  # unmodified generation config\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            model.save_pretrained(tmp_dir)\n+            new_model = T5ForConditionalGeneration.from_pretrained(tmp_dir)\n+            # moved to generation config\n+            self.assertTrue(new_model.generation_config.num_beams == 2)\n+            self.assertTrue(new_model.generation_config.early_stopping is True)\n+            # reset in the model config\n+            self.assertTrue(new_model.config.num_beams is None)\n+            self.assertTrue(new_model.config.early_stopping is None)\n+\n+            # Sanity check: We can run `generate` with the new model without any warnings\n+            random_ids = torch.randint(0, 100, (1, 5))\n+            with warnings.catch_warnings(record=True) as w:\n+                new_model.generate(random_ids, max_new_tokens=3)\n+            self.assertTrue(len(w) == 0)\n+\n \n @slow\n @require_torch"
        }
    ],
    "stats": {
        "total": 162,
        "additions": 132,
        "deletions": 30
    }
}