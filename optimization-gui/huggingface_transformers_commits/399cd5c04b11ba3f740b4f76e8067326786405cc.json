{
    "author": "Cyrilvallez",
    "message": "Fix modular for modernbert-decoder (#40431)\n\n* fix the modular\n\n* CI",
    "sha": "399cd5c04b11ba3f740b4f76e8067326786405cc",
    "files": [
        {
            "sha": "0869467f0bb4a2395707f929bfe12321be41acc4",
            "filename": "src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 153,
            "deletions": 37,
            "changes": 190,
            "blob_url": "https://github.com/huggingface/transformers/blob/399cd5c04b11ba3f740b4f76e8067326786405cc/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/399cd5c04b11ba3f740b4f76e8067326786405cc/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py?ref=399cd5c04b11ba3f740b4f76e8067326786405cc",
            "patch": "@@ -27,20 +27,14 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n-from ...models.modernbert.modeling_modernbert import (\n-    ModernBertEmbeddings,\n-    ModernBertMLP,\n-    ModernBertPredictionHead,\n-    ModernBertPreTrainedModel,\n-    ModernBertRotaryEmbedding,\n-    apply_rotary_pos_emb,\n-)\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.deprecation import deprecate_kwarg\n@@ -51,6 +45,126 @@\n logger = logging.get_logger(__name__)\n \n \n+class ModernBertDecoderEmbeddings(nn.Module):\n+    \"\"\"\n+    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n+    \"\"\"\n+\n+    def __init__(self, config: ModernBertDecoderConfig):\n+        super().__init__()\n+        self.config = config\n+        self.tok_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n+        self.norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n+        self.drop = nn.Dropout(config.embedding_dropout)\n+\n+    @torch.compile(dynamic=True)\n+    def compiled_embeddings(self, input_ids: torch.LongTensor) -> torch.Tensor:\n+        return self.drop(self.norm(self.tok_embeddings(input_ids)))\n+\n+    def forward(\n+        self, input_ids: Optional[torch.LongTensor] = None, inputs_embeds: Optional[torch.Tensor] = None\n+    ) -> torch.Tensor:\n+        if inputs_embeds is not None:\n+            hidden_states = self.drop(self.norm(inputs_embeds))\n+        else:\n+            hidden_states = (\n+                self.compiled_embeddings(input_ids)\n+                if self.config.reference_compile\n+                else self.drop(self.norm(self.tok_embeddings(input_ids)))\n+            )\n+        return hidden_states\n+\n+\n+class ModernBertDecoderMLP(nn.Module):\n+    \"\"\"Applies the GLU at the end of each ModernBertDecoder layer.\n+\n+    Compared to the default BERT architecture, this block replaces :class:`~transformers.model.bert.modeling_bert.BertIntermediate`\n+    and :class:`~transformers.model.bert.modeling_bert.SelfOutput` with a single module that has similar functionality.\n+    \"\"\"\n+\n+    def __init__(self, config: ModernBertDecoderConfig):\n+        super().__init__()\n+        self.config = config\n+        self.Wi = nn.Linear(config.hidden_size, int(config.intermediate_size) * 2, bias=config.mlp_bias)\n+        self.act = ACT2FN[config.hidden_activation]\n+        self.drop = nn.Dropout(config.mlp_dropout)\n+        self.Wo = nn.Linear(config.intermediate_size, config.hidden_size, bias=config.mlp_bias)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        input, gate = self.Wi(hidden_states).chunk(2, dim=-1)\n+        return self.Wo(self.drop(self.act(input) * gate))\n+\n+\n+class ModernBertDecoderRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: ModernBertDecoderConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n def eager_attention_forward(\n     module: \"ModernBertDecoderAttention\",\n     query: torch.Tensor,\n@@ -173,7 +287,7 @@ def __init__(self, config: ModernBertDecoderConfig, layer_idx: Optional[int] = N\n         )\n         self.attn = ModernBertDecoderAttention(config=config, layer_idx=layer_idx)\n         self.mlp_norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n-        self.mlp = ModernBertMLP(config)\n+        self.mlp = ModernBertDecoderMLP(config)\n \n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n@@ -218,12 +332,28 @@ def forward(\n         return hidden_states\n \n \n+class ModernBertDecoderPredictionHead(nn.Module):\n+    def __init__(self, config: ModernBertDecoderConfig):\n+        super().__init__()\n+        self.config = config\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size, config.classifier_bias)\n+        self.act = ACT2FN[config.classifier_activation]\n+        self.norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        return self.norm(self.act(self.dense(hidden_states)))\n+\n+\n @auto_docstring\n-class ModernBertDecoderPreTrainedModel(ModernBertPreTrainedModel):\n+class ModernBertDecoderPreTrainedModel(PreTrainedModel):\n     config: ModernBertDecoderConfig\n-    _skip_keys_device_placement = [\"past_key_values\"]\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n     _no_split_modules = [\"ModernBertDecoderLayer\"]\n-    _can_compile_fullgraph = False\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": ModernBertDecoderLayer,\n@@ -255,56 +385,42 @@ def init_weight(module: nn.Module, std: float):\n             \"final_out\": self.config.hidden_size**-0.5,\n         }\n \n-        if isinstance(module, ModernBertEmbeddings):\n+        if isinstance(module, ModernBertDecoderEmbeddings):\n             init_weight(module.tok_embeddings, stds[\"embedding\"])\n-        elif isinstance(module, ModernBertMLP):\n+        elif isinstance(module, ModernBertDecoderMLP):\n             init_weight(module.Wi, stds[\"in\"])\n             init_weight(module.Wo, stds[\"out\"])\n         elif isinstance(module, ModernBertDecoderAttention):\n             init_weight(module.q_proj, stds[\"in\"])\n             init_weight(module.k_proj, stds[\"in\"])\n             init_weight(module.v_proj, stds[\"in\"])\n             init_weight(module.Wo, stds[\"out\"])\n-        elif isinstance(module, ModernBertPredictionHead):\n+        elif isinstance(module, ModernBertDecoderPredictionHead):\n             init_weight(module.dense, stds[\"out\"])\n-        elif isinstance(module, ModernBertDecoderForSequenceClassification):\n+        elif module.__class__.__name__ == \"ModernBertDecoderForSequenceClassification\":\n             init_weight(module.classifier, stds[\"final_out\"])\n-        elif isinstance(module, ModernBertDecoderForCausalLM):\n+        elif module.__class__.__name__ == \"ModernBertDecoderForCausalLM\":\n             init_weight(module.decoder, stds[\"out\"])\n         elif isinstance(module, nn.LayerNorm):\n             module.weight.data.fill_(1.0)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n \n-    def _check_and_adjust_attn_implementation(\n-        self, attn_implementation: Optional[str], is_init_check: bool = False\n-    ) -> str:\n-        \"\"\"We overwrite this to make sdpa the first selection again if nothing was requested.\"\"\"\n-\n-        try:\n-            attn_implementation = (\n-                \"sdpa\" if attn_implementation is None and self._sdpa_can_dispatch() else attn_implementation\n-            )\n-        except (ValueError, ImportError):\n-            pass\n-\n-        return super()._check_and_adjust_attn_implementation(attn_implementation, is_init_check)\n-\n \n @auto_docstring\n class ModernBertDecoderModel(ModernBertDecoderPreTrainedModel):\n     def __init__(self, config: ModernBertDecoderConfig):\n         super().__init__(config)\n         self.config = config\n-        self.embeddings = ModernBertEmbeddings(config)\n+        self.embeddings = ModernBertDecoderEmbeddings(config)\n         self.layers = nn.ModuleList(\n             [ModernBertDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.final_norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n         self.gradient_checkpointing = False\n \n-        self.global_rotary_emb = ModernBertRotaryEmbedding(config=config)\n-        self.local_rotary_emb = ModernBertRotaryEmbedding(config=config)\n+        self.global_rotary_emb = ModernBertDecoderRotaryEmbedding(config=config)\n+        self.local_rotary_emb = ModernBertDecoderRotaryEmbedding(config=config)\n \n         self.post_init()\n \n@@ -407,7 +523,7 @@ def __init__(self, config: ModernBertDecoderConfig):\n         super().__init__(config)\n         self.config = config\n         self.model = ModernBertDecoderModel(config)\n-        self.lm_head = ModernBertPredictionHead(config)\n+        self.lm_head = ModernBertDecoderPredictionHead(config)\n         self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=config.decoder_bias)\n \n         # Initialize weights and apply final processing\n@@ -515,7 +631,7 @@ def __init__(self, config: ModernBertDecoderConfig):\n         self.num_labels = config.num_labels\n         self.model = ModernBertDecoderModel(config)\n \n-        self.head = ModernBertPredictionHead(config)\n+        self.head = ModernBertDecoderPredictionHead(config)\n         self.classifier = nn.Linear(config.hidden_size, config.num_labels, bias=config.classifier_bias)\n         self.drop = torch.nn.Dropout(config.classifier_dropout)\n "
        },
        {
            "sha": "0f0c752a4d63965a700bdb9110ecc0cff42fa6ca",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 39,
            "deletions": 29,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/399cd5c04b11ba3f740b4f76e8067326786405cc/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/399cd5c04b11ba3f740b4f76e8067326786405cc/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=399cd5c04b11ba3f740b4f76e8067326786405cc",
            "patch": "@@ -28,18 +28,18 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n-from ...models.modernbert.modeling_modernbert import (\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n+from ..modernbert.modeling_modernbert import (\n     ModernBertEmbeddings,\n     ModernBertMLP,\n     ModernBertPredictionHead,\n     ModernBertPreTrainedModel,\n     ModernBertRotaryEmbedding,\n     apply_rotary_pos_emb,\n )\n-from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n-from ...utils.generic import check_model_inputs\n \n \n logger = logging.get_logger(__name__)\n@@ -228,6 +228,18 @@ def __init__(\n         self.sliding_window = local_attention // 2 if local_attention else -1\n \n \n+class ModernBertDecoderEmbeddings(ModernBertEmbeddings):\n+    pass\n+\n+\n+class ModernBertDecoderMLP(ModernBertMLP):\n+    pass\n+\n+\n+class ModernBertDecoderRotaryEmbedding(ModernBertRotaryEmbedding):\n+    pass\n+\n+\n def eager_attention_forward(\n     module: \"ModernBertDecoderAttention\",\n     query: torch.Tensor,\n@@ -350,7 +362,7 @@ def __init__(self, config: ModernBertDecoderConfig, layer_idx: Optional[int] = N\n         )\n         self.attn = ModernBertDecoderAttention(config=config, layer_idx=layer_idx)\n         self.mlp_norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n-        self.mlp = ModernBertMLP(config)\n+        self.mlp = ModernBertDecoderMLP(config)\n \n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n@@ -395,12 +407,15 @@ def forward(\n         return hidden_states\n \n \n+class ModernBertDecoderPredictionHead(ModernBertPredictionHead):\n+    pass\n+\n+\n @auto_docstring\n class ModernBertDecoderPreTrainedModel(ModernBertPreTrainedModel):\n-    config: ModernBertDecoderConfig\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _no_split_modules = [\"ModernBertDecoderLayer\"]\n-    _can_compile_fullgraph = False\n+    _supports_flex_attn = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": ModernBertDecoderLayer,\n@@ -432,56 +447,51 @@ def init_weight(module: nn.Module, std: float):\n             \"final_out\": self.config.hidden_size**-0.5,\n         }\n \n-        if isinstance(module, ModernBertEmbeddings):\n+        if isinstance(module, ModernBertDecoderEmbeddings):\n             init_weight(module.tok_embeddings, stds[\"embedding\"])\n-        elif isinstance(module, ModernBertMLP):\n+        elif isinstance(module, ModernBertDecoderMLP):\n             init_weight(module.Wi, stds[\"in\"])\n             init_weight(module.Wo, stds[\"out\"])\n         elif isinstance(module, ModernBertDecoderAttention):\n             init_weight(module.q_proj, stds[\"in\"])\n             init_weight(module.k_proj, stds[\"in\"])\n             init_weight(module.v_proj, stds[\"in\"])\n             init_weight(module.Wo, stds[\"out\"])\n-        elif isinstance(module, ModernBertPredictionHead):\n+        elif isinstance(module, ModernBertDecoderPredictionHead):\n             init_weight(module.dense, stds[\"out\"])\n-        elif isinstance(module, ModernBertDecoderForSequenceClassification):\n+        elif module.__class__.__name__ == \"ModernBertDecoderForSequenceClassification\":\n             init_weight(module.classifier, stds[\"final_out\"])\n-        elif isinstance(module, ModernBertDecoderForCausalLM):\n+        elif module.__class__.__name__ == \"ModernBertDecoderForCausalLM\":\n             init_weight(module.decoder, stds[\"out\"])\n         elif isinstance(module, nn.LayerNorm):\n             module.weight.data.fill_(1.0)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n \n-    def _check_and_adjust_attn_implementation(\n-        self, attn_implementation: Optional[str], is_init_check: bool = False\n-    ) -> str:\n-        \"\"\"We overwrite this to make sdpa the first selection again if nothing was requested.\"\"\"\n+    def _check_and_adjust_attn_implementation(self, attn_implementation, is_init_check):\n+        raise AttributeError(\"No need to inherit!\")\n \n-        try:\n-            attn_implementation = (\n-                \"sdpa\" if attn_implementation is None and self._sdpa_can_dispatch() else attn_implementation\n-            )\n-        except (ValueError, ImportError):\n-            pass\n+    def _maybe_set_compile(self):\n+        raise AttributeError(\"No need to inherit!\")\n \n-        return super()._check_and_adjust_attn_implementation(attn_implementation, is_init_check)\n+    def resize_token_embeddings(self, *args, **kwargs):\n+        raise AttributeError(\"No need to inherit!\")\n \n \n @auto_docstring\n class ModernBertDecoderModel(ModernBertDecoderPreTrainedModel):\n     def __init__(self, config: ModernBertDecoderConfig):\n         super().__init__(config)\n         self.config = config\n-        self.embeddings = ModernBertEmbeddings(config)\n+        self.embeddings = ModernBertDecoderEmbeddings(config)\n         self.layers = nn.ModuleList(\n             [ModernBertDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.final_norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n         self.gradient_checkpointing = False\n \n-        self.global_rotary_emb = ModernBertRotaryEmbedding(config=config)\n-        self.local_rotary_emb = ModernBertRotaryEmbedding(config=config)\n+        self.global_rotary_emb = ModernBertDecoderRotaryEmbedding(config=config)\n+        self.local_rotary_emb = ModernBertDecoderRotaryEmbedding(config=config)\n \n         self.post_init()\n \n@@ -584,7 +594,7 @@ def __init__(self, config: ModernBertDecoderConfig):\n         super().__init__(config)\n         self.config = config\n         self.model = ModernBertDecoderModel(config)\n-        self.lm_head = ModernBertPredictionHead(config)\n+        self.lm_head = ModernBertDecoderPredictionHead(config)\n         self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=config.decoder_bias)\n \n         # Initialize weights and apply final processing\n@@ -692,7 +702,7 @@ def __init__(self, config: ModernBertDecoderConfig):\n         self.num_labels = config.num_labels\n         self.model = ModernBertDecoderModel(config)\n \n-        self.head = ModernBertPredictionHead(config)\n+        self.head = ModernBertDecoderPredictionHead(config)\n         self.classifier = nn.Linear(config.hidden_size, config.num_labels, bias=config.classifier_bias)\n         self.drop = torch.nn.Dropout(config.classifier_dropout)\n "
        }
    ],
    "stats": {
        "total": 258,
        "additions": 192,
        "deletions": 66
    }
}