{
    "author": "rootonchair",
    "message": "Add Fast Image Processor for Flava (#37135)\n\n* support flava fast image processor\n\n* run style and quality\n\n* update test\n\n* update according to reviews\n\n* make style\n\n* update comment on BICUBIC\n\n* make style\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "49b9a69a3694aa4cbdad8fbd4d9f0b76650c7097",
    "files": [
        {
            "sha": "c809be73589a9c06552b8633e270864f05a5feb7",
            "filename": "docs/source/en/model_doc/flava.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/49b9a69a3694aa4cbdad8fbd4d9f0b76650c7097/docs%2Fsource%2Fen%2Fmodel_doc%2Fflava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/49b9a69a3694aa4cbdad8fbd4d9f0b76650c7097/docs%2Fsource%2Fen%2Fmodel_doc%2Fflava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflava.md?ref=49b9a69a3694aa4cbdad8fbd4d9f0b76650c7097",
            "patch": "@@ -72,6 +72,11 @@ This model was contributed by [aps](https://huggingface.co/aps). The original co\n [[autodoc]] FlavaImageProcessor\n     - preprocess\n \n+## FlavaImageProcessorFast\n+\n+[[autodoc]] FlavaImageProcessorFast\n+    - preprocess\n+\n ## FlavaForPreTraining\n \n [[autodoc]] FlavaForPreTraining"
        },
        {
            "sha": "5e8232d94118444a8397c1dff181e7b89841191f",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/49b9a69a3694aa4cbdad8fbd4d9f0b76650c7097/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49b9a69a3694aa4cbdad8fbd4d9f0b76650c7097/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=49b9a69a3694aa4cbdad8fbd4d9f0b76650c7097",
            "patch": "@@ -84,7 +84,7 @@\n             (\"dpt\", (\"DPTImageProcessor\",)),\n             (\"efficientformer\", (\"EfficientFormerImageProcessor\",)),\n             (\"efficientnet\", (\"EfficientNetImageProcessor\",)),\n-            (\"flava\", (\"FlavaImageProcessor\",)),\n+            (\"flava\", (\"FlavaImageProcessor\", \"FlavaImageProcessorFast\")),\n             (\"focalnet\", (\"BitImageProcessor\",)),\n             (\"fuyu\", (\"FuyuImageProcessor\",)),\n             (\"gemma3\", (\"Gemma3ImageProcessor\", \"Gemma3ImageProcessorFast\")),"
        },
        {
            "sha": "292593cb4a201e35a9fd571baec639d9b940e76c",
            "filename": "src/transformers/models/flava/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/49b9a69a3694aa4cbdad8fbd4d9f0b76650c7097/src%2Ftransformers%2Fmodels%2Fflava%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49b9a69a3694aa4cbdad8fbd4d9f0b76650c7097/src%2Ftransformers%2Fmodels%2Fflava%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2F__init__.py?ref=49b9a69a3694aa4cbdad8fbd4d9f0b76650c7097",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_flava import *\n     from .feature_extraction_flava import *\n     from .image_processing_flava import *\n+    from .image_processing_flava_fast import *\n     from .modeling_flava import *\n     from .processing_flava import *\n else:"
        },
        {
            "sha": "89beb9ab5f5f2949a32fe34d6ad2827bec0c36aa",
            "filename": "src/transformers/models/flava/image_processing_flava_fast.py",
            "status": "added",
            "additions": 549,
            "deletions": 0,
            "changes": 549,
            "blob_url": "https://github.com/huggingface/transformers/blob/49b9a69a3694aa4cbdad8fbd4d9f0b76650c7097/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49b9a69a3694aa4cbdad8fbd4d9f0b76650c7097/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py?ref=49b9a69a3694aa4cbdad8fbd4d9f0b76650c7097",
            "patch": "@@ -0,0 +1,549 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Flava.\"\"\"\n+\n+import math\n+import random\n+from functools import lru_cache\n+from typing import Any, Dict, Iterable, Optional, Tuple, Union\n+\n+from ...image_processing_utils_fast import (\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+    BaseImageProcessorFast,\n+    BatchFeature,\n+    DefaultFastImageProcessorKwargs,\n+    get_size_dict,\n+)\n+from ...image_transforms import ChannelDimension, group_images_by_shape, reorder_images\n+from ...image_utils import ImageInput, PILImageResampling, SizeDict\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    add_start_docstrings,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+from .image_processing_flava import (\n+    FLAVA_CODEBOOK_MEAN,\n+    FLAVA_CODEBOOK_STD,\n+    FLAVA_IMAGE_MEAN,\n+    FLAVA_IMAGE_STD,\n+    LOGIT_LAPLACE_EPS,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    from ...image_utils import pil_torch_interpolation_mapping\n+\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+class FlavaMaskingGenerator:\n+    def __init__(\n+        self,\n+        input_size: Union[int, Tuple[int, int]] = 14,\n+        total_mask_patches: int = 75,\n+        mask_group_max_patches: Optional[int] = None,\n+        mask_group_min_patches: int = 16,\n+        mask_group_min_aspect_ratio: Optional[float] = 0.3,\n+        mask_group_max_aspect_ratio: float = None,\n+    ):\n+        if not isinstance(input_size, tuple):\n+            input_size = (input_size,) * 2\n+        self.height, self.width = input_size\n+\n+        self.num_patches = self.height * self.width\n+        self.total_mask_patches = total_mask_patches\n+\n+        self.mask_group_min_patches = mask_group_min_patches\n+        self.mask_group_max_patches = total_mask_patches if mask_group_max_patches is None else mask_group_max_patches\n+\n+        mask_group_max_aspect_ratio = mask_group_max_aspect_ratio or 1 / mask_group_min_aspect_ratio\n+        self.log_aspect_ratio = (math.log(mask_group_min_aspect_ratio), math.log(mask_group_max_aspect_ratio))\n+\n+    def __repr__(self):\n+        repr_str = \"MaskingGenerator(%d, %d -> [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n+            self.height,\n+            self.width,\n+            self.mask_group_min_patches,\n+            self.mask_group_max_patches,\n+            self.total_mask_patches,\n+            self.log_aspect_ratio[0],\n+            self.log_aspect_ratio[1],\n+        )\n+        return repr_str\n+\n+    def get_shape(self):\n+        return self.height, self.width\n+\n+    def _mask(self, mask, max_mask_patches):\n+        delta = 0\n+        for _attempt in range(10):\n+            target_area = random.uniform(self.mask_group_min_patches, max_mask_patches)\n+            aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n+            height = int(round(math.sqrt(target_area * aspect_ratio)))\n+            width = int(round(math.sqrt(target_area / aspect_ratio)))\n+            if width < self.width and height < self.height:\n+                top = random.randint(0, self.height - height)\n+                left = random.randint(0, self.width - width)\n+\n+                num_masked = mask[top : top + height, left : left + width].sum()\n+                # Overlap\n+                if 0 < height * width - num_masked <= max_mask_patches:\n+                    zeros_pos = mask[top : top + height, left : left + width] == 0\n+                    mask[top : top + height, left : left + width][zeros_pos] = 1\n+                    delta += zeros_pos.sum()\n+\n+                if delta > 0:\n+                    break\n+        return delta\n+\n+    def __call__(self):\n+        mask = torch.zeros(self.get_shape(), dtype=torch.int)\n+        mask_count = 0\n+        while mask_count < self.total_mask_patches:\n+            max_mask_patches = self.total_mask_patches - mask_count\n+            max_mask_patches = min(max_mask_patches, self.mask_group_max_patches)\n+\n+            delta = self._mask(mask, max_mask_patches)\n+            if delta == 0:\n+                break\n+            else:\n+                mask_count += delta\n+\n+        return mask\n+\n+\n+class FlavaFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    # Mask related params\n+    return_image_mask: Optional[bool]\n+    input_size_patches: Optional[int]\n+    total_mask_patches: Optional[int]\n+    mask_group_min_patches: Optional[int]\n+    mask_group_max_patches: Optional[int]\n+    mask_group_min_aspect_ratio: Optional[float]\n+    mask_group_max_aspect_ratio: Optional[float]\n+    # Codebook related params\n+    return_codebook_pixels: Optional[bool]\n+    codebook_do_resize: Optional[bool]\n+    codebook_size: Optional[bool]\n+    codebook_resample: Optional[int]\n+    codebook_do_center_crop: Optional[bool]\n+    codebook_crop_size: Optional[int]\n+    codebook_do_rescale: Optional[bool]\n+    codebook_rescale_factor: Optional[Union[int, float]]\n+    codebook_do_map_pixels: Optional[bool]\n+    codebook_do_normalize: Optional[bool]\n+    codebook_image_mean: Optional[Union[float, Iterable[float]]]\n+    codebook_image_std: Optional[Union[float, Iterable[float]]]\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast Flava image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    \"\"\"\n+        return_image_mask (`bool`, *optional*, defaults to `False`):\n+            Whether to return the image mask. Can be overridden by the `return_image_mask` parameter in `preprocess`.\n+        input_size_patches (`int`, *optional*, defaults to 14):\n+            Number of patches in the image in height and width direction. 14x14 = 196 total patches. Can be overridden\n+            by the `input_size_patches` parameter in `preprocess`.\n+        total_mask_patches (`int`, *optional*, defaults to 75):\n+            Total number of patches that should be masked. Can be overridden by the `total_mask_patches` parameter in\n+            `preprocess`.\n+        mask_group_min_patches (`int`, *optional*, defaults to 16):\n+            Minimum number of patches that should be masked. Can be overridden by the `mask_group_min_patches`\n+            parameter in `preprocess`.\n+        mask_group_max_patches (`int`, *optional*):\n+            Maximum number of patches that should be masked. Can be overridden by the `mask_group_max_patches`\n+            parameter in `preprocess`.\n+        mask_group_min_aspect_ratio (`float`, *optional*, defaults to 0.3):\n+            Minimum aspect ratio of the mask window. Can be overridden by the `mask_group_min_aspect_ratio` parameter\n+            in `preprocess`.\n+        mask_group_max_aspect_ratio (`float`, *optional*):\n+            Maximum aspect ratio of the mask window. Can be overridden by the `mask_group_max_aspect_ratio` parameter\n+            in `preprocess`.\n+        codebook_do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the input for codebook to a certain. Can be overridden by the `codebook_do_resize`\n+            parameter in `preprocess`. `codebook_size`.\n+        codebook_size (`Dict[str, int]`, *optional*, defaults to `{\"height\": 224, \"width\": 224}`):\n+            Resize the input for codebook to the given size. Can be overridden by the `codebook_size` parameter in\n+            `preprocess`.\n+        codebook_resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.LANCZOS`):\n+            Resampling filter to use if resizing the codebook image. Can be overridden by the `codebook_resample`\n+            parameter in `preprocess`.\n+        codebook_do_center_crop (`bool`, *optional*, defaults to `True`):\n+            Whether to crop the input for codebook at the center. If the input size is smaller than\n+            `codebook_crop_size` along any edge, the image is padded with 0's and then center cropped. Can be\n+            overridden by the `codebook_do_center_crop` parameter in `preprocess`.\n+        codebook_crop_size (`Dict[str, int]`, *optional*, defaults to `{\"height\": 224, \"width\": 224}`):\n+            Desired output size for codebook input when applying center-cropping. Can be overridden by the\n+            `codebook_crop_size` parameter in `preprocess`.\n+        codebook_do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the input for codebook by the specified scale `codebook_rescale_factor`. Can be\n+            overridden by the `codebook_do_rescale` parameter in `preprocess`.\n+        codebook_rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Defines the scale factor to use if rescaling the codebook image. Can be overridden by the\n+            `codebook_rescale_factor` parameter in `preprocess`.\n+        codebook_do_map_pixels (`bool`, *optional*, defaults to `True`):\n+            Whether to map the pixel values of the codebook input to (1 - 2e)x + e. Can be overridden by the\n+            `codebook_do_map_pixels` parameter in `preprocess`.\n+        codebook_do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether or not to normalize the input for codebook with `codebook_image_mean` and `codebook_image_std`. Can\n+            be overridden by the `codebook_do_normalize` parameter in `preprocess`.\n+        codebook_image_mean (`Optional[Union[float, Iterable[float]]]`, *optional*, defaults to `[0, 0, 0]`):\n+            The sequence of means for each channel, to be used when normalizing images for codebook. Can be overridden\n+            by the `codebook_image_mean` parameter in `preprocess`.\n+        codebook_image_std (`Optional[Union[float, Iterable[float]]]`, *optional*, defaults to `[0.5, 0.5, 0.5]`):\n+            The sequence of standard deviations for each channel, to be used when normalizing images for codebook. Can\n+            be overridden by the `codebook_image_std` parameter in `preprocess`.\n+    \"\"\",\n+)\n+class FlavaImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = FLAVA_IMAGE_MEAN\n+    image_std = FLAVA_IMAGE_STD\n+    size = {\"height\": 224, \"width\": 224}\n+    crop_size = {\"height\": 224, \"width\": 224}\n+    do_resize = True\n+    do_center_crop = True\n+    do_rescale = True\n+    do_normalize = True\n+\n+    # Mask related params\n+    return_image_mask = False\n+    input_size_patches = 14\n+    total_mask_patches = 75\n+    mask_group_min_patches = 16\n+    mask_group_max_patches = None\n+    mask_group_min_aspect_ratio = 0.3\n+    mask_group_max_aspect_ratio = None\n+    # Codebook related params\n+    return_codebook_pixels = False\n+    codebook_do_resize = True\n+    codebook_size = {\"height\": 112, \"width\": 112}\n+    # LANCZOS resample does not support torch Tensor. Use BICUBIC as closest alternative\n+    codebook_resample = PILImageResampling.BICUBIC\n+    codebook_do_center_crop = True\n+    codebook_crop_size = {\"height\": 112, \"width\": 112}\n+    codebook_do_rescale = True\n+    codebook_rescale_factor = 1 / 255\n+    codebook_do_map_pixels = True\n+    codebook_do_normalize = True\n+    codebook_image_mean = FLAVA_CODEBOOK_MEAN\n+    codebook_image_std = FLAVA_CODEBOOK_STD\n+    valid_kwargs = FlavaFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[FlavaFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @add_start_docstrings(\n+        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+        \"\"\"\n+            return_image_mask (`bool`, *optional*, defaults to `False`):\n+                Whether to return the image mask. Can be overridden by the `return_image_mask` parameter in `preprocess`.\n+            input_size_patches (`int`, *optional*, defaults to 14):\n+                Number of patches in the image in height and width direction. 14x14 = 196 total patches. Can be overridden\n+                by the `input_size_patches` parameter in `preprocess`.\n+            total_mask_patches (`int`, *optional*, defaults to 75):\n+                Total number of patches that should be masked. Can be overridden by the `total_mask_patches` parameter in\n+                `preprocess`.\n+            mask_group_min_patches (`int`, *optional*, defaults to 16):\n+                Minimum number of patches that should be masked. Can be overridden by the `mask_group_min_patches`\n+                parameter in `preprocess`.\n+            mask_group_max_patches (`int`, *optional*):\n+                Maximum number of patches that should be masked. Can be overridden by the `mask_group_max_patches`\n+                parameter in `preprocess`.\n+            mask_group_min_aspect_ratio (`float`, *optional*, defaults to 0.3):\n+                Minimum aspect ratio of the mask window. Can be overridden by the `mask_group_min_aspect_ratio` parameter\n+                in `preprocess`.\n+            mask_group_max_aspect_ratio (`float`, *optional*):\n+                Maximum aspect ratio of the mask window. Can be overridden by the `mask_group_max_aspect_ratio` parameter\n+                in `preprocess`.\n+            codebook_do_resize (`bool`, *optional*, defaults to `True`):\n+                Whether to resize the input for codebook to a certain. Can be overridden by the `codebook_do_resize`\n+                parameter in `preprocess`. `codebook_size`.\n+            codebook_size (`Dict[str, int]`, *optional*, defaults to `{\"height\": 224, \"width\": 224}`):\n+                Resize the input for codebook to the given size. Can be overridden by the `codebook_size` parameter in\n+                `preprocess`.\n+            codebook_resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.LANCZOS`):\n+                Resampling filter to use if resizing the codebook image. Can be overridden by the `codebook_resample`\n+                parameter in `preprocess`.\n+            codebook_do_center_crop (`bool`, *optional*, defaults to `True`):\n+                Whether to crop the input for codebook at the center. If the input size is smaller than\n+                `codebook_crop_size` along any edge, the image is padded with 0's and then center cropped. Can be\n+                overridden by the `codebook_do_center_crop` parameter in `preprocess`.\n+            codebook_crop_size (`Dict[str, int]`, *optional*, defaults to `{\"height\": 224, \"width\": 224}`):\n+                Desired output size for codebook input when applying center-cropping. Can be overridden by the\n+                `codebook_crop_size` parameter in `preprocess`.\n+            codebook_do_rescale (`bool`, *optional*, defaults to `True`):\n+                Whether to rescale the input for codebook by the specified scale `codebook_rescale_factor`. Can be\n+                overridden by the `codebook_do_rescale` parameter in `preprocess`.\n+            codebook_rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+                Defines the scale factor to use if rescaling the codebook image. Can be overridden by the\n+                `codebook_rescale_factor` parameter in `preprocess`.\n+            codebook_do_map_pixels (`bool`, *optional*, defaults to `True`):\n+                Whether to map the pixel values of the codebook input to (1 - 2e)x + e. Can be overridden by the\n+                `codebook_do_map_pixels` parameter in `preprocess`.\n+            codebook_do_normalize (`bool`, *optional*, defaults to `True`):\n+                Whether or not to normalize the input for codebook with `codebook_image_mean` and `codebook_image_std`. Can\n+                be overridden by the `codebook_do_normalize` parameter in `preprocess`.\n+            codebook_image_mean (`Optional[Union[float, Iterable[float]]]`, *optional*, defaults to `[0, 0, 0]`):\n+                The sequence of means for each channel, to be used when normalizing images for codebook. Can be overridden\n+                by the `codebook_image_mean` parameter in `preprocess`.\n+            codebook_image_std (`Optional[Union[float, Iterable[float]]]`, *optional*, defaults to `[0.5, 0.5, 0.5]`):\n+                The sequence of standard deviations for each channel, to be used when normalizing images for codebook. Can\n+                be overridden by the `codebook_image_std` parameter in `preprocess`.\n+        \"\"\",\n+    )\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[DefaultFastImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    @classmethod\n+    def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n+        \"\"\"\n+        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n+        created using from_dict and kwargs e.g. `FlavaImageProcessor.from_pretrained(checkpoint, codebook_size=600)`\n+        \"\"\"\n+        image_processor_dict = image_processor_dict.copy()\n+        if \"codebook_size\" in kwargs:\n+            image_processor_dict[\"codebook_size\"] = kwargs.pop(\"codebook_size\")\n+        if \"codebook_crop_size\" in kwargs:\n+            image_processor_dict[\"codebook_crop_size\"] = kwargs.pop(\"codebook_crop_size\")\n+        return super().from_dict(image_processor_dict, **kwargs)\n+\n+    @lru_cache()\n+    def masking_generator(\n+        self,\n+        input_size_patches,\n+        total_mask_patches,\n+        mask_group_min_patches,\n+        mask_group_max_patches,\n+        mask_group_min_aspect_ratio,\n+        mask_group_max_aspect_ratio,\n+    ) -> FlavaMaskingGenerator:\n+        return FlavaMaskingGenerator(\n+            input_size=input_size_patches,\n+            total_mask_patches=total_mask_patches,\n+            mask_group_min_patches=mask_group_min_patches,\n+            mask_group_max_patches=mask_group_max_patches,\n+            mask_group_min_aspect_ratio=mask_group_min_aspect_ratio,\n+            mask_group_max_aspect_ratio=mask_group_max_aspect_ratio,\n+        )\n+\n+    def map_pixels(self, image: \"torch.Tensor\") -> \"torch.Tensor\":\n+        return (1 - 2 * LOGIT_LAPLACE_EPS) * image + LOGIT_LAPLACE_EPS\n+\n+    def _further_process_kwargs(\n+        self,\n+        size: Optional[SizeDict] = None,\n+        crop_size: Optional[SizeDict] = None,\n+        default_to_square: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        codebook_size: Optional[SizeDict] = None,\n+        codebook_crop_size: Optional[SizeDict] = None,\n+        codebook_image_mean: Optional[Union[float, list[float]]] = None,\n+        codebook_image_std: Optional[Union[float, list[float]]] = None,\n+        codebook_resample: Optional[PILImageResampling] = None,\n+        data_format: Optional[ChannelDimension] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Update kwargs that need further processing before being validated\n+        Can be overridden by subclasses to customize the processing of kwargs.\n+        \"\"\"\n+        if kwargs is None:\n+            kwargs = {}\n+        if size is not None:\n+            size = SizeDict(**get_size_dict(size=size, default_to_square=default_to_square))\n+        if crop_size is not None:\n+            crop_size = SizeDict(**get_size_dict(crop_size, param_name=\"crop_size\"))\n+        if isinstance(image_mean, list):\n+            image_mean = tuple(image_mean)\n+        if isinstance(image_std, list):\n+            image_std = tuple(image_std)\n+        if data_format is None:\n+            data_format = ChannelDimension.FIRST\n+        if codebook_size is not None:\n+            codebook_size = SizeDict(**get_size_dict(size=codebook_size, default_to_square=default_to_square))\n+        if codebook_crop_size is not None:\n+            codebook_crop_size = SizeDict(**get_size_dict(codebook_crop_size, param_name=\"codebook_crop_size\"))\n+        if isinstance(codebook_image_mean, list):\n+            codebook_image_mean = tuple(codebook_image_mean)\n+        if isinstance(codebook_image_std, list):\n+            codebook_image_std = tuple(codebook_image_std)\n+\n+        kwargs[\"size\"] = size\n+        kwargs[\"crop_size\"] = crop_size\n+        kwargs[\"default_to_square\"] = default_to_square\n+        kwargs[\"image_mean\"] = image_mean\n+        kwargs[\"image_std\"] = image_std\n+        kwargs[\"codebook_size\"] = codebook_size\n+        kwargs[\"codebook_crop_size\"] = codebook_crop_size\n+        kwargs[\"codebook_image_mean\"] = codebook_image_mean\n+        kwargs[\"codebook_image_std\"] = codebook_image_std\n+        kwargs[\"data_format\"] = data_format\n+        kwargs[\"codebook_interpolation\"] = (\n+            pil_torch_interpolation_mapping[codebook_resample]\n+            if isinstance(codebook_resample, (PILImageResampling, int))\n+            else codebook_resample\n+        )\n+\n+        return kwargs\n+\n+    def _preprocess_image(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        do_map_pixels: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+    ) -> \"torch.Tensor\":\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_center_crop:\n+                stacked_images = self.center_crop(stacked_images, crop_size)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            if do_map_pixels:\n+                stacked_images = self.map_pixels(image=stacked_images)\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return processed_images\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        # Mask related params\n+        return_image_mask: Optional[bool],\n+        input_size_patches: Optional[int],\n+        total_mask_patches: Optional[int],\n+        mask_group_min_patches: Optional[int],\n+        mask_group_max_patches: Optional[int],\n+        mask_group_min_aspect_ratio: Optional[float],\n+        mask_group_max_aspect_ratio: Optional[float],\n+        # Codebook related params\n+        return_codebook_pixels: Optional[bool],\n+        codebook_do_resize: Optional[bool],\n+        codebook_size: Optional[SizeDict],\n+        codebook_interpolation: Optional[\"F.InterpolationMode\"],\n+        codebook_do_center_crop: Optional[bool],\n+        codebook_crop_size: Optional[SizeDict],\n+        codebook_do_rescale: Optional[bool],\n+        codebook_rescale_factor: Optional[float],\n+        codebook_do_map_pixels: Optional[bool],\n+        codebook_do_normalize: Optional[bool],\n+        codebook_image_mean: Optional[Union[float, list[float]]],\n+        codebook_image_std: Optional[Union[float, list[float]]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        processed_images = self._preprocess_image(\n+            images=images,\n+            do_resize=do_resize,\n+            size=size,\n+            interpolation=interpolation,\n+            do_center_crop=do_center_crop,\n+            crop_size=crop_size,\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            do_map_pixels=False,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            return_tensors=return_tensors,\n+        )\n+        data = {\n+            \"pixel_values\": processed_images,\n+        }\n+\n+        if return_codebook_pixels:\n+            codebook_processed_images = self._preprocess_image(\n+                images=images,\n+                do_resize=codebook_do_resize,\n+                size=codebook_size,\n+                interpolation=codebook_interpolation,\n+                do_center_crop=codebook_do_center_crop,\n+                crop_size=codebook_crop_size,\n+                do_rescale=codebook_do_rescale,\n+                rescale_factor=codebook_rescale_factor,\n+                do_normalize=codebook_do_normalize,\n+                do_map_pixels=codebook_do_map_pixels,\n+                image_mean=codebook_image_mean,\n+                image_std=codebook_image_std,\n+                return_tensors=return_tensors,\n+            )\n+            data[\"codebook_pixel_values\"] = codebook_processed_images\n+\n+        if return_image_mask:\n+            mask_generator = self.masking_generator(\n+                input_size_patches=input_size_patches,\n+                total_mask_patches=total_mask_patches,\n+                mask_group_min_patches=mask_group_min_patches,\n+                mask_group_max_patches=mask_group_max_patches,\n+                mask_group_min_aspect_ratio=mask_group_min_aspect_ratio,\n+                mask_group_max_aspect_ratio=mask_group_max_aspect_ratio,\n+            )\n+            masks = [mask_generator() for _ in range(len(images))]\n+            masks = torch.stack(masks, dim=0) if return_tensors else masks\n+            data[\"bool_masked_pos\"] = masks\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"FlavaImageProcessorFast\"]"
        },
        {
            "sha": "5edb1997abbec7be832a221a90a98bc5fa3a57dc",
            "filename": "tests/models/flava/test_image_processing_flava.py",
            "status": "modified",
            "additions": 225,
            "deletions": 178,
            "changes": 403,
            "blob_url": "https://github.com/huggingface/transformers/blob/49b9a69a3694aa4cbdad8fbd4d9f0b76650c7097/tests%2Fmodels%2Fflava%2Ftest_image_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49b9a69a3694aa4cbdad8fbd4d9f0b76650c7097/tests%2Fmodels%2Fflava%2Ftest_image_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_image_processing_flava.py?ref=49b9a69a3694aa4cbdad8fbd4d9f0b76650c7097",
            "patch": "@@ -16,9 +16,11 @@\n import unittest\n \n import numpy as np\n+import requests\n+from PIL import Image\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -30,6 +32,9 @@\n     import PIL\n \n     from transformers import FlavaImageProcessor\n+\n+    if is_torchvision_available():\n+        from transformers import FlavaImageProcessorFast\n     from transformers.image_utils import PILImageResampling\n     from transformers.models.flava.image_processing_flava import (\n         FLAVA_CODEBOOK_MEAN,\n@@ -105,7 +110,8 @@ def __init__(\n \n         self.codebook_do_resize = codebook_do_resize\n         self.codebook_size = codebook_size\n-        self.codebook_resample = codebook_resample if codebook_resample is not None else PILImageResampling.LANCZOS\n+        # LANCZOS resample does not support torch Tensor. Use BICUBIC as closest alternative\n+        self.codebook_resample = codebook_resample if codebook_resample is not None else PILImageResampling.BICUBIC\n         self.codebook_do_center_crop = codebook_do_center_crop\n         self.codebook_crop_size = codebook_crop_size\n         self.codebook_do_map_pixels = codebook_do_map_pixels\n@@ -171,6 +177,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class FlavaImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = FlavaImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = FlavaImageProcessorFast if is_torchvision_available() else None\n     maxDiff = None\n \n     def setUp(self):\n@@ -182,157 +189,161 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"resample\"))\n-        self.assertTrue(hasattr(image_processing, \"crop_size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n-        self.assertTrue(hasattr(image_processing, \"masking_generator\"))\n-        self.assertTrue(hasattr(image_processing, \"codebook_do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"codebook_size\"))\n-        self.assertTrue(hasattr(image_processing, \"codebook_resample\"))\n-        self.assertTrue(hasattr(image_processing, \"codebook_do_center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"codebook_crop_size\"))\n-        self.assertTrue(hasattr(image_processing, \"codebook_do_map_pixels\"))\n-        self.assertTrue(hasattr(image_processing, \"codebook_do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"codebook_image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"codebook_image_std\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"resample\"))\n+            self.assertTrue(hasattr(image_processing, \"crop_size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"masking_generator\"))\n+            self.assertTrue(hasattr(image_processing, \"codebook_do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"codebook_size\"))\n+            self.assertTrue(hasattr(image_processing, \"codebook_resample\"))\n+            self.assertTrue(hasattr(image_processing, \"codebook_do_center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"codebook_crop_size\"))\n+            self.assertTrue(hasattr(image_processing, \"codebook_do_map_pixels\"))\n+            self.assertTrue(hasattr(image_processing, \"codebook_do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"codebook_image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"codebook_image_std\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 224, \"width\": 224})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 224, \"width\": 224})\n-        self.assertEqual(image_processor.codebook_size, {\"height\": 112, \"width\": 112})\n-        self.assertEqual(image_processor.codebook_crop_size, {\"height\": 112, \"width\": 112})\n-\n-        image_processor = self.image_processing_class.from_dict(\n-            self.image_processor_dict, size=42, crop_size=84, codebook_size=33, codebook_crop_size=66\n-        )\n-        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n-        self.assertEqual(image_processor.codebook_size, {\"height\": 33, \"width\": 33})\n-        self.assertEqual(image_processor.codebook_crop_size, {\"height\": 66, \"width\": 66})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 224, \"width\": 224})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 224, \"width\": 224})\n+            self.assertEqual(image_processor.codebook_size, {\"height\": 112, \"width\": 112})\n+            self.assertEqual(image_processor.codebook_crop_size, {\"height\": 112, \"width\": 112})\n+\n+            image_processor = self.image_processing_class.from_dict(\n+                self.image_processor_dict, size=42, crop_size=84, codebook_size=33, codebook_crop_size=66\n+            )\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n+            self.assertEqual(image_processor.codebook_size, {\"height\": 33, \"width\": 33})\n+            self.assertEqual(image_processor.codebook_crop_size, {\"height\": 66, \"width\": 66})\n \n     def test_call_pil(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PIL images\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, PIL.Image.Image)\n-\n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\")\n-\n-        # Test no bool masked pos\n-        self.assertFalse(\"bool_masked_pos\" in encoded_images)\n-\n-        expected_height, expected_width = self.image_processor_tester.get_expected_image_size()\n-\n-        self.assertEqual(\n-            encoded_images.pixel_values.shape,\n-            (1, self.image_processor_tester.num_channels, expected_height, expected_width),\n-        )\n-\n-        # Test batched\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\")\n-        expected_height, expected_width = self.image_processor_tester.get_expected_image_size()\n-\n-        # Test no bool masked pos\n-        self.assertFalse(\"bool_masked_pos\" in encoded_images)\n-\n-        self.assertEqual(\n-            encoded_images.pixel_values.shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.num_channels,\n-                expected_height,\n-                expected_width,\n-            ),\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, PIL.Image.Image)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\")\n+\n+            # Test no bool masked pos\n+            self.assertFalse(\"bool_masked_pos\" in encoded_images)\n+\n+            expected_height, expected_width = self.image_processor_tester.get_expected_image_size()\n+\n+            self.assertEqual(\n+                encoded_images.pixel_values.shape,\n+                (1, self.image_processor_tester.num_channels, expected_height, expected_width),\n+            )\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\")\n+            expected_height, expected_width = self.image_processor_tester.get_expected_image_size()\n+\n+            # Test no bool masked pos\n+            self.assertFalse(\"bool_masked_pos\" in encoded_images)\n+\n+            self.assertEqual(\n+                encoded_images.pixel_values.shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_channels,\n+                    expected_height,\n+                    expected_width,\n+                ),\n+            )\n \n     def _test_call_framework(self, instance_class, prepare_kwargs):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, **prepare_kwargs)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, instance_class)\n-\n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\")\n-\n-        expected_height, expected_width = self.image_processor_tester.get_expected_image_size()\n-        self.assertEqual(\n-            encoded_images.pixel_values.shape,\n-            (1, self.image_processor_tester.num_channels, expected_height, expected_width),\n-        )\n-\n-        encoded_images = image_processing(image_inputs, return_image_mask=True, return_tensors=\"pt\")\n-\n-        expected_height, expected_width = self.image_processor_tester.get_expected_image_size()\n-        self.assertEqual(\n-            encoded_images.pixel_values.shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.num_channels,\n-                expected_height,\n-                expected_width,\n-            ),\n-        )\n-\n-        expected_height, expected_width = self.image_processor_tester.get_expected_mask_size()\n-        self.assertEqual(\n-            encoded_images.bool_masked_pos.shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                expected_height,\n-                expected_width,\n-            ),\n-        )\n-\n-        # Test batched\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n-\n-        expected_height, expected_width = self.image_processor_tester.get_expected_image_size()\n-        self.assertEqual(\n-            encoded_images.shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.num_channels,\n-                expected_height,\n-                expected_width,\n-            ),\n-        )\n-\n-        # Test masking\n-        encoded_images = image_processing(image_inputs, return_image_mask=True, return_tensors=\"pt\")\n-\n-        expected_height, expected_width = self.image_processor_tester.get_expected_image_size()\n-        self.assertEqual(\n-            encoded_images.pixel_values.shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.num_channels,\n-                expected_height,\n-                expected_width,\n-            ),\n-        )\n-\n-        expected_height, expected_width = self.image_processor_tester.get_expected_mask_size()\n-        self.assertEqual(\n-            encoded_images.bool_masked_pos.shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                expected_height,\n-                expected_width,\n-            ),\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, **prepare_kwargs)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, instance_class)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\")\n+\n+            expected_height, expected_width = self.image_processor_tester.get_expected_image_size()\n+            self.assertEqual(\n+                encoded_images.pixel_values.shape,\n+                (1, self.image_processor_tester.num_channels, expected_height, expected_width),\n+            )\n+\n+            encoded_images = image_processing(image_inputs, return_image_mask=True, return_tensors=\"pt\")\n+\n+            expected_height, expected_width = self.image_processor_tester.get_expected_image_size()\n+            self.assertEqual(\n+                encoded_images.pixel_values.shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_channels,\n+                    expected_height,\n+                    expected_width,\n+                ),\n+            )\n+\n+            expected_height, expected_width = self.image_processor_tester.get_expected_mask_size()\n+            self.assertEqual(\n+                encoded_images.bool_masked_pos.shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    expected_height,\n+                    expected_width,\n+                ),\n+            )\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+\n+            expected_height, expected_width = self.image_processor_tester.get_expected_image_size()\n+            self.assertEqual(\n+                encoded_images.shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_channels,\n+                    expected_height,\n+                    expected_width,\n+                ),\n+            )\n+\n+            # Test masking\n+            encoded_images = image_processing(image_inputs, return_image_mask=True, return_tensors=\"pt\")\n+\n+            expected_height, expected_width = self.image_processor_tester.get_expected_image_size()\n+            self.assertEqual(\n+                encoded_images.pixel_values.shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_channels,\n+                    expected_height,\n+                    expected_width,\n+                ),\n+            )\n+\n+            expected_height, expected_width = self.image_processor_tester.get_expected_mask_size()\n+            self.assertEqual(\n+                encoded_images.bool_masked_pos.shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    expected_height,\n+                    expected_width,\n+                ),\n+            )\n \n     def test_call_numpy(self):\n         self._test_call_framework(np.ndarray, prepare_kwargs={\"numpify\": True})\n@@ -346,40 +357,76 @@ def test_call_pytorch(self):\n         self._test_call_framework(torch.Tensor, prepare_kwargs={\"torchify\": True})\n \n     def test_masking(self):\n-        # Initialize image_processing\n-        random.seed(1234)\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            random.seed(1234)\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n \n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs[0], return_image_mask=True, return_tensors=\"pt\")\n-        self.assertEqual(encoded_images.bool_masked_pos.sum().item(), 75)\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_image_mask=True, return_tensors=\"pt\")\n+            self.assertEqual(encoded_images.bool_masked_pos.sum().item(), 75)\n \n     def test_codebook_pixels(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PIL images\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, PIL.Image.Image)\n-\n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs[0], return_codebook_pixels=True, return_tensors=\"pt\")\n-        expected_height, expected_width = self.image_processor_tester.get_expected_codebook_image_size()\n-        self.assertEqual(\n-            encoded_images.codebook_pixel_values.shape,\n-            (1, self.image_processor_tester.num_channels, expected_height, expected_width),\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, PIL.Image.Image)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_codebook_pixels=True, return_tensors=\"pt\")\n+            expected_height, expected_width = self.image_processor_tester.get_expected_codebook_image_size()\n+            self.assertEqual(\n+                encoded_images.codebook_pixel_values.shape,\n+                (1, self.image_processor_tester.num_channels, expected_height, expected_width),\n+            )\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_codebook_pixels=True, return_tensors=\"pt\")\n+            expected_height, expected_width = self.image_processor_tester.get_expected_codebook_image_size()\n+            self.assertEqual(\n+                encoded_images.codebook_pixel_values.shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_channels,\n+                    expected_height,\n+                    expected_width,\n+                ),\n+            )\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image = Image.open(\n+            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n+        )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(\n+            dummy_image, return_tensors=\"pt\", return_codebook_pixels=True, return_image_mask=True\n+        )\n+        encoding_fast = image_processor_fast(\n+            dummy_image, return_tensors=\"pt\", return_codebook_pixels=True, return_image_mask=True\n+        )\n+        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n         )\n \n-        # Test batched\n-        encoded_images = image_processing(image_inputs, return_codebook_pixels=True, return_tensors=\"pt\")\n-        expected_height, expected_width = self.image_processor_tester.get_expected_codebook_image_size()\n-        self.assertEqual(\n-            encoded_images.codebook_pixel_values.shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.num_channels,\n-                expected_height,\n-                expected_width,\n-            ),\n+        self.assertTrue(\n+            torch.allclose(encoding_slow.codebook_pixel_values, encoding_fast.codebook_pixel_values, atol=1e-1)\n+        )\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(encoding_slow.codebook_pixel_values - encoding_fast.codebook_pixel_values)).item(),\n+            1e-3,\n         )"
        }
    ],
    "stats": {
        "total": 960,
        "additions": 781,
        "deletions": 179
    }
}