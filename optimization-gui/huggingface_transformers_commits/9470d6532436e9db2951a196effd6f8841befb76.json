{
    "author": "zucchini-nlp",
    "message": "Fix low memory beam search (#34746)\n\n* fix\r\n\r\n* higher max positions in tests",
    "sha": "9470d6532436e9db2951a196effd6f8841befb76",
    "files": [
        {
            "sha": "490280ce813bd636bf46a52c5329a3c933547ab3",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/9470d6532436e9db2951a196effd6f8841befb76/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9470d6532436e9db2951a196effd6f8841befb76/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=9470d6532436e9db2951a196effd6f8841befb76",
            "patch": "@@ -528,7 +528,7 @@ def from_batch_splits(cls, splits: List[\"DynamicCache\"], num_hidden_layers: int\n         cache = cls()\n         for idx in range(len(splits[0])):\n             key_cache = [current.key_cache[idx] for current in splits if current.key_cache[idx] != []]\n-            value_cache = [current.key_cache[idx] for current in splits if current.key_cache[idx] != []]\n+            value_cache = [current.value_cache[idx] for current in splits if current.value_cache[idx] != []]\n             if key_cache != []:\n                 layer_keys = torch.cat(key_cache, dim=0)\n                 layer_values = torch.cat(value_cache, dim=0)\n@@ -1523,7 +1523,10 @@ def crop(self, maximum_length: int):\n         self.check_dynamic_cache(self.crop.__name__)\n         self.self_attention_cache.crop(maximum_length)\n \n-    def batch_split(self, full_batch_size: int, split_size: int) -> \"List[EncoderDecoderCache]\":\n+    @deprecate_kwarg(\"num_hidden_layers\", version=\"4.47.0\")\n+    def batch_split(\n+        self, full_batch_size: int, split_size: int, num_hidden_layers: int = None\n+    ) -> \"List[EncoderDecoderCache]\":\n         \"\"\"Split the current instance into a list of `DynamicCache` by the batch size. This will be used by\n         `_split_model_inputs()` in `generation.utils`\"\"\"\n         self.check_dynamic_cache(self.batch_split.__name__)\n@@ -1536,7 +1539,10 @@ def batch_split(self, full_batch_size: int, split_size: int) -> \"List[EncoderDec\n         return out\n \n     @classmethod\n-    def from_batch_splits(cls, splits: List[\"EncoderDecoderCache\"]) -> \"EncoderDecoderCache\":\n+    @deprecate_kwarg(\"num_hidden_layers\", version=\"4.47.0\")\n+    def from_batch_splits(\n+        cls, splits: List[\"EncoderDecoderCache\"], num_hidden_layers: int = None\n+    ) -> \"EncoderDecoderCache\":\n         \"\"\"This is the opposite of the above `batch_split()` method. This will be used by `stack_model_outputs` in\n         `generation.utils`\"\"\"\n         self_attention_cache = DynamicCache()"
        },
        {
            "sha": "76dc23ed9bf7c1133f7d1ec0ef84f81569eadf2a",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9470d6532436e9db2951a196effd6f8841befb76/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9470d6532436e9db2951a196effd6f8841befb76/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=9470d6532436e9db2951a196effd6f8841befb76",
            "patch": "@@ -1046,7 +1046,6 @@ def test_contrastive_generate_low_memory(self):\n             self.assertListEqual(low_output.tolist(), high_output.tolist())\n \n     @pytest.mark.generate\n-    @unittest.skip(\"Started to break with https://github.com/huggingface/transformers/pull/33703\")\n     def test_beam_search_low_memory(self):\n         # Check that choosing 'low_memory' does not change the model output\n         for model_class in self.all_generative_model_classes:"
        },
        {
            "sha": "a141ef40be195943d8d3d2d78ffafd6d4eab6359",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9470d6532436e9db2951a196effd6f8841befb76/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9470d6532436e9db2951a196effd6f8841befb76/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=9470d6532436e9db2951a196effd6f8841befb76",
            "patch": "@@ -330,7 +330,7 @@ def __init__(\n         hidden_act=\"gelu\",\n         hidden_dropout_prob=0.1,\n         attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=20,\n+        max_position_embeddings=512,\n         eos_token_id=2,\n         pad_token_id=1,\n         bos_token_id=0,"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 10,
        "deletions": 5
    }
}