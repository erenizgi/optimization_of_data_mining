{
    "author": "ydshieh",
    "message": "Use another repo. for Mistral3 processor testing (#36925)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "340500b1a9380c685b7a9c5721befcd7da5dbfd6",
    "files": [
        {
            "sha": "73e9b832277dce9652b03096f12fdafaba9695bb",
            "filename": "tests/models/mistral3/test_modeling_mistral3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/340500b1a9380c685b7a9c5721befcd7da5dbfd6/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/340500b1a9380c685b7a9c5721befcd7da5dbfd6/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py?ref=340500b1a9380c685b7a9c5721befcd7da5dbfd6",
            "patch": "@@ -25,6 +25,7 @@\n from transformers.testing_utils import (\n     cleanup,\n     require_bitsandbytes,\n+    require_read_token,\n     require_torch,\n     require_torch_gpu,\n     slow,\n@@ -315,6 +316,7 @@ def setUp(self):\n     def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n+    @require_read_token\n     def test_mistral3_integration_generate_text_only(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n         model = Mistral3ForConditionalGeneration.from_pretrained(\n@@ -342,6 +344,7 @@ def test_mistral3_integration_generate_text_only(self):\n         expected_output = \"Sure, here's a haiku for you:\\n\\nWhispers of the breeze,\\nCherry blossoms softly fall,\\nSpring's gentle embrace.\"\n         self.assertEqual(decoded_output, expected_output)\n \n+    @require_read_token\n     def test_mistral3_integration_generate(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n         model = Mistral3ForConditionalGeneration.from_pretrained(\n@@ -368,6 +371,7 @@ def test_mistral3_integration_generate(self):\n         expected_output = \"The image depicts two cats lying on a pink blanket. The larger cat, which appears to be an\"\n         self.assertEqual(decoded_output, expected_output)\n \n+    @require_read_token\n     def test_mistral3_integration_batched_generate(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n         model = Mistral3ForConditionalGeneration.from_pretrained(\n@@ -418,6 +422,7 @@ def test_mistral3_integration_batched_generate(self):\n             f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n         )\n \n+    @require_read_token\n     @require_bitsandbytes\n     def test_mistral3_integration_batched_generate_multi_image(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)"
        },
        {
            "sha": "13edfd80e5f0bfafb80ea1187e79877d7ce19aab",
            "filename": "tests/models/mistral3/test_processor_mistral3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/340500b1a9380c685b7a9c5721befcd7da5dbfd6/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/340500b1a9380c685b7a9c5721befcd7da5dbfd6/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py?ref=340500b1a9380c685b7a9c5721befcd7da5dbfd6",
            "patch": "@@ -20,7 +20,7 @@\n import requests\n \n from transformers import PixtralProcessor\n-from transformers.testing_utils import require_read_token, require_vision\n+from transformers.testing_utils import require_vision\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -35,7 +35,6 @@\n \n \n @require_vision\n-@require_read_token\n class Mistral3ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     \"\"\"This tests Pixtral processor with the new `spatial_merge_size` argument in Mistral3.\"\"\"\n \n@@ -52,7 +51,9 @@ def setUpClass(cls):\n \n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n-        processor = self.processor_class.from_pretrained(\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\")\n+        processor = PixtralProcessor.from_pretrained(\n+            \"hf-internal-testing/Mistral-Small-3.1-24B-Instruct-2503-only-processor\"\n+        )\n         processor.save_pretrained(self.tmpdirname)\n \n     def get_processor(self):"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 9,
        "deletions": 3
    }
}