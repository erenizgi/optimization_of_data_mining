{
    "author": "VladOS95-cyber",
    "message": "Add gguf support for StableLM (#33793)\n\n* add stablelm gguf architecture support\r\n\r\n* add additional quantization tests\r\n\r\n* resolve merge conflict, add weight conversion tests for fp16",
    "sha": "faa0f63b9375d8de18968b971ee342de566d4491",
    "files": [
        {
            "sha": "9f45d2ca4cf5d4d9385968bd3ce6ddde1acb8809",
            "filename": "docs/source/en/gguf.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/faa0f63b9375d8de18968b971ee342de566d4491/docs%2Fsource%2Fen%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/faa0f63b9375d8de18968b971ee342de566d4491/docs%2Fsource%2Fen%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgguf.md?ref=faa0f63b9375d8de18968b971ee342de566d4491",
            "patch": "@@ -82,6 +82,7 @@ For now the supported model architectures are the architectures that have been v\n - Phi3\n - Bloom\n - Falcon\n+- StableLM\n \n ## Example usage\n "
        },
        {
            "sha": "8b2497fc3d7eee18233592a09620e6ed8e9e63bc",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 30,
            "deletions": 3,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/faa0f63b9375d8de18968b971ee342de566d4491/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/faa0f63b9375d8de18968b971ee342de566d4491/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=faa0f63b9375d8de18968b971ee342de566d4491",
            "patch": "@@ -148,6 +148,21 @@\n         \".output.\": \".lm_head.\",\n         \"output_norm\": \"ln_f\",\n     },\n+    \"stablelm\": {\n+        \"token_embd\": \"model.embed_tokens\",\n+        \"blk\": \"model.layers\",\n+        \"ffn_up\": \"mlp.up_proj\",\n+        \"ffn_down\": \"mlp.down_proj\",\n+        \"ffn_gate\": \"mlp.gate_proj\",\n+        \"ffn_norm\": \"post_attention_layernorm\",\n+        \"attn_norm\": \"input_layernorm\",\n+        \"attn_q\": \"self_attn.q_proj\",\n+        \"attn_v\": \"self_attn.v_proj\",\n+        \"attn_k\": \"self_attn.k_proj\",\n+        \"attn_output\": \"self_attn.o_proj\",\n+        \"output.weight\": \"lm_head.weight\",\n+        \"output_norm\": \"model.norm\",\n+    },\n }\n \n \n@@ -245,6 +260,17 @@\n         \"vocab_size\": \"vocab_size\",\n         \"attention.layer_norm_epsilon\": \"layer_norm_epsilon\",\n     },\n+    \"stablelm\": {\n+        \"context_length\": \"max_position_embeddings\",\n+        \"block_count\": \"num_hidden_layers\",\n+        \"feed_forward_length\": \"intermediate_size\",\n+        \"embedding_length\": \"hidden_size\",\n+        \"rope.dimension_count\": None,\n+        \"attention.head_count\": \"num_attention_heads\",\n+        \"attention.head_count_kv\": \"num_key_value_heads\",\n+        \"attention.layer_norm_epsilon\": \"layer_norm_eps\",\n+        \"vocab_size\": \"vocab_size\",\n+    },\n }\n \n GGUF_TOKENIZER_MAPPING = {\n@@ -554,7 +580,7 @@ def converted(self) -> Tokenizer:\n         return tokenizer\n \n \n-class GGUFBloomConverter(GPT2Converter):\n+class GGUFGPTConverter(GPT2Converter):\n     def __init__(self, tokenizer_dict):\n         self.original_tokenizer = GGUFTokenizerSkeleton(tokenizer_dict)\n         self.additional_kwargs = {}\n@@ -571,8 +597,9 @@ def converted(self) -> Tokenizer:\n     \"qwen2\": GGUFQwen2Converter,\n     \"qwen2_moe\": GGUFQwen2Converter,\n     \"phi3\": GGUFPhi3Converter,\n-    \"bloom\": GGUFBloomConverter,\n-    \"falcon\": GGUFBloomConverter,\n+    \"bloom\": GGUFGPTConverter,\n+    \"falcon\": GGUFGPTConverter,\n+    \"stablelm\": GGUFGPTConverter,\n }\n \n "
        },
        {
            "sha": "7fafa440d0511305435e26380a65cb4fef6f2a95",
            "filename": "src/transformers/models/gpt_neox/tokenization_gpt_neox_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/faa0f63b9375d8de18968b971ee342de566d4491/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/faa0f63b9375d8de18968b971ee342de566d4491/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox_fast.py?ref=faa0f63b9375d8de18968b971ee342de566d4491",
            "patch": "@@ -105,8 +105,8 @@ def __init__(\n         **kwargs,\n     ):\n         super().__init__(\n-            vocab_file,\n-            merges_file,\n+            vocab_file=vocab_file,\n+            merges_file=merges_file,\n             tokenizer_file=tokenizer_file,\n             unk_token=unk_token,\n             bos_token=bos_token,"
        },
        {
            "sha": "4034ec167c4be027d9049939fb735fd0f1c8c903",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 74,
            "deletions": 0,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/faa0f63b9375d8de18968b971ee342de566d4491/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/faa0f63b9375d8de18968b971ee342de566d4491/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=faa0f63b9375d8de18968b971ee342de566d4491",
            "patch": "@@ -48,6 +48,9 @@ class GgufIntegrationTests(unittest.TestCase):\n     falcon7b_model_id = \"xaviviro/falcon-7b-quantized-gguf\"\n     falcon40b_model_id = \"maddes8cht/tiiuae-falcon-40b-gguf\"\n     original_flacon7b_model_id = \"tiiuae/falcon-7b\"\n+    stablelm_model_id = \"afrideva/stablelm-3b-4e1t-GGUF\"\n+    stablelm2_model_id = \"afrideva/stablelm-2-1_6b-GGUF\"\n+    original_stablelm2_model_id = \"stabilityai/stablelm-2-1_6b\"\n \n     # standard quants\n     q4_0_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q4_0.gguf\"\n@@ -59,6 +62,7 @@ class GgufIntegrationTests(unittest.TestCase):\n     q4_k_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n     q5_k_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf\"\n     q6_k_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q6_K.gguf\"\n+    q4_k_m_stablelm_model_id = \"stablelm-3b-4e1t.q4_k_m.gguf\"\n     # imatrix\n     iq1_m_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ1_M.gguf\"\n     iq1_s_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ1_S.gguf\"\n@@ -76,6 +80,7 @@ class GgufIntegrationTests(unittest.TestCase):\n     q8_qwen2moe_model_id = \"Qwen1.5-MoE-A2.7B_Q8_0.gguf\"\n     q4_llama3_model_id = \"Meta-Llama-3-8B-Q4_K_M.gguf\"\n     fp16_bloom_model_id = \"bloom-560m.fp16.gguf\"\n+    fp16_stablelm2_model_id = \"stablelm-2-1_6b.fp16.gguf\"\n     q8_bloom_model_id = \"bloom-560m.q8_0.gguf\"\n     f16_tinyllama_model_id = \"TinyLlama-1.1B-Chat-v1.0.FP16.gguf\"\n     q2_k_falcon7b_model_id = \"falcon-7b-q2_k.gguf\"\n@@ -523,6 +528,75 @@ def test_falcon7b_weights_conversion_fp16(self):\n                 self.assertTrue(original_params.shape == quantized_state_dict[layer_name].shape)\n                 torch.testing.assert_close(original_params, quantized_state_dict[layer_name])\n \n+    def test_stablelm_q4_k_m(self):\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.stablelm_model_id,\n+            gguf_file=self.q4_k_m_stablelm_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+        )\n+\n+        tokenizer = AutoTokenizer.from_pretrained(self.stablelm_model_id, gguf_file=self.q4_k_m_stablelm_model_id)\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello-\\nI am trying to create a new user\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_stablelm_fp16(self):\n+        original_model = AutoModelForCausalLM.from_pretrained(\n+            self.original_stablelm2_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        converted_model = AutoModelForCausalLM.from_pretrained(\n+            self.stablelm2_model_id,\n+            gguf_file=self.fp16_stablelm2_model_id,\n+            torch_dtype=torch.float16,\n+            # for precise comparison it is required to use the original model config\n+            # as quantized one is different in parameters: use_parallel_residual and use_qkv_bias\n+            # and it highly influences on the output results\n+            config=original_model.config,\n+        )\n+\n+        tokenizer = AutoTokenizer.from_pretrained(self.stablelm2_model_id, gguf_file=self.fp16_stablelm2_model_id)\n+        text = tokenizer(self.example_text, return_tensors=\"pt\")\n+        original_out = original_model.generate(**text, max_new_tokens=10)\n+        converted_out = converted_model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello, I am a 20 year old male\"\n+        self.assertEqual(tokenizer.decode(converted_out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+        self.assertEqual(\n+            tokenizer.decode(converted_out[0], skip_special_tokens=True),\n+            tokenizer.decode(original_out[0], skip_special_tokens=True),\n+        )\n+\n+    def test_stablelm_weights_conversion_fp16(self):\n+        original_model = AutoModelForCausalLM.from_pretrained(\n+            self.original_stablelm2_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+        )\n+\n+        converted_model = AutoModelForCausalLM.from_pretrained(\n+            self.stablelm2_model_id,\n+            gguf_file=self.fp16_stablelm2_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+            # for precise comparison it is required to use the original model config\n+            # as quantized one is different in parameters: use_parallel_residual and use_qkv_bias\n+            # and it highly influences on the output results\n+            config=original_model.config,\n+        )\n+\n+        converted_state_dict = converted_model.state_dict()\n+        original_state_dict = original_model.state_dict()\n+\n+        for layer_name, original_params in original_state_dict.items():\n+            if layer_name in converted_state_dict:\n+                self.assertTrue(original_params.shape == converted_state_dict[layer_name].shape)\n+                torch.testing.assert_close(original_params, converted_state_dict[layer_name])\n+\n     def test_tokenization_xnli(self):\n         import tqdm\n         from datasets import load_dataset"
        }
    ],
    "stats": {
        "total": 112,
        "additions": 107,
        "deletions": 5
    }
}