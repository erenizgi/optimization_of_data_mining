{
    "author": "gante",
    "message": "[custom_generate] don't forward `custom_generate` and `trust_remote_code` (#38304)\n\n* prevent infinite loops\n\n* docs\n\n* more links to custom generation methods",
    "sha": "54cd86708d2b63a1f696ee1c59384a2f04100f57",
    "files": [
        {
            "sha": "9e2cbf485c427116fa37e979f1e3c206255cd0b5",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/54cd86708d2b63a1f696ee1c59384a2f04100f57/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/54cd86708d2b63a1f696ee1c59384a2f04100f57/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=54cd86708d2b63a1f696ee1c59384a2f04100f57",
            "patch": "@@ -327,7 +327,6 @@ We enable custom decoding methods through model repositories, assuming a specifi\n \n If a model repository holds a custom decoding method, the easiest way to try it out is to load the model and generate with it:\n \n-<!-- TODO before merging: 1) better repo name (use a `generate-community` org?) 2) prettify the repo -->\n ```py\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n@@ -430,7 +429,7 @@ This is the core of your decoding method. It *must* contain a method named `gene\n > [!WARNING]\n > `generate.py` must be placed in a folder named `custom_generate`, and not at the root level of the repository. The file paths for this feature are hardcoded.\n \n-Under the hood, when the base [`~GenerationMixin.generate`] method is called with a `custom_generate` argument, it first checks its Python requirements (if any), then locates the custom `generate` method in `generate.py`, and finally calls the custom `generate`. All received arguments and `model` are forwarded to your custom `generate` method.\n+Under the hood, when the base [`~GenerationMixin.generate`] method is called with a `custom_generate` argument, it first checks its Python requirements (if any), then locates the custom `generate` method in `generate.py`, and finally calls the custom `generate`. All received arguments and `model` are forwarded to your custom `generate` method, with the exception of the arguments used to trigger the custom generation (`trust_remote_code` and `custom_generate`).\n \n This means your `generate` can have a mix of original and custom arguments (as well as a different output type) as shown below.\n "
        },
        {
            "sha": "1283e8b6a4c88eaaefa88f99701fae149cec7937",
            "filename": "docs/source/en/llm_tutorial.md",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/54cd86708d2b63a1f696ee1c59384a2f04100f57/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/54cd86708d2b63a1f696ee1c59384a2f04100f57/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial.md?ref=54cd86708d2b63a1f696ee1c59384a2f04100f57",
            "patch": "@@ -84,14 +84,17 @@ GenerationConfig {\n }\n ```\n \n-You can customize [`~GenerationMixin.generate`] by overriding the parameters and values in [`GenerationConfig`]. Some of the most commonly adjusted parameters are [max_new_tokens](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.max_new_tokens), [num_beams](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.num_beams), [do_sample](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.do_sample), and [num_return_sequences](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.num_return_sequences).\n+You can customize [`~GenerationMixin.generate`] by overriding the parameters and values in [`GenerationConfig`]. See [this section below](#common-options) for commonly adjusted parameters.\n \n ```py\n # enable beam search sampling strategy\n model.generate(**inputs, num_beams=4, do_sample=True)\n ```\n \n-[`~GenerationMixin.generate`] can also be extended with external libraries or custom code. The `logits_processor` parameter accepts custom [`LogitsProcessor`] instances for manipulating the next token probability distribution. `stopping_criteria` supports custom [`StoppingCriteria`] to stop text generation. Check out the [logits-processor-zoo](https://github.com/NVIDIA/logits-processor-zoo) for more examples of external [`~GenerationMixin.generate`]-compatible extensions.\n+[`~GenerationMixin.generate`] can also be extended with external libraries or custom code:\n+1. the `logits_processor` parameter accepts custom [`LogitsProcessor`] instances for manipulating the next token probability distribution;\n+2. the `stopping_criteria` parameters supports custom [`StoppingCriteria`] to stop text generation;\n+3. other custom generation methods can be loaded through the `custom_generate` flag ([docs](generation_strategies.md/#custom-decoding-methods)).\n \n Refer to the [Generation strategies](./generation_strategies) guide to learn more about search, sampling, and decoding strategies.\n "
        },
        {
            "sha": "8ea1e7e76014ff1e3ab31ba6628810c7ecc4d4d6",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/54cd86708d2b63a1f696ee1c59384a2f04100f57/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54cd86708d2b63a1f696ee1c59384a2f04100f57/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=54cd86708d2b63a1f696ee1c59384a2f04100f57",
            "patch": "@@ -2347,9 +2347,15 @@ def generate(\n         if custom_generate is not None:\n             trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n             # Get all `generate` arguments in a single variable. Custom functions are responsible for handling them:\n-            # they receive the same inputs as `generate`, only with `model` instead of `self`. They can access to\n-            # methods from `GenerationMixin` through `model`.\n-            global_keys_to_exclude = {\"self\", \"kwargs\"}\n+            # they receive the same inputs as `generate`, with `model` instead of `self` and excluding the arguments to\n+            # trigger the custom generation. They can access to methods from `GenerationMixin` through `model`.\n+            global_keys_to_exclude = {\n+                \"self\",\n+                \"kwargs\",\n+                \"global_keys_to_exclude\",\n+                \"trust_remote_code\",\n+                \"custom_generate\",\n+            }\n             generate_arguments = {key: value for key, value in locals().items() if key not in global_keys_to_exclude}\n             generate_arguments.update(kwargs)\n "
        }
    ],
    "stats": {
        "total": 22,
        "additions": 15,
        "deletions": 7
    }
}