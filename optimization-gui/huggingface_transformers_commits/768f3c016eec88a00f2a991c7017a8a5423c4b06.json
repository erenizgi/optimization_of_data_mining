{
    "author": "AhmedAlmaghz",
    "message": "[i18n-ar] Translated file : `docs/source/ar/trainer.md` into Arabic (#33080)\n\n* Add docs/source/ar/trainer.md to Add_docs_source_ar_trainer.md\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\n\r\n* Update trainer.md\r\n\r\n* Update trainer.md\r\n\r\n* Update trainer.md\r\n\r\n* Create _toctree.yml\r\n\r\n* Delete docs/source/ar/_toctree.yml\r\n\r\n* Update _toctree.yml - add trainer\r\n\r\n* Update _toctree.yml\r\n\r\n* merge serialization.md into this branch\r\n\r\n* merge sagemaker.md into this PR\r\n\r\n* Update _toctree.yml\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/ar/trainer.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "768f3c016eec88a00f2a991c7017a8a5423c4b06",
    "files": [
        {
            "sha": "67564c43556db736382a12cc065cabe986550ae6",
            "filename": "docs/source/ar/_toctree.yml",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/768f3c016eec88a00f2a991c7017a8a5423c4b06/docs%2Fsource%2Far%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/768f3c016eec88a00f2a991c7017a8a5423c4b06/docs%2Fsource%2Far%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2F_toctree.yml?ref=768f3c016eec88a00f2a991c7017a8a5423c4b06",
            "patch": "@@ -119,12 +119,12 @@\n     title: Ù…Ø´Ø§Ø±ÙƒØ© Ù†Ù…ÙˆØ°Ø¬ Ù…Ø®ØµØµ\n   - local: chat_templating\n     title: Ù‚ÙˆØ§Ù„Ø¨ Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©\n-#   - local: trainer\n-#     title: Ø§Ù„Ù…Ø¯Ø±Ø¨\n-#   - local: sagemaker\n-#     title: ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Amazon SageMaker\n-#   - local: serialization\n-#     title: Ø§Ù„ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ ONNX\n+  - local: trainer\n+    title: Ø§Ù„Ù…Ø¯Ø±Ø¨\n+  - local: sagemaker\n+    title: ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Amazon SageMaker\n+  - local: serialization\n+    title: Ø§Ù„ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ ONNX\n   - local: tflite\n     title: Ø§Ù„ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ TFLite\n #   - local: torchscript"
        },
        {
            "sha": "6bb53816baaaee483277076a54a530bfa2e9f278",
            "filename": "docs/source/ar/sagemaker.md",
            "status": "added",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/768f3c016eec88a00f2a991c7017a8a5423c4b06/docs%2Fsource%2Far%2Fsagemaker.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/768f3c016eec88a00f2a991c7017a8a5423c4b06/docs%2Fsource%2Far%2Fsagemaker.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fsagemaker.md?ref=768f3c016eec88a00f2a991c7017a8a5423c4b06",
            "patch": "@@ -0,0 +1,8 @@\n+# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Amazon SageMaker\n+\n+ØªÙ… Ù†Ù‚Ù„ Ø§Ù„ØªÙˆØ«ÙŠÙ‚ Ø¥Ù„Ù‰ [hf.co/docs/sagemaker](https://huggingface.co/docs/sagemaker). ÙˆØ³ÙŠØªÙ… Ø¥Ø²Ø§Ù„Ø© Ù‡Ø°Ù‡ Ø§Ù„ØµÙØ­Ø© ÙÙŠ Ø§Ù„Ø¥ØµØ¯Ø§Ø± 5.0 Ù…Ù† Ø¨Ø±Ù†Ø§Ù…Ø¬ Transformers.\n+\n+### Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙŠØ§Øª\n+\n+- [ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Hugging Face Ø¹Ù„Ù‰ Amazon SageMaker Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… SageMaker Python SDK](https://huggingface.co/docs/sagemaker/train)\n+- [Ù†Ø´Ø± Ù†Ù…Ø§Ø°Ø¬ Hugging Face Ø¹Ù„Ù‰ Amazon SageMaker Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… SageMaker Python SDK](https://huggingface.co/docs/sagemaker/inference)\n\\ No newline at end of file"
        },
        {
            "sha": "2df620d86239a09d3a78edb63ffcdcacf63406d2",
            "filename": "docs/source/ar/serialization.md",
            "status": "added",
            "additions": 170,
            "deletions": 0,
            "changes": 170,
            "blob_url": "https://github.com/huggingface/transformers/blob/768f3c016eec88a00f2a991c7017a8a5423c4b06/docs%2Fsource%2Far%2Fserialization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/768f3c016eec88a00f2a991c7017a8a5423c4b06/docs%2Fsource%2Far%2Fserialization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fserialization.md?ref=768f3c016eec88a00f2a991c7017a8a5423c4b06",
            "patch": "@@ -0,0 +1,170 @@\n+# Ø§Ù„ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ ONNX\n+\n+ØºØ§Ù„Ø¨Ø§Ù‹ Ù…Ø§ ÙŠØªØ·Ù„Ø¨ Ù†Ø´Ø± Ù†Ù…Ø§Ø°Ø¬ ğŸ¤— Transformers ÙÙŠ Ø¨ÙŠØ¦Ø§Øª Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ø£Ùˆ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ³ØªÙÙŠØ¯ Ù…Ù† ØªØµØ¯ÙŠØ± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ ØªØ³Ù„Ø³Ù„ÙŠ ÙŠÙÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„Ù‡ ÙˆØªÙ†ÙÙŠØ°Ù‡ Ø¹Ù„Ù‰ Ø£Ø¬Ù‡Ø²Ø© ÙˆØ¨Ø±Ø§Ù…Ø¬ ØªØ´ØºÙŠÙ„ Ù…ÙØªØ®ØµØµØ©.\n+\n+ğŸ¤— Optimum Ù‡Ùˆ Ø§Ù…ØªØ¯Ø§Ø¯ Ù„Ù€ Transformers ÙŠÙ…ÙƒÙ‘Ù† Ù…Ù† ØªØµØ¯ÙŠØ± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† PyTorch Ø£Ùˆ TensorFlow Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚Ø§Øª Ù…ÙØªØ³Ù„Ø³Ù„Ø© Ù…Ø«Ù„ ONNX Ùˆ TFLite Ù…Ù† Ø®Ù„Ø§Ù„ ÙˆØ­Ø¯Ø© `exporters` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù‡. ÙŠÙˆÙØ± ğŸ¤— Optimum Ø£ÙŠØ¶Ù‹Ø§ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø£Ø¯ÙˆØ§Øª ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØªØ´ØºÙŠÙ„Ù‡Ø§ Ø¹Ù„Ù‰ Ø£Ø¬Ù‡Ø²Ø© Ù…Ø³ØªÙ‡Ø¯ÙØ© Ø¨ÙƒÙØ§Ø¡Ø© Ù‚ØµÙˆÙ‰.\n+\n+ÙŠÙˆØ¶Ø­ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ© ØªØµØ¯ÙŠØ± Ù†Ù…Ø§Ø°Ø¬ ğŸ¤— Transformers Ø¥Ù„Ù‰ ONNX Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ğŸ¤— OptimumØŒ ÙˆÙ„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø®Ø§Øµ Ø¨ØªØµØ¯ÙŠØ± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¥Ù„Ù‰ TFLiteØŒ ÙŠÙØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ ØµÙØ­Ø© [Ø§Ù„ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ TFLite](tflite).\n+\n+## Ø§Ù„ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ ONNX\n+\n+Ù…Ø¬Ù…Ø¯ [ONNX (Open Neural Network Exchange)](http://onnx.ai) Ù‡Ùˆ Ù…Ø¹ÙŠØ§Ø± Ù…ÙØªÙˆØ­ ÙŠÙØ­Ø¯Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ø´ØªØ±ÙƒØ© Ù…Ù† Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ ÙˆØªÙ†Ø³ÙŠÙ‚ Ù…Ù„Ù Ù…Ø´ØªØ±Ùƒ Ù„ØªÙ…Ø«ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ØªÙ†ÙˆØ¹Ø© ÙˆØ§Ø³Ø¹Ø© Ù…Ù† Ø§Ù„Ø£Ø·Ø±ØŒ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ PyTorch ÙˆTensorFlow. Ø¹Ù†Ø¯Ù…Ø§ ÙŠØªÙ… ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ ONNXØŒ ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´ØºÙ„Ø§Øª Ù„Ø¨Ù†Ø§Ø¡ Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ø­Ø§Ø³ÙˆØ¨ÙŠ (ÙŠÙØ·Ù„Ù‚ Ø¹Ù„ÙŠÙ‡ ØºØ§Ù„Ø¨Ù‹Ø§ Ø§Ø³Ù… _ØªÙ…Ø«ÙŠÙ„ ÙˆØ³ÙŠØ·_) ÙˆØ§Ù„Ø°ÙŠ ÙŠÙ…Ø«Ù„ ØªØ¯ÙÙ‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ø¨Ø± Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©.\n+\n+Ù…Ù† Ø®Ù„Ø§Ù„ Ø¹Ø±Ø¶ Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ø¨Ø¹ÙˆØ§Ù…Ù„ ÙˆØ£Ù†ÙˆØ§Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ÙŠØ§Ø±ÙŠØ©ØŒ ÙŠÙØ³Ù‡Ù‘Ù„ ONNX  Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ø¨ÙŠÙ† Ø§Ù„Ø£Ø·Ø±. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠÙÙ…ÙƒÙ† ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¯Ø±Ø¨ ÙÙŠ PyTorch Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ ONNX Ø«Ù… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ ÙÙŠ TensorFlow (ÙˆØ§Ù„Ø¹ÙƒØ³ ØµØ­ÙŠØ­).\n+\n+Ø¨Ù…Ø¬Ø±Ø¯ Ø§Ù„ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ ONNXØŒ ÙŠÙÙ…ÙƒÙ†:\n+\n+-  ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¹Ø¨Ø± ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø«Ù„ [ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization) Ùˆ [Ø§Ù„ØªÙƒÙ…ÙŠÙ…](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization).\n+- ØªØ´ØºÙŠÙ„Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ONNX Runtime Ø¹Ø¨Ø± ÙØ¦Ø§Øª [`ORTModelForXXX`](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort)ØŒ ÙˆØ§Ù„ØªÙŠ ØªØªØ¨Ø¹ Ù†ÙØ³ ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª (API) Ù„Ù€ `AutoModel` Ø§Ù„ØªÙŠ Ø§Ø¹ØªØ¯Øª Ø¹Ù„ÙŠÙ‡Ø§ ÙÙŠ ğŸ¤— Transformers.\n+- ØªØ´ØºÙŠÙ„Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [Ù‚Ù†ÙˆØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…ÙØ­Ø³Ù‘Ù†Ø©](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines)ØŒ ÙˆØ§Ù„ØªÙŠ Ù„Ù‡Ø§ Ù†ÙØ³ ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª (API) Ù…Ø«Ù„ ÙˆØ¸ÙŠÙØ© [`pipeline`] ÙÙŠ ğŸ¤— Transformers.\n+\n+ÙŠÙˆÙØ± ğŸ¤— Optimum Ø¯Ø¹Ù…Ù‹Ø§ Ù„ØªØµØ¯ÙŠØ± ONNX Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†. ØªØ£ØªÙŠ ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† Ù‡Ø°Ù‡ Ø¬Ø§Ù‡Ø²Ø© Ù„Ø¹Ø¯Ø¯ Ù…Ù† Ù…Ø¹Ù…Ø§Ø±ÙŠØ§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ØŒ ÙˆÙ‚Ø¯ ØªÙ… ØªØµÙ…ÙŠÙ…Ù‡Ø§ Ù„ØªÙƒÙˆÙ† Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙˆØ³Ø¹Ø© Ø¨Ø³Ù‡ÙˆÙ„Ø© Ø¥Ù„Ù‰ Ù…Ø¹Ù…Ø§Ø±ÙŠØ§Øª Ø£Ø®Ø±Ù‰.\n+\n+Ù„Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„ØªÙƒÙˆÙŠÙ†Ø§Øª Ø§Ù„Ø¬Ø§Ù‡Ø²Ø©ØŒ ÙŠÙØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ [ÙˆØ«Ø§Ø¦Ù‚ ğŸ¤— Optimum](https://huggingface.co/docs/optimum/exporters/onnx/overview).\n+\n+Ù‡Ù†Ø§Ùƒ Ø·Ø±ÙŠÙ‚ØªØ§Ù† Ù„ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ğŸ¤— Transformers Ø¥Ù„Ù‰ ONNXØŒ  Ù†Ø¹Ø±Ø¶ Ù‡Ù†Ø§ ÙƒÙ„ÙŠÙ‡Ù…Ø§:\n+\n+- Ø§Ù„ØªØµØ¯ÙŠØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ğŸ¤— Optimum Ø¹Ø¨Ø± ÙˆØ§Ø¬Ù‡Ø© Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø± (CLI).\n+- Ø§Ù„ØªØµØ¯ÙŠØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ğŸ¤— Optimum Ù…Ø¹ `optimum.onnxruntime`.\n+\n+### ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ğŸ¤— Transformers Ø¥Ù„Ù‰ ONNX Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±\n+\n+Ù„ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ğŸ¤— Transformers Ø¥Ù„Ù‰ ONNXØŒ Ù‚Ù… Ø£ÙˆÙ„Ø§Ù‹ Ø¨ØªØ«Ø¨ÙŠØª Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¥Ø¶Ø§ÙÙŠ:\n+\n+```bash\n+pip install optimum[exporters]\n+```\n+\n+Ù„Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ø§Ù…ï»»Øª Ø§Ù„Ù…ØªØ§Ø­Ø©ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ [ÙˆØ«Ø§Ø¦Ù‚ ğŸ¤— Optimum](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli)ØŒ Ø£Ùˆ Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±:\n+\n+```bash\n+optimum-cli export onnx --help\n+```\n+```bash\n+optimum-cli export onnx --help\n+```\n+\n+Ù„ØªØµØ¯ÙŠØ± Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† ğŸ¤— HubØŒ Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ `distilbert/distilbert-base-uncased-distilled-squad`ØŒ Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØªØ§Ù„ÙŠ:\n+\n+```bash\n+optimum-cli export onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/\n+```\n+\n+ÙŠØ¬Ø¨ Ø£Ù† ØªØ´Ø§Ù‡Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„ØªÙ‚Ø¯Ù… Ø§Ù„Ù…Ø­Ø±Ø² ÙˆØªØ¸Ù‡Ø± Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„Ø°ÙŠ ØªÙ… ÙÙŠÙ‡ Ø­ÙØ¸ Ù…Ù„Ù `model.onnx` Ø§Ù„Ù†Ø§ØªØ¬ØŒ Ù…Ø«Ù„ Ù‡Ø°Ø§:\n+\n+```bash\n+Validating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...\n+\t-[âœ“] ONNX model output names match reference model (start_logits, end_logits)\n+\t- Validating ONNX Model output \"start_logits\":\n+\t\t-[âœ“] (2, 16) matches (2, 16)\n+\t\t-[âœ“] all values close (atol: 0.0001)\n+\t- Validating ONNX Model output \"end_logits\":\n+\t\t-[âœ“] (2, 16) matches (2, 16)\n+\t\t-[âœ“] all values close (atol: 0.0001)\n+The ONNX export succeeded and the exported model was saved at: distilbert_base_uncased_squad_onnx\n+```\n+\n+ÙŠÙˆØ¶Ø­ Ø§Ù„Ù…Ø«Ø§Ù„ Ø£Ø¹Ù„Ø§Ù‡ ØªØµØ¯ÙŠØ± Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ù…Ù† ğŸ¤— Hub. Ø¹Ù†Ø¯ ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ù„ÙŠØŒ ØªØ£ÙƒØ¯ Ø£ÙˆÙ„Ø§Ù‹ Ù…Ù† Ø­ÙØ¸ Ù…Ù„ÙØ§Øª Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…Ø­ÙˆÙ„ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø¯Ù„ÙŠÙ„ (`local_path`). Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± `local_path` Ø¥Ù„Ù‰ ÙˆØ³ÙŠØ· `model` Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ø³Ù… Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´ Ø¹Ù„Ù‰ ğŸ¤— Hub ÙˆÙ‚Ø¯Ù… ÙˆØ³ÙŠØ· `--task`. ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø±Ø§Ø¬Ø¹Ø© Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© ÙÙŠ [ÙˆØ«Ø§Ø¦Ù‚ ğŸ¤— Optimum](https://huggingface.co/docs/optimum/exporters/task_manager). Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªÙˆÙÙŠØ± ÙˆØ³ÙŠØ· `task`ØŒ ÙØ³ÙŠØªÙ… ØªØ¹ÙŠÙŠÙ†Ù‡ Ø§ÙØªØ±Ø§Ø¶ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¯ÙˆÙ† Ø£ÙŠ Ø±Ø£Ø³ Ù…Ø­Ø¯Ø¯ Ù„Ù„Ù…Ù‡Ù…Ø©.\n+\n+```bash\n+optimum-cli export onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/\n+```\n+\n+ÙŠÙ…ÙƒÙ† Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ ØªØ´ØºÙŠÙ„ Ù…Ù„Ù `model.onnx` Ø§Ù„Ù†Ø§ØªØ¬ Ø¹Ù„Ù‰ Ø£Ø­Ø¯ [Ø§Ù„Ù…Ø³Ø±Ø¹Ø§Øª](https://onnx.ai/supported-tools.html#deployModel) Ø§Ù„Ø¹Ø¯ÙŠØ¯Ø© Ø§Ù„ØªÙŠ ØªØ¯Ø¹Ù… Ù…Ø¹ÙŠØ§Ø± ONNX. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØªØ´ØºÙŠÙ„Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [ONNX Runtime](https://onnxruntime.ai/) ÙƒÙ…Ø§ ÙŠÙ„ÙŠ:\n+\n+```python\n+>>> from transformers import AutoTokenizer\n+>>> from optimum.onnxruntime import ORTModelForQuestionAnswering\n+\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n+>>> model = ORTModelForQuestionAnswering.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n+>>> inputs = tokenizer(\"What am I using?\", \"Using DistilBERT with ONNX Runtime!\", return_tensors=\"pt\")\n+>>> outputs = model(**inputs)\n+```\n+\n+ØªÙƒÙˆÙ† Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù…Ù…Ø§Ø«Ù„Ø© Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ø¥Ù„Ù‰ Ù†Ù‚Ø§Ø· ØªÙØªÙŠØ´ TensorFlow Ø¹Ù„Ù‰ Hub. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¥Ù„ÙŠÙƒ ÙƒÙŠÙÙŠØ© ØªØµØ¯ÙŠØ± Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ TensorFlow Ù†Ù‚ÙŠØ© Ù…Ù† [Ù…Ù†Ø¸Ù…Ø© Keras](https://huggingface.co/keras-io):\n+\n+```bash\n+optimum-cli export onnx --model keras-io/transformers-qa distilbert_base_cased_squad_onnx/\n+```\n+\n+### ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ğŸ¤— Transformers Ø¥Ù„Ù‰ ONNX Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `optimum.onnxruntime`\n+\n+ÙƒØ¨Ø¯ÙŠÙ„ Ù„ÙˆØ§Ø¬Ù‡Ø© Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±ØŒ ÙŠÙÙ…ÙƒÙ†Ùƒ ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ğŸ¤— Transformers Ø¥Ù„Ù‰ ONNX Ø¨Ø±Ù…Ø¬ÙŠÙ‹Ø§ ÙƒÙ…Ø§ ÙŠÙ„ÙŠ:\n+\n+```python\n+>>> from optimum.onnxruntime import ORTModelForSequenceClassification\n+>>> from transformers import AutoTokenizer\n+\n+>>> model_checkpoint = \"distilbert_base_uncased_squad\"\n+>>> save_directory = \"onnx/\"\n+\n+>>> # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† transformers ÙˆØªØµØ¯ÙŠØ±Ù‡ Ø¥Ù„Ù‰ ONNX\n+>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n+\n+>>> # Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ onnx ÙˆÙ…Ø¬Ø²Ù‰Ø¡ Ø§Ù„Ù†ØµÙˆØµ\n+>>> ort_model.save_pretrained(save_directory)\n+>>> tokenizer.save_pretrained(save_directory)\n+```\n+\n+### ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù‡Ù†Ø¯Ø³Ø© ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©\n+\n+Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ØºØ¨ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø© Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ø¶Ø§ÙØ© Ø¯Ø¹Ù… Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§ ÙŠÙÙ…ÙƒÙ† ØªØµØ¯ÙŠØ±Ù‡ Ø­Ø§Ù„ÙŠÙ‹Ø§ØŒ ÙÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø£ÙˆÙ„Ø§Ù‹ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø¯Ø¹ÙˆÙ…Ù‹Ø§ ÙÙŠ [`optimum.exporters.onnx`](https://huggingface.co/docs/optimum/exporters/onnx/overview)ØŒ ÙˆØ¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…Ø¯Ø¹ÙˆÙ…Ù‹Ø§ØŒ [ÙÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø© ÙÙŠ ğŸ¤— Optimum](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute) Ù…ÙØ¨Ø§Ø´Ø±Ø©Ù‹.\n+\n+### ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `transformers.onnx`\n+\n+<Tip warning={true}>\n+\n+Ù„Ù… ÙŠØ¹Ø¯ ÙŠØªÙ… Ø¯Ø¹Ù… `tranformers.onnx`  ÙŠÙØ±Ø¬Ù‰ ØªØµØ¯ÙŠØ± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ğŸ¤— Optimum ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ Ø£Ø¹Ù„Ø§Ù‡. Ø³ÙŠØªÙ… Ø¥Ø²Ø§Ù„Ø© Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… ÙÙŠ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©.\n+\n+</Tip>\n+\n+Ù„ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ğŸ¤— Transformers Ø¥Ù„Ù‰ ONNX Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `tranformers.onnx`ØŒ Ø«Ø¨Ù‘Øª Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©:\n+\n+```bash\n+pip install transformers[onnx]\n+```\n+\n+Ø§Ø³ØªØ®Ø¯Ù… Ø­Ø²Ù…Ø© `transformers.onnx` ÙƒÙ†Ù…ÙˆØ°Ø¬ Python Ù„ØªØµØ¯ÙŠØ± Ù†Ù‚Ø·Ø© Ø­ÙØ¸ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙƒÙˆÙŠÙ† Ø¬Ø§Ù‡Ø²:\n+\n+```bash\n+python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/\n+```\n+\n+ÙŠÙØµØ¯Ù‘Ø± Ù‡Ø°Ø§ Ø±Ø³Ù…Ù‹Ø§ Ø¨ÙŠØ§Ù†ÙŠÙ‹Ø§ ONNX Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ù…ÙØ­Ø¯Ø¯Ø© Ø¨ÙˆØ§Ø³Ø·Ø© ÙˆØ³ÙŠØ·Ø© `--model`. Ù…Ø±Ø± Ø£ÙŠ Ù†Ù‚Ø·Ø© Ø­ÙØ¸ Ø¹Ù„Ù‰ ğŸ¤— Hub Ø£Ùˆ Ù†Ù‚Ø·Ø© Ø­ÙØ¸ Ù…ÙØ®Ø²Ù†Ø© Ù…Ø­Ù„ÙŠÙ‹Ø§.\n+ÙŠÙÙ…ÙƒÙ† Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ ØªØ´ØºÙŠÙ„ Ù…Ù„Ù `model.onnx` Ø§Ù„Ù†Ø§ØªØ¬ Ø¹Ù„Ù‰ Ø£Ø­Ø¯ Ø§Ù„Ù…ÙØ³Ø±Ø¹Ø§Øª Ø§Ù„Ø¹Ø¯ÙŠØ¯Ø© Ø§Ù„ØªÙŠ ØªØ¯Ø¹Ù… Ù…Ø¹ÙŠØ§Ø± ONNX. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ONNX Runtime ÙƒÙ…Ø§ ÙŠÙ„ÙŠ:\n+\n+```python\n+>>> from transformers import AutoTokenizer\n+>>> from onnxruntime import InferenceSession\n+\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n+>>> session = InferenceSession(\"onnx/model.onnx\")\n+>>> # ÙŠØªÙˆÙ‚Ø¹ ONNX Runtime Ù…ØµÙÙˆÙØ§Øª NumPy ÙƒÙ…Ø¯Ø®Ù„Ø§Øª\n+>>> inputs = tokenizer(\"Using DistilBERT with ONNX Runtime!\", return_tensors=\"np\")\n+>>> outputs = session.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs))\n+```\n+\n+ÙŠÙÙ…ÙƒÙ† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© (Ù…Ø«Ù„ `[\"last_hidden_state\"]`) Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ù„Ù‚Ø§Ø¡ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ ØªÙƒÙˆÙŠÙ† ONNX Ù„ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù€ DistilBERTØŒ Ù„Ø¯ÙŠÙ†Ø§:\n+\n+```python\n+>>> from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig\n+\n+>>> config = DistilBertConfig()\n+>>> onnx_config = DistilBertOnnxConfig(config)\n+>>> print(list(onnx_config.outputs.keys()))\n+[\"last_hidden_state\"]\n+```\n+\n+Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ù…ÙØªØ·Ø§Ø¨Ù‚Ø© Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø­ÙØ¸ TensorFlow Ø¹Ù„Ù‰ Hub. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ØµØ¯Ù‘Ø± Ù†Ù‚Ø·Ø© Ø­ÙØ¸ TensorFlow Ø®Ø§Ù„ØµØ© ÙƒÙ…Ø§ ÙŠÙ„ÙŠ:\n+\n+```bash\n+python -m transformers.onnx --model=keras-io/transformers-qa onnx/\n+```\n+\n+Ù„ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù…ÙØ®Ø²Ù† Ù…Ø­Ù„ÙŠÙ‹Ø§ØŒ Ø§Ø­ÙØ¸ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…Ø¬Ø²Ù‰Ø¡ Ø§Ù„Ù„ØºÙˆÙ‰ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø¯Ù„ÙŠÙ„ (Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ `local-pt-checkpoint`)ØŒ Ø«Ù… Ù‚Ù… Ø¨ØªØµØ¯ÙŠØ±Ù‡ Ø¥Ù„Ù‰ ONNX Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙˆØ¬ÙŠÙ‡ ÙˆØ³ÙŠØ· `--model` Ù„Ø­Ø²Ù…Ø© `transformers.onnx` Ø¥Ù„Ù‰ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:\n+\n+```bash\n+python -m transformers.onnx --model=local-pt-checkpoint onnx/\n+```\n\\ No newline at end of file"
        },
        {
            "sha": "7da7cbf4e1714bb3694f07c0c1307c30110000f1",
            "filename": "docs/source/ar/trainer.md",
            "status": "added",
            "additions": 720,
            "deletions": 0,
            "changes": 720,
            "blob_url": "https://github.com/huggingface/transformers/blob/768f3c016eec88a00f2a991c7017a8a5423c4b06/docs%2Fsource%2Far%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/768f3c016eec88a00f2a991c7017a8a5423c4b06/docs%2Fsource%2Far%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftrainer.md?ref=768f3c016eec88a00f2a991c7017a8a5423c4b06",
            "patch": "@@ -0,0 +1,720 @@\n+# Trainer\n+\n+ØªÙØªÙŠØ­ ÙˆØ­Ø¯Ø© [`Trainer`] Ø­Ù„Ù‚Ø© ØªØ¯Ø±ÙŠØ¨ ÙˆØªÙ‚ÙŠÙŠÙ… Ù…ØªÙƒØ§Ù…Ù„Ø© Ù„Ù†Ù…Ø§Ø°Ø¬ PyTorch Ø§Ù„Ù…Ø·Ø¨Ù‚Ø© ÙÙŠ Ù…ÙƒØªØ¨Ø© Transformers. ØªØ­ØªØ§Ø¬ ÙÙ‚Ø· Ø¥Ù„Ù‰ ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ (Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙˆØ§Ù„Ù…Ø¬Ø²Ù‰Ø¡ Ø§Ù„Ù†ØµÙ‰ØŒ ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ‚ÙŠÙŠÙ…ØŒ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙØ§Ø¦Ù‚Ø©ØŒ Ø¥Ù„Ø®)ØŒ ÙˆØ³ØªØªÙˆÙ„Ù‰ ÙØ¦Ø© [`Trainer`] Ø§Ù„Ø¨Ø§Ù‚ÙŠ. Ù‡Ø°Ø§ ÙŠÙØ³Ù‡Ù‘Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø´ÙƒÙ„ Ø£Ø³Ø±Ø¹ Ø¯ÙˆÙ† ÙƒØªØ§Ø¨Ø© Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ÙŠØ¯ÙˆÙŠÙ‹Ø§. ÙˆÙ„ÙƒÙ† ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ù†ÙØ³Ù‡ØŒ ÙØ¥Ù† [`Trainer`] Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªØ®ØµÙŠØµ Ø¨Ø¯Ø±Ø¬Ø© ÙƒØ¨ÙŠØ±Ø© ÙˆÙŠÙˆÙØ± Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† ØªØ®ØµÙŠØµÙ‡ ÙˆÙÙ‚Ù‹Ø§ Ù„Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¨Ø¯Ù‚Ø©.\n+\n+<Tip>\n+\n+Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ ÙØ¦Ø© [`Trainer`], ØªÙˆÙØ± Ù…ÙƒØªØ¨Ø© Transformers Ø£ÙŠØ¶Ù‹Ø§ ÙØ¦Ø© [`Seq2SeqTrainer`] Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ³Ù„Ø³Ù„ÙŠØ© Ù…Ø«Ù„ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø£Ùˆ Ø§Ù„ØªÙ„Ø®ÙŠØµ. Ù‡Ù†Ø§Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ÙØ¦Ø© [`~trl.SFTTrainer`] Ù…Ù† Ù…ÙƒØªØ¨Ø© [TRL](https://hf.co/docs/trl) Ø§Ù„ØªÙŠ ØªØºÙ„Ù‘Ù ÙØ¦Ø© [`Trainer`] ÙˆÙ‡ÙŠ Ù…ÙØ­ÙØ³ÙÙ‘Ù†Ø© Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ù…Ø«Ù„ Llama-2 ÙˆMistral Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù„ØºÙˆÙŠ. ÙƒÙ…Ø§ ÙŠØ¯Ø¹Ù… [`~trl.SFTTrainer`] Ù…ÙŠØ²Ø§Øª Ù…Ø«Ù„ Ø­Ø²Ù… Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§ØªØŒ ÙˆLoRAØŒ ÙˆØ§Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„ÙƒÙ…ÙŠØŒ ÙˆDeepSpeed Ù…Ù…Ø§ ÙŠÙÙ…ÙƒÙ‘Ù† Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨ÙƒÙØ§Ø¡Ø© Ø¹Ù„Ù‰ Ù†Ù…Ø§Ø°Ø¬ Ø¶Ø®Ù…Ø© Ø§Ù„Ø­Ø¬Ù….\n+\n+<br>\n+\n+Ù„Ø§ ØªØªØ±Ø¯Ø¯ ÙÙŠ Ø§Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ [Ù…Ø±Ø¬Ø¹ API](./main_classes/trainer) Ù„Ù‡Ø°Ù‡ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ Ù…Ù† Ø§Ù„Ù†ÙˆØ¹ [`Trainer`] Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ Ù…ØªÙ‰ ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„ Ù…Ù†Ù‡Ø§. Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù…ØŒ [`Trainer`] Ù‡Ùˆ Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ø£ÙƒØ«Ø± ØªÙ†ÙˆØ¹Ù‹Ø§ ÙˆÙ…Ù†Ø§Ø³Ø¨Ù‹Ø§ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© ÙˆØ§Ø³Ø¹Ø© Ù…Ù† Ø§Ù„Ù…Ù‡Ø§Ù…. ØªÙ… ØªØµÙ…ÙŠÙ… [`Seq2SeqTrainer`] Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ³Ù„Ø³Ù„ÙŠØ© ØŒ Ùˆ [`~trl.SFTTrainer`] Ù…ÙØµÙ…Ù… Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø©.\n+\n+</Tip>\n+\n+Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ù…ÙƒØªØ¨Ø© [Accelerate](https://hf.co/docs/accelerate) - ÙˆÙ‡ÙŠ Ù…ÙƒØªØ¨Ø© ØªÙÙ…ÙƒÙ‘Ù† ØªØ´ØºÙŠÙ„ ØªØ¯Ø±ÙŠØ¨ PyTorch ÙÙŠ Ø¨ÙŠØ¦Ø§Øª Ù…ÙÙˆØ²Ø¹Ø©.\n+\n+```bash\n+pip install accelerate\n+\n+# upgrade\n+pip install accelerate --upgrade\n+```\n+\n+ÙŠÙˆÙØ± Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© Ø¹Ù„Ù‰ ÙØ¦Ø© [`Trainer`].\n+\n+## Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ\n+\n+ÙŠØªØ¶Ù…Ù† [`Trainer`] Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø§Ù„ØªÙŠ Ø³ØªØ¬Ø¯Ù‡Ø§ ÙÙŠ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:\n+\n+1. Ù‚Ù… Ø¨ØªÙ†ÙÙŠØ° Ø®Ø·ÙˆØ© ØªØ¯Ø±ÙŠØ¨ Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø±Ø©\n+2. Ø§Ø­Ø³Ø¨ Ø§Ù„Ù…Ø´ØªÙ‚Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~accelerate.Accelerator.backward`]\n+3. ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´ØªÙ‚Ø§Øª\n+4. ÙƒØ±Ø± Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø­ØªÙ‰ ØªØµÙ„ Ø¥Ù„Ù‰ Ø¹Ø¯Ø¯ Ù…Ø­Ø¯Ø¯ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Ø§Ù„Ø¯ÙˆØ±Ø§Øª (epochs).\n+\n+ØªÙØ¬Ø±Ø¯ ÙØ¦Ø© [`Trainer`] ÙƒÙ„ Ù‡Ø°Ù‡ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø­ØªÙ‰ Ù„Ø§ ØªØ¶Ø·Ø± Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ù„Ù‚ Ø¨Ø´Ø£Ù† ÙƒØªØ§Ø¨Ø© Ø­Ù„Ù‚Ø© ØªØ¯Ø±ÙŠØ¨ ÙŠØ¯ÙˆÙŠÙ‹Ø§ ÙÙŠ ÙƒÙ„ Ù…Ø±Ø© Ø£Ù…Ø§ Ø¥Ø°Ø§ ÙƒÙ†Øª Ø¨Ø¯Ø£Øª Ù„Ù„ØªÙˆ ÙÙŠ PyTorch ÙˆØ§Ù„ØªØ¯Ø±ÙŠØ¨. ÙƒÙ„ Ù…Ø§ Ø¹Ù„ÙŠÙƒ ÙØ¹Ù„Ù‡ Ù‡Ùˆ ØªÙˆÙÙŠØ± Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ù…Ø«Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆØªØªØ¹Ø§Ù…Ù„ ÙØ¦Ø© [`Trainer`] Ù…Ø¹ ÙƒÙ„ Ø´ÙŠØ¡ Ø¢Ø®Ø±.\n+\n+Ø¥Ø°Ø§ ÙƒÙ†Øª ØªÙØ±ÙŠØ¯ ØªØ­Ø¯ÙŠØ¯ Ø£ÙŠ Ø®ÙŠØ§Ø±Ø§Øª ØªØ¯Ø±ÙŠØ¨ Ø£Ùˆ Ù…Ø¹Ù„Ù…Ø§Øª ÙØ§Ø¦Ù‚Ø©ØŒ ÙÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡Ø§ ÙÙŠ ÙØ¦Ø© [`TrainingArguments`]. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¯Ø¹Ù†Ø§ Ù†Ø­Ø¯Ø¯ Ø£ÙŠÙ† ÙŠØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ `output_dir` ÙˆØ±ÙØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Hub Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `push_to_hub=True`.\n+\n+```py\n+from transformers import TrainingArguments\n+\n+training_args = TrainingArguments(\n+    output_dir=\"your-model\"ØŒ\n+    learning_rate=2e-5,\n+    per_device_train_batch_size=16,\n+    per_device_eval_batch_size=16,\n+    num_train_epochs=2,\n+    weight_decay=0.01,\n+    eval_strategy=\"epoch\"ØŒ\n+    save_strategy=\"epoch\"ØŒ\n+    load_best_model_at_end=True,\n+    push_to_hub=True,\n+)\n+```\n+Ù…Ø±Ø± `training_args` Ø¥Ù„Ù‰ [`Trainer`] Ø¬Ù†Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆØ´Ø¦ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø³Ø¨Ù‚Ù‹Ø§ (Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙÙ‚Ø¯ ÙŠÙƒÙˆÙ† Ù…Ø­Ù„Ù„Ù‹Ø§ Ø±Ù…Ø²ÙŠÙ‹Ø§ Ø£Ùˆ Ù…Ø³ØªØ®Ø±Ø¬ Ù…ÙŠØ²Ø§Øª Ø£Ùˆ Ù…Ø¹Ø§Ù„Ø¬ ØµÙˆØ±)ØŒ ÙˆØ¬Ø§Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆØ¯Ø§Ù„Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªÙŠ ØªÙØ±ÙŠØ¯ ØªØªØ¨Ø¹Ù‡Ø§ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨.\n+\n+Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ø§Ø³ØªØ¯Ø¹Ù [`~Trainer.train`] Ù„Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨!\n+\n+```py\n+from transformers import Trainer\n+\n+trainer = Trainer(\n+    model=model,\n+    args=training_args,\n+    train_dataset=dataset[\"train\"]ØŒ\n+    eval_dataset=dataset[\"test\"]ØŒ\n+    tokenizer=tokenizer,\n+    data_collator=data_collator,\n+    compute_metrics=compute_metrics,\n+)\n+\n+trainer.train()\n+```\n+\n+### Ù†Ù‚Ø§Ø· Ø§Ù„Ø­ÙØ¸\n+\n+ØªØ­ÙØ¸ ÙØ¦Ø© [`Trainer`] Ù†Ù‚Ø§Ø· Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø¯Ø¯ ÙÙŠ Ù…Ø¹Ø§Ù…Ù„ `output_dir` Ù…Ù† [`TrainingArguments`]. Ø³ØªØ¬Ø¯ Ù†Ù‚Ø§Ø· Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù…Ø¬Ù„Ø¯ ÙØ±Ø¹ÙŠ ÙŠØ³Ù…Ù‰ `checkpoint-000` Ø­ÙŠØ« ØªØªÙˆØ§ÙÙ‚ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ù…Ø¹ Ø®Ø·ÙˆØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨. Ø¥Ù† Ø­ÙØ¸ Ù†Ù‚Ø§Ø· Ø§Ù„Ø­ÙØ¸ Ù…ÙÙŠØ¯ Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ø§Ø­Ù‚Ù‹Ø§.\n+\n+```py\n+# Ø§Ø³ØªØ£Ù†Ù Ù…Ù† Ø£Ø­Ø¯Ø« Ù†Ù‚Ø·Ø© Ø­ÙØ¸\n+trainer.train(resume_from_checkpoint=True)\n+\n+# Ø§Ø³ØªØ£Ù†Ù Ù…Ù† Ù†Ù‚Ø·Ø© Ø­ÙØ¸ Ù…Ø­Ø¯Ø¯Ø© Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬\n+trainer.train(resume_from_checkpoint=\"your-model/checkpoint-1000\")\n+```\n+\n+ÙŠÙ…ÙƒÙ†Ùƒ Ø­ÙØ¸ Ù†Ù‚Ø§Ø· Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ (Ù„Ø§ ÙŠØªÙ… Ø­ÙØ¸ Ø­Ø§Ù„Ø© Ø§Ù„Ù…ÙØ¬Ø²Ù‰Ø¡ Ø§Ù„Ù„ØºÙˆÙ‰ ØªÙ‚Ø§Ø¦ÙŠÙ‹Ø§)  Ø¥Ù„Ù‰ Hub Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `push_to_hub=True` ÙÙŠ [`TrainingArguments`] Ù„Ø±ÙØ¹Ù‡Ø§. Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ Ù„Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø± Ø¨Ø´Ø£Ù† ÙƒÙŠÙÙŠØ© Ø­ÙØ¸ Ù‡Ø°Ø© Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ ÙÙŠ Ù…Ø¹Ø§Ù…Ù„ [`hub_strategy`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.hub_strategy):\n+\n+* `hub_strategy=\"checkpoint\"` ÙŠØ¯ÙØ¹ Ø£Ø­Ø¯Ø« Ù†Ù‚Ø·Ø© Ø­ÙØ¸ Ø¥Ù„Ù‰ Ù…Ø¬Ù„Ø¯ ÙØ±Ø¹ÙŠ ÙŠØ³Ù…Ù‰ \"last-checkpoint\" ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ¦Ù†Ø§Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù†Ù‡\n+* `hub_strategy=\"all_checkpoints\"` ÙŠØ¯ÙØ¹ Ø¬Ù…ÙŠØ¹ Ù†Ù‚Ø§Ø· Ø§Ù„Ø­ÙØ¸ Ø¥Ù„Ù‰ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø¯Ø¯ ÙÙŠ `output_dir` (Ø³ØªØ±Ù‰ Ù†Ù‚Ø·Ø© Ø­ÙØ¸ ÙˆØ§Ø­Ø¯Ø© Ù„ÙƒÙ„ Ù…Ø¬Ù„Ø¯ ÙÙŠ Ù…Ø³ØªÙˆØ¯Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ)\n+\n+Ø¹Ù†Ø¯ Ø§Ø³ØªØ¦Ù†Ø§Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù† Ù†Ù‚Ø·Ø© Ø­ÙØ¸ØŒ ØªÙØ­Ø§ÙˆÙ„ [`Trainer`] Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø§Øª RNG Python ÙˆNumPy ÙˆPyTorch ÙƒÙ…Ø§ ÙƒØ§Ù†Øª Ø¹Ù†Ø¯Ù…Ø§ ØªÙ… Ø­ÙØ¸ Ù†Ù‚Ø·Ø© Ø§Ù„Ø­ÙØ¸. ÙˆÙ„ÙƒÙ† Ù„Ø£Ù† PyTorch Ù„Ø¯ÙŠÙ‡Ø§ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ØºÙŠØ± Ø§Ù„Ø­ØªÙ…ÙŠØ© Ù…ÙØªÙ†ÙˆØ¹Ø©ØŒ ÙØ¥Ù† Ø­Ø§Ù„Ø§Øª RNG Ù„ÙŠØ³Øª Ù…Ø¶Ù…ÙˆÙ†Ø© Ù„ØªÙƒÙˆÙ† Ù‡ÙŠ Ù†ÙØ³Ù‡Ø§. Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ ØªÙ…ÙƒÙŠÙ† Ø§Ù„Ø­ØªÙ…ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø©ØŒ ÙØ±Ø§Ø¬Ø¹ Ø¯Ù„ÙŠÙ„ [Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©](https://pytorch.org/docs/stable/notes/randomness#controlling-sources-of-randomness) Ù„Ù…Ø¹Ø±ÙØ© Ù…Ø§ ÙŠÙÙ…ÙƒÙ†Ùƒ ØªÙ…ÙƒÙŠÙ†Ù‡ Ù„Ø¬Ø¹Ù„ ØªØ¯Ø±ÙŠØ¨Ùƒ Ø­ØªÙ…ÙŠÙ‹Ø§ ØªÙ…Ø§Ù…Ù‹Ø§. Ø¶Ø¹ ÙÙŠ Ø§Ø¹ØªØ¨Ø§Ø±Ùƒ Ø£Ù†Ù‡ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¬Ø¹Ù„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø¹ÙŠÙ†Ø© Ø­ØªÙ…ÙŠØ©ØŒ ÙÙ‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø£Ø¨Ø·Ø£.\n+\n+## ØªØ®ØµÙŠØµ Ø§Ù„Ù…Ø¯Ø±Ø¨\n+\n+ÙÙŠ Ø­ÙŠÙ† Ø£Ù† ÙØ¦Ø© [`Trainer`] Ù…ÙØµÙ…Ù…Ø© Ù„ØªÙƒÙˆÙ† Ø³Ù‡Ù„Ø© Ø§Ù„ÙˆØµÙˆÙ„ ÙˆØ³Ù‡Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…ØŒ ÙØ¥Ù†Ù‡Ø§ ØªÙˆÙØ± Ø£ÙŠØ¶Ù‹Ø§ Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªØ®ØµÙŠØµ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ù…ØºØ§Ù…Ø±ÙŠÙ†.  ÙŠÙÙ…ÙƒÙ† Ø¥Ù†Ø´Ø§Ø¡ ÙØ¦Ø§Øª ÙØ±Ø¹ÙŠØ© Ù…Ù† Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø£Ø³Ø§Ù„ÙŠØ¨ [`Trainer`] ÙˆØªØ¬Ø§ÙˆØ²Ù‡Ø§ Ù„Ø¯Ø¹Ù… Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„ØªÙŠ ØªÙØ±ÙŠØ¯Ù‡Ø§ØŒ Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ù„Ø§Ø³ØªÙŠØ¹Ø§Ø¨Ù‡Ø§. ØªØªØ¶Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø³Ø§Ù„ÙŠØ¨:\n+\n+* [`~Trainer.get_train_dataloader`] ÙŠÙ†Ø´Ø¦ DataLoader Ù„Ù„ØªØ¯Ø±ÙŠØ¨\n+* [`~Trainer.get_eval_dataloader`] ÙŠÙ†Ø´Ø¦ DataLoader Ù„Ù„ØªÙ‚ÙŠÙŠÙ…\n+* [`~Trainer.get_test_dataloader`] ÙŠÙ†Ø´Ø¦ DataLoader Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±\n+* [`~Trainer.log`] ÙŠØ³Ø¬Ù„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­ÙˆÙ„ Ù…Ø®ØªÙ„Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„ØªÙŠ ØªØ±Ø§Ù‚Ø¨ Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n+* [`~Trainer.create_optimizer_and_scheduler`] ÙŠÙ†Ø´Ø¦ Ù…Ø­Ø³Ù†Ù‹Ø§ ÙˆÙ…Ø®Ø·Ø·Ù‹Ø§ Ù„Ù…ÙØ¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªÙ…Ø±ÙŠØ±Ù‡Ù…Ø§ ÙÙŠ `__init__`Ø› ÙŠÙ…ÙƒÙ† Ø£ÙŠØ¶Ù‹Ø§ ØªØ®ØµÙŠØµ Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø¨Ø´ÙƒÙ„ Ù…Ù†ÙØµÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~Trainer.create_optimizer`] Ùˆ [`~Trainer.create_scheduler`] Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ§Ù„ÙŠ\n+* [`~Trainer.compute_loss`] ÙŠØ­Ø³Ø¨ Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø© Ù…Ù† Ù…ÙØ¯Ø®Ù„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n+* [`~Trainer.training_step`] ÙŠÙÙ†ÙØ° Ø®Ø·ÙˆØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n+* [`~Trainer.prediction_step`] ÙŠÙÙ†ÙØ° Ø®Ø·ÙˆØ© Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø±\n+* [`~Trainer.evaluate`] ÙŠÙÙ‚ÙŠÙ‘Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙŠØ¹ÙŠØ¯ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n+* [`~Trainer.predict`] ÙŠÙØ¬Ø±ÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª (Ù…Ø¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ù…ØªØ§Ø­Ø©) Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±\n+\n+Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ ØªØ®ØµÙŠØµ Ø·Ø±ÙŠÙ‚Ø© [`~Trainer.compute_loss`] Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø°Ø§Øª ØªØ±Ø¬ÙŠØ­ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø°Ù„Ùƒ.\n+\n+\n+```py\n+from torch import nn\n+from transformers import Trainer\n+\n+class CustomTrainer(Trainer):\n+    def compute_loss(self, model, inputs, return_outputs=False):\n+        labels = inputs.pop(\"labels\")\n+        # forward pass\n+        outputs = model(**inputs)\n+        logits = outputs.get(\"logits\")\n+        # compute custom loss for 3 labels with different weights\n+        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))\n+        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n+        return (loss, outputs) if return_outputs else loss\n+```\n+\n+### Ø¯ÙˆØ§Ù„ Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Callbacks\n+\n+Ø®ÙŠØ§Ø± Ø¢Ø®Ø± Ù„ØªØ®ØµÙŠØµ [`Trainer`] Ù‡Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… [Ø¯ÙˆØ§Ù„ Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡](callbacks). Ù„Ø§ *ØªØºÙŠØ±* Ø¯ÙˆØ§Ù„ Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø£ÙŠ Ø´ÙŠØ¡ ÙÙŠ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨. Ø¥Ù†Ù‡Ù… ØªÙØ­Øµ Ø­Ø§Ù„Ø© Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø«Ù… ØªÙÙ†ÙØ° Ø¨Ø¹Ø¶ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª (Ù…Ø«Ù„ Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø¨ÙƒØ± Ø£Ùˆ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ØŒ Ø¥Ù„Ø®) Ø§Ø¹ØªÙ…Ø§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø§Ù„Ø©. ÙˆØ¨Ø¹Ø¨Ø§Ø±Ø© Ø£Ø®Ø±Ù‰ØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù„ØªÙ†ÙÙŠØ° Ø´ÙŠØ¡ Ù…Ø«Ù„ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ù…Ø®ØµØµØ©ØŒ ÙˆÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªØ¬Ø§ÙˆØ² Ø¯Ø§Ù„Ø© [`~Trainer.compute_loss`] Ù„Ø°Ù„Ùƒ.\n+\n+Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ Ø¥Ø¶Ø§ÙØ© Ø¯Ø§Ù„Ø© Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¥ÙŠÙ‚Ø§Ù Ù…Ø¨ÙƒØ± Ø¥Ù„Ù‰ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø¹Ø¯ 10 Ø®Ø·ÙˆØ§Øª.\n+\n+```py\n+from transformers import TrainerCallback\n+\n+class EarlyStoppingCallback(TrainerCallback):\n+    def __init__(self, num_steps=10):\n+        self.num_steps = num_steps\n+    \n+    def on_step_end(self, args, state, control, **kwargs):\n+        if state.global_step >= self.num_steps:\n+            return {\"should_training_stop\": True}\n+        else:\n+            return {}\n+```\n+\n+Ø«Ù… Ù…Ø±Ø±Ù‡ Ø¥Ù„Ù‰ Ù…Ø¹Ø§Ù…Ù„ `callback` ÙÙŠ [`Trainer`].\n+\n+```py\n+from transformers import Trainer\n+\n+trainer = Trainer(\n+    model=model,\n+    args=training_args,\n+    train_dataset=dataset[\"train\"]ØŒ\n+    eval_dataset=dataset[\"test\"]ØŒ\n+    tokenizer=tokenizer,\n+    data_collator=data_collator,\n+    compute_metrics=compute_metrics,\n+    callback=[EarlyStoppingCallback()],\n+)\n+```\n+\n+## ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« (Logging)\n+\n+<Tip>\n+\n+Ø±Ø§Ø¬Ø¹ Ù…Ø±Ø¬Ø¹ [API](./main_classes/logging) Ù„Ù„ØªØ³Ø¬ÙŠÙ„ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­ÙˆÙ„ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ù„Ù„Ø£Ø­Ø¯Ø§Ø«.\n+\n+</Tip>\n+\n+ÙŠØªÙ… ØªØ¹ÙŠÙŠÙ† [`Trainer`] Ø¥Ù„Ù‰ `logging.INFO` Ø§ÙØªØ±Ø§Ø¶ÙŠÙ‹Ø§ ÙˆØ§Ù„Ø°ÙŠ ÙŠÙØ¨Ù„Øº Ø¹Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª ÙˆÙ…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ© Ø£Ø®Ø±Ù‰. ÙŠØªÙ… ØªØ¹ÙŠÙŠÙ† Ù†Ø³Ø®Ø© [`Trainer`] - ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø§Øª Ø§Ù„Ù…ÙˆØ²Ø¹Ø© - Ø¥Ù„Ù‰ `logging.WARNING` ÙˆØ§Ù„ØªÙŠ ÙŠÙØ¨Ù„Øº ÙÙ‚Ø· Ø¹Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª. ÙŠÙ…ÙƒÙ†Ùƒ ØªØºÙŠÙŠØ± Ù…Ø³ØªÙˆÙ‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù…Ù„ÙŠ [`log_level`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.log_level) Ùˆ [`log_level_replica`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.log_level_replica) ÙÙŠ [`TrainingArguments`].\n+\n+Ù„ØªÙ‡ÙŠØ¦Ø© Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ÙØ³ØªÙˆÙ‰ ØªØ³Ø¬ÙŠÙ„  Ø§ï»·Ø­Ø¯Ø§Ø« Ù„ÙƒÙ„ Ø¹Ù‚Ø¯Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ø§Ù…Ù„ [`log_on_each_node`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments.log_on_each_node) Ù„ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙØ³ØªÙˆÙ‰ Ø§Ù„Ø³Ø¬Ù„ Ø¹Ù„Ù‰ ÙƒÙ„ Ø¹Ù‚Ø¯Ø© Ø£Ùˆ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù‚Ø¯Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©.\n+\n+<Tip>\n+\n+ÙŠØ­Ø¯Ø¯ [`Trainer`] Ù…ÙØ³ØªÙˆÙ‰ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø¨Ø´ÙƒÙ„ Ù…ÙÙ†ÙØµÙ„ Ù„ÙƒÙ„ Ø¹Ù‚Ø¯Ø© ÙÙŠ Ø·Ø±ÙŠÙ‚Ø© [`Trainer.__init__`]ØŒ Ù„Ø°Ø§ ÙÙ‚Ø¯ ØªØ±ØºØ¨ ÙÙŠ Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ ØªØ¹ÙŠÙŠÙ† Ù‡Ø°Ø§ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ ÙÙŠ ÙˆÙ‚Øª Ø³Ø§Ø¨Ù‚ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ³ØªØ®Ø¯Ù… ÙˆØ¸Ø§Ø¦Ù Transformers Ø§Ù„Ø£Ø®Ø±Ù‰ Ù‚Ø¨Ù„ Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† [`Trainer`].\n+\n+</Tip>\n+\n+Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù„ØªØ¹ÙŠÙŠÙ† Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© ÙˆØ§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù†Ù…Ø·ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ÙØ³ Ù…ÙØ³ØªÙˆÙ‰ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ÙˆÙÙ‚Ù‹Ø§ Ù„ÙƒÙ„ Ø¹Ù‚Ø¯Ø©:\n+\n+```py\n+logger = logging.getLogger(__name__)\n+\n+logging.basicConfig(\n+    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\"ØŒ\n+    datefmt=\"%m/%d/%Y %H:%M:%S\"ØŒ\n+    handlers=[logging.StreamHandler(sys.stdout)],\n+)\n+\n+log_level = training_args.get_process_log_level()\n+logger.setLevel(log_level)\n+datasets.utils.logging.set_verbosity(log_level)\n+transformers.utils.logging.set_verbosity(log_level)\n+\n+trainer = Trainer(...)\n+```\n+\n+Ø§Ø³ØªØ®Ø¯Ù… ØªØ±ÙƒÙŠØ¨Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ù…Ù† `log_level` Ùˆ `log_level_replica` Ù„ØªÙ‡ÙŠØ¦Ø© Ù…Ø§ ÙŠØªÙ… ØªØ³Ø¬ÙŠÙ„Ù‡ Ø¹Ù„Ù‰ ÙƒÙ„ Ù…Ù† Ø§Ù„Ø¹Ù‚Ø¯.\n+\n+\n+<hfoptions id=\"logging\">\n+<hfoption id=\"single node\">\n+\n+```bash\n+my_app.py ... --log_level warning --log_level_replica error\n+```\n+\n+</hfoption>\n+<hfoption id=\"multi-node\">\n+\n+Ø£Ø¶Ù Ù…Ø¹Ù„Ù…Ø© `log_on_each_node 0` Ù„Ø¨ÙŠØ¦Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø¹Ù‚Ø¯.\n+\n+```bash\n+my_app.py ... --log_level warning --log_level_replica error --log_on_each_node 0\n+\n+# set to only report errors\n+my_app.py ... --log_level error --log_level_replica error --log_on_each_node 0\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## NEFTune\n+\n+[NEFTune](https://hf.co/papers/2310.05914) Ù‡ÙŠ ØªÙ‚Ù†ÙŠØ© ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ­Ø³Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¥Ø¶Ø§ÙØ© Ø¶ÙˆØ¶Ø§Ø¡ Ø¥Ù„Ù‰ Ù…ÙØªØ¬Ù‡Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨. Ù„ØªÙ…ÙƒÙŠÙ†Ù‡ ÙÙŠ [`Trainer`], Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† Ù…Ø¹Ø§Ù…Ù„ `neftune_noise_alpha` ÙÙŠ [`TrainingArguments`] Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ù…Ù‚Ø¯Ø§Ø± Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ Ø§Ù„Ù…ÙØ¶Ø§ÙØ©.\n+\n+```py\n+from transformers import TrainingArguments, Trainer\n+\n+training_args = TrainingArguments(..., neftune_noise_alpha=0.1)\n+trainer = Trainer(..., args=training_args)\n+```\n+\n+ÙŠØªÙ… ØªØ¹Ø·ÙŠÙ„ NEFTune Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„ØªØ¬Ù†Ø¨ Ø£ÙŠ Ø³Ù„ÙˆÙƒ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹.\n+\n+## Ù†ÙˆØ§Ø© Liger\n+[Liger-Kernel](https://github.com/linkedin/Liger-Kernel) Kernel Ù‡ÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ù†ÙˆÙ‰ Triton Ø§Ù„ØªÙŠ Ø·ÙˆØ±ØªÙ‡Ø§ Linkedin Ù…ÙØµÙ…Ù…Ø© Ø®ØµÙŠØµÙ‹Ø§ Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø© (LLM). Ù„Ù‚Ø¯ Ù‚Ù…Ù†Ø§ Ø¨ØªÙ†ÙÙŠØ° RMSNorm Ùˆ RoPE Ùˆ SwiGLU Ùˆ CrossEntropy Ùˆ FusedLinearCrossEntropy Ù…ÙØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Hugging FaceØŒ ÙˆØ§Ù„Ù…Ø²ÙŠØ¯ Ù‚Ø§Ø¯Ù…. ÙŠÙÙ…ÙƒÙ†Ù‡Ø§ Ø²ÙŠØ§Ø¯Ø© Ø¥Ù†ØªØ§Ø¬ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…ØªØ¹Ø¯Ø¯ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU) Ø¨Ù†Ø³Ø¨Ø© 20Ùª ÙˆØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ù†Ø³Ø¨Ø© 60Ùª. ØªØ¹Ù…Ù„ Ø§Ù„Ù†ÙˆØ§Ø© Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…Ø¹ flash attention Ùˆ PyTorch FSDP Ùˆ Microsoft DeepSpeed.\n+\n+Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© Ø¨Ù†Ø³Ø¨Ø© 20Ùª ÙˆØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ù†Ø³Ø¨Ø© 60Ùª Ø¹Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ LLaMA 3-8B. Ø­Ù‚Ù‚ Ø£Ø·ÙˆØ§Ù„ Ø³ÙŠØ§Ù‚ Ø£ÙƒØ¨Ø± ÙˆØ£Ø­Ø¬Ø§Ù… Ø¯ÙØ¹Ø§Øª Ø£ÙƒØ¨Ø±. ÙƒÙ…Ø§ Ø£Ù†Ù‡Ø§ Ù…ÙÙÙŠØ¯Ø© Ø¥Ø°Ø§ ÙƒÙ†Øª ØªÙØ±ÙŠØ¯ Ø²ÙŠØ§Ø¯Ø© Ø­Ø¬Ù… Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø±Ø¤ÙˆØ³ Ø£Ùˆ Ø£Ø­Ø¬Ø§Ù… Ù…ÙÙØ±Ø¯Ø§Øª Ø¶Ø®Ù…Ø©. Ø£Ø·Ù„Ù‚ Ø§Ù„Ø¹Ù†Ø§Ù† Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø±Ø¤ÙˆØ³ (medusa) ÙˆØ§Ù„Ù…Ø²ÙŠØ¯. Ø±Ø§Ø¬Ø¹ Ø§Ù„ØªÙØ§ØµÙŠÙ„ ÙˆØ§Ù„Ø£Ù…Ø«Ù„Ø© ÙÙŠ [Liger](https://github.com/linkedin/Liger-Kernel/tree/main/examples)\n+ØªØ£ÙƒØ¯ Ø£ÙˆÙ„Ø§Ù‹ Ù…Ù† ØªØ«Ø¨ÙŠØª Ù…Ø³ØªÙˆØ¯Ø¹ Liger Ø§Ù„Ø±Ø³Ù…ÙŠ:\n+```bash\n+pip install liger-kernel\n+```\n+ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªÙ…Ø±ÙŠØ± `use_liger_kernel=True` Ù„ØªØ·Ø¨ÙŠÙ‚ Ù†ÙˆØ§Ø© `liger` Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ÙƒØŒ Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„:\n+\n+```python\n+from transformers import TrainingArguments\n+\n+training_args = TrainingArguments(\n+    output_dir=\"your-model\",\n+    learning_rate=2e-5,\n+    per_device_train_batch_size=16,\n+    per_device_eval_batch_size=16,\n+    num_train_epochs=2,\n+    weight_decay=0.01,\n+    eval_strategy=\"epoch\",\n+    save_strategy=\"epoch\",\n+    load_best_model_at_end=True,\n+    push_to_hub=True,\n+    use_liger_kernel=True\n+)\n+```\n+\n+ØªØ¯Ø¹Ù… Ø§Ù„Ù†ÙˆØ§Ø© Ù…Ø¹Ù…Ø§Ø±ÙŠØ§Øª Ù†Ù…Ø§Ø°Ø¬ Llama Ùˆ Gemma Ùˆ Mistral Ùˆ Mixtral. ÙŠÙÙ…ÙƒÙ† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø­Ø¯Ø« Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ù†Ù…Ø§Ø¦Ø¬ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© [Ù‡Ù†Ø§](https://github.com/linkedin/Liger-Kernel). Ø¹Ù†Ø¯Ù…Ø§ ÙŠØªÙ… ØªØ¹ÙŠÙŠÙ† `use_liger_kernel` Ø¥Ù„Ù‰ `True`ØŒ Ø³ÙŠØªÙ… ØªØµØ­ÙŠØ­ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù…ÙÙ‚Ø§Ø¨Ù„Ø© ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ØµÙ„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ·Ø¨ÙŠÙ‚ Liger Ø§Ù„ÙØ¹Ø§Ù„ØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ÙØ¹Ù„ Ø£ÙŠ Ø´ÙŠØ¡ Ø¥Ø¶Ø§ÙÙŠ Ø¨Ø®Ù„Ø§Ù ØªØ¹ÙŠÙŠÙ† Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø¹Ø§Ù…Ù„.\n+\n+## Ø§Ù„Ù…ÙØ­Ø³Ù‘ÙÙ†Ø§Øª\n+ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø®ØªÙŠØ§Ø± Ù…ÙØ­Ø³Ù‘ÙÙ† Ù…Ø¯Ù…Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…:\n+```python\n+from transformers import TrainingArguments\n+training_args = TrainingArguments(..., optim=\"adamw_torch\")\n+```\n+Ø§Ø·Ù„Ø¹ Ø¹Ù„Ù‰ [`OptimizerNames`](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py) Ù„Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ù„Ø®ÙŠØ§Ø±Ø§Øª. Ù†ÙØ¯Ø±Ø¬ Ø£Ù…Ø«Ù„Ø© Ù…ÙØªÙ‚Ø¯Ù…Ø© ÙÙŠ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø£Ø¯Ù†Ø§Ù‡.\n+\n+ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙØ­Ø³Ù‘ÙÙ† PyTorch Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¹Ø¨Ø±:\n+```python\n+import torch\n+\n+optimizer_cls = torch.optim.AdamW\n+optimizer_kwargs = {\n+    \"lr\": 4e-3,\n+    \"betas\": (0.9, 0.999),\n+    \"weight_decay\": 0.05,\n+}\n+\n+from transformers import Trainer\n+trainer = Trainer(..., optimizer_cls_and_kwargs=(optimizer_cls, optimizer_kwargs))\n+```\n+\n+\n+\n+\n+### GaLore\n+\n+Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„ØªØ¯Ø±Ø¬ Ø°Ùˆ Ø§Ù„Ø±ØªØ¨Ø© Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø© (GaLore) Ù‡Ùˆ Ø¥Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ØªØ¯Ø±ÙŠØ¨ Ø°Ø§Øª Ø±ØªØ¨Ø© Ù…Ù†Ø®ÙØ¶Ø© ÙØ¹Ù‘Ø§Ù„Ø© Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ø°Ø§ÙƒØ±Ø©ØŒ ØªØ³Ù…Ø­ Ø¨ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø© ÙˆÙ„ÙƒÙ†Ù‡Ø§ Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ù† Ø£Ø³Ø§Ù„ÙŠØ¨ Ø§Ù„ØªÙƒÙŠÙ‘Ù Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ø°Ø§Øª Ø§Ù„Ø±ØªØ¨Ø© Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø©ØŒ Ù…Ø«Ù„ LoRA.\n+\n+Ø£ÙˆÙ„Ø§Ù‹ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø§Ù„Ø±Ø³Ù…ÙŠ Ù„Ù€ GaLore:\n+\n+```bash\n+pip install galore-torch\n+```\n+\n+Ø«Ù… Ø£Ø¶Ù Ø¨Ø¨Ø³Ø§Ø·Ø© Ø£Ø­Ø¯ `[\"galore_adamw\"ØŒ \"galore_adafactor\"ØŒ \"galore_adamw_8bit\"]` ÙÙŠ `optim` Ø¬Ù†Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ Ù…Ø¹ `optim_target_modules`ØŒ ÙˆØ§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ø£Ùˆ Ø§Ù„ØªØ¹Ø¨ÙŠØ±Ø§Øª Ø§Ù„Ù†Ù…Ø·ÙŠØ© regex Ø£Ùˆ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚ Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ© Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ ØªÙƒÙŠÙŠÙÙ‡Ø§. ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ ÙƒØ§Ù…Ù„(ØªØ£ÙƒØ¯ Ù…Ù† `pip install trl datasets`):\n+\n+```python\n+import torch\n+import datasets\n+import trl\n+\n+from transformers import TrainingArguments, AutoConfig, AutoTokenizer, AutoModelForCausalLM\n+\n+train_dataset = datasets.load_dataset('imdb', split='train')\n+\n+args = TrainingArguments(\n+    output_dir=\"./test-galore\"ØŒ\n+    max_steps=100,\n+    per_device_train_batch_size=2,\n+    optim=\"galore_adamw\"ØŒ\n+    optim_target_modules=[r\".*.attn.*\"ØŒ r\".*.mlp.*\"]\n+)\n+\n+model_id = \"google/gemma-2b\"\n+\n+config = AutoConfig.from_pretrained(model_id)\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+model = AutoModelForCausalLM.from_config(config).to(0)\n+\n+trainer = trl.SFTTrainer(\n+    model=model, \n+    args=args,\n+    train_dataset=train_dataset,\n+    dataset_text_field='text',\n+    max_seq_length=512,\n+)\n+\n+trainer.train()\n+```\n+\n+Ù„ØªÙ…Ø±ÙŠØ± Ù…Ø¹Ø§Ù…ï»»Øª Ø¥Ø¶Ø§ÙÙŠØ© ÙŠØ¯Ø¹Ù…Ù‡Ø§  GaLoreØŒ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªÙ…Ø±ÙŠØ± `optim_args` Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ØŒ Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„:\n+\n+```python\n+import torch\n+import datasets\n+import trl\n+\n+from transformers import TrainingArguments, AutoConfig, AutoTokenizer, AutoModelForCausalLM\n+\n+train_dataset = datasets.load_dataset('imdb', split='train')\n+\n+args = TrainingArguments(\n+    output_dir=\"./test-galore\",\n+    max_steps=100,\n+    per_device_train_batch_size=2,\n+    optim=\"galore_adamw\",\n+    optim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"],\n+    optim_args=\"rank=64, update_proj_gap=100, scale=0.10\",\n+)\n+\n+model_id = \"google/gemma-2b\"\n+\n+config = AutoConfig.from_pretrained(model_id)\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+model = AutoModelForCausalLM.from_config(config).to(0)\n+\n+trainer = trl.SFTTrainer(\n+    model=model, \n+    args=args,\n+    train_dataset=train_dataset,\n+    dataset_text_field='text',\n+    max_seq_length=512,\n+)\n+\n+trainer.train()\n+```\n+ÙŠÙ…ÙƒÙ†Ùƒ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© ÙÙŠ [Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø§Ù„Ø£ØµÙ„ÙŠ](https://github.com/jiaweizzhao/GaLore) Ø£Ùˆ [Ø§Ù„ÙˆØ±Ù‚Ø© Ø§Ù„Ø¨Ø­Ø«ÙŠØ©](https://arxiv.org/abs/2403.03507).\n+\n+Ø­Ø§Ù„ÙŠÙ‹Ø§ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ÙÙ‚Ø· ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø®Ø·ÙŠØ© Ø§Ù„ØªÙŠ ØªØ¹ØªØ¨Ø± Ø·Ø¨Ù‚Ø§Øª GaLore ÙˆØ³ØªØ³ØªØ®Ø¯Ù… Ø§Ù„ØªØ­Ù„Ù„  Ø°Ùˆ Ø§Ù„Ø±ØªØ¨Ø© Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨ÙŠÙ†Ù…Ø§ Ø³ÙŠØªÙ… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© Ø¨Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©.\n+\n+Ù„Ø§Ø­Ø¸ Ø£Ù†Ù‡ Ø³ÙŠØ³ØªØºØ±Ù‚ Ø§Ù„Ø£Ù…Ø± Ø¨Ø¹Ø¶ Ø§Ù„ÙˆÙ‚Øª Ù‚Ø¨Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (~3 Ø¯Ù‚Ø§Ø¦Ù‚ Ù„Ù†Ù…ÙˆØ°Ø¬ 2B Ø¹Ù„Ù‰ NVIDIA A100)ØŒ ÙˆÙ„ÙƒÙ† ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ³ÙŠØ± Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø³Ù„Ø§Ø³Ø© Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ.\n+\n+ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ø¬Ø±Ø§Ø¡ ØªØ­Ø³ÙŠÙ† Ø·Ø¨Ù‚Ø© ØªÙ„Ùˆ Ø§Ù„Ø£Ø®Ø±Ù‰ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¥Ø¶Ø§ÙØ© `layerwise` Ø¥Ù„Ù‰ Ø§Ø³Ù… Ø§Ù„Ù…ÙØ­Ø³Ù‘ÙÙ† ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ Ø£Ø¯Ù†Ø§Ù‡:\n+\n+```python\n+import torch\n+import datasets\n+import trl\n+\n+from transformers import TrainingArgumentsØŒ AutoConfigØŒ AutoTokenizerØŒ AutoModelForCausalLM\n+\n+train_dataset = datasets.load_dataset('imdb'ØŒ split='train')\n+\n+args = TrainingArguments(\n+    output_dir=\"./test-galore\"ØŒ\n+    max_steps=100ØŒ\n+    per_device_train_batch_size=2ØŒ\n+    optim=\"galore_adamw_layerwise\"ØŒ\n+    optim_target_modules=[r\".*.attn.*\"ØŒ r\".*.mlp.*\"]\n+)\n+\n+model_id = \"google/gemma-2b\"\n+\n+config = AutoConfig.from_pretrained(model_id)\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+model = AutoModelForCausalLM.from_config(config).to(0)\n+\n+trainer = trl.SFTTrainer(\n+    model=modelØŒ\n+    args=argsØŒ\n+    train_dataset=train_datasetØŒ\n+    dataset_text_field='text'ØŒ\n+    max_seq_length=512ØŒ\n+)\n+\n+trainer.train()\n+```\n+\n+Ù„Ø§Ø­Ø¸ Ø£Ù† ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø·Ø¨Ù‚Ø© ØªØ¬Ø±ÙŠØ¨ÙŠ Ø¥Ù„Ù‰ Ø­Ø¯ Ù…Ø§ ÙˆÙ„Ø§ ÙŠØ¯Ø¹Ù… DDP (Distributed Data Parallel)ØŒ ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©  Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU) ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ [Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ù…Ù†Ø§Ø³Ø¨](https://github.com/jiaweizzhao/GaLore?tab=readme-ov-file#train-7b-model-with-a-single-gpu-with-24gb-memory) Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„. Ù‚Ø¯ Ù„Ø§ ØªØ¯Ø¹Ù… Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ Ù…Ø«Ù„ ØªÙ‚Ù„ÙŠÙ… Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ø£Ùˆ DeepSpeedØŒ Ø¥Ù„Ø®. Ù…Ù† Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚. ÙŠØ±Ø¬Ù‰ [ØªÙ‚Ø¯ÙŠÙ… ØªÙ‚Ø±ÙŠØ± Ø¹Ù† Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø¹Ù„Ù‰ GitHub](https://github.com/huggingface/transformers/issues) Ø¥Ø°Ø§ ÙˆØ§Ø¬Ù‡ØªÙƒ Ù…Ø«Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©.\n+\n+### Ù…Ø­Ø³Ù†Ø§Øª LOMO\n+\n+ØªÙ… ØªÙ‚Ø¯ÙŠÙ… Ù…ÙØ­Ø³Ù‘ÙÙ†Ø§Øª LOMO ÙÙŠ [Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙˆØ§Ø±Ø¯ Ù…Ø­Ø¯ÙˆØ¯Ø©](https://hf.co/papers/2306.09782) Ùˆ [AdaLomo: ØªØ­Ø³ÙŠÙ† Ø°Ø§ÙƒØ±Ø© Ù…Ù†Ø®ÙØ¶Ø© Ø¨Ù…Ø¹Ø¯Ù„ ØªØ¹Ù„Ù… Ù…ØªÙƒÙŠÙ](https://hf.co/papers/2310.10195).\n+ÙŠØªÙƒÙˆÙ† ÙƒÙ„Ø§Ù‡Ù…Ø§ Ù…Ù† Ø·Ø±ÙŠÙ‚Ø© ÙØ¹Ø§Ù„Ø© Ù„Ø¶Ø¨Ø· Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©. ØªØ¯Ù…Ø¬ Ù…Ø­Ø³Ù†Ø§Øª LOMO Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚ ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª ÙÙŠ Ø®Ø·ÙˆØ© ÙˆØ§Ø­Ø¯Ø© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©. Ù…Ø­Ø³Ù†Ø§Øª LOMO Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© Ù‡ÙŠ `\"lomo\"` Ùˆ `\"adalomo\"`. Ø£ÙˆÙ„Ø§Ù‹ Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª LOMO Ù…Ù† pypi `pip install lomo-optim` Ø£Ùˆ Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØªÙ‡ Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `pip install git+https://github.com/OpenLMLab/LOMO.git`.\n+\n+<Tip>\n+\n+ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„Ù…Ø¤Ù„ÙÙŠÙ†ØŒ ÙŠÙˆØµÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `AdaLomo` Ø¨Ø¯ÙˆÙ† `grad_norm` Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡ Ø£ÙØ¶Ù„ ÙˆØ³Ø±Ø¹Ø© Ø£Ø¹Ù„Ù‰.\n+\n+</Tip>\n+\n+ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ù†Øµ Ø¨Ø±Ù…Ø¬ÙŠ Ø¨Ø³ÙŠØ· ÙŠÙˆØ¶Ø­ ÙƒÙŠÙÙŠØ© Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ [google/gemma-2b](https://huggingface.co/google/gemma-2b) Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª IMDB ÙÙŠ Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©:\n+\n+```python\n+import torch\n+import datasets\n+from transformers import TrainingArgumentsØŒ AutoTokenizerØŒ AutoModelForCausalLM\n+import trl\n+\n+train_dataset = datasets.load_dataset('imdb'ØŒ split='train')\n+\n+args = TrainingArguments(\n+    output_dir=\"./test-lomo\"ØŒ\n+    max_steps=100ØŒ\n+    per_device_train_batch_size=4ØŒ\n+    optim=\"adalomo\"ØŒ\n+    gradient_checkpointing=TrueØŒ\n+    logging_strategy=\"steps\"ØŒ\n+    logging_steps=1ØŒ\n+    learning_rate=2e-6ØŒ\n+    save_strategy=\"no\"ØŒ\n+    run_name=\"lomo-imdb\"ØŒ\n+)\n+\n+model_id = \"google/gemma-2b\"\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+model = AutoModelForCausalLM.from_pretrained(model_idØŒ low_cpu_mem_usage=True).to(0)\n+\n+trainer = trl.SFTTrainer(\n+    model=modelØŒ\n+    args=argsØŒ\n+    train_dataset=train_datasetØŒ\n+    dataset_text_field='text'ØŒ\n+    max_seq_length=1024ØŒ\n+)\n+\n+trainer.train()\n+```\n+\n+### Ù…ÙØ­Ø³Ù‘ÙÙ† GrokAdamW\n+ØªÙ… ØªØµÙ…ÙŠÙ… Ù…ÙØ­Ø³Ù‘ÙÙ† GrokAdamW Ù„ØªØ¹Ø²ÙŠØ² Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø³ØªÙ‚Ø±Ø§Ø±Ù‡ØŒ Ø®Ø§ØµØ©Ù‹ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙŠ ØªØ³ØªÙÙŠØ¯ Ù…Ù† Ø¯ÙˆØ§Ù„ Ø¥Ø´Ø§Ø±Ø© `grokking`. Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… `GrokAdamW`ØŒ Ù‚Ù… Ø£ÙˆÙ„Ø§Ù‹ Ø¨ØªØ«Ø¨ÙŠØª Ø­Ø²Ù…Ø© Ø§Ù„Ù…ÙØ­Ø³Ù‘ÙÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `pip install grokadamw`.\n+<Tip>\n+ÙŠÙØ¹Ø¯ GrokAdamW Ù…ÙÙŠØ¯Ù‹Ø§ Ø¨Ø´ÙƒÙ„ Ø®Ø§Øµ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙŠ ØªØªØ·Ù„Ø¨ ØªÙ‚Ù†ÙŠØ§Øª ØªØ­Ø³ÙŠÙ† Ù…ÙØªÙ‚Ø¯Ù…Ø© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ø¯Ø§Ø¡ ÙˆØ§Ø³ØªÙ‚Ø±Ø§Ø± Ø£ÙØ¶Ù„.\n+</Tip>\n+\n+ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ù†Øµ Ø¨Ø±Ù…Ø¬Ù‰ Ø¨Ø³ÙŠØ· Ù„Ø´Ø±Ø­ ÙƒÙŠÙÙŠØ© Ø¶Ø¨Ø· [google/gemma-2b](https://huggingface.co/google/gemma-2b) Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª IMDB Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙØ­Ø³Ù‘ÙÙ† GrokAdamW:\n+```python\n+import torch\n+import datasets\n+from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM, Trainer\n+\n+# ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª IMDB\n+train_dataset = datasets.load_dataset('imdb', split='train')\n+\n+# ØªØ¹Ø±ÙŠÙ Ù…Ø¹Ø§Ù…ï»»Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n+args = TrainingArguments(\n+    output_dir=\"./test-grokadamw\",\n+    max_steps=1000,\n+    per_device_train_batch_size=4,\n+    optim=\"grokadamw\",\n+    logging_strategy=\"steps\",\n+    logging_steps=1,\n+    learning_rate=2e-5,\n+    save_strategy=\"no\",\n+    run_name=\"grokadamw-imdb\",\n+)\n+\n+# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø¬Ø²Ù‰Ø¡ Ø§Ù„Ù„ØºÙˆÙŠ\n+model_id = \"google/gemma-2b\"\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True).to(0)\n+\n+# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø¯Ø±Ø¨\n+trainer = Trainer(\n+    model=model,\n+    args=args,\n+    train_dataset=train_dataset,\n+)\n+\n+# ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n+trainer.train()\n+```\n+ÙŠÙˆØ¶Ø­ Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬Ù‰ ÙƒÙŠÙÙŠØ© Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ google/gemma-2b Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª IMDB Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙØ­Ø³Ù‘ÙÙ† GrokAdamW. ÙŠØªÙ… ØªÙƒÙˆÙŠÙ† TrainingArguments Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… GrokAdamWØŒ ÙˆÙŠØªÙ… ØªÙ…Ø±ÙŠØ± Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Trainer Ù„Ù„ØªØ¯Ø±ÙŠØ¨.\n+\n+### Ù…ÙØ­Ø³Ù‘ÙÙ† Ø¨Ø¯ÙˆÙ† Ø¬Ø¯ÙˆÙ„Ù‡ (Schedule Free Optimizer)\n+ØªÙ… ØªÙ‚Ø¯ÙŠÙ… Ù…ÙØ­Ø³Ù‘ÙÙ†Ø§Øª Ø¨Ø¯ÙˆÙ† Ø¬Ø¯ÙˆÙ„Ù‡ ÙÙŠ [The Road Less Scheduled](https://hf.co/papers/2405.15682).\n+ÙŠØ³ØªØ¨Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø¨Ø¯ÙˆÙ† Ø¬Ø¯ÙˆÙ„Ù‡ Ø²Ø®Ù… Ø§Ù„Ù…ÙØ­Ø³Ù‘ÙÙ† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø¨Ù…Ø²ÙŠØ¬ Ù…Ù† Ø§Ù„Ù…ØªÙˆØ³Ø· â€‹â€‹ÙˆØ§Ù„ØªØ¯Ø§Ø®Ù„ØŒ Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ø¬Ø© ØªÙ…Ø§Ù…Ù‹Ø§ Ø¥Ù„Ù‰ ØªØ®ÙÙŠÙ Ù…ÙØ¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ø¯ÙˆÙ„Ù‡ ØªÙ‚Ù„ÙŠØ¯ÙŠÙ‡.\n+Ø§Ù„Ù…ÙØ­Ø³Ù‘ÙÙ†Ø§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© Ù„Ù€ SFO Ù‡ÙŠ \"schedule_free_adamw\" Ùˆ \"schedule_free_sgd\". Ù‚Ù… Ø£ÙˆÙ„Ø§Ù‹ Ø¨ØªØ«Ø¨ÙŠØª `schedulefree` Ù…Ù† pypi Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ù…Ø±  `pip install schedulefree`.\n+\n+ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ù†Øµ Ø¨Ø±Ù…Ø¬Ù‰ Ø¨Ø³ÙŠØ· Ù„Ø´Ø±Ø­ ÙƒÙŠÙÙŠØ© Ø¶Ø¨Ø· [google/gemma-2b](https://huggingface.co/google/gemma-2b) Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª IMDB Ø¨Ø¯Ù‚Ø© ÙƒØ§Ù…Ù„Ø©:\n+```python\n+import torch\n+import datasets\n+from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n+import trl\n+\n+train_dataset = datasets.load_dataset('imdb', split='train')\n+\n+args = TrainingArguments(\n+    output_dir=\"./test-schedulefree\",\n+    max_steps=1000,\n+    per_device_train_batch_size=4,\n+    optim=\"schedule_free_adamw\",\n+    gradient_checkpointing=True,\n+    logging_strategy=\"steps\",\n+    logging_steps=1,\n+    learning_rate=2e-6,\n+    save_strategy=\"no\",\n+    run_name=\"sfo-imdb\",\n+)\n+\n+model_id = \"google/gemma-2b\"\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True).to(0)\n+\n+trainer = trl.SFTTrainer(\n+    model=model, \n+    args=args,\n+    train_dataset=train_dataset,\n+    dataset_text_field='text',\n+    max_seq_length=1024,\n+)\n+\n+trainer.train()\n+```\n+## ØªØ³Ø±ÙŠØ¹ ÙˆÙ…Ø¯Ø±Ø¨\n+\n+ÙŠØªÙ… ØªØ´ØºÙŠÙ„ ÙØ¦Ø© [`Trainer`] Ø¨ÙˆØ§Ø³Ø·Ø© [ØªØ³Ø±ÙŠØ¹](https://hf.co/docs/accelerate)ØŒ ÙˆÙ‡ÙŠ Ù…ÙƒØªØ¨Ø© Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ PyTorch Ø¨Ø³Ù‡ÙˆÙ„Ø© ÙÙŠ Ø¨ÙŠØ¦Ø§Øª Ù…ÙˆØ²Ø¹Ø© Ù…Ø¹ Ø¯Ø¹Ù… Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø«Ù„ [FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) Ùˆ [DeepSpeed](https://www.deepspeed.ai/).\n+\n+<Tip>\n+\n+ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªØ¬Ø²Ø¦Ø© FSDPØŒ ÙˆØªÙØ±ÙŠØº ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© (CPU)ØŒ ÙˆØ§Ù„Ù…Ø²ÙŠØ¯ Ù…Ø¹ [`Trainer`] ÙÙŠ [Ø¯Ù„ÙŠÙ„ Fully Sharded Data Parallel](fsdp).\n+\n+</Tip>\n+\n+Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Accelerate Ù…Ø¹ [`Trainer`]]ØŒ Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ù…Ø± [`accelerate.config`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config) Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ. Ù†Ø´Ø¦ Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø± Ù…Ù„Ù `config_file.yaml` Ø§Ù„Ø°ÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ø¹Ù†Ø¯ ØªØ´ØºÙŠÙ„ Ù†Øµ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¨Ø±Ù…Ø¬Ù‰. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¨Ø¹Ø¶ ØªÙƒÙˆÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø¯Ø§Ø¯Ù‡Ø§ Ù‡ÙŠ:\n+\n+<hfoptions id=\"config\">\n+<hfoption id=\"DistributedDataParallel\">\n+\n+```yml\n+compute_environment: LOCAL_MACHINE                                                                                             \n+distributed_type: MULTI_GPU                                                                                                    \n+downcast_bf16: 'no'\n+gpu_ids: all\n+machine_rank: 0 #change rank as per the node\n+main_process_ip: 192.168.20.1\n+main_process_port: 9898\n+main_training_function: main\n+mixed_precision: fp16\n+num_machines: 2\n+num_processes: 8\n+rdzv_backend: static\n+same_network: true\n+tpu_env: []\n+tpu_use_cluster: false\n+tpu_use_sudo: false\n+use_cpu: false\n+```\n+\n+</hfoption>\n+<hfoption id=\"FSDP\">\n+\n+```yml\n+compute_environment: LOCAL_MACHINE\n+distributed_type: FSDP\n+downcast_bf16: 'no'\n+fsdp_config:\n+  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n+  fsdp_backward_prefetch_policy: BACKWARD_PRE\n+  fsdp_forward_prefetch: true\n+  fsdp_offload_params: false\n+  fsdp_sharding_strategy: 1\n+  fsdp_state_dict_type: FULL_STATE_DICT\n+  fsdp_sync_module_states: true\n+  fsdp_transformer_layer_cls_to_wrap: BertLayer\n+  fsdp_use_orig_params: true\n+machine_rank: 0\n+main_training_function: main\n+mixed_precision: bf16\n+num_machines: 1\n+num_processes: 2\n+rdzv_backend: static\n+same_network: true\n+tpu_env: []\n+tpu_use_cluster: false\n+tpu_use_sudo: false\n+use_cpu: false\n+```\n+\n+</hfoption>\n+<hfoption id=\"DeepSpeed\">\n+\n+```yml\n+compute_environment: LOCAL_MACHINE\n+deepspeed_config:\n+  deepspeed_config_file: /home/user/configs/ds_zero3_config.json\n+  zero3_init_flag: true\n+distributed_type: DEEPSPEED\n+downcast_bf16: 'no'\n+machine_rank: 0\n+main_training_function: main\n+num_machines: 1\n+num_processes: 4\n+rdzv_backend: static\n+same_network: true\n+tpu_env: []\n+tpu_use_cluster: false\n+tpu_use_sudo: false\n+use_cpu: false\n+```\n+\n+</hfoption>\n+<hfoption id=\"DeepSpeed with Accelerate plugin\">\n+\n+```yml\n+compute_environment: LOCAL_MACHINE                                                                                             \n+deepspeed_config:                                                                                                              \n+  gradient_accumulation_steps: 1\n+  gradient_clipping: 0.7\n+  offload_optimizer_device: cpu\n+  offload_param_device: cpu\n+  zero3_init_flag: true\n+  zero_stage: 2\n+distributed_type: DEEPSPEED\n+downcast_bf16: 'no'\n+machine_rank: 0\n+main_training_function: main\n+mixed_precision: bf16\n+num_machines: 1\n+num_processes: 4\n+rdzv_backend: static\n+same_network: true\n+tpu_env: []\n+tpu_use_cluster: false\n+tpu_use_sudo: false\n+use_cpu: false\n+```\n+\n+</hfoption>\n+</hfoptions>\n+ÙŠÙØ¹Ø¯ Ø£Ù…Ø±  [`accelerate_launch`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-launch) Ù‡Ùˆ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…ÙÙˆØµÙ‰ Ø¨Ù‡Ø§ Ù„ØªØ´ØºÙŠÙ„ Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬Ù‰ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ù†Ø¸Ø§Ù… Ù…ÙˆØ²Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Accelerate Ùˆ [`Trainer`] Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙÙŠ `config_file.yaml`. ÙŠØªÙ… Ø­ÙØ¸ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù€ Accelerate ÙˆÙŠØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¹Ù†Ø¯ ØªØ´ØºÙŠÙ„ `accelerate_launch`.\n+\n+Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ÙŠ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ [run_glue.py](https://github.com/huggingface/transformers/blob/f4db565b695582891e43a5e042e5d318e28f20b8/examples/pytorch/text-classification/run_glue.py#L4) Ù…Ø¹ ØªÙƒÙˆÙŠÙ† FSDP:\n+\n+```bash\n+accelerate launch \\\n+    ./examples/pytorch/text-classification/run_glue.py \\\n+    --model_name_or_path google-bert/bert-base-cased \\\n+    --task_name $TASK_NAME \\\n+    --do_train \\\n+    --do_eval \\\n+    --max_seq_length 128 \\\n+    --per_device_train_batch_size 16 \\\n+    --learning_rate 5e-5 \\\n+    --num_train_epochs 3 \\\n+    --output_dir /tmp/$TASK_NAME/ \\\n+    --overwrite_output_dir\n+```\n+\n+ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ù…Ù† Ù…Ù„Ù `config_file.yaml` Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±:\n+\n+```bash\n+accelerate launch --num_processes=2 \\\n+    --use_fsdp \\\n+    --mixed_precision=bf16 \\\n+    --fsdp_auto_wrap_policy=TRANSFORMER_BASED_WRAP  \\\n+    --fsdp_transformer_layer_cls_to_wrap=\"BertLayer\" \\\n+    --fsdp_sharding_strategy=1 \\\n+    --fsdp_state_dict_type=FULL_STATE_DICT \\\n+    ./examples/pytorch/text-classification/run_glue.py\n+    --model_name_or_path google-bert/bert-base-cased \\\n+    --task_name $TASK_NAME \\\n+    --do_train \\\n+    --do_eval \\\n+    --max_seq_length 128 \\\n+    --per_device_train_batch_size 16 \\\n+    --learning_rate 5e-5 \\\n+    --num_train_epochs 3 \\\n+    --output_dir /tmp/$TASK_NAME/ \\\n+    --overwrite_output_dir\n+```\n+\n+Ø§Ø·Ù„Ø¹ Ø¹Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªØ¹Ù„ÙŠÙ…ÙŠ [Launching your Accelerate scripts](https://huggingface.co/docs/accelerate/basic_tutorials/launch) Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ `accelerate_launch` ÙˆØ§Ù„ØªÙƒÙˆÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø®ØµØµØ©."
        }
    ],
    "stats": {
        "total": 910,
        "additions": 904,
        "deletions": 6
    }
}