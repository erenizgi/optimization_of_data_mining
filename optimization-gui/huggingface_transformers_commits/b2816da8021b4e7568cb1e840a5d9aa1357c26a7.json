{
    "author": "yao-matrix",
    "message": "fix xpu failures on PT 2.7 and 2.8 w/o IPEX and enable hqq cases on XPU (#39187)\n\n* chameleon xpu bnb groundtruth update on bnb triton backend since we are\ndeprecating ipex backend\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* enable hqq uts on XPU, all passed\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix comment\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>",
    "sha": "b2816da8021b4e7568cb1e840a5d9aa1357c26a7",
    "files": [
        {
            "sha": "160a689d80c168b3b13c45f49f547632cd16f582",
            "filename": "src/transformers/quantizers/quantizer_hqq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2816da8021b4e7568cb1e840a5d9aa1357c26a7/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2816da8021b4e7568cb1e840a5d9aa1357c26a7/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py?ref=b2816da8021b4e7568cb1e840a5d9aa1357c26a7",
            "patch": "@@ -15,7 +15,7 @@\n from typing import TYPE_CHECKING, Any\n \n from ..integrations import prepare_for_hqq_linear\n-from ..utils import is_accelerate_available, is_hqq_available, is_torch_available, logging\n+from ..utils import is_accelerate_available, is_hqq_available, is_torch_available, is_torch_xpu_available, logging\n from .base import HfQuantizer\n from .quantizers_utils import get_module_from_name\n \n@@ -71,8 +71,8 @@ def validate_environment(self, *args, **kwargs):\n                 \" sure the weights are in PyTorch format.\"\n             )\n \n-        if not torch.cuda.is_available():\n-            raise RuntimeError(\"No GPU found. A GPU is needed for quantization.\")\n+        if not (torch.cuda.is_available() or is_torch_xpu_available()):\n+            raise RuntimeError(\"No GPU or XPU found. A GPU or XPU is needed for quantization.\")\n \n         if self.torch_dtype is None:\n             if \"torch_dtype\" in kwargs:"
        },
        {
            "sha": "67baab37c0990fd8f94ba2002674a5bbc16de6ff",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2816da8021b4e7568cb1e840a5d9aa1357c26a7/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2816da8021b4e7568cb1e840a5d9aa1357c26a7/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=b2816da8021b4e7568cb1e840a5d9aa1357c26a7",
            "patch": "@@ -416,7 +416,7 @@ def test_model_7b_batched(self):\n         EXPECTED_TEXT_COMPLETIONS = Expectations(\n             {\n                 (\"xpu\", 3): [\n-                    'Describe what do you see here and tell me about the history behind it?The image depicts a star map, with a bright blue dot in the center representing the star Altair. The star map is set against a black background, with the constellations visible in the night',\n+                    'Describe what do you see here and tell me about the history behind it?The image depicts a star map, with a bright blue dot in the center representing the star Alpha Centauri. The star map is a representation of the night sky, showing the positions of stars in',\n                     'What constellation is this image showing?The image shows the constellation of Orion.The image shows the constellation of Orion.The image shows the constellation of Orion.The image shows the constellation of Orion.',\n                 ],\n                 (\"cuda\", 7): ["
        },
        {
            "sha": "a3aae71552ff6d4688831a63c772df525ddd5292",
            "filename": "tests/quantization/hqq/test_hqq.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2816da8021b4e7568cb1e840a5d9aa1357c26a7/tests%2Fquantization%2Fhqq%2Ftest_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2816da8021b4e7568cb1e840a5d9aa1357c26a7/tests%2Fquantization%2Fhqq%2Ftest_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fhqq%2Ftest_hqq.py?ref=b2816da8021b4e7568cb1e840a5d9aa1357c26a7",
            "patch": "@@ -21,9 +21,10 @@\n from transformers.testing_utils import (\n     backend_empty_cache,\n     require_accelerate,\n+    require_deterministic_for_xpu,\n     require_hqq,\n-    require_torch_gpu,\n-    require_torch_multi_gpu,\n+    require_torch_accelerator,\n+    require_torch_multi_accelerator,\n     slow,\n     torch_device,\n )\n@@ -87,7 +88,7 @@ def check_forward(test_module, model, batch_size=1, context_size=1024):\n MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n @require_hqq\n class HqqConfigTest(unittest.TestCase):\n     def test_to_dict(self):\n@@ -101,7 +102,7 @@ def test_to_dict(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n @require_accelerate\n @require_hqq\n class HQQTest(unittest.TestCase):\n@@ -131,7 +132,6 @@ def test_quantized_model_to_new_device_and_new_dtype(self):\n             model_id=MODEL_ID, quant_config=quant_config, compute_dtype=torch.float16, device=torch_device\n         )\n \n-        original_device = hqq_runner.model.model.layers[0].self_attn.v_proj.device\n         check_hqqlayer(self, hqq_runner.model.model.layers[0].self_attn.v_proj)\n         check_forward(self, hqq_runner.model)\n \n@@ -142,7 +142,7 @@ def test_quantized_model_to_new_device_and_new_dtype(self):\n         check_hqqlayer(self, hqq_runner.model.model.layers[0].self_attn.v_proj)\n         check_forward(self, hqq_runner.model)\n \n-        hqq_runner.model.cuda(original_device)\n+        hqq_runner.model.to(torch_device)\n         check_hqqlayer(self, hqq_runner.model.model.layers[0].self_attn.v_proj)\n         check_forward(self, hqq_runner.model)\n \n@@ -158,8 +158,8 @@ def test_quantized_model_fake_weight_dtype(self):\n \n \n @slow\n-@require_torch_gpu\n-@require_torch_multi_gpu\n+@require_torch_accelerator\n+@require_torch_multi_accelerator\n @require_accelerate\n @require_hqq\n class HQQTestMultiGPU(unittest.TestCase):\n@@ -182,7 +182,7 @@ def test_fp16_quantized_model_multipgpu(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n @require_accelerate\n @require_hqq\n class HQQTestBias(unittest.TestCase):\n@@ -202,6 +202,7 @@ def test_fp16_quantized_model(self):\n         check_hqqlayer(self, hqq_runner.model.model.decoder.layers[0].self_attn.v_proj)\n         check_forward(self, hqq_runner.model)\n \n+    @require_deterministic_for_xpu\n     def test_save_and_load_quantized_model(self):\n         \"\"\"\n         Test saving and loading a quantized model with bias\n@@ -237,7 +238,7 @@ def test_save_and_load_quantized_model(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n @require_accelerate\n @require_hqq\n class HQQSerializationTest(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 29,
        "additions": 15,
        "deletions": 14
    }
}