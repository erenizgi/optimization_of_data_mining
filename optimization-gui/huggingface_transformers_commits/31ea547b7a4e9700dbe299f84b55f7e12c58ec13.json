{
    "author": "cyr0930",
    "message": "[fix] make legacy bnb code work (#37331)\n\n* [fix] make legacy bnb code work\n\n* [fix] use get with default instead of getter\n\n* add test for bnb 8bit optim skip embed\n\n* [fix] style\n\n* add require annotation of bnb\n\n---------\n\nCo-authored-by: jaycha <jaycha@ncsoft.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "31ea547b7a4e9700dbe299f84b55f7e12c58ec13",
    "files": [
        {
            "sha": "d6445340206eb24437d0fe329b116c7f838626e1",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31ea547b7a4e9700dbe299f84b55f7e12c58ec13/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31ea547b7a4e9700dbe299f84b55f7e12c58ec13/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=31ea547b7a4e9700dbe299f84b55f7e12c58ec13",
            "patch": "@@ -1247,7 +1247,7 @@ def create_optimizer(self):\n \n             self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)\n \n-            if optimizer_cls.__name__ == \"Adam8bit\":\n+            if \"bitsandbytes\" in str(optimizer_cls) and optimizer_kwargs.get(\"optim_bits\", None) == 8:\n                 import bitsandbytes\n \n                 manager = bitsandbytes.optim.GlobalOptimManager.get_instance()"
        },
        {
            "sha": "0eb2ba69891f16aba956c7a0a8a3d28903bc660f",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/31ea547b7a4e9700dbe299f84b55f7e12c58ec13/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31ea547b7a4e9700dbe299f84b55f7e12c58ec13/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=31ea547b7a4e9700dbe299f84b55f7e12c58ec13",
            "patch": "@@ -5962,3 +5962,22 @@ def test_get_optimizer_group(self):\n             param = next(model.parameters())\n             group = trainer.get_optimizer_group(param)\n             self.assertIn(param, group[\"params\"])\n+\n+    @require_bitsandbytes\n+    def test_bnb_8bit_optimizer_skip_embedding(self):\n+        model = BasicTextGenerationModel(8, 4)\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            for name_optim in [\"rmsprop_bnb_8bit\", \"adamw_8bit\"]:\n+                args = TrainingArguments(\n+                    output_dir=tmp_dir,\n+                    report_to=\"none\",\n+                    optim=name_optim,\n+                )\n+                trainer = Trainer(model=model, args=args)\n+                optimizer = trainer.create_optimizer()\n+                modules = optimizer.mng.module_weight_config_triple\n+                self.assertNotEqual(len(modules), 0)\n+                module, name, config = modules[0]\n+                self.assertIsInstance(module, torch.nn.Embedding)\n+                self.assertEqual(name, \"weight\")\n+                self.assertDictEqual(config, {\"optim_bits\": 32})"
        }
    ],
    "stats": {
        "total": 21,
        "additions": 20,
        "deletions": 1
    }
}