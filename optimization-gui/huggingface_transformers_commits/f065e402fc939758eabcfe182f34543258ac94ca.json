{
    "author": "yonigozlan",
    "message": "[v5] ðŸš¨Refactor subprocessors handling in processors (#41633)\n\n* remove attributes and add all missing sub processors to their auto classes\n\n* remove all mentions of .attributes\n\n* cleanup\n\n* fix processor tests\n\n* fix modular\n\n* remove last attributes\n\n* fixup\n\n* fixes after merge\n\n* fix wrong tokenizer in auto florence2\n\n* fix missing audio_processor + nits\n\n* Override __init__ in NewProcessor and change hf-internal-testing-repo (temporarily)\n\n* fix auto tokenizer test\n\n* add init to markup_lm\n\n* update CustomProcessor in custom_processing\n\n* remove print\n\n* nit\n\n* fix test modeling owlv2\n\n* fix test_processing_layoutxlm\n\n* Fix owlv2, wav2vec2, markuplm, voxtral issues\n\n* add support for loading and saving multiple tokenizer natively\n\n* remove exclude_attributes from save_pretrained\n\n* modifs after review",
    "sha": "f065e402fc939758eabcfe182f34543258ac94ca",
    "files": [
        {
            "sha": "ac927b8d23063f154000da36a41fa6c8ede0efcd",
            "filename": "src/transformers/models/align/processing_align.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -59,9 +59,6 @@ class AlignProcessor(ProcessorMixin):\n \n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"EfficientNetImageProcessor\"\n-    tokenizer_class = (\"BertTokenizer\", \"BertTokenizerFast\")\n     valid_processor_kwargs = AlignProcessorKwargs\n \n     def __init__(self, image_processor, tokenizer):"
        },
        {
            "sha": "933a5e48dfeda795928355528d5d8258c8ebdeaa",
            "filename": "src/transformers/models/altclip/processing_altclip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -35,10 +35,6 @@ class AltCLIPProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")\n-    tokenizer_class = (\"XLMRobertaTokenizer\", \"XLMRobertaTokenizerFast\")\n-\n     @deprecate_kwarg(old_name=\"feature_extractor\", version=\"5.0.0\", new_name=\"image_processor\")\n     def __init__(self, image_processor=None, tokenizer=None):\n         super().__init__(image_processor, tokenizer)"
        },
        {
            "sha": "4d471fe40f6a29db6e4fee5b1049a428f93b21ba",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -906,10 +906,6 @@ class AriaProcessor(ProcessorMixin):\n             A dictionary indicating size conversions for images.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AriaImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "c29c289649da842654b5cfaa39d6d381a31dd99b",
            "filename": "src/transformers/models/aria/processing_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -67,10 +67,6 @@ class AriaProcessor(ProcessorMixin):\n             A dictionary indicating size conversions for images.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AriaImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "b9a16ae4272eea7b1a5b430cdcf90bfa1bfe0c09",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -223,6 +223,7 @@\n         (\"layoutlm\", \"LayoutLMConfig\"),\n         (\"layoutlmv2\", \"LayoutLMv2Config\"),\n         (\"layoutlmv3\", \"LayoutLMv3Config\"),\n+        (\"layoutxlm\", \"LayoutLMv2Config\"),\n         (\"led\", \"LEDConfig\"),\n         (\"levit\", \"LevitConfig\"),\n         (\"lfm2\", \"Lfm2Config\"),"
        },
        {
            "sha": "7d3b9df512fee7aaa0fa7f5714d1c1d154760c60",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -41,6 +41,7 @@\n         (\"audio-spectrogram-transformer\", \"ASTFeatureExtractor\"),\n         (\"clap\", \"ClapFeatureExtractor\"),\n         (\"clvp\", \"ClvpFeatureExtractor\"),\n+        (\"csm\", \"EncodecFeatureExtractor\"),\n         (\"dac\", \"DacFeatureExtractor\"),\n         (\"data2vec-audio\", \"Wav2Vec2FeatureExtractor\"),\n         (\"dia\", \"DiaFeatureExtractor\"),\n@@ -49,14 +50,20 @@\n         (\"granite_speech\", \"GraniteSpeechFeatureExtractor\"),\n         (\"hubert\", \"Wav2Vec2FeatureExtractor\"),\n         (\"kyutai_speech_to_text\", \"KyutaiSpeechToTextFeatureExtractor\"),\n+        (\"markuplm\", \"MarkupLMFeatureExtractor\"),\n         (\"mctct\", \"MCTCTFeatureExtractor\"),\n         (\"mimi\", \"EncodecFeatureExtractor\"),\n         (\"moonshine\", \"Wav2Vec2FeatureExtractor\"),\n         (\"moshi\", \"EncodecFeatureExtractor\"),\n+        (\"musicgen\", \"EncodecFeatureExtractor\"),\n+        (\"musicgen_melody\", \"MusicgenMelodyFeatureExtractor\"),\n         (\"parakeet_ctc\", \"ParakeetFeatureExtractor\"),\n         (\"parakeet_encoder\", \"ParakeetFeatureExtractor\"),\n         (\"phi4_multimodal\", \"Phi4MultimodalFeatureExtractor\"),\n         (\"pop2piano\", \"Pop2PianoFeatureExtractor\"),\n+        (\"qwen2_5_omni\", \"WhisperFeatureExtractor\"),\n+        (\"qwen2_audio\", \"WhisperFeatureExtractor\"),\n+        (\"qwen3_omni_moe\", \"WhisperFeatureExtractor\"),\n         (\"seamless_m4t\", \"SeamlessM4TFeatureExtractor\"),\n         (\"seamless_m4t_v2\", \"SeamlessM4TFeatureExtractor\"),\n         (\"sew\", \"Wav2Vec2FeatureExtractor\"),\n@@ -66,6 +73,7 @@\n         (\"unispeech\", \"Wav2Vec2FeatureExtractor\"),\n         (\"unispeech-sat\", \"Wav2Vec2FeatureExtractor\"),\n         (\"univnet\", \"UnivNetFeatureExtractor\"),\n+        (\"voxtral\", \"WhisperFeatureExtractor\"),\n         (\"wav2vec2\", \"Wav2Vec2FeatureExtractor\"),\n         (\"wav2vec2-bert\", \"Wav2Vec2FeatureExtractor\"),\n         (\"wav2vec2-conformer\", \"Wav2Vec2FeatureExtractor\"),"
        },
        {
            "sha": "5ed14d4f3b1d10a1ec7d9e6e96212267b499305c",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -62,7 +62,9 @@\n             (\"aimv2\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"aimv2_vision_model\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"align\", (\"EfficientNetImageProcessor\", \"EfficientNetImageProcessorFast\")),\n+            (\"altclip\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"aria\", (\"AriaImageProcessor\", None)),\n+            (\"aya_vision\", (\"GotOcr2ImageProcessor\", \"GotOcr2ImageProcessorFast\")),\n             (\"beit\", (\"BeitImageProcessor\", \"BeitImageProcessorFast\")),\n             (\"bit\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n             (\"blip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n@@ -73,6 +75,8 @@\n             (\"clip\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"clipseg\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"cohere2_vision\", (None, \"Cohere2VisionImageProcessorFast\")),\n+            (\"colpali\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n+            (\"colqwen2\", (\"Qwen2VLImageProcessor\", \"Qwen2VLImageProcessorFast\")),\n             (\"conditional_detr\", (\"ConditionalDetrImageProcessor\", \"ConditionalDetrImageProcessorFast\")),\n             (\"convnext\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"convnextv2\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n@@ -95,8 +99,10 @@\n             (\"efficientformer\", (\"EfficientFormerImageProcessor\", None)),\n             (\"efficientloftr\", (\"EfficientLoFTRImageProcessor\", \"EfficientLoFTRImageProcessorFast\")),\n             (\"efficientnet\", (\"EfficientNetImageProcessor\", \"EfficientNetImageProcessorFast\")),\n+            (\"emu3\", (\"Emu3ImageProcessor\", None)),\n             (\"eomt\", (\"EomtImageProcessor\", \"EomtImageProcessorFast\")),\n             (\"flava\", (\"FlavaImageProcessor\", \"FlavaImageProcessorFast\")),\n+            (\"florence2\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"focalnet\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n             (\"fuyu\", (\"FuyuImageProcessor\", \"FuyuImageProcessorFast\")),\n             (\"gemma3\", (\"Gemma3ImageProcessor\", \"Gemma3ImageProcessorFast\")),\n@@ -114,11 +120,13 @@\n             (\"ijepa\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"imagegpt\", (\"ImageGPTImageProcessor\", \"ImageGPTImageProcessorFast\")),\n             (\"instructblip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n+            (\"internvl\", (\"GotOcr2ImageProcessor\", \"GotOcr2ImageProcessorFast\")),\n             (\"janus\", (\"JanusImageProcessor\", \"JanusImageProcessorFast\")),\n             (\"kosmos-2\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"kosmos-2.5\", (\"Kosmos2_5ImageProcessor\", \"Kosmos2_5ImageProcessorFast\")),\n             (\"layoutlmv2\", (\"LayoutLMv2ImageProcessor\", \"LayoutLMv2ImageProcessorFast\")),\n             (\"layoutlmv3\", (\"LayoutLMv3ImageProcessor\", \"LayoutLMv3ImageProcessorFast\")),\n+            (\"layoutxlm\", (\"LayoutLMv2ImageProcessor\", \"LayoutLMv2ImageProcessor\")),\n             (\"levit\", (\"LevitImageProcessor\", \"LevitImageProcessorFast\")),\n             (\"lfm2_vl\", (None, \"Lfm2VlImageProcessorFast\")),\n             (\"lightglue\", (\"LightGlueImageProcessor\", \"LightGlueImageProcessorFast\")),\n@@ -141,6 +149,7 @@\n             (\"mobilevitv2\", (\"MobileViTImageProcessor\", \"MobileViTImageProcessorFast\")),\n             (\"nat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"nougat\", (\"NougatImageProcessor\", \"NougatImageProcessorFast\")),\n+            (\"omdet-turbo\", (\"DetrImageProcessor\", \"DetrImageProcessorFast\")),\n             (\"oneformer\", (\"OneFormerImageProcessor\", \"OneFormerImageProcessorFast\")),\n             (\"ovis2\", (\"Ovis2ImageProcessor\", \"Ovis2ImageProcessorFast\")),\n             (\"owlv2\", (\"Owlv2ImageProcessor\", \"Owlv2ImageProcessorFast\")),\n@@ -155,14 +164,17 @@\n             (\"prompt_depth_anything\", (\"PromptDepthAnythingImageProcessor\", \"PromptDepthAnythingImageProcessorFast\")),\n             (\"pvt\", (\"PvtImageProcessor\", \"PvtImageProcessorFast\")),\n             (\"pvt_v2\", (\"PvtImageProcessor\", \"PvtImageProcessorFast\")),\n+            (\"qwen2_5_omni\", (\"Qwen2VLImageProcessor\", \"Qwen2VLImageProcessorFast\")),\n             (\"qwen2_5_vl\", (\"Qwen2VLImageProcessor\", \"Qwen2VLImageProcessorFast\")),\n             (\"qwen2_vl\", (\"Qwen2VLImageProcessor\", \"Qwen2VLImageProcessorFast\")),\n+            (\"qwen3_omni_moe\", (\"Qwen2VLImageProcessor\", \"Qwen2VLImageProcessorFast\")),\n             (\"qwen3_vl\", (\"Qwen2VLImageProcessor\", \"Qwen2VLImageProcessorFast\")),\n             (\"regnet\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"resnet\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"rt_detr\", (\"RTDetrImageProcessor\", \"RTDetrImageProcessorFast\")),\n             (\"sam\", (\"SamImageProcessor\", \"SamImageProcessorFast\")),\n             (\"sam2\", (None, \"Sam2ImageProcessorFast\")),\n+            (\"sam2_video\", (None, \"Sam2ImageProcessorFast\")),\n             (\"sam_hq\", (\"SamImageProcessor\", \"SamImageProcessorFast\")),\n             (\"segformer\", (\"SegformerImageProcessor\", \"SegformerImageProcessorFast\")),\n             (\"seggpt\", (\"SegGptImageProcessor\", None)),\n@@ -180,12 +192,14 @@\n             (\"textnet\", (\"TextNetImageProcessor\", \"TextNetImageProcessorFast\")),\n             (\"timesformer\", (\"VideoMAEImageProcessor\", None)),\n             (\"timm_wrapper\", (\"TimmWrapperImageProcessor\", None)),\n+            (\"trocr\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"tvlt\", (\"TvltImageProcessor\", None)),\n             (\"tvp\", (\"TvpImageProcessor\", \"TvpImageProcessorFast\")),\n             (\"udop\", (\"LayoutLMv3ImageProcessor\", \"LayoutLMv3ImageProcessorFast\")),\n             (\"upernet\", (\"SegformerImageProcessor\", \"SegformerImageProcessorFast\")),\n             (\"van\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"video_llama_3\", (\"VideoLlama3ImageProcessor\", \"VideoLlama3ImageProcessorFast\")),\n+            (\"video_llava\", (\"VideoLlavaImageProcessor\", None)),\n             (\"videomae\", (\"VideoMAEImageProcessor\", None)),\n             (\"vilt\", (\"ViltImageProcessor\", \"ViltImageProcessorFast\")),\n             (\"vipllava\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),"
        },
        {
            "sha": "aaea1522a9c059c6ac12bce708e9312d70e3fc16",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -107,6 +107,7 @@\n         (\"mllama\", \"MllamaProcessor\"),\n         (\"mm-grounding-dino\", \"GroundingDinoProcessor\"),\n         (\"moonshine\", \"Wav2Vec2Processor\"),\n+        (\"omdet-turbo\", \"OmDetTurboProcessor\"),\n         (\"oneformer\", \"OneFormerProcessor\"),\n         (\"ovis2\", \"Ovis2Processor\"),\n         (\"owlv2\", \"Owlv2Processor\"),"
        },
        {
            "sha": "65a3885a1c46bf6f95589a0acd353a0b2822a985",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 27,
            "deletions": 1,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -72,6 +72,7 @@\n             ),\n         ),\n         (\"align\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"altclip\", (\"XLMRobertaTokenizer\", \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"arcee\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"aria\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"aya_vision\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n@@ -156,6 +157,7 @@\n         (\"codegen\", (\"CodeGenTokenizer\", \"CodeGenTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"cohere\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"cohere2\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"cohere2_vision\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"colpali\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"colqwen2\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n         (\"convbert\", (\"ConvBertTokenizer\", \"ConvBertTokenizerFast\" if is_tokenizers_available() else None)),\n@@ -224,6 +226,7 @@\n             ),\n         ),\n         (\"distilbert\", (\"DistilBertTokenizer\", \"DistilBertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"donut\", (\"XLMRobertaTokenizer\", \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n             \"dpr\",\n             (\n@@ -238,6 +241,7 @@\n         (\"ernie4_5_moe\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"ernie_m\", (\"ErnieMTokenizer\" if is_sentencepiece_available() else None, None)),\n         (\"esm\", (\"EsmTokenizer\", None)),\n+        (\"evolla\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n             \"exaone4\",\n             (\n@@ -252,10 +256,13 @@\n             (\"FastSpeech2ConformerTokenizer\" if is_g2p_en_available() else None, None),\n         ),\n         (\"flaubert\", (\"FlaubertTokenizer\", None)),\n+        (\"flava\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"flex_olmo\", (None, \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"florence2\", (\"BartTokenizer\", \"BartTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"fnet\", (\"FNetTokenizer\", \"FNetTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"fsmt\", (\"FSMTTokenizer\", None)),\n         (\"funnel\", (\"FunnelTokenizer\", \"FunnelTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"fuyu\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n             \"gemma\",\n             (\n@@ -304,6 +311,7 @@\n         (\"glm4_moe\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"glm4v\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"glm4v_moe\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"got_ocr2\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"gpt-sw3\", (\"GPTSw3Tokenizer\" if is_sentencepiece_available() else None, None)),\n         (\"gpt2\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n         (\"gpt_bigcode\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n@@ -314,6 +322,7 @@\n         (\"gptj\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n         (\"gptsan-japanese\", (\"GPTSanJapaneseTokenizer\", None)),\n         (\"granite\", (\"GPT2Tokenizer\", None)),\n+        (\"granite_speech\", (\"GPT2Tokenizer\", None)),\n         (\"granitemoe\", (\"GPT2Tokenizer\", None)),\n         (\"granitemoehybrid\", (\"GPT2Tokenizer\", None)),\n         (\"granitemoeshared\", (\"GPT2Tokenizer\", None)),\n@@ -353,11 +362,14 @@\n             ),\n         ),\n         (\"kosmos-2.5\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"kyutai_speech_to_text\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"layoutlm\", (\"LayoutLMTokenizer\", \"LayoutLMTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"layoutlmv2\", (\"LayoutLMv2Tokenizer\", \"LayoutLMv2TokenizerFast\" if is_tokenizers_available() else None)),\n         (\"layoutlmv3\", (\"LayoutLMv3Tokenizer\", \"LayoutLMv3TokenizerFast\" if is_tokenizers_available() else None)),\n         (\"layoutxlm\", (\"LayoutXLMTokenizer\", \"LayoutXLMTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"led\", (\"LEDTokenizer\", \"LEDTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"lfm2\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"lfm2_vl\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"lilt\", (\"LayoutLMv3Tokenizer\", \"LayoutLMv3TokenizerFast\" if is_tokenizers_available() else None)),\n         (\n             \"llama\",\n@@ -398,6 +410,7 @@\n         (\"mamba\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"mamba2\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"marian\", (\"MarianTokenizer\" if is_sentencepiece_available() else None, None)),\n+        (\"markuplm\", (\"MarkupLMTokenizer\", \"MarkupLMTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n             \"mbart\",\n             (\n@@ -484,6 +497,7 @@\n                 \"NllbTokenizerFast\" if is_tokenizers_available() else None,\n             ),\n         ),\n+        (\"nougat\", (None, \"NougatTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n             \"nystromformer\",\n             (\n@@ -505,6 +519,7 @@\n             (\"OpenAIGPTTokenizer\", \"OpenAIGPTTokenizerFast\" if is_tokenizers_available() else None),\n         ),\n         (\"opt\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"ovis2\", (None, \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n         (\"owlv2\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"owlvit\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"paligemma\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n@@ -530,6 +545,7 @@\n                 None,\n             ),\n         ),\n+        (\"perception_lm\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n             \"persimmon\",\n             (\n@@ -539,6 +555,7 @@\n         ),\n         (\"phi\", (\"CodeGenTokenizer\", \"CodeGenTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"phi3\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"phi4_multimodal\", (None, \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n         (\"phimoe\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"phobert\", (\"PhobertTokenizer\", None)),\n         (\"pix2struct\", (\"T5Tokenizer\", \"T5TokenizerFast\" if is_tokenizers_available() else None)),\n@@ -552,6 +569,7 @@\n             ),\n         ),\n         (\"plbart\", (\"PLBartTokenizer\" if is_sentencepiece_available() else None, None)),\n+        (\"pop2piano\", (\"Pop2PianoTokenizer\", None)),\n         (\"prophetnet\", (\"ProphetNetTokenizer\", None)),\n         (\"qdqbert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n@@ -658,6 +676,7 @@\n             ),\n         ),\n         (\"smollm3\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"smolvlm\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"speech_to_text\", (\"Speech2TextTokenizer\" if is_sentencepiece_available() else None, None)),\n         (\"speech_to_text_2\", (\"Speech2Text2Tokenizer\", None)),\n         (\"speecht5\", (\"SpeechT5Tokenizer\" if is_sentencepiece_available() else None, None)),\n@@ -692,6 +711,7 @@\n         (\"tapas\", (\"TapasTokenizer\", None)),\n         (\"tapex\", (\"TapexTokenizer\", None)),\n         (\"transfo-xl\", (\"TransfoXLTokenizer\", None)),\n+        (\"trocr\", (\"XLMRobertaTokenizer\", \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"tvp\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n             \"udop\",\n@@ -707,9 +727,14 @@\n                 \"T5TokenizerFast\" if is_tokenizers_available() else None,\n             ),\n         ),\n+        (\"video_llama_3\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n         (\"video_llava\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"vilt\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"vipllava\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"vision_text_dual_encoder\",\n+            (\"PreTrainedTokenizer\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        ),\n         (\"visual_bert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"vits\", (\"VitsTokenizer\", None)),\n         (\n@@ -725,6 +750,7 @@\n         (\"wav2vec2-bert\", (\"Wav2Vec2CTCTokenizer\", None)),\n         (\"wav2vec2-conformer\", (\"Wav2Vec2CTCTokenizer\", None)),\n         (\"wav2vec2_phoneme\", (\"Wav2Vec2PhonemeCTCTokenizer\", None)),\n+        (\"wav2vec2_with_lm\", (\"Wav2Vec2CTCTokenizer\", None)),\n         (\"whisper\", (\"WhisperTokenizer\", \"WhisperTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"xclip\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n@@ -1160,7 +1186,7 @@ def register(config_class, slow_tokenizer_class=None, fast_tokenizer_class=None,\n                 The configuration corresponding to the model to register.\n             slow_tokenizer_class ([`PretrainedTokenizer`], *optional*):\n                 The slow tokenizer to register.\n-            fast_tokenizer_class ([`PretrainedTokenizerFast`], *optional*):\n+            fast_tokenizer_class ([`PreTrainedTokenizerFast`], *optional*):\n                 The fast tokenizer to register.\n         \"\"\"\n         if slow_tokenizer_class is None and fast_tokenizer_class is None:"
        },
        {
            "sha": "2f9d8796afc33e5c88f265ab79d413fc5c70df84",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -60,6 +60,7 @@\n             (\"qwen3_vl_moe\", \"Qwen3VLVideoProcessor\"),\n             (\"sam2_video\", \"Sam2VideoVideoProcessor\"),\n             (\"smolvlm\", \"SmolVLMVideoProcessor\"),\n+            (\"video_llama_3\", \"VideoLlama3VideoProcessor\"),\n             (\"video_llava\", \"VideoLlavaVideoProcessor\"),\n             (\"videomae\", \"VideoMAEVideoProcessor\"),\n             (\"vjepa2\", \"VJEPA2VideoProcessor\"),"
        },
        {
            "sha": "049b0e5d24eb61d3e77c94b22f3b0b097ce35d98",
            "filename": "src/transformers/models/aya_vision/processing_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -70,10 +70,6 @@ class AyaVisionProcessor(ProcessorMixin):\n             in a chat into a tokenizable string.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "403d107f48f97a3bb74dc66ec7b86d4321bb1c54",
            "filename": "src/transformers/models/bark/processing_bark.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -49,9 +49,6 @@ class BarkProcessor(ProcessorMixin):\n \n     \"\"\"\n \n-    tokenizer_class = \"AutoTokenizer\"\n-    attributes = [\"tokenizer\"]\n-\n     preset_shape = {\n         \"semantic_prompt\": 1,  # 1D array of shape (X,)\n         \"coarse_prompt\": 2,  # 2D array of shape (2,X)"
        },
        {
            "sha": "965164206c5adf6119cd16b05dfeefe1aab94300",
            "filename": "src/transformers/models/blip/processing_blip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -53,10 +53,6 @@ class BlipProcessor(ProcessorMixin):\n             An instance of ['BertTokenizerFast`]. The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = (\"BlipImageProcessor\", \"BlipImageProcessorFast\")\n-    tokenizer_class = (\"BertTokenizer\", \"BertTokenizerFast\")\n-\n     def __init__(self, image_processor, tokenizer, **kwargs):\n         tokenizer.return_token_type_ids = False\n         super().__init__(image_processor, tokenizer)"
        },
        {
            "sha": "5949e2c648cef16698b7d227807ff76254c55176",
            "filename": "src/transformers/models/blip_2/processing_blip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -60,10 +60,6 @@ class Blip2Processor(ProcessorMixin):\n             Number of tokens used by the Qformer as queries, should be same as in model's config.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = (\"BlipImageProcessor\", \"BlipImageProcessorFast\")\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(self, image_processor, tokenizer, num_query_tokens=None, **kwargs):\n         tokenizer.return_token_type_ids = False\n         if not hasattr(tokenizer, \"image_token\"):"
        },
        {
            "sha": "5de97ec411dcb42a2de638cdacb620de38664d35",
            "filename": "src/transformers/models/bridgetower/processing_bridgetower.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -54,9 +54,6 @@ class BridgeTowerProcessor(ProcessorMixin):\n             An instance of ['RobertaTokenizerFast`]. The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"BridgeTowerImageProcessor\"\n-    tokenizer_class = (\"RobertaTokenizer\", \"RobertaTokenizerFast\")\n     valid_processor_kwargs = BridgeTowerProcessorKwargs\n \n     def __init__(self, image_processor, tokenizer):"
        },
        {
            "sha": "d92b163955a71c5aa8d213e680a9b3da4dda8a03",
            "filename": "src/transformers/models/bros/processing_bros.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -46,8 +46,6 @@ class BrosProcessor(ProcessorMixin):\n             An instance of ['BertTokenizerFast`]. The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"tokenizer\"]\n-    tokenizer_class = (\"BertTokenizer\", \"BertTokenizerFast\")\n     valid_processor_kwargs = BrosProcessorKwargs\n \n     def __init__(self, tokenizer=None, **kwargs):"
        },
        {
            "sha": "694be7ab8f26d24add9436c8d7efee8edb2a6220",
            "filename": "src/transformers/models/chameleon/processing_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -69,10 +69,6 @@ class ChameleonProcessor(ProcessorMixin):\n             The special token used to indicate image in the text.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    tokenizer_class = (\"LlamaTokenizer\", \"LlamaTokenizerFast\")\n-    image_processor_class = \"ChameleonImageProcessor\"\n-\n     def __init__(self, image_processor, tokenizer, image_seq_length: int = 1024, image_token: str = \"<image>\"):\n         self.image_seq_length = image_seq_length\n         self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token"
        },
        {
            "sha": "6508136f772eabbdba4a7649856bcccd6179a8c0",
            "filename": "src/transformers/models/chinese_clip/processing_chinese_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -34,10 +34,6 @@ class ChineseCLIPProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = (\"ChineseCLIPImageProcessor\", \"ChineseCLIPImageProcessorFast\")\n-    tokenizer_class = (\"BertTokenizer\", \"BertTokenizerFast\")\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "a72151cb9b63144951deec0d99b433bfdc453782",
            "filename": "src/transformers/models/clap/processing_clap.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -42,9 +42,6 @@ class ClapProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    feature_extractor_class = \"ClapFeatureExtractor\"\n-    tokenizer_class = (\"RobertaTokenizer\", \"RobertaTokenizerFast\")\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n "
        },
        {
            "sha": "9258d2e8fee385b0ceed454505fe52a05ed5cfe8",
            "filename": "src/transformers/models/clip/processing_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -33,10 +33,6 @@ class CLIPProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "4d431181cb4ff6b9a3f23a821688f44c4b24cea9",
            "filename": "src/transformers/models/clipseg/processing_clipseg.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -34,10 +34,6 @@ class CLIPSegProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = (\"ViTImageProcessor\", \"ViTImageProcessorFast\")\n-    tokenizer_class = (\"CLIPTokenizer\", \"CLIPTokenizerFast\")\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "331589a2399973945e7622651428e882f78652d1",
            "filename": "src/transformers/models/clvp/processing_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -38,9 +38,6 @@ class ClvpProcessor(ProcessorMixin):\n             An instance of [`ClvpTokenizer`]. The tokenizer is a required input.\n     \"\"\"\n \n-    feature_extractor_class = \"ClvpFeatureExtractor\"\n-    tokenizer_class = \"ClvpTokenizer\"\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n "
        },
        {
            "sha": "b34fd1c5594eac549337b6226a25ada385e56b4b",
            "filename": "src/transformers/models/cohere2_vision/processing_cohere2_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -47,10 +47,6 @@ class Cohere2VisionProcessor(ProcessorMixin):\n             in a chat into a tokenizable string.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "1ad511ced7a76398531768ad56cbf27aa125689d",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -93,10 +93,6 @@ class ColPaliProcessor(ProcessorMixin):\n             A prefix to be used for the query.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")\n-    tokenizer_class = (\"GemmaTokenizer\", \"GemmaTokenizerFast\")\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "a96ecc6c7416f8256e6467250d8fae622e25cd6d",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -65,9 +65,6 @@ class ColQwen2Processor(ColPaliProcessor):\n         query_prefix (`str`, *optional*): A prefix to be used for the query.\n     \"\"\"\n \n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "00f00c9208565b0791825aae6961c4fed2721657",
            "filename": "src/transformers/models/colqwen2/processing_colqwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -64,11 +64,6 @@ class ColQwen2Processor(ProcessorMixin):\n         query_prefix (`str`, *optional*): A prefix to be used for the query.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "d77ffeffd8968558c2f259ad2b8d6dd165ddaf9a",
            "filename": "src/transformers/models/csm/processing_csm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -95,10 +95,6 @@ class CsmProcessor(ProcessorMixin):\n \n     \"\"\"\n \n-    attributes = [\"feature_extractor\", \"tokenizer\"]\n-    feature_extractor_class = \"EncodecFeatureExtractor\"\n-    tokenizer_class = \"PreTrainedTokenizerFast\"\n-\n     def __init__(\n         self,\n         feature_extractor,"
        },
        {
            "sha": "21cb19d79c3b1b7187f938778626f282956f623a",
            "filename": "src/transformers/models/deepseek_vl/modular_deepseek_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -222,11 +222,6 @@ class DeepseekVLProcessor(ProcessorMixin):\n             The number of special image tokens used as placeholders for visual content in text sequences.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    valid_kwargs = [\"chat_template\", \"num_image_tokens\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         image_processor,"
        },
        {
            "sha": "22b1c2ab71dded497b914c7bf0f0c30270a178d6",
            "filename": "src/transformers/models/deepseek_vl/processing_deepseek_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -52,11 +52,6 @@ class DeepseekVLProcessor(ProcessorMixin):\n             The number of special image tokens used as placeholders for visual content in text sequences.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    valid_kwargs = [\"chat_template\", \"num_image_tokens\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         image_processor,"
        },
        {
            "sha": "8f842db7346f11454bf28c844a73a931c0d4b9a8",
            "filename": "src/transformers/models/deepseek_vl_hybrid/processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -52,11 +52,6 @@ class DeepseekVLHybridProcessor(ProcessorMixin):\n             The number of special image tokens used as placeholders for visual content in text sequences.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    valid_kwargs = [\"chat_template\", \"num_image_tokens\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         image_processor,"
        },
        {
            "sha": "23c04687308cef1a350f5fec3a6be9d1d9c7ddcc",
            "filename": "src/transformers/models/dia/processing_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -77,8 +77,6 @@ class DiaProcessor(ProcessorMixin):\n             An instance of [`DacModel`] used to encode/decode audio into/from codebooks. It is is a required input.\n     \"\"\"\n \n-    feature_extractor_class = \"DiaFeatureExtractor\"\n-    tokenizer_class = \"DiaTokenizer\"\n     audio_tokenizer_class = \"DacModel\"\n \n     def __init__(self, feature_extractor, tokenizer, audio_tokenizer):"
        },
        {
            "sha": "fedd173117ebfd0179b37cbe4558a03032747f50",
            "filename": "src/transformers/models/donut/processing_donut.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -48,10 +48,6 @@ class DonutProcessor(ProcessorMixin):\n             An instance of [`XLMRobertaTokenizer`/`XLMRobertaTokenizerFast`]. The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "52f39a913c542b6fece99f1ff1938258163a140b",
            "filename": "src/transformers/models/emu3/processing_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -64,10 +64,6 @@ class Emu3Processor(ProcessorMixin):\n             in a chat into a tokenizable string.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    tokenizer_class = (\"GPT2Tokenizer\", \"GPT2TokenizerFast\")\n-    image_processor_class = \"Emu3ImageProcessor\"\n-\n     def __init__(\n         self,\n         image_processor,"
        },
        {
            "sha": "807bd294c406bfcf9d21f3900bc103c5ae52db09",
            "filename": "src/transformers/models/evolla/processing_evolla.py",
            "status": "modified",
            "additions": 0,
            "deletions": 48,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -16,14 +16,12 @@\n Processor class for EVOLLA.\n \"\"\"\n \n-import os\n from typing import Optional, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...processing_utils import (\n     ProcessorMixin,\n )\n-from ..auto import AutoTokenizer\n \n \n PROTEIN_VALID_KEYS = [\"aa_seq\", \"foldseek\", \"msa\"]\n@@ -47,15 +45,6 @@ class EvollaProcessor(ProcessorMixin):\n             The maximum length of the text to be generated.\n     \"\"\"\n \n-    attributes = [\"protein_tokenizer\", \"tokenizer\"]\n-    valid_kwargs = [\"sequence_max_length\"]\n-    # protein_tokenizer_class = \"EsmTokenizer\"\n-    # tokenizer_class = \"LlamaTokenizerFast\"\n-    protein_tokenizer_class = \"AutoTokenizer\"\n-    tokenizer_class = \"AutoTokenizer\"\n-    protein_tokenizer_dir_name = \"protein_tokenizer\"\n-    # tokenizer_dir_name = \"text_tokenizer\"\n-\n     def __init__(self, protein_tokenizer, tokenizer=None, protein_max_length=1024, text_max_length=512, **kwargs):\n         if protein_tokenizer is None:\n             raise ValueError(\"You need to specify an `protein_tokenizer`.\")\n@@ -206,42 +195,5 @@ def protein_batch_decode(self, *args, **kwargs):\n     def protein_decode(self, *args, **kwargs):\n         return self.protein_tokenizer.decode(*args, **kwargs)\n \n-    # overwrite to save the protein tokenizer in a separate folder\n-    # Adapted from instructblip.processing_instructblip.py (https://github.com/huggingface/transformers/blob/9b479a245b793cac2a8b2e87c6d8e81bb24e20c4/src/transformers/models/instructblip/processing_instructblip.py#L191-L221)\n-    def save_pretrained(self, save_directory, **kwargs):\n-        # only save the protein tokenizer in sub_dir\n-        self.protein_tokenizer.save_pretrained(os.path.join(save_directory, self.protein_tokenizer_dir_name))\n-\n-        # we modify the attributes so that only the text tokenizer are saved in the main folder\n-        protein_tokenizer_present = \"protein_tokenizer\" in self.attributes\n-        # find the correct position of it in the attributes list\n-        protein_tokenizer_index = self.attributes.index(\"protein_tokenizer\") if protein_tokenizer_present else None\n-        if protein_tokenizer_present and protein_tokenizer_index is not None:\n-            self.attributes.remove(\"protein_tokenizer\")\n-\n-        outputs = super().save_pretrained(save_directory, **kwargs)\n-\n-        if protein_tokenizer_present and protein_tokenizer_index is not None:\n-            self.attributes.insert(protein_tokenizer_index, \"protein_tokenizer\")\n-\n-        return outputs\n-\n-    # overwrite to load the protein tokenizer from a separate folder\n-    # Adapted from instructblip.processing_instructblip.py (https://github.com/huggingface/transformers/blob/9b479a245b793cac2a8b2e87c6d8e81bb24e20c4/src/transformers/models/instructblip/processing_instructblip.py#L191-L221)\n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n-        processor = super().from_pretrained(pretrained_model_name_or_path, **kwargs)\n-\n-        # if return_unused_kwargs a tuple is returned where the second element is 'unused_kwargs'\n-        if isinstance(processor, tuple):\n-            processor = processor[0]\n-        protein_tokenizer = AutoTokenizer.from_pretrained(\n-            pretrained_model_name_or_path, subfolder=cls.protein_tokenizer_dir_name\n-        )\n-\n-        processor.protein_tokenizer = protein_tokenizer\n-\n-        return processor\n-\n \n __all__ = [\"EvollaProcessor\"]"
        },
        {
            "sha": "7e5b3c0e012ee226059a8bce34e5f7f611ee8434",
            "filename": "src/transformers/models/flava/processing_flava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -31,10 +31,6 @@ class FlavaProcessor(ProcessorMixin):\n         tokenizer ([`BertTokenizerFast`], *optional*): The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"FlavaImageProcessor\"\n-    tokenizer_class = (\"BertTokenizer\", \"BertTokenizerFast\")\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "049beffce14b9b5256f0d53411f14d21a86e817d",
            "filename": "src/transformers/models/florence2/modular_florence2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -256,10 +256,6 @@ class Florence2Processor(ProcessorMixin):\n             thresholds, or banned tokens.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = (\"BartTokenizer\", \"BartTokenizerFast\")\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "c8d699e4bc3ede3e344afddb4f14efc0125ee7a0",
            "filename": "src/transformers/models/florence2/processing_florence2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -62,10 +62,6 @@ class Florence2Processor(ProcessorMixin):\n             thresholds, or banned tokens.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = (\"BartTokenizer\", \"BartTokenizerFast\")\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "ee697deccf9e5aec7354588907f7aa6d8f1e7e62",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -347,10 +347,6 @@ class FuyuProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"FuyuImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(self, image_processor, tokenizer, **kwargs):\n         super().__init__(image_processor=image_processor, tokenizer=tokenizer)\n         self.image_processor = image_processor"
        },
        {
            "sha": "11574e30b7c1f61ae1bf8a54423d5b5d1acb7f4a",
            "filename": "src/transformers/models/gemma3/processing_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -42,10 +42,6 @@ class Gemma3ProcessorKwargs(ProcessingKwargs, total=False):\n \n \n class Gemma3Processor(ProcessorMixin):\n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         image_processor,"
        },
        {
            "sha": "51b686557ed0bc2a0bb3e5915018b6de140eba90",
            "filename": "src/transformers/models/gemma3n/processing_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -51,11 +51,6 @@ class Gemma3nProcessor(ProcessorMixin):\n             The number of image soft tokens that should be added to\n     \"\"\"\n \n-    attributes = [\"feature_extractor\", \"image_processor\", \"tokenizer\"]\n-    feature_extractor_class = \"AutoFeatureExtractor\"\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         feature_extractor,"
        },
        {
            "sha": "89cfc9618987b66c3cf31994923af0431f88b5ee",
            "filename": "src/transformers/models/git/processing_git.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -33,10 +33,6 @@ class GitProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "9ff12163b12a47223a31ff1bd52c8c51e919496c",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -1545,8 +1545,6 @@ class Glm4vProcessor(Qwen2VLProcessor):\n             in a chat into a tokenizable string.\n     \"\"\"\n \n-    tokenizer_class = (\"PreTrainedTokenizer\", \"PreTrainedTokenizerFast\")\n-\n     def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n         super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)\n         self.image_token = \"<|image|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token"
        },
        {
            "sha": "79935cbde7b4b6adb82a24b2696b4024793d51b8",
            "filename": "src/transformers/models/glm4v/processing_glm4v.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -59,12 +59,6 @@ class Glm4vProcessor(ProcessorMixin):\n             in a chat into a tokenizable string.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\", \"video_processor\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    video_processor_class = \"AutoVideoProcessor\"\n-\n-    tokenizer_class = (\"PreTrainedTokenizer\", \"PreTrainedTokenizerFast\")\n-\n     def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n         self.image_token = \"<|image|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token"
        },
        {
            "sha": "162efef5e9f9896eacc6974db44a620d376b06e4",
            "filename": "src/transformers/models/got_ocr2/processing_got_ocr2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -93,10 +93,6 @@ class GotOcr2Processor(ProcessorMixin):\n             in a chat into a tokenizable string.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"PreTrainedTokenizerFast\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, chat_template=None, **kwargs):\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n "
        },
        {
            "sha": "910840bd661cffbf23bd9dc91434f97a1eae24ed",
            "filename": "src/transformers/models/granite_speech/processing_granite_speech.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -30,10 +30,6 @@\n \n \n class GraniteSpeechProcessor(ProcessorMixin):\n-    attributes = [\"audio_processor\", \"tokenizer\"]\n-    audio_processor_class = \"GraniteSpeechFeatureExtractor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         audio_processor,"
        },
        {
            "sha": "74565588d852489937f58f01a3b56fd7e5646089",
            "filename": "src/transformers/models/grounding_dino/processing_grounding_dino.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -130,9 +130,6 @@ class GroundingDinoProcessor(ProcessorMixin):\n             An instance of ['PreTrainedTokenizer`]. The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"GroundingDinoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n     valid_processor_kwargs = GroundingDinoProcessorKwargs\n \n     def __init__(self, image_processor, tokenizer):"
        },
        {
            "sha": "7cb640e56854f694d99e6cfdfd33c00cfd2a3415",
            "filename": "src/transformers/models/idefics/processing_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -153,10 +153,6 @@ class IdeficsProcessor(ProcessorMixin):\n             The string representation of token representing end of utterance\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"IdeficsImageProcessor\"\n-    tokenizer_class = \"LlamaTokenizerFast\"\n-\n     def __init__(self, image_processor, tokenizer=None, image_size=224, add_end_of_utterance_token=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n         self.image_token_id = ("
        },
        {
            "sha": "df5f9ca73a8b34fce82ef60351ca092e66a64ec8",
            "filename": "src/transformers/models/idefics2/processing_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -75,10 +75,6 @@ class Idefics2Processor(ProcessorMixin):\n             in a chat into a tokenizable string.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"Idefics2ImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self, image_processor, tokenizer=None, image_seq_len: int = 64, chat_template: Optional[str] = None, **kwargs\n     ):"
        },
        {
            "sha": "5c978eb3b230ecd2a0c5e0be4d60c378ef1d4950",
            "filename": "src/transformers/models/idefics3/processing_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -121,10 +121,6 @@ class Idefics3Processor(ProcessorMixin):\n             in a chat into a tokenizable string.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"Idefics3ImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self, image_processor, tokenizer=None, image_seq_len: int = 169, chat_template: Optional[str] = None, **kwargs\n     ):"
        },
        {
            "sha": "cfed52f745ae03921b11e6a8bc28ccac65d81529",
            "filename": "src/transformers/models/instructblip/processing_instructblip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -16,15 +16,13 @@\n Processor class for InstructBLIP. Largely copy of Blip2Processor with addition of a tokenizer for the Q-Former.\n \"\"\"\n \n-import os\n from typing import Optional, Union\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import AddedToken, PreTokenizedInput, TextInput\n from ...utils import logging\n-from ..auto import AutoTokenizer\n \n \n logger = logging.get_logger(__name__)\n@@ -65,11 +63,6 @@ class InstructBlipProcessor(ProcessorMixin):\n             Number of tokens used by the Qformer as queries, should be same as in model's config.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\", \"qformer_tokenizer\"]\n-    image_processor_class = (\"BlipImageProcessor\", \"BlipImageProcessorFast\")\n-    tokenizer_class = \"AutoTokenizer\"\n-    qformer_tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(self, image_processor, tokenizer, qformer_tokenizer, num_query_tokens=None, **kwargs):\n         if not hasattr(tokenizer, \"image_token\"):\n             self.image_token = AddedToken(\"<image>\", normalized=False, special=True)\n@@ -152,36 +145,5 @@ def model_input_names(self):\n         qformer_input_names = [\"qformer_input_ids\", \"qformer_attention_mask\"]\n         return tokenizer_input_names + image_processor_input_names + qformer_input_names\n \n-    # overwrite to save the Q-Former tokenizer in a separate folder\n-    def save_pretrained(self, save_directory, **kwargs):\n-        if os.path.isfile(save_directory):\n-            raise ValueError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n-        os.makedirs(save_directory, exist_ok=True)\n-        qformer_tokenizer_path = os.path.join(save_directory, \"qformer_tokenizer\")\n-        self.qformer_tokenizer.save_pretrained(qformer_tokenizer_path)\n-\n-        # We modify the attributes so that only the tokenizer and image processor are saved in the main folder\n-        qformer_present = \"qformer_tokenizer\" in self.attributes\n-        if qformer_present:\n-            self.attributes.remove(\"qformer_tokenizer\")\n-\n-        outputs = super().save_pretrained(save_directory, **kwargs)\n-\n-        if qformer_present:\n-            self.attributes += [\"qformer_tokenizer\"]\n-        return outputs\n-\n-    # overwrite to load the Q-Former tokenizer from a separate folder\n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n-        processor = super().from_pretrained(pretrained_model_name_or_path, **kwargs)\n-\n-        # if return_unused_kwargs a tuple is returned where the second element is 'unused_kwargs'\n-        if isinstance(processor, tuple):\n-            processor = processor[0]\n-        qformer_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"qformer_tokenizer\")\n-        processor.qformer_tokenizer = qformer_tokenizer\n-        return processor\n-\n \n __all__ = [\"InstructBlipProcessor\"]"
        },
        {
            "sha": "81d0103b2742b24f7e05fa7102e7133504a1b12d",
            "filename": "src/transformers/models/instructblipvideo/processing_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 39,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -16,7 +16,6 @@\n Processor class for InstructBLIP. Largely copy of Blip2Processor with addition of a tokenizer for the Q-Former.\n \"\"\"\n \n-import os\n from typing import Optional, Union\n \n from ...image_processing_utils import BatchFeature\n@@ -30,7 +29,6 @@\n )\n from ...utils import TensorType, logging\n from ...video_utils import VideoInput\n-from ..auto import AutoTokenizer\n \n \n logger = logging.get_logger(__name__)\n@@ -55,11 +53,6 @@ class InstructBlipVideoProcessor(ProcessorMixin):\n             Number of tokens used by the Qformer as queries, should be same as in model's config.\n     \"\"\"\n \n-    attributes = [\"video_processor\", \"tokenizer\", \"qformer_tokenizer\"]\n-    video_processor_class = \"AutoVideoProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-    qformer_tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(self, video_processor, tokenizer, qformer_tokenizer, num_query_tokens=None, **kwargs):\n         if not hasattr(tokenizer, \"video_token\"):\n             self.video_token = AddedToken(\"<video>\", normalized=False, special=True)\n@@ -90,7 +83,7 @@ def __call__(\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n-        This method uses [`InstructBlipVideoImageProcessor.__call__`] method to prepare image(s) or video(s) for the model, and\n+        This method uses [`InstructBlipVideoVideoProcessor.__call__`] method to prepare image(s) or video(s) for the model, and\n         [`BertTokenizerFast.__call__`] to prepare text for the model.\n \n         Please refer to the docstring of the above two methods for more information.\n@@ -180,36 +173,5 @@ def model_input_names(self):\n         qformer_input_names = [\"qformer_input_ids\", \"qformer_attention_mask\"]\n         return tokenizer_input_names + video_processor_input_names + qformer_input_names\n \n-    # overwrite to save the Q-Former tokenizer in a separate folder\n-    def save_pretrained(self, save_directory, **kwargs):\n-        if os.path.isfile(save_directory):\n-            raise ValueError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n-        os.makedirs(save_directory, exist_ok=True)\n-        qformer_tokenizer_path = os.path.join(save_directory, \"qformer_tokenizer\")\n-        self.qformer_tokenizer.save_pretrained(qformer_tokenizer_path)\n-\n-        # We modify the attributes so that only the tokenizer and image processor are saved in the main folder\n-        qformer_present = \"qformer_tokenizer\" in self.attributes\n-        if qformer_present:\n-            self.attributes.remove(\"qformer_tokenizer\")\n-\n-        outputs = super().save_pretrained(save_directory, **kwargs)\n-\n-        if qformer_present:\n-            self.attributes += [\"qformer_tokenizer\"]\n-        return outputs\n-\n-    # overwrite to load the Q-Former tokenizer from a separate folder\n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n-        processor = super().from_pretrained(pretrained_model_name_or_path, **kwargs)\n-\n-        # if return_unused_kwargs a tuple is returned where the second element is 'unused_kwargs'\n-        if isinstance(processor, tuple):\n-            processor = processor[0]\n-        qformer_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"qformer_tokenizer\")\n-        processor.qformer_tokenizer = qformer_tokenizer\n-        return processor\n-\n \n __all__ = [\"InstructBlipVideoProcessor\"]"
        },
        {
            "sha": "fd2a52a768abccf1b966f8823be0dac2ff642b4a",
            "filename": "src/transformers/models/internvl/processing_internvl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -58,11 +58,6 @@ class InternVLProcessor(ProcessorMixin):\n             in a chat into a tokenizable string.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\", \"video_processor\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    video_processor_class = \"AutoVideoProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "354570314a78b657acc13084b7d11b7826332ab6",
            "filename": "src/transformers/models/janus/processing_janus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -64,10 +64,6 @@ class JanusProcessor(ProcessorMixin):\n             Use default system prompt for Text Generation.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"JanusImageProcessor\"\n-    tokenizer_class = \"LlamaTokenizerFast\"\n-\n     def __init__(self, image_processor, tokenizer, chat_template=None, use_default_system_prompt=False, **kwargs):\n         self.num_image_tokens = 576\n         self.image_token = tokenizer.image_token"
        },
        {
            "sha": "d6fd1e6ec758661a8ffaa7ab3ec590ff09853ddf",
            "filename": "src/transformers/models/kosmos2/processing_kosmos2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -85,10 +85,6 @@ class Kosmos2Processor(ProcessorMixin):\n             The number of tokens that represent patch indices.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(self, image_processor, tokenizer, num_patch_index_tokens=1024, *kwargs):\n         tokenizer.return_token_type_ids = False\n "
        },
        {
            "sha": "5d1ec20c75de82443a8bb096162fcbcb1e97fe19",
            "filename": "src/transformers/models/kosmos2_5/processing_kosmos2_5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -61,10 +61,6 @@ class Kosmos2_5Processor(ProcessorMixin):\n             Number of image tokens used as a placeholder.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"PreTrainedTokenizerFast\"\n-\n     def __init__(self, image_processor, tokenizer, num_image_tokens: int = 2048):\n         self.image_start_token = tokenizer.boi_token  # \"<image>\" : fixed token for the start of image\n         self.image_end_token = tokenizer.eoi_token  # \"</image>\" : fixed token for the end of image"
        },
        {
            "sha": "53c6b7d395df5ff2b0f4352bc34380fbdbd363e4",
            "filename": "src/transformers/models/kyutai_speech_to_text/processing_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fprocessing_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fprocessing_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fprocessing_kyutai_speech_to_text.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -34,9 +34,10 @@ class KyutaiSpeechToTextProcessor(ProcessorMixin):\n     information.\n     \"\"\"\n \n-    feature_extractor_class = \"KyutaiSpeechToTextFeatureExtractor\"\n-    tokenizer_class = \"PreTrainedTokenizerFast\"\n     valid_processor_kwargs = KyutaiSpeechToTextProcessorKwargs\n \n+    def __init__(self, feature_extractor, tokenizer):\n+        super().__init__(feature_extractor, tokenizer)\n+\n \n __all__ = [\"KyutaiSpeechToTextProcessor\"]"
        },
        {
            "sha": "0f3e7dc8a9d9ed3fe2e8345149d28e78bb473df3",
            "filename": "src/transformers/models/layoutlmv2/processing_layoutlmv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -43,10 +43,6 @@ class LayoutLMv2Processor(ProcessorMixin):\n             An instance of [`LayoutLMv2Tokenizer`] or [`LayoutLMv2TokenizerFast`]. The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"LayoutLMv2ImageProcessor\"\n-    tokenizer_class = (\"LayoutLMv2Tokenizer\", \"LayoutLMv2TokenizerFast\")\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "5f7de3dd91479d4819e0436c8a86344bb33d44c1",
            "filename": "src/transformers/models/layoutlmv3/processing_layoutlmv3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -43,10 +43,6 @@ class LayoutLMv3Processor(ProcessorMixin):\n             An instance of [`LayoutLMv3Tokenizer`] or [`LayoutLMv3TokenizerFast`]. The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"LayoutLMv3ImageProcessor\"\n-    tokenizer_class = (\"LayoutLMv3Tokenizer\", \"LayoutLMv3TokenizerFast\")\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "887d150ab366ac765c629b96bbea56811539695f",
            "filename": "src/transformers/models/layoutxlm/processing_layoutxlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -43,10 +43,6 @@ class LayoutXLMProcessor(ProcessorMixin):\n             An instance of [`LayoutXLMTokenizer`] or [`LayoutXLMTokenizerFast`]. The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"LayoutLMv2ImageProcessor\"\n-    tokenizer_class = (\"LayoutXLMTokenizer\", \"LayoutXLMTokenizerFast\")\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "73038b9f37aafc05ab4170db610bea501d6a8744",
            "filename": "src/transformers/models/lfm2_vl/processing_lfm2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -64,10 +64,6 @@ class Lfm2VlProcessor(ProcessorMixin):\n             A Jinja template which will be used to convert lists of messages in a chat into a tokenizable string.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"Lfm2VlImageProcessorFast\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         image_processor,"
        },
        {
            "sha": "c9ad6884fa8d0f64b040a841f517f07a55ca5197",
            "filename": "src/transformers/models/llama4/processing_llama4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -68,10 +68,6 @@ class Llama4Processor(ProcessorMixin):\n             in a chat into a tokenizable string.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "a11e80280b74f486ed5aec0e4e10ec08660d1871",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -67,10 +67,6 @@ class LlavaProcessor(ProcessorMixin):\n             extra tokens appended, no need to set this arg.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "d79c5a0edf6b44627d7a083cb52b011254b7b034",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -74,10 +74,6 @@ class LlavaNextProcessor(ProcessorMixin):\n             extra tokens appended, no need to set this arg.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "582002b6165cc645e302510891cd727493c9a881",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -77,11 +77,6 @@ class LlavaNextVideoProcessor(ProcessorMixin):\n \n     # video and image processor share same args, but have different processing logic\n     # only image processor config is saved in the hub\n-    attributes = [\"video_processor\", \"image_processor\", \"tokenizer\"]\n-    image_processor_class = (\"LlavaNextImageProcessor\", \"LlavaNextImageProcessorFast\")\n-    video_processor_class = \"AutoVideoProcessor\"\n-    tokenizer_class = (\"LlamaTokenizer\", \"LlamaTokenizerFast\")\n-\n     def __init__(\n         self,\n         video_processor=None,"
        },
        {
            "sha": "4ea891e50cf1693be21532d87f6bfb4c3890bd35",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -74,11 +74,6 @@ class LlavaOnevisionProcessor(ProcessorMixin):\n             Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\", \"video_processor\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-    video_processor_class = \"AutoVideoProcessor\"\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "5c2f181d35a6e3751008cfb9f14aa70991a8dbd4",
            "filename": "src/transformers/models/markuplm/processing_markuplm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fprocessing_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fprocessing_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fprocessing_markuplm.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -43,10 +43,11 @@ class MarkupLMProcessor(ProcessorMixin):\n             Whether or not to use `MarkupLMFeatureExtractor` to parse HTML strings into nodes and corresponding xpaths.\n     \"\"\"\n \n-    feature_extractor_class = \"MarkupLMFeatureExtractor\"\n-    tokenizer_class = (\"MarkupLMTokenizer\", \"MarkupLMTokenizerFast\")\n     parse_html = True\n \n+    def __init__(self, feature_extractor, tokenizer):\n+        super().__init__(feature_extractor, tokenizer)\n+\n     def __call__(\n         self,\n         html_strings=None,"
        },
        {
            "sha": "7686b43f00e883b5f38b847b5df65f555cf9c718",
            "filename": "src/transformers/models/mgp_str/processing_mgp_str.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -50,10 +50,6 @@ class MgpstrProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"char_tokenizer\"]\n-    image_processor_class = (\"ViTImageProcessor\", \"ViTImageProcessorFast\")\n-    char_tokenizer_class = \"MgpstrTokenizer\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         self.char_tokenizer = tokenizer\n         self.bpe_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")"
        },
        {
            "sha": "7c1148f19cf38e3ab4372cf9fccdc66966fba7eb",
            "filename": "src/transformers/models/mllama/processing_mllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -198,10 +198,6 @@ class MllamaProcessor(ProcessorMixin):\n \n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"MllamaImageProcessor\"\n-    tokenizer_class = \"PreTrainedTokenizerFast\"\n-\n     def __init__(self, image_processor, tokenizer, chat_template=None):\n         if not hasattr(tokenizer, \"image_token\"):\n             self.image_token = \"<|image|>\""
        },
        {
            "sha": "228253e20993f55e8f955d355ef52c7b91ab1b64",
            "filename": "src/transformers/models/musicgen/processing_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fprocessing_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fprocessing_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fprocessing_musicgen.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -39,9 +39,6 @@ class MusicgenProcessor(ProcessorMixin):\n             An instance of [`T5Tokenizer`]. The tokenizer is a required input.\n     \"\"\"\n \n-    feature_extractor_class = \"EncodecFeatureExtractor\"\n-    tokenizer_class = (\"T5Tokenizer\", \"T5TokenizerFast\")\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n "
        },
        {
            "sha": "49092f80cd456e1f6d08a9315a776e8f9e232c2e",
            "filename": "src/transformers/models/musicgen_melody/processing_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -41,9 +41,6 @@ class MusicgenMelodyProcessor(ProcessorMixin):\n             An instance of [`T5Tokenizer`]. The tokenizer is a required input.\n     \"\"\"\n \n-    feature_extractor_class = \"MusicgenMelodyFeatureExtractor\"\n-    tokenizer_class = (\"T5Tokenizer\", \"T5TokenizerFast\")\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n "
        },
        {
            "sha": "4e071dbd8109ebc6069171adf8b4bbeea5653563",
            "filename": "src/transformers/models/nougat/processing_nougat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -38,10 +38,6 @@ class NougatProcessor(ProcessorMixin):\n             An instance of [`NougatTokenizerFast`]. The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "3601655e3e994bf26f4fa02c3ee5b9cf4f36bd6f",
            "filename": "src/transformers/models/omdet_turbo/processing_omdet_turbo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -215,10 +215,6 @@ class OmDetTurboProcessor(ProcessorMixin):\n             An instance of ['PreTrainedTokenizer`]. The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = (\"DetrImageProcessor\", \"DetrImageProcessorFast\")\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "ec90d63f7bd7e34c098104b1de2681ce4e2b85d5",
            "filename": "src/transformers/models/oneformer/processing_oneformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Foneformer%2Fprocessing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Foneformer%2Fprocessing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fprocessing_oneformer.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -41,10 +41,6 @@ class OneFormerProcessor(ProcessorMixin):\n             Sequence length for input task token.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"OneFormerImageProcessor\"\n-    tokenizer_class = (\"CLIPTokenizer\", \"CLIPTokenizerFast\")\n-\n     def __init__(\n         self, image_processor=None, tokenizer=None, max_seq_length: int = 77, task_seq_length: int = 77, **kwargs\n     ):"
        },
        {
            "sha": "f67657c140d8c1f71fe18970ab8eaf57e83f46d5",
            "filename": "src/transformers/models/ovis2/processing_ovis2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -54,10 +54,6 @@ class Ovis2Processor(ProcessorMixin):\n             The number of image tokens to be used for each image in the input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "52889721820f7ef33867f26787ac66cab5bd927b",
            "filename": "src/transformers/models/owlv2/processing_owlv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -66,10 +66,6 @@ class Owlv2Processor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = (\"Owlv2ImageProcessor\", \"Owlv2ImageProcessorFast\")\n-    tokenizer_class = (\"CLIPTokenizer\", \"CLIPTokenizerFast\")\n-\n     def __init__(self, image_processor, tokenizer, **kwargs):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "0443ab64eda9e05d04e7707c84d145542f5e6235",
            "filename": "src/transformers/models/owlvit/processing_owlvit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -66,10 +66,6 @@ class OwlViTProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"OwlViTImageProcessor\"\n-    tokenizer_class = (\"CLIPTokenizer\", \"CLIPTokenizerFast\")\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "0c28f0eb4631c29198e6e3e615d14e5afc5694ae",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -108,10 +108,6 @@ class PaliGemmaProcessor(ProcessorMixin):\n             in a chat into a tokenizable string.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")\n-    tokenizer_class = (\"GemmaTokenizer\", \"GemmaTokenizerFast\")\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "9d69f1458b60d8641f37fe35f3addc4d20e39614",
            "filename": "src/transformers/models/parakeet/processing_parakeet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fparakeet%2Fprocessing_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fparakeet%2Fprocessing_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fprocessing_parakeet.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -39,9 +39,8 @@ class ParakeetProcessorKwargs(ProcessingKwargs, total=False):\n \n \n class ParakeetProcessor(ProcessorMixin):\n-    attributes = [\"feature_extractor\", \"tokenizer\"]\n-    feature_extractor_class = \"ParakeetFeatureExtractor\"\n-    tokenizer_class = \"ParakeetTokenizerFast\"\n+    def __init__(self, feature_extractor, tokenizer):\n+        super().__init__(feature_extractor, tokenizer)\n \n     def __call__(\n         self,"
        },
        {
            "sha": "412996873807686fcdcf87eb64d669ced18782a5",
            "filename": "src/transformers/models/perception_lm/processing_perception_lm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -62,11 +62,6 @@ class PerceptionLMProcessor(ProcessorMixin):\n             Pooling ratio for vision tokens. If not 1, 2D adaptive pooling is applied over projected vision tokens.\n     \"\"\"\n \n-    attributes = [\"video_processor\", \"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    video_processor_class = \"AutoVideoProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         video_processor=None,"
        },
        {
            "sha": "8eec69b0448e31a892015a339398ce5b5d7f64bb",
            "filename": "src/transformers/models/phi4_multimodal/processing_phi4_multimodal.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -58,9 +58,6 @@ class Phi4MultimodalProcessor(ProcessorMixin):\n             The fake audio token pattern.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"audio_processor\", \"tokenizer\"]\n-    tokenizer_class = \"GPT2TokenizerFast\"\n-    image_processor_class = \"Phi4MultimodalImageProcessorFast\"\n     audio_processor_class = \"Phi4MultimodalFeatureExtractor\"\n \n     def __init__("
        },
        {
            "sha": "b7446cb696841a903a6c6735a2e0d771cd256170",
            "filename": "src/transformers/models/pix2struct/processing_pix2struct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -61,10 +61,6 @@ class Pix2StructProcessor(ProcessorMixin):\n             An instance of ['T5TokenizerFast`] or ['T5Tokenizer`]. The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"Pix2StructImageProcessor\"\n-    tokenizer_class = (\"T5Tokenizer\", \"T5TokenizerFast\")\n-\n     def __init__(self, image_processor, tokenizer):\n         tokenizer.return_token_type_ids = False\n         super().__init__(image_processor, tokenizer)"
        },
        {
            "sha": "b62deee98300f0d993524bc0a53ccbce0d850cb1",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -87,10 +87,6 @@ class PixtralProcessor(ProcessorMixin):\n             Special token used to denote the end of an image input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "a68168e36739a1d2ac14c285665cfd4baefb388d",
            "filename": "src/transformers/models/pop2piano/processing_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -42,10 +42,6 @@ class Pop2PianoProcessor(ProcessorMixin):\n             An instance of ['Pop2PianoTokenizer`]. The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"feature_extractor\", \"tokenizer\"]\n-    feature_extractor_class = \"Pop2PianoFeatureExtractor\"\n-    tokenizer_class = \"Pop2PianoTokenizer\"\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n "
        },
        {
            "sha": "ead9dbe10da4e546cda1dd11a8b872a390b6d13c",
            "filename": "src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -89,12 +89,6 @@ class Qwen2_5OmniProcessor(ProcessorMixin):\n             The Jinja template to use for formatting the conversation. If not provided, the default chat template is used.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"video_processor\", \"feature_extractor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    video_processor_class = \"AutoVideoProcessor\"\n-    feature_extractor_class = \"WhisperFeatureExtractor\"\n-    tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n-\n     def __init__(\n         self, image_processor=None, video_processor=None, feature_extractor=None, tokenizer=None, chat_template=None\n     ):"
        },
        {
            "sha": "164483fc00e840815f86f71a66e9187f44237027",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -858,8 +858,6 @@ class Qwen2_5_VLProcessor(Qwen2VLProcessor):\n             in a chat into a tokenizable string.\n     \"\"\"\n \n-    image_processor_class = \"AutoImageProcessor\"\n-\n     @property\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names"
        },
        {
            "sha": "8734f56b94180c100d617a040c5769be030cba0d",
            "filename": "src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -60,12 +60,6 @@ class Qwen2_5_VLProcessor(ProcessorMixin):\n             in a chat into a tokenizable string.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\", \"video_processor\"]\n-\n-    image_processor_class = \"AutoImageProcessor\"\n-    video_processor_class = \"AutoVideoProcessor\"\n-    tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n-\n     def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n         self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token"
        },
        {
            "sha": "449480df4588e86b90541410edb48d5890d3312b",
            "filename": "src/transformers/models/qwen2_audio/processing_qwen2_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -57,10 +57,6 @@ class Qwen2AudioProcessor(ProcessorMixin):\n             The token to use for audio eos tokens.\n     \"\"\"\n \n-    attributes = [\"feature_extractor\", \"tokenizer\"]\n-    feature_extractor_class = \"WhisperFeatureExtractor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         feature_extractor=None,"
        },
        {
            "sha": "e9487a8197bfe6bdb11bea4b37a1da2f8d09c21d",
            "filename": "src/transformers/models/qwen2_vl/processing_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -61,11 +61,6 @@ class Qwen2VLProcessor(ProcessorMixin):\n             in a chat into a tokenizable string.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\", \"video_processor\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    video_processor_class = \"AutoVideoProcessor\"\n-    tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n-\n     def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n         self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token"
        },
        {
            "sha": "ceacd2b854d24ff4c2c8dfb46f4600a6215dc348",
            "filename": "src/transformers/models/qwen3_omni_moe/processing_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -102,12 +102,6 @@ class Qwen3OmniMoeProcessor(ProcessorMixin):\n             The Jinja template to use for formatting the conversation. If not provided, the default chat template is used.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"video_processor\", \"feature_extractor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    video_processor_class = \"AutoVideoProcessor\"\n-    feature_extractor_class = \"WhisperFeatureExtractor\"\n-    tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n-\n     def __init__(\n         self, image_processor=None, video_processor=None, feature_extractor=None, tokenizer=None, chat_template=None\n     ):"
        },
        {
            "sha": "b86e3c282ed448715415d8a19be9120c325f33c2",
            "filename": "src/transformers/models/qwen3_vl/processing_qwen3_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -60,11 +60,6 @@ class Qwen3VLProcessor(ProcessorMixin):\n             in a chat into a tokenizable string.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\", \"video_processor\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    video_processor_class = \"AutoVideoProcessor\"\n-    tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n-\n     def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n         self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token"
        },
        {
            "sha": "cda5d7ed5aeb3e4d78702fc5583d6b3979d76116",
            "filename": "src/transformers/models/sam/processing_sam.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -65,9 +65,6 @@ class SamProcessor(ProcessorMixin):\n             An instance of [`SamImageProcessor`]. The image processor is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\"]\n-    image_processor_class = \"SamImageProcessor\"\n-\n     def __init__(self, image_processor):\n         super().__init__(image_processor)\n         self.target_size = self.image_processor.size[\"longest_edge\"]"
        },
        {
            "sha": "21a5f9dc5913174f7d8f7808a1ddede8dcd4d74e",
            "filename": "src/transformers/models/sam2/processing_sam2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -52,9 +52,6 @@ class Sam2Processor(ProcessorMixin):\n             The value used for padding input points.\n     \"\"\"\n \n-    attributes = [\"image_processor\"]\n-    image_processor_class = \"Sam2ImageProcessorFast\"\n-\n     def __init__(self, image_processor, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs):\n         super().__init__(image_processor, **kwargs)\n         self.point_pad_value = point_pad_value"
        },
        {
            "sha": "80422bef2333cd9cb7752455f6b16f1483a9b279",
            "filename": "src/transformers/models/sam2_video/modular_sam2_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -620,10 +620,6 @@ class Sam2VideoProcessor(Sam2Processor):\n             The value used for padding input points.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"video_processor\"]\n-    image_processor_class = \"Sam2ImageProcessorFast\"\n-    video_processor_class = \"Sam2VideoVideoProcessor\"\n-\n     def __init__(\n         self, image_processor, video_processor, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs\n     ):"
        },
        {
            "sha": "839449ba505dbe6066015dce096021e97411ee46",
            "filename": "src/transformers/models/sam2_video/processing_sam2_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -53,10 +53,6 @@ class Sam2VideoProcessor(ProcessorMixin):\n             The value used for padding input points.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"video_processor\"]\n-    image_processor_class = \"Sam2ImageProcessorFast\"\n-    video_processor_class = \"Sam2VideoVideoProcessor\"\n-\n     def __init__(\n         self, image_processor, video_processor, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs\n     ):"
        },
        {
            "sha": "1434a9ca5a2d715d7e0c6c41e303a49a7b15b475",
            "filename": "src/transformers/models/sam_hq/processing_samhq.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -65,9 +65,6 @@ class SamHQProcessor(ProcessorMixin):\n             An instance of [`SamImageProcessor`]. The image processor is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\"]\n-    image_processor_class = \"SamImageProcessor\"\n-\n     def __init__(self, image_processor):\n         super().__init__(image_processor)\n         # Ensure image_processor is properly initialized"
        },
        {
            "sha": "a506d81af61d582368dea0bb41dfe4cfd554cd5a",
            "filename": "src/transformers/models/seamless_m4t/processing_seamless_m4t.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -54,8 +54,6 @@ class SeamlessM4TProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    feature_extractor_class = \"SeamlessM4TFeatureExtractor\"\n-    tokenizer_class = (\"SeamlessM4TTokenizer\", \"SeamlessM4TTokenizerFast\")\n     valid_processor_kwargs = SeamlessM4TProcessorKwargs\n \n     def __init__(self, feature_extractor, tokenizer):"
        },
        {
            "sha": "2d63eacd1747a38718b249caf643aac2b926c632",
            "filename": "src/transformers/models/siglip/processing_siglip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -33,10 +33,6 @@ class SiglipProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "fe33ad11dbe75af02ae14c0b97f8c6d08858a196",
            "filename": "src/transformers/models/siglip2/processing_siglip2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -47,10 +47,6 @@ class Siglip2Processor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n     valid_processor_kwargs = Siglip2ProcessorKwargs\n \n     def __init__(self, image_processor, tokenizer):"
        },
        {
            "sha": "2ce6465ee971c3b040d0962747d426c68e17db07",
            "filename": "src/transformers/models/smolvlm/processing_smolvlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -141,11 +141,6 @@ class SmolVLMProcessor(ProcessorMixin):\n             in a chat into a tokenizable string.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\", \"video_processor\"]\n-    image_processor_class = \"SmolVLMImageProcessor\"\n-    video_processor_class = \"SmolVLMVideoProcessor\"  # NOTE: uses different interpolation than slow processors\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         image_processor,"
        },
        {
            "sha": "ffcb4e3d4497e2fdd9d97913c38922c61d9b90ae",
            "filename": "src/transformers/models/speech_to_text/processing_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fprocessing_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fprocessing_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fprocessing_speech_to_text.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -37,9 +37,6 @@ class Speech2TextProcessor(ProcessorMixin):\n             An instance of [`Speech2TextTokenizer`]. The tokenizer is a required input.\n     \"\"\"\n \n-    feature_extractor_class = \"Speech2TextFeatureExtractor\"\n-    tokenizer_class = \"Speech2TextTokenizer\"\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n "
        },
        {
            "sha": "bfac305ab641796f6e9de86e4bda65b638150376",
            "filename": "src/transformers/models/speecht5/processing_speecht5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fprocessing_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fprocessing_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fprocessing_speecht5.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -31,9 +31,6 @@ class SpeechT5Processor(ProcessorMixin):\n             An instance of [`SpeechT5Tokenizer`]. The tokenizer is a required input.\n     \"\"\"\n \n-    feature_extractor_class = \"SpeechT5FeatureExtractor\"\n-    tokenizer_class = \"SpeechT5Tokenizer\"\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n "
        },
        {
            "sha": "366bb0850d2ddb3b230c69ffb7f066058efa004b",
            "filename": "src/transformers/models/trocr/processing_trocr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -43,10 +43,6 @@ class TrOCRProcessor(ProcessorMixin):\n             An instance of [`RobertaTokenizer`/`XLMRobertaTokenizer`]. The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "259246962d27c3e02438913a187cca47fd3e68d8",
            "filename": "src/transformers/models/tvp/processing_tvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Ftvp%2Fprocessing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Ftvp%2Fprocessing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fprocessing_tvp.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -44,10 +44,6 @@ class TvpProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"TvpImageProcessor\"\n-    tokenizer_class = (\"BertTokenizer\", \"BertTokenizerFast\")\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n         self.video_processor = image_processor"
        },
        {
            "sha": "5e37a021e6abc139ea97038d29ba0f80ac1cd1c9",
            "filename": "src/transformers/models/udop/processing_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -73,10 +73,6 @@ class UdopProcessor(ProcessorMixin):\n             An instance of [`UdopTokenizer`] or [`UdopTokenizerFast`]. The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"LayoutLMv3ImageProcessor\"\n-    tokenizer_class = (\"UdopTokenizer\", \"UdopTokenizerFast\")\n-\n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "d5ea2c75e9d87e95489239febd9bafa68cd211f3",
            "filename": "src/transformers/models/video_llama_3/processing_video_llama_3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fprocessing_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fprocessing_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fprocessing_video_llama_3.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -57,11 +57,6 @@ class VideoLlama3Processor(ProcessorMixin):\n         chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\", \"video_processor\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    video_processor_class = \"AutoVideoProcessor\"\n-    tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n-\n     def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n         self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token"
        },
        {
            "sha": "8d6d916834e81821313bc7b95c44fb7f8aa01788",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -60,11 +60,6 @@ class VideoLlavaProcessor(ProcessorMixin):\n             extra tokens appended, no need to set this arg.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"video_processor\", \"tokenizer\"]\n-    image_processor_class = \"VideoLlavaImageProcessor\"\n-    video_processor_class = \"AutoVideoProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "26738d890d6563b7084031f7331dfa5bc0bd2de6",
            "filename": "src/transformers/models/vilt/processing_vilt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -48,9 +48,6 @@ class ViltProcessor(ProcessorMixin):\n             An instance of ['BertTokenizerFast`]. The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"ViltImageProcessor\"\n-    tokenizer_class = (\"BertTokenizer\", \"BertTokenizerFast\")\n     valid_processor_kwargs = ViltProcessorKwargs\n \n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):"
        },
        {
            "sha": "cc1cd01e50c6a2265cd29e5697e57ad838cc2936",
            "filename": "src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -39,10 +39,6 @@ class VisionTextDualEncoderProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"AutoImageProcessor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "f07904a896904ce26e61acc6951993134e12868c",
            "filename": "src/transformers/models/voxtral/processing_voxtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -74,10 +74,6 @@ class VoxtralProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"feature_extractor\", \"tokenizer\"]\n-    feature_extractor_class = \"WhisperFeatureExtractor\"\n-    tokenizer_class = \"MistralCommonTokenizer\"\n-\n     def __init__(\n         self,\n         feature_extractor,"
        },
        {
            "sha": "8a8b7ded7116d1a1209b61ceca0a5ba89630fa62",
            "filename": "src/transformers/models/wav2vec2/processing_wav2vec2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -44,9 +44,6 @@ class Wav2Vec2Processor(ProcessorMixin):\n             An instance of [`PreTrainedTokenizer`]. The tokenizer is a required input.\n     \"\"\"\n \n-    feature_extractor_class = \"Wav2Vec2FeatureExtractor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n "
        },
        {
            "sha": "90da8b651677055979415e5818660faae488d0bd",
            "filename": "src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -44,9 +44,6 @@ class Wav2Vec2BertProcessor(ProcessorMixin):\n             An instance of [`PreTrainedTokenizer`]. The tokenizer is a required input.\n     \"\"\"\n \n-    feature_extractor_class = \"SeamlessM4TFeatureExtractor\"\n-    tokenizer_class = \"AutoTokenizer\"\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n "
        },
        {
            "sha": "71973334dfd6ce2d56a2cd83302862567203b4ca",
            "filename": "src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fwav2vec2_with_lm%2Fprocessing_wav2vec2_with_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fwav2vec2_with_lm%2Fprocessing_wav2vec2_with_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_with_lm%2Fprocessing_wav2vec2_with_lm.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -80,9 +80,6 @@ class Wav2Vec2ProcessorWithLM(ProcessorMixin):\n             An instance of [`pyctcdecode.BeamSearchDecoderCTC`]. The decoder is a required input.\n     \"\"\"\n \n-    feature_extractor_class = \"AutoFeatureExtractor\"\n-    tokenizer_class = \"Wav2Vec2CTCTokenizer\"\n-\n     def __init__(\n         self,\n         feature_extractor: \"FeatureExtractionMixin\","
        },
        {
            "sha": "e71a7a545281fbcc706542ba4aae6b1e8aa4f114",
            "filename": "src/transformers/models/whisper/processing_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fwhisper%2Fprocessing_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fwhisper%2Fprocessing_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fprocessing_whisper.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -34,9 +34,6 @@ class WhisperProcessor(ProcessorMixin):\n             An instance of [`WhisperTokenizer`]. The tokenizer is a required input.\n     \"\"\"\n \n-    feature_extractor_class = \"WhisperFeatureExtractor\"\n-    tokenizer_class = (\"WhisperTokenizer\", \"WhisperTokenizerFast\")\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n "
        },
        {
            "sha": "ae31cd075b8a95a2e4246d7cd35d259cb1b55403",
            "filename": "src/transformers/models/x_clip/processing_x_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fx_clip%2Fprocessing_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fmodels%2Fx_clip%2Fprocessing_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fprocessing_x_clip.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -23,20 +23,16 @@ class XCLIPProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs an X-CLIP processor which wraps a VideoMAE image processor and a CLIP tokenizer into a single processor.\n \n-    [`XCLIPProcessor`] offers all the functionalities of [`VideoMAEImageProcessor`] and [`CLIPTokenizerFast`]. See the\n+    [`XCLIPProcessor`] offers all the functionalities of [`CLIPImageProcessor`] and [`CLIPTokenizerFast`]. See the\n     [`~XCLIPProcessor.__call__`] and [`~XCLIPProcessor.decode`] for more information.\n \n     Args:\n-        image_processor ([`VideoMAEImageProcessor`], *optional*):\n+        image_processor ([`CLIPImageProcessor`], *optional*):\n             The image processor is a required input.\n         tokenizer ([`CLIPTokenizerFast`], *optional*):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"VideoMAEImageProcessor\"\n-    tokenizer_class = (\"CLIPTokenizer\", \"CLIPTokenizerFast\")\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n         self.video_processor = self.image_processor"
        },
        {
            "sha": "232503a8adf1c05b563631f2a18cf7cfb18d0718",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 104,
            "deletions": 71,
            "changes": 175,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -35,24 +35,6 @@\n from .dynamic_module_utils import custom_object_save\n from .feature_extraction_utils import BatchFeature\n from .image_utils import ChannelDimension, ImageInput, is_vision_available\n-from .utils.chat_template_utils import render_jinja_template\n-from .utils.type_validators import (\n-    device_validator,\n-    image_size_validator,\n-    padding_validator,\n-    positive_any_number,\n-    positive_int,\n-    resampling_validator,\n-    tensor_type_validator,\n-    truncation_validator,\n-    video_metadata_validator,\n-)\n-from .video_utils import VideoInput, VideoMetadataType\n-\n-\n-if is_vision_available():\n-    from .image_utils import PILImageResampling\n-\n from .tokenization_utils_base import (\n     PaddingStrategy,\n     PreTokenizedInput,\n@@ -78,12 +60,27 @@\n     list_repo_templates,\n     logging,\n )\n+from .utils.chat_template_utils import render_jinja_template\n from .utils.deprecation import deprecate_kwarg\n+from .utils.type_validators import (\n+    device_validator,\n+    image_size_validator,\n+    padding_validator,\n+    positive_any_number,\n+    positive_int,\n+    resampling_validator,\n+    tensor_type_validator,\n+    truncation_validator,\n+    video_metadata_validator,\n+)\n+from .video_utils import VideoInput, VideoMetadataType\n \n \n if is_torch_available():\n     from .modeling_utils import PreTrainedAudioTokenizerBase\n \n+if is_vision_available():\n+    from .image_utils import PILImageResampling\n \n logger = logging.get_logger(__name__)\n \n@@ -94,11 +91,43 @@\n transformers_module = direct_transformers_import(Path(__file__).parent)\n \n \n-AUTO_TO_BASE_CLASS_MAPPING = {\n-    \"AutoTokenizer\": \"PreTrainedTokenizerBase\",\n-    \"AutoFeatureExtractor\": \"FeatureExtractionMixin\",\n-    \"AutoImageProcessor\": \"ImageProcessingMixin\",\n-    \"AutoVideoProcessor\": \"BaseVideoProcessor\",\n+class _LazyAutoProcessorMapping(dict):\n+    \"\"\"\n+    Lazy dictionary to avoid circular imports.\n+    The mapping names are only imported when accessed.\n+    \"\"\"\n+\n+    _MAPPING_NAMES = {\n+        \"image_processor\": (\"transformers.models.auto.image_processing_auto\", \"AutoImageProcessor\"),\n+        \"video_processor\": (\"transformers.models.auto.video_processing_auto\", \"AutoVideoProcessor\"),\n+        \"feature_extractor\": (\"transformers.models.auto.feature_extraction_auto\", \"AutoFeatureExtractor\"),\n+        \"audio_processor\": (\"transformers.models.auto.feature_extraction_auto\", \"AutoFeatureExtractor\"),\n+        \"tokenizer\": (\"transformers.models.auto.tokenization_auto\", \"AutoTokenizer\"),\n+    }\n+\n+    def __getitem__(self, key):\n+        if key not in self._MAPPING_NAMES:\n+            raise KeyError(key)\n+        module_name, attr_name = self._MAPPING_NAMES[key]\n+        module = __import__(module_name, fromlist=[attr_name])\n+        return getattr(module, attr_name)\n+\n+    def __contains__(self, key):\n+        return key in self._MAPPING_NAMES\n+\n+    def keys(self):\n+        return self._MAPPING_NAMES.keys()\n+\n+\n+MODALITY_TO_AUTOPROCESSOR_MAPPING = _LazyAutoProcessorMapping()\n+\n+MODALITY_TO_BASE_CLASS_MAPPING = {\n+    \"audio_tokenizer\": \"DacModel\",\n+    \"audio_processor\": \"FeatureExtractionMixin\",\n+    \"tokenizer\": (\"PreTrainedTokenizerBase\", \"MistralCommonTokenizer\"),\n+    \"feature_extractor\": \"FeatureExtractionMixin\",\n+    \"image_processor\": \"ImageProcessingMixin\",\n+    \"video_processor\": \"BaseVideoProcessor\",\n }\n \n if sys.version_info >= (3, 11):\n@@ -526,11 +555,7 @@ class ProcessorMixin(PushToHubMixin):\n     This is a mixin used to provide saving/loading functionality for all processor classes.\n     \"\"\"\n \n-    attributes = [\"feature_extractor\", \"tokenizer\"]\n-    optional_call_args: list[str] = []\n     # Names need to be attr_class for attr in attributes\n-    feature_extractor_class = None\n-    tokenizer_class = None\n     _auto_class = None\n     valid_processor_kwargs = ProcessingKwargs\n \n@@ -551,17 +576,17 @@ def __init__(self, *args, **kwargs):\n \n         # Sanitize args and kwargs\n         for key in kwargs:\n-            if key not in self.attributes:\n+            if key not in self.get_attributes():\n                 raise TypeError(f\"Unexpected keyword argument {key}.\")\n-        for arg, attribute_name in zip(args, self.attributes):\n+        for arg, attribute_name in zip(args, self.get_attributes()):\n             if attribute_name in kwargs:\n                 raise TypeError(f\"Got multiple values for argument {attribute_name}.\")\n             else:\n                 kwargs[attribute_name] = arg\n \n-        if len(kwargs) != len(self.attributes):\n+        if len(kwargs) != len(self.get_attributes()):\n             raise ValueError(\n-                f\"This processor requires {len(self.attributes)} arguments: {', '.join(self.attributes)}. Got \"\n+                f\"This processor requires {len(self.get_attributes())} arguments: {', '.join(self.get_attributes())}. Got \"\n                 f\"{len(args)} arguments instead.\"\n             )\n \n@@ -621,7 +646,7 @@ def __call__(\n             \"feature_extractor\": (audio, \"audio_kwargs\"),\n         }\n         outputs = {}\n-        for attribute_name in self.attributes:\n+        for attribute_name in self.get_attributes():\n             attribute = getattr(self, attribute_name, None)\n             input_data, input_kwargs = attribute_to_kwargs[attribute_name]\n             if input_data is not None and attribute is not None:\n@@ -636,9 +661,9 @@ def check_argument_for_proper_class(self, argument_name, argument):\n         mismatch between expected and actual class, an error is raise. Otherwise, the proper retrieved class\n         is returned.\n         \"\"\"\n-        class_name = getattr(self, f\"{argument_name}_class\")\n-        # Nothing is ever going to be an instance of \"AutoXxx\", in that case we check the base class.\n-        class_name = AUTO_TO_BASE_CLASS_MAPPING.get(class_name, class_name)\n+        if argument_name not in MODALITY_TO_BASE_CLASS_MAPPING and \"tokenizer\" in argument_name:\n+            argument_name = \"tokenizer\"\n+        class_name = MODALITY_TO_BASE_CLASS_MAPPING.get(argument_name)\n         if isinstance(class_name, tuple):\n             proper_class = tuple(self.get_possibly_dynamic_module(n) for n in class_name if n is not None)\n         else:\n@@ -664,7 +689,7 @@ def to_dict(self) -> dict[str, Any]:\n         sig = inspect.signature(self.__init__)\n         # Only save the attributes that are presented in the kwargs of `__init__`.\n         # or in the attributes\n-        attrs_to_save = list(sig.parameters) + self.__class__.attributes\n+        attrs_to_save = list(sig.parameters) + self.__class__.get_attributes()\n         # extra attributes to be kept\n         attrs_to_save += [\"auto_map\"]\n \n@@ -748,7 +773,7 @@ def to_json_file(self, json_file_path: Union[str, os.PathLike]):\n             writer.write(self.to_json_string())\n \n     def __repr__(self):\n-        attributes_repr = [f\"- {name}: {repr(getattr(self, name))}\" for name in self.attributes]\n+        attributes_repr = [f\"- {name}: {repr(getattr(self, name))}\" for name in self.get_attributes()]\n         attributes_repr = \"\\n\".join(attributes_repr)\n         return f\"{self.__class__.__name__}:\\n{attributes_repr}\\n\\n{self.to_json_string()}\"\n \n@@ -786,25 +811,29 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n         # If we have a custom config, we copy the file defining it in the folder and set the attributes so it can be\n         # loaded from the Hub.\n         if self._auto_class is not None:\n-            attrs = [getattr(self, attribute_name) for attribute_name in self.attributes]\n+            attrs = [getattr(self, attribute_name) for attribute_name in self.get_attributes()]\n             configs = [(a.init_kwargs if isinstance(a, PreTrainedTokenizerBase) else a) for a in attrs]\n             configs.append(self)\n             custom_object_save(self, save_directory, config=configs)\n \n-        for attribute_name in self.attributes:\n+        for attribute_name in self.get_attributes():\n             attribute = getattr(self, attribute_name)\n             if hasattr(attribute, \"_set_processor_class\"):\n                 attribute._set_processor_class(self.__class__.__name__)\n \n             # Save the tokenizer in its own vocab file. The other attributes are saved as part of `processor_config.json`\n             if attribute_name == \"tokenizer\":\n                 attribute.save_pretrained(save_directory)\n+            # if a model has multiple tokenizers, save the additional tokenizers in their own folders.\n+            # Note that the additional tokenizers must have \"tokenizer\" in their attribute name.\n+            elif \"tokenizer\" in attribute_name:\n+                attribute.save_pretrained(os.path.join(save_directory, attribute_name))\n             elif attribute._auto_class is not None:\n                 custom_object_save(attribute, save_directory, config=attribute)\n \n         if self._auto_class is not None:\n             # We added an attribute to the init_kwargs of the tokenizers, which needs to be cleaned up.\n-            for attribute_name in self.attributes:\n+            for attribute_name in self.get_attributes():\n                 attribute = getattr(self, attribute_name)\n                 if isinstance(attribute, PreTrainedTokenizerBase):\n                     del attribute.init_kwargs[\"auto_map\"]\n@@ -1121,7 +1150,7 @@ def from_args_and_dict(cls, args, processor_dict: dict[str, Any], **kwargs):\n \n         # We have to pop up some unused (but specific) kwargs and then validate that it doesn't contain unused kwargs\n         # If we don't pop, some specific kwargs will raise a warning or error\n-        for unused_kwarg in cls.attributes + [\"auto_map\", \"processor_class\"]:\n+        for unused_kwarg in cls.get_attributes() + [\"auto_map\", \"processor_class\"]:\n             processor_dict.pop(unused_kwarg, None)\n \n         # override processor_dict with given kwargs\n@@ -1374,6 +1403,18 @@ def from_pretrained(\n         processor_dict, kwargs = cls.get_processor_dict(pretrained_model_name_or_path, **kwargs)\n         return cls.from_args_and_dict(args, processor_dict, **kwargs)\n \n+    @classmethod\n+    def get_attributes(cls):\n+        args_in_init = inspect.signature(cls.__init__).parameters.keys()\n+        attributes = []\n+        for sub_processor_type in args_in_init:\n+            # don't treat audio_tokenizer as an attribute\n+            if sub_processor_type == \"audio_tokenizer\":\n+                continue\n+            if sub_processor_type in MODALITY_TO_AUTOPROCESSOR_MAPPING or \"tokenizer\" in sub_processor_type:\n+                attributes.append(sub_processor_type)\n+        return attributes\n+\n     @classmethod\n     def register_for_auto_class(cls, auto_class=\"AutoProcessor\"):\n         \"\"\"\n@@ -1399,37 +1440,29 @@ def register_for_auto_class(cls, auto_class=\"AutoProcessor\"):\n     @classmethod\n     def _get_arguments_from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n         \"\"\"\n-        Identify and instantiate the subcomponents of Processor classes, like image processors and\n-        tokenizers. This method uses the Processor attributes like `tokenizer_class` to figure out what class those\n-        subcomponents should be. Note that any subcomponents must either be library classes that are accessible in\n-        the `transformers` root, or they must be custom code that has been registered with the relevant autoclass,\n-        via methods like `AutoTokenizer.register()`. If neither of these conditions are fulfilled, this method\n-        will be unable to find the relevant subcomponent class and will raise an error.\n+        Identify and instantiate the subcomponents of Processor classes, such as image processors, tokenizers,\n+        and feature extractors. This method inspects the processor's `__init__` signature to identify parameters\n+        that correspond to known modality types (image_processor, tokenizer, feature_extractor, etc.) or contain\n+        \"tokenizer\" in their name. It then uses the appropriate Auto class (AutoImageProcessor, AutoTokenizer, etc.)\n+        from `MODALITY_TO_AUTOPROCESSOR_MAPPING` to load each subcomponent via `.from_pretrained()`. For tokenizer-like\n+        parameters not explicitly in the mapping, the method uses AutoTokenizer with a subfolder argument.\n         \"\"\"\n         args = []\n-        for attribute_name in cls.attributes:\n-            class_name = getattr(cls, f\"{attribute_name}_class\")\n-            if isinstance(class_name, tuple):\n-                classes = tuple(cls.get_possibly_dynamic_module(n) if n is not None else None for n in class_name)\n-                if attribute_name == \"image_processor\":\n-                    # TODO: @yoni, change logic in v4.52 (when use_fast set to True by default)\n-                    use_fast = kwargs.get(\"use_fast\")\n-                    if use_fast is None:\n-                        logger.warning_once(\n-                            \"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. \"\n-                            \"`use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. \"\n-                            \"This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\"\n-                        )\n-                else:\n-                    use_fast = kwargs.get(\"use_fast\", True)\n-                if use_fast and classes[1] is not None:\n-                    attribute_class = classes[1]\n-                else:\n-                    attribute_class = classes[0]\n-            else:\n-                attribute_class = cls.get_possibly_dynamic_module(class_name)\n-\n-            args.append(attribute_class.from_pretrained(pretrained_model_name_or_path, **kwargs))\n+        # get args from processor init signature\n+        sub_processors = cls.get_attributes()\n+        for sub_processor_type in sub_processors:\n+            if sub_processor_type in MODALITY_TO_AUTOPROCESSOR_MAPPING:\n+                auto_processor_class = MODALITY_TO_AUTOPROCESSOR_MAPPING[sub_processor_type]\n+                sub_processor = auto_processor_class.from_pretrained(pretrained_model_name_or_path, **kwargs)\n+                args.append(sub_processor)\n+            elif \"tokenizer\" in sub_processor_type:\n+                # Special case: tokenizer-like parameters not in the mapping (e.g., \"protein_tokenizer\")\n+                # Load using AutoTokenizer with subfolder\n+                auto_processor_class = MODALITY_TO_AUTOPROCESSOR_MAPPING[\"tokenizer\"]\n+                sub_processor = auto_processor_class.from_pretrained(\n+                    pretrained_model_name_or_path, subfolder=sub_processor_type, **kwargs\n+                )\n+                args.append(sub_processor)\n \n         return args\n \n@@ -1479,7 +1512,7 @@ def decode(self, *args, **kwargs):\n     @property\n     def model_input_names(self):\n         model_input_names = []\n-        for attribute_name in self.attributes:\n+        for attribute_name in self.get_attributes():\n             attribute = getattr(self, attribute_name, None)\n             attr_input_names = getattr(attribute, \"model_input_names\")\n             model_input_names.extend(attr_input_names)"
        },
        {
            "sha": "bb799abdd243eb61ac581e65e5e70239f4a446a5",
            "filename": "tests/models/align/test_processing_align.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Falign%2Ftest_processing_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Falign%2Ftest_processing_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falign%2Ftest_processing_align.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -70,6 +70,11 @@ def setUp(self):\n         processor = AlignProcessor(tokenizer=self.get_tokenizer(), image_processor=image_processor)\n         processor.save_pretrained(self.tmpdirname)\n \n+        image_processor = EfficientNetImageProcessor.from_pretrained(self.tmpdirname)\n+        image_processor.save_pretrained(self.tmpdirname)\n+        tokenizer = BertTokenizer.from_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(self.tmpdirname)\n+\n     def get_tokenizer(self, **kwargs):\n         return BertTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n "
        },
        {
            "sha": "3719c89b566049f89ae587df6558eff5c19fc722",
            "filename": "tests/models/auto/test_processor_auto.py",
            "status": "modified",
            "additions": 14,
            "deletions": 17,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -173,14 +173,16 @@ def test_processor_from_local_directory_from_model_config(self):\n     def test_from_pretrained_dynamic_processor(self):\n         # If remote code is not set, we will time out when asking whether to load the model.\n         with self.assertRaises(ValueError):\n-            processor = AutoProcessor.from_pretrained(\"hf-internal-testing/test_dynamic_processor\")\n+            processor = AutoProcessor.from_pretrained(\"hf-internal-testing/test_dynamic_processor_updated\")\n         # If remote code is disabled, we can't load this config.\n         with self.assertRaises(ValueError):\n             processor = AutoProcessor.from_pretrained(\n-                \"hf-internal-testing/test_dynamic_processor\", trust_remote_code=False\n+                \"hf-internal-testing/test_dynamic_processor_updated\", trust_remote_code=False\n             )\n \n-        processor = AutoProcessor.from_pretrained(\"hf-internal-testing/test_dynamic_processor\", trust_remote_code=True)\n+        processor = AutoProcessor.from_pretrained(\n+            \"hf-internal-testing/test_dynamic_processor_updated\", trust_remote_code=True\n+        )\n         self.assertTrue(processor.special_attribute_present)\n         self.assertEqual(processor.__class__.__name__, \"NewProcessor\")\n \n@@ -195,7 +197,7 @@ def test_from_pretrained_dynamic_processor(self):\n \n             # Test we can also load the slow version\n             new_processor = AutoProcessor.from_pretrained(\n-                \"hf-internal-testing/test_dynamic_processor\", trust_remote_code=True, use_fast=False\n+                \"hf-internal-testing/test_dynamic_processor_updated\", trust_remote_code=True, use_fast=False\n             )\n             new_tokenizer = new_processor.tokenizer\n             self.assertTrue(new_tokenizer.special_attribute_present)\n@@ -249,25 +251,26 @@ class NewTokenizer(BertTokenizer):\n             special_attribute_present = False\n \n         class NewProcessor(ProcessorMixin):\n-            feature_extractor_class = \"AutoFeatureExtractor\"\n-            tokenizer_class = \"AutoTokenizer\"\n             special_attribute_present = False\n \n+            def __init__(self, feature_extractor, tokenizer):\n+                super().__init__(feature_extractor, tokenizer)\n+\n         try:\n             AutoConfig.register(\"custom\", CustomConfig)\n             AutoFeatureExtractor.register(CustomConfig, NewFeatureExtractor)\n             AutoTokenizer.register(CustomConfig, slow_tokenizer_class=NewTokenizer)\n             AutoProcessor.register(CustomConfig, NewProcessor)\n             # If remote code is not set, the default is to use local classes.\n-            processor = AutoProcessor.from_pretrained(\"hf-internal-testing/test_dynamic_processor\")\n+            processor = AutoProcessor.from_pretrained(\"hf-internal-testing/test_dynamic_processor_updated\")\n             self.assertEqual(processor.__class__.__name__, \"NewProcessor\")\n             self.assertFalse(processor.special_attribute_present)\n             self.assertFalse(processor.feature_extractor.special_attribute_present)\n             self.assertFalse(processor.tokenizer.special_attribute_present)\n \n             # If remote code is disabled, we load the local ones.\n             processor = AutoProcessor.from_pretrained(\n-                \"hf-internal-testing/test_dynamic_processor\", trust_remote_code=False\n+                \"hf-internal-testing/test_dynamic_processor_updated\", trust_remote_code=False\n             )\n             self.assertEqual(processor.__class__.__name__, \"NewProcessor\")\n             self.assertFalse(processor.special_attribute_present)\n@@ -276,7 +279,7 @@ class NewProcessor(ProcessorMixin):\n \n             # If remote is enabled, we load from the Hub.\n             processor = AutoProcessor.from_pretrained(\n-                \"hf-internal-testing/test_dynamic_processor\", trust_remote_code=True\n+                \"hf-internal-testing/test_dynamic_processor_updated\", trust_remote_code=True\n             )\n             self.assertEqual(processor.__class__.__name__, \"NewProcessor\")\n             self.assertTrue(processor.special_attribute_present)\n@@ -303,9 +306,6 @@ class NewTokenizer(BertTokenizer):\n             pass\n \n         class NewProcessor(ProcessorMixin):\n-            feature_extractor_class = \"AutoFeatureExtractor\"\n-            tokenizer_class = \"AutoTokenizer\"\n-\n             def __init__(self, feature_extractor, tokenizer, processor_attr_1=1, processor_attr_2=True):\n                 super().__init__(feature_extractor, tokenizer)\n \n@@ -319,7 +319,7 @@ def __init__(self, feature_extractor, tokenizer, processor_attr_1=1, processor_a\n             AutoProcessor.register(CustomConfig, NewProcessor)\n             # If remote code is not set, the default is to use local classes.\n             processor = AutoProcessor.from_pretrained(\n-                \"hf-internal-testing/test_dynamic_processor\", processor_attr_2=False\n+                \"hf-internal-testing/test_dynamic_processor_updated\", processor_attr_2=False\n             )\n             self.assertEqual(processor.__class__.__name__, \"NewProcessor\")\n             self.assertEqual(processor.processor_attr_1, 1)\n@@ -344,9 +344,6 @@ class NewTokenizer(BertTokenizer):\n             pass\n \n         class NewProcessor(ProcessorMixin):\n-            feature_extractor_class = \"NewFeatureExtractor\"\n-            tokenizer_class = \"NewTokenizer\"\n-\n             def __init__(self, feature_extractor, tokenizer):\n                 super().__init__(feature_extractor, tokenizer)\n \n@@ -357,7 +354,7 @@ def __init__(self, feature_extractor, tokenizer):\n             AutoProcessor.register(CustomConfig, NewProcessor)\n             # If remote code is not set, the default is to use local classes.\n             processor = AutoProcessor.from_pretrained(\n-                \"hf-internal-testing/test_dynamic_processor\",\n+                \"hf-internal-testing/test_dynamic_processor_updated\",\n             )\n             self.assertEqual(processor.__class__.__name__, \"NewProcessor\")\n         finally:"
        },
        {
            "sha": "0ee96029a82d8b9dfe4775d350595a6ed1f9b393",
            "filename": "tests/models/blip/test_processing_blip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fblip%2Ftest_processing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fblip%2Ftest_processing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_processing_blip.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -132,7 +132,7 @@ def test_tokenizer_decode(self):\n     @require_torch\n     @require_vision\n     def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\")"
        },
        {
            "sha": "ebaa2e6a0d076be3a18e1252a29492d4426f521b",
            "filename": "tests/models/bridgetower/test_processing_bridgetower.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fbridgetower%2Ftest_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fbridgetower%2Ftest_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_processing_bridgetower.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -61,7 +61,7 @@ def tearDownClass(cls):\n     @require_torch\n     @require_vision\n     def test_image_processor_defaults_preserved_by_image_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\n             \"image_processor\",\n@@ -81,7 +81,7 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n     @require_torch\n     @require_vision\n     def test_structured_kwargs_nested_from_dict(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n \n         image_processor = self.get_component(\"image_processor\")\n@@ -109,7 +109,7 @@ def test_structured_kwargs_nested_from_dict(self):\n     @require_torch\n     @require_vision\n     def test_kwargs_overrides_default_image_processor_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\", crop_size={\"shortest_edge\": 234})\n         tokenizer = self.get_component(\"tokenizer\", max_length=117)\n@@ -126,7 +126,7 @@ def test_kwargs_overrides_default_image_processor_kwargs(self):\n     @require_torch\n     @require_vision\n     def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\")\n@@ -152,7 +152,7 @@ def test_unstructured_kwargs_batched(self):\n     @require_torch\n     @require_vision\n     def test_unstructured_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\")\n@@ -178,7 +178,7 @@ def test_unstructured_kwargs(self):\n     @require_torch\n     @require_vision\n     def test_structured_kwargs_nested(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\")"
        },
        {
            "sha": "f4fbf2ebde3e3a12f6755c95c541f3430bd4fd2e",
            "filename": "tests/models/clipseg/test_processing_clipseg.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fclipseg%2Ftest_processing_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fclipseg%2Ftest_processing_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_processing_clipseg.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -64,6 +64,11 @@ def setUp(self):\n         processor = CLIPSegProcessor(tokenizer=self.get_tokenizer(), image_processor=image_processor)\n         processor.save_pretrained(self.tmpdirname)\n \n+        image_processor = ViTImageProcessor.from_pretrained(self.tmpdirname)\n+        image_processor.save_pretrained(self.tmpdirname)\n+        tokenizer = CLIPTokenizer.from_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(self.tmpdirname)\n+\n     def get_tokenizer(self, **kwargs):\n         return CLIPTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n "
        },
        {
            "sha": "ca849408af0ef306bfcbb6818e61d5b45a75f14e",
            "filename": "tests/models/colpali/test_processing_colpali.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -120,7 +120,7 @@ def test_process_queries(self):\n     # The following tests override the parent tests because ColPaliProcessor can only take one of images or text as input at a time.\n \n     def test_tokenizer_defaults_preserved_by_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n@@ -137,7 +137,7 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         We then check that the mean of the pixel_values is less than or equal to 0 after processing.\n         Since the original pixel_values are in [0, 255], this is a good indicator that the rescale_factor is indeed applied.\n         \"\"\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"image_processor\"] = self.get_component(\n@@ -154,7 +154,7 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n \n     def test_kwargs_overrides_default_tokenizer_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", padding=\"longest\")\n@@ -166,7 +166,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs(self):\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 112)\n \n     def test_kwargs_overrides_default_image_processor_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"image_processor\"] = self.get_component(\n@@ -183,7 +183,7 @@ def test_kwargs_overrides_default_image_processor_kwargs(self):\n         self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n \n     def test_unstructured_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor = self.processor_class(**processor_components)\n@@ -202,7 +202,7 @@ def test_unstructured_kwargs(self):\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n \n     def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor = self.processor_class(**processor_components)\n@@ -221,7 +221,7 @@ def test_unstructured_kwargs_batched(self):\n         self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n \n     def test_doubly_passed_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor = self.processor_class(**processor_components)\n@@ -237,7 +237,7 @@ def test_doubly_passed_kwargs(self):\n             )\n \n     def test_structured_kwargs_nested(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor = self.processor_class(**processor_components)\n@@ -258,7 +258,7 @@ def test_structured_kwargs_nested(self):\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n \n     def test_structured_kwargs_nested_from_dict(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor = self.processor_class(**processor_components)"
        },
        {
            "sha": "5923754f717c66691ddee8d4d8ebb97cfbfa95c8",
            "filename": "tests/models/colqwen2/test_processing_colqwen2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -119,7 +119,7 @@ def test_process_queries(self):\n     # The following tests override the parent tests because ColQwen2Processor can only take one of images or text as input at a time.\n \n     def test_tokenizer_defaults_preserved_by_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n@@ -136,7 +136,7 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         We then check that the mean of the pixel_values is less than or equal to 0 after processing.\n         Since the original pixel_values are in [0, 255], this is a good indicator that the rescale_factor is indeed applied.\n         \"\"\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"image_processor\"] = self.get_component(\n@@ -153,7 +153,7 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n \n     def test_kwargs_overrides_default_tokenizer_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", padding=\"longest\")\n@@ -165,7 +165,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs(self):\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 112)\n \n     def test_kwargs_overrides_default_image_processor_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"image_processor\"] = self.get_component(\n@@ -182,7 +182,7 @@ def test_kwargs_overrides_default_image_processor_kwargs(self):\n         self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n \n     def test_unstructured_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor = self.processor_class(**processor_components)\n@@ -201,7 +201,7 @@ def test_unstructured_kwargs(self):\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n \n     def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor = self.processor_class(**processor_components)\n@@ -220,7 +220,7 @@ def test_unstructured_kwargs_batched(self):\n         self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n \n     def test_doubly_passed_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor = self.processor_class(**processor_components)\n@@ -236,7 +236,7 @@ def test_doubly_passed_kwargs(self):\n             )\n \n     def test_structured_kwargs_nested(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor = self.processor_class(**processor_components)\n@@ -257,7 +257,7 @@ def test_structured_kwargs_nested(self):\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n \n     def test_structured_kwargs_nested_from_dict(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor = self.processor_class(**processor_components)"
        },
        {
            "sha": "4587e19c41b71b61a6311275ea9daf0c318ec230",
            "filename": "tests/models/csm/test_processing_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fcsm%2Ftest_processing_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fcsm%2Ftest_processing_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_processing_csm.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -81,7 +81,7 @@ def _test_apply_chat_template(\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n-        if processor_name not in self.processor_class.attributes:\n+        if processor_name not in self.processor_class.get_attributes():\n             self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n \n         # some models have only Fast image processor"
        },
        {
            "sha": "52a957f2d60fd02817b3ea2cb4f7049161e92eb0",
            "filename": "tests/models/flava/test_processing_flava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fflava%2Ftest_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fflava%2Ftest_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_processing_flava.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -79,6 +79,11 @@ def setUp(self):\n         processor = FlavaProcessor(tokenizer=self.get_tokenizer(), image_processor=image_processor)\n         processor.save_pretrained(self.tmpdirname)\n \n+        image_processor = FlavaImageProcessor.from_pretrained(self.tmpdirname)\n+        image_processor.save_pretrained(self.tmpdirname)\n+        tokenizer = BertTokenizer.from_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(self.tmpdirname)\n+\n     def get_tokenizer(self, **kwargs):\n         return BertTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n "
        },
        {
            "sha": "d88843c6d15813bb8b0fd69c69089a8722b94d73",
            "filename": "tests/models/fuyu/test_processing_fuyu.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -197,7 +197,7 @@ def test_fuyu_processing_multiple_image_sample(self):\n     @require_vision\n     @require_torch\n     def test_kwargs_overrides_default_tokenizer_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\", max_length=117)\n@@ -225,7 +225,7 @@ def test_kwargs_overrides_default_image_processor_kwargs(self):\n     @require_vision\n     @require_torch\n     def test_tokenizer_defaults_preserved_by_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n@@ -243,7 +243,7 @@ def test_tokenizer_defaults_preserved_by_kwargs(self):\n     @require_torch\n     @require_vision\n     def test_structured_kwargs_nested(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\")\n@@ -270,7 +270,7 @@ def test_structured_kwargs_nested(self):\n     @require_torch\n     @require_vision\n     def test_structured_kwargs_nested_from_dict(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n \n         image_processor = self.get_component(\"image_processor\")\n@@ -296,7 +296,7 @@ def test_structured_kwargs_nested_from_dict(self):\n     @require_torch\n     @require_vision\n     def test_unstructured_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\")\n@@ -321,7 +321,7 @@ def test_unstructured_kwargs(self):\n     @require_torch\n     @require_vision\n     def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\")"
        },
        {
            "sha": "b22cdce7a4e9d4c4535808b1795d1552d326c7ca",
            "filename": "tests/models/glm4v/test_processor_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -78,7 +78,7 @@ def _test_apply_chat_template(\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n-        if processor_name not in self.processor_class.attributes:\n+        if processor_name not in self.processor_class.get_attributes():\n             self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n \n         batch_messages = ["
        },
        {
            "sha": "30a478ada42757293e9a114b206467bbb2590ab0",
            "filename": "tests/models/grounding_dino/test_processing_grounding_dino.py",
            "status": "modified",
            "additions": 11,
            "deletions": 17,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import json\n import os\n import shutil\n import tempfile\n@@ -23,7 +22,7 @@\n from transformers import BertTokenizer, BertTokenizerFast, GroundingDinoProcessor\n from transformers.models.bert.tokenization_bert import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import IMAGE_PROCESSOR_NAME, is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n@@ -52,21 +51,16 @@ def setUpClass(cls):\n         with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n-        image_processor_map = {\n-            \"do_resize\": True,\n-            \"size\": None,\n-            \"do_normalize\": True,\n-            \"image_mean\": [0.5, 0.5, 0.5],\n-            \"image_std\": [0.5, 0.5, 0.5],\n-            \"do_rescale\": True,\n-            \"rescale_factor\": 1 / 255,\n-            \"do_pad\": True,\n-        }\n-        cls.image_processor_file = os.path.join(cls.tmpdirname, IMAGE_PROCESSOR_NAME)\n-        with open(cls.image_processor_file, \"w\", encoding=\"utf-8\") as fp:\n-            json.dump(image_processor_map, fp)\n-\n-        image_processor = GroundingDinoImageProcessor()\n+        image_processor = GroundingDinoImageProcessor(\n+            do_resize=True,\n+            size=None,\n+            do_normalize=True,\n+            image_mean=[0.5, 0.5, 0.5],\n+            image_std=[0.5, 0.5, 0.5],\n+            do_rescale=True,\n+            rescale_factor=1 / 255,\n+            do_pad=True,\n+        )\n         tokenizer = BertTokenizer.from_pretrained(cls.from_pretrained_id)\n \n         processor = GroundingDinoProcessor(image_processor, tokenizer)"
        },
        {
            "sha": "154b02b17da803983e8d97141baf4922f66268d8",
            "filename": "tests/models/internvl/test_processing_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -292,7 +292,7 @@ def _test_apply_chat_template(\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n-        if processor_name not in self.processor_class.attributes:\n+        if processor_name not in self.processor_class.get_attributes():\n             self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n \n         batch_messages = ["
        },
        {
            "sha": "56b193eda110e9e303d3ab5ebefe45f918a3de32",
            "filename": "tests/models/kosmos2/test_processing_kosmos2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 24,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fkosmos2%2Ftest_processing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fkosmos2%2Ftest_processing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_processing_kosmos2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -22,7 +22,6 @@\n import pytest\n \n from transformers.image_utils import load_image\n-from transformers.models.auto.processing_auto import processor_class_from_name\n from transformers.testing_utils import (\n     get_tests_dir,\n     require_sentencepiece,\n@@ -70,23 +69,6 @@ def setUpClass(cls):\n         processor = Kosmos2Processor(image_processor, fast_tokenizer)\n         processor.save_pretrained(cls.tmpdirname)\n \n-    # We override this method to take the fast tokenizer by default\n-    def get_component(self, attribute, **kwargs):\n-        assert attribute in self.processor_class.attributes\n-        component_class_name = getattr(self.processor_class, f\"{attribute}_class\")\n-        if isinstance(component_class_name, tuple):\n-            if attribute == \"image_processor\":\n-                component_class_name = component_class_name[0]\n-            else:\n-                component_class_name = component_class_name[-1]\n-\n-        component_class = processor_class_from_name(component_class_name)\n-        component = component_class.from_pretrained(self.tmpdirname, **kwargs)  # noqa\n-        if attribute == \"tokenizer\" and not component.pad_token:\n-            component.pad_token = \"[TEST_PAD]\"\n-\n-        return component\n-\n     def get_tokenizer(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n \n@@ -474,7 +456,7 @@ def check(texts, bboxes, expected_input_ids):\n     @require_vision\n     @require_torch\n     def test_kwargs_overrides_default_tokenizer_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\", max_length=117)\n@@ -499,7 +481,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs(self):\n     @require_torch\n     @require_vision\n     def test_structured_kwargs_nested(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\")\n@@ -525,7 +507,7 @@ def test_structured_kwargs_nested(self):\n     @require_torch\n     @require_vision\n     def test_structured_kwargs_nested_from_dict(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n \n         image_processor = self.get_component(\"image_processor\")\n@@ -549,7 +531,7 @@ def test_structured_kwargs_nested_from_dict(self):\n     @require_vision\n     @require_torch\n     def test_tokenizer_defaults_preserved_by_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n@@ -567,7 +549,7 @@ def test_tokenizer_defaults_preserved_by_kwargs(self):\n     @require_torch\n     @require_vision\n     def test_unstructured_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\")\n@@ -592,7 +574,7 @@ def test_unstructured_kwargs(self):\n     @require_torch\n     @require_vision\n     def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\")"
        },
        {
            "sha": "f141afd97d84d46ab416507bae4cae347e94ef28",
            "filename": "tests/models/kosmos2_5/test_processor_kosmos2_5.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fkosmos2_5%2Ftest_processor_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fkosmos2_5%2Ftest_processor_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2_5%2Ftest_processor_kosmos2_5.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -156,7 +156,7 @@ def test_model_input_names(self):\n     @require_vision\n     def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         # Rewrite as KOSMOS-2.5 processor return \"flattened_patches\" and not \"pixel_values\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\", max_patches=1024, patch_size={\"height\": 8, \"width\": 8})\n         tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n@@ -174,7 +174,7 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n     @require_vision\n     def test_kwargs_overrides_default_image_processor_kwargs(self):\n         # Rewrite as KOSMOS-2.5 processor return \"flattened_patches\" and not \"pixel_values\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\", max_patches=4096)\n         tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n@@ -192,7 +192,7 @@ def test_kwargs_overrides_default_image_processor_kwargs(self):\n     @require_vision\n     def test_unstructured_kwargs(self):\n         # Rewrite as KOSMOS-2.5 processor doesn't use `rescale_factor`\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\")\n@@ -218,7 +218,7 @@ def test_unstructured_kwargs(self):\n     @require_vision\n     def test_unstructured_kwargs_batched(self):\n         # Rewrite as KOSMOS-2.5 processor doesn't use `rescale_factor`\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\")\n@@ -245,7 +245,7 @@ def test_unstructured_kwargs_batched(self):\n     @require_vision\n     def test_structured_kwargs_nested(self):\n         # Rewrite as KOSMOS-2.5 processor doesn't use `rescale_factor`\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\")\n@@ -274,7 +274,7 @@ def test_structured_kwargs_nested(self):\n     @require_vision\n     def test_structured_kwargs_nested_from_dict(self):\n         # Rewrite as KOSMOS-2.5 processor doesn't use `rescale_factor`\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n \n         image_processor = self.get_component(\"image_processor\")"
        },
        {
            "sha": "effbc97943534d340480cc712a72b5fdc1254fd5",
            "filename": "tests/models/layoutxlm/test_processing_layoutxlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Flayoutxlm%2Ftest_processing_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Flayoutxlm%2Ftest_processing_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutxlm%2Ftest_processing_layoutxlm.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -62,7 +62,9 @@ def setUpClass(cls):\n         cls.tokenizer_pretrained_name = \"hf-internal-testing/tiny-random-layoutxlm\"\n \n         tokenizer = cls.get_tokenizer()\n+        tokenizer.save_pretrained(cls.tmpdirname)\n         image_processor = cls.get_image_processor()\n+        image_processor.save_pretrained(cls.tmpdirname)\n         processor = LayoutXLMProcessor(tokenizer=tokenizer, image_processor=image_processor)\n         processor.save_pretrained(cls.tmpdirname)\n "
        },
        {
            "sha": "5d4ff4621c0115e4496fe3870662c26cb3789f8b",
            "filename": "tests/models/markuplm/test_tokenization_markuplm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -1058,7 +1058,7 @@ def test_torch_encode_plus_sent_to_model(self):\n                 nodes, xpaths = self.get_nodes_and_xpaths()\n                 encoded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, return_tensors=\"pt\")\n                 batch_encoded_sequence = tokenizer.batch_encode_plus(\n-                    [nodes, nodes], [xpaths, xpaths], return_tensors=\"pt\"\n+                    batch_text_or_text_pairs=[nodes, nodes], xpaths=[xpaths, xpaths], return_tensors=\"pt\"\n                 )\n                 # This should not fail\n "
        },
        {
            "sha": "0e6eeed845e88f7949153d87b595a57b954bd0a4",
            "filename": "tests/models/mllama/test_processing_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -371,7 +371,7 @@ def test_process_interleaved_images_prompts_image_error(self):\n     def test_unstructured_kwargs_batched(self):\n         # Overridden because Mllama expects images in nested format. For 2 images it can't infer\n         # the correct nesting, so we better throw an error\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_kwargs = self.prepare_processor_dict()"
        },
        {
            "sha": "7d48148f1de16e459eeaadce0e598cf6e285a005",
            "filename": "tests/models/owlv2/test_modeling_owlv2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 9,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -51,7 +51,7 @@\n if is_vision_available():\n     from PIL import Image\n \n-    from transformers import OwlViTProcessor\n+    from transformers import OwlViTImageProcessor, OwlViTProcessor\n \n \n # Copied from tests.models.owlvit.test_modeling_owlvit.OwlViTVisionModelTester with OwlViT->Owlv2\n@@ -615,7 +615,9 @@ class Owlv2ModelIntegrationTest(unittest.TestCase):\n     def test_inference(self):\n         model_name = \"google/owlv2-base-patch16\"\n         model = Owlv2Model.from_pretrained(model_name).to(torch_device)\n-        processor = OwlViTProcessor.from_pretrained(model_name)\n+        image_processor = OwlViTImageProcessor.from_pretrained(model_name)\n+        processor = OwlViTProcessor.from_pretrained(model_name, image_processor=image_processor)\n+        print(\"processor:\", processor)\n \n         image = prepare_img()\n         inputs = processor(\n@@ -646,7 +648,8 @@ def test_inference(self):\n     def test_inference_interpolate_pos_encoding(self):\n         model_name = \"google/owlv2-base-patch16\"\n         model = Owlv2Model.from_pretrained(model_name).to(torch_device)\n-        processor = OwlViTProcessor.from_pretrained(model_name)\n+        image_processor = OwlViTImageProcessor.from_pretrained(model_name)\n+        processor = OwlViTProcessor.from_pretrained(model_name, image_processor=image_processor)\n         processor.image_processor.size = {\"height\": 1024, \"width\": 1024}\n \n         image = prepare_img()\n@@ -709,7 +712,8 @@ def test_inference_interpolate_pos_encoding(self):\n \n         # Deactivate interpolate_pos_encoding on same model, and use default image size.\n         # Verify the dynamic change caused by the activation/deactivation of interpolate_pos_encoding of variables: self.sqrt_num_patches, self.box_bias from (OwlViTForObjectDetection).\n-        processor = OwlViTProcessor.from_pretrained(model_name)\n+        image_processor = OwlViTImageProcessor.from_pretrained(model_name)\n+        processor = OwlViTProcessor.from_pretrained(model_name, image_processor=image_processor)\n \n         image = prepare_img()\n         inputs = processor(\n@@ -784,8 +788,8 @@ def test_inference_interpolate_pos_encoding(self):\n     def test_inference_object_detection(self):\n         model_name = \"google/owlv2-base-patch16\"\n         model = Owlv2ForObjectDetection.from_pretrained(model_name).to(torch_device)\n-\n-        processor = OwlViTProcessor.from_pretrained(model_name)\n+        image_processor = OwlViTImageProcessor.from_pretrained(model_name)\n+        processor = OwlViTProcessor.from_pretrained(model_name, image_processor=image_processor)\n \n         image = prepare_img()\n         text_labels = [[\"a photo of a cat\", \"a photo of a dog\"]]\n@@ -834,8 +838,8 @@ def test_inference_object_detection(self):\n     def test_inference_one_shot_object_detection(self):\n         model_name = \"google/owlv2-base-patch16\"\n         model = Owlv2ForObjectDetection.from_pretrained(model_name).to(torch_device)\n-\n-        processor = OwlViTProcessor.from_pretrained(model_name)\n+        image_processor = OwlViTImageProcessor.from_pretrained(model_name)\n+        processor = OwlViTProcessor.from_pretrained(model_name, image_processor=image_processor)\n \n         image = prepare_img()\n         query_image = prepare_img()\n@@ -865,7 +869,8 @@ def test_inference_one_shot_object_detection_fp16(self):\n         model_name = \"google/owlv2-base-patch16\"\n         model = Owlv2ForObjectDetection.from_pretrained(model_name, dtype=torch.float16).to(torch_device)\n \n-        processor = OwlViTProcessor.from_pretrained(model_name)\n+        image_processor = OwlViTImageProcessor.from_pretrained(model_name)\n+        processor = OwlViTProcessor.from_pretrained(model_name, image_processor=image_processor)\n \n         image = prepare_img()\n         query_image = prepare_img()"
        },
        {
            "sha": "ee327b08c21cd4195f11bde18fd761014bcee3c6",
            "filename": "tests/models/owlvit/test_processing_owlvit.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fowlvit%2Ftest_processing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fowlvit%2Ftest_processing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_processing_owlvit.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -64,6 +64,11 @@ def setUp(self):\n         processor = OwlViTProcessor(tokenizer=self.get_tokenizer(), image_processor=image_processor)\n         processor.save_pretrained(self.tmpdirname)\n \n+        image_processor = OwlViTImageProcessor.from_pretrained(self.tmpdirname)\n+        image_processor.save_pretrained(self.tmpdirname)\n+        tokenizer = CLIPTokenizer.from_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(self.tmpdirname)\n+\n     def get_tokenizer(self, **kwargs):\n         return CLIPTokenizer.from_pretrained(self.tmpdirname, pad_token=\"!\", **kwargs)\n "
        },
        {
            "sha": "e93f91f5b93b9539674ef5db02441bb905cc2183",
            "filename": "tests/models/pix2struct/test_processing_pix2struct.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fpix2struct%2Ftest_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fpix2struct%2Ftest_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_processing_pix2struct.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -169,7 +169,7 @@ def test_tokenizer_decode(self):\n     @require_vision\n     def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         # Rewrite as pix2struct processor return \"flattened_patches\" and not \"pixel_values\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\", max_patches=1024, patch_size={\"height\": 8, \"width\": 8})\n         print(\"image_processor\", image_processor)\n@@ -188,7 +188,7 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n     @require_vision\n     def test_kwargs_overrides_default_image_processor_kwargs(self):\n         # Rewrite as pix2struct processor return \"flattened_patches\" and not \"pixel_values\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\", max_patches=4096)\n         tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n@@ -206,7 +206,7 @@ def test_kwargs_overrides_default_image_processor_kwargs(self):\n     @require_vision\n     def test_unstructured_kwargs(self):\n         # Rewrite as pix2struct processor return \"decoder_input_ids\" and not \"input_ids\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\")\n@@ -232,7 +232,7 @@ def test_unstructured_kwargs(self):\n     @require_vision\n     def test_unstructured_kwargs_batched(self):\n         # Rewrite as pix2struct processor return \"decoder_input_ids\" and not \"input_ids\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\")\n@@ -259,7 +259,7 @@ def test_unstructured_kwargs_batched(self):\n     @require_vision\n     def test_structured_kwargs_nested(self):\n         # Rewrite as pix2struct processor return \"decoder_input_ids\" and not \"input_ids\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\")\n@@ -288,7 +288,7 @@ def test_structured_kwargs_nested(self):\n     @require_vision\n     def test_structured_kwargs_nested_from_dict(self):\n         # Rewrite as pix2struct processor return \"decoder_input_ids\" and not \"input_ids\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n \n         image_processor = self.get_component(\"image_processor\")"
        },
        {
            "sha": "91fb5ffcf08771adfd869013164528b3345f94c8",
            "filename": "tests/models/qwen2_5_omni/test_processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -58,7 +58,7 @@ class Qwen2_5OmniProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     #  text + audio kwargs testing\n     @require_torch\n     def test_tokenizer_defaults_preserved_by_kwargs_audio(self):\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n         feature_extractor = self.get_component(\"feature_extractor\")\n         if hasattr(self, \"get_tokenizer\"):\n@@ -69,7 +69,7 @@ def test_tokenizer_defaults_preserved_by_kwargs_audio(self):\n             self.assertTrue(False, \"Processor doesn't have get_tokenizer or get_component defined\")\n         if not tokenizer.pad_token:\n             tokenizer.pad_token = \"[TEST_PAD]\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         video_processor = self.get_component(\"video_processor\")\n@@ -91,7 +91,7 @@ def test_tokenizer_defaults_preserved_by_kwargs_audio(self):\n     @require_torch\n     @require_vision\n     def test_structured_kwargs_audio_nested(self):\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n         feature_extractor = self.get_component(\"feature_extractor\")\n         if hasattr(self, \"get_tokenizer\"):\n@@ -100,7 +100,7 @@ def test_structured_kwargs_audio_nested(self):\n             tokenizer = self.get_component(\"tokenizer\")\n         if not tokenizer.pad_token:\n             tokenizer.pad_token = \"[TEST_PAD]\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         video_processor = self.get_component(\"video_processor\")\n@@ -129,7 +129,7 @@ def test_structured_kwargs_audio_nested(self):\n \n     @require_torch\n     def test_unstructured_kwargs_audio(self):\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n         feature_extractor = self.get_component(\"feature_extractor\")\n         if hasattr(self, \"get_tokenizer\"):\n@@ -138,7 +138,7 @@ def test_unstructured_kwargs_audio(self):\n             tokenizer = self.get_component(\"tokenizer\", max_length=117)\n         if not tokenizer.pad_token:\n             tokenizer.pad_token = \"[TEST_PAD]\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         video_processor = self.get_component(\"video_processor\")\n@@ -167,7 +167,7 @@ def test_unstructured_kwargs_audio(self):\n \n     @require_torch\n     def test_doubly_passed_kwargs_audio(self):\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n         feature_extractor = self.get_component(\"feature_extractor\")\n         if hasattr(self, \"get_tokenizer\"):\n@@ -176,7 +176,7 @@ def test_doubly_passed_kwargs_audio(self):\n             tokenizer = self.get_component(\"tokenizer\")\n         if not tokenizer.pad_token:\n             tokenizer.pad_token = \"[TEST_PAD]\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         video_processor = self.get_component(\"video_processor\")\n@@ -189,7 +189,7 @@ def test_doubly_passed_kwargs_audio(self):\n \n     @require_torch\n     def test_kwargs_overrides_default_tokenizer_kwargs_audio(self):\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n         feature_extractor = self.get_component(\"feature_extractor\")\n         if hasattr(self, \"get_tokenizer\"):\n@@ -198,7 +198,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs_audio(self):\n             tokenizer = self.get_component(\"tokenizer\", max_length=117)\n         if not tokenizer.pad_token:\n             tokenizer.pad_token = \"[TEST_PAD]\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         video_processor = self.get_component(\"video_processor\")\n@@ -334,7 +334,7 @@ def _test_apply_chat_template(\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n-        if processor_name not in self.processor_class.attributes:\n+        if processor_name not in self.processor_class.get_attributes():\n             self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n \n         batch_messages = [\n@@ -556,7 +556,7 @@ def test_chat_template_audio_from_video(self):\n         ):\n             self.skipTest(f\"{self.processor_class} does not support video inputs\")\n \n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n \n         video_file_path = hf_hub_download("
        },
        {
            "sha": "608dbfa5414e24f867d36e8f7d4ee470b504e329",
            "filename": "tests/models/qwen2_5_vl/test_processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processing_qwen2_5_vl.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -152,7 +152,7 @@ def _test_apply_chat_template(\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n-        if processor_name not in self.processor_class.attributes:\n+        if processor_name not in self.processor_class.get_attributes():\n             self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n \n         batch_messages = ["
        },
        {
            "sha": "6e83c998cb95f3a72271c1ca9ee75047d262b489",
            "filename": "tests/models/qwen2_vl/test_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -153,7 +153,7 @@ def _test_apply_chat_template(\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n-        if processor_name not in self.processor_class.attributes:\n+        if processor_name not in self.processor_class.get_attributes():\n             self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n \n         batch_messages = ["
        },
        {
            "sha": "43fbd1808b892a5ffc1a91d47c51e1a1d2b844d9",
            "filename": "tests/models/qwen3_omni_moe/test_processing_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fqwen3_omni_moe%2Ftest_processing_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fqwen3_omni_moe%2Ftest_processing_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_omni_moe%2Ftest_processing_qwen3_omni_moe.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -59,7 +59,7 @@ class Qwen3OmniMoeProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     #  text + audio kwargs testing\n     @require_torch\n     def test_tokenizer_defaults_preserved_by_kwargs_audio(self):\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n         feature_extractor = self.get_component(\"feature_extractor\")\n         if hasattr(self, \"get_tokenizer\"):\n@@ -70,7 +70,7 @@ def test_tokenizer_defaults_preserved_by_kwargs_audio(self):\n             self.assertTrue(False, \"Processor doesn't have get_tokenizer or get_component defined\")\n         if not tokenizer.pad_token:\n             tokenizer.pad_token = \"[TEST_PAD]\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         video_processor = self.get_component(\"video_processor\")\n@@ -92,7 +92,7 @@ def test_tokenizer_defaults_preserved_by_kwargs_audio(self):\n     @require_torch\n     @require_vision\n     def test_structured_kwargs_audio_nested(self):\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n         feature_extractor = self.get_component(\"feature_extractor\")\n         if hasattr(self, \"get_tokenizer\"):\n@@ -101,7 +101,7 @@ def test_structured_kwargs_audio_nested(self):\n             tokenizer = self.get_component(\"tokenizer\")\n         if not tokenizer.pad_token:\n             tokenizer.pad_token = \"[TEST_PAD]\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         video_processor = self.get_component(\"video_processor\")\n@@ -130,7 +130,7 @@ def test_structured_kwargs_audio_nested(self):\n \n     @require_torch\n     def test_unstructured_kwargs_audio(self):\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n         feature_extractor = self.get_component(\"feature_extractor\")\n         if hasattr(self, \"get_tokenizer\"):\n@@ -139,7 +139,7 @@ def test_unstructured_kwargs_audio(self):\n             tokenizer = self.get_component(\"tokenizer\", max_length=117)\n         if not tokenizer.pad_token:\n             tokenizer.pad_token = \"[TEST_PAD]\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         video_processor = self.get_component(\"video_processor\")\n@@ -168,7 +168,7 @@ def test_unstructured_kwargs_audio(self):\n \n     @require_torch\n     def test_doubly_passed_kwargs_audio(self):\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n         feature_extractor = self.get_component(\"feature_extractor\")\n         if hasattr(self, \"get_tokenizer\"):\n@@ -177,7 +177,7 @@ def test_doubly_passed_kwargs_audio(self):\n             tokenizer = self.get_component(\"tokenizer\")\n         if not tokenizer.pad_token:\n             tokenizer.pad_token = \"[TEST_PAD]\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         video_processor = self.get_component(\"video_processor\")\n@@ -190,7 +190,7 @@ def test_doubly_passed_kwargs_audio(self):\n \n     @require_torch\n     def test_kwargs_overrides_default_tokenizer_kwargs_audio(self):\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n         feature_extractor = self.get_component(\"feature_extractor\")\n         if hasattr(self, \"get_tokenizer\"):\n@@ -199,7 +199,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs_audio(self):\n             tokenizer = self.get_component(\"tokenizer\", max_length=117)\n         if not tokenizer.pad_token:\n             tokenizer.pad_token = \"[TEST_PAD]\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         video_processor = self.get_component(\"video_processor\")\n@@ -335,7 +335,7 @@ def _test_apply_chat_template(\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n-        if processor_name not in self.processor_class.attributes:\n+        if processor_name not in self.processor_class.get_attributes():\n             self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n \n         batch_messages = [\n@@ -557,7 +557,7 @@ def test_chat_template_audio_from_video(self):\n         ):\n             self.skipTest(f\"{self.processor_class} does not support video inputs\")\n \n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n \n         video_file_path = hf_hub_download("
        },
        {
            "sha": "979cf53680e17a46669e9c25bcfa94438c9affb3",
            "filename": "tests/models/qwen3_vl/test_processing_qwen3_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fqwen3_vl%2Ftest_processing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fqwen3_vl%2Ftest_processing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_vl%2Ftest_processing_qwen3_vl.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -163,7 +163,7 @@ def _test_apply_chat_template(\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n-        if processor_name not in self.processor_class.attributes:\n+        if processor_name not in self.processor_class.get_attributes():\n             self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n \n         batch_messages = ["
        },
        {
            "sha": "c5639ff13c7012ba3a2306ab6b7d4d2788e8792d",
            "filename": "tests/models/smolvlm/test_processing_smolvlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -436,7 +436,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n     @require_torch\n     @require_vision\n     def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\")\n         video_processor = self.get_component(\"video_processor\")\n@@ -467,7 +467,7 @@ def test_unstructured_kwargs_batched(self):\n     @require_torch\n     @require_vision\n     def test_unstructured_kwargs_batched_video(self):\n-        if \"video_processor\" not in self.processor_class.attributes:\n+        if \"video_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_kwargs = self.prepare_processor_dict()"
        },
        {
            "sha": "0801828f0086dcce8afe05eb7cc44e68d3351285",
            "filename": "tests/models/video_llama_3/test_processing_video_llama_3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fvideo_llama_3%2Ftest_processing_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fvideo_llama_3%2Ftest_processing_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llama_3%2Ftest_processing_video_llama_3.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -173,7 +173,7 @@ def _test_apply_chat_template(\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n-        if processor_name not in self.processor_class.attributes:\n+        if processor_name not in self.processor_class.get_attributes():\n             self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n \n         batch_messages = ["
        },
        {
            "sha": "4f61a37234d0abe19adea42de696c1d132052142",
            "filename": "tests/models/wav2vec2/test_processing_wav2vec2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fwav2vec2%2Ftest_processing_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fwav2vec2%2Ftest_processing_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_processing_wav2vec2.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -65,8 +65,9 @@ def get_tokenizer(cls, **kwargs_init):\n         kwargs.update(kwargs_init)\n         return Wav2Vec2CTCTokenizer.from_pretrained(cls.tmpdirname, **kwargs)\n \n-    def get_feature_extractor(self, **kwargs):\n-        return Wav2Vec2FeatureExtractor.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    def get_feature_extractor(cls, **kwargs):\n+        return Wav2Vec2FeatureExtractor.from_pretrained(cls.tmpdirname, **kwargs)\n \n     @classmethod\n     def tearDownClass(cls):"
        },
        {
            "sha": "5a1eeed1a850ca696f11ea5c52c9055e4992e96a",
            "filename": "tests/models/wav2vec2_bert/test_processing_wav2vec2_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_processing_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_processing_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_processing_wav2vec2_bert.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -66,8 +66,9 @@ def get_tokenizer(cls, **kwargs_init):\n         kwargs.update(kwargs_init)\n         return Wav2Vec2CTCTokenizer.from_pretrained(cls.tmpdirname, **kwargs)\n \n-    def get_feature_extractor(self, **kwargs):\n-        return SeamlessM4TFeatureExtractor.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    def get_feature_extractor(cls, **kwargs):\n+        return SeamlessM4TFeatureExtractor.from_pretrained(cls.tmpdirname, **kwargs)\n \n     @classmethod\n     def tearDownClass(cls):"
        },
        {
            "sha": "2553bd3273b84779cbcd9cd7c8322540d9342186",
            "filename": "tests/models/wav2vec2_with_lm/test_processing_wav2vec2_with_lm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fwav2vec2_with_lm%2Ftest_processing_wav2vec2_with_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Fmodels%2Fwav2vec2_with_lm%2Ftest_processing_wav2vec2_with_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_with_lm%2Ftest_processing_wav2vec2_with_lm.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -71,7 +71,6 @@ def setUp(self):\n \n         # load decoder from hub\n         self.decoder_name = \"hf-internal-testing/ngram-beam-search-decoder\"\n-\n         feature_extractor = Wav2Vec2FeatureExtractor(**feature_extractor_map)\n         processor = Wav2Vec2ProcessorWithLM(\n             tokenizer=self.get_tokenizer(), feature_extractor=feature_extractor, decoder=self.get_decoder()"
        },
        {
            "sha": "962cd1200b95f2097dc31093af9ae0e0cde8f201",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 47,
            "deletions": 80,
            "changes": 127,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -25,8 +25,10 @@\n from huggingface_hub import hf_hub_download\n from parameterized import parameterized\n \n-from transformers.models.auto.processing_auto import processor_class_from_name\n-from transformers.processing_utils import Unpack\n+from transformers.processing_utils import (\n+    MODALITY_TO_AUTOPROCESSOR_MAPPING,\n+    Unpack,\n+)\n from transformers.testing_utils import (\n     check_json_file_has_correct_format,\n     require_av,\n@@ -65,7 +67,6 @@\n     ],\n }\n \n-\n for modality, urls in MODALITY_INPUT_DATA.items():\n     MODALITY_INPUT_DATA[modality] = [url_to_local_path(url) for url in urls]\n \n@@ -106,17 +107,12 @@ def prepare_processor_dict():\n         return {}\n \n     def get_component(self, attribute, **kwargs):\n-        assert attribute in self.processor_class.attributes\n-        component_class_name = getattr(self.processor_class, f\"{attribute}_class\")\n-        if isinstance(component_class_name, tuple):\n-            if attribute == \"image_processor\":\n-                # TODO: @yoni, change logic in v4.52 (when use_fast set to True by default)\n-                component_class_name = component_class_name[0]\n-            else:\n-                component_class_name = component_class_name[-1]\n-\n-        component_class = processor_class_from_name(component_class_name)\n-        component = component_class.from_pretrained(self.tmpdirname, **kwargs)  # noqa\n+        if attribute not in MODALITY_TO_AUTOPROCESSOR_MAPPING and \"tokenizer\" in attribute:\n+            auto_processor_class = MODALITY_TO_AUTOPROCESSOR_MAPPING[\"tokenizer\"]\n+            component = auto_processor_class.from_pretrained(self.tmpdirname, subfolder=attribute, **kwargs)  # noqa\n+        else:\n+            auto_processor_class = MODALITY_TO_AUTOPROCESSOR_MAPPING[attribute]\n+            component = auto_processor_class.from_pretrained(self.tmpdirname, **kwargs)  # noqa\n         if \"tokenizer\" in attribute and not component.pad_token:\n             component.pad_token = \"[TEST_PAD]\"\n             if component.pad_token_id is None:\n@@ -126,7 +122,7 @@ def get_component(self, attribute, **kwargs):\n \n     def prepare_components(self):\n         components = {}\n-        for attribute in self.processor_class.attributes:\n+        for attribute in self.processor_class.get_attributes():\n             component = self.get_component(attribute)\n             components[attribute] = component\n \n@@ -211,7 +207,7 @@ def test_processor_from_and_save_pretrained(self):\n \n                 self.assertEqual(processor_second.to_dict(), processor_first.to_dict())\n \n-                for attribute in processor_first.attributes:\n+                for attribute in processor_first.get_attributes():\n                     attribute_first = getattr(processor_first, attribute)\n                     attribute_second = getattr(processor_second, attribute)\n \n@@ -231,17 +227,13 @@ def test_processor_from_and_save_pretrained_as_nested_dict(self):\n             self.assertEqual(processor_second.to_dict(), processor_first.to_dict())\n \n             # Try to load each attribute separately from saved directory\n-            for attribute in processor_first.attributes:\n-                attribute_class_name = getattr(processor_first, f\"{attribute}_class\")\n-                if isinstance(attribute_class_name, tuple):\n-                    if attribute == \"image_processor\":\n-                        # TODO: @yoni, change logic in v4.52 (when use_fast set to True by default)\n-                        attribute_class_name = attribute_class_name[0]\n-                    else:\n-                        attribute_class_name = attribute_class_name[-1]\n-\n-                attribute_class = processor_class_from_name(attribute_class_name)\n-                attribute_reloaded = attribute_class.from_pretrained(tmpdirname)\n+            for attribute in processor_first.get_attributes():\n+                if attribute not in MODALITY_TO_AUTOPROCESSOR_MAPPING and \"tokenizer\" in attribute:\n+                    auto_processor_class = MODALITY_TO_AUTOPROCESSOR_MAPPING[\"tokenizer\"]\n+                    attribute_reloaded = auto_processor_class.from_pretrained(tmpdirname, subfolder=attribute)\n+                else:\n+                    auto_processor_class = MODALITY_TO_AUTOPROCESSOR_MAPPING[attribute]\n+                    attribute_reloaded = auto_processor_class.from_pretrained(tmpdirname)\n                 attribute_first = getattr(processor_first, attribute)\n \n                 # tokenizer repr contains model-path from where we loaded\n@@ -368,7 +360,7 @@ def skip_processor_without_typed_kwargs(self, processor):\n             self.skipTest(f\"{self.processor_class} doesn't have typed kwargs.\")\n \n     def test_tokenizer_defaults_preserved_by_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n@@ -387,7 +379,7 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         We then check that the mean of the pixel_values is less than or equal to 0 after processing.\n         Since the original pixel_values are in [0, 255], this is a good indicator that the rescale_factor is indeed applied.\n         \"\"\"\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"image_processor\"] = self.get_component(\n@@ -406,7 +398,7 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n \n     def test_kwargs_overrides_default_tokenizer_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", padding=\"longest\")\n@@ -422,7 +414,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs(self):\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 112)\n \n     def test_kwargs_overrides_default_image_processor_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"image_processor\"] = self.get_component(\n@@ -443,7 +435,7 @@ def test_kwargs_overrides_default_image_processor_kwargs(self):\n         self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n \n     def test_unstructured_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_kwargs = self.prepare_processor_dict()\n@@ -466,7 +458,7 @@ def test_unstructured_kwargs(self):\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n \n     def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_kwargs = self.prepare_processor_dict()\n@@ -492,7 +484,7 @@ def test_unstructured_kwargs_batched(self):\n         )\n \n     def test_doubly_passed_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_kwargs = self.prepare_processor_dict()\n@@ -511,7 +503,7 @@ def test_doubly_passed_kwargs(self):\n             )\n \n     def test_args_overlap_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_first = self.get_processor()\n         image_processor = processor_first.image_processor\n@@ -523,7 +515,7 @@ def test_args_overlap_kwargs(self):\n             self.assertTrue(processor_second.image_processor.is_override)\n \n     def test_structured_kwargs_nested(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_kwargs = self.prepare_processor_dict()\n@@ -547,7 +539,7 @@ def test_structured_kwargs_nested(self):\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n \n     def test_structured_kwargs_nested_from_dict(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_kwargs = self.prepare_processor_dict()\n@@ -570,7 +562,7 @@ def test_structured_kwargs_nested_from_dict(self):\n     # text + audio kwargs testing\n     @require_torch\n     def test_tokenizer_defaults_preserved_by_kwargs_audio(self):\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n \n         feature_extractor = self.get_component(\"feature_extractor\")\n@@ -587,7 +579,7 @@ def test_tokenizer_defaults_preserved_by_kwargs_audio(self):\n \n     @require_torch\n     def test_kwargs_overrides_default_tokenizer_kwargs_audio(self):\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n \n         feature_extractor = self.get_component(\"feature_extractor\")\n@@ -605,7 +597,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs_audio(self):\n \n     @require_torch\n     def test_unstructured_kwargs_audio(self):\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n \n         feature_extractor = self.get_component(\"feature_extractor\")\n@@ -623,7 +615,7 @@ def test_unstructured_kwargs_audio(self):\n \n     @require_torch\n     def test_doubly_passed_kwargs_audio(self):\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n \n         feature_extractor = self.get_component(\"feature_extractor\")\n@@ -646,7 +638,7 @@ def test_doubly_passed_kwargs_audio(self):\n     @require_torch\n     @require_vision\n     def test_structured_kwargs_audio_nested(self):\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n \n         feature_extractor = self.get_component(\"feature_extractor\")\n@@ -670,7 +662,7 @@ def test_structured_kwargs_audio_nested(self):\n         self.assertEqual(len(inputs[self.text_input_name][0]), 76)\n \n     def test_tokenizer_defaults_preserved_by_kwargs_video(self):\n-        if \"video_processor\" not in self.processor_class.attributes:\n+        if \"video_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=167, padding=\"max_length\")\n@@ -689,7 +681,7 @@ def test_video_processor_defaults_preserved_by_video_kwargs(self):\n         We then check that the mean of the pixel_values is less than or equal to 0 after processing.\n         Since the original pixel_values are in [0, 255], this is a good indicator that the rescale_factor is indeed applied.\n         \"\"\"\n-        if \"video_processor\" not in self.processor_class.attributes:\n+        if \"video_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"video_processor\"] = self.get_component(\n@@ -708,7 +700,7 @@ def test_video_processor_defaults_preserved_by_video_kwargs(self):\n         self.assertLessEqual(inputs[self.videos_input_name][0].mean(), 0)\n \n     def test_kwargs_overrides_default_tokenizer_kwargs_video(self):\n-        if \"video_processor\" not in self.processor_class.attributes:\n+        if \"video_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", padding=\"longest\")\n@@ -729,7 +721,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs_video(self):\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 162)\n \n     def test_kwargs_overrides_default_video_processor_kwargs(self):\n-        if \"video_processor\" not in self.processor_class.attributes:\n+        if \"video_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"video_processor\"] = self.get_component(\n@@ -755,7 +747,7 @@ def test_kwargs_overrides_default_video_processor_kwargs(self):\n         self.assertLessEqual(inputs[self.videos_input_name][0].mean(), 0)\n \n     def test_unstructured_kwargs_video(self):\n-        if \"video_processor\" not in self.processor_class.attributes:\n+        if \"video_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_kwargs = self.prepare_processor_dict()\n@@ -779,7 +771,7 @@ def test_unstructured_kwargs_video(self):\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 176)\n \n     def test_unstructured_kwargs_batched_video(self):\n-        if \"video_processor\" not in self.processor_class.attributes:\n+        if \"video_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_kwargs = self.prepare_processor_dict()\n@@ -806,7 +798,7 @@ def test_unstructured_kwargs_batched_video(self):\n         )\n \n     def test_doubly_passed_kwargs_video(self):\n-        if \"video_processor\" not in self.processor_class.attributes:\n+        if \"video_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_kwargs = self.prepare_processor_dict()\n@@ -826,7 +818,7 @@ def test_doubly_passed_kwargs_video(self):\n             )\n \n     def test_structured_kwargs_nested_video(self):\n-        if \"video_processor\" not in self.processor_class.attributes:\n+        if \"video_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_kwargs = self.prepare_processor_dict()\n@@ -850,7 +842,7 @@ def test_structured_kwargs_nested_video(self):\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 176)\n \n     def test_structured_kwargs_nested_from_dict_video(self):\n-        if \"video_processor\" not in self.processor_class.attributes:\n+        if \"video_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_kwargs = self.prepare_processor_dict()\n@@ -873,7 +865,7 @@ def test_structured_kwargs_nested_from_dict_video(self):\n     # TODO: the same test, but for audio + text processors that have strong overlap in kwargs\n     # TODO (molbap) use the same structure of attribute kwargs for other tests to avoid duplication\n     def test_overlapping_text_image_kwargs_handling(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n+        if \"image_processor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n \n         processor_components = self.prepare_components()\n@@ -898,7 +890,7 @@ def test_overlapping_text_audio_kwargs_handling(self):\n         Checks that `padding`, or any other overlap arg between audio extractor and tokenizer\n         is be passed to only text and ignored for audio for BC purposes\n         \"\"\"\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n \n         processor_components = self.prepare_components()\n@@ -913,31 +905,6 @@ def test_overlapping_text_audio_kwargs_handling(self):\n         # padding = True should not raise an error and will if the audio processor popped its value to None\n         _ = processor(text=input_str, audio=raw_speech, padding=True, return_tensors=\"pt\")\n \n-    def test_prepare_and_validate_optional_call_args(self):\n-        processor = self.get_processor()\n-        optional_call_args_name = getattr(processor, \"optional_call_args\", [])\n-        num_optional_call_args = len(optional_call_args_name)\n-        if num_optional_call_args == 0:\n-            self.skipTest(\"No optional call args\")\n-        # test all optional call args are given\n-        optional_call_args = processor.prepare_and_validate_optional_call_args(\n-            *(f\"optional_{i}\" for i in range(num_optional_call_args))\n-        )\n-        self.assertEqual(\n-            optional_call_args, {arg_name: f\"optional_{i}\" for i, arg_name in enumerate(optional_call_args_name)}\n-        )\n-        # test only one optional call arg is given\n-        optional_call_args = processor.prepare_and_validate_optional_call_args(\"optional_1\")\n-        self.assertEqual(optional_call_args, {optional_call_args_name[0]: \"optional_1\"})\n-        # test no optional call arg is given\n-        optional_call_args = processor.prepare_and_validate_optional_call_args()\n-        self.assertEqual(optional_call_args, {})\n-        # test too many optional call args are given\n-        with self.assertRaises(ValueError):\n-            processor.prepare_and_validate_optional_call_args(\n-                *(f\"optional_{i}\" for i in range(num_optional_call_args + 1))\n-            )\n-\n     def test_chat_template_save_loading(self):\n         processor = self.processor_class.from_pretrained(self.tmpdirname)\n         signature = inspect.signature(processor.__init__)\n@@ -991,7 +958,7 @@ def _test_apply_chat_template(\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n-        if processor_name not in self.processor_class.attributes:\n+        if processor_name not in self.processor_class.get_attributes():\n             self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n \n         # some models have only Fast image processor\n@@ -1252,7 +1219,7 @@ def test_chat_template_audio_from_video(self):\n         ):\n             self.skipTest(f\"{self.processor_class} does not support video inputs\")\n \n-        if \"feature_extractor\" not in self.processor_class.attributes:\n+        if \"feature_extractor\" not in self.processor_class.get_attributes():\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n \n         video_file_path = hf_hub_download("
        },
        {
            "sha": "b1b43abc5c500a3327f00b662bc0b9deaf05432e",
            "filename": "utils/create_dummy_models.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/utils%2Fcreate_dummy_models.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/utils%2Fcreate_dummy_models.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcreate_dummy_models.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -300,7 +300,7 @@ def build_processor(config_class, processor_class, allow_no_checkpoint=False):\n         # Try to build each component (tokenizer & feature extractor) of a `ProcessorMixin`.\n         if issubclass(processor_class, ProcessorMixin):\n             attrs = {}\n-            for attr_name in processor_class.attributes:\n+            for attr_name in processor_class.get_attributes():\n                 attrs[attr_name] = []\n                 # This could be a tuple (for tokenizers). For example, `CLIPProcessor` has\n                 #   - feature_extractor_class = \"CLIPFeatureExtractor\""
        },
        {
            "sha": "19c164716f412c364a28291367d106a9a5c38034",
            "filename": "utils/test_module/custom_processing.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f065e402fc939758eabcfe182f34543258ac94ca/utils%2Ftest_module%2Fcustom_processing.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f065e402fc939758eabcfe182f34543258ac94ca/utils%2Ftest_module%2Fcustom_processing.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ftest_module%2Fcustom_processing.py?ref=f065e402fc939758eabcfe182f34543258ac94ca",
            "patch": "@@ -2,5 +2,5 @@\n \n \n class CustomProcessor(ProcessorMixin):\n-    feature_extractor_class = \"AutoFeatureExtractor\"\n-    tokenizer_class = \"AutoTokenizer\"\n+    def __init__(self, feature_extractor, tokenizer):\n+        super().__init__(feature_extractor, tokenizer)"
        }
    ],
    "stats": {
        "total": 1207,
        "additions": 368,
        "deletions": 839
    }
}