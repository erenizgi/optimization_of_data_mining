{
    "author": "jeasinema",
    "message": "Fixing the example in generation strategy doc (#37598)\n\nUpdate generation_strategies.md\n\nThe prompt text shown in the example does not match what is inside the generated output. As the generated output always include the prompt, the correct prompt should be \"Hugging Face is an open-source company\".",
    "sha": "e1f379bb090390acb62d36b7037cd93950ed322b",
    "files": [
        {
            "sha": "55889f880b59e8397ed37825dc2c39b53195939e",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1f379bb090390acb62d36b7037cd93950ed322b/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1f379bb090390acb62d36b7037cd93950ed322b/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=e1f379bb090390acb62d36b7037cd93950ed322b",
            "patch": "@@ -31,7 +31,7 @@ import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n-inputs = tokenizer(\"I look forward to\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n \n model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n # explicitly set to default length because Llama2 generation length is 4096"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}